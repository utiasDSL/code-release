"{\"Conference\":{\"0\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"2\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"3\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"4\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"5\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"6\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"7\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"8\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"9\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"10\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"11\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"12\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"13\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"14\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"15\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"16\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"17\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"18\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"19\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"20\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"21\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"22\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"23\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"24\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"25\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"26\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"27\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"28\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"29\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"30\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"31\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"32\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"33\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"34\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"35\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"36\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"37\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"38\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"39\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"40\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"41\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"42\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"43\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"44\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"45\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"46\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"47\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"48\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"49\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"50\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"51\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"52\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"53\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"54\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"55\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"56\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"57\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"58\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"59\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"60\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"61\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"62\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"63\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"64\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"65\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"66\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"67\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"68\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"69\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"70\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"71\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"72\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"73\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"74\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"75\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"76\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"77\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"78\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"79\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"80\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"81\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"82\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"83\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"84\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"85\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"86\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"87\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"88\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"89\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"90\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"91\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"92\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"93\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"94\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"95\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"96\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"97\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"98\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"99\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"100\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"101\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"102\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"103\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"104\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"105\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"106\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"107\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"108\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"109\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"110\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"111\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"112\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"113\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"114\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"115\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"116\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"117\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"118\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"119\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"120\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"121\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"122\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"123\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"124\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"125\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"126\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"127\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"128\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"129\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"130\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"131\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"132\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"133\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"134\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"135\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"136\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"137\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"138\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"139\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"140\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"141\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"142\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"143\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"144\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"145\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"146\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"147\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"148\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"149\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"150\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"151\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"152\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"153\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"154\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"155\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"156\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"157\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"158\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"159\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"160\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"161\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"162\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"163\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"164\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"165\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"166\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"167\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"168\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"169\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"170\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"171\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"172\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"173\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"174\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"175\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"176\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"177\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"178\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"179\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"180\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"181\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"182\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"183\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"184\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"185\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"186\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"187\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"188\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"189\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"190\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"191\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"192\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"193\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"194\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"195\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"196\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"197\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"198\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"199\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"200\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"201\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"202\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"203\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"204\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"205\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"206\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"207\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"208\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"209\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"210\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"211\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"212\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"213\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"214\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"215\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"216\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"217\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"218\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"219\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"220\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"221\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"222\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"223\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"224\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"225\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"226\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"227\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"228\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"229\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"230\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"231\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"232\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"233\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"234\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"235\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"236\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"237\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"238\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"239\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"240\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"241\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"242\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"243\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"244\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"245\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"246\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"247\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"248\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"249\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"250\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"251\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"252\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"253\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"254\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"255\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"256\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"257\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"258\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"259\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"260\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"261\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"262\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"263\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"264\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"265\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"266\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"267\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"268\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"269\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"270\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"271\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"272\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"273\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"274\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"275\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"276\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"277\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"278\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"279\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"280\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"281\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"282\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"283\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"284\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"285\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"286\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"287\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"288\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"289\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"290\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"291\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"292\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"293\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"294\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"295\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"296\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"297\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"298\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"299\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"300\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"301\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"302\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"303\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"304\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"305\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"306\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"307\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"308\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"309\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"310\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"311\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"312\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"313\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"314\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"315\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"316\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"317\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"318\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"319\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"320\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"321\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"322\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"323\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"324\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"325\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"326\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"327\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"328\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"329\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"330\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"331\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"332\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"333\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"334\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"335\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"336\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"337\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"338\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"339\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"340\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"341\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"342\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"343\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"344\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"345\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"346\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"347\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"348\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"349\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"350\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"351\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"352\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"353\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"354\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"355\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"356\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"357\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"358\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"359\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"360\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"361\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"362\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"363\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"364\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"365\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"366\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"367\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"368\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"369\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"370\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"371\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"372\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"373\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"374\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"375\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"376\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"377\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"378\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"379\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"380\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"381\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"382\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"383\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"384\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"385\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"386\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"387\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"388\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"389\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"390\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"391\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"392\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"393\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"394\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"395\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"396\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"397\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"398\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"399\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"400\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"401\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"402\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"403\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"404\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"405\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"406\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"407\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"408\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"409\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"410\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"411\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"412\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"413\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"414\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"415\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"416\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"417\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"418\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"419\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"420\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"421\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"422\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"423\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"424\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"425\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"426\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"427\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"428\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"429\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"430\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"431\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"432\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"433\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"434\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"435\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"436\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"437\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"438\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"439\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"440\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"441\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"442\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"443\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"444\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"445\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"446\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"447\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"448\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"449\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"450\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"451\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"452\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"453\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"454\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"455\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"456\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"457\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"458\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"459\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"460\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"461\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"462\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"463\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"464\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"465\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"466\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"467\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"468\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"469\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"470\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"471\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"472\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"473\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"474\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"475\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"476\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"477\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"478\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"479\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"480\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"481\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"482\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"483\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"484\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"485\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"486\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"487\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"488\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"489\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"490\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"491\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"492\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"493\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"494\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"495\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"496\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"497\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"498\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"499\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"500\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"501\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"502\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"503\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"504\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"505\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"506\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"507\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"508\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"509\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"510\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"511\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"512\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"513\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"514\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"515\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"516\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"517\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"518\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"519\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"520\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"521\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"522\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"523\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"524\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"525\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"526\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"527\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"528\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"529\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"530\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"531\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"532\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"533\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"534\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"535\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"536\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"537\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"538\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"539\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"540\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"541\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"542\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"543\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"544\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"545\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"546\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"547\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"548\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"549\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"550\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"551\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"552\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"553\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"554\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"555\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"556\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"557\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"558\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"559\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"560\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"561\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"562\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"563\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"564\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"565\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"566\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"567\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"568\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"569\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"570\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"571\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"572\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"573\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"574\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"575\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"576\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"577\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"578\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"579\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"580\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"581\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"582\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"583\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"584\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"585\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"586\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"587\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"588\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"589\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"590\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"591\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"592\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"593\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"594\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"595\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"596\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"597\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"598\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"599\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"600\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"601\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"602\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"603\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"604\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"605\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"606\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"607\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"608\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"609\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"610\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"611\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"612\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"613\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"614\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"615\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"616\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"617\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"618\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"619\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"620\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"621\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"622\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"623\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"624\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"625\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"626\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"627\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"628\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"629\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"630\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"631\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"632\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"633\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"634\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"635\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"636\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"637\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"638\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"639\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"640\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"641\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"642\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"643\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"644\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"645\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"646\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"647\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"648\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"649\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"650\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"651\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"652\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"653\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"654\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"655\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"656\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"657\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"658\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"659\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"660\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"661\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"662\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"663\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"664\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"665\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"666\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"667\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"668\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"669\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"670\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"671\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"672\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"673\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"674\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"675\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"676\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"677\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"678\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"679\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"680\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"681\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"682\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"683\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"684\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"685\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"686\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"687\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"688\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"689\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"690\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"691\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"692\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"693\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"694\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"695\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"696\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"697\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"698\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"699\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"700\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"701\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"702\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"703\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"704\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"705\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"706\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"707\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"708\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"709\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"710\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"711\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"712\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"713\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"714\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"715\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"716\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"717\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"718\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"719\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"720\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"721\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"722\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"723\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"724\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"725\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"726\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"727\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"728\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"729\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"730\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"731\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"732\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"733\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"734\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"735\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"736\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"737\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"738\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"739\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"740\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"741\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"742\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"743\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"744\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"745\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"746\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"747\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"748\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"749\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"750\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"751\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"752\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"753\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"754\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"755\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"756\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"757\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"758\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"759\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"760\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"761\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"762\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"763\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"764\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"765\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"766\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"767\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"768\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"769\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"770\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"771\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"772\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"773\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"774\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"775\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"776\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"777\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"778\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"779\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"780\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"781\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"782\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"783\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"784\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"785\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"786\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"787\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"788\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"789\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"790\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"791\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"792\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"793\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"794\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"795\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"796\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"797\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"798\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"799\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"800\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"801\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"802\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"803\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"804\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"805\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"806\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"807\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"808\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"809\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"810\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"811\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"812\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"813\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"814\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"815\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"816\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"817\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"818\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"819\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"820\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"821\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"822\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"823\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"824\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"825\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"826\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"827\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"828\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"829\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"830\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"831\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"832\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"833\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"834\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"835\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"836\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"837\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"838\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"839\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"840\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"841\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"842\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"843\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"844\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"845\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"846\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"847\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"848\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"849\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"850\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"851\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"852\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"853\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"854\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"855\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"856\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"857\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"858\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"859\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"860\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"861\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"862\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"863\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"864\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"865\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"866\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"867\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"868\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"869\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"870\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"871\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"872\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"873\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"874\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"875\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"876\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"877\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"878\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"879\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"880\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"881\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"882\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"883\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"884\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"885\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"886\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"887\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"888\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"889\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"890\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"891\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"892\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"893\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"894\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"895\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"896\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"897\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"898\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"899\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"900\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"901\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"902\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"903\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"904\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"905\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"906\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"907\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"908\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"909\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"910\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"911\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"912\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"913\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"914\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"915\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"916\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"917\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"918\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"919\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"920\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"921\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"922\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"923\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"924\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"925\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"926\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"927\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"928\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"929\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"930\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"931\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"932\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"933\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"934\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"935\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"936\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"937\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"938\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"939\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"940\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"941\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"942\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"943\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"944\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"945\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"946\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"947\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"948\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"949\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"950\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"951\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"952\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"953\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"954\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"955\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"956\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"957\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"958\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"959\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"960\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"961\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"962\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"963\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"964\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"965\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"966\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"967\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"968\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"969\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"970\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"971\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"972\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"973\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"974\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"975\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"976\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"977\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"978\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"979\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"980\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"981\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"982\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"983\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"984\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"985\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"986\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"987\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"988\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"989\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"990\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"991\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"992\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"993\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"994\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"995\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"996\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"997\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"998\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"999\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1000\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1001\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1002\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1003\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1004\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1005\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1006\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1007\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1008\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1009\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1010\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1011\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1012\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1013\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1014\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1015\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1016\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1017\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1018\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1019\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1020\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1021\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1022\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1023\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1024\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1025\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1026\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1027\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1028\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1029\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1030\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1031\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1032\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1033\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1034\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1035\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1036\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1037\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1038\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1039\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1040\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1041\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1042\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1043\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1044\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1045\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1046\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1047\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1048\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1049\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1050\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1051\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1052\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1053\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1054\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1055\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1056\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1057\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1058\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1059\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1060\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1061\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1062\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1063\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1064\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1065\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1066\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1067\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1068\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1069\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1070\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1071\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"1072\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\"},\"Year\":{\"0\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"2\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"3\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"4\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"5\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"6\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"7\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"8\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"9\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"10\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"11\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"12\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"13\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"14\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"15\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"16\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"17\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"18\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"19\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"20\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"21\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"22\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"23\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"24\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"25\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"26\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"27\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"28\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"29\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"30\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"31\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"32\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"33\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"34\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"35\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"36\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"37\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"38\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"39\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"40\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"41\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"42\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"43\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"44\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"45\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"46\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"47\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"48\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"49\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"50\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"51\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"52\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"53\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"54\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"55\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"56\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"57\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"58\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"59\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"60\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"61\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"62\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"63\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"64\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"65\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"66\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"67\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"68\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"69\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"70\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"71\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"72\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"73\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"74\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"75\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"76\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"77\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"78\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"79\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"80\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"81\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"82\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"83\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"84\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"85\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"86\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"87\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"88\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"89\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"90\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"91\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"92\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"93\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"94\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"95\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"96\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"97\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"98\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"99\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"100\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"101\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"102\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"103\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"104\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"105\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"106\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"107\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"108\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"109\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"110\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"111\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"112\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"113\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"114\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"115\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"116\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"117\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"118\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"119\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"120\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"121\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"122\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"123\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"124\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"125\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"126\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"127\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"128\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"129\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"130\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"131\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"132\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"133\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"134\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"135\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"136\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"137\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"138\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"139\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"140\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"141\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"142\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"143\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"144\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"145\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"146\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"147\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"148\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"149\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"150\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"151\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"152\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"153\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"154\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"155\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"156\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"157\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"158\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"159\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"160\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"161\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"162\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"163\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"164\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"165\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"166\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"167\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"168\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"169\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"170\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"171\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"172\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"173\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"174\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"175\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"176\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"177\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"178\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"179\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"180\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"181\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"182\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"183\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"184\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"185\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"186\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"187\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"188\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"189\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"190\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"191\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"192\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"193\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"194\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"195\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"196\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"197\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"198\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"199\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"200\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"201\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"202\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"203\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"204\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"205\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"206\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"207\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"208\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"209\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"210\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"211\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"212\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"213\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"214\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"215\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"216\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"217\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"218\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"219\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"220\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"221\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"222\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"223\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"224\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"225\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"226\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"227\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"228\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"229\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"230\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"231\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"232\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"233\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"234\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"235\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"236\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"237\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"238\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"239\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"240\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"241\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"242\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"243\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"244\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"245\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"246\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"247\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"248\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"249\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"250\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"251\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"252\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"253\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"254\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"255\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"256\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"257\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"258\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"259\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"260\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"261\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"262\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"263\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"264\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"265\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"266\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"267\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"268\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"269\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"270\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"271\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"272\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"273\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"274\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"275\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"276\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"277\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"278\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"279\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"280\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"281\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"282\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"283\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"284\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"285\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"286\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"287\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"288\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"289\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"290\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"291\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"292\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"293\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"294\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"295\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"296\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"297\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"298\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"299\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"300\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"301\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"302\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"303\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"304\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"305\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"306\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"307\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"308\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"309\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"310\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"311\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"312\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"313\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"314\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"315\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"316\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"317\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"318\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"319\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"320\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"321\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"322\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"323\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"324\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"325\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"326\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"327\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"328\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"329\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"330\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"331\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"332\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"333\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"334\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"335\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"336\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"337\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"338\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"339\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"340\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"341\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"342\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"343\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"344\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"345\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"346\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"347\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"348\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"349\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"350\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"351\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"352\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"353\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"354\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"355\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"356\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"357\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"358\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"359\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"360\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"361\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"362\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"363\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"364\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"365\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"366\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"367\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"368\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"369\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"370\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"371\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"372\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"373\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"374\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"375\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"376\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"377\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"378\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"379\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"380\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"381\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"382\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"383\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"384\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"385\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"386\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"387\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"388\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"389\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"390\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"391\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"392\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"393\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"394\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"395\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"396\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"397\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"398\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"399\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"400\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"401\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"402\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"403\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"404\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"405\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"406\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"407\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"408\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"409\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"410\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"411\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"412\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"413\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"414\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"415\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"416\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"417\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"418\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"419\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"420\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"421\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"422\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"423\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"424\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"425\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"426\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"427\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"428\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"429\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"430\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"431\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"432\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"433\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"434\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"435\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"436\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"437\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"438\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"439\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"440\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"441\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"442\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"443\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"444\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"445\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"446\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"447\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"448\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"449\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"450\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"451\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"452\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"453\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"454\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"455\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"456\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"457\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"458\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"459\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"460\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"461\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"462\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"463\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"464\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"465\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"466\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"467\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"468\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"469\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"470\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"471\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"472\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"473\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"474\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"475\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"476\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"477\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"478\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"479\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"480\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"481\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"482\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"483\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"484\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"485\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"486\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"487\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"488\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"489\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"490\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"491\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"492\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"493\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"494\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"495\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"496\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"497\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"498\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"499\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"500\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"501\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"502\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"503\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"504\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"505\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"506\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"507\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"508\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"509\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"510\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"511\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"512\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"513\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"514\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"515\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"516\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"517\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"518\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"519\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"520\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"521\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"522\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"523\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"524\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"525\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"526\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"527\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"528\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"529\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"530\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"531\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"532\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"533\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"534\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"535\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"536\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"537\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"538\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"539\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"540\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"541\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"542\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"543\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"544\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"545\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"546\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"547\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"548\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"549\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"550\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"551\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"552\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"553\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"554\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"555\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"556\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"557\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"558\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"559\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"560\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"561\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"562\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"563\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"564\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"565\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"566\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"567\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"568\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"569\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"570\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"571\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"572\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"573\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"574\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"575\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"576\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"577\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"578\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"579\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"580\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"581\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"582\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"583\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"584\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"585\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"586\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"587\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"588\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"589\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"590\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"591\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"592\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"593\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"594\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"595\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"596\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"597\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"598\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"599\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"600\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"601\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"602\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"603\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"604\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"605\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"606\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"607\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"608\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"609\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"610\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"611\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"612\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"613\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"614\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"615\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"616\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"617\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"618\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"619\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"620\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"621\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"622\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"623\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"624\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"625\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"626\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"627\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"628\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"629\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"630\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"631\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"632\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"633\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"634\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"635\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"636\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"637\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"638\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"639\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"640\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"641\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"642\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"643\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"644\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"645\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"646\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"647\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"648\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"649\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"650\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"651\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"652\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"653\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"654\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"655\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"656\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"657\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"658\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"659\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"660\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"661\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"662\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"663\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"664\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"665\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"666\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"667\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"668\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"669\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"670\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"671\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"672\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"673\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"674\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"675\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"676\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"677\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"678\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"679\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"680\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"681\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"682\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"683\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"684\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"685\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"686\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"687\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"688\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"689\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"690\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"691\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"692\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"693\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"694\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"695\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"696\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"697\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"698\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"699\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"700\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"701\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"702\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"703\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"704\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"705\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"706\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"707\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"708\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"709\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"710\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"711\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"712\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"713\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"714\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"715\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"716\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"717\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"718\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"719\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"720\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"721\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"722\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"723\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"724\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"725\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"726\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"727\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"728\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"729\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"730\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"731\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"732\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"733\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"734\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"735\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"736\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"737\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"738\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"739\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"740\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"741\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"742\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"743\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"744\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"745\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"746\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"747\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"748\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"749\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"750\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"751\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"752\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"753\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"754\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"755\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"756\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"757\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"758\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"759\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"760\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"761\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"762\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"763\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"764\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"765\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"766\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"767\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"768\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"769\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"770\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"771\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"772\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"773\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"774\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"775\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"776\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"777\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"778\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"779\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"780\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"781\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"782\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"783\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"784\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"785\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"786\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"787\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"788\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"789\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"790\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"791\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"792\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"793\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"794\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"795\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"796\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"797\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"798\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"799\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"800\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"801\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"802\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"803\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"804\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"805\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"806\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"807\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"808\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"809\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"810\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"811\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"812\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"813\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"814\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"815\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"816\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"817\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"818\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"819\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"820\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"821\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"822\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"823\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"824\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"825\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"826\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"827\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"828\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"829\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"830\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"831\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"832\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"833\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"834\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"835\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"836\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"837\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"838\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"839\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"840\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"841\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"842\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"843\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"844\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"845\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"846\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"847\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"848\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"849\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"850\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"851\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"852\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"853\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"854\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"855\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"856\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"857\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"858\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"859\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"860\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"861\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"862\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"863\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"864\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"865\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"866\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"867\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"868\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"869\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"870\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"871\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"872\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"873\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"874\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"875\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"876\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"877\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"878\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"879\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"880\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"881\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"882\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"883\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"884\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"885\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"886\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"887\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"888\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"889\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"890\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"891\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"892\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"893\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"894\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"895\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"896\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"897\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"898\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"899\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"900\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"901\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"902\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"903\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"904\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"905\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"906\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"907\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"908\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"909\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"910\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"911\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"912\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"913\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"914\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"915\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"916\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"917\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"918\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"919\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"920\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"921\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"922\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"923\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"924\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"925\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"926\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"927\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"928\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"929\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"930\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"931\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"932\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"933\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"934\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"935\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"936\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"937\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"938\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"939\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"940\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"941\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"942\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"943\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"944\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"945\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"946\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"947\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"948\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"949\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"950\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"951\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"952\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"953\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"954\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"955\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"956\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"957\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"958\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"959\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"960\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"961\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"962\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"963\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"964\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"965\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"966\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"967\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"968\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"969\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"970\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"971\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"972\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"973\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"974\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"975\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"976\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"977\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"978\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"979\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"980\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"981\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"982\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"983\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"984\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"985\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"986\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"987\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"988\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"989\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"990\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"991\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"992\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"993\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"994\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"995\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"996\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"997\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"998\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"999\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1000\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1001\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1002\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1003\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1004\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1005\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1006\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1007\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1008\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1009\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1010\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1011\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1012\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1013\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1014\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1015\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1016\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1017\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1018\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1019\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1020\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1021\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1022\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1023\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1024\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1025\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1026\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1027\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1028\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1029\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1030\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1031\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1032\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1033\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1034\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1035\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1036\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1037\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1038\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1039\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1040\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1041\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1042\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1043\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1044\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1045\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1046\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1047\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1048\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1049\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1050\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1051\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1052\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1053\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1054\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1055\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1056\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1057\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1058\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1059\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1060\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1061\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1062\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1063\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1064\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1065\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1066\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1067\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1068\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1069\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1070\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1071\":\"Date of Conference: 31 May 2020 - 31 August 2020\",\"1072\":\"Date of Conference: 31 May 2020 - 31 August 2020\"},\"Paper Title\":{\"0\":\"Metrically-Scaled Monocular SLAM using Learned Scale Factors\",\"1\":\"Inertial-Only Optimization for Visual-Inertial Initialization\",\"2\":\"Hierarchical Quadtree Feature Optical Flow Tracking Based Sparse Pose-Graph Visual-Inertial SLAM\",\"3\":\"Keypoint Description by Descriptor Fusion Using Autoencoders\",\"4\":\"Towards Noise Resilient SLAM\",\"5\":\"LAMP: Large-Scale Autonomous Mapping and Positioning for Exploration of Perceptually-Degraded Subterranean Environments\",\"6\":\"BayesOD: A Bayesian Approach for Uncertainty Estimation in Deep Object Detectors\",\"7\":\"Learning Object Placements For Relational Instructions by Hallucinating Scene Representations\",\"8\":\"FADNet: A Fast and Accurate Network for Disparity Estimation\",\"9\":\"Training Adversarial Agents to Exploit Weaknesses in Deep Control Policies\",\"10\":\"TRASS: Time Reversal as Self-Supervision\",\"11\":\"Advanced BIT (ABIT): Sampling-Based Planning with Advanced Graph-Search Techniques\",\"12\":\"Voxel-based General Voronoi Diagram for Complex Data with Application on Motion Planning\",\"13\":\"Dynamic Movement Primitives for moving goals with temporal scaling adaptation\",\"14\":\"Navigating Discrete Difference Equation Governed WMR by Virtual Linear Leader Guided HMPC\",\"15\":\"Aggregation and localization of simple robots in curved environments\",\"16\":\"Stable Control in Climbing and Descending Flight under Upper Walls using Ceiling Effect Model based on Aerodynamics\",\"17\":\"Motion Primitives-based Path Planning for Fast and Agile Exploration using Aerial Robots\",\"18\":\"Unsupervised Anomaly Detection for Self-flying Delivery Drones\",\"19\":\"Keyfilter-Aware Real-Time UAV Object Tracking\",\"20\":\"Aerial Regrasping: Pivoting with Transformable Multilink Aerial Robot\",\"21\":\"Grounding Language to Landmarks in Arbitrary Outdoor Environments\",\"22\":\"Deep Merging: Vehicle Merging Controller Based on Deep Reinforcement Learning with Embedding Network\",\"23\":\"Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar Labels\",\"24\":\"Robust Lane Detection with Binary Integer Optimization\",\"25\":\"A Synchronization Approach for Achieving Cooperative Adaptive Cruise Control Based Non-Stop Intersection Passing\",\"26\":\"Urban Driving with Conditional Imitation Learning\",\"27\":\"Vehicle Localization Based on Visual Lane Marking and Topological Map Matching\",\"28\":\"RISE: A Novel Indoor Visual Place Recogniser\",\"29\":\"Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching\",\"30\":\"ICS: Incremental Constrained Smoothing for State Estimation\",\"31\":\"Drone-aided Localization in LoRa IoT Networks\",\"32\":\"A fast and practical method of indoor localization for resource-constrained devices with limited sensing\",\"33\":\"Long-Term Robot Navigation in Indoor Environments Estimating Patterns in Traversability Changes\",\"34\":\"Sample-and-computation-efficient Probabilistic Model Predictive Control with Random Features\",\"35\":\"Sample-Efficient Robot Motion Learning using Gaussian Process Latent Variable Models\",\"36\":\"Iterative Learning based feedforward control for Transition of a Biplane-Quadrotor Tailsitter UAS\",\"37\":\"Reinforcement Learning for Adaptive Illumination with X-rays\",\"38\":\"Efficient Updates for Data Association with Mixtures of Gaussian Processes\",\"39\":\"Real-time Data Driven Precision Estimator for RAVEN-II Surgical Robot End Effector Position\",\"40\":\"Temporal Segmentation of Surgical Sub-tasks through Deep Learning with Multiple Data Sources\",\"41\":\"Controlling Assistive Robots with Learned Latent Actions\",\"42\":\"On the efficient control of series-parallel compliant articulated robots\",\"43\":\"Preintegrated Velocity Bias Estimation to Overcome Contact Nonlinearities in Legged Robot Odometry\",\"44\":\"Optimized Foothold Planning and Posture Searching for Energy-Efficient Quadruped Locomotion over Challenging Terrains\",\"45\":\"Extracting Legged Locomotion Heuristics with Regularized Predictive Control\",\"46\":\"Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning\",\"47\":\"SoRX: A Soft Pneumatic Hexapedal Robot to Traverse Rough, Steep, and Unstable Terrain\",\"48\":\"UBAT: On Jointly Optimizing UAV Trajectories and Placement of Battery Swap Stations\",\"49\":\"Efficient Multi-Agent Trajectory Planning with Feasibility Guarantee using Relative Bernstein Polynomial\",\"50\":\"Optimal Sequential Task Assignment and Path Finding for Multi-Agent Robotic Assembly Planning\",\"51\":\"Cooperative Multi-Robot Navigation in Dynamic Environment with Deep Reinforcement Learning\",\"52\":\"Adaptive Directional Path Planner for Real-Time, Energy-Efficient, Robust Navigation of Mobile Robots\",\"53\":\"Exploiting sparsity in robot trajectory optimization with direct collocation and geometric algorithms\",\"54\":\"Bi-Convex Approximation of Non-Holonomic Trajectory Optimization\",\"55\":\"Fast, Versatile, and Open-loop Stable Running Behaviors with Proprioceptive-only Sensing using Model-based Optimization\",\"56\":\"Wasserstein Distributionally Robust Motion Planning and Control with Safety Constraints Using Conditional Value-at-Risk\",\"57\":\"Grasping Fragile Objects Using A Stress-Minimization Metric\",\"58\":\"Grasp Control for Enhancing Dexterity of Parallel Grippers\",\"59\":\"Theoretical Derivation and Realization of Adaptive Grasping Based on Rotational Incipient Slip Detection\",\"60\":\"Grasp State Assessment of Deformable Objects Using Visual-Tactile Fusion Perception\",\"61\":\"Beyond Top-Grasps Through Scene Completion\",\"62\":\"Dex-Net AR: Distributed Deep Grasp Planning Using a Commodity Cellphone and Augmented Reality App\",\"63\":\"OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems\",\"64\":\"What\\u2019s in my Room? Object Recognition on Indoor Panoramic Images\",\"65\":\"FisheyeDistanceNet: Self-Supervised Scale-Aware Distance Estimation using Monocular Fisheye Camera for Autonomous Driving\",\"66\":\"360SD-Net: 360\\u00b0 Stereo Depth Estimation with Learnable Cost Volume\",\"67\":\"Omnidirectional Depth Extension Networks\",\"68\":\"3D Orientation Estimation and Vanishing Point Extraction from Single Panoramas Using Convolutional Neural Network\",\"69\":\"Curvature sensing with a spherical tactile sensor using the color-interference of a marker array\",\"70\":\"Center-of-Mass-based Robust Grasp Planning for Unknown Objects Using Tactile-Visual Sensors\",\"71\":\"OmniTact: A Multi-Directional High-Resolution Touch Sensor\",\"72\":\"Highly sensitive bio-inspired sensor for fine surface exploration and characterization\",\"73\":\"Implementing Tactile and Proximity Sensing for Crack Detection\",\"74\":\"Novel Proximity Sensor for Realizing Tactile Sense in Suction Cups\",\"75\":\"Constrained Filtering-based Fusion of Images, Events, and Inertial Measurements for Pose Estimation\",\"76\":\"Schmidt-EKF-based Visual-Inertial Moving Object Tracking\",\"77\":\"Learning View and Target Invariant Visual Servoing for Navigation\",\"78\":\"Tightly-Coupled Single-Anchor Ultra-wideband-Aided Monocular Visual Odometry System\",\"79\":\"Scaling Local Control to Large-Scale Topological Navigation\",\"80\":\"Zero-shot Imitation Learning from Demonstrations for Legged Robot Visual Navigation\",\"81\":\"Pressure-Driven Manipulator with Variable Stiffness Structure\",\"82\":\"Human Interface for Teleoperated Object Manipulation with a Soft Growing Robot\",\"83\":\"Modulating hip stiffness with a robotic exoskeleton immediately changes gait\",\"84\":\"Swing-Assist for Enhancing Stair Ambulation in a Primarily-Passive Knee Prosthesis\",\"85\":\"Proof-of-concept of a Pneumatic Ankle Foot Orthosis Powered by a Custom Compressor for Drop Foot Correction\",\"86\":\"Knowledge-Guided Reinforcement Learning Control for Robotic Lower Limb Prosthesis\",\"87\":\"Development of a Twisted String Actuator-based Exoskeleton for Hip Joint Assistance in Lifting Tasks\",\"88\":\"A Novel Portable Lower Limb Exoskeleton for Gravity Compensation during Walking\",\"89\":\"Steerable Burrowing Robot: Design, Modeling and Experiments\",\"90\":\"High Force Density Gripping with UV Activation and Sacrificial Adhesion\",\"91\":\"Stiffness optimization of a cable driven parallel robot for additive manufacturing\",\"92\":\"CAMI - Analysis, Design and Realization of a Force-Compliant Variable Cam System\",\"93\":\"Using Manipulation to Enable Adaptive Ground Mobility\",\"94\":\"SNIAE-SSE Deformation Mechanism Enabled Scalable Multicopter: Design, Modeling and Flight Performance Validation\",\"95\":\"Cooperative Autonomy and Data Fusion for Underwater Surveillance With Networked AUVs\",\"96\":\"Bidirectional Resonant Propulsion and Localization for AUVs\",\"97\":\"Hierarchical Planning in Time-Dependent Flow Fields for Marine Robots\",\"98\":\"Navigation in the Presence of Obstacles for an Agile Autonomous Underwater Vehicle\",\"99\":\"Underwater Image Super-Resolution using Deep Residual Multipliers\",\"100\":\"Nonlinear Synchronization Control for Short-Range Mobile Sensors Drifting in Geophysical Flows\",\"101\":\"Energy-based Safety in Series Elastic Actuation\",\"102\":\"Safe high impedance control of a series-elastic actuator with a disturbance observer\",\"103\":\"Variable Stiffness Springs for Energy Storage Applications\",\"104\":\"Parallel-motion Thick Origami Structure for Robotic Design\",\"105\":\"Real-time Simulation of Non-Deformable Continuous Tracks with Explicit Consideration of Friction and Grouser Geometry\",\"106\":\"Test Your SLAM! The SubT-Tunnel dataset and metric for mapping\",\"107\":\"Uncertainty Measured Markov Decision Process in Dynamic Environments\",\"108\":\"Natural Scene Facial Expression Recognition with Dimension Reduction Network\",\"109\":\"Hand Pose Estimation for Hand-Object Interaction Cases using Augmented Autoencoder\",\"110\":\"Accurate detection and 3D localization of humans using a novel YOLO-based RGB-D fusion approach and synthetic training data\",\"111\":\"Wide-range Load Sensor Using Vacuum Sealed Quartz Crystal Resonator for Simultaneous Biosignals Measurement on Bed\",\"112\":\"Joint Pedestrian Detection and Risk-level Prediction with Motion-Representation-by-Detection\",\"113\":\"Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships\",\"114\":\"Linear RGB-D SLAM for Atlanta World\",\"115\":\"Stereo Visual Inertial Odometry with Online Baseline Calibration\",\"116\":\"Lidar-Monocular Visual Odometry using Point and Line Features\",\"117\":\"Probabilistic Data Association via Mixture Models for Robust Semantic SLAM\",\"118\":\"Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy\",\"119\":\"PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points\",\"120\":\"Learning error models for graph SLAM\",\"121\":\"SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability\",\"122\":\"A 3D-Deep-Learning-based Augmented Reality Calibration Method for Robotic Environments using Depth Sensor Data\",\"123\":\"Adversarial Feature Training for Generalizable Robotic Visuomotor Control\",\"124\":\"Efficient Bimanual Manipulation Using Learned Task Schemas\",\"125\":\"Real-Time UAV Path Planning for Autonomous Urban Scene Reconstruction\",\"126\":\"A Fast Marching Gradient Sampling Strategy for Motion Planning using an Informed Certificate Set\",\"127\":\"Privacy-Aware UAV Flights through Self-Configuring Motion Planning\",\"128\":\"Improved C-Space Exploration and Path Planning for Robotic Manipulators Using Distance Information\",\"129\":\"Tuning-Free Contact-Implicit Trajectory Optimization\",\"130\":\"Robust Real-time UAV Replanning Using Guided Gradient-based Optimization and Topological Paths\",\"131\":\"Learning-based Path Planning for Autonomous Exploration of Subterranean Environments\",\"132\":\"Visual-Inertial Telepresence for Aerial Manipulation\",\"133\":\"Distributed Rotor-Based Vibration Suppression for Flexible Object Transport and Manipulation\",\"134\":\"Aerial Manipulation using Model Predictive Control for Opening a Hinged Door\",\"135\":\"Integrated Motion Planner for Real-time Aerial Videography with a Drone in a Dense Environment\",\"136\":\"FG-GMM-based Interactive Behavior Estimation for Autonomous Driving Vehicles in Ramp Merging Control\",\"137\":\"Cooperative Perception and Localization for Cooperative Driving\",\"138\":\"Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images\",\"139\":\"RoadTrack: Realtime Tracking of Road Agents in Dense and Heterogeneous Environments\",\"140\":\"Association-Free Multilateration Based on Times of Arrival\",\"141\":\"Adversarial Feature Disentanglement for Place Recognition Across Changing Appearance\",\"142\":\"A Fast and Accurate Solution for Pose Estimation from 3D Correspondences\",\"143\":\"Ground Texture Based Localization Using Compact Binary Descriptors\",\"144\":\"Reliable Data Association for Feature-Based Vehicle Localization using Geometric Hashing Methods\",\"145\":\"Context-Aware Task Execution Using Apprenticeship Learning\",\"146\":\"Hierarchical Interest-Driven Goal Babbling for Efficient Bootstrapping of Sensorimotor skills\",\"147\":\"Robot-Supervised Learning for Object Segmentation\",\"148\":\"Gradient and Log-based Active Learning for Semantic Segmentation of Crop and Weed for Agricultural Robots\",\"149\":\"Learning How to Walk: Warm-starting Optimal Control Solver with Memory of Motion\",\"150\":\"Feedback Linearization for Uncertain Systems via Reinforcement Learning\",\"151\":\"Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and Progress Prediction\",\"152\":\"Neural Network based Inverse Dynamics Identification and External Force Estimation on the da Vinci Research Kit\",\"153\":\"Reliable Trajectories for Dynamic Quadrupeds using Analytical Costs and Learned Initializations\",\"154\":\"On the Hardware Feasibility of Nonlinear Trajectory Optimization for Legged Locomotion based on a Simplified Dynamics\",\"155\":\"Agile Legged-Wheeled Reconfigurable Navigation Planner Applied on the CENTAURO Robot\",\"156\":\"Bounded haptic teleoperation of a quadruped robot\\u2019s foot posture for sensing and manipulation\",\"157\":\"Pinbot: A Walking Robot with Locking Pin Arrays for Passive Adaptability to Rough Terrains\",\"158\":\"Planning for the Unexpected: Explicitly Optimizing Motions for Ground Uncertainty in Running\",\"159\":\"One-Shot Multi-Path Planning for Robotic Applications Using Fully Convolutional Networks\",\"160\":\"Efficient Iterative Linear-Quadratic Approximations for Nonlinear Multi-Player General-Sum Differential Games\",\"161\":\"Path-Following Model Predictive Control of Ballbots\",\"162\":\"Underactuated Waypoint Trajectory Optimization for Light Painting Photography\",\"163\":\"Whole-Body Walking Generation using Contact Parametrization: A Non-Linear Trajectory Optimization Approach\",\"164\":\"Controlling Fast Height Variation of an Actively Articulated Wheeled Humanoid Robot Using Center of Mass Trajectory\",\"165\":\"Contact-Aware Controller Design for Complementarity Systems\",\"166\":\"Learning to Generate 6-DoF Grasp Poses with Reachability Awareness\",\"167\":\"Enhancing Grasp Pose Computation in Gripper Workspace Spheres\",\"168\":\"Minimal Work: A Grasp Quality Metric for Deformable Hollow Objects\",\"169\":\"Hierarchical 6-DoF Grasping with Approaching Direction Selection\",\"170\":\"Geometric Characterization of Two-Finger Basket Grasps of 2-D Objects: Contact Space Formulation\",\"171\":\"Robust Sound Source Localization considering Similarity of Back-Propagation Signals\",\"172\":\"BatVision: Learning to See 3D Spatial Layout with Two Ears\",\"173\":\"Self-Supervised Learning for Alignment of Objects and Sound\",\"174\":\"The OmniScape Dataset\",\"175\":\"An ERT-based Robotic Skin with Sparsely Distributed Electrodes: Structure, Fabrication, and DNN-based Signal Processing\",\"176\":\"FBG-Based Triaxial Force Sensor Integrated with an Eccentrically Configured Imaging Probe for Endoluminal Optical Biopsy\",\"177\":\"Calibrating a Soft ERT-Based Tactile Sensor with a Multiphysics Model and Sim-to-real Transfer Learning\",\"178\":\"Sim-to-Real Transfer for Optical Tactile Sensing\",\"179\":\"Semi-Empirical Simulation of Learned Force Response Models for Heterogeneous Elastic Objects\",\"180\":\"Low-Cost Fiducial-based 6-Axis Force-Torque Sensor\",\"181\":\"Reliable frame-to-frame motion estimation for vehicle-mounted surround-view camera systems\",\"182\":\"Enabling Topological Planning with Monocular Vision\",\"183\":\"DeepMEL: Compiling Visual Multi-Experience Localization into a Deep Neural Network\",\"184\":\"SnapNav: Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference\",\"185\":\"Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping\",\"186\":\"CityLearn: Diverse Real-World Environments for Sample-Efficient Navigation Policy Learning\",\"187\":\"High Resolution Soft Tactile Interface for Physical Human-Robot Interaction\",\"188\":\"Design and Validation of a Soft Robotic Ankle-Foot Orthosis (SR-AFO) Exosuit for Inversion and Eversion Ankle Support\",\"189\":\"Velocity Field based Active-Assistive Control for Upper Limb Rehabilitation Exoskeleton Robot\",\"190\":\"Design, Development, and Control of a Tendon-actuated Exoskeleton for Wrist Rehabilitation and Training\",\"191\":\"Impedance Control of a Transfemoral Prosthesis using Continuously Varying Ankle Impedances and Multiple Equilibria\",\"192\":\"Gait patterns generation based on basis functions interpolation for the TWIN lower-limb exoskeleton\",\"193\":\"Human-Centric Active Perception for Autonomous Observation\",\"194\":\"Prediction of Human Full-Body Movements with Motion Optimization and Recurrent Neural Networks\",\"195\":\"Predicting and Optimizing Ergonomics in Physical Human-Robot Cooperation Tasks\",\"196\":\"Active Reward Learning for Co-Robotic Vision Based Exploration in Bandwidth Limited Environments\",\"197\":\"VariPath: A Database for Modelling the Variance of Human Pathways in Manual and HRC Processes with Heavy-Duty Robots\",\"198\":\"A Compact and Low-cost Robotic Manipulator Driven by Supercoiled Polymer Actuators\",\"199\":\"Internally-Balanced Magnetic Mechanisms Using a Magnetic Spring for Producing a Large Amplified Clamping Force\",\"200\":\"A Continuum Manipulator with Closed-form Inverse Kinematics and Independently Tunable Stiffness\",\"201\":\"Design and Compensation Control of a Flexible Instrument for Endoscopic Surgery\",\"202\":\"Distance and Steering Heuristics for Streamline-Based Flow Field Planning\",\"203\":\"Enhancing Coral Reef Monitoring Utilizing a Deep Semi-Supervised Learning Approach\",\"204\":\"DOB-Net: Actively Rejecting Unknown Excessive Time-Varying Disturbances\",\"205\":\"Demonstration of Autonomous Nested Search for Local Maxima Using an Unmanned Underwater Vehicle\",\"206\":\"Towards distortion based underwater domed viewport camera calibration\",\"207\":\"How far are Pneumatic Artificial Muscles from biological muscles?\",\"208\":\"Shared Control Templates for Assistive Robotics\",\"209\":\"Enabling Robots to Understand Incomplete Natural Language Instructions Using Commonsense Reasoning\",\"210\":\"A Holistic Approach in Designing Tabletop Robot\\u2019s Expressivity\",\"211\":\"DirtNet: Visual Dirt Detection for Autonomous Cleaning Robots\",\"212\":\"Semantic Linking Maps for Active Visual Object Search\",\"213\":\"Active Depth Estimation: Stability Analysis and its Applications\",\"214\":\"VALID: A Comprehensive Virtual Aerial Image Dataset\",\"215\":\"Intensity Scan Context: Coding Intensity and Geometry Relations for Loop Closure Detection\",\"216\":\"TextSLAM: Visual SLAM with Planar Text Features\",\"217\":\"FlowNorm: A Learning-based Method for Increasing Convergence Range of Direct Alignment\",\"218\":\"Redesigning SLAM for Arbitrary Multi-Camera Systems\",\"219\":\"Dynamic SLAM: The Need For Speed\",\"220\":\"\\u2207SLAM: Dense SLAM meets Automatic Differentiation\",\"221\":\"Learning local behavioral sequences to better infer non-local properties in real multi-robot systems\",\"222\":\"Unsupervised Geometry-Aware Deep LiDAR Odometry\",\"223\":\"SA-Net: Robust State-Action Recognition for Learning from Observations\",\"224\":\"A Generative Approach for Socially Compliant Navigation\",\"225\":\"Scalable Multi-Task Imitation Learning with Autonomous Improvement\",\"226\":\"Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos\",\"227\":\"A New Path Planning Architecture to Consider Motion Uncertainty in Natural Environment\",\"228\":\"Revisiting the Asymptotic Optimality of RRT\",\"229\":\"Sample Complexity of Probabilistic Roadmaps via \\u03b5-nets\",\"230\":\"Reinforcement Learning Based Manipulation Skill Transferring for Robot-assisted Minimally Invasive Surgery\",\"231\":\"Safe Mission Planning under Dynamical Uncertainties\",\"232\":\"An Iterative Quadratic Method for General-Sum Differential Games with Feedback Linearizable Dynamics\",\"233\":\"A Morphable Aerial-Aquatic Quadrotor with Coupled Symmetric Thrust Vectoring\",\"234\":\"An Autonomous Intercept Drone with Image-based Visual Servo\",\"235\":\"On the Human Control of a Multiple Quadcopters with a Cable-suspended Payload System\",\"236\":\"A 3D Dataset: Towards Autonomous Driving in Challenging Environments\",\"237\":\"SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud\",\"238\":\"Fine-Grained Driving Behavior Prediction via Context-Aware Multi-Task Inverse Reinforcement Learning\",\"239\":\"How to Keep HD Maps for Automated Driving Up To Date\",\"240\":\"Binary DAD-Net: Binarized Driveable Area Detection Network for Autonomous Driving\",\"241\":\"UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes\",\"242\":\"Map As the Hidden Sensor: Fast Odometry-Based Global Localization\",\"243\":\"Joint Human Pose Estimation and Stereo 3D Localization\",\"244\":\"Self-Supervised Deep Pose Corrections for Robust Visual Odometry\",\"245\":\"Ultra-High-Accuracy Visual Marker for Indoor Precise Positioning\",\"246\":\"Accurate position tracking with a single UWB anchor\",\"247\":\"Preference-Based Learning for Exoskeleton Gait Optimization\",\"248\":\"Adaptive Neural Trajectory Tracking Control for Flexible-Joint Robots with Online Learning\",\"249\":\"BiCF: Learning Bidirectional Incongruity-Aware Correlation Filter for Efficient UAV Object Tracking\",\"250\":\"Adaptive Unknown Object Rearrangement Using Low-Cost Tabletop Robot\",\"251\":\"Unsupervised Learning and Exploration of Reachable Outcome Space\",\"252\":\"Context-aware Cost Shaping to Reduce the Impact of Model Error in Receding Horizon Control\",\"253\":\"Aortic 3D Deformation Reconstruction using 2D X-ray Fluoroscopy and 3D Pre-operative Data for Endovascular Interventions\",\"254\":\"Design and Kinematic Modeling of a Novel Steerable Needle for Image-Guided Insertion\",\"255\":\"Robotic needle insertion in moving soft tissues using constraint-based inverse Finite Element simulation\",\"256\":\"Collaborative Robot-Assisted Endovascular Catheterization with Generative Adversarial Imitation Learning\",\"257\":\"GA3C Reinforcement Learning for Surgical Steerable Catheter Path Planning\",\"258\":\"MPC-based Controller with Terrain Insight for Dynamic Legged Locomotion\",\"259\":\"An Adaptive Supervisory Control Approach to Dynamic Locomotion Under Parametric Uncertainty\",\"260\":\"Joint Space Position\\/Torque Hybrid Control of the Quadruped Robot for Locomotion and Push Reaction\",\"261\":\"Improved Performance on Moving-Mass Hopping Robots with Parallel Elasticity\",\"262\":\"Vision Aided Dynamic Exploration of Unstructured Terrain with a Small-Scale Quadruped Robot\",\"263\":\"Distributed Attack-Robust Submodular Maximization for Multi-Robot Planning\",\"264\":\"Multirobot Patrolling Against Adaptive Opponents with Limited Information\",\"265\":\"Distributed Optimization of Nonlinear, Non-Gaussian, Communication-Aware Information using Particle Methods\",\"266\":\"Targeted Drug Delivery: Algorithmic Methods for Collecting a Swarm of Particles with Uniform, External Forces\",\"267\":\"Enhancing Bilevel Optimization for UAV Time-Optimal Trajectory using a Duality Gap Approach\",\"268\":\"Constrained Sampling-based Trajectory Optimization using Stochastic Approximation\",\"269\":\"Learning Control Policies from Optimal Trajectories\",\"270\":\"Crocoddyl: An Efficient and Versatile Framework for Multi-Contact Optimal Control\",\"271\":\"Grasp for Stacking via Deep Reinforcement Learning\",\"272\":\"CAGE: Context-Aware Grasping Engine\",\"273\":\"Super-Pixel Sampler: a Data-driven Approach for Depth Sampling and Reconstruction\",\"274\":\"Physics-based Simulation of Continuous-Wave LIDAR for Localization, Calibration and Tracking\",\"275\":\"A Spatial-temporal Multiplexing Method for Dense 3D Surface Reconstruction of Moving Objects\",\"276\":\"PhaRaO: Direct Radar Odometry using Phase Correlation\",\"277\":\"DeepTemporalSeg: Temporally Consistent Semantic Segmentation of 3D LiDAR Scans\",\"278\":\"Discrete Bimanual Manipulation for Wrench Balancing\",\"279\":\"NeuroTac: A Neuromorphic Optical Tactile Sensor applied to Texture Recognition\",\"280\":\"Reducing Uncertainty in Pose Estimation under Complex Contacts via Force Forecast\",\"281\":\"Comparison of Constrained and Unconstrained Human Grasp Forces Using Fingernail Imaging and Visual Servoing\",\"282\":\"Robust and Efficient Estimation of Absolute Camera Pose for Monocular Visual Odometry\",\"283\":\"Robust Vision-based Obstacle Avoidance for Micro Aerial Vehicles in Dynamic Environments\",\"284\":\"Proximity Estimation Using Vision Features Computed On Sensor\",\"285\":\"Efficient Globally-Optimal Correspondence-Less Visual Odometry for Planar Ground Vehicles\",\"286\":\"egoTEB: Egocentric, Perception Space Navigation Using Timed-Elastic-Bands\",\"287\":\"Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation\",\"288\":\"Meta Reinforcement Learning for Sim-to-real Domain Adaptation\",\"289\":\"Variational Auto-Regularized Alignment for Sim-to-Real Control\",\"290\":\"Experience Selection Using Dynamics Similarity for Efficient Multi-Source Transfer Learning Between Robots\",\"291\":\"DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning\",\"292\":\"A closed-loop and ergonomic control for prosthetic wrist rotation\",\"293\":\"Comparison of online algorithms for the tracking of multiple magnetic targets in a myokinetic control interface\",\"294\":\"Congestion-aware Evacuation Routing using Augmented Reality Devices\",\"295\":\"Human-robot interaction for robotic manipulator programming in Mixed Reality\",\"296\":\"Heart Rate Sensing with a Robot Mounted mmWave Radar\",\"297\":\"Prediction of Gait Cycle Percentage Using Instrumented Shoes with Artificial Neural Networks\",\"298\":\"Flow Compensation for Hydraulic Direct-Drive System with a Single-rod Cylinder Applied to Biped Humanoid Robot\",\"299\":\"Mechanically Programmed Miniature Origami Grippers\",\"300\":\"Bio-inspired Tensegrity Fish Robot\",\"301\":\"Gaussian-Dirichlet Random Fields for Inference over High Dimensional Categorical Observations\",\"302\":\"Investigation of a Multistable Tensegrity Robot applied as Tilting Locomotion System\",\"303\":\"A Novel Articulated Soft Robot Capable of Variable Stiffness through Bistable Structure\",\"304\":\"Modeling and Experiments on the Swallowing and Disgorging Characteristics of an Underwater Continuum Manipulator\",\"305\":\"Salamanderbot: A soft-rigid composite continuum mobile robot to traverse complex environments\",\"306\":\"Flexure Hinge-based Biomimetic Thumb with a Rolling-Surface Metacarpal Joint\",\"307\":\"Ibex: A reconfigurable ground vehicle with adaptive terrain navigation capability\",\"308\":\"Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors\",\"309\":\"Generating Locomotion with Effective Wheel Radius Manipulation\",\"310\":\"A GNC Architecture for Planetary Rovers with Autonomous Navigation\",\"311\":\"Learning Face Recognition Unsupervisedly by Disentanglement and Self-Augmentation\",\"312\":\"PARC: A Plan and Activity Recognition Component for Assistive Robots\",\"313\":\"Image-Based Place Recognition on Bucolic Environment Across Seasons From Semantic Edge Description\",\"314\":\"A Multilayer-Multimodal Fusion Architecture for Pattern Recognition of Natural Manipulations in Percutaneous Coronary Interventions\",\"315\":\"Real-Time Graph-Based SLAM with Occupancy Normal Distributions Transforms\",\"316\":\"Spatio-Temporal Non-Rigid Registration of 3D Point Clouds of Plants\",\"317\":\"Uncertainty-Based Adaptive Sensor Fusion for Visual-Inertial Odometry under Various Motion Characteristics\",\"318\":\"Loam livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV\",\"319\":\"Active SLAM using 3D Submap Saliency for Underwater Volumetric Exploration\",\"320\":\"Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM\",\"321\":\"RoNIN: Robust Neural Inertial Navigation in the Wild: Benchmark, Evaluations, & New Methods\",\"322\":\"Segmenting 2K-Videos at 36.5 FPS with 24.3 GFLOPs: Accurate and Lightweight Realtime Semantic Segmentation Network\",\"323\":\"Temporally Consistent Horizon Lines\",\"324\":\"Full-Scale Continuous Synthetic Sonar Data Generation with Markov Conditional Generative Adversarial Networks\",\"325\":\"Adaptively Informed Trees (AIT): Fast Asymptotically Optimal Path Planning through Adaptive Heuristics\",\"326\":\"Informing Multi-Modal Planning with Synergistic Discrete Leads\",\"327\":\"Hierarchical Coverage Path Planning in Complex 3D Environments\",\"328\":\"Perception-aware time optimal path parameterization for quadrotors\",\"329\":\"Generating Visibility-Aware Trajectories for Cooperative and Proactive Motion Planning\",\"330\":\"An obstacle-interaction planning method for navigation of actuated vine robots\",\"331\":\"Distributed Consensus Control of Multiple UAVs in a Constrained Environment\",\"332\":\"Neural-Swarm: Decentralized Close-Proximity Multirotor Control Using Learned Interactions\",\"333\":\"Line Coverage with Multiple Robots\",\"334\":\"Visual Coverage Maintenance for Quadcopters Using Nonsmooth Barrier Functions\",\"335\":\"Goal-Directed Occupancy Prediction for Lane-Following Actors\",\"336\":\"Intent-Aware Pedestrian Prediction for Adaptive Crowd Navigation\",\"337\":\"Brno Urban Dataset - The New Data for Self-Driving Agents and Mapping Tasks\",\"338\":\"Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching\",\"339\":\"Imitative Reinforcement Learning Fusing Vision and Pure Pursuit for Self-driving\",\"340\":\"Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving\",\"341\":\"ROI-cloud: A Key Region Extraction Method for LiDAR Odometry and Localization\",\"342\":\"To Learn or Not to Learn: Visual Localization from Essential Matrices\",\"343\":\"Hierarchical Multi-Process Fusion for Visual Place Recognition\",\"344\":\"Camera Tracking in Lighting Adaptable Maps of Indoor Environments\",\"345\":\"Fast, Compact and Highly Scalable Visual Place Recognition through Sequence-based Matching of Overloaded Representations\",\"346\":\"Vision-based Multi-MAV Localization with Anonymous Relative Measurements Using Coupled Probabilistic Data Association Filter\",\"347\":\"MANGA: Method Agnostic Neural-policy Generalization and Adaptation\",\"348\":\"Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference\",\"349\":\"Variational Inference with Mixture Model Approximation for Applications in Robotics\",\"350\":\"Injection of a Fluorescent Microsensor into a Specific Cell by Laser Manipulation and Heating with Multiple Wavelengths of Light\",\"351\":\"Passive Quadrupedal Gait Synchronization for Extra Robotic Legs Using a Dynamically Coupled Double Rimless Wheel Model\",\"352\":\"Optimal Fast Entrainment Waveform for Indirectly Controlled Limit Cycle Walker Against External Disturbances\",\"353\":\"Correspondence Identification in Collaborative Robot Perception through Maximin Hypergraph Matching\",\"354\":\"Distributed Multi-Target Tracking for Autonomous Vehicle Fleets\",\"355\":\"Flying batteries: In-flight battery switching to increase multirotor flight time\",\"356\":\"Optimal Control of an Energy-Recycling Actuator for Mobile Robotics Applications\",\"357\":\"An NMPC Approach using Convex Inner Approximations for Online Motion Planning with Guaranteed Collision Avoidance\",\"358\":\"Action Image Representation: Learning Scalable Deep Grasping Policies with Zero Real World Data\",\"359\":\"High Accuracy and Efficiency Grasp Pose Detection Scheme with Dense Predictions\",\"360\":\"Transferable Active Grasping and Real Embodied Dataset\",\"361\":\"PointNet++ Grasping: Learning An End-to-end Spatial Grasp Generation Algorithm from Sparse Point Clouds\",\"362\":\"Clear Grasp: 3D Shape Estimation of Transparent Objects for Manipulation\",\"363\":\"6D Object Pose Regression via Supervised Learning on Point Clouds\",\"364\":\"YCB-M: A Multi-Camera RGB-D Dataset for Object Recognition and 6DoF Pose Estimation\",\"365\":\"Self-supervised 6D Object Pose Estimation for Robot Manipulation\",\"366\":\"Low-cost GelSight with UV Markings: Feature Extraction of Objects Using AlexNet and Optical Flow without 3D Image Reconstruction\",\"367\":\"Evaluation of Non-collocated Force Feedback Driven by Signal-independent Noise\",\"368\":\"Tactile sensing based on fingertip suction flow for submerged dexterous manipulation\",\"369\":\"Highly Robust Visual Place Recognition Through Spatial Matching of CNN Features\",\"370\":\"Online Trajectory Planning Through Combined Trajectory Optimization and Function Approximation: Application to the Exoskeleton Atalante\",\"371\":\"Act, Perceive, and Plan in Belief Space for Robot Localization\",\"372\":\"Decentralized Task Allocation in Multi-Agent Systems Using a Decentralized Genetic Algorithm\",\"373\":\"Fast and resilient manipulation planning for target retrieval in clutter\",\"374\":\"Untethered Soft Millirobot with Magnetic Actuation\",\"375\":\"Accelerated Robot Learning via Human Brain Signals\",\"376\":\"Muscle and Brain Activations in Cylindrical Rotary Controller Manipulation with Index Finger and Thumb\",\"377\":\"Real-Time Robot Reach-To-Grasp Movements Control Via EOG and EMG Signals Decoding\",\"378\":\"Simultaneous Estimations of Joint Angle and Torque in Interactions with Environments using EMG\",\"379\":\"High-Density Electromyography Based Control of Robotic Devices: On the Execution of Dexterous Manipulation Tasks\",\"380\":\"Perception-Action Coupling in Usage of Telepresence Cameras\",\"381\":\"A technical framework for human-like motion generation with autonomous anthropomorphic redundant manipulators\",\"382\":\"Real-Time Adaptive Assembly Scheduling in Human-Multi-Robot Collaboration According to Human Capability\",\"383\":\"Microscope-Guided Autonomous Clear Corneal Incision\",\"384\":\"Asynchronous and decoupled control of the position and the stiffness of a spatial RCM tensegrity mechanism for needle manipulation\",\"385\":\"Redundancy Resolution Integrated Model Predictive Control of CDPRs: Concept, Implementation and Experiments\",\"386\":\"Mechanics for Tendon Actuated Multisection Continuum Arms\",\"387\":\"Trajectory Optimization for a Six-DOF Cable-Suspended Parallel Robot with Dynamic Motions Beyond the Static Workspace\",\"388\":\"An Intelligent Spraying System with Deep Learning-based Semantic Segmentation of Fruit Trees in Orchards\",\"389\":\"An Efficient Planning and Control Framework for Pruning Fruit Trees\",\"390\":\"Context Dependant Iterative Parameter Optimisation for Robust Robot Navigation\",\"391\":\"Extending Riemmanian Motion Policies to a Class of Underactuated Wheeled-Inverted-Pendulum Robots\",\"392\":\"Augmenting Self-Stability: Height Control of a Bernoulli Ball via Bang-Bang Control\",\"393\":\"Singularity-Free Inverse Dynamics for Underactuated Systems with a Rotating Mass\",\"394\":\"Robust capture of unknown objects with a highly under-actuated gripper\",\"395\":\"SUMMIT: A Simulator for Urban Driving in Massive Mixed Traffic\",\"396\":\"A Model-Based Reinforcement Learning and Correction Framework for Process Control of Robotic Wire Arc Additive Manufacturing\",\"397\":\"Toward Optimal FDM Toolpath Planning with Monte Carlo Tree Search\",\"398\":\"Optimizing performance in automation through modular robots\",\"399\":\"Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning\",\"400\":\"SwarmMesh: A Distributed Data Structure for Cooperative Multi-Robot Applications\",\"401\":\"Avalanche victim search via robust observers\",\"402\":\"Reactive Control and Metric-Topological Planning for Exploration\",\"403\":\"Information Theoretic Active Exploration in Signed Distance Fields\",\"404\":\"Bayesian Learning-Based Adaptive Control for Safety Critical Systems\",\"405\":\"Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure\",\"406\":\"Voxel Map for Visual SLAM\",\"407\":\"Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video\",\"408\":\"Event-Based Angular Velocity Regression with Spiking Networks\",\"409\":\"Visual Odometry Revisited: What Should Be Learnt?\",\"410\":\"3D Scene Geometry-Aware Constraint for Camera Localization with Deep Learning\",\"411\":\"ACDER: Augmented Curiosity-Driven Experience Replay\",\"412\":\"TrueRMA: Learning Fast and Smooth Robot Trajectories with Recursive Midpoint Adaptations in Cartesian Space\",\"413\":\"Fog Robotics Algorithms for Distributed Motion Planning Using Lambda Serverless Computing\",\"414\":\"Exploration of 3D terrains using potential fields with elevation-based local distortions\",\"415\":\"R3T: Rapidly-exploring Random Reachable Set Tree for Optimal Kinodynamic Planning of Nonlinear Hybrid Systems\",\"416\":\"DeepSemanticHPPC: Hypothesis-based Planning over Uncertain Semantic Point Clouds\",\"417\":\"Balancing Actuation and Computing Energy in Motion Planning\",\"418\":\"Posterior Sampling for Anytime Motion Planning on Graphs with Expensive-to-Evaluate Edges\",\"419\":\"Upset Recovery Control for Quadrotors Subjected to a Complete Rotor Failure from Large Initial Disturbances\",\"420\":\"Identification and evaluation of a force model for multirotor UAVs\",\"421\":\"Preliminary Study of an Aerial Manipulator with Elastic Suspension\",\"422\":\"Towards Low-Latency High-Bandwidth Control of Quadrotors using Event Cameras\",\"423\":\"Perception-constrained and Motor-level Nonlinear MPC for both Underactuated and Tilted-propeller UAVS\",\"424\":\"Coordinate-Free Dynamics and Differential Flatness of a Class of 6DOF Aerial Manipulators\",\"425\":\"CMTS: A Conditional Multiple Trajectory Synthesizer for Generating Safety-Critical Driving Scenarios\",\"426\":\"LiDAR Inertial Odometry Aided Robust LiDAR Localization System in Changing City Scenes\",\"427\":\"Dynamic Interaction-Aware Scene Understanding for Reinforcement Learning in Autonomous Driving\",\"428\":\"Interacting Vehicle Trajectory Prediction with Convolutional Recurrent Neural Networks\",\"429\":\"Navigation Command Matching for Vision-based Autonomous Driving\",\"430\":\"GraphRQI: Classifying Driver Behaviors Using Graph Spectrums\",\"431\":\"Kidnapped Radar: Topological Radar Localisation using Rotationally-Invariant Metric Learning\",\"432\":\"Global visual localization in LiDAR-maps through shared 2D-3D embedding space\",\"433\":\"Unsupervised Learning Methods for Visual Place Recognition in Discretely and Continuously Changing Environments\",\"434\":\"LOL: Lidar-only Odometry and Localization in 3D point cloud maps\",\"435\":\"Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments\",\"436\":\"Set-membership state estimation by solving data association\",\"437\":\"A Linearly Constrained Nonparametric Framework for Imitation Learning\",\"438\":\"An Energy-based Approach to Ensure the Stability of Learned Dynamical Systems\",\"439\":\"IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data\",\"440\":\"Geometry-aware Dynamic Movement Primitives\",\"441\":\"Learning a Pile Loading Controller from Demonstrations\",\"442\":\"Learning Navigation Costs from Demonstration in Partially Observable Environments\",\"443\":\"Towards Bimanual Vein Cannulation: Preliminary Study of a Bimanual Robotic System With a Dual Force Constraint Controller\",\"444\":\"Evaluation of a combined grip of pinch and power grips in manipulating a master manipulator\",\"445\":\"Contact Stability Analysis of Magnetically-Actuated Robotic Catheter Under Surface Motion\",\"446\":\"Fast and accurate intracorporeal targeting through an anatomical orifice exhibiting unknown behavior\",\"447\":\"Robotic Swarm Control for Precise and On-Demand Embolization\",\"448\":\"Bilateral Teleoperation Control of a Redundant Manipulator with an RCM Kinematic Constraint\",\"449\":\"From Bipedal Walking to Quadrupedal Locomotion: Full-Body Dynamics Decomposition for Rapid Gait Generation\",\"450\":\"Posture Control for a Low-Cost Commercially-Available Hexapod Robot\",\"451\":\"Collaborative Multi-Robot Localization in Natural Terrain\",\"452\":\"Multi-Robot Control Using Coverage Over Time-Varying Non-Convex Domains\",\"453\":\"Efficient Large-Scale Multi-Drone Delivery Using Transit Networks\",\"454\":\"Resilience in multi-robot target tracking through reconfiguration\",\"455\":\"Teleoperation of Multi-Robot Systems to Relax Topological Constraints\",\"456\":\"Eciton robotica: Design and Algorithms for an Adaptive Self-Assembling Soft Robot Collective\",\"457\":\"Stable Tool-Use with Flexible Musculoskeletal Hands by Learning the Predictive Model of Sensor State Transition\",\"458\":\"Learning to Transfer Dynamic Models of Underactuated Soft Robotic Hands\",\"459\":\"Periodic movement learning in a soft-robotic arm\",\"460\":\"Mechanism and Model of a Soft Robot for Head Stabilization in Cancer Radiation Therapy\",\"461\":\"Learning Precise 3D Manipulation from Multiple Uncalibrated Cameras\",\"462\":\"Surfing on an uncertain edge: Precision cutting of soft tissue using torque-based medium classification\",\"463\":\"Dynamic Cloth Manipulation with Deep Reinforcement Learning\",\"464\":\"Learning to combine primitive skills: A step towards versatile robotic manipulation \\u00a7\",\"465\":\"Learning Affordance Space in Physical World for Vision-based Robotic Object Manipulation\",\"466\":\"Observability Analysis of Flight State Estimation for UAVs and Experimental Validation\",\"467\":\"OpenVINS: A Research Platform for Visual-Inertial Estimation\",\"468\":\"Decentralized Collaborative State Estimation for Aided Inertial Navigation\",\"469\":\"Analytic Combined IMU Integration (ACI2) For Visual Inertial Navigation\",\"470\":\"Second-order Kinematics for Floating-base Robots using the Redundant Acceleration Feedback of an Artificial Sensory Skin\",\"471\":\"Clock-based time synchronization for an event-based camera dataset acquisition platform\",\"472\":\"Model Predictive Impedance Control\",\"473\":\"Kinematic Modeling and Compliance Modulation of Redundant Manipulators Under Bracing Constraints\",\"474\":\"Successive Stiffness Increment and Time Domain Passivity Approach for Stable and High Bandwidth Control of Series Elastic Actuator\",\"475\":\"Arm-hand motion-force coordination for physical interactions with non-flat surfaces using dynamical systems: Toward compliant robotic massage\",\"476\":\"A Bio-Signal Enhanced Adaptive Impedance Controller for Lower Limb Exoskeleton\",\"477\":\"Differentiable Mapping Networks: Learning Structured Map Representations for Sparse Visual Localization\",\"478\":\"Attentive Task-Net: Self Supervised Task-Attention Network for Imitation Learning using Video Demonstration\",\"479\":\"OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong Deep Learning\",\"480\":\"Geometric Pretraining for Monocular Depth Estimation\",\"481\":\"Joint Rotation Angle Sensing of Flexible Endoscopic Surgical Robots\",\"482\":\"Soft, Round, High Resolution Tactile Fingertip Sensors for Dexterous Robotic Manipulation\",\"483\":\"FootTile: a Rugged Foot Sensor for Force and Center of Pressure Sensing in Soft Terrain\",\"484\":\"Learning a Control Policy for Fall Prevention on an Assistive Walking Device\",\"485\":\"Assistive Force of a Belt-type Hip Assist Suit for Lifting the Swing Leg during Walking\",\"486\":\"Soft Pneumatic System for Interface Pressure Regulation and Automated Hands-Free Donning in Robotic Prostheses\",\"487\":\"Automated detection of soleus concentric contraction in variable gait conditions for improved exosuit control\",\"488\":\"Soft Sensing Shirt for Shoulder Kinematics Estimation\",\"489\":\"Motion Reasoning for Goal-Based Imitation Learning\",\"490\":\"Flexible online adaptation of learning strategy using EEG-based reinforcement signals in real-world robotic applications\",\"491\":\"Object-oriented Semantic Graph Based Natural Question Generation\",\"492\":\"Towards Safe Human-Robot Collaboration Using Deep Reinforcement Learning\",\"493\":\"Deep compositional robotic planners that follow natural language commands\",\"494\":\"Learning User Preferences from Corrections on State Lattices\",\"495\":\"Visual Servoing-based Navigation for Monitoring Row-Crop Fields\",\"496\":\"Optimal Routing Schedules for Robots Operating in Aisle-Structures\",\"497\":\"Time Optimal Motion Planning with ZMP Stability Constraint for Timber Manipulation\",\"498\":\"Push and Drag: An Active Obstacle Separation Method for Fruit Harvesting Robots\",\"499\":\"A Novel Calibration Method between a Camera and a 3D LiDAR with Infrared Images\",\"500\":\"Online Camera-LiDAR Calibration with Sensor Semantic Information\",\"501\":\"Precise 3D Calibration of Wafer Handling Robot by Visual Detection and Tracking of Elliptic-shape Wafers\",\"502\":\"Globally Optimal Relative Pose Estimation for Camera on a Selfie Stick\",\"503\":\"Online calibration of exterior orientations of a vehicle-mounted surround-view camera system\",\"504\":\"Learning Camera Miscalibration Detection\",\"505\":\"Robotic General Parts Feeder: Bin-picking, Regrasping, and Kitting\",\"506\":\"Planning, Learning and Reasoning Framework for Robot Truck Unloading\",\"507\":\"Evaluation of Perception Latencies in a Human-Robot Collaborative Environment\",\"508\":\"Assembly of randomly placed parts realized by using only one robot arm with a general parallel-jaw gripper\",\"509\":\"Bio-Inspired Distance Estimation using the Self-Induced Acoustic Signature of a Motor-Propeller System\",\"510\":\"A bio-inspired 3-DOF light-weight manipulator with tensegrity X-joints\",\"511\":\"The Lobster-inspired Antagonistic Actuation Mechanism Towards a Bending Module\",\"512\":\"Emulating duration and curvature of coral snake anti-predator thrashing behaviors using a soft-robotic platform\",\"513\":\"Directional Mechanical Impedance of the Human Ankle During Standing with Active Muscles\",\"514\":\"Contact Surface Estimation via Haptic Perception\",\"515\":\"Local Policy Optimization for Trajectory-Centric Reinforcement Learning\",\"516\":\"Automatic Snake Gait Generation Using Model Predictive Control\",\"517\":\"On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause Detection and Identification\",\"518\":\"GOMP: Grasp-Optimized Motion Planning for Bin Picking\",\"519\":\"Motion Planning and Task Allocation for a Jumping Rover Team\",\"520\":\"Active 3D Modeling via Online Multi-View Stereo\",\"521\":\"Reoriented Short-Cuts (RSC): An Adjustment Method for Locally Optimal Path Short-Cutting in High DoF Configuration Spaces\",\"522\":\"Learning Resilient Behaviors for Navigation Under Uncertainty\",\"523\":\"Nonlinear Vector-Projection Control for Agile Fixed-Wing Unmanned Aerial Vehicles\",\"524\":\"Adaptive Nonlinear Control of Fixed-Wing VTOL with Airflow Vector Sensing\",\"525\":\"The Reconfigurable Aerial Robotic Chain: Modeling and Control\",\"526\":\"Direct Acceleration Feedback Control of Quadrotor Aerial Vehicles\",\"527\":\"Trajectory Tracking Nonlinear Model Predictive Control for an Overactuated MAV\",\"528\":\"Optimal Oscillation Damping Control of cable-Suspended Aerial Manipulator with a Single IMU Sensor\",\"529\":\"TUNERCAR: A Superoptimization Toolchain for Autonomous Racing\",\"530\":\"Risk Assessment and Planning with Bidirectional Reachability for Autonomous Driving\",\"531\":\"Game theoretic decision making based on real sensor data for autonomous vehicles\\u2019 maneuvers in high traffic\",\"532\":\"Driving in Dense Traffic with Model-Free Reinforcement Learning\",\"533\":\"Enhancing Game-Theoretic Autonomous Car Racing Using Control Barrier Functions\",\"534\":\"SPRINT: Subgraph Place Recognition for INtelligent Transportation\",\"535\":\"OneShot Global Localization: Instant LiDAR-Visual Pose Estimation\",\"536\":\"Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-to-end Robot Learning Scheme\",\"537\":\"Mini-Batched Online Incremental Learning Through Supervisory Teleoperation with Kinesthetic Coupling\",\"538\":\"Recurrent Neural Network Control of a Hybrid Dynamical Transfemoral Prosthesis with EdgeDRNN Accelerator\",\"539\":\"Cross-context Visual Imitation Learning from Demonstrations\",\"540\":\"Improving Generalisation in Learning Assistance by Demonstration for Smart Wheelchairs\",\"541\":\"Analyzing the Suitability of Cost Functions for Explaining and Imitating Human Driving Behavior based on Inverse Reinforcement Learning\",\"542\":\"Magnetic Sensor Based Topographic Localization for Automatic Dislocation of Ingested Button Battery\",\"543\":\"A Fully Actuated Body-Mounted Robotic Assistant for MRI-Guided Low Back Pain Injection\",\"544\":\"Fault Tolerant Control in Shape-Changing Internal Robots\",\"545\":\"Evaluation of Increasing Camera Baseline on Depth Perception in Surgical Robotics\",\"546\":\"Toward Autonomous Robotic Micro-Suturing using Optical Coherence Tomography Calibration and Path Planning\",\"547\":\"Improved Multiple Objects Tracking based Autonomous Simultaneous Magnetic Actuation & Localization for WCE\",\"548\":\"Probe-before-step walking strategy for multi-legged robots on terrain with risk of collapse\",\"549\":\"Representing Multi-Robot Structure through Multimodal Graph Embedding for the Selection of Robot Teams\",\"550\":\"MAMS-A: Multi-Agent Multi-Scale A\",\"551\":\"Connectivity Maintenance: Global and Optimized approach through Control Barrier Functions\",\"552\":\"Controller Synthesis for Infinitesimally Shape-Similar Formations\",\"553\":\"A Distributed Source Term Estimation Algorithm for Multi-Robot Systems\",\"554\":\"Weighted Buffered Voronoi Cells for Distributed Semi-Cooperative Behavior\",\"555\":\"Learning to Control Reconfigurable Staged Soft Arms\",\"556\":\"Modeling and Analysis of SMA Actuator Embedded in Stretchable Coolant Vascular Pursuing Artificial Muscles\",\"557\":\"Grasping Unknown Objects by Coupling Deep Reinforcement Learning, Generative Adversarial Networks, and Visual Servoing\",\"558\":\"Incorporating Motion Planning Feasibility Considerations during Task-Agent Assignment to Perform Complex Tasks Using Mobile Manipulators\",\"559\":\"Learning to Scaffold the Development of Robotic Manipulation Skills\",\"560\":\"Online Replanning in Belief Space for Partially Observable Task and Motion Problems\",\"561\":\"A Code for Unscented Kalman Filtering on Manifolds (UKF-M)\",\"562\":\"Efficient and precise sensor fusion for non-linear systems with out-of-sequence measurements by example of mobile robotics\",\"563\":\"UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated Input Degradation\",\"564\":\"Intermittent GPS-aided VIO: Online Initialization and Calibration\",\"565\":\"A Mathematical Framework for IMU Error Propagation with Applications to Preintegration\",\"566\":\"Radar-Inertial Ego-Velocity Estimation for Visually Degraded Environments\",\"567\":\"Position-based Impedance Control of a 2-DOF Compliant Manipulator for a Facade Cleaning Operation\",\"568\":\"Robust, Locally Guided Peg-in-Hole using Impedance-Controlled Robots\",\"569\":\"Design of Spatial Admittance for Force-Guided Assembly of Polyhedral Parts in Single Point Frictional Contact\",\"570\":\"Characterisation of Self-locking High-contraction Electro-ribbon Actuators\",\"571\":\"Helically Wrapped Supercoiled Polymer (HW-SCP) Artificial Muscles: Design, Characterization, and Modeling\",\"572\":\"A Variable Stiffness Soft Continuum Robot Based on Pre-charged Air, Particle Jamming, and Origami\",\"573\":\"SwarmRail: A Novel Overhead Robot System for Indoor Transport and Mobile Manipulation\",\"574\":\"Fast Local Planning and Mapping in Unknown Off-Road Terrain\",\"575\":\"Scaled Autonomy: Enabling Human Operators to Control Robot Fleets\",\"576\":\"An Actor-Critic Approach for Legible Robot Motion Planner\",\"577\":\"Intuitive 3D Control of a Quadrotor in User Proximity with Pointing Gestures\",\"578\":\"Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs\",\"579\":\"Audiovisual cognitive architecture for autonomous learning of face localisation by a Humanoid Robot\",\"580\":\"Planetary Rover Exploration Combining Remote and In Situ Measurements for Active Spectroscopic Mapping\",\"581\":\"Magnetic Docking Mechanism for Free-flying Space Robots with Spherical Surfaces\",\"582\":\"Barefoot Rover: a Sensor-Embedded Rover Wheel Demonstrating In-Situ Engineering and Science Extractions using Machine Learning\",\"583\":\"Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering\",\"584\":\"Concurrent Parameter Identification and Control for Free-Floating Robotic Systems During On-Orbit Servicing\",\"585\":\"A Dual Quaternion-Based Discrete Variational Approach for Accurate and Online Inertial Parameter Estimation in Free-Flying obots\",\"586\":\"Unified Intrinsic and Extrinsic Camera and LiDAR Calibration under Uncertainties\",\"587\":\"AC\\/DCC : Accurate Calibration of Dynamic Camera Clusters for Visual SLAM\",\"588\":\"Analytic Plane Covariances Construction for Precise Planarity-based Extrinsic Calibration of Camera and LiDAR\",\"589\":\"An End-Effector Wrist Module for the Kinematically Redundant Manipulation of Arm-Type Robots\",\"590\":\"Online Trajectory Planning for an Industrial Tractor Towing Multiple Full Trailers\",\"591\":\"A Bio-Inspired Transportation Network for Scalable Swarm Foraging\",\"592\":\"Stance Control Inspired by Cerebellum Stabilizes Reflex-Based Locomotion on HyQ Robot\",\"593\":\"Error estimation and correction in a spiking neural network for map formation in neuromorphic hardware\",\"594\":\"Adaptive Visual Shock Absorber with Visual-based Maxwell Model Using a Magnetic Gear\",\"595\":\"Slip-Based Nonlinear Recursive Backstepping Path Following Controller for Autonomous Ground Vehicles\",\"596\":\"Fast and Safe Path-Following Control using a State-Dependent Directional Metric\",\"597\":\"Backlash-Compensated Active Disturbance Rejection Control of Nonlinear Multi-Input Series Elastic Actuators\",\"598\":\"On Generalized Homogenization of Linear Quadrotor Controller\",\"599\":\"In-Hand Object Pose Tracking via Contact Feedback and GPU-Accelerated Robotic Simulation\",\"600\":\"Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive Hands\",\"601\":\"Robust 6D Object Pose Estimation by Learning RGB-D Features\",\"602\":\"Split Deep Q-Learning for Robust Object Singulation\",\"603\":\"6-DOF Grasping for Target-driven Object Manipulation in Clutter\",\"604\":\"Single Shot 6D Object Pose Estimation\",\"605\":\"MulRan: Multimodal Range Dataset for Urban Place Recognition\",\"606\":\"GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization\",\"607\":\"Large-Scale Volumetric Scene Reconstruction using LiDAR\",\"608\":\"Topological Mapping for Manhattan-like Repetitive Environments\",\"609\":\"Robust RGB-D Camera Tracking using Optimal Key-frame Selection\",\"610\":\"Aggressive Online Control of a Quadrotor via Deep Network Representations of Optimality Principles\",\"611\":\"Refined Analysis of Asymptotically-Optimal Kinodynamic Planning in the State-Cost Space\",\"612\":\"Robust quadcopter control with artificial vector fields\",\"613\":\"Simulation-Based Reinforcement Learning for Real-World Autonomous Driving\",\"614\":\"Driving Style Encoder: Situational Reward Adaptation for General-Purpose Planning in Automated Driving\",\"615\":\"Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions\",\"616\":\"The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset\",\"617\":\"Multi-modal Experts Network for Autonomous Driving\",\"618\":\"Localising PMDs through CNN Based Perception of Urban Streets\",\"619\":\"Hybrid Localization using Model- and Learning-Based Methods: Fusion of Monte Carlo and E2E Localizations via Importance Sampling\",\"620\":\"Visual Localization with Google Earth Images for Robust Global Pose Estimation of UAVs\",\"621\":\"Adaptive Curriculum Generation from Demonstrations for Sim-to-Real Visuomotor Control\",\"622\":\"Accept Synthetic Objects as Real: End-to-End Training of Attentive Deep Visuomotor Policies for Manipulation in Clutter\",\"623\":\"Learning of Exception Strategies in Assembly Tasks\",\"624\":\"An Open-Source Framework for Rapid Development of Interactive Soft-Body Simulations for Real-Time Training\",\"625\":\"Towards 5-DoF Control of an Untethered Magnetic Millirobot via MRI Gradient Coils\",\"626\":\"Balance of Humanoid Robots in a Mix of Fixed and Sliding Multi-Contact Scenarios\",\"627\":\"Fast Whole-Body Motion Control of Humanoid Robots with Inertia Constraints\",\"628\":\"SL1M: Sparse L1-norm Minimization for contact planning on uneven terrain\",\"629\":\"Finding Locomanipulation Plans Quickly in the Locomotion Constrained Manifold\",\"630\":\"Force-based Control of Bipedal Balancing on Dynamic Terrain with the \\\"Tallahassee Cassie\\\" Robotic Platform\",\"631\":\"Dense r-robust formations on lattices\",\"632\":\"Optimizing Topologies for Probabilistically Secure Multi-Robot Systems\",\"633\":\"Efficient Communication in Large Multi-robot Networks\",\"634\":\"CyPhyHouse: A programming, simulation, and deployment toolchain for heterogeneous distributed coordination\",\"635\":\"Chance Constrained Simultaneous Path Planning and Task Assignment for Multiple Robots with Stochastic Path Costs\",\"636\":\"Optimal Topology Selection for Stable Coordination of Asymmetrically Interacting Multi-Robot Systems\",\"637\":\"Non-Prehensile Manipulation in Clutter with Human-In-The-Loop\",\"638\":\"PuzzleFlex: kinematic motion of chains with loose joints\",\"639\":\"Accurate Vision-based Manipulation through Contact Reasoning\",\"640\":\"A Probabilistic Framework for Constrained Manipulations and Task and Motion Planning under Uncertainty\",\"641\":\"Planning with Selective Physics-based Simulation for Manipulation Among Movable Objects\",\"642\":\"Hybrid Differential Dynamic Programming for Planar Manipulation Primitives\",\"643\":\"Deep Depth Fusion for Black, Transparent, Reflective and Texture-Less Objects\",\"644\":\"LiDAR-enhanced Structure-from-Motion\",\"645\":\"Low Latency And Low-Level Sensor Fusion For Automotive Use-Cases\",\"646\":\"Robot-Assisted and Wearable Sensor-Mediated Autonomous Gait Analysis\\u00a7\",\"647\":\"A Control Framework Definition to Overcome Position\\/Interaction Dynamics Uncertainties in Force-Controlled Tasks\",\"648\":\"Identification of Compliant Contact Parameters and Admittance Force Modulation on a Non-stationary Compliant Surface\",\"649\":\"Force Adaptation in Contact Tasks with Dynamical Systems\",\"650\":\"Weakly Supervised Silhouette-based Semantic Scene Change Detection\",\"651\":\"3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via Coupled Feature Selection\",\"652\":\"Who2com: Collaborative Perception via Learnable Handshake Communication\",\"653\":\"Comparing View-Based and Map-Based Semantic Labelling in Real-Time SLAM\",\"654\":\"Generative Modeling of Environments with Scene Grammars and Variational Inference\",\"655\":\"SHOP-VRB: A Visual Reasoning Benchmark for Object Perception\",\"656\":\"Sensorization of a Continuum Body Gripper for High Force and Delicate Object Grasping\",\"657\":\"A Soft Gripper with Retractable Nails for Advanced Grasping and Manipulation\",\"658\":\"Real-time Continuous Hand Motion Myoelectric Decoding by Automated Data Labeling\",\"659\":\"Towards Proactive Navigation: A Pedestrian-Vehicle Cooperation Based Behavioral Model\",\"660\":\"Studying Navigation as a Form of Interaction: a Design Approach for Social Robot Navigation Methods\",\"661\":\"Robot Plan Model Generation and Execution with Natural Language Interface\",\"662\":\"Mapless Navigation among Dynamics with Social-safety-awareness: a reinforcement learning approach from 2D laser scans\",\"663\":\"Steering Control of Magnetic Helical Swimmers in Swirling Flows due to Confinement\",\"664\":\"Sim2real gap is non-monotonic with robot complexity for morphology-in-the-loop flapping wing design\",\"665\":\"A Linearized Model for an Ornithopter in Gliding Flight: Experiments and Simulations\",\"666\":\"Towards biomimicry of a bat-style perching maneuver on structures: the manipulation of inertial dynamics\",\"667\":\"Bioinspired object motion filters as the basis of obstacle negotiation in micro aerial systems\",\"668\":\"ARCSnake: An Archimedes\\u2019 Screw-Propelled, Reconfigurable Serpentine Robot for Complex Environments\",\"669\":\"GPR-based Subsurface Object Detection and Reconstruction Using Random Motion and DepthNet\",\"670\":\"Real-time Stereo Visual Servoing for Rose Pruning with Robotic Arm\",\"671\":\"Slip-Limiting Controller for Redundant Line-Suspended Robots: Application to LineRanger\",\"672\":\"Interval Search Genetic Algorithm Based on Trajectory to Solve Inverse Kinematics of Redundant Manipulators and Its Application\",\"673\":\"Analytical Expressions of Serial Manipulator Jacobians and their High-Order Derivatives based on Lie Theory\",\"674\":\"Inverse Kinematics for Serial Kinematic Chains via Sum of Squares Optimization\",\"675\":\"Multi-task closed-loop inverse kinematics stability through semidefinite programming\",\"676\":\"Securing Industrial Operators with Collaborative Robots: Simulation and Experimental Validation for a Carpentry task\",\"677\":\"Learning Shape-based Representation for Visual Localization in Extremely Changing Conditions\",\"678\":\"Trajectory Planning with Safety Guaranty for a Multirotor based on the Forward and Backward Reachability Analysis\",\"679\":\"A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning\",\"680\":\"Enhancing Privacy in Robotics via Judicious Sensor Selection\",\"681\":\"Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics\",\"682\":\"Segregation of Heterogeneous Swarms of Robots in Curves\",\"683\":\"A Fast, Accurate, and Scalable Probabilistic Sample-Based Approach for Counting Swarm Size\",\"684\":\"Bayes Bots: Collective Bayesian Decision-Making in Decentralized Robot Swarms\",\"685\":\"Supervisory Control of Robot Swarms Using Public Events\",\"686\":\"Automatic tool for Gazebo world construction: from a grayscale image to a 3D solid model\",\"687\":\"A ROS Gazebo plugin to simulate ARVA sensors\",\"688\":\"Is That a Chair? Imagining Affordances Using Simulations of an Articulated Human Body\",\"689\":\"Toward Sim-to-Real Directional Semantic Grasping\",\"690\":\"Inferring the Material Properties of Granular Media for Robotic Tasks\",\"691\":\"KETO: Learning Keypoint Representations for Tool Manipulation\",\"692\":\"Learning to See before Learning to Act: Visual Pre-training for Manipulation\",\"693\":\"Contact-based in-hand pose estimation using Bayesian state estimation and particle filtering\",\"694\":\"A Single Multi-Task Deep Neural Network with Post-Processing for Object Detection with Reasoning and Robotic Grasp Detection\",\"695\":\"Practical Persistence Reasoning in Visual SLAM\",\"696\":\"FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow\",\"697\":\"Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control\",\"698\":\"Autonomously Navigating a Surgical Tool Inside the Eye by Learning from Demonstration\",\"699\":\"Learn-to-Recover: Retrofitting UAVs with Reinforcement Learning-Assisted Flight Control Under Cyber-Physical Attacks\",\"700\":\"Towards the Probabilistic Fusion of Learned Priors into Standard Pipelines for 3D Reconstruction\",\"701\":\"Model Reference Adaptive Control of Multirotor for Missions with Dynamic Change of Payloads During Flight\",\"702\":\"The Tiercel: A novel autonomous micro aerial vehicle that can map the environment by flying into obstacles\",\"703\":\"Adaptive Control of Variable-Pitch Propellers: Pursuing Minimum-Effort Operation\",\"704\":\"On Simple Reactive Neural Networks for Behaviour-Based Reinforcement Learning\",\"705\":\"Predicting optimal value functions by interpolating reward functions in scalarized multi-objective reinforcement learning\",\"706\":\"Integrated moment-based LGMD and deep reinforcement learning for UAV obstacle avoidance\",\"707\":\"Interactive Reinforcement Learning with Inaccurate Feedback\",\"708\":\"Guided Uncertainty-Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning\",\"709\":\"Benchmark for Skill Learning from Demonstration: Impact of User Experience, Task Complexity, and Start Configuration on Performance\",\"710\":\"Robot Programming without Coding\",\"711\":\"Predictive Modeling of Periodic Behavior for Human-Robot Symbiotic Walking\",\"712\":\"Agile 3D-Navigation of a Helical Magnetic Swimmer\",\"713\":\"Inferring the Geometric Nullspace of Robot Skills from Human Demonstrations\",\"714\":\"A Dynamical System Approach for Adaptive Grasping, Navigation and Co-Manipulation with Humanoid Robots\",\"715\":\"Subspace Projectors for State-Constrained Multi-Robot Consensus\",\"716\":\"Multi-Agent Task Allocation using Cross-Entropy Temporal Logic Optimization\",\"717\":\"Adaptive Task Allocation for Heterogeneous Multi-Robot Teams with Evolving and Unknown Robot Capabilities\",\"718\":\"Mobile Wireless Network Infrastructure on Demand\",\"719\":\"Monitoring Over the Long Term: Intermittent Deployment and Sensing Strategies for Multi-Robot Teams\",\"720\":\"Multi-Robot Coordination for Estimation and Coverage of Unknown Spatial Fields\",\"721\":\"Learning Robotic Assembly Tasks with Lower Dimensional Systems by Leveraging Physical Softness and Environmental Constraints\",\"722\":\"Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body Robotics using NVIDIA CUDA\",\"723\":\"Motion Planning with Competency-Aware Transition Models for Underactuated Adaptive Hands\",\"724\":\"Human-like Planning for Reaching in Cluttered Environments\",\"725\":\"Where to relocate?: Object rearrangement inside cluttered and confined environments for robotic manipulation\",\"726\":\"Autonomous Modification of Unstructured Environments with Found Material\",\"727\":\"LiStereo: Generate Dense Depth Maps from LIDAR and Stereo Imagery\",\"728\":\"Monocular Visual-Inertial Odometry in Low-Textured Environments with Smooth Gradients: A Fully Dense Direct Filtering Approach\",\"729\":\"Interaction Stability Analysis from the Input-Output Viewpoints\",\"730\":\"Improving the contact instant detection of sensing antennae using a Super-Twisting algorithm\",\"731\":\"6DFC: Efficiently Planning Soft Non-Planar Area Contact Grasps using 6D Friction Cones\",\"732\":\"Long-Horizon Prediction and Uncertainty Propagation with Residual Point Contact Learners\",\"733\":\"Versatile Trajectory Optimization Using a LCP Wheel Model for Dynamic Vehicle Maneuvers\",\"734\":\"Highly Parallelizable Plane Extraction for Organized Point Clouds Using Spherical Convex Hulls\",\"735\":\"View-Invariant Loop Closure with Oriented Semantic Landmarks\",\"736\":\"Active Acoustic Contact Sensing for Soft Pneumatic Actuators\",\"737\":\"A Bidirectional 3D-printed Soft Pneumatic Actuator and Graphite-based Flex Sensor for Versatile Grasping\",\"738\":\"Simultaneous Learning from Human Pose and Object Cues for Real-Time Activity Recognition\",\"739\":\"Demonstration of Hospital Receptionist Robot with Extended Hybrid Code Network to Select Responses and Gestures\",\"740\":\"Can I Trust You? A User Study of Robot Mediation of a Support Group\",\"741\":\"Coronal Plane Spine Twisting Composes Shape To Adjust the Energy Landscape for Grounded Reorientation\",\"742\":\"Motion Design for a Snake Robot Negotiating Complicated Pipe Structures of a Constant Diameter\",\"743\":\"Single Actuator Peristaltic Robot for Subsurface Exploration and Device Emplacement\",\"744\":\"Dynamic modeling of robotic manipulators for accuracy evaluation\",\"745\":\"A Real-Robot Dataset for Assessing Transferability of Learned Dynamics Models\",\"746\":\"MagNet: Discovering Multi-agent Interaction Dynamics using Neural Network\",\"747\":\"Development of a Robotic System for Automated Decaking of 3D-Printed Parts\",\"748\":\"A Novel Solar Tracker Driven by Waves: From Idea to Implementation\",\"749\":\"Design and Implementation of Hydraulic-Cable driven Manipulator for Disaster Response Operation\",\"750\":\"Designs for an Expressive Mechatronic Chordophone\",\"751\":\"OmBURo: A Novel Unicycle Robot with Active Omnidirectional Wheel\",\"752\":\"Recognition and Reconfiguration of Lattice-Based Cellular Structures by Simple Robots\",\"753\":\"A Fast Configuration Space Algorithm for Variable Topology Truss Modular Robots\",\"754\":\"ModQuad-DoF: A Novel Yaw Actuation for Modular Quadrotors\",\"755\":\"An Actuation Fault Tolerance Approach to Reconfiguration Planning of Modular Self-folding Robots\",\"756\":\"Parallel Permutation for Linear Full-resolution Reconfiguration of Heterogeneous Sliding-only Cubic Modular Robots\",\"757\":\"Determining and Improving the Localization Accuracy of AprilTag Detection\",\"758\":\"Change of Optimal Values: A Pre-calculated Metric\",\"759\":\"A Flexible Method for Performance Evaluation of Robot Localization\",\"760\":\"Quantifying Good Seamanship For Autonomous Surface Vessel Performance Evaluation\",\"761\":\"Action-conditioned Benchmarking of Robotic Video Prediction Models: a Comparative Study\",\"762\":\"LyRN (Lyapunov Reaching Network): A Real-Time Closed Loop approach from Monocular Vision\",\"763\":\"Object Finding in Cluttered Scenes Using Interactive Perception\",\"764\":\"CCAN: Constraint Co-Attention Network for Instance Grasping\",\"765\":\"3D Object Detection and Tracking Based on Streaming Data\",\"766\":\"Object-Centric Stereo Matching for 3D Object Detection\",\"767\":\"The Relative Confusion Matrix, a Tool to Assess Classifiablility in Large Scale Picking Applications\",\"768\":\"Pose-guided Auto-Encoder and Feature-Based Refinement for 6-DoF Object Pose Regression\",\"769\":\"PrimiTect: Fast Continuous Hough Voting for Primitive Detection\",\"770\":\"FarSee-Net: Real-Time Semantic Segmentation by Efficient Multi-scale Context Aggregation and Feature Space Super-resolution\",\"771\":\"Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph Convolutional Networks\",\"772\":\"C-3PO: Cyclic-Three-Phase Optimization for Human-Robot Motion Retargeting based on Reinforcement Learning\",\"773\":\"AP-MTL: Attention Pruned Multi-task Learning Model for Real-time Instrument Detection and Segmentation in Robot-assisted Surgery\",\"774\":\"Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search\",\"775\":\"ACNN: a Full Resolution DCNN for Medical Image Segmentation\",\"776\":\"Hyperproperties for Robotics: Planning via HyperLTL\",\"777\":\"Abstractions for computing all robotic sensors that suffice to solve a planning problem\",\"778\":\"T: A Heuristic Search Based Path Planning Algorithm for Temporal Logic Specifications\",\"779\":\"Global\\/local motion planning based on Dynamic Trajectory Reconfiguration and Dynamical Systems for Autonomous Surgical Robots\",\"780\":\"Deep Imitative Reinforcement Learning for Temporal Logic Robot Motion Planning with Noisy Semantic Observations\",\"781\":\"Minimal 3D Dubins Path with Bounded Curvature and Pitch Angle\",\"782\":\"AU-AIR: A Multi-modal Unmanned Aerial Vehicle Dataset for Low Altitude Traffic Surveillance\",\"783\":\"Design and Autonomous Stabilization of a Ballistically-Launched Multirotor\",\"784\":\"Asynchronous event-based clustering and tracking for intrusion monitoring in UAS\",\"785\":\"SHIFT: Selective Heading Image for Translation An onboard monocular optical flow estimator for fast constantly rotating UAVs\",\"786\":\"Flydar: Magnetometer-based High Angular Rate Estimation during Gyro Saturation for SLAM\",\"787\":\"Nonlinear MPC with Motor Failure Identification and Recovery for Safe and Aggressive Multicopter Flight\",\"788\":\"Temporal information integration for video semantic segmentation\",\"789\":\"Map-Predictive Motion Planning in Unknown Environments\",\"790\":\"Using multiple short hops for multicopter navigation with only inertial sensors\",\"791\":\"An Efficient and Continuous Approach to Information-Theoretic Exploration\",\"792\":\"A Feature-Based Underwater Path Planning Approach using Multiple Perspective Prior Maps\",\"793\":\"Automatic LiDAR-Camera Calibration of Extrinsic Parameters Using a Spherical Target\",\"794\":\"Motion Estimation in Occupancy Grid Maps in Stationary Settings Using Recurrent Neural Networks\",\"795\":\"A Divide and Conquer Method for 3D Registration of Inhomogeneous, Partially Overlapping Scans with Fourier Mellin SOFT (FMS)\",\"796\":\"Estimating Motion Uncertainty with Bayesian ICP\",\"797\":\"Actively Mapping Industrial Structures with Information Gain-Based Planning on a Quadruped Robot\",\"798\":\"Efficient Covisibility-based Image Matching for Large-Scale SfM\",\"799\":\"Probabilistic TSDF Fusion Using Bayesian Deep Learning for Dense 3D Reconstruction with a Single RGB Camera\",\"800\":\"IF-Net: An Illumination-invariant Feature Network\",\"801\":\"Deep-Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction\",\"802\":\"Least-squares Optimal Relative Planar Motion for Vehicle-mounted Cameras\",\"803\":\"Relative planar motion for vehicle-mounted cameras from a single affine correspondence\",\"804\":\"Moving object detection for visual odometry in a dynamic environment based on occlusion accumulation\",\"805\":\"A Low-Rank Matrix Approximation Approach to Multiway Matching with Applications in Multi-Sensory Data Association\",\"806\":\"A Parametric Grasping Methodology for Multi-Manual Interactions in Real-Time Dynamic Simulations\",\"807\":\"A methodology for the incorporation of arbitrarily-shaped feet in passive bipedal walking dynamics\",\"808\":\"Experimental Analysis of Structural Vibration Problems of a Biped Walking Robot\",\"809\":\"Dynamic Coupling as an Indicator of Gait Robustness for Underactuated Biped Robots\",\"810\":\"ZMP Constraint Restriction for Robust Gait Generation in Humanoids\",\"811\":\"Hybrid Zero Dynamics Inspired Feedback Control Policy Design for 3D Bipedal Locomotion using Reinforcement Learning\",\"812\":\"Optimal Reduced-order Modeling of Bipedal Locomotion\",\"813\":\"CAPRICORN: Communication Aware Place Recognition using Interpretable Constellations of Objects in Robot Networks\",\"814\":\"Online Planning for Quadrotor Teams in 3-D Workspaces via Reachability Analysis On Invariant Geometric Trees\",\"815\":\"Decentralized Visual-Inertial-UWB Fusion for Relative State Estimation of Aerial Swarm\",\"816\":\"DC-CAPT: Concurrent Assignment and Planning of Trajectories for Dubins Cars\",\"817\":\"Anti-Jackknifing Control of Tractor-Trailer Vehicles via Intrinsically Stable MPC\",\"818\":\"On sensing-aware model predictive path-following control for a reversing general 2-trailer with a car-like tractor\",\"819\":\"Offline Practising and Runtime Training Framework for Autonomous Motion Control of Snake Robots\",\"820\":\"Control of a differentially driven nonholonomic robot subject to a restricted wheels rotation\",\"821\":\"Inferring Task-Space Central Pattern Generator Parameters for Closed-loop Control of Underactuated Robots\",\"822\":\"In-Hand Manipulation of Objects with Unknown Shapes\",\"823\":\"Learning Hierarchical Control for Robust In-Hand Manipulation\",\"824\":\"Tactile Dexterity: Manipulation Primitives with Tactile Feedback\",\"825\":\"Design of a Roller-Based Dexterous Hand for Object Grasping and Within-Hand Manipulation\",\"826\":\"High-Resolution Optical Fiber Shape Sensing of Continuum Robots: A Comparative Study\",\"827\":\"Local Trajectory Stabilization for Dexterous Manipulation via Piecewise Affine Approximations\",\"828\":\"Monocular Direct Sparse Localization in a Prior 3D Surfel Map\",\"829\":\"LINS: A Lidar-Inertial State Estimator for Robust and Efficient Navigation\",\"830\":\"Automated Eye-in-Hand Robot-3D Scanner Calibration for Low Stitching Errors\",\"831\":\"Monocular Visual Odometry using Learned Repeatability and Description\",\"832\":\"Interaction Graphs for Object Importance Estimation in On-road Driving Videos\",\"833\":\"A Robotics Inspection System for Detecting Defects on Semi-specular Painted Automotive Surfaces\",\"834\":\"A Novel Underactuated End-Effector for Planar Sequential Grasping of Multiple Objects\",\"835\":\"Design and Analysis of a Synergy-Inspired Three-Fingered Hand\",\"836\":\"Multiplexed Manipulation: Versatile Multimodal Grasping via a Hybrid Soft Gripper\",\"837\":\"Underactuated Gecko Adhesive Gripper for Simple and Versatile Grasp\",\"838\":\"Active Deformation through Visual Servoing of Soft Objects\",\"839\":\"Visual Geometric Skill Inference by Watching Human Demonstration\",\"840\":\"DFVS: Deep Flow Guided Scene Agnostic Image Based Visual Servoing\",\"841\":\"Photometric Path Planning for Vision-Based Navigation\",\"842\":\"A memory of motion for visual predictive control tasks\",\"843\":\"Design and Workspace Characterisation of Malleable Robots\",\"844\":\"A Tri-Stable Soft Robotic Finger Capable of Pinch and Wrap Grasps\",\"845\":\"A Dexterous Tip-extending Robot with Variable-length Shape-locking\",\"846\":\"Compliant Electromagnetic Actuator Architecture for Soft Robotics\",\"847\":\"Dynamically Reconfigurable Discrete Distributed Stiffness for Inflated Beam Robots\",\"848\":\"Data-Driven Reinforcement Learning for Walking Assistance Control of a Lower Limb Exoskeleton with Hemiplegic Patients\",\"849\":\"On the Effects of Visual Anticipation of Floor Compliance Changes on Human Gait: Towards Model-based Robot-Assisted Rehabilitation\",\"850\":\"A Visual Positioning System for Indoor Blind Navigation\",\"851\":\"An Outsole-Embedded Optoelectronic Sensor to Measure Shear Ground Reaction Forces During Locomotion\",\"852\":\"Bump\\u2019em: an Open-Source, Bump-Emulation System for Studying Human Balance and Gait\",\"853\":\"A Hybrid, Soft Exoskeleton Glove Equipped with a Telescopic Extra Thumb and Abduction Capabilities\",\"854\":\"Controlling an upper-limb exoskeleton by EMG signal while carrying unknown load\",\"855\":\"Learning Grasping Points for Garment Manipulation in Robot-Assisted Dressing\",\"856\":\"TACTO-Selector: Enhanced Hierarchical Fusion of PBVS with Reactive Skin Control for Physical Human-Robot Interaction\",\"857\":\"Towards an Intelligent Collaborative Robotic System for Mixed Case Palletizing\",\"858\":\"Treadmill Based Three Tether Parallel Robot for Evaluating Auditory Warnings While Running\",\"859\":\"Evaluation of Human-Robot Object Co-manipulation Under Robot Impedance Control\",\"860\":\"Whole-Body Bilateral Teleoperation of a Redundant Aerial Manipulator\",\"861\":\"Shared Autonomous Interface for Reducing Physical Effort in Robot Teleoperation via Human Motion Mapping\",\"862\":\"DexPilot: Vision-Based Teleoperation of Dexterous Robotic Hand-Arm System\",\"863\":\"Distributed Winner-Take-All Teleoperation of A Multi-Robot System\",\"864\":\"Enhanced Teleoperation Using Autocomplete\",\"865\":\"Contact-based Bounding Volume Hierarchy for Assembly Tasks\",\"866\":\"Construction of Bounding Volume Hierarchies for Triangle Meshes with Mixed Face Sizes\",\"867\":\"Strategy for automated dense parking: how to navigate in narrow lanes\",\"868\":\"Multimodal Trajectory Predictions for Urban Environments Using Geometric Relationships between a Vehicle and Lanes\",\"869\":\"Online optimal motion generation with guaranteed safety in shared workspace\",\"870\":\"Episodic Koopman Learning of Nonlinear Robot Dynamics with Application to Fast Multirotor Landing\",\"871\":\"Eye-in-Hand 3D Visual Servoing of Helical Swimmers Using Parallel Mobile Coils\",\"872\":\"A Mobile Paramagnetic Nanoparticle Swarm with Automatic Shape Deformation Control\",\"873\":\"Magnetic miniature swimmers with multiple rigid flagella\",\"874\":\"Design and Control of a Large-Range Nil-Stiffness Electro-Magnetic Active Force Sensor\",\"875\":\"Modeling Electromagnetic Navigation Systems for Medical Applications using Random Forests and Artificial Neural Networks\",\"876\":\"Automated Tracking System with Head and Tail Recognition for Time-Lapse Observation of Free-Moving C. elegans\",\"877\":\"Towards Adaptive Benthic Habitat Mapping\",\"878\":\"Multispectral Domain Invariant Image for Retrieval-based Place Recognition\",\"879\":\"Probabilistic Effect Prediction through Semantic Augmentation and Physical Simulation\",\"880\":\"Anytime Integrated Task and Motion Policies for Stochastic Environments\",\"881\":\"CCRobot-III: a Split-type Wire-driven Cable Climbing Robot for Cable-stayed Bridge Inspection\",\"882\":\"Omnidirectional Tractable Three Module Robot\",\"883\":\"A Practical Climbing Robot for Steel Bridge Inspection\",\"884\":\"Development of a Wheeled Wall-Climbing Robot with a Shape-Adaptive Magnetic Adhesion Mechanism\",\"885\":\"Algebraic Fault Detection and Identification for Rigid Robots\",\"886\":\"Fault tolerance analysis of a hexarotor with reconfigurable tilted rotors\",\"887\":\"Detecting Execution Anomalies As an Oracle for Autonomy Software Robustness\",\"888\":\"Reliability Validation of Learning Enabled Vehicle Tracking\",\"889\":\"Real-Time, Highly Accurate Robotic Grasp Detection using Fully Convolutional Neural Network with Rotation Ensemble Module\",\"890\":\"Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly\",\"891\":\"Learning Rope Manipulation Policies Using Dense Object Descriptors Trained on Synthetic Depth Data\",\"892\":\"Efficient two step optimization for large embedded deformation graph based SLAM\",\"893\":\"Camera-to-Robot Pose Estimation from a Single Image\",\"894\":\"PST900: RGB-Thermal Calibration, Dataset and Segmentation Network\",\"895\":\"Instance Segmentation of LiDAR Point Clouds\",\"896\":\"Generation of Object Candidates Through Simply Looking Around\",\"897\":\"Dilated Point Convolutions: On the Receptive Field Size of Point Convolutions on 3D Point Clouds\",\"898\":\"A water-obstacle separation and refinement network for unmanned surface vehicles\",\"899\":\"Dynamic Anchor Selection for Improving Object Localization\",\"900\":\"Under the Radar: Learning to Predict Robust Keypoints for Odometry Estimation and Metric Localisation in Radar\",\"901\":\"SpAGNN: Spatially-Aware Graph Neural Networks for Relational Behavior Forecasting from Sensor Data\",\"902\":\"Any Motion Detector: Learning Class-agnostic Scene Dynamics from a Sequence of LiDAR Point Clouds\",\"903\":\"Where and When: Event-Based Spatiotemporal Trajectory Prediction from the iCub\\u2019s Point-Of-View\",\"904\":\"A Data-driven Planning Framework for Robotic Texture Painting on 3D Surfaces\",\"905\":\"Learned Critical Probabilistic Roadmaps for Robotic Motion Planning\",\"906\":\"Learning Heuristic A: Efficient Graph Search using Neural Network\",\"907\":\"3D-CNN Based Heuristic Guided Task-Space Planner for Faster Motion Planning\",\"908\":\"Learned Sampling Distributions for Efficient Planning in Hybrid Geometric and Object-Level Representations\",\"909\":\"Deep Visual Heuristics: Learning Feasibility of Mixed-Integer Programs for Manipulation Planning\",\"910\":\"Fast Frontier-based Information-driven Autonomous Exploration with an MAV\",\"911\":\"Dynamic Landing of an Autonomous Quadrotor on a Moving Platform in Turbulent Wind Conditions\",\"912\":\"Direct NMPC for Post-Stall Motion Planning with Fixed-Wing UAVs\",\"913\":\"A Flight Envelope Determination and Protection System for Fixed-Wing UAVs\",\"914\":\"Multi-Head Attention for Multi-Modal Joint Vehicle Motion Forecasting\",\"915\":\"A Volumetric Albedo Framework for 3D Imaging Sonar Reconstruction\",\"916\":\"Map Management Approach for SLAM in Large-Scale Indoor and Outdoor Areas\",\"917\":\"A Hierarchical Framework for Collaborative Probabilistic Semantic Mapping\",\"918\":\"Autonomous Navigation in Unknown Environments using Sparse Kernel-based Occupancy Mapping\",\"919\":\"Hybrid Topological and 3D Dense Mapping through Autonomous Exploration for Large Indoor Environments\",\"920\":\"Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique Constraints\",\"921\":\"Anticipating the Start of User Interaction for Service Robot in the Wild\",\"922\":\"Spin Detection in Robotic Table Tennis\",\"923\":\"Look, Listen, and Act: Towards Audio-Visual Embodied Navigation\",\"924\":\"Autonomous Tool Construction with Gated Graph Neural Network\",\"925\":\"Training-Set Distillation for Real-Time UAV Object Tracking\",\"926\":\"CNN-Based Simultaneous Dehazing and Depth Estimation\",\"927\":\"Internet of Things (IoT)-based Collaborative Control of a Redundant Manipulator for Teleoperated Minimally Invasive Surgeries\",\"928\":\"Passive Dynamic Balancing and Walking in Actuated Environments\",\"929\":\"Biped Stabilization by Linear Feedback of the Variable-Height Inverted Pendulum Model\",\"930\":\"Stability Criteria of Balanced and Steppable Unbalanced States for Full-Body Systems with Implications in Robotic and Human Gait\",\"931\":\"Material Handling by Humanoid Robot While Pushing Carts Using a Walking Pattern Based on Capture Point\",\"932\":\"Interconnection and Damping Assignment Passivity-Based Control for Gait Generation in Underactuated Compass-Like Robots\",\"933\":\"Multi-Robot Path Deconfliction through Prioritization by Path Prospects\",\"934\":\"A Connectivity-Prediction Algorithm and its Application in Active Cooperative Localization for Multi-Robot Systems\",\"935\":\"Behavior Mixing with Minimum Global and Subgroup Connectivity Maintenance for Large-Scale Multi-Robot Systems\",\"936\":\"Energy-Optimal Cooperative Manipulation via Provable Internal-Force Regulation\",\"937\":\"Robot Telekinesis: Application of a Unimanual and Bimanual Object Manipulation Technique to Robot Control\",\"938\":\"A Set-Theoretic Approach to Multi-Task Execution and Prioritization\",\"939\":\"Variable Impedance Control in Cartesian Latent Space while Avoiding Obstacles in Null Space\",\"940\":\"MagicHand: Context-Aware Dexterous Grasping Using an Anthropomorphic Robotic Hand\",\"941\":\"Learning Pregrasp Manipulation of Objects from Ungraspable Poses\",\"942\":\"Picking Thin Objects by Tilt-and-Pivot Manipulation and Its Application to Bin Picking\",\"943\":\"Attention-Guided Lightweight Network for Real-Time Segmentation of Robotic Surgical Instruments\",\"944\":\"Automated robotic breast ultrasound acquisition using ultrasound feedback\",\"945\":\"Robust and Accurate 3D Curve to Surface Registration with Tangent and Normal Vectors\",\"946\":\"Single-Shot Pose Estimation of Surgical Robot Instruments\\u2019 Shafts from Monocular Endoscopic Images\",\"947\":\"End-to-End Real-time Catheter Segmentation with Optical Flow-Guided Warping during Endovascular Intervention\",\"948\":\"Pathological Airway Segmentation with Cascaded Neural Networks for Bronchoscopic Navigation\",\"949\":\"Design of 3D-printed assembly mechanisms based on special wooden joinery techniques and its application to a robotic hand\",\"950\":\"Parallel gripper with displacement-magnification mechanism and extendable finger mechanism\",\"951\":\"A Shape Memory Polymer Adhesive Gripper For Pick-and-Place Applications\",\"952\":\"Multi-person Pose Tracking using Sequential Monte Carlo with Probabilistic Neural Pose Predictor\",\"953\":\"4D Generic Video Object Proposals\",\"954\":\"Simultaneous Tracking and Elasticity Parameter Estimation of Deformable Objects\",\"955\":\"AVOT: Audio-Visual Object Tracking of Multiple Objects for Robotics\",\"956\":\"Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering\",\"957\":\"6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints\",\"958\":\"Designing Ferromagnetic Soft Robots (FerroSoRo) with Level-Set-Based Multiphysics Topology Optimization\",\"959\":\"Exoskeleton-covered soft finger with vision-based proprioception and tactile sensing\",\"960\":\"Tuning the Energy Landscape of Soft Robots for Fast and Strong Motion\",\"961\":\"REBOund: Untethered Origami Jumping Robot with Controllable Jump Height\",\"962\":\"Motion Intensity Extraction Scheme for Simultaneous Recognition of Wrist\\/Hand Motions\",\"963\":\"Simultaneous Online Motion Discrimination and Evaluation of Whole-body Exercise by Synergy Probes for Home Rehabilitation\",\"964\":\"Validation of a Forward Kinematics Based Controller for a mobile Tethered Pelvic Assist Device to Augment Pelvic Forces during Walking\",\"965\":\"Model Learning for Control of a Paralyzed Human Arm with Functional Electrical Stimulation\",\"966\":\"Transient Behavior and Predictability in Manipulating Complex Objects\",\"967\":\"A Variable-Fractional Order Admittance Controller for pHRI\",\"968\":\"Assistive Gym: A Physics Simulation Framework for Assistive Robotics\",\"969\":\"Learning Whole-Body Human-Robot Haptic Interaction in Social Contexts\",\"970\":\"Human Preferences in Using Damping to Manage Singularities During Physical Human-Robot Collaboration\",\"971\":\"MOCA-MAN: A MObile and reconfigurable Collaborative Robot Assistant for conjoined huMAN-robot actions\",\"972\":\"Closing the Force Loop to Enhance Transparency in Time-delayed Teleoperation\",\"973\":\"Evaluation of an Exoskeleton-based Bimanual Teleoperation Architecture with Independently Passivated Slave Devices\",\"974\":\"Hand-worn Haptic Interface for Drone Teleoperation\",\"975\":\"Toward Human-like Teleoperated Robot Motion: Performance and Perception of a Choreography-inspired Method in Static and Dynamic Tasks for Rapid Pose Selection of Articulated Robots\",\"976\":\"Helping Robots Learn: A Human-Robot Master-Apprentice Model Using Demonstrations via Virtual Reality Teleoperation\",\"977\":\"A Framework for Interactive Virtual Fixture Generation for Shared Teleoperation in Unstructured Environments\",\"978\":\"Local Obstacle-Skirting Path Planning for a Fast Bi-steerable Rover using B\\u00e9zier Curves\",\"979\":\"Collision Avoidance with Proximity Servoing for Redundant Serial Robot Manipulators\",\"980\":\"Predicting Obstacle Footprints from 2D Occupancy Maps by Learning from Physical Interactions\",\"981\":\"Path Planning in Dynamic Environments using Generative RNNs and Monte Carlo Tree Search\",\"982\":\"Safety-Critical Rapid Aerial Exploration of Unknown Environments\",\"983\":\"Reconfigurable Magnetic Microswarm for Thrombolysis under Ultrasound Imaging\",\"984\":\"Improving Optical Micromanipulation with Force-Feedback Bilateral Coupling\",\"985\":\"Maneuver at Micro Scale: Steering by Actuation Frequency Control in Micro Bristle Robots\",\"986\":\"Scaling down an insect-size microrobot, HAMR-VI into HAMR-Jr\",\"987\":\"Reality as a simulation of reality: robot illusions, fundamental limits, and a physical demonstration\",\"988\":\"Finding Missing Skills for High-Level Behaviors\",\"989\":\"Near-Optimal Reactive Synthesis Incorporating Runtime Information\",\"990\":\"Control Synthesis from Linear Temporal Logic Specifications using Model-Free Reinforcement Learning\",\"991\":\"R-Min: a Fast Collaborative Underactuated Parallel Robot for Pick-and-Place Operations\",\"992\":\"High-Flexibility Locomotion and Whole-Torso Control for a Wheel-Legged Robot on Challenging Terrain\",\"993\":\"The Prince\\u2019s tears, a large cable-driven parallel robot for an artistic exhibition\",\"994\":\"Singularity analysis and reconfiguration mode of the 3-CRS parallel manipulator\",\"995\":\"Trajectory optimization for a class of robots belonging to Constrained Collaborative Mobile Agents (CCMA) family\",\"996\":\"Development of Body Rotational Wheeled Robot and its Verification of Effectiveness\",\"997\":\"Radar Sensors in Collaborative Robotics: Fast Simulation and Experimental Validation\",\"998\":\"Transferable Task Execution from Pixels through Deep Planning Domain Learning\",\"999\":\"Depth by Poking: Learning to Estimate Depth from Self-Supervised Grasping\",\"1000\":\"Online Learning of Object Representations by Appearance Space Feature Alignment\",\"1001\":\"Visual Prediction of Priors for Articulated Object Interaction\",\"1002\":\"MT-DSSD: Deconvolutional Single Shot Detector Using Multi Task Learning for Object Detection, Segmentation, and Grasping Detection\",\"1003\":\"Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for Object Grasping\",\"1004\":\"Stillleben: Realistic Scene Synthesis for Deep Learning in Robotics\",\"1005\":\"A Generative Approach Towards Improved Robotic Detection of Marine Litter\",\"1006\":\"Spatiotemporal Representation Learning with GAN Trained LSTM-LSTM Networks\",\"1007\":\"Belief Regulated Dual Propagation Nets for Learning Action Effects on Groups of Articulated Objects\",\"1008\":\"Deep Kinematic Models for Kinematically Feasible Vehicle Trajectory Predictions\",\"1009\":\"Human Driver Behavior Prediction based on UrbanFlow\",\"1010\":\"Environment Prediction from Sparse Samples for Robotic Information Gathering\",\"1011\":\"Predicting Pushing Action Effects on Spatial Object Relations by Learning Internal Prediction Models\",\"1012\":\"Learning of Key Pose Evaluation for Efficient Multi-contact Motion Planner\",\"1013\":\"Differentiable Gaussian Process Motion Planning\",\"1014\":\"Learn and Link: Learning Critical Regions for Efficient Planning\",\"1015\":\"Pose-Estimate-Based Target Tracking for Human-Guided Remote Sensor Mounting with a UAV\",\"1016\":\"EVDodgeNet: Deep Dynamic Obstacle Dodging with Event Cameras\",\"1017\":\"On training datasets for machine learning-based visual relative localization of micro-scale UAVs\",\"1018\":\"Dynamic Actor-Advisor Programming for Scalable Safe Reinforcement Learning\",\"1019\":\"Discrete Deep Reinforcement Learning for Mapless Navigation\",\"1020\":\"Learning Multi-Robot Decentralized Macro-Action-Based Policies via a Centralized Q-Net\",\"1021\":\"Robust Model-free Reinforcement Learning with Multi-objective Bayesian Optimization\",\"1022\":\"A Unified Framework for Piecewise Semantic Reconstruction in Dynamic Scenes via Exploiting Superpixel Relations\",\"1023\":\"Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps\",\"1024\":\"Informative Path Planning for Active Field Mapping under Localization Uncertainty\",\"1025\":\"Ensemble of Sparse Gaussian Process Experts for Implicit Surface Mapping with Streaming Data\",\"1026\":\"Robust Method for Removing Dynamic Objects from Point Clouds\",\"1027\":\"Real-Time Semantic Stereo Matching\",\"1028\":\"Multi-Task Learning for Single Image Depth Estimation and Segmentation Based on Unsupervised Network\",\"1029\":\"Leveraging the Template and Anchor Framework for Safe, Online Robotic Gait Design\",\"1030\":\"Unified Push Recovery Fundamentals: Inspiration from Human Study\",\"1031\":\"DISCO: Double Likelihood-free Inference Stochastic Control\",\"1032\":\"Sufficiently Accurate Model Learning\",\"1033\":\"Towards Plan Transformations for Real-World Mobile Fetch and Place\",\"1034\":\"Planning an Efficient and Robust Base Sequence for a Mobile Manipulator Performing Multiple Pick-and-place Tasks\",\"1035\":\"Towards Mobile Multi-Task Manipulation in a Confined and Integrated Environment with Irregular Objects\",\"1036\":\"Linear Time-Varying MPC for Nonprehensile Object Manipulation with a Nonholonomic Mobile Robot\",\"1037\":\"A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in Homes\",\"1038\":\"2D to 3D Line-Based Registration with Unknown Associations via Mixed-Integer Programming\",\"1039\":\"An efficient solution to the relative pose estimation with a common direction\",\"1040\":\"Task-Aware Novelty Detection for Visual-based Deep Learning in Autonomous Systems\",\"1041\":\"DirectShape: Direct Photometric Alignment of Shape Priors for Visual Vehicle Pose and Shape Estimation\",\"1042\":\"RoadText-1K: Text Detection & Recognition Dataset for Driving Videos\",\"1043\":\"End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera\",\"1044\":\"Learning an Action-Conditional Model for Haptic Texture Generation\",\"1045\":\"Just Noticeable Differences for Joint Torque Feedback During Static Poses\",\"1046\":\"Design of a Parallel Haptic Device with Gravity Compensation by using its System Weight\",\"1047\":\"Multimodal tracking framework for visual odometry in challenging illumination conditions\",\"1048\":\"Realtime Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration\",\"1049\":\"Autonomous Tissue Scanning under Free-Form Motion for Intraoperative Tissue Characterisation\",\"1050\":\"3D-Printed Electroactive Hydraulic Valves for Use in Soft Robotic Applications\",\"1051\":\"Variable Damping Control of a Robotic Arm to Improve Trade-off between Agility and Stability and Reduce User Effort\",\"1052\":\"Cognitive and motor compliance in intentional human-robot interaction\",\"1053\":\"Adaptive Authority Allocation in Shared Control of Robots Using Bayesian Filters\",\"1054\":\"Tactile Telerobots for Dull, Dirty, Dangerous, and Inaccessible Tasks\",\"1055\":\"A Novel Orientability Index and the Kinematic Design of the RemoT-ARM: A Haptic Master with Large and Dexterous Workspace\",\"1056\":\"RAVEN-S: Design and Simulation of a Robot for Teleoperated Microgravity Rodent Dissection Under Time Delay\",\"1057\":\"Collision-free Navigation of Human-centered Robots via Markov Games\",\"1058\":\"DenseCAvoid: Real-time Navigation in Dense Crowds using Anticipatory Behaviors\",\"1059\":\"DeepCrashTest: Turning Dashcam Videos into Virtual Crash Tests for Automated Driving Systems\",\"1060\":\"Robotic Control of a Magnetic Swarm for On-Demand Intracellular Measurement\",\"1061\":\"Acoustofluidic Tweezers for the 3D Manipulation of Microparticles\",\"1062\":\"An online scheduling algorithm for human-robot collaborative kitting\",\"1063\":\"A Model-Free Approach to Meta-Level Control of Anytime Algorithms\",\"1064\":\"Simultaneous task allocation and motion scheduling for complex tasks executed by multiple robots\",\"1065\":\"Efficient Planning for High-Speed MAV Flight in Unknown Environments Using Online Sparse Topological Graphs\",\"1066\":\"Evaluating Adaptation Performance of Hierarchical Deep Reinforcement Learning\",\"1067\":\"Iterator-Based Temporal Logic Task Planning\",\"1068\":\"Reactive Temporal Logic Planning for Multiple Robots in Unknown Environments\",\"1069\":\"Higher Order Function Networks for View Planning and Multi-View Reconstruction\",\"1070\":\"Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments\",\"1071\":\"Online Grasp Plan Refinement for Reducing Defects During Robotic Layup of Composite Prepreg Sheets\",\"1072\":\"Learning Continuous 3D Reconstructions for Geometrically Aware Grasping\"},\"First and Last Author Affiliations\":{\"0\":null,\"1\":null,\"2\":null,\"3\":null,\"4\":null,\"5\":null,\"6\":null,\"7\":null,\"8\":null,\"9\":null,\"10\":null,\"11\":null,\"12\":null,\"13\":null,\"14\":null,\"15\":null,\"16\":null,\"17\":null,\"18\":null,\"19\":null,\"20\":null,\"21\":null,\"22\":null,\"23\":null,\"24\":null,\"25\":null,\"26\":null,\"27\":null,\"28\":null,\"29\":null,\"30\":null,\"31\":null,\"32\":null,\"33\":null,\"34\":null,\"35\":null,\"36\":null,\"37\":null,\"38\":null,\"39\":null,\"40\":null,\"41\":null,\"42\":null,\"43\":null,\"44\":null,\"45\":null,\"46\":null,\"47\":null,\"48\":null,\"49\":null,\"50\":null,\"51\":null,\"52\":null,\"53\":null,\"54\":null,\"55\":null,\"56\":null,\"57\":null,\"58\":null,\"59\":null,\"60\":null,\"61\":null,\"62\":null,\"63\":null,\"64\":null,\"65\":null,\"66\":null,\"67\":null,\"68\":null,\"69\":null,\"70\":null,\"71\":null,\"72\":null,\"73\":null,\"74\":null,\"75\":null,\"76\":null,\"77\":null,\"78\":null,\"79\":null,\"80\":null,\"81\":null,\"82\":null,\"83\":null,\"84\":null,\"85\":null,\"86\":null,\"87\":null,\"88\":null,\"89\":null,\"90\":null,\"91\":null,\"92\":null,\"93\":null,\"94\":null,\"95\":null,\"96\":null,\"97\":null,\"98\":null,\"99\":null,\"100\":null,\"101\":null,\"102\":null,\"103\":null,\"104\":null,\"105\":null,\"106\":null,\"107\":null,\"108\":null,\"109\":null,\"110\":null,\"111\":null,\"112\":null,\"113\":null,\"114\":null,\"115\":null,\"116\":null,\"117\":null,\"118\":null,\"119\":null,\"120\":null,\"121\":null,\"122\":null,\"123\":null,\"124\":null,\"125\":null,\"126\":null,\"127\":null,\"128\":null,\"129\":null,\"130\":null,\"131\":null,\"132\":null,\"133\":null,\"134\":null,\"135\":null,\"136\":null,\"137\":null,\"138\":null,\"139\":null,\"140\":null,\"141\":null,\"142\":null,\"143\":null,\"144\":null,\"145\":null,\"146\":null,\"147\":null,\"148\":null,\"149\":null,\"150\":null,\"151\":null,\"152\":null,\"153\":null,\"154\":null,\"155\":null,\"156\":null,\"157\":null,\"158\":null,\"159\":null,\"160\":null,\"161\":null,\"162\":null,\"163\":null,\"164\":null,\"165\":null,\"166\":null,\"167\":null,\"168\":null,\"169\":null,\"170\":null,\"171\":null,\"172\":null,\"173\":null,\"174\":null,\"175\":null,\"176\":null,\"177\":null,\"178\":null,\"179\":null,\"180\":null,\"181\":null,\"182\":null,\"183\":null,\"184\":null,\"185\":null,\"186\":null,\"187\":null,\"188\":null,\"189\":null,\"190\":null,\"191\":null,\"192\":null,\"193\":null,\"194\":null,\"195\":null,\"196\":null,\"197\":null,\"198\":null,\"199\":null,\"200\":null,\"201\":null,\"202\":null,\"203\":null,\"204\":null,\"205\":null,\"206\":null,\"207\":null,\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":null,\"213\":null,\"214\":null,\"215\":null,\"216\":null,\"217\":null,\"218\":null,\"219\":null,\"220\":null,\"221\":null,\"222\":null,\"223\":null,\"224\":null,\"225\":null,\"226\":null,\"227\":null,\"228\":null,\"229\":null,\"230\":null,\"231\":null,\"232\":null,\"233\":null,\"234\":null,\"235\":null,\"236\":null,\"237\":null,\"238\":null,\"239\":null,\"240\":null,\"241\":null,\"242\":null,\"243\":null,\"244\":null,\"245\":null,\"246\":null,\"247\":null,\"248\":null,\"249\":null,\"250\":null,\"251\":null,\"252\":null,\"253\":null,\"254\":null,\"255\":null,\"256\":null,\"257\":null,\"258\":null,\"259\":null,\"260\":null,\"261\":null,\"262\":null,\"263\":null,\"264\":null,\"265\":null,\"266\":null,\"267\":null,\"268\":null,\"269\":null,\"270\":null,\"271\":null,\"272\":null,\"273\":null,\"274\":null,\"275\":null,\"276\":null,\"277\":null,\"278\":null,\"279\":null,\"280\":null,\"281\":null,\"282\":null,\"283\":null,\"284\":null,\"285\":null,\"286\":null,\"287\":null,\"288\":null,\"289\":null,\"290\":null,\"291\":null,\"292\":null,\"293\":null,\"294\":null,\"295\":null,\"296\":null,\"297\":null,\"298\":null,\"299\":null,\"300\":null,\"301\":null,\"302\":null,\"303\":null,\"304\":null,\"305\":null,\"306\":null,\"307\":null,\"308\":null,\"309\":null,\"310\":null,\"311\":null,\"312\":null,\"313\":null,\"314\":null,\"315\":null,\"316\":null,\"317\":null,\"318\":null,\"319\":null,\"320\":null,\"321\":null,\"322\":null,\"323\":null,\"324\":null,\"325\":null,\"326\":null,\"327\":null,\"328\":null,\"329\":null,\"330\":null,\"331\":null,\"332\":null,\"333\":null,\"334\":null,\"335\":null,\"336\":null,\"337\":null,\"338\":null,\"339\":null,\"340\":null,\"341\":null,\"342\":null,\"343\":null,\"344\":null,\"345\":null,\"346\":null,\"347\":null,\"348\":null,\"349\":null,\"350\":null,\"351\":null,\"352\":null,\"353\":null,\"354\":null,\"355\":null,\"356\":null,\"357\":null,\"358\":null,\"359\":null,\"360\":null,\"361\":null,\"362\":null,\"363\":null,\"364\":null,\"365\":null,\"366\":null,\"367\":null,\"368\":null,\"369\":null,\"370\":null,\"371\":null,\"372\":null,\"373\":null,\"374\":null,\"375\":null,\"376\":null,\"377\":null,\"378\":null,\"379\":null,\"380\":null,\"381\":null,\"382\":null,\"383\":null,\"384\":null,\"385\":null,\"386\":null,\"387\":null,\"388\":null,\"389\":null,\"390\":null,\"391\":null,\"392\":null,\"393\":null,\"394\":null,\"395\":null,\"396\":null,\"397\":null,\"398\":null,\"399\":null,\"400\":null,\"401\":null,\"402\":null,\"403\":null,\"404\":null,\"405\":null,\"406\":null,\"407\":null,\"408\":null,\"409\":null,\"410\":null,\"411\":null,\"412\":null,\"413\":null,\"414\":null,\"415\":null,\"416\":null,\"417\":null,\"418\":null,\"419\":null,\"420\":null,\"421\":null,\"422\":null,\"423\":null,\"424\":null,\"425\":null,\"426\":null,\"427\":null,\"428\":null,\"429\":null,\"430\":null,\"431\":null,\"432\":null,\"433\":null,\"434\":null,\"435\":null,\"436\":null,\"437\":null,\"438\":null,\"439\":null,\"440\":null,\"441\":null,\"442\":null,\"443\":null,\"444\":null,\"445\":null,\"446\":null,\"447\":null,\"448\":null,\"449\":null,\"450\":null,\"451\":null,\"452\":null,\"453\":null,\"454\":null,\"455\":null,\"456\":null,\"457\":null,\"458\":null,\"459\":null,\"460\":null,\"461\":null,\"462\":null,\"463\":null,\"464\":null,\"465\":null,\"466\":null,\"467\":null,\"468\":null,\"469\":null,\"470\":null,\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":null,\"476\":null,\"477\":null,\"478\":null,\"479\":null,\"480\":null,\"481\":null,\"482\":null,\"483\":null,\"484\":null,\"485\":null,\"486\":null,\"487\":null,\"488\":null,\"489\":null,\"490\":null,\"491\":null,\"492\":null,\"493\":null,\"494\":null,\"495\":null,\"496\":null,\"497\":null,\"498\":null,\"499\":null,\"500\":null,\"501\":null,\"502\":null,\"503\":null,\"504\":null,\"505\":null,\"506\":null,\"507\":null,\"508\":null,\"509\":null,\"510\":null,\"511\":null,\"512\":null,\"513\":null,\"514\":null,\"515\":null,\"516\":null,\"517\":null,\"518\":null,\"519\":null,\"520\":null,\"521\":null,\"522\":null,\"523\":null,\"524\":null,\"525\":null,\"526\":null,\"527\":null,\"528\":null,\"529\":null,\"530\":null,\"531\":null,\"532\":null,\"533\":null,\"534\":null,\"535\":null,\"536\":null,\"537\":null,\"538\":null,\"539\":null,\"540\":null,\"541\":null,\"542\":null,\"543\":null,\"544\":null,\"545\":null,\"546\":null,\"547\":null,\"548\":null,\"549\":null,\"550\":null,\"551\":null,\"552\":null,\"553\":null,\"554\":null,\"555\":null,\"556\":null,\"557\":null,\"558\":null,\"559\":null,\"560\":null,\"561\":null,\"562\":null,\"563\":null,\"564\":null,\"565\":null,\"566\":null,\"567\":null,\"568\":null,\"569\":null,\"570\":null,\"571\":null,\"572\":null,\"573\":null,\"574\":null,\"575\":null,\"576\":null,\"577\":null,\"578\":null,\"579\":null,\"580\":null,\"581\":null,\"582\":null,\"583\":null,\"584\":null,\"585\":null,\"586\":null,\"587\":null,\"588\":null,\"589\":null,\"590\":null,\"591\":null,\"592\":null,\"593\":null,\"594\":null,\"595\":null,\"596\":null,\"597\":null,\"598\":null,\"599\":null,\"600\":null,\"601\":null,\"602\":null,\"603\":null,\"604\":null,\"605\":null,\"606\":null,\"607\":null,\"608\":null,\"609\":null,\"610\":null,\"611\":null,\"612\":null,\"613\":null,\"614\":null,\"615\":null,\"616\":null,\"617\":null,\"618\":null,\"619\":null,\"620\":null,\"621\":null,\"622\":null,\"623\":null,\"624\":null,\"625\":null,\"626\":null,\"627\":null,\"628\":null,\"629\":null,\"630\":null,\"631\":null,\"632\":null,\"633\":null,\"634\":null,\"635\":null,\"636\":null,\"637\":null,\"638\":null,\"639\":null,\"640\":null,\"641\":null,\"642\":null,\"643\":null,\"644\":null,\"645\":null,\"646\":null,\"647\":null,\"648\":null,\"649\":null,\"650\":null,\"651\":null,\"652\":null,\"653\":null,\"654\":null,\"655\":null,\"656\":null,\"657\":null,\"658\":null,\"659\":null,\"660\":null,\"661\":null,\"662\":null,\"663\":null,\"664\":null,\"665\":null,\"666\":null,\"667\":null,\"668\":null,\"669\":null,\"670\":null,\"671\":null,\"672\":null,\"673\":null,\"674\":null,\"675\":null,\"676\":null,\"677\":null,\"678\":null,\"679\":null,\"680\":null,\"681\":null,\"682\":null,\"683\":null,\"684\":null,\"685\":null,\"686\":null,\"687\":null,\"688\":null,\"689\":null,\"690\":null,\"691\":null,\"692\":null,\"693\":null,\"694\":null,\"695\":null,\"696\":null,\"697\":null,\"698\":null,\"699\":null,\"700\":null,\"701\":null,\"702\":null,\"703\":null,\"704\":null,\"705\":null,\"706\":null,\"707\":null,\"708\":null,\"709\":null,\"710\":null,\"711\":null,\"712\":null,\"713\":null,\"714\":null,\"715\":null,\"716\":null,\"717\":null,\"718\":null,\"719\":null,\"720\":null,\"721\":null,\"722\":null,\"723\":null,\"724\":null,\"725\":null,\"726\":null,\"727\":null,\"728\":null,\"729\":null,\"730\":null,\"731\":null,\"732\":null,\"733\":null,\"734\":null,\"735\":null,\"736\":null,\"737\":null,\"738\":null,\"739\":null,\"740\":null,\"741\":null,\"742\":null,\"743\":null,\"744\":null,\"745\":null,\"746\":null,\"747\":null,\"748\":null,\"749\":null,\"750\":null,\"751\":null,\"752\":null,\"753\":null,\"754\":null,\"755\":null,\"756\":null,\"757\":null,\"758\":null,\"759\":null,\"760\":null,\"761\":null,\"762\":null,\"763\":null,\"764\":null,\"765\":null,\"766\":null,\"767\":null,\"768\":null,\"769\":null,\"770\":null,\"771\":null,\"772\":null,\"773\":null,\"774\":null,\"775\":null,\"776\":null,\"777\":null,\"778\":null,\"779\":null,\"780\":null,\"781\":null,\"782\":null,\"783\":null,\"784\":null,\"785\":null,\"786\":null,\"787\":null,\"788\":null,\"789\":null,\"790\":null,\"791\":null,\"792\":null,\"793\":null,\"794\":null,\"795\":null,\"796\":null,\"797\":null,\"798\":null,\"799\":null,\"800\":null,\"801\":null,\"802\":null,\"803\":null,\"804\":null,\"805\":null,\"806\":null,\"807\":null,\"808\":null,\"809\":null,\"810\":null,\"811\":null,\"812\":null,\"813\":null,\"814\":null,\"815\":null,\"816\":null,\"817\":null,\"818\":null,\"819\":null,\"820\":null,\"821\":null,\"822\":null,\"823\":null,\"824\":null,\"825\":null,\"826\":null,\"827\":null,\"828\":null,\"829\":null,\"830\":null,\"831\":null,\"832\":null,\"833\":null,\"834\":null,\"835\":null,\"836\":null,\"837\":null,\"838\":null,\"839\":null,\"840\":null,\"841\":null,\"842\":null,\"843\":null,\"844\":null,\"845\":null,\"846\":null,\"847\":null,\"848\":null,\"849\":null,\"850\":null,\"851\":null,\"852\":null,\"853\":null,\"854\":null,\"855\":null,\"856\":null,\"857\":null,\"858\":null,\"859\":null,\"860\":null,\"861\":null,\"862\":null,\"863\":null,\"864\":null,\"865\":null,\"866\":null,\"867\":null,\"868\":null,\"869\":null,\"870\":null,\"871\":null,\"872\":null,\"873\":null,\"874\":null,\"875\":null,\"876\":null,\"877\":null,\"878\":null,\"879\":null,\"880\":null,\"881\":null,\"882\":null,\"883\":null,\"884\":null,\"885\":null,\"886\":null,\"887\":null,\"888\":null,\"889\":null,\"890\":null,\"891\":null,\"892\":null,\"893\":null,\"894\":null,\"895\":null,\"896\":null,\"897\":null,\"898\":null,\"899\":null,\"900\":null,\"901\":null,\"902\":null,\"903\":null,\"904\":null,\"905\":null,\"906\":null,\"907\":null,\"908\":null,\"909\":null,\"910\":null,\"911\":null,\"912\":null,\"913\":null,\"914\":null,\"915\":null,\"916\":null,\"917\":null,\"918\":null,\"919\":null,\"920\":null,\"921\":null,\"922\":null,\"923\":null,\"924\":null,\"925\":null,\"926\":null,\"927\":null,\"928\":null,\"929\":null,\"930\":null,\"931\":null,\"932\":null,\"933\":null,\"934\":null,\"935\":null,\"936\":null,\"937\":null,\"938\":null,\"939\":null,\"940\":null,\"941\":null,\"942\":null,\"943\":null,\"944\":null,\"945\":null,\"946\":null,\"947\":null,\"948\":null,\"949\":null,\"950\":null,\"951\":null,\"952\":null,\"953\":null,\"954\":null,\"955\":null,\"956\":null,\"957\":null,\"958\":null,\"959\":null,\"960\":null,\"961\":null,\"962\":null,\"963\":null,\"964\":null,\"965\":null,\"966\":null,\"967\":null,\"968\":null,\"969\":null,\"970\":null,\"971\":null,\"972\":null,\"973\":null,\"974\":null,\"975\":null,\"976\":null,\"977\":null,\"978\":null,\"979\":null,\"980\":null,\"981\":null,\"982\":null,\"983\":null,\"984\":null,\"985\":null,\"986\":null,\"987\":null,\"988\":null,\"989\":null,\"990\":null,\"991\":null,\"992\":null,\"993\":null,\"994\":null,\"995\":null,\"996\":null,\"997\":null,\"998\":null,\"999\":null,\"1000\":null,\"1001\":null,\"1002\":null,\"1003\":null,\"1004\":null,\"1005\":null,\"1006\":null,\"1007\":null,\"1008\":null,\"1009\":null,\"1010\":null,\"1011\":null,\"1012\":null,\"1013\":null,\"1014\":null,\"1015\":null,\"1016\":null,\"1017\":null,\"1018\":null,\"1019\":null,\"1020\":null,\"1021\":null,\"1022\":null,\"1023\":null,\"1024\":null,\"1025\":null,\"1026\":null,\"1027\":null,\"1028\":null,\"1029\":null,\"1030\":null,\"1031\":null,\"1032\":null,\"1033\":null,\"1034\":null,\"1035\":null,\"1036\":null,\"1037\":null,\"1038\":null,\"1039\":null,\"1040\":null,\"1041\":null,\"1042\":null,\"1043\":null,\"1044\":null,\"1045\":null,\"1046\":null,\"1047\":null,\"1048\":null,\"1049\":null,\"1050\":null,\"1051\":null,\"1052\":null,\"1053\":null,\"1054\":null,\"1055\":null,\"1056\":null,\"1057\":null,\"1058\":null,\"1059\":null,\"1060\":null,\"1061\":null,\"1062\":null,\"1063\":null,\"1064\":null,\"1065\":null,\"1066\":null,\"1067\":null,\"1068\":null,\"1069\":null,\"1070\":null,\"1071\":null,\"1072\":null},\"Keywords or Approach\":{\"0\":\"simultaneous localization and mapping\\nfeature extraction\\ncameras\\nloss measurement\\nneural networks\\nestimation\\ngraph theory\\nmobile robots\\nneural nets\\nrobot vision\\nslam (robots)\\ngeometric slam factor graph\\nslam systems\\nrelative geometry\\nlearned depth estimation approaches\\nlearned depth predictions\\nimage space\\nnetwork architecture\\ncoarse images\\ngpu acceleration\\nlearned metric data\\nunary scale factors\\nhardware accelerators\\nobservable epipolar geometry\\nmonocular slam\\nlearned scale factors\\nmonocular simultaneous localization and mapping\\nhardware acceleration\\nneural network\",\"1\":\"estimation\\ntrajectory\\nsimultaneous localization and mapping\\ngravity\\nvisualization\\noptimization\\naccelerometers\\nfeature extraction\\nleast squares approximations\\nmaximum likelihood estimation\\noptimisation\\nslam (robots)\\neuroc dataset show\\ntime visual-inertial initialization\\noptimal estimation problem\\nmaximum-a-posteriori estimation\\nalgebraic equations\\nad-hoc cost functions\\norb-slam visual-inertial boosting\\ninertial-only optimization\\nimu measurement uncertainty\\nmap estimation\\nleast squares\",\"2\":\"optical flow\\noptimization\\nsimultaneous localization and mapping\\nrobustness\\ntracking\\nfeature extraction\\nvisualization\\ncomputational complexity\\ngraph theory\\nimage sequences\\noptimisation\\npose estimation\\nquadtrees\\npose-graph optimization time cost\\nlocalization accuracy\\nsparse pose-graph visual-inertial slam algorithms\\nhierarchical quadtree feature optical flow tracking algorithm\\nspvis\\nhigh-precision pose estimation\\nvio-vi-slam system\\ngpu\",\"3\":\"fuses\\nlighting\\nrobustness\\ncomputer vision\\nsimultaneous localization and mapping\\nimage coding\\nconvolutional neural nets\\nimage fusion\\nimage matching\\nlearning (artificial intelligence)\\nrobot vision\\nslam (robots)\\nkeypoint description\\nkeypoint matching\\nvisual simultaneous localization and mapping\\nslam\\nmatching operation\\ndescriptor fusion model\\nrobust keypoint descriptor\\ncnn-based descriptors\\ndfm architecture\\ncnn models\\nmean map\\nhardnet\\ndensenet169\\nconvolutional neural networks\",\"4\":\"cameras\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nmeasurement\\nfeature extraction\\noptimization\\nimage colour analysis\\nimage sensors\\noptimisation\\nphotometry\\npose estimation\\nslam (robots)\\nstereo image processing\\nrgb-d input\\ntum datasets\\neuroc datasets\\nstereo image pairs\\nadaptive algorithm\\nerror vector\\noutlier rejection\\ncomputational efficiency\\nmap-point consensus\\nadaptive virtual camera\\nnoise resilient slam\\norb-slam2\\nsparse-indirect slam systems\\nvirtual camera location\\naxial depth error\\npose optimization\\nconsensus information\\naxial noise\\nlateral noise\\ndepth noise components\\naxial components\\nlateral components\\nnoise sources\\nscale information\\nslam frameworks\\ndepth sensors\\nphotometric invariance properties\",\"5\":\"simultaneous localization and mapping\\nlaser radar\\nthree-dimensional displays\\nbase stations\\ntrajectory\\ndistance measurement\\ngeophysical image processing\\nmobile robots\\nmulti-robot systems\\noptical radar\\nrobot vision\\nslam (robots)\\nstereo image processing\\nterrain mapping\\ntunnels\\nlong corridors\\nsalient features\\nspurious loop closures\\nrepetitive appearance\\nstark contrast\\nhighly-accurate 3d maps\\nunderground extraterrestrial worlds\\nlidar-based multirobot slam system\\ndarpa subterranean challenge\\nsubterranean operation\\naccurate lidar-based front-end\\nperceptually-degraded subterranean environments\\ncomplex subterranean environments\\noff-nominal conditions\\nuneven terrains\\nslippery terrains\\nlarge-scale autonomous mapping-positioning\\nunknown subterranean environment\\nlarge-scale subterranean environment\\ncomplex subterranean environment\\ninaccurate wheel odometry\\ndisaster response\\nflexible back-end\\nrobust back-end\\ntunnel circuit\",\"6\":\"uncertainty\\ndetectors\\nneural networks\\nbayes methods\\nestimation\\nobject detection\\nmeasurement uncertainty\\ngaussian processes\\nneural nets\\nnonmaximum suppression components\\ncommon object detection datasets\\nbayesod\\nminimum gaussian uncertainty error metric\\nminimum categorical uncertainty error metric\\nbayesian approach\\ndeep object detectors\\ndeep neural networks\\nuncertainty measures\\noutput predictions\\ndetectors nonmaximum suppression stage\\nanchor-based object detection\\nuncertainty estimation approach\\nstandard object detector inference\",\"7\":\"robots\\ntraining\\nthree-dimensional displays\\nnatural languages\\ngraphical models\\ndistribution functions\\nsolid modeling\\ncontrol engineering computing\\nconvolutional neural nets\\nhuman-robot interaction\\nimage classification\\nimage representation\\nlearning (artificial intelligence)\\nprobability\\nrobot vision\\nobject placement learning\\nrelational instructions\\nspatial relation\\nconvolutional neural network\\npixelwise object placement probability estimation\\nhallucinated high-level scene representation classification\\npixelwise relational probabilities\\nhuman-robot experiments\\nsingle input image\\nlearning signal\\n3d models\",\"8\":\"estimation\\nconvolution\\ncorrelation\\nthree-dimensional displays\\ntraining\\nfeature extraction\\ncomputer architecture\\ncomputer vision\\nconvolutional neural nets\\nimage matching\\nstereo image processing\\ndeep neural networks\\ndisparity estimation problem\\nstereo matching\\ntraditional hand-crafted feature based methods\\ndesigned dnns\\ncomputation resources\\n3d convolution based networks\\nreal-time applications\\ncomputation-efficient networks\\nexpression capability\\nlarge-scale datasets\\nmultiscale predictions\\nfadnet\\nmultiscale weight scheduling training technique\",\"9\":\"testing\\nautonomous vehicles\\ntraining\\nlearning (artificial intelligence)\\nmachine learning\\nrobots\\nrobustness\\ncontrol engineering computing\\nmanipulators\\nmobile robots\\nneural nets\\nadversarial agent training\\nlearned control policies\\nautonomous driving\\ndeep neural network-driven\\nadversarial reinforcement learning agent\\nautonomous vehicle problem\\nautomated black box testing\\nsafety-critical applications\\ncontrol policy\\ndeep neural networks\\nrobot navigation\\nrobotic arm manipulation\\ncontrol problems\\ndeep learning\\ndeep control policies\",\"10\":\"task analysis\\ntrajectory\\nrobots\\ntransmission line measurements\\nvisualization\\npredictive models\\ngrippers\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\noptimal control\\npredictive control\\nrobot programming\\nrobot vision\\nself-supervision technique\\nhigh level supervision\\ntime-reversal model\\nself-supervised model\\ngoal states\\ncomplex manipulation tasks\\ntetris-style block pairs\\nvisual model predictive control\\nrobot learning\\nrgb camera input\\ntrass\\ntime reversal as self-supervision\",\"11\":\"planning\\napproximation algorithms\\nsearch problems\\npath planning\\nrobots\\nacceleration\\nconferences\\napproximation theory\\ngraph theory\\nsampling methods\\nsampling-based approximation\\nadvanced bit*\\nalmost-surely asymptotically optimal sampling-based planners\\nabit*\\nunified planning paradigm\\npath planning problem\\nadvanced graph-search techniques\\nrobotics\\ntruncated anytime graph-based searches\\nrrt*\\nsingle-query\",\"12\":\"octrees\\nplanning\\nthree-dimensional displays\\napproximation algorithms\\ntask analysis\\nruntime\\nassembly planning\\ncad\\ncomputational geometry\\ndata structures\\ngraphics processing units\\npath planning\\nproduction engineering computing\\nrendering (computer graphics)\\nmotion planning\\nassembly sequence planning\\nreal-world cad-scenarios\\ndisassembly path\\ngvd\\nvoronoi voxel history\\ndisassembly paths\\ngeneral voronoi diagram graph\\nhash table-based data structure\\nerror-bounded wavefront propagation\\nerror-bounded gpu render approach\\nvoxel-based general voronoi diagram\\nroadmap\\nrepresentative vehicle data set\",\"13\":\"trajectory\\nrobots\\ndynamics\\nencoding\\ncollaboration\\nadaptation models\\nkinematics\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nhuman robot collaboration\\nmotion profiles\\nlearned kinematic pattern\\nkuka lwr4+ robot\\ndmp\\ndynamic movement primitives framework\\nadaptive temporal scaling\\nstatic goal\\ntemporal scaling adaptation\\nmoving goal\",\"14\":\"navigation\\nlinear systems\\nstability analysis\\nplanning\\nrobots\\nmathematical model\\nnonlinear dynamical systems\\ndifference equations\\nmobile robots\\nmulti-robot systems\\nnonlinear control systems\\npath planning\\npredictive control\\nreachability analysis\\nset theory\\nnavigating discrete difference equation governed wmr\\nvirtual linear leader guided hmpc\\nmodel predictive control\\nclassical wheeled mobile robot navigation problem\\nhierarchical mpc\\nstate-of-the-art mpc\\nwmr navigation\\nnonexistence\\nnontrivial linear system\\nunder-approximate reachable set\\nvll-mpc\\nhmpc structure\\nvirtual linear system\\nunder-approximate path\\nrrt*\",\"15\":\"robot sensing systems\\ncollision avoidance\\ncomputational modeling\\nbiological system modeling\\npropulsion\\nfriction\\nmicrorobots\\nmobile robots\\npath planning\\ncurved environments\\nextremely simple robots\\nbiomedical applications\\ntiny robots moves\\nshared external stimulus\\nlow-friction models\\nenvironment boundaries\",\"16\":\"rotors\\ndata models\\nadaptation models\\naerodynamics\\nsteady-state\\natmospheric modeling\\nsensors\\naircraft control\\nautonomous aerial vehicles\\nhelicopters\\nmotion control\\nrotors (mechanical)\\nstability\\nwakes\\nceiling effect model\\nflight control stability\\nmultirotor unmanned aerial vehicles\\nrotor thrust\\nvertical flight tests\\nin unsteady state model based controller\\naerodynamics based thrust model\\nvertical climbing\\nvertical descending\\nwake interaction\\nmomentum theory\",\"17\":\"vehicle dynamics\\ncollision avoidance\\npath planning\\nunmanned aerial vehicles\\neducational robots\\ndynamics\\naerospace robotics\\nmobile robots\\nmotion control\\nmotion primitives-based path planning\\nagile exploration\\naerial robots\\npath planning strategy\\nmicroaerial vehicles\\nvolumetric representation\\ncollision-tolerant flying robot\\nvelocity 2.0 m\\/s\\nsize 0.8 m\",\"18\":\"aerodynamics\\nsmoothing methods\\nrobustness\\ntraining\\nanomaly detection\\noptimization\\naircraft\\nautonomous aerial vehicles\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nregression analysis\\nunsupervised anomaly detection\\nhybrid aerial vehicles\\nmachine learning models\\nflight profiles\\nflight log measurements\\nsensor readings\\npredictive flight dynamics models\\naircraft aerodynamics\\nself-flying delivery drones\",\"19\":\"unmanned aerial vehicles\\ncorrelation\\nvisualization\\nobject tracking\\nfrequency-domain analysis\\nreal-time systems\\nautonomous aerial vehicles\\nimage filtering\\nimage motion analysis\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nrobot vision\\nslam (robots)\\nkeyframe-based simultaneous localization and mapping\\nkeyfilter restriction\\nvisual tracking\\nbackground distraction\\nfilter corruption\\nboundary effect\\nunmanned aerial vehicle\\ncorrelation filter-based tracking\\nkeyfilter-aware real-time uav object tracking\",\"20\":\"task analysis\\nforce\\ngrasping\\nunmanned aerial vehicles\\nrotors\\nend effectors\\naerospace robotics\\ndexterous manipulators\\ngrippers\\nmotion control\\nremotely operated vehicles\\nstability\\naerial regrasping\\naerial manipulator\\ndexterous manipulation\\ntransformable multilink aerial robot\\ntransformable multilink drone\\ngrasping stability\\nthrust force\\ncontinous grasping force\\nadmittance controller\\nimpedance controller\\ncontact aware regrasping\",\"21\":\"natural languages\\nsemantics\\nrobots\\ngrounding\\ntraining\\nplanning\\ndatabases\\nhuman-robot interaction\\nmobile robots\\nnatural language processing\\npath planning\\nrobot vision\\nnatural language phrases\\narbitrary outdoor environments\\nurban environments\\nnatural language commands\\nrobot control\\nsemantic similarities\",\"22\":\"merging\\nmachine learning\\nfeature extraction\\nacceleration\\nroad transportation\\nvehicle dynamics\\nnetwork architecture\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nroad traffic control\\nroad vehicles\\ntraffic engineering computing\\ntraffic conditions\\ntraffic flow\\ndeep merging\\nvehicle merging controller\\nembedding network\\nhighway merging sections\\nlane change\\nvehicle controller\\nmerging efficiency\\nmerging section\\ntarget vehicle speed\\ncontrolled vehicle speed\\ndeep reinforcement learning network architecture\\nlearning efficiency\\nmerging behavior\",\"23\":\"training\\nnoise measurement\\ntraining data\\nradar imaging\\nlabeling\\nlenses\\ndata analysis\\nobject detection\\nroad vehicles\\nsupervised learning\\ntraffic engineering computing\\nnoisy labels\\nnoise-aware training techniques\\nweakly supervised vehicle detection\\nradar labels\\nobject detector\\nimage-based vehicle detection\",\"24\":\"automobiles\\noptimization\\nrobustness\\nmeters\\nroads\\nreal-time systems\\npipelines\\ndriver information systems\\ninteger programming\\nobject detection\\nroad safety\\nroad vehicles\\nvehicle dynamics\\nfalse positive cone detections\\nbinary integer optimization\\ncompetition rules\\naverage cone spacings\\nminimum track width\\nrobust lane detection\\nformula student driverless\\nfsd\\nstudent teams\\nautonomous racecar\\nmain dynamic event\\nunknown track\",\"25\":\"synchronization\\nstability analysis\\ncruise control\\nrobustness\\nautonomous vehicles\\nmotion control\\nacceleration\\nadaptive control\\ncontrol system synthesis\\ndistributed control\\nlyapunov methods\\nmobile robots\\nmulti-robot systems\\nposition control\\nroad traffic control\\nroad vehicles\\nstability\\nvelocity control\\nsynchronization approach\\nadaptive cruise control based nonstop intersection passing\\nintelligent vehicles\\ncruise control performance\\ntraffic congestion\\nincreasing traffic flow capacity\\ncacc problem\\nsynchronization control\\nspatial-temporal synchronization mechanism\\nvehicle platoon control\\nrobust cacc\\ncross-coupling based space synchronization mechanism\\ndistributed control algorithm\\nsingle-lane cacc\\nvehicle-to-vehicle communications\\ndesired platoon trajectory\\nexpected inter-vehicle distance\\nenter-time scheduling mechanism\\nhigh-level intersection control strategy\\nlyapunov-based time-domain stability analysis approach\\ntraditional string stability based approach\\ncacc system\",\"26\":\"cameras\\nsensor fusion\\nautonomous vehicles\\nroads\\ncomputational modeling\\naerospace electronics\\ntraining\\ncomputer vision\\ndecision making\\ndriver information systems\\nlearning (artificial intelligence)\\nmobile robots\\nroad traffic\\nroad vehicles\\nreal-world urban autonomous driving\\nhuman driving demonstrations\\nuser-defined route\\nsingle camera view\\nheavily cropped frames\\nlateral control\\nlongitudinal control\\nreal-world complexities\\nend-to-end conditional imitation learning approach\\nurban routes\\nsimple traffic\\nautonomous vehicle\\neuropean urban streets\\nurban driving\\nhand-crafting generalised decision-making rules\",\"27\":\"roads\\niterative closest point algorithm\\nglobal positioning system\\ndead reckoning\\nsensors\\nvisualization\\ncameras\\nimage filtering\\nimage fusion\\nimage matching\\niterative methods\\nkalman filters\\nnonlinear filters\\nroad vehicles\\ntraffic engineering computing\\nvisual lane marking\\ntopological map matching\\nautonomous vehicle navigation\\ndriver assistance systems\\nonline vehicle localization\\ndistinct map matching algorithms\\nvisual lane tracker\\nmap matching algorithm\\ngrid map\\niterative closest point based lane level map matching\\nmap relative localization\\nlane level matching\\nautonomous vehicles\",\"28\":\"training\\ndeep learning\\nvisualization\\nthree-dimensional displays\\ndatabases\\nneural networks\\nlasers\\ncameras\\nimage retrieval\\nimage sensors\\nlearning (artificial intelligence)\\nneural nets\\nindoor visual place recogniser\\nimage similarity metric\\n3d laser sensor\\ncalibrated spherical camera\\ndeep neural network\\ngeo-referenced images\\ndata collection stage\\n3d laser measurements\\nspherical panoramas\\nindoor areas\\nobserved pixels\\nimage mapping\\nquery image\\nrise\\nindoor visual place recognition problem\",\"29\":\"measurement\\nrobustness\\ncameras\\nestimation\\nvisual odometry\\nsimultaneous localization and mapping\\nrobot vision systems\\ndistance measurement\\ngradient methods\\nimage matching\\nimage registration\\nimage sensors\\npose estimation\\nrobot vision\\nslam (robots)\\nstereo image processing\\nvisual slam systems\\nphotometric consistency\\ngradient-based dissimilarity\\ncamera pose estimation\\nmap building\\ncentral ingredients\\nautonomous robots\\nphotometric error\\ngradient orientation\\nmagnitude-dependent scaling term\\nstereo estimation\\nvisual odometry systems\\ndirect image registration tasks\\nrobust estimates\\nscene depth\\ncamera trajectory\\nmapping capabilities\\nmobile robots\\nsensor data registration\",\"30\":\"optimization\\nsmoothing methods\\ntime measurement\\nintegrated circuits\\nsimultaneous localization and mapping\\nmatrix decomposition\\nmobile robots\\noptimisation\\npath planning\\nrobot vision\\nslam (robots)\\nstate estimation\\nics\\nprimal-dual method\\nmatrix factorizations\\nprimal-dual methods\\nincremental factorization\\nmatrix structure\\nincremental unconstrained optimization\\nrobot state estimate\\nsmoothing-based estimation methods\\nincremental constrained smoothing\",\"31\":\"logic gates\\ndrones\\nestimation\\nreceivers\\nservers\\ninternet of things\\nglobal navigation satellite system\\nautonomous aerial vehicles\\nwide area networks\\nrealistic simulated scenario\\nfully autonomous localization system\\nten-fold improvement\\nlocalization precision\\nfixed network\\nuav\\nlocalization accuracy\\nlora iot networks\\nnode localization\\nwidespread iot communication technologies\\nlong range wide area network\\nlong communication distances\\ndrone-aided localization system\\ncommunication system\\nsearch algorithm\\n3d mobility\",\"32\":\"wireless fidelity\\nposition measurement\\ncomputational modeling\\ndead reckoning\\nrobot sensing systems\\nbuildings\\ncollision avoidance\\nindoor radio\\nmobile robots\\nnavigation\\nprobability\\nfast method\\nindoor localization\\nresource-constrained devices\\nlimited sensing capabilities\\nwearable devices\\nsparse wifi\\nimage-based measurements\\ndense signal maps\\nconditional random fields\\nagent positions\\nknown floor plan\\nsparse absolute position estimates\\nmotion sequences\\nlow-quality measurements\\nhandheld devices\\nmobile devices\\nsensory data\",\"33\":\"robots\\nnavigation\\npredictive models\\ncorrelation\\nprobabilistic logic\\nplanning\\nindoor environments\\nmobile robots\\npath planning\\nterm robot navigation\\ntraversability changes\\nhospitals\\ninformed decisions\\nprobabilistic graphical model\\ncurrently unobserved locations\",\"34\":\"training\\npredictive models\\nkernel\\nprobabilistic logic\\ncomputational modeling\\ncomputational efficiency\\nuncertainty\\napproximation theory\\ngaussian processes\\nlearning (artificial intelligence)\\npredictive control\\nrandom features\\nreinforcement learning methods\\nmodel predictive control\\nmpc\\ncomputational cost\\nlinear gaussian model\\napproximated gp dynamics\\nstate prediction\\nsimulated robot control tasks\\nsample-and-computation-efficient nature\\nmodel-based rl method\\nanalytic moment-matching scheme\",\"35\":\"gaussian processes\\noptimization\\ntrajectory\\ntask analysis\\nrobots\\nmutual information\\nkernel\\nlearning systems\\nmanipulator dynamics\\nmotion control\\nsearch problems\\nsample-efficient robot motion learning\\ngaussian process latent variable models\\nrobotic manipulators\\nhousehold environments\\nkinesthetic teaching\\nparametric function\\nmovement primitive\\nmutual-information-weighted gaussian process latent variable model\\ntrial-and-error\\ntrajectory production\\ntask dynamics\\nmp parameter latent space\\nrobot motion task\\nsearch space\\nsurrogate model\\nps algorithms\\npolicy search reinforcement learning\",\"36\":\"aerodynamics\\npropellers\\nwind tunnels\\natmospheric modeling\\nfeedforward systems\\ndata models\\ntrajectory\\naircraft control\\nattitude control\\nautonomous aerial vehicles\\nerror compensation\\nfeedforward\\nhelicopters\\niterative learning control\\nlearning systems\\nneurocontrollers\\npitch control (position)\\npolynomials\\nrobust control\\niterative learning based feedforward control\\nbiplane-quadrotor tailsitter uas\\ntime on-board algorithm\\nforward transition maneuver\\nrepeated flight trials\\npitch angle\\npropeller thrust\\nsimplified aerodynamics\\noptimal coefficients\\nterminal conditions\\nair speed\\nmodeling error compensation\\ngeometric attitude controller\\nflight modes\\nfeedforward law\\nhigh-fidelity thrust model\\norientation angle\\nneural network model\\nexperimental flight trials\\nlearning algorithm\\nmaneuver control\\nwind tunnel data\\nfeedforward thrust\\nrobustness\\nuas\\nvtol uas\\ntransition maneuver\\niterative learning\",\"37\":\"apertures\\ntrajectory\\nlearning (artificial intelligence)\\ntime measurement\\nimaging\\nimage reconstruction\\nx-rays\\nconvolutional neural nets\\nimage resolution\\nimage sampling\\nimage segmentation\\nx-ray imaging\\nadaptive illumination\\nreinforcement learning\\nimage surface\\nconvolutional neural network\\nrastering method\",\"38\":\"covariance matrices\\nrobots\\ngaussian processes\\nmixture models\\nsparse representation\\ninference algorithms\\nprobabilistic logic\\nprobability\\nrobot vision\\ngp mixtures\\ndata association\\nprobabilistic approach\\nimportant estimation\\nclassification tasks\\nrobotics applications\\ngp-based methods\\nsparse methods\\ncovariance matrix\\northogonal approach\\ngp inference\\nonline update scheme\\nsparse gps\\nmemoisation approach\\nrobotic vision applications\",\"39\":\"end effectors\\ncameras\\nmedical robotics\\nrobot sensing systems\\nimage edge detection\\nsurgery\\nbiomechanics\\nposition control\\ntelerobotics\\ntime data driven precision estimator\\nraven-ii surgical robot end effector position\\nsurgical robots\\ncable-driven nature\\nkinematics calcu-lation\\nreported end effector position\\nposition inaccuracy\\ndata-driven pipeline\\nrobot end effector position precision estimation\\nimproved end effector position error\\nrms\\nentire robot workspace\",\"40\":\"state estimation\\ndata models\\ntask analysis\\nfeature extraction\\nhidden markov models\\nrobots\\nkinematics\\nbiomedical ultrasonics\\nfinite state machines\\nlearning (artificial intelligence)\\nmedical computing\\nmedical image processing\\nmedical robotics\\nsurgery\\nskill assessment working set\\nrobotic intra-operative ultrasound imaging\\nda vinci\\u00ae xi surgical system\\nsuperior frame-wise state estimation accuracy\\ntemporal segmentation\\ndeep learning\\ndata sources\\nrobot-assisted surgeries\\nfinite-state machines\\nsurgical task\\ntemporal perception\\ncurrent surgical scene\\nreal-time estimation\\ntask progresses\\nstate estimation models\\nsurgical state estimation models\",\"41\":\"robot kinematics\\ntask analysis\\nmanipulators\\naerospace electronics\\nrobot sensing systems\\nglass\\ncontrol system synthesis\\nhandicapped aids\\nlearning systems\\ntelerobotics\\nlearned latent actions\\nassistive robotic arms\\nhigh-dimensional robot behavior\\nuser-friendly latent actions\\nlow-dimensional embeddings\\nrobotic arm\\nassistive eating\\ncooking tasks\\nassistive robot control\\nphysical disabilities\\nteleoperation\\nhandheld joystick\\nshared autonomy baselines\\nphysically assistive devices\\ncognitive humanrobot interaction\\nhuman-centered robotics\",\"42\":\"torque\\nactuators\\njoints\\ntendons\\nlegged locomotion\\ntopology\\noptimisation\\nposition control\\nrobot dynamics\\ntorque control\\nseries-parallel compliant articulated leg prototype\\nhighly-efficient parallel actuation branches\\ntorque allocation\\ntransmission ratio\\nactuator hardware specifications\\nperiodic squat motions\\nmotion efficiency\\nparallel actuators\\nquadratic criteria\\noptimization based controller\\nredundant robots\\ntorque distribution\\nseries-parallel compliant articulated robots\",\"43\":\"legged locomotion\\nrobot sensing systems\\nvelocity measurement\\nestimation\\nkinematics\\nfoot\\ncontrol nonlinearities\\ndistance measurement\\ngraph theory\\nimage fusion\\ninertial navigation\\nmotion control\\nrobot dynamics\\nrobot vision\\nstate estimation\\npreintegrated velocity bias estimation\\ncontact nonlinearities\\nlegged robot odometry\\nfactor graph formulation\\nquadruped robot\\nslippery terrain\\ndeformable terrain\\npreintegrated velocity factor\\nleg flexibility\\nleg odometry drift\\nimu factors\\nanymal robot\\nproprioceptive state estimator\",\"44\":\"legged locomotion\\ntorque\\nthigh\\nhip\\nknee\\nenergy efficiency\\nenergy conservation\\nmotion control\\noptimisation\\nsearch problems\\ntorque control\\nposture searching\\nenergy-efficient quadruped locomotion\\nenergy-efficient locomotion\\nlegged robot\\nquadrupedal robot\\nnominal stance parameters\\nleg torque distribution\\nfoothold planner\\nstanding legs\\nenergy-saving stance posture\\nstairs climbing\\ncenter of gravity trajectory planner\\ncog\\nfoothold planning optimization\",\"45\":\"cost function\\nlegged locomotion\\ndata models\\ntuning\\npredictive control\\nadaptive control\\ncontrol system synthesis\\nlearning (artificial intelligence)\\nmotion control\\noptimisation\\nlegged locomotion heuristics\\nregularized predictive control\\nlegged robots\\ndynamic maneuvers\\ndifficult terrains\\nmeaningful cost functions\\nhigh-fidelity models\\ntiming restrictions\\nprincipled regularization heuristics\\nlegged locomotion optimization control\\ncost space offline\\ndesired commands\\noptimal control actions\\nrobot states\\nheuristic candidates\\nadaptation laws\\nmodels online\\npowerful heuristics\\napproximate complex dynamics\\nmodel simplifications\\nparameter uncertainty\\nparameter tuning process\\nincreased capabilities\\nnewly extracted heuristics\\ncontroller structure\\nmini cheetah robot\",\"46\":\"hardware\\nlegged locomotion\\ntraining\\ntask analysis\\nplanning\\nheuristic algorithms\\nlearning (artificial intelligence)\\npath planning\\npredictive control\\nrobot dynamics\\nrobot kinematics\\ngeneralizable locomotion skills\\nhierarchical reinforcement learning\\narbitrary goals\\nhierarchical framework\\nsample-efficiency\\ngeneralizability\\nlearned locomotion skills\\nreal-world robots\\ngoal-oriented locomotion\\ndiverse primitives skills\\nfreedom robot\\ncoarse dynamics models\\nprimitive cycles\\nmodel predictive control framework\\ndaisy hexapod hardware\\nsize 12.0 m\",\"47\":\"legged locomotion\\npneumatic systems\\nsoft robotics\\npneumatic actuators\\ntrajectory\\nactuators\\nmotion control\\nsorx\\nsoft pneumatic hexapedal robot\\n2-degree-of-freedom soft pneumatic actuator\\ntripod gait\\npneumatically-actuated legged robots\\nrough terrain\\nsteep terrain\\nopen-loop control\\ncyclic foot trajectories\\nphysical testing\",\"48\":\"charging stations\\nbatteries\\ntrajectory\\noptimization\\nsensors\\nunmanned aerial vehicles\\neuclidean distance\\nant colony optimisation\\nautonomous aerial vehicles\\nbattery powered vehicles\\nelectric vehicles\\noptimal number\\nubat\\nant colony optimization\\nuav trajectories\\nbattery swap stations\\nuavs\\nflight time\\ncharging station deployment problem\\nnp-hard problem\",\"49\":\"trajectory\\nplanning\\nheuristic algorithms\\ncollision avoidance\\noptimization\\nsystem recovery\\nthree-dimensional displays\\nmulti-agent systems\\noptimisation\\npolynomials\\noptimization-based approaches\\nerroneous optimization setup\\ninfeasible collision constraints\\nsequential optimization method\\ndummy agents\\nrelative bernstein polynomial\\nnonconvex collision avoidance constraints\\nmultiagent trajectory planning problems\\nobstacle-dense environments\\ngrid-based approaches\",\"50\":\"robots\\ntask analysis\\nschedules\\nmanufacturing\\ncollision avoidance\\nrouting\\nproduction facilities\\nassembly planning\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nrobotic assembly\\nnonholonomic differential-drive robots\\noptimal sequential task assignment\\ncollision-free trajectories\\nrobotic manufacturing\\ncollision-free routing\\nmultiagent robotic assembly planning\\npath finding\",\"51\":\"collision avoidance\\nnavigation\\nrobot sensing systems\\nrobot kinematics\\ntraining\\nadaptation models\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\npath planning\\noptimal paths\\nmultiple robots\\ndynamics randomization\\ndifferential drive robots\\ndynamic environment\\nobstacle complexities\\nmultirobot navigation problem\\ndeep reinforcement learning framework\\noptimal target locations\\ndrl based framework\\nnavigation policy\",\"52\":\"mobile robots\\ntrajectory\\nreal-time systems\\nnavigation\\nkinematics\\nenergy conservation\\ngraph theory\\npath planning\\nrobust control\\nsample based methods\\nsub-optimal memory-intensive\\nadaptive directional planner algorithm\\nrobust local path planning\\nautonomous navigation\\nform factor mobile robots\\nlow memory footprint\\nrobust navigation\\nunknown environments\\ncomplex environments\\nfundamental capability\\nrobotic applications\\noptimal robot path planning\\ncomplex memory intensive task\\nadaptive directional path planner\\nadp algorithm implementation\\nmemory size 28.0 kbyte\",\"53\":\"robots\\nheuristic algorithms\\njacobian matrices\\noptimal control\\nsystem dynamics\\noptimization\\ngeometry\\nlie algebras\\nlie groups\\noptimisation\\ntrajectory control\\ngeometric algorithm\\nrobot trajectory optimization\\nlie group method\\nfloating-point operations\\nfirst-order information\\nsparsity exploitation\\nnumerical differentiation\\nanalytical differentiation\\nstate equations\\nrecursive algorithms\\narticulated robots\\ndirect collocation algorithm\",\"54\":\"trajectory optimization\\nminimization\\ncomputational modeling\\ncollision avoidance\\nrobots\\nmathematical model\\napproximation theory\\nconvex programming\\nminimisation\\nquadratic programming\\nrobot kinematics\\nnonholonomic trajectory optimization\\nnonholonomic kinematics\\nnonlinearly maps control input\\nnonconvex\\nbi-convex cost\\nconstraint functions\\nbi-convex part\\nnonholonomic behavior\\nnonlinear penalty\\nnonlinear costs\\nbi-convex structure\\nbi-convex approximation\\nautonomous cars\\nfixed-wing aerial vehicles\\ncomputational tractability\\nalternating minimization\\nsequential quadratic programming\\ninterior-point methods\",\"55\":\"legged locomotion\\nforce\\noptimization\\nsprings\\nmodulation\\nhip\\nmotion control\\nopen loop systems\\noptimisation\\nrobot dynamics\\nstability\\nstate feedback\\nmodel-based trajectory optimization\\ndirect-drive robot\\ndirect-collocation-formulated optimization\\nsingle-legged planar robot\\nopen-loop stable motion primitives\\nproprioceptive-only sensing\\nversatile dynamic behaviors\\nexpensive inertial sensors\\nagile control\\nstable control\\nmodel-based optimization\\nopen-loop stable running behaviors\",\"56\":\"robustness\\nsafety\\nplanning\\nrobots\\ntrajectory\\nprobability distribution\\ncollision avoidance\\ndecision making\\nmobile robots\\noptimisation\\npath planning\\npredictive control\\nprobability\\nrobust control\\nwasserstein distributionally robust motion planning\\nsafety constraints\\nconditional value-at-risk\\noptimization-based decision-making tool\\nsafe motion planning\\npre-specified threshold\\nobstacles\\nwasserstein ball\\navailable empirical distribution\\nout-of-sample performance guarantee\\nrisk constraint\\ncomputationally tractable method\\ndistributionally robust model predictive control problem\\ndistributionally robust method\",\"57\":\"measurement\\nplanning\\nforce\\ntensile stress\\ngrasping\\nresilience\\ndexterous manipulators\\ngrippers\\nminimisation\\nstress-minimization metric\\nhomogeneous isotopic materials\\nsm metric measures\\nfragile objects grasping\\noptimal grasp planning algorithms\",\"58\":\"force\\ngrippers\\nfriction\\ndynamics\\nmathematical model\\ncomputational modeling\\ngrasping\\ndexterous manipulators\\nmanipulator dynamics\\nmaterials handling\\nmotion control\\npath planning\\ntactile sensors\\ngrasp control\\nenhancing dexterity\\nparallel grippers\\nrobust grasp controller\\nslipping avoidance\\nmodel-based algorithm\\nmodified lugre friction model\\nrotational frictional sliding motions\\nlimit surface concept\\ncomputationally efficient method\\nminimum grasping force\\ntangential loads\\ntorsional loads\\ncontrol modalities\\nrobot motion\\nautomatically generates robot motions\\ngripper commands\",\"59\":\"conferences\\nautomation\\nadaptive control\\nforce control\\nfriction\\ngrippers\\nmanipulators\\nrobust control\\nobject manipulation\\ngrasp force control algorithm\\nadaptive grasping\\nrotational incipient slip detection\\nrobotics\\nincipient slip robust detection\\ncenter of gravity\\nfriction coefficient\",\"60\":\"visualization\\ngrasping\\nfeature extraction\\ntask analysis\\ntactile sensors\\nconvolutional neural nets\\ndeformation\\ndexterous manipulators\\nforce control\\ngrippers\\nimage classification\\nimage fusion\\nlearning (artificial intelligence)\\nrobot vision\\ndexterity grasping\\nautomatic force control\\nclassification\\ntactile sensor\\nwrist camera\\nrobotic arm\\ndeformable objects\\n3d convolution based visual tactile fusion deep neural network\\nadaptive grasping\\nextensive grasping\\nc3d-vtfn\\nsliding deformation\\ngrasp state assessment\",\"61\":\"shape\\ncameras\\ngrasping\\nplanning\\nrobot vision systems\\npipelines\\nend effectors\\ngrippers\\nimage colour analysis\\nimage sensors\\npath planning\\nposition control\\nrobot vision\\ncamera images\\ngrasp success rate\\nsimulated images\\ntop-grasps\\nscene completion\\nsix-degree-of-freedom grasps\\nsimulated viewpoints\\ngeneration method\\nfully convolutional grasp quality cnn\\nend-to-end grasp\",\"62\":\"three-dimensional displays\\ncameras\\nplanning\\nsensors\\nsmart phones\\nfeature extraction\\nrobots\\napple computers\\naugmented reality\\ncomputational geometry\\ncontrol engineering computing\\ndexterous manipulators\\ndistributed processing\\nimage colour analysis\\nimage sequences\\nmobile computing\\npath planning\\nrobot vision\\ndistributed deep grasp planning\\ncommodity cellphone\\naugmented reality app\\nconsumer demand\\nmobile phone applications\\nar apps\\nrgb image sequence\\npoint clouds\\ndistributed pipeline\\ndex-net ar\\ndex-net grasp planner\\nerror estimation\\nrobot gripper\\niphone\\narkit\",\"63\":\"cameras\\nthree-dimensional displays\\nestimation\\nfeature extraction\\nvisual odometry\\nsensors\\ntrajectory\\ncomputerised instrumentation\\ndistance measurement\\nimage matching\\nimage reconstruction\\nimage sequences\\nneural nets\\nstereo image processing\\nomnidirectional localization\\nwide-baseline multicamera systems\\ndense mapping system\\nwide-baseline multiview stereo setup\\nultrawide field-of-view fisheye cameras\\nstereo observations\\nlight-weighted deep neural networks\\nloop closing module\\nefficient feature matching process\\nomnidirectional depth maps\\ntruncated signed distance function volume\\nrig estimation\\nomnidirectional depth map estimation\\nvo\\nfov\\ntsdf\\nomnislam\",\"64\":\"image segmentation\\nobject recognition\\nthree-dimensional displays\\nsemantics\\ntask analysis\\nlayout\\ndistortion\\nlearning (artificial intelligence)\\nnatural scenes\\nneural nets\\nobject detection\\nsolid modelling\\n3d model\\ninstance segmentation masks\\nequirectangular images\\ndeep learning\\nsemantic segmentation tasks\\nobject recognition system\\nindoor scenes\\nindoor panoramic images\",\"65\":\"cameras\\nestimation\\ntraining\\neuclidean distance\\nimage reconstruction\\nthree-dimensional displays\\nrobot vision systems\\ndistance measurement\\nfeature extraction\\nimage classification\\nimage motion analysis\\nimage sampling\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nroad vehicles\\nrobot vision\\ntraffic engineering computing\\nvideo signal processing\\nautonomous driving\\nnonlinear distortions\\ncomplex algorithms\\neuclidean distance estimation\\nfisheye cameras\\nautomotive scenes\\naccurate depth supervision\\ndense depth supervision\\nself-supervised learning approaches\\nself-supervised scale-aware framework\\nraw monocular fisheye videos\\napplying rectification\\npiece-wise linear approximation\\nfisheye projection surface\\nre-sampling distortion\\nmonocular methods\\nunseen fisheye video\\nself-supervised scale-aware distance estimation\\nmonocular fisheye camera\",\"66\":\"cameras\\nestimation\\nthree-dimensional displays\\ndistortion\\nfeature extraction\\nconvolution\\ntraining\\nneural nets\\nstereo image processing\\n360sd-net\\nstereo depth estimation\\nlearnable cost volume\\nend-to-end trainable deep neural networks\\nperspective images\\nequirectangular projection\\ndistortion issue\\nlearnable shifting filter\\nstereo datasets\\ncamera pairs\\nspherical disparity\",\"67\":\"convolution\\nestimation\\nsensors\\ncameras\\ntransforms\\nkernel\\ntask analysis\\ncalibration\\ncomputer vision\\nconvolutional neural nets\\nfeature extraction\\nimage reconstruction\\nimage sensors\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nstereo image processing\\nomnidirectional depth extension networks\\nomnidirectional 360\\u00b0 camera\\nautonomous robots\\nperception ability\\nfov\\ndepth sensors\\nperception system\\nlow-cost 3d sensing system\\nomnidirectional camera\\ncalibrated projective depth camera\\nrecorded omnidirectional image\\nomnidirectional depth extension convolutional neural network\\nspherical feature\\nfeature encoding layers\\ndeformable convolutional spatial propagation network\\nfeature decoding layers\\nomnidirectional coordination\\nprojective coordination\\nfeature learning\\nestimated depths\\nproposed ode-cnn\\npopular 360d dataset\\ndepth error\",\"68\":\"cameras\\nthree-dimensional displays\\nfeature extraction\\ngoogle\\nestimation\\ndata mining\\nrobot vision systems\\ncomputer vision\\nconvolutional neural nets\\nimage classification\\nregression analysis\\nstereo image processing\\nconvolutional neural network\\n3d orientation estimation\\n3d scene understanding\\nsingle spherical panorama\\nlabeled 3d orientation\\nvanishing point extraction\\nsingle panoramas\\nvop60k\\ngoogle street view\\npinhole cameras\\npanorama geometric information\\ntwo column vector regression loss\\nrotation matrix\\ncnn architecture\\nclassification loss\\nedge extractor layer\",\"69\":\"shape\\ncameras\\ntactile sensors\\nthree-dimensional displays\\ncolour\\nelasticity\\nforce sensors\\nimage colour analysis\\nindentation\\nsubtractive color-mixing principle\\nplanar manufacturing process\\nfunctional features\\nchromatouch\\nmillimeter-size indentation\\nelastic membrane\\n3-dimensional displacement field\\ndistributed tactile sensor\\nfine distributed sense\\nrobots\\nfine manipulation\\nlocal shape\\nsoft fingers\\ncues\\nmarker array\\ncolor-interference\\nspherical tactile sensor\\ncurvature sensing\\nnormal sensing\\ncontact mechanics\\nhertz contact theory\\ncurved surface\\nrelative motion\\ncolored patches\\nlocal 3d displacement\\nmeasurement points\\nspherical shape\\nflat functional panels\\nflat surface\\nsize 40 mm\",\"70\":\"grasping\\ntactile sensors\\nforce\\nrobustness\\ndexterous manipulators\\ngrippers\\npath planning\\nrobot vision\\nfranka emika robot arm\\nmultisensor modules\\nregrasp planner\\nslip detection\\nvisual sensors\\ncenter-of-mass-based robust grasp planning\",\"71\":\"cameras\\ntactile sensors\\nsensitivity\\ntask analysis\\ncomputer vision\\nconvolutional neural nets\\ndexterous manipulators\\ngels\\nimage resolution\\nmicrosensors\\nneurocontrollers\\nsensor fusion\\nstate estimation\\ndexterous robotic manipulation\\ndeep convolutional neural networks\\nelectrical connector\\nmultidirectional high-resolution touch sensor\\nlow-resolution signals\\nmultidirectional high-resolution tactile sensor\\nrobotic hands\\nmultiple microcameras\\nmultidirectional deformations\\ngel-based skin\\ncontact state variables\\nimage processing\\ncomputer vision methods\\nstate estimation problem\\nrobotic control task\\nomnitact combination\",\"72\":\"robot sensing systems\\ntunneling magnetoresistance\\nmagnetic separation\\nsubstrates\\nbridge circuits\\nmagnetic tunneling\\nassembling\\nmagnetic field measurement\\nmagnetic sensors\\nmagnetoresistive devices\\npermanent magnets\\nsignal detection\\nsurface texture\\nsurface topography measurement\\ntunnelling magnetoresistance\\nbioinspired sensor\\nfine surface exploration\\ntexture sensing\\nrobotics\\ntexture topography sensor\\nciliary structure\\nbiological structure\\npermanent magnetization\\ntunneling magnetoresistance sensor\\nelectronic signal acquisition board\\ntopography scanner\\nelastic cilia brush\\ntmr sensor\\nsurface roughness\\nsize 7.0 mum\\nsize 9.2 mum to 213.0 mum\",\"73\":\"robot sensing systems\\nforce\\nsurface cracks\\noptics\\nfeature extraction\\ncrack detection\\nlearning (artificial intelligence)\\nobject detection\\npattern classification\\ntactile sensors\\ntelerobotics\\nvideo cameras\\nproximity sensing\\nremote characterisation\\nphysical robot-environment interaction\\nautomatic crack detection\\nproximity sensor\\nphysical environment\\nfibre optics\\ncracks\\nbumps\\nundulations\\nmachine learning\\nclassifier\\naverage crack detection accuracy\\nwidth classification accuracy\\nkruskal-wallis results\\nforce data\\nproximity data\\noptical fibres\\nextreme environments\",\"74\":\"robot sensing systems\\ncapacitance\\nelectrodes\\ncapacitance measurement\\nsensitivity\\nservice robots\\ncapacitive sensors\\ndeformation\\nmechanical variables measurement\\nobject detection\\ntactile sensors\\npartial contact position estimation\\npush-in stroke detection\\ndeformation detection\\ntactile sensor module\\ncapacitive proximity sensor module\\nsuction cup\",\"75\":\"optical imaging\\noptical filters\\noptical sensors\\nintegrated optics\\noptical variables measurement\\nadaptive optics\\ncameras\\nimage filtering\\nimage sensors\\nimage sequences\\npose estimation\\nconstrained filtering-based fusion\\ninertial measurements\\ncamera poses\\nbio-inspired sensor\\nbrightness changes\\nindependent pixels\\nhigh dynamic range\\noptical flow\\nintensity images\\npixel difference\\ninverse scene-depth\\ndvs dataset\\nfiltering-based estimator\\ndynamic vision sensor\",\"76\":\"target tracking\\nrobot sensing systems\\nestimation\\nthree-dimensional displays\\ndynamics\\nimage motion analysis\\nimage representation\\nkalman filters\\nobject tracking\\npose estimation\\nrobot vision\\ntracking sensor\\ntarget motion model\\nschmidt-ekf-based visual-inertial moving object tracking\\ntightly-coupled estimation\\nvisual-inertial localization\\njoint estimation system\\nschmidt-kalman filter\\nego-motion accuracy degradation\\nrobot-centric representation\\nobject pose tracking\",\"77\":\"visual servoing\\nnavigation\\nvisualization\\nfeature extraction\\ntask analysis\\nsemantics\\ncameras\\nconvolutional neural nets\\nfeedback\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\ndeep reinforcement learning\\nmobile robot navigation\\ndeep convolutional network controller\\nviewpoint invariant visual servoing\\ntarget invariant visual servoing\\nfeedback control error\",\"78\":\"cameras\\ndistance measurement\\nrobot sensing systems\\nvisualization\\nfeature extraction\\ngraph theory\\nleast squares approximations\\noptimisation\\npose estimation\\nposition measurement\\nrobot vision\\nlevenberg-marquardt nonlinear least squares optimization scheme\\nscale factor\\nvisual features\\npose-graph optimization scheme\\nlandmark reprojection errors\\nvisual drift\\nmonocular visual feature observations\\ndistance measurements\\nultrawideband-aided monocular visual odometry system\\nsingle-anchor monocular visual odometry system\\ntightly-coupled odometry framework\\nanchor position estimation\\nrobot operating system\",\"79\":\"navigation\\ntrajectory\\nrobustness\\nrobot kinematics\\nplanning\\nvisualization\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\ndeep learning\\nrobot perception\\nreliability issue\\nworld images\\nmechanical constraints\\nlocal controller\\nlarge-scale visual topological navigation\\nlarge-scale environments\\nlocal control\\nlarge-scale topological navigation\",\"80\":\"feature extraction\\nnavigation\\nlegged locomotion\\nvisualization\\ntraining\\ngraphical user interfaces\\nhumanoid robots\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\ncost-effective data collection\\nthird-person demonstrations\\ncamera perspectives\\nperspective-invariant state features\\nmodel-based imitation learning approach\\naction-labeled human demonstrations\\neffective policy\\nzero-shot imitation\\nlegged robot visual navigation\\ntraining effective visual navigation policies\\nexpert demonstrations\\ngoal-driven visual navigation policy\\nhigh-quality navigation\",\"81\":\"manipulators\\nactuators\\nencapsulation\\nfabrication\\nforce\\ncontrollability\\ndeformation\\nfibre reinforced composites\\nposition control\\nrigidity\\nair pressure\\nposition dependent stiffness\\nmanipulator stiffness\\nvariable stiffness structure\\npressure driven manipulator\\nsoft rigid stiffness control structure\\nunidirectional fiber reinforced actuators\\nsoft robot deformability\\nsoft robot compliance\\nbidirectional in-plane manipulator\",\"82\":\"robot kinematics\\ntask analysis\\nmanipulators\\nsoft robotics\\ntracking\\nrobot sensing systems\\nbiomechanics\\nhuman-robot interaction\\nrobots\\nservice robots\\ntelerobotics\\nsoft growing robot manipulator\\nbody-movement-based interface\\npick-and-place manipulation task\\nhuman-centered interface\\ncomplex manipulation tasks\\nteleoperated object manipulation\\nhuman interface\",\"83\":\"hip\\nexoskeletons\\nlegged locomotion\\nread only memory\\nkinematics\\nsprings\\ntime measurement\\ngait analysis\\nmedical control systems\\nmedical robotics\\npatient rehabilitation\\nmodulating hip stiffness\\nrobotic exoskeleton\\nhealthy kinematics\\ncritical component\\nassisting rehabilitating impaired locomotion\\nspatiotemporal gait patterns\\nmechanical impedance\\nhip joints\\nsamsung gems-h\\nvirtual spring\\nshort repeated exposures\\nspatiotemporal measures\\nstiffness controller\\ngait behavior\\nmechanical effect\\nlower-limb assistive devices\\nhealthy gait patterns\",\"84\":\"knee\\nprosthetics\\nvalves\\nhip\\ntorque\\nactuators\\nlegged locomotion\\nartificial limbs\\ngait analysis\\nkinematics\\nmedical control systems\\nstair ascent\\nscsa prosthesis prototype\\nunilateral transfemoral amputee\\nmotion capture apparatus\\nmicroprocessor-controlled daily-use\\npassive prosthesis\\nscsa knee\\nstair activity\\nswing-assist\\nenhancing stair ambulation\\nprimarily-passive knee prosthesis\\nprimarily-passive stance-controlled swing\",\"85\":\"foot\\ntorque\\ndc motors\\nvalves\\noptical sensors\\nforce\\ncompressors\\ngait analysis\\nmedical control systems\\northopaedics\\northotics\\npneumatic systems\\nunilateral drop-foot patients\\nmaximum assistive torque\\nunilateral active afo\\npressurized air\\nwearable custom compressor\\nassistive devices\\nstationary air compressors\\nmass distribution\\npneumatic components\\npowered ankle foot orthosis systems\\npneumatic transmission\\ndrop foot correction\\npneumatic ankle foot orthosis powered\\npressure 1050.0 kpa\\npressure 550.0 kpa\",\"86\":\"prosthetics\\ntask analysis\\nimpedance\\nknee\\nlegged locomotion\\ntuning\\nartificial limbs\\nbiomechanics\\ngait analysis\\nlearning (artificial intelligence)\\nmedical robotics\\nneurophysiology\\npatient rehabilitation\\nknowledge transfer\\ncontrol tuning performance\\namputee subject\\nab subjects\\ntransfer knowledge\\ndata requirements\\nrl controller\\nrobotic prosthetic limb\\ncontrol method\\nknowledge-guided q-learning\\nrl agents learn\\ncontrolled prosthesis\\nprosthesis control\\nprosthesis device\\ntrans-femoral amputees\\npassive prostheses\\nlost functions\\nrobotic lower limb prosthesis\\nguided reinforcement learning control\",\"87\":\"hip\\nexoskeletons\\ntorque\\npulleys\\nactuators\\nkinematics\\ntask analysis\\nbiomechanics\\ndesign engineering\\nelectric motors\\nhandicapped aids\\npatient rehabilitation\\nrobot dynamics\\nrobot kinematics\\ntorque control\\nwearable robots\\ntwisted string actuator-based exoskeleton\\nhip joint assistance\\ncompliant cable-driven exoskeleton\\ninjuries\\nvocational setting\\npowerful exoskeleton\\ninherent tsa advantages\\ntypical torque-speed requirements\\nexoskeleton design\\nmotor selection\\npractical exoskeleton prototype\",\"88\":\"springs\\nexoskeletons\\ngravity\\nlegged locomotion\\ngears\\npotential energy\\nhip\\ngait analysis\\nhandicapped aids\\nman-machine systems\\nmedical robotics\\nmotion control\\npatient rehabilitation\\nrobot kinematics\\nsprings (mechanical)\\ntorque\\ngravity compensation\\nwalking assistance\\nspring mechanisms\\nknee joints\\ngravity balancing\\nhuman leg\\nmating gears\\ntension force\\nsafety\\nuser acceptance\\ndesign principle\\nlimb joints\\nsingle leg exoskeleton\\nportable passive lower limb exoskeleton\\ndriving torques\",\"89\":\"robot kinematics\\nneedles\\nnumerical models\\nsolid modeling\\ntrajectory\\nsprings\\ndrag\\nimpact (mechanical)\\nmobile robots\\nnumerical analysis\\nrobot dynamics\\nvehicle dynamics\\nthrusting mechanism\\ndepth dependent model\\nsteerable burrowing robot\\nvibro-impact mechanism\\nrotating bevel-tip head\\nnonholonomic model\\nsteering mechanism\\nhybrid dynamics model\\ns-shaped trajectory\",\"90\":\"grippers\\nforce\\ncuring\\nadhesives\\nchemicals\\nmobile robots\\nadhesion\\nelectric motors\\nplastics\\nprototypes\\nultraviolet sources\\nplastic parts\\nforce-to-weight ratio\\nhigh force density gripping\\nuv activation\\nsacrificial adhesion\\nlight-activated chemical adhesive\\nultraviolet light sensitive acrylics\\nrapid curing\\nelectric motor\\nproof-of concept prototypes\\nsize 380.0 nm\\ntime 15.0 s to 75.0 s\\nmechanism design\\nmanipulation\",\"91\":\"vibrations\\nparallel robots\\nautomation\\nsimulation\\nrobot kinematics\\nconferences\\nthree-dimensional printing\\ncables (mechanical)\\nindustrial robots\\noptimisation\\nposition control\\nrigidity\\nvibration control\\nstiffness optimization\\ncable driven parallel robot\\nadditive manufacturing\\nanchor points\\nrobot stiffness\\ntool path\\nplatform rigidity\\ncdpr\\n3d printing\\nvibration modes\",\"92\":\"legged locomotion\\ntrajectory\\nend effectors\\ncouplings\\nshape\\nactuators\\ncams (mechanical)\\ncompliant mechanisms\\nforce control\\nmotion control\\npath planning\\ntrajectory control\\ncami\\nmultilegged locomotion\\ncontinuous gait transition\\nend effector trajectory\\nend effector motions\\nthree dimensional cam system\\nforce compliant variable cam system\\nbipedal robot\",\"93\":\"legged locomotion\\nwheels\\nmanipulators\\nsteel\\nforce\\nadhesion\\npermanent magnets\\npropulsion\\nroad vehicles\\nswappable propulsors\\nadhesion forces\\nwheeled locomotion\\nautonomous ground vehicles\\nterrain\\nwhegs\\nphysical adaptation\\nmultipurpose manipulators\\npropulsion system\\nadaptive ground mobility\\nfunctional prototype robot\\nmechanism design\\nmobile manipulation\\nwheeled robots\",\"94\":\"strain\\nrotors\\nservomotors\\nprototypes\\ndeformable models\\ntask analysis\\ntorque\\nactuators\\ndeformation\\nhelicopters\\nsymmetrical deformation\\nsynchronous deformation\\nmulticopter system\\nflight missions\\nstable flight behavior\\nfolding\\nunfolding body deformations\\nsniae-sse deformation mechanism\\nflight performance validation\\nmodeling validating\\nstraight scissor-like elements\\nsimple non-intersecting angulated elements\\nactuation capability\",\"95\":\"target tracking\\nrobot kinematics\\nsonar\\nreceivers\\nsignal processing algorithms\\nautonomous underwater vehicles\\nmobile robots\\nsensor fusion\\nunderwater acoustic communication\\nauv cooperative strategies\\ndata fusion\\nrealistic underwater surveillance scenarios\\nnetworked auvs\\ndata sharing\\nrobotic networks\\nunderwater surveillance applications\\ncmre anti-submarine warfare network\\ntrack management module\\nrobot autonomy software\\ntrack classification\\nt2t association\",\"96\":\"propulsion\\nresonant frequency\\nstrain\\nstandards\\ndamping\\nreliability engineering\\nautonomous underwater vehicles\\ndiaphragms\\nelectromagnetic actuators\\nmarine control\\nmobile robots\\nmotion control\\nrobot vision\\nslam (robots)\\nelectromagnetic voice coil motor\\nbidirectional resonant propulsion\\nauv localization\\nthrust vectors\\ndiaphragm pump mechanism\\nresonant motion\\nactuator design\\nbidirectional resonant pump\",\"97\":\"planning\\nrobots\\nvehicle dynamics\\noceans\\nheuristic algorithms\\nprediction algorithms\\ntrajectory\\nautonomous underwater vehicles\\ncomputational complexity\\ngraph theory\\nmarine vehicles\\nmobile robots\\npath planning\\nremotely operated vehicles\\nhierarchical planning\\ntime-dependent flow fields\\nshortest paths\\nflow predictions\\nmotion planning\\nslow marine robots\\ndynamic ocean currents\\ntime-dependent graphs\\npolynomial-time algorithm\\ncontinuous trajectories\\ntime-varying edge costs\\nunderlying flow field\\ncontinuous algorithm\\ntime complexity\\npath quality properties\\nautonomous marine vehicle\\nmarine robotics\\ntime-varying ocean predictions\\neast australian current\",\"98\":\"planning\\nthree-dimensional displays\\nnavigation\\noptimization\\nrobots\\ntrajectory\\ncameras\\nautonomous underwater vehicles\\ncollision avoidance\\nfeature extraction\\nmobile robots\\noptimisation\\nrobot vision\\nstereo image processing\\nfly-overs\\nauv\\ncluttered space\\nnavigation framework\\nsampling-based correction procedure\\nobstacles detection\\nreal-time 3d autonomous navigation\\nagile autonomous underwater vehicle\\ntrajopt\\n3d path-optimization planning\\nvisual features detection\",\"99\":\"training\\nimage resolution\\nrobots\\ndata models\\ncameras\\npipelines\\ngenerators\\nlearning (artificial intelligence)\\nneural nets\\nrobot vision\\nunderwater vehicles\\nsingle image super-resolution\\nautonomous underwater robots\\nadversarial training pipeline\\nperceptual quality\\nglobal content\\nlocal style information\\nusr-248\\nsisr\\nstate-of-the-art models\\ndeep residual multipliers\\ndeep residual network-based generative model\\nunderwater image super-resolution\\nnoisy visual conditions\",\"100\":\"synchronization\\nsensors\\nvehicle dynamics\\norbits\\nrobots\\noscillators\\ndynamics\\nactuators\\nmobile radio\\noceanographic techniques\\nsynchronisation\\ntelecommunication control\\nwireless sensor networks\\nshort-range mobile sensors drifting\\ngeophysical flows\\nocean monitoring applications\\nminimal actuation capabilities\\nactive drifters\\ngyre flows\\ndata exchange\\nnonlinear synchronization control strategy\\nrendezvous regions\\nlarge-scale mobile sensor networks\\nnumerical simulations\\nsmall-scale experiments\",\"101\":\"safety\\nrobots\\nenergy storage\\nactuators\\nimpedance\\ntorque\\nelasticity\\nenergy-based safety\\nseries elastic actuation\\ngeneric actuation passivity\\npower flow properties\\npower limits\",\"102\":\"impedance\\ntorque\\ntorque control\\nsprings\\nstability analysis\\nsafety\\nmeasurement\\nactuators\\nelasticity\\nfeedforward\\nobservers\\nhigh-stiffness environment\\ndob approaches\\nfeedforward controller\\ndob controllers\\nmaximum safe stiffness\\ndob torque control\\npassivity conditions\\nload port passivity\\nsafe impedance range\\ntorque tracking performance\\nseries-elastic actuator applications\\ndisturbance observer\\nsafe high impedance control\",\"103\":\"springs\\nenergy storage\\nactuators\\npotential energy\\nstrain\\nforce\\nvalves\\nmathematical analysis\\nrigidity\\nrobot dynamics\\nsprings (mechanical)\\nvariable stiffness actuation technology\\nvariable stiffness springs\\nenergy storage capacity\\nlinear helical springs\\nvariable stiffness actuators\\nhuman performance augmentation\\nspring exoskeleton\\ncontrollable volume air spring\\nmathematical conditions\",\"104\":\"fasteners\\ngrippers\\nrobots\\nshape\\nactuators\\nforce\\nelectronic mail\\nart\\ncontrol system synthesis\\nmotion control\\npaper\\nparallel-motion thick origami structure\\nrobotic design\\nthree-dimensional shapes\\nzero-thickness flat paper sheets\\norigami facets\\nmultiple layer origami structures\\nparallel-motion gripper\",\"105\":\"robots\\ntrajectory\\ntracking\\nfriction\\ncollision avoidance\\nreal-time systems\\nwheels\\nmobile robots\\nmotion control\\ntracked vehicles\\ntrajectory control\\nvelocity control\\nnondeformable continuous tracks\\ngrouser geometry\\nreal-time simulation\\ncircular segments\\nrobot body\\nsegment link\\ntrack rotation\\nrough terrain\\ntrack trajectory\\nvelocity constraints\",\"106\":\"simultaneous localization and mapping\\ncameras\\nmeasurement\\nlaser radar\\nrobot vision systems\\nmobile robots\\npublic domain software\\nrobot vision\\nslam (robots)\\nslam\\nopen source tools\\nrobotic mapping algorithms\\ndarpa subterranean challenge\\nsubt-tunnel dataset\\nsubterranean mine rescue dataset\",\"107\":\"uncertainty\\nplanning\\nrobots\\nmarkov processes\\ntarget tracking\\nprobabilistic logic\\nmeasurement uncertainty\\nmobile robots\\npath planning\\nrobot motion planning\\npath planning method\\ntracking robot\\ndynamic environments\\nrobot path planning\\nvisual occlusions\\nmoving targets\\nvisioning\\nperception algorithms\\npartially observable markov decision\\npursuit-evasion\\nrobot tracking\\npredictive path planning\",\"108\":\"feature extraction\\nfaces\\nface recognition\\nneural networks\\ntraining\\ndimensionality reduction\\nemotion recognition\\nimage classification\\nneural nets\\nnatural scene facial expression recognition\\ndimension reduction network\\nhuman-computer interaction\\nexpression recognition methods\\nnatural scenes\\nexpression classification\\npattern recognition problem\\nintra-class distance\\ninter-class distance\\ngeneralization error\\ndata dimension reduction module\\ngeneral classification network\\nclassification tasks\",\"109\":\"three-dimensional displays\\npose estimation\\ndecoding\\nimage reconstruction\\ntask analysis\\nshape\\nfeature extraction\\nimage capture\\nimage coding\\nimage representation\\nimage sampling\\nlearning (artificial intelligence)\\nneural nets\\nsolid modelling\\nannotated hand-object samples\\nhand-object interaction cases\\naugmented autoencoder\\ndeep learning method\\n3d point cloud\\nlatent representation\\nauxiliary point cloud decoder\\naugmented clean hand data\\nhand pose estimation\",\"110\":\"three-dimensional displays\\ntwo dimensional displays\\ntraining\\ndetectors\\nfeature extraction\\nrobustness\\nsolid modeling\\nimage colour analysis\\nimage fusion\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\nsynthetic training data\\nreal-time detection\\nhuman 3d centroids\\nrgb-d data\\nimage-based detection approach\\nyolo v3 architecture\\n3d centroid loss\\nmid-level feature fusion\\ntransfer learning scheme\\nlarge-scale 2d object detection datasets\\nend-to-end 3d localization\\nprecise 3d groundtruth\\n3d localization accuracy\\nlearning 3d localization\\nyolo-based rgb-d fusion approach\\ndepth-aware crop augmentation\\nintralogistics dataset\",\"111\":\"bonding\\nweight measurement\\nrobot sensing systems\\nforce\\nheart beat\\nresists\\nstress\\nbiomechanics\\nbiomedical equipment\\nbiomedical measurement\\ncardiology\\ncrystal resonators\\nelectrocardiography\\nforce measurement\\nforce sensors\\nmedical signal detection\\nmedical signal processing\\nmicrofabrication\\nmicrosensors\\npatient monitoring\\npneumodynamics\\npressure sensors\\nquartz\\nsensors\\nwide-range load sensor\\nvacuum sealed quartz crystal resonator\\nbiosignal measurement\\nqcr load sensor\\nmeasurement range\\nsensor structure\\nforce sensor\\nqcr load sensing system\",\"112\":\"videos\\ndatabases\\ndetectors\\nfeature extraction\\nobject detection\\naccidents\\nautonomous automobiles\\nimage motion analysis\\nimage representation\\npedestrians\\ntraffic engineering computing\\nrisk-level prediction\\npedestrian near-miss detection\\nrisk-level assignment\\nmotion-representation-by-detection\\npedestrian near-miss dataset\\nsingle-shot multibox detector with motion representation\\nssd-mr\\nmotion-based features extraction\",\"113\":\"visualization\\nsimultaneous localization and mapping\\nrobustness\\nstrain\\nimage recognition\\ntensile stress\\ngraph theory\\nimage matching\\nmobile robots\\nrobot vision\\nslam (robots)\\nrobotics applications\\nsimultaneously localization and mapping\\nspatial relationship similarities\\nspatial cues\\nvisual cues\\nold landmarks\\nlong-term environment changes\\nlandmark information\\nintegrate landmark appearances\\nworst-case graph matching\\nplace recognition performance\\nlong-term place recognition\\nworst appearance similarity\\nsimilar appearances\\nworst-case scenario\\ngraph matching problem\\nvisual appearances\\nangular spatial relationships\\ngraph representation\",\"114\":\"simultaneous localization and mapping\\ncameras\\nthree-dimensional displays\\ntracking\\nkalman filters\\nvisualization\\nrobustness\\nimage colour analysis\\nmobile robots\\nobject detection\\nobject tracking\\npose estimation\\nslam (robots)\\nmanhattan world assumption\\northogonal directions\\natlanta world\\nvertical direction\\nhorizontal directions\\nslam techniques\\natlanta representation\\natlanta frame-aware linear slam framework\\natlanta structure\\nlinear kalman filter\\nlinear rgb-d slam\\ntracking-by-detection scheme\\nscene structure\\ncamera motion\\nplanar map\\nsynthetic datasets\\nreal datasets\",\"115\":\"cameras\\ncalibration\\nestimation\\nvisualization\\nacceleration\\njacobian matrices\\noptimization\\ndistance measurement\\ninertial navigation\\nkalman filters\\nstereo image processing\\nupdate jacobian sub-block\\nfeature reprojection error\\nreal-world outdoor dataset\\neuroc dataset\\ncamera poses\\nimu\\ninertial measurement unit\\nestimation performance\\nstereo-vision devices\\nstereo extrinsic parameters\\nmultistate constraint kalman filter\\nstereo vio extrinsic parameters correction\\nonline calibration method\\ncamera extrinsic parameters\\nstereo visual inertial odometry\\nextrinsic parameter calibration\\nonline baseline calibration\",\"116\":\"feature extraction\\ncameras\\nbundle adjustment\\nlaser radar\\nimage segmentation\\noptimization\\ndistance measurement\\nimage sequences\\nmobile robots\\nmotion estimation\\noptical radar\\npose estimation\\nrobot vision\\nstereo image processing\\nline features\\nlidar-visual odometry\\nline depth extraction\\npoint-line bundle adjustment\\npurely visual motion tracking method\\npublic kitti odometry benchmark\\nlidar-monocular visual odometry approach\\npoint-only based lidar-visual odometry\\nenvironment structure information\\nefficient lidar-monocular visual odometry system\",\"117\":\"semantics\\nsimultaneous localization and mapping\\nrobustness\\noptimization\\nobject detection\\nuncertainty\\ngaussian processes\\nimage sensors\\nmobile robots\\nprobability\\nrobot vision\\nslam (robots)\\ntarget tracking\\nprobabilistic data association\\nmixture models\\nrobust semantic slam\\nrobotic systems\\ncameras\\nlidar\\nvisual models\\nreliable navigation\\nsemantic uncertainty inherent\\ngeometric uncertainty inherent\\nobject detection methods\\ndata association ambiguity\\nnonlinear gaussian formulation\\ndata association variables\\nmax-marginalization\\nstandard gaussian posterior assumptions\\nmax-mixture-type model\\nmultiple data association hypotheses\\nindoor navigation tasks\\noutdoor semantic navigation tasks\\nsemantic slam approaches\\nnoisy odometry\",\"118\":\"visualization\\nnavigation\\nsimultaneous localization and mapping\\nbenchmark testing\\nestimation\\nclosed loop systems\\nglobal positioning system\\nmobile robots\\nrobot vision\\nslam (robots)\\nstereo image processing\\ntracking\\nrepresentative state-of-the-art visual-inertial slam systems\\nvisual estimation module\\nstereo visual-inertial slam systems\\nopen-loop analysis\\nclosed-loop navigation tasks\\naccurate trajectory tracking\\nvisualinertial slam systems\\nclosed-loop benchmarking simulation\\nvisual-inertial estimation\\ntrajectory tracking performance\",\"119\":\"three-dimensional displays\\npicture archiving and communication systems\\nconvolution\\nsemantics\\nimage edge detection\\ntask analysis\\ndecoding\\ncodecs\\nconvolutional neural nets\\nedge detection\\ngraph theory\\nimage coding\\nimage filtering\\nimage representation\\nimage sampling\\nlearning (artificial intelligence)\\nstereo image processing\\nvisual perception\\ndeep hierarchical encoder-decoder\\nunorganized 3d points\\nmultiscale contextual information\\nimage analysis\\npointatrousgraph\\npag\\nmultiscale edge features\\npoint clouds\\npoint atrous convolution\\npac\\nedge-preserved unpooling\\nmultiscale point features\\nnonoverlapping maxpooling operations\\ncritical edge features\\neu modules\\ndeep permutation-invariant hierarchical encoder-decoder\\nedge preserved pooling\\nchained skip subsampling-upsampling modules\\n3d semantic perception applications\",\"120\":\"simultaneous localization and mapping\\nresistance\\nuncertainty\\ncomputational modeling\\ncomputer architecture\\npredictive models\\ncameras\\nautonomous aerial vehicles\\ngraph theory\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nresistance distance\\ncovisibility graph\\nsimulated uav coverage path\\nuncertainty models\\nmonocular graph slam\\ntopological features\\nerror model learning\\nuav coverage path planning trajectories\",\"121\":\"decoding\\ncomputational modeling\\nvisualization\\nmagnetic heads\\nrobots\\nnatural languages\\nencoding\\nhuman-robot interaction\\nnatural language processing\\nrobot vision\\nvideo signal processing\\nvideo captioning\\ncomputational requirements\\nfully-attentive captioning algorithm\\nlanguage generation\\ntransformer layers\\ndecoding stages\\nimage regions\\ncaption quality\\nautonomous agents\\ndomestic robots\\nsmart\\nrobotic explainability\\nnatural language explanations\\nvisual perception\\nshallow memory-aware transformer training\\nmemory-aware encoding\\nimage captioning\",\"122\":\"robot sensing systems\\nthree-dimensional displays\\ncalibration\\nneural networks\\naugmented reality\\nrobot kinematics\\ncameras\\ncontrol engineering computing\\nimage sensors\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\npublic domain software\\nrobot vision\\nsolid modelling\\nrobotic environments\\ndepth camera\\ndeep learning-based calibration\\nopen source 3d point cloud labeling tool\\nhead mounted augmented reality device\\n3d depth sensor data\\nmicrosoft hololens\\nneural network\\nvotenet architecture\\n3d-deep-learning-based augmented reality calibration\",\"123\":\"task analysis\\ntraining\\nvisualization\\nfeature extraction\\nrobots\\ntrajectory\\nclutter\\nlearning (artificial intelligence)\\nrobot vision\\nrobotic policy training\\nlarge-scale data collection\\ntask-setup\\ntask-irrelevant objects\\ninteractive samples\\nadversarial training\\ndeep rl capabilities\\ntransfer learning\\nrobotic tasks\\nadversarial feature training\\ngeneralizable robotic visuomotor control\\ndeep reinforcement learning\\naction-selection policies\\nimage pixels mapping\\nvisuomotor robotic policy training\",\"124\":\"task analysis\\nlearning (artificial intelligence)\\nneural networks\\nforce\\ngeometry\\nend effectors\\ncontrol engineering computing\\nmanipulators\\nparameterizations\\nsparse-reward tasks\\nrobotic bimanual manipulation tasks\\nparameterized skills\\nstate-independent task schema\\nmodel-free reinforcement learning\\nrobotic systems\",\"125\":\"buildings\\nimage reconstruction\\nthree-dimensional displays\\npath planning\\ndrones\\ncameras\\nlayout\\nautonomous aerial vehicles\\ncomputational geometry\\nrobot vision\\nslam (robots)\\nunmanned aerial vehicles\\nlarge-scale scene mapping\\nautonomous urban scene reconstruction\\npoint cloud reconstruction\\nreconstruction quality\\nlarge-scale scene reconstruction\\nreal-time uav path planning\\nslam\",\"126\":\"planning\\nsafety\\nconvergence\\nalgorithms\\nrobots\\ndata structures\\ncollision avoidance\\ngradient methods\\ngraph theory\\nmobile robots\\npath planning\\nsampling methods\\nconvergence speed\\nsafety certificate algorithms\\nfast marching gradient sampling strategy\\nsampling-based motion planning algorithms\\nmarching seed\\ngoal set\\ninformed certificate set\\nplanning space\\nrrt* algorithms\",\"127\":\"privacy\\nplanning\\ncameras\\nsensors\\ntrajectory\\nsafety\\nunmanned aerial vehicles\\naircraft control\\nautonomous aerial vehicles\\ncollision avoidance\\ndata privacy\\ndecision making\\nmobile robots\\nmotion control\\nprivacy-aware uav flights\\nunmanned aerial vehicle\\nuncertain obstacles\\nmotion planning algorithms\\nprivacy-preserving requirements\\nprivacy risk aware motion planning method\\nprivacy-sensitive sensor\\nenergy hard constraints\\ndynamically detected restricted areas\\ndecision making method\\ntest flights\\ndji matrice 100 uav\\nself-configuring motion planning\",\"128\":\"collision avoidance\\nrobot kinematics\\nkinematics\\nmanipulators\\npath planning\\nplanning\\ntrees (mathematics)\\ngeneralized bur captures large portions\\nfree c-space\\naccelerated exploration\\nexact collision-free paths\\nimproved c-space exploration\\nrobotic manipulators\\ndistance information\\ngeometrical structure\\nstar-like tree\\narbitrary number\\nguaranteed collision-free edges\\nsimple forward kinematics\\nrrt-like planning algorithm\\ngeneralized burs\",\"129\":\"robots\\ntask analysis\\ntrajectory optimization\\ntuning\\ncomputational modeling\\nhumanoid robots\\nmanipulators\\nmobile robots\\ntuning-free contact-implicit trajectory optimization\\ncontact-implicit trajectory optimization framework\\ncontact-interaction trajectories\\nrobot architectures\\ntrivial initial guess\\nparameter tuning\\nrelaxed contact model\\nautomatic penalty adjustment loop\\ncontact information\\nmobile robot\\nnonprehensile manipulation\\n7-dof arm\\nplanar locomotion\",\"130\":\"splines (mathematics)\\nlinear programming\\ntrajectory optimization\\nrobustness\\nsafety\\nautonomous aerial vehicles\\ngradient methods\\nhelicopters\\nmobile robots\\noptimisation\\npath planning\\nsearch problems\\ntrajectory control\\nquadrotor trajectory replanning\\nreplanning method\\ngto\\npath-guided optimization approach\\ntopological path searching algorithm\\nindependent trajectory optimization\\noutput superior replanned trajectories\\ngradient-based trajectory optimization\\nuav replanning\",\"131\":\"robot sensing systems\\npath planning\\ntraining\\ntraining data\\nplanning\\nrobot kinematics\\nautonomous aerial vehicles\\ngraph theory\\nlearning by example\\nmobile robots\\noptical radar\\nrobot programming\\nsampled data systems\\ntunnels\\nautonomous exploration\\nsubterranean environments\\naerial robots\\ntraining expert\\nimitation learning\\nunderground mine drifts\\ngraph based path planner\\nlearning based path planning\\nlidar\\nrange data sampling\",\"132\":\"three-dimensional displays\\nmanipulators\\nvisualization\\ncameras\\ntask analysis\\nsensors\\naerospace robotics\\ndistance measurement\\nfeedback\\ngrippers\\nhaptic interfaces\\nhuman-robot interaction\\nobject tracking\\nrobot vision\\nsensor fusion\\ntelerobotics\\nvirtual reality\\nvisual-inertial telepresence\\nhaptic device\\n3d visual feedback\\ninertial sensors\\nobject tracking algorithm\\nmarker tracking algorithm\\nvisual-inertial odometry\\naerial manipulation\\nremotely located teleoperator\\nonboard visual sensors\\nhuman in the loop\",\"133\":\"rotors\\nvibrations\\nmathematical model\\nmanipulators\\ncontrollability\\ntorque\\nautonomous aerial vehicles\\ncontrol engineering computing\\nhelicopters\\nmechanical engineering computing\\nmobile robots\\noptimisation\\npath planning\\nvibration control\\nrvm design\\noptimal placement\\nflexible object transport\\nmanipulated object\\nobject size\\nquadrotor usage\\ndistributed rvms\\nconstrained optimization problem\\naerial-ground manipulator system\\nrobot-based vibration suppression module\\ndistributed rotor-based vibration suppression\\ncontrollability gramian\\nmultiple aerial-ground manipulator system\",\"134\":\"manipulators\\nvehicle dynamics\\nmathematical model\\ndynamics\\ntrajectory\\nservomotors\\naerospace robotics\\ncollision avoidance\\ncontrol system synthesis\\ndynamic programming\\nobservers\\nposition control\\npredictive control\\nrobust control\\nthree-term control\\nmodel predictive control\\nhinged door\\nenvironment interaction\\naerial robot\\nmultirotor-based aerial manipulator\\ndaily-life moving structure\\ndifferential dynamic programming\\ndisturbance observer\\nrobust controller\",\"135\":\"drones\\ntrajectory\\nsafety\\noptimization\\nmeasurement\\nshape\\nreal-time systems\\nautonomous aerial vehicles\\ncollision avoidance\\ngraph theory\\nmobile robots\\nobject detection\\nquadratic programming\\nvideo recording\\ndense environment\\ndrone\\nautonomous videography task\\n3-d obstacle environment\\nmoving object\\ntarget motion prediction module\\nhierarchical chasing planner\\ncovariant optimization\\nbi-level structure\\nsmooth planner\\ngraph-search method\\nchasing corridor\\nsubsequent phase\\nsmooth trajectory\\ndynamically feasible trajectory\\nintegrated motion planner\\nreal-time aerial videography\\nsource code\",\"136\":\"merging\\nautomobiles\\nestimation\\nautonomous vehicles\\nprobabilistic logic\\nmathematical model\\nroads\\ngaussian processes\\ngraph theory\\nmobile robots\\nprobability\\nroad traffic control\\nroad vehicles\\ntraffic engineering computing\\nautonomous driving vehicles\\nautonomous driving cars\\nfactor graph\\nhuman-designed models\\nfg-gmm-based interactive behavior estimation\\nramp merging control\\nsignificant social interaction\\nprobabilistic graphical model merging control model\",\"137\":\"sensor systems\\ntime measurement\\nroads\\nfuses\\ncurrent measurement\\nbandwidth\\ncooperative systems\\nkalman filters\\nlocation based services\\nmobile robots\\nmulti-robot systems\\nnonlinear filters\\nroad vehicles\\nsensor fusion\\nvehicle sensors\\nextended kalman filters\\nfully autonomous road vehicles\\ncooperative driving\\ncooperative perception\\nhigh fidelity sensors\\nlow fidelity sensors\\nlocalization information\",\"138\":\"predictive models\\nnavigation\\ncameras\\nplanning\\ncomputational modeling\\nvisualization\\nrobots\\ncollision avoidance\\nmobile robots\\noff-road vehicles\\nrobot programming\\nrobot vision\\nsupervised learning\\nautonomous driving\\nvision based controllers\\nnavigation learning\\nmodel robustmess\\nplanning foresight\\nself supervised method\\nsparse aerial images\\noff road driving\\nsmooth terrain traversal\\nvisual obstructions\\non-board sensors\\nterrain roughness\\nmodel free reinforcement learning\\nunstructured outdoor environments\\non-board camera\\nrough terrain\",\"139\":\"tracking\\npredictive models\\nroads\\nvideos\\ncollision avoidance\\nautomobiles\\nimage motion analysis\\nobject detection\\nobject tracking\\nroad traffic\\ntraffic engineering computing\\nvideo signal processing\\ntraffic videos\\nroad agents\\ndense environments\\nheterogeneous environments\\nroad-track\\ntracking-by-detection approach\\nbounding box region\\nsimultaneous collision avoidance and interaction model\",\"140\":\"receivers\\nnoise measurement\\nposition measurement\\noptimization\\nacoustic measurements\\ntarget tracking\\nacoustics\\nradar tracking\\ntime-of-arrival estimation\\nspatially distributed receivers\\nstatic measurements\\nmultitarget trackers\\nassociation-free multilateration\\ntimes of arrival estimation\\nuncorrelated measurement\\ninitialization routine\",\"141\":\"feature extraction\\ntraining\\nimage reconstruction\\nimage recognition\\nrobustness\\nmachine learning\\nneural networks\\nimage matching\\nimage sequences\\nmobile robots\\nneural nets\\nrobot vision\\nsupervised learning\\nadversarial feature disentanglement\\nseasonal variation\\nvisual place recognition\\nimage descriptors\\nadversarial network\\ndomain related features\\nself supervised manner\",\"142\":\"three-dimensional displays\\npose estimation\\ncost function\\napproximation algorithms\\nreal-time systems\\niterative closest point algorithm\\nsimultaneous localization and mapping\\napproximation theory\\ncomputational geometry\\ncomputer vision\\nconvex programming\\nleast squares approximations\\nminimisation\\npoint-to-plane correspondences\\nleast-squares problem\\nglobal minimizer\\nreal-time applications\\nlocal minimizer\\ncayley-gibbs-rodriguez parameterization\\nfirst-order optimality conditions\\n3d correspondences\\ncgr parameterization\",\"143\":\"feature extraction\\ncameras\\nrobots\\nlatches\\ntask analysis\\nasphalt\\npose estimation\\nimage matching\\nimage texture\\nrobot vision\\nground texture based localization\\ncompact binary descriptors\\nglobal localization\\nsubsequent local localization updates\\ncompact binary feature descriptors\\nlocalization success rates\\nself-contained method\\nmatching strategy\\nidentity matching\",\"144\":\"feature extraction\\nreliability\\ndata mining\\nnoise measurement\\nstandards\\nvisualization\\nobject recognition\\noptical radar\\nroad vehicles\\nsensor fusion\\nfeature-based vehicle localization\\ndata association\\nlocal environment\\nplausible feature associations\\nsafe localization\\nlocalization features\\ngeometric hashing methods\\nerror propagation\\ncylindrical objects\\nlidar data\",\"145\":\"robots\\ntask analysis\\ntrajectory\\ncontext modeling\\nencoding\\nadaptation models\\nlearning (artificial intelligence)\\nhuman-robot interaction\\nservice robots\\nubiquitous computing\\ndemonstrated motion\\nlearned policy\\nperceived behaviour\\ncontext-aware task execution\\napprenticeship learning\\nassistive service robots\\nhuman-oriented tasks\\ntask parameters\\noptimal behaviour\\nrobot-to-human object hand-over\\nreinforcement learning\\ndemonstrator\\ncontextualized variants\\ndemonstrated action\\ndynamic movement primitives\\ncompact motion representations\\nmodel-based c-reps algorithm\\nhand-over position\\ncontext variables\\nsimulated task executions\\nevaluating emergent behaviours\\ncontext-aware action generalization\",\"146\":\"task analysis\\ncurrent measurement\\nrobot sensing systems\\nservice robots\\nstability analysis\\nmeasurement uncertainty\\nadaptive control\\nhierarchical systems\\nlearning (artificial intelligence)\\nlearning systems\\nmanipulators\\nneurocontrollers\\nradial basis function networks\\nstability\\nhierarchical interest-driven goal babbling\\nbootstrapping\\nsensorimotor skills\\ntime-dependent changes\\nintrinsic motivation signal\\nonline associative radial basis function network\\nassociative dynamic network\\nparameter-sharing technique\\nexhaustive parameter tuning\\nlearning process\\nphysical robot manipulator\\ndata-driven robot model learning\",\"147\":\"robot kinematics\\nmanipulators\\nimage segmentation\\nrobot sensing systems\\nobject segmentation\\ntraining\\ncomputer vision\\nlearning (artificial intelligence)\\nobject detection\\nrobot manipulator\\nhuman supervision\\nforeground segmentation technique\\ngrasped object\\nstate-of-the-art adaptable in-hand object segmentation\\nsegmentation performance\\nrobot-supervised\\nunstructured changing environments\\ndeep learning\\nhuman annotators\\nlearning-based segmentation methods\\nrobotics applications\\nannotated training data\\npixelwise segmentation\",\"148\":\"image segmentation\\nsemantics\\ntask analysis\\nagriculture\\nrobots\\ntraining\\nsugar industry\\nconvolutional neural nets\\nentropy\\nimage sampling\\nindustrial robots\\nlearning (artificial intelligence)\\ncrop\\nweed\\nagricultural robots\\nannotated datasets\\nsupervised learning\\ntedious time-intensive task\\nactive learning\\nimage data\\nexisting semantic segmentation cnn\\ngrowth stage\\nrough foreground segmentation\\nsubstantially different field\\nchallenging datasets\\nagricultural robotics domain\\nentropy based sampling\\nhuman labeling effort\",\"149\":\"trajectory\\ndatabases\\noptimal control\\ntask analysis\\nlegged locomotion\\nground penetrating radar\\nhumanoid robots\\niterative methods\\nlearning (artificial intelligence)\\nmotion control\\npath planning\\nregression analysis\\ntrajectory control\\noptimal control solver\\nlocomotion task\\nhumanoid robot\\nhpp loco3d\\nversatile locomotion planner\\nwhole-body trajectory\\nregression problem\\nsingle-step motion\\nmultistep motion\\npredicted motion\\ncrocoddyl control solver\",\"150\":\"feedback linearization\\nconferences\\nautomation\\nuncertain systems\\nlearning (artificial intelligence)\\ncontrol design\\nnonlinear systems\\napproximation theory\\ncontinuous time systems\\ncontrol system synthesis\\nfeedback\\nfunction approximation\\nlinearisation techniques\\nnonlinear control systems\\noptimisation\\nmodel-free policy optimization techniques\\nnonlinear control\\nnonlinear plant\\nfeedback controller\\nlinear control techniques\\nexact linearizing controllers\\nlearned linearizing controller\\nmodel-free policy optimization algorithms\",\"151\":\"task analysis\\ntraining\\nkinematics\\nestimation\\nneedles\\ngesture recognition\\nsurgery\\nfeature extraction\\nimage segmentation\\nmedical robotics\\nrecurrent neural nets\\nrobot dynamics\\nrobot kinematics\\nmultitask recurrent neural network\\nsurgical gesture recognition\\nsurgical data science\\ncomputer-aided intervention\\nrobotic kinematic information\\nrobot kinematic data\",\"152\":\"robots\\ndynamics\\nforce\\nestimation\\ntraining\\nbiological neural networks\\nforce control\\nforce sensors\\nmean square error methods\\nmedical robotics\\nneurocontrollers\\nrobot dynamics\\nsurgical robotic systems\\ninternal joint torques\\nrobot inverse dynamics\\nda vinci surgical robot\\nenvironment forces\\nmodel-based approaches\\nexternal force sensor\\nexternal force estimation\\nda vinci research kit\\nneural network based inverse dynamics identification\\nnormalized rootmean-square error\\nnrmse\\ntool\\/tissue interaction forces\",\"153\":\"legged locomotion\\ndynamics\\nfoot\\ntrajectory optimization\\nlearning (artificial intelligence)\\nmotion control\\nnavigation\\nnonlinear programming\\npath planning\\npredictive control\\nreliability\\nrobot dynamics\\nrobust control\\ntrajectory control\\ndynamic quadrupeds\\ndynamic traversal\\nlegged robotics\\nrobust dynamic motion\\nuneven terrain navigation\\ntowr\\nlearning based scheme\\nwhole body tracking controller\\ntrajectory optimization for walking robots\\nmodel predictive control\\ndynamic trajectory reliability\\nnonlinear program\\ndynamic motions\",\"154\":\"legged locomotion\\nfoot\\ncollision avoidance\\nforce\\ntrajectory\\naerodynamics\\nhydraulic actuators\\nmotion control\\npath planning\\nposition control\\nrobot dynamics\\nhyq robot\\nhydraulically actuated quadruped robot\\nsimplified nonlinear nonconvex trajectory optimization\\nsingle rigid body dynamics-based trajectory optimizer\\nleg collision\\nleg model\\njoint positions\\nadmissible contact forces\\njoint-torque limits\\nchallenging terrain\\nrobust motions\\nfeasibility constraints\\nmotion planning\\ncomputational efficiency\\nsimplified dynamics\\nnonlinear trajectory optimization\\nhardware feasibility\",\"155\":\"robot kinematics\\nmobile robots\\nplanning\\nwheels\\nnavigation\\ncollision avoidance\\nlegged locomotion\\nmotion control\\npath planning\\nsearch problems\\ncentauro robot\\nagile legged wheeled reconfigurable navigation planner\\nhybrid legged-wheeled robots\\ntheta* based planner\\ntrapezium-like search\",\"156\":\"impedance\\naerospace electronics\\nrobot kinematics\\nhaptic interfaces\\ntorso\\nforce\\ncontrol engineering computing\\nforce feedback\\nlegged locomotion\\nmechanical engineering computing\\nmotion control\\nposition control\\nquadratic programming\\nrobot dynamics\\ntelerobotics\\nquadruped robot anymal\\nbounded haptic teleoperation\\ncontrol framework\\noperator-guided haptic exploration\\nfoot posture control\\nwhole-body controller\\nanalytical cartesian impedance controllers\\nnull space projector\\ncontact forces\\nforce-feedback\\n7d haptic joystick\",\"157\":\"conferences\\nautomation\\ndesign engineering\\nlegged locomotion\\nmotion control\\nrobot dynamics\\nlegged robots\\npassive adaptability\\nlocking pin arrays\\npin array mechanisms\\nwalking robot\\nlegged robot design\\nrough terrain locomotion\\nunstructured terrains\\nrough terrains\\nstable locomotion\",\"158\":\"legged locomotion\\nsprings\\ndynamics\\nfoot\\nrobustness\\nacceleration\\nmotion control\\noptimisation\\nrobot dynamics\\nground uncertainty\\nactuation plans\\ndynamic model\\nbipedal running\\nfixed body trajectory\\npassive dynamics\\nreduced order model\\nemergent robustness\\nlegged robots\\nlinked inputs\\ninput linking\\nhybrid dynamics\\nrunning model\\noptimization procedure\\nstandard trajectory optimization\\nrobust gaits\",\"159\":\"two dimensional displays\\nthree-dimensional displays\\ntraining\\nrobots\\npath planning\\nprediction algorithms\\nplanning\\nconvolutional neural nets\\niterative methods\\nmobile robots\\nneurocontrollers\\nrobot action execution\\nmotion trajectory\\nfully convolutional neural network\\nnetwork prediction iteration\\noptimal paths\\nsingle path predictions\\nsimultaneously generated paths\\nshot multipath planning\\nrobotic applications\",\"160\":\"games\\nheuristic algorithms\\napproximation algorithms\\noptimal control\\niterative methods\\ntrajectory\\nautomobiles\\napproximation theory\\ndecision making\\ndifferential games\\nlinear quadratic control\\nmulti-agent systems\\nmulti-robot systems\\nnonlinear control systems\\niterative linear-quadratic regulator\\nlinear dynamics\\nquadratic costs\\nlinear-quadratic games\\ncomplex interactive behavior\\nefficient iterative linear-quadratic approximations\\nnonlinear multiplayer general-sum differential games\\nrobotics\\nmultiple decision making agents\\nexpressive theoretical framework\\nmultiagent problems\\nnumerical solution techniques\\nstate dimension\\nsingle agent optimal control problem\\nilqr\\nrepeated approximations\\nthree-player 14-state simulated intersection problem\\nhardware collision-avoidance test\\ntime 0.25 s\\ntime 50.0 ms\",\"161\":\"robots\\nsolid modeling\\nfriction\\nquaternions\\nplanning\\nacceleration\\ntrajectory\\nmobile robots\\npredictive control\\nrobot dynamics\\nmodel predictive control\\npath-following tasks\\ndynamically unstable mobile robots\\nsingle ball\\nsimplied version\\nphysical ballbot system\\nhigh fidelity model\\nonline implementation\\nquaternion-based model\",\"162\":\"linear programming\\ntask analysis\\ntrajectory optimization\\npainting\\nphotography\\ncontrol system synthesis\\nnonlinear control systems\\noptimisation\\npendulums\\ntrajectory control\\nunderactuated waypoint trajectory optimization\\nlight painting photography\\ncontrol engineering\\nauxiliary optimization variables\\nwaypoint activations\\nletter drawing task\\nlong exposure photography\\nr control engineering\",\"163\":\"legged locomotion\\nfoot\\nforce\\ntrajectory\\nrobot kinematics\\nmathematical model\\nhumanoid robots\\noptimal control\\noptimisation\\nrobot dynamics\\ntrajectory control\\nhumanoid robot model\\nwalking surface\\ncontact parametrization\\ncomplementarity-free\\npredefined contact sequence\\nwalking trajectories\\ndynamic equations\\noptimization problem\\ndirect multiple shooting approach\\nbody walking generation\\nnonlinear trajectory optimization\\ncentroidal dynamics\\nhumanoid robot kinematics\\nhumanoid robot dynamics\",\"164\":\"aerodynamics\\nmathematical model\\nacceleration\\nrobot kinematics\\nstability analysis\\nadaptive control\\nhumanoid robots\\nlegged locomotion\\nmotion control\\nnonlinear dynamical systems\\noptimal control\\npath planning\\nquadratic programming\\nrobot dynamics\\nrobust control\\nsplines (mathematics)\\nstability criteria\\ntrajectory control\\ntask-space inverse dynamics controller\\noptimal 7th order spline coefficients\\ndynamic stability\\ntsid controller\\nsimplified passive dynamics model\\naerobot platform\\nfast height adaptation\\nactively articulated wheeled humanoid robot\\ncenter of mass trajectory\\nhybrid wheel-legged robots\\ncomplex terrain\\npurely wheeled morphologies\\nhighly adaptive behaviours\\nnonlinear dynamics control problem\\nhybrid humanoid platform\\noffline trajectory optimisation\\noptimal center of mass kinematic trajectories\\nfast height variation control\\nnonlinear zero moment point\\nsequential quadratic programming\\nstability criterion\\nmotion plan\\ntask jacobians\",\"165\":\"lyapunov methods\\ncontrol systems\\nforce\\ndynamics\\ntask analysis\\ntactile sensors\\ncontrol system synthesis\\nmobile robots\\nmotion control\\nmulti-robot systems\\noptimisation\\nrobot dynamics\\nrobust control\\nmulticontact motion\\ncombinatoric structure\\nreal-time control\\ncomplementarity structure\\ncontact dynamics\\ncontrol framework\\nmulticontact robotics problems\\ncontact-aware controller design\\nrobotic tasks\\nlocomotion\",\"166\":\"grasping\\nthree-dimensional displays\\nrobot kinematics\\nplanning\\nmeasurement\\ndata models\\nconvolutional neural nets\\nlearning systems\\nmanipulators\\nneurocontrollers\\nstability\\n3d cnn\\n6-dof grasp poses\\nreachability awareness\\nvoxel-based deep 3d convolutional neural network\\nreachability predictor\\nrobot\\ngrasp pose stability\\ndeep learning in robotics and automation\\nperception for grasping and manipulation\",\"167\":\"grippers\\nthree-dimensional displays\\nellipsoids\\ngrasping\\nmeasurement\\nshape\\nplanning\\ncomputational geometry\\ndexterous manipulators\\npath planning\\npose estimation\\nposition control\\nrobot vision\\ngrasp pose computation\\ngripper workspace spheres\\nregistered point cloud\\ngripper position sampling\\norientation sampling\\nobject orientation estimation\\njaw gripper\\nfranka panda gripper\\ngeometric based methods\\nmultifingered hands\\nintel realsense-d435 depth camera\\nmanipulation\",\"168\":\"measurement\\nforce\\ntask analysis\\nstrain\\ncomputational modeling\\nfriction\\ngrippers\\ndeformation\\nelasticity\\nlinear programming\\nrobotiq gripper\\nur5 robot\\nobject empirical stiffness\\nlinear program\\nmanipulation task\\nwrench resistance\\nphysical grasps\\nwork quality metric\\nwrench-based quality metrics\\nreal-world grasps\\ngripper jaw displacements\\ngrasp force\\nexternal wrench\\nobject deformation\\nrobot grasping\\ndeformable hollow objects\\ngrasp quality metric\",\"169\":\"grasping\\nthree-dimensional displays\\ngrippers\\nservice robots\\nmanipulators\\ngeometry\\nconvolutional neural nets\\nentropy\\nhierarchical systems\\niterative methods\\nlearning systems\\nneurocontrollers\\noptimisation\\nposition control\\ncluttered objects\\ncross entropy method\\niterative direction optimization\\nderivative-free optimization\\ngeometry-based prior\\npoint clouds\\ninput grasp representations\\nrobot arm\\ndetection problem\\nhierarchical approach\\nrobot grasping\\nhierarchical 6-dof grasping\\nsurface normal directions\\napproaching direction selection method\\ngrasp quality\\nfully convolutional grasp quality network\",\"170\":\"robots\\ngravity\\nsearch problems\\nsecurity\\nlayout\\nsafety\\nair pollution\\ndexterous manipulators\\ngrippers\\ncontact space formulation\\ntwo-finger basket grasps\\nhigh-dimensional configuration space\\nlow-dimensional contact space\\ntwo-finger contacts\\nobject boundary\\ncritical finger opening\\ntwo-finger robot hand\\ngeometric techniques\\ndepth and drop-off finger opening\",\"171\":\"acoustics\\ndirection-of-arrival estimation\\narray signal processing\\nthree-dimensional displays\\nmicrophone arrays\\nposition measurement\\nacoustic generators\\nacoustic radiators\\nacoustic wave propagation\\nbackpropagation\\nmonte carlo methods\\nray tracing\\nsignal processing\\ntransient response\\nground-truth sound source position\\nrobust sound source localization\\nbackpropagation signals\\ndirect reflection acoustic ray paths\\nbackward sound propagation path estimation\\nimpulse response\\nmonte carlo localization method\\nsize 3.0 m\\nnoise figure 77.0 db\\nnoise figure 67.0 db\",\"172\":\"microphones\\nvisualization\\near\\nchirp\\ncameras\\nthree-dimensional displays\\ntraining\\nacoustic signal processing\\naudio signal processing\\nbioacoustics\\nimage colour analysis\\nimage sensors\\nmechanoception\\nmobile robots\\nobject detection\\npath planning\\nrobot vision\\nstereo image processing\\nvisual perception\\nmachine vision\\n3d spatial layout\\nears\\nnonvisual perception\\nartificial systems\\nultrasound complement camera-based vision\\ninformation gain\\nharness sound\\nmachine perception\\nlow- cost batvision system\\nshort chirps\\nartificial human pinnae pair\\nstereo camera\\ncolor images\\nscene depths\\ntrained batvision\\n2d visual scenes\\nvision system\\nrobot navigation\",\"173\":\"visualization\\nvideos\\nfeature extraction\\nobject detection\\nspectrogram\\ntraining\\nrobots\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nsource separation\\nscene understanding\\nsound source separation task\\nself-supervised learning framework\\nsound separation modules\\nsound components\\nvisual information\\naudio information\",\"174\":\"cameras\\nsemantics\\nvehicle dynamics\\nmotorcycles\\nimage segmentation\\nvirtual environments\\nroads\\nobject detection\\nstereo image processing\\ntraffic engineering computing\\nomnidirectional images\\nsemantic segmentation\\ndepth map\\nground truth images\\ncarla simulator\\nopen-source simulator\\ncatadioptric images\\nomniscape dataset\\nautonomous driving research\\ngrand theft auto v\\ntwo-wheeled vehicles\\nmotorcycle\",\"175\":\"robot sensing systems\\nelectrodes\\nconductivity\\nimage reconstruction\\ninverse problems\\nbiomedical electrodes\\ncarbon nanotubes\\nmanipulators\\nneural nets\\ntactile sensors\\ntomography\\ncylindrical surface\\nsensor output images\\n3d-shaped sensors\\nert-based robotic skin\\nsparsely distributed electrodes\\ndnn-based signal processing\\nelectrical resistance tomography\\nlarge-scale tactile sensor\\nconductivity distribution\\nphysical model\\ncurved surface\\nelectrode configuration\\nedge region\\nsensor performance\\ncarbon nanotube-dispersed solution\\nconductive sensing domain\",\"176\":\"robot sensing systems\\ntemperature sensors\\nforce\\nforce sensors\\noptical fibers\\nbiological tissues\\nbiomedical optical imaging\\nbragg gratings\\nfibre optic sensors\\nforce measurement\\nmedical image processing\\nsupport vector machines\\nfbg-based triaxial force sensor integrated\\neccentrically configured imaging probe\\nendoluminal optical biopsy\\nendoluminal intervention\\nlesion\\nrobotic bronchoscopy\\nfbg sensors\\nconical substrate\\neccentric inner lumen\\nflexible imaging probe\\nlaser-profiled continuum robot\\nsensor substrate\\ndeveloped triaxial force sensor\",\"177\":\"fabrics\\ncomputational modeling\\ntactile sensors\\nmathematical model\\nelectrodes\\nforce\\nconductivity\\ncalibration\\ninverse problems\\nlearning (artificial intelligence)\\nneural nets\\nrobots\\ntomography\\nsoft ert-based tactile sensor\\nsim-to-real transfer learning\\nelectrical resistance tomography\\nfinite element multiphysics model\\ncontact pressure distributions\\nvoltage measurements\\nmodel parameters\\nsingle-point dataset\\ncontact force\\ncalibration method\\nert-based tactile sensors\",\"178\":\"robot sensing systems\\npins\\nforce\\nstrain\\ndata models\\ncameras\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\ntactile sensors\\nsim-to-real transfer methods\\ntactip optical tactile sensor\\ndeformable tip\\nsoft body simulation\\nunity physics engine\\ndomain randomisation techniques\\nreal-world data\\noptical tactile sensing\\ndeep learning\\nreinforcement learning methods\\nflexible robot controllers\\ncomplex robot controllers\\ntraining data\\ndata collection\\nsize 1.0 mm\",\"179\":\"force\\nprobes\\ndeformable models\\ndata models\\nrobot sensing systems\\nstrain\\nelastic deformation\\nimage representation\\nrobot vision\\nelastically deformable objects\\ndata-driven models\\npoint-based surface representation\\ninhomogeneous force response model\\nnonlinear force response model\\nrobotic arm\\narbitrary rigid object\\nhertzian contact model\\nheterogeneous elastic objects\\nsemiempirical method\\npoint stiffness models\",\"180\":\"robot sensing systems\\nforce\\nthree-dimensional displays\\nwebcams\\nsensitivity\\nprototypes\\nforce sensors\\nstrain gauges\\n6-axis force-torque sensor\\nsix-axis force-torque sensors\\nhard-touse\\nfiducial-based design\\ninexpensive webcam\\nconsumer-grade 3d printer\\nopen-source software\\napplied force-torque\\nbrowser-based interface\\nopen source design files\\nhuman-computer interfaces\\nsix-axis force-torque sensing\\ntraditional strain-gauge based sensors\\nopen-source sensor design\",\"181\":\"conferences\\nautomation\\nreliability\\nmotion estimation\\ncameras\\nrobot vision systems\\ndistance measurement\\nmobile robots\\noptimisation\\npath planning\\nrobot vision\\nframe-to-frame visual odometry\\nvehicle-mounted surround-view camera system\\nreliable frame-to-frame motion estimation\\nvehicle-mounted surround-view camera systems\\nsurround-view multicamera system\\nautonomous driving\\n3d point related optimization variables\\ntwo-view optimization scheme\\nnonholonomic characteristics\\nrelative displacements\\nnonholonomic vehicle motion\\noverly simplified assumptions\\nsingle camera\\nexisting camera\\nrelative vehicle displacement\",\"182\":\"planning\\nimage edge detection\\nnavigation\\nrobot sensing systems\\nbuildings\\nrobustness\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\npath planning\\nrobot vision\\nsensors\\nslam (robots)\\nheuristic priors\\nintelligent planning\\nmonocular slam\\nlow texture\\nhighly cluttered environments\\nrobust sparse map representation\\nmonocular vision\\nlearned sensor\\nhigh-level structure\\nsparse vertices\\nknown free space\\nmapping technique\\nsubgoal planning applications\\nenabling topological planning\\ntopological strategies\\npossible actions\",\"183\":\"pose estimation\\nimage edge detection\\nneural networks\\nrobots\\nlighting\\nsnow\\ndistance measurement\\nimage colour analysis\\nmobile robots\\nneurocontrollers\\npath planning\\nrobot vision\\nrobust control\\nstereo image processing\\noutdoor driving\\ndeep neural network\\nvisual odometry\\nvision-based path following\\nunstructured outdoor environments\\nvisual multiexperience localization\\ncolour-constant imaging\\nmultiexperience vt&r\\ndeepmel\\nstereo visual teach and repeat\\nrobust long-range path following\\nenvironmental conditions\\npose estimates\\nin-the-loop path following\",\"184\":\"robots\\nnavigation\\nvisualization\\ntask analysis\\ntraining\\ncollision avoidance\\nturning\\nlearning (artificial intelligence)\\nmobile robots\\nneurocontrollers\\nrobot vision\\nrobust control\\nsnapnav\\nmapless visual navigation\\nsparse directional guidance\\nvisual reference\\nrobotics\\ndeep neural network\\nvisual navigation system\\ntwo-level hierarchy\\ndirectional commands\\nreal-time control\\nobstacle avoidance\\nautonomous navigation\\nlearning-based visual navigation\",\"185\":\"three-dimensional displays\\nsimultaneous localization and mapping\\nrobustness\\nsemantics\\nlibraries\\nvisualization\\nreal-time systems\\nc++ language\\ncontrol engineering computing\\ngraph theory\\nimage reconstruction\\nimage segmentation\\nlearning (artificial intelligence)\\npublic domain software\\nrobot vision\\nslam (robots)\\nopen-source c++ library\\nvisual-inertial slam libraries\\norb-slam\\nvins-mono\\nsemantic labeling\\nvisual-inertial odometry module\\nstate estimation\\nrobust pose graph optimizer\\nglobal trajectory estimation\\nlightweight 3d mesher module\\nfast mesh reconstruction\\n3d metric-semantic reconstruction module\\nsemantically labeled images\\nmetric-semantic slam\\nreal-time metric-semantic localization and mapping\\nkimera\\ndeep learning\",\"186\":\"navigation\\nvisualization\\ntask analysis\\nrobot sensing systems\\nmachine learning\\ntraining\\ndecision making\\nimage representation\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nrobot vision\\nhigh-dimensional data\\ndecision-making problems\\ndeep reinforcement learning\\nplace recognition feedback\\nvisual navigation tasks\\nsample-efficient navigation policy learning\\ncitylearn environments\\nvisual place recognition\\nextreme visual appearance changes\\nrealistic environments\\nnavigation algorithms\\nbimodal image representations\\ncompact image representations\\ngoal destination feedback\\ndeep learning techniques\\nsample complexity\",\"187\":\"robot sensing systems\\ncameras\\nsafety\\ntask analysis\\nvisualization\\ncontrol engineering computing\\nhaptic interfaces\\nhuman-robot interaction\\nimage processing\\ntactile sensors\\ntouch (physiological)\\nhigh resolution soft tactile interface\\nphysical human-robot interaction\\ntactile interactions\\nintuitive communication tool\\nfundamental method\\ntactile abilities\\nmechanical safety\\nsensory intelligence\\nhuman-sized geometries\\nsoft tactile interfaces\\nintrinsically safe mechanical properties\\nnonlinear characteristics\\nrobotic system\\ncompletely soft interface\\nhuman upper limbs\\nhigh resolution tactile sensory readings\\nhuman finger\\ntactile input\\nhuman forearm\\nsafe tactile interface\",\"188\":\"actuators\\nfabrics\\nsolids\\nstructural beams\\nload modeling\\nsoft robotics\\nbiomedical equipment\\nbiomedical measurement\\nfinite element analysis\\ngait analysis\\nmechanoception\\nmedical robotics\\nmuscle\\northotics\\npatient rehabilitation\\nfrontal plane\\nsagittal plane\\nsr-afo exosuit\\nwearable ankle robot\\nankle stiffness\\nsoft robotic ankle-foot orthosis exosuit\\neversion ankle support\\npressure 50.0 kpa\\npressure 30.0 kpa\\nwearable robots\\nassistive robots\\nrehabilitation\",\"189\":\"task analysis\\ntorque\\nrobots\\nkalman filters\\nobservers\\ncontrol systems\\nsensors\\nbiomechanics\\nmedical robotics\\nmotion control\\npath planning\\npatient rehabilitation\\ntorque control\\nwearable robots\\nupper limb rehabilitation exoskeleton robot\\ntime-dependent trajectories\\ntask-based rehabilitation exercise\\nmultijoint motion\\nassistive mechanism\\nactive-assistive control system\\njoint-position-dependent velocity field\\ntask motion pattern\\ntime-independent assistance\\nactive motions\\nassistive motions\\nrehabilitation task\\nsingle joint tasks\\nkalman filter based interactive torque observer\\nsubject active motion intention\\nsubject torque exertion\",\"190\":\"exoskeletons\\nwrist\\nrobots\\nsensors\\ntracking\\nmagnetometers\\nmedical treatment\\nactuators\\nbiomechanics\\ndiseases\\nmedical robotics\\nneurophysiology\\npatient rehabilitation\\npatient treatment\\nrobot-mediated therapies\\nrobotic technologies\\nsocial contexts\\nmotion training\\ndevice design\\nwrist mobility\\ntendon-actuated exoskeleton\\nwrist rehabilitation\\nrobot rehabilitation\\nemerging promising topic\\nneuroscience\\nneurological diseases\\nrehabilitation process\",\"191\":\"impedance\\ndamping\\ntorque\\noptimization\\nknee\\nprosthetics\\nperturbation methods\\nartificial limbs\\ngait analysis\\nleast squares approximations\\nmedical robotics\\nsprings (mechanical)\\nvibration control\\nsquares estimation\\nimpedance controller\\nsquares optimization method\\nknee impedance\\nimpedance control\\nlower limb prostheses\\nhuman joint torque\\nperturbation studies\\nleast squares estimates\\nankle impedance parameters\\npowered transfemoral prosthesis\",\"192\":\"trajectory\\nlegged locomotion\\nexoskeletons\\ntorso\\nhip\\nfoot\\nknee\\ngait analysis\\ninjuries\\ninterpolation\\nmedical robotics\\nneurophysiology\\northotics\\npatient rehabilitation\\ngait trajectory pattern\\ntwin exoskeleton\\nspinal-cord injury patient\\nspinal cord injuries\\nrehabilitation centers\\nbasis function interpolation method\\nstable trajectory pattern\\nfeasible trajectory pattern\\nbiomedical orthotic devices\\ntwin lower-limb exoskeleton\\nbasis function interpolation\\ngait pattern generation\",\"193\":\"task analysis\\ncameras\\nrobot vision systems\\ncollision avoidance\\ncost function\\naerospace robotics\\nmarkov processes\\nmobile communication\\nmobile robots\\noptimisation\\nspace vehicles\\nautonomous observation systems\\nhuman activity\\nmultiobjective optimization\\nautonomous human observation problem\\nrobot-centric costs\\nscalarization-based multiobjective mdp methods\\nnasa astrobee robot operating\\nhuman-centric active perception\\nrobot autonomy\\nsemimdp formulation\\nconstrained mdp method\\nnasa astrobee robot\",\"194\":\"optimization\\ntrajectory\\npredictive models\\nrecurrent neural networks\\ncollision avoidance\\nrobot kinematics\\ngradient methods\\nhuman-robot interaction\\nimage motion analysis\\nmobile robots\\noptimisation\\npath planning\\nrecurrent neural nets\\nrobot vision\\nrobot trajectory planning\\ngradient-based trajectory optimization\\nhuman full-body movement prediction\\nmotion prediction\\ntrajectory optimization\\nenvironmental constraints\\nshort-term dynamics\\nlong-term prediction\\ninternal body dynamics\\nshort-term prediction\\nrecurrent neural network\\nmotion optimization\",\"195\":\"ergonomics\\nrobots\\ntask analysis\\noptimization\\npredictive models\\ncomputational modeling\\nforce\\nbiomechanics\\ngraph theory\\nhuman-robot interaction\\nmobile robots\\nmulti-robot systems\\npath planning\\ntelerobotics\\n32 dof bimanual mobile robot\\nergonomic-enhanced planner\\nreduced ergonomic cost\\nphysical human-robot cooperation tasks\\naction sequences\\ncontinuous physical interaction\\ncomputational model\\nergonomics assessment\\nhuman motion capture data\\nprediction model\\ninformed graph search algorithm\\nergonomic assessment\\nbimanual human-robot cooperation tasks\",\"196\":\"robot sensing systems\\nsemantics\\nbandwidth\\ncomputational modeling\\nvisualization\\ntrajectory\\ndecision theory\\nlearning (artificial intelligence)\\nmarkov processes\\nmobile robots\\nquery processing\\nrobot vision\\nmaking queries\\nregret-based criterion\\nactive reward learning strategy\\nco-robotic vision based exploration\\nrobotic explorer\\nbandwidth-limited environments\\nautonomous visual exploration\\nhigh-dimensional observation space\\ncommunication strategy\\nreward model\\nobservation model\\nhuman operator\\nscientifically relevant images\\npomdp problem formulation\",\"197\":\"legged locomotion\\nservice robots\\ntask analysis\\nplanning\\natmospheric measurements\\nparticle measurements\\nhuman-robot interaction\\nindustrial robots\\noccupational safety\\npath planning\\nheavy-duty robots\\nhuman pathway variations\\nhuman-robot collaboration\\nhrc process\\nwomen walking pathways\\nvaripath database\\nplanning process\\nsafety\",\"198\":\"actuators\\nmanipulators\\nmuscles\\ngrippers\\npolymers\\ncoils\\ndesign engineering\\ndexterous manipulators\\nelectroactive polymer actuators\\nmobile robots\\npneumatic actuators\\nfragile fruit\\npick and place demonstration\\nfin ray effect inspired soft gripper\\nball-and-socket joints\\njoule heating\\nelectrical activation\\nbio-inspired robotic manipulator design\\nbio-inspired arm\\nmanipulator prototype\\nscp actuators\\nrobotic arm\\nmuscle-like form\\ntwisting coiling polymer fibers\\nartificial muscle\\nsupercoiled polymer actuator\\nlow-cost robotic manipulator\",\"199\":\"springs\\nforce\\nmagnetic noise\\nmagnetic shielding\\nmagnetic levitation\\nmagnetic liquids\\nmagnetic separation\\nclamps\\ncontrol system synthesis\\nforce control\\nmagnetic devices\\nmobile robots\\nmulti-robot systems\\npermanent magnets\\nsprings (mechanical)\\nmagnetic spring\\nmagnetic mechanisms\\nclamping force\\npermanent magnet\\nattractive force\\ninternally-balanced magnetic unit\\ninternal force\\ninternally-balanced magnetic mechanisms\\nib magnet\\nnonlinear spring\\nunlike-pole pair\\nwall-climbing robots\\nceiling-dangling drones\\nmodular swarm robots\\nrobotic clamp\\nmechanism design of manipulators\",\"200\":\"manipulators\\nkinematics\\nshape\\nelectron tubes\\nfriction\\npayloads\\ncontrol system synthesis\\ndexterous manipulators\\nend effectors\\nmanipulator kinematics\\nrigidity\\ncontinuum manipulator\\nclosed-form inverse kinematics\\nindependently tunable stiffness\\ncompliant structures\\narticulated manipulator design\\nmanipulator end-effector\\nanalytical inverse kinematics\",\"201\":\"tendons\\ninstruments\\ngrippers\\ngears\\nrobots\\njoints\\nsurgery\\nactuators\\ncompensation\\nend effectors\\nendoscopes\\nmedical robotics\\nflexible instrument\\nendoscopic surgery\\nsnake-like robots\\nflexible tendon-driven instruments\\nmicrosurgical tasks\\nstandard endoscopic surgeries\\narticulated wrists\\ndistal-roll gripper\\ncompensation control scheme\\nend-effector rolling motion\",\"202\":\"planning\\naerospace electronics\\nvehicle dynamics\\ntwo dimensional displays\\nspace exploration\\nspace vehicles\\noceans\\ncomputational fluid dynamics\\nflow simulation\\nmarine robots\\nmobile robots\\npath planning\\nrobot dynamics\\nartificial flow field\\neast australian current\\nstreamline-based flow field planning\\nmotion planning\\nstreamline-based planning\\nfluid dynamics\\ntravel distance\\nincompressible flows\\nocean currents\\ndistance functions\\neuclidean distance\\nstream function\\nsteering heuristics\\nocean prediction data\\nautonomous marine robots\",\"203\":\"detectors\\ntraining\\nobject detection\\nmonitoring\\ntracking\\npredictive models\\nsemantics\\nconvolutional neural nets\\nfeature extraction\\nmarine engineering\\nsupervised learning\\ndeep neural network\\nsample extraction\\ncoral object dataset\\ncoral reef monitoring\\ndeep semisupervised learning approach\\ncoral species detection\\nconvolutional neural network-based object detector\",\"204\":\"observers\\nhistory\\nvehicle dynamics\\nrobots\\noptimization\\noptimal control\\ndynamics\\ncontrol system synthesis\\nfeedback\\nlearning (artificial intelligence)\\nneurocontrollers\\nposition control\\nrecurrent neural nets\\nrobot control capabilities\\ndisturbance dynamics observer network\\ncontroller network\\nconventional dob mechanisms\\nrecurrent neural networks\\noptimal control signals\\nconventional feedback controllers\\ndob-net\\ndisturbance ob-server network\\nobserver-integrated reinforcement learning\",\"205\":\"vents\\noceans\\nvehicle dynamics\\nunderwater vehicles\\nearth\\nnumerical models\\nbase stations\\nautonomous underwater vehicles\\noceanographic equipment\\noceanographic techniques\\nsearch problems\\nhydrothermal plume model\\nhydrothermal vent emissions\\nlocal maxima\\nautonomous nested search method\\nhydrothermal venting\\nsufficient autonomy\\nmission concept\\nsolar system\\nextra-terrestrial life\\nocean world\\nunmanned underwater vehicle\",\"206\":\"cameras\\ncalibration\\nthree-dimensional displays\\noptical distortion\\nsolid modeling\\ndistortion\\nadaptive optics\\ncomputer vision\\ngeometry\\nimage motion analysis\\nimage reconstruction\\nmotion estimation\\noptical transfer function\\nphotogrammetry\\nstereo image processing\\nimage-world correspondences\\nrefractive calibration methods\\nphotogrammetry techniques\\nprojective geometry\\nimage formation process\\nrefraction\\nlight rays\\nhousing interface\\nnonlinear effects\\nsystematic errors\\nspherical domes\\ncamera centers\\ndistortion based underwater domed viewport camera calibration\\npoint spread function\\npsf\\noptical system\\n3d reconstructions\",\"207\":\"muscles\\nmathematical model\\nforce\\nbiological system modeling\\nrobots\\ndynamics\\nbiomechanics\\nelasticity\\nelectroactive polymer actuators\\nlegged locomotion\\nmuscle\\npneumatic actuators\\npneumatic artificial muscles\\nbiological muscles\\nartificial copies\\nforce generation mechanism\\npam force-length\\nadditive passive parallel elastic element\\npam dynamic behaviors\\ndynamic muscle-like model\\nliving creatures\\nmultiplicative formulation\\ntwo-segmented leg\\nlegged robots\",\"208\":\"task analysis\\nrobot kinematics\\nmanipulators\\nwheelchairs\\nmanifolds\\nrehabilitation robotics\\nassisted living\\nhandicapped aids\\nmedical robotics\\nmobile robots\\npath planning\\nservice robots\\nconstraint-based shared control\\nspecific user command mappings\\ntask execution\\nimpairments\\ncontrol interface\\nmotor disability\\nlight-weight robotic manipulators\\nassistive robotics\\nshared control templates\\nlow-dimensional interface\\nhigh-dimensional tasks\\nhuman-readable format\\nstate transitions\",\"209\":\"natural languages\\nrobot sensing systems\\ncognition\\ntask analysis\\nsemantics\\nneural networks\\ncommon-sense reasoning\\ncontrol engineering computing\\nhuman-robot interaction\\nnatural language processing\\ntext analysis\\nenvironmental context\\nnatural language instruction\\nunconstrained natural language\\nlanguage-model-based commonsense reasoning\\ncommonsense knowledge\\nspoken natural language\\nlmcr\\nparsing\\nverb frames\\nunstructured textual corpora\\nrobot\",\"210\":\"animation\\nhardware\\nneck\\nlight emitting diodes\\nmouth\\nservice robots\\ncomputer animation\\ncontrol engineering computing\\ncontrol system synthesis\\nhumanoid robots\\nrobot modalities\\nzoomorphic-designed robots\\nrobot design\\nanimated characters\\ntable top robot\\nanimation techniques\\nrobot hardware\\nharu\",\"211\":\"cleaning\\nobject detection\\nrobots\\ntraining\\ntask analysis\\nimage resolution\\nvisualization\\nfeature extraction\\nrobot vision\\nservice robots\\noffice item detection system\\nyolov3 framework\\nvisual dirt detection\\nautonomous cleaning\\nvacuum cleaning\\nprofessional cleaning robots\\ndirtnet\",\"212\":\"search problems\\nrobots\\nprobabilistic logic\\nsemantics\\nbuildings\\ninference algorithms\\nvisualization\\ninference mechanisms\\nmanipulators\\nmobile robots\\nrobot vision\\nsemantic linking maps model\\ntarget object\\nlandmark objects\\nprobabilistic inter-object spatial relations\\nhybrid search strategy\\nslim-based search strategy\\nfetch mobile manipulation robot\\ncommon human environments\\nunseen target objects\\nreasoning\\nsearch space\\ncommon spatial relations\\nactive visual object search strategy\",\"213\":\"cameras\\nthree-dimensional displays\\nstability analysis\\nasymptotic stability\\nconvergence\\nestimation error\\nimage sequences\\nlyapunov methods\\nmobile robots\\nrobot vision\\nsolid modelling\\nstability\\nsfm\\nincremental active depth estimation\\nchronological sequence\\nimage frames\\ncamera actuation\\ncontrol inputs\\nimage plane\\nvision-controlled structure-from-motion scheme\\ndepth estimation filter\\nlyapunov theory\",\"214\":\"image segmentation\\nsemantics\\ntask analysis\\nobject detection\\nimage color analysis\\nbenchmark testing\\nlabeling\\ncomputer vision\\nfeature extraction\\nimage classification\\nstereo image processing\\naerial imagery\\nunmanned aerial vehicle tasks\\nsingle ground truth type\\nvirtual environment\\nhigh-resolution images\\nvirtual scenes\\ncomprehensive virtual aerial image dataset\\nvisual ground truth data\",\"215\":\"geometry\\nlaser radar\\nthree-dimensional displays\\nhistograms\\nsimultaneous localization and mapping\\nrough surfaces\\nsurface roughness\\ncomputational geometry\\nfeature extraction\\nimage matching\\nmobile robots\\noptical radar\\nrobot vision\\nslam (robots)\\nstereo image processing\\nintensity scan context\\nlight detection and ranging sensor\\ndiscard intensity reading\\ngeometric relation\\nintensity structure re-identification\\ncoding intensity\\nlidar sensor\\n3d loop closure detection\\ngeometrical only descriptor matching\\nplace recognition\\nrobot navigation\\nfast point feature histogram\",\"216\":\"simultaneous localization and mapping\\nthree-dimensional displays\\nvisualization\\nfeature extraction\\nnavigation\\ncameras\\nrobustness\\naugmented reality\\ndata visualisation\\nrobot vision\\nslam (robots)\\nstereo image processing\\ntext analysis\\ntext detection\\ntext object integration\\nscene understanding\\nillumination-invariant photometric error\\ntextslam\\ntext-based visual slam\\n3d text maps\\nvisual slam pipeline\\nplanar text features\\nvisual slam system\\nplanar feature\",\"217\":\"optimization\\nconvergence\\ncameras\\nrobustness\\noptical imaging\\nlearning systems\\nnonlinear optics\\nimage registration\\nlearning (artificial intelligence)\\npose estimation\\npatch alignments\\nglobal information\\nlearning-based network\\nlocal information\\nlearning-based method\\ndirect alignment\\ncamera poses\\nphotometric error\\nnonconvex property\\noutlier terms\\nlocal error term\\nglobal image registration information\\nflownorm\\nhuber norm\\ndso\\nba-net\",\"218\":\"cameras\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nuncertainty\\nvisualization\\ndistance measurement\\nrobot vision\\nslam (robots)\\nstereo image processing\\nsensor-specific modifications\\nslam systems\\nrobustness\\ncamera configurations\\nadaptive slam system\\nmulticamera setup\\nvisual slam\\nadaptive initialization\\nscalable voxel-based map\\nsensor-agnostic information-theoretic keyframe selection algorithm\\nvisual front-end design\\nvisual-inertial odometry\",\"219\":\"simultaneous localization and mapping\\nheuristic algorithms\\ndynamics\\nthree-dimensional displays\\nsolid modeling\\ntracking\\nfeature extraction\\nimage motion analysis\\nimage segmentation\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nrigid moving objects\\nstatic structure\\ndynamic structure\\nrigid objects\\nobject-aware dynamic slam algorithm\\nmodel-free\\nsignificant motion constraints\\n3d models\\nslam based approaches\\nunstructured dynamic environments\\nautonomous systems\\nincreased deployment\\nsimultaneous localisation\\nstatic world assumption\",\"220\":\"simultaneous localization and mapping\\noptimization\\nthree-dimensional displays\\ndamping\\ntask analysis\\nneural networks\\ngradient methods\\ngraph theory\\nlearning (artificial intelligence)\\nrobot vision\\nslam (robots)\\nautomatic differentiation\\ndense simultaneous localization\\nlearning-based approaches\\nrepresentation learning approaches\\nclassical slam systems\\ndifferentiable function\\noptimize task performance\\ntypical dense slam system\\n\\u2207slam\\nposing slam systems\\ndifferentiable computational graphs\\ndifferentiable trust-region optimizers\\ntask-based error signals\",\"221\":\"robot sensing systems\\nrobot kinematics\\ntraining\\nmulti-robot systems\\nmachine learning\\nshape\\nlearning (artificial intelligence)\\nneurocontrollers\\nrecurrent neural nets\\ntwo-wheeled robotic platform\\nlocal behavioral sequences\\nmultirobot systems\\nmultirobot team\\ntraditional observer-based approach\\nmachine learning methods\\nremote teammate localization modules\\nlong-short-term-memory\",\"222\":\"three-dimensional displays\\nlaser radar\\ntraining\\nestimation\\ntwo dimensional displays\\nrobot sensing systems\\ndistance measurement\\ngeometry\\noptical radar\\nradar computing\\nreliability\\nsupervised learning\\nunsupervised learning\\nunsupervised geometry-aware deep lidar odometry\\nvisual perception\\nsupervised learning-based approaches\\nsupervised training\\nground-truth pose labels\\ntrainable lo\\nuncertainty-aware loss\\nlego-loam\\nunsupervised learning-based approaches\\negomotion estimation approaches\\nstereo-vo datasets\\ncomplex urban datasets\\noxford robotcar datasets\\nkitti\",\"223\":\"image recognition\\ntask analysis\\nrobot sensing systems\\nfeature extraction\\nrobot kinematics\\nobject detection\\ncontrol engineering computing\\nimage colour analysis\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nneural nets\\nrobot vision\\nrobust state-action recognition\\nlfo methods\\nsa-net\\nrgb-d data streams\\nreplicated robotic applications\\nmobile ground robots\\nrobotic manipulator\\nphysical robot\\ndeep neural network architecture\\nlearning from observation\",\"224\":\"navigation\\nrobots\\nforce\\ngenerators\\ntrajectory\\nlearning (artificial intelligence)\\ncollision avoidance\\nmobile robots\\noptimisation\\npath planning\\nrobot vision\\nsocially compliant navigation\\nrobots navigation\\nsocially compliant behavior\\noptimization objectives\\ninverse reinforcement learning approaches\\nnatural behavior\\ngenerative navigation algorithm\\nnavigation path\\nlatent social rules\\ntrained social navigation behavior\\nnavigan\",\"225\":\"task analysis\\nrobots\\nlearning (artificial intelligence)\\nstandards\\ntrajectory\\ndata collection\\nlearning systems\\ndeploying learning-based systems\\nscalable multitask imitation learning\\nsparse task-agnostic reward signals\\nreinforcement learning algorithms\\ncontinuous improvement\\nprior imitation learning approaches\\ninitial demonstration dataset\\nlearned latent space\\nmultitask demonstration data\\nmultitask setting\\nautonomous improvement\\nsupervised imitation\\nautonomous data collection\\nimitation learning system\\nrobot learning\\nstable approach\",\"226\":\"videos\\nmotion segmentation\\nimage segmentation\\nmeasurement\\nhidden markov models\\ntask analysis\\nmedical image processing\\nrecurrent neural nets\\nsupervised learning\\nvideo signal processing\\nvisual representations\\nembedding space\\ndownstream tasks\\naction segmentation\\nmotion-centric representation\\nsurgical video demonstrations\\ndeep embedding feature space\\nvideo observations\\nsiamese network\\naction segment\\nrandomly sampled images\\nrecurrent neural network\\nlabeled video segments\\nlearned model parameters\\nsurgical suturing kinematic motions\\nkinematic pose imitation\\nsemisupervised representation learning\\njigsaws dataset\",\"227\":\"uncertainty\\nrobot kinematics\\npath planning\\nmobile robots\\nplanning\\nglobal positioning system\\ncollision avoidance\\nmotion control\\nprobability\\nrandom processes\\ntrees (mathematics)\\nuncertain systems\\nrough environments\\nuncertainty propagation\\nrapidly-exploring random tree\\nposition uncertainty\\nmotion uncertainty\\nnatural environment\\nwheeled robots\\npath planning architecture\\npath-following\\npath replanning\",\"228\":\"robustness\\nrobots\\nheuristic algorithms\\nmanganese\\nplanning\\naerodynamics\\nsafety\\ncomputational complexity\\nsampling methods\\nsearch problems\\ntrees (mathematics)\\nmathematically-rigorous proof\\nasymptotic optimality\\nrrt*\\nasymptotically-optimal motion planning\\noptimality proof\\nsampling-based algorithms\\nconnection radius\",\"229\":\"planning\\ncomplexity theory\\nprobabilistic logic\\ntwo dimensional displays\\nrobots\\ncollision avoidance\\nbenchmark testing\\ncomputational complexity\\ndeterministic algorithms\\ngraph theory\\nprobability\\nshortest \\u03b4-clear path\\nsample complexity\\nprobabilistic roadmaps\\n\\u03b5-nets\\noptimality guarantees\\ndeterministic sampling distribution\\nmotion planning problem\\nparameter completeness\",\"230\":\"learning (artificial intelligence)\\nrobots\\nsurgery\\ntrajectory\\ntask analysis\\nshape\\neducation\\nend effectors\\ngaussian processes\\nhuman-robot interaction\\nmanipulator dynamics\\nmedical robotics\\nmotion control\\nregression analysis\\ncomplex tasks demonstrations\\nreinforcement learning algorithm based manipulation skill transferring technique\\nrobot-assisted minimally invasive surgery\\ngaussian mixture model\\ngaussian mixture regression\\nmultiple demonstrations\\ntrial phase performed offline\\npractical surgical operation\\nkuka lwr4+ robot\\nhuman manipulation skill\\nsurgical robots\",\"231\":\"uncertainty\\nplanning\\nautomata\\ncomputational modeling\\nhazards\\ncollision avoidance\\nmobile robots\\nmonte carlo methods\\nuncertain systems\\nuncertainty model\\npath planning\\ndynamical uncertainties\\nprobabilistic model\\nsafe robot mission planning\\nmonte carlo method\\ncollision free path\",\"232\":\"games\\nheuristic algorithms\\nplanning\\nfeedback linearization\\niterative methods\\nvehicle dynamics\\noptimal control\\ncontrol system synthesis\\nconvergence of numerical methods\\ndifferential games\\nfeedback\\ngame theory\\nlinear quadratic control\\nlinearisation techniques\\nnonlinear control systems\\npath planning\\nfeedback linearizable dynamics\\nnonlinear optimal control community\\nmultiplayer general-sum differential games\\nilq methods\\nlocal equilibria\\ninteractive motion planning problems\\niterative procedures\\ninitial conditions\\nhyperparameter choices\\nunsafe trajectories\\ndynamical systems\\nalgorithmic reliability\\nfeedback linearizable structure\\niterative linear-quadratic method\",\"233\":\"buoyancy\\nrobots\\nforce\\ntorque\\nprototypes\\npropellers\\nautonomous aerial vehicles\\nautonomous underwater vehicles\\ndesign engineering\\nhelicopters\\nmobile robots\\nstability\\nsingle design difficult\\nnormal aerial vehicles\\nrotational acceleration\\nquadrotor based vehicle\\nvehicle body\\ndesign considerations\\naerial-aquatic quadrotor\\ncoupled symmetric thrust vectoring\\naerial-aquatic vehicles\\nfluid resistance\\nenergy efficient position\\nmorphable aerial-aquatic quadrotor\\nmechanical actuation\\nstatic stability\",\"234\":\"cameras\\nvisualization\\nmathematical model\\ndrones\\nchannel models\\nservomotors\\nangular velocity\\nautonomous aerial vehicles\\nrobot vision\\nvisual servoing\\nautonomous intercept drone\\nunwanted drone\\nradio wave gun\\nimage-based visual servo algorithm\",\"235\":\"payloads\\noscillators\\nangular velocity\\nattitude control\\nvehicle dynamics\\ntrajectory\\nactuators\\nautonomous aerial vehicles\\ncontrol system synthesis\\nhelicopters\\nmobile robots\\nmulti-robot systems\\nposition control\\ncable-suspended payload system\\nhuman control\\nmultiple quadcopters system\\nleader quadcopter\\npayload attitude controller\\ncable attitude controller\\nquadcopter-payload system\\nquadcopters\\ncable-suspended payload\\ncollaborative transportation\\nmulti-agents\",\"236\":\"three-dimensional displays\\nlaser radar\\nautonomous vehicles\\ncameras\\ncalibration\\nobject detection\\nrobot sensing systems\\nimage annotation\\nimage colour analysis\\nmobile robots\\noptical radar\\nroad vehicle radar\\nstereo image processing\\ntraffic engineering computing\\na*3d dataset\\nself-driving cars\\n3d object detection\\n3d object annotations\\nautonomous driving research\\nnuscenes dataset\\nkitti dataset\\nhigh-density images\\nlidar data\\nrgb images\\ncomputer vision tasks\",\"237\":\"three-dimensional displays\\nsemantics\\nfeature extraction\\ntwo dimensional displays\\nvehicle detection\\nhead\\nconvolution\\nimage denoising\\nimage segmentation\\nobject detection\\noptical radar\\nstereo image processing\\ntraffic engineering computing\\ndepth-aware features\\n3d vehicle detection\\npoint cloud distribution\\nsemantic context information\\nambiguous vehicles\\nsemantic context encoder\\ntarget detection range\\nemantic segmentation masks\\ndepth-aware head\\nsegvoxelnet\\nlidar\\nnoisy region suppression\",\"238\":\"roads\\ncontext modeling\\ntask analysis\\nvehicles\\nhidden markov models\\nsafety\\npredictive models\\nbehavioural sciences computing\\ndriver information systems\\nlearning (artificial intelligence)\\nroad accidents\\nroad safety\\nroad traffic\\nunexpected vru movements\\nresidential roads\\nproficient acceleration\\ndeceleration\\nroad width\\ntraffic direction\\nmultilinear reward function\\ncontextual information\\nlong-term prediction\\ndefensive driving strategy\\ncontext-aware multitask inverse reinforcement learning\\nadvanced driver assistance systems\\nvulnerable road users\\ntraffic accident reduction rate\\nmultitask irl approach\\nfine-grained driving behavior prediction\\ninverse reinforcement learning\",\"239\":\"roads\\nvehicle dynamics\\ntopology\\nrobot sensing systems\\nshape\\ncartography\\ndata privacy\\nroad vehicles\\ntraffic engineering computing\\ndedicated mapping vehicles\\nlow traversal frequencies\\nanonymized data\\nup-to-dateness\\ncrowdsourced data\\nautomatically trigger map update jobs\\nmap patches\\ndate hd map\\nautomated driving functions\\nhd maps\\nautomotive high definition digital map generation\\nautomated driving up to date\",\"240\":\"convolutional codes\\ntask analysis\\nsemantics\\ndecoding\\ntraining\\nautonomous vehicles\\ncomputational modeling\\nembedded systems\\nfield programmable gate arrays\\nimage segmentation\\ninference mechanisms\\nlearning (artificial intelligence)\\ntraffic engineering computing\\nbinarized driveable area detection network\\nautonomous driving\\nground-plane detection\\nobstacle detection\\nmaneuver planning\\nover-parameterized networks\\nslim binary networks\\nbinary weights\\nbinary dilated convolutions\\nbinary dad-net\\nsemantic segmentation networks\\nfpga\\nmemory size 0.9 mbyte\",\"241\":\"global navigation satellite system\\ncameras\\nlaser radar\\nurban areas\\nrobot sensing systems\\ntrajectory\\nsatellites\\nimage matching\\nimage registration\\ninertial navigation\\noptical radar\\nsatellite navigation\\nurban canyon\\nurban terrain\\nhong kong\\nsan francisco\\nimu\\ngnss-based solutions\\nlidar\\nurban scene localization\\nurban scene mapping\\ninertia measurement units\\ncamera-based methods\\ninertia navigation\\nvisual feature matching\\npoint cloud registration\\nfull sensor suite dataset\\nurbanloco\",\"242\":\"robot sensing systems\\ntensile stress\\ntrajectory\\nrobustness\\nuncertainty\\nreal-time systems\\ndistance measurement\\nmobile robots\\npath planning\\nsensors\\ntensors\\nodometry-based global localization\\nambiguous observations\\nodometry drift\\nblind robots\\nrobot state\\nbelief tensor\\nmap-corrected odometry localization\\nmap traversability\\nrobotics applications\\nhidden sensor\",\"243\":\"three-dimensional displays\\ncorrelation\\ntwo dimensional displays\\npose estimation\\nuncertainty\\ndecoding\\ntask analysis\\nneural nets\\nstereo image processing\\n3d localization task\\nkitti dataset\\nstereo 3d localization\\nneural network architecture\\nstereo imaging\\nhuman body\\njoint human pose estimation\\nimage stereo pair\\nstereo pose dataset\",\"244\":\"image reconstruction\\ncameras\\ntraining\\nlighting\\npipelines\\nrobustness\\nvisual odometry\\ndistance measurement\\nlearning (artificial intelligence)\\nneural nets\\npose estimation\\nregression analysis\\nstereo image processing\\nself-supervised deep pose corrections\\nrobust visual odometry\\ndata-driven learning\\nsix-degrees-of-freedom ground truth\\nself-supervised dpc network\\nstereo odometry estimators\\npose corrections regression\\nmonocular odometry estimators\",\"245\":\"visualization\\nlenses\\nposition measurement\\ncameras\\nmeasurement uncertainty\\npose estimation\\nattitude control\\nattitude measurement\\nautonomous aerial vehicles\\nindoor navigation\\nmobile robots\\nposition control\\nrobot vision\\nlocal positioning\\nmarker coordinate system\\nhigh-accuracy global positioning\\nultra-high-accuracy visual marker\\nindoor precise positioning\\nindoor positioning\\nindoor mobile robots\\ndrones\\ngeneral-purpose technology\\nmultiple dynamic moires\\nlenticular lens\\nattitude estimation error\\nmarker position error\\nreprojection error\\nsize 10.0 m\\nsize 1.0 cm\\nsize 10.0 cm\\nattitude accuracy\",\"246\":\"robot sensing systems\\nestimation\\nobservability\\nvelocity measurement\\ndistance measurement\\nmobile robots\\ninertial systems\\nkalman filters\\nnonlinear filters\\nobject tracking\\nposition measurement\\nslam (robots)\\nultra wideband technology\\nultrawideband technology\\nuwb anchor\\nuwb range\\nmoving robot tracking\\nposition tracking\\nrobotic applications\\nlocalization systems\\noptical tracking\\n9 dof inertial measurement unit\\nuwb ranging source\\nuwb technology\\nrobot speed estimation\\norientation estimation\\nimu sensor\\nextended kalman filter\\nekf\\nrobot pose estimation\",\"247\":\"exoskeletons\\nlegged locomotion\\noptimization\\nbayes methods\\nreliability\\ntrajectory\\ngait analysis\\nlearning (artificial intelligence)\\noptimisation\\nwearable robots\\npersonalized gait optimization framework\\nlower-body exoskeleton\\nnumerical objectives\\npreference-based interactive learning\\ncospar algorithm\\npairwise preferences\\nexoskeleton walking\\nnonintuitive behavior\\nnumerical feedback\\nhuman walking trajectory features\\nuser-preferred parameters\\nadapting personalizing exoskeletons\\nexoskeleton gait optimization\",\"248\":\"artificial neural networks\\ntrajectory\\nmanipulator dynamics\\naerodynamics\\nactuators\\nadaptive control\\nbackpropagation\\ncontrol system synthesis\\nfeedforward\\nflexible manipulators\\nneurocontrollers\\nnonlinear control systems\\nposition control\\nstability\\nonline backpropagation\\ncollaborative robots\\nmultilayer neural network\\ncontrol architecture\\nflexible joint dynamics\\ncontrol bandwidth\\ncontrol design\\nspace manipulators\\nonline learning\\nflexible-joint robots\\nadaptive neural trajectory tracking control\\nseries-elastic joint actuators\\njoint flexibility\\nbaxter robot\\ncommanded joint position\\nouter loop control\\nnonlinear basis functions\\ninternal weights\\ntracking error\\noutput layer weights\\nlinear output layer\\nrobot dynamics\\nlinear-in-parameter representation\\nfeedforward control\\napproximate unknown dynamics\",\"249\":\"unmanned aerial vehicles\\ntraining\\ntarget tracking\\ncorrelation\\nrobustness\\nfeature extraction\\nautonomous aerial vehicles\\nimage motion analysis\\nlearning (artificial intelligence)\\nobject detection\\nobject tracking\\nremotely operated vehicles\\nvideo signal processing\\nunmanned aerial vehicle tracking scenarios\\nhigh computational efficiency\\nuav tracking process\\nviewpoint variations\\nbackground appearance\\ncf-based trackers\\nideal tracker\\nobject position\\nresponse-based errors\\nforward errors\\nbackward errors\\ncurrent training sample\\nhistorical training samples\\nbicf\\nresponse-based bidirectional incongruity error\\nuav datasets\\nuav123\\nbidirectional incongruity-aware correlation filter\",\"250\":\"physics\\nengines\\ntask analysis\\noptimization\\nrobots\\nadaptation models\\nplanning\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nrobot vision\\ntabletop environment\\nobject rearrangement\\nlow-cost tabletop robot\\nobject rearrangement planning\\nlearning-based methods\\nsingle-step interaction\\nadaptive learning procedure\\nsize 3.5 cm\",\"251\":\"task analysis\\nrobots\\ntraining\\nspace exploration\\naerospace electronics\\nextraterrestrial measurements\\nsearch problems\\nunsupervised learning\\nreachable outcome space\\nsparse rewards settings\\nreinforcement learning\\nlearning process\\nsearch strategy\\ntaxons\\ntask agnostic exploration\\npopulation-based divergent-search approach\\ndiverse policies\\nhigh-dimensional observation\\ntask-specific information\\nlow-dimensional outcome space\\nlearned outcome space\\nground-truth outcome space\",\"252\":\"robots\\ncomputational modeling\\npredictive models\\naerodynamics\\ncost function\\ntask analysis\\nstochastic processes\\nmobile robots\\npredictive control\\nprobability\\nremotely operated vehicles\\nrobot dynamics\\nstochastic systems\\nmodel error\\nreceding horizon control\\nrepetitive path-following task\\nsimple learned dynamics model\\nmpc horizon\\nstochastic mpc\\nprediction horizon\\nonline model learning\\nground robot\\ncontext-aware cost shaping\\nstochastic model predictive control\",\"253\":\"three-dimensional displays\\nstrain\\nsolid modeling\\nimage reconstruction\\nx-ray imaging\\ntwo dimensional displays\\ndeformable models\\nbiomedical mri\\ncatheters\\ncomputerised tomography\\ndeformation\\ndiagnostic radiography\\nimage registration\\nmedical image processing\\nphantoms\\nlive 3d aortic deformation\\nstatic 3d model\\nstereo images\\nreconstruction process\\ndeformation graph approach\\nreconstruction accuracy\\naortic 3d deformation reconstruction\\n2d x-ray fluoroscopy\\ncurrent clinical endovascular interventions\\ncatheter manipulation\\naortic 3d surface\\ndeformation reconstruction frameworks\\n3d intraoperative guidance\\naortic deformation reconstruction\\nfluoroscopy\\nendovascular interventions\",\"254\":\"needles\\nkinematics\\nfasteners\\npath planning\\nshape\\nelectron tubes\\nlaser beam cutting\\nbiomedical ultrasonics\\nmedical image processing\\nmedical robotics\\npose estimation\\nsurgery\\ntumours\\nimage-guided insertion\\nnovel steerable needle\\nkinematics model\\npassive needle tip articulation\\nneedle path consistency\\narticulated tip\\nneedle design\\ntissue mechanics\\nneedle-tissue interaction\\nneedle placement\\npercutaneous tumor ablation\\nbiopsy tumor ablation\",\"255\":\"needles\\nrobots\\nmathematical model\\nlinear programming\\ncomputational modeling\\nnumerical models\\ninverse problems\\nbiological tissues\\nend effectors\\nfinite element analysis\\nmedical image processing\\nmedical robotics\\nrobotic needle insertion\\nsoft tissues\\nrobotic steering\\nflexible needle\\npredefined path\\ninverse problem\\nrobot end effector\\nconstraint-based formulation\\nsimulation-guided needle insertion\\ndirect simulation\\nrespiratory motion\\nnumerical simulation\\nconstraint-based inverse finite element simulation\",\"256\":\"catheters\\nrobots\\ntask analysis\\ntraining\\ncatheterization\\ninstruments\\nsurgery\\nblood vessels\\ndiagnostic radiography\\nhaemodynamics\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\npulsatile flow\\nreduced procedural duration\\ndeep reinforcement learning technologies\\ncomplex endovascular tasks\\nreduced fatigue\\ncognitive workload\\nmodel-based approaches\\nmodel-free generative adversarial imitation learning\\nstandard arterial catherization task\\nautomation policies\\ncatheter motions\\ncollaborative robot-assisted endovascular catheterization\\nmaster-slave systems\\nclinical benefits\\nradiation doses\\nvascular anatomical model\",\"257\":\"trajectory\\nneedles\\nthree-dimensional displays\\nlearning (artificial intelligence)\\ncatheters\\nkinematics\\nbiomedical mri\\ncollision avoidance\\nmedical image processing\\nmedical robotics\\npath planning\\nrobot kinematics\\ntrajectory smoothness\\nga3c reinforcement learning\\nsurgical steerable catheter path planning\\npath planning algorithms\\nsteerable catheters\\nanatomical obstacles avoidance\\ninsertion length\\nneedle kinematics\\nsmooth trajectories\\npath planning problem\\nreinforcement learning problem\\ntrajectory planning model\\noptimal trajectories\\nobstacle clearance\\nkinematic constraints\\nmri images processing\\npath planning model\\ncurvilinear trajectories\\nrrt* algorithms\\nobstacle avoidance\",\"258\":\"legged locomotion\\ntrajectory\\ntask analysis\\ncomputational modeling\\ndynamics\\nfoot\\nconvolutional neural nets\\nhydraulic actuators\\nneurocontrollers\\npredictive control\\nrobot dynamics\\non-board mapping\\ncontact sequence task\\nconvolutional neural network\\nmodel predictive controller\\non-board sensing\\nmpc-based controller\\nterrain insight\\ndynamic legged locomotion\\nhydraulically actuated quadruped robot hyqreal\",\"259\":\"switches\\nuncertainty\\nrobots\\nsupervisory control\\nadaptive control\\nlibraries\\ncontrol system synthesis\\nfeedback\\nlegged locomotion\\nparameter estimation\\nrobot dynamics\\nuncertain systems\\nfeedback control\\nestimator design\\ndynamic locomotion\\nparametric uncertainty\\nrobotic systems\\nadaptive supervisory control\\ndynamically walking bipedal robot\",\"260\":\"force\\naerospace electronics\\nlegged locomotion\\ntorque\\nrobot kinematics\\ndynamics\\nforce control\\nmotion control\\nposition control\\nrobot dynamics\\ntorque control\\ncontact-force-control\\njoint space hybrid control\\nlegged robot platform\\nhybrid control algorithm\\nrobot displayed stability\\nmammal-type quadruped robot\\ndynamic locomotion\\npush reaction abilities\\ncartesian space\\nexternal push disturbances\",\"261\":\"springs\\nactuators\\nforce\\nrobot kinematics\\ndynamics\\njacobian matrices\\nelasticity\\nlegged locomotion\\nrobot dynamics\\nsprings (mechanical)\\nstability\\nmechanical design\\nrobotic hopping\\nmoving-mass hopping robots\\nhop heights\\nsingle-spring model\\ndouble-spring model\\nhopping effort\\nparallel spring\\nhybrid systems models\\nrigorous trajectory optimization method\\none-dimensional hopping robot\\nactuator\\nsingle spring\\nmoving-mass robot\\nstable hopping\\nground contact\",\"262\":\"cameras\\nrobot vision systems\\nrobot kinematics\\nlegged locomotion\\ncollision avoidance\\ngait analysis\\nrobot dynamics\\nfully sensorized mini-cheetah quadruped robot\\nunstructured terrain\\ndynamic exploration\\ndynamic trotting\\nhighly irregular terrain\\nobstacle avoidance\\nfoothold adjustment\\nevaluation algorithms\\neffective filtering\\nmit mini-cheetah\\nintel realsense sensors\\njumping\\ndynamic locomotion\\neffective sensor integration\\nwalking speed\\nobstacle clearance\\nnarrow paths\\ncluttered environments\\nrough terrain locomotion capability\\nrescue scenarios\\ndisaster response\\nmobile platforms\\nlegged robots\\nsmall-scale quadruped robot\\nsize 0.3 m\\nmass 9.0 kg\",\"263\":\"planning\\ntarget tracking\\nrobot kinematics\\npartitioning algorithms\\nrobot sensing systems\\nrobustness\\ncontrol system security\\ndistributed algorithms\\ndivide and conquer methods\\nmulti-robot systems\\noptimisation\\npath planning\\ndistributed attack-robust submodular maximization\\nmultirobot planning\\nswarm-robotics applications\\nmultirobot motion\\nattack-robust algorithms\\nrobust optimization\\ndistributed robust maximization\\ndrm performance\\nmultiple robots\\ndenial-of-service attacks\\ndos attacks\\nlarge-scale robotic applications\\ngeneral-purpose distributed algorithm\\ndivide-and-conquer approach\\nrobot communication range\\nclose-to-optimal performance\\nactive target tracking\",\"264\":\"robot kinematics\\ntask analysis\\ndelays\\noptimization\\ngames\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\noptimisation\\nmultirobot patrolling\\nadaptive opponents\\npatrolling problem\\nmultiple agents\\ntime consuming\",\"265\":\"robot sensing systems\\noptimization\\nmutual information\\nplanning\\natmospheric measurements\\nparticle measurements\\nentropy\\nmobile robots\\nmulti-robot systems\\noptimisation\\nwireless sensor networks\\nmobile robotic sensor networks\\nneighbor robots\\nconditional mutual information\\ncommunication properties\\nspecific measurement set\\nparticle methods\\ninformation computation\\ndistributed optimization\\nlocal utility design\\ncommunication-aware information gathering\\nsampling procedures\\nentropy reduction\",\"266\":\"magnetic resonance imaging\\nrobots\\ntumors\\nforce\\nblood\\nelectromagnetics\\nmachine learning\\ncomputational complexity\\ndeterministic algorithms\\ndrug delivery systems\\ndrugs\\nlearning (artificial intelligence)\\nneural nets\\noptimisation\\nactuation steps\\ntargeted drug delivery\\nmaze-like environment\\nvascular system\\nbasic scenario\\nglobal external force\\nfluidic flow\\ndeep learning\",\"267\":\"trajectory\\ncost function\\nswitches\\nacceleration\\nrobustness\\nrobots\\nautonomous aerial vehicles\\nbang-bang control\\nduality (mathematics)\\nmobile robots\\nnewton method\\nnonlinear programming\\npath planning\\nsensitivity analysis\\ntime optimal control\\nnlp solvers\\nparametric optimization problems\\nquasinewton method\\ngeometric path\\ntime-optimal velocity profile\\nhierarchical optimization\\nbang-bang control structure\\nnonlinear programming solvers\\ndynamic robotic vehicles\\ntime-optimal trajectories\\nduality gap approach\\nuav time-optimal trajectory\\ngap-free bilevel optimization\\ninterior-point method\",\"268\":\"heuristic algorithms\\ntrajectory optimization\\nconvergence\\napproximation algorithms\\nrobots\\napproximation theory\\ndiscrete systems\\ngradient methods\\noptimal control\\noptimisation\\nsampling methods\\nstochastic processes\\nconstrained problems\\nstochastic search\\nbox control constraints\\nnonlinear state constraints\\ndiscrete dynamical systems\\nsampling-based trajectory optimization methodology\\nstochastic approximation\\nconstrained sampling-based trajectory optimization\\nnonsmooth penalty functions\\ncontrol inputs\\ntruncated parameterized distributions\",\"269\":\"trajectory\\ncomputational modeling\\ntask analysis\\nnumerical models\\nrobots\\nfeedback control\\noptimal control\\nfeedback\\ngaussian processes\\nindustrial robots\\nlearning systems\\nmobile robots\\nnonlinear control systems\\noptimisation\\npendulums\\ntrajectory control\\ntime-dependent optimal trajectories\\noptimal feedback control policies\\nreal-world systems\\nfrequent correction\\nmodel errors\\noptimal reference trajectories\\nhigh dimensional state space\\nlearning control policies\\noptimally control robotic systems\\nhigh dimensional nonlinear system dynamic models\\nswing-up problem\\nunderactuated pendulum\\nenergy-minimal point-to-point movement\\n3-dof industrial robot\",\"270\":\"optimal control\\ntrajectory\\noptimization\\nheuristic algorithms\\ndynamic programming\\nrobots\\nacceleration\\niterative methods\\nmotion control\\ncrocoddyl\\ncontact robot control by differential dynamic library\\nopen-source framework\\nmulticontact optimal control\\nstate trajectory\\ncontrol policy\\nsparse analytical derivatives\\ndifferential geometry\\nfloating-base systems\\nfddp\\ncomputation time\\ninfeasible state-control trajectories\\nhighly-dynamic maneuvers\\nfeasibility-driven differential dynamic programming\",\"271\":\"grasping\\nstacking\\ntask analysis\\nfeature extraction\\nmanipulators\\ntraining\\ngrippers\\nindustrial manipulators\\nlearning systems\\nneurocontrollers\\noptimal control\\ndeep reinforcement learning\\nintegrated robotic arm system\\nobject grasping\\nmodel-free deep q-learning method\\ngrasping-stacking task\\ngsn\\ngrasping for stacking network\\nindustrial environments\\ngnet\\noptimal location\\nlong-range planning\",\"272\":\"semantics\\ntask analysis\\ngrasping\\nfeature extraction\\nrobots\\ncognition\\ncontext modeling\\nlearning (artificial intelligence)\\nmanipulators\\nubiquitous computing\\ncontext-aware grasping engine\\nsemantic grasping\\nstable grasps\\nspecific object manipulation tasks\\ntask constraints\\nsemantic representation\\ngrasp contexts\\nobject states\\nmemorization\\nsemantic grasps\\ncage\\nstatistically significant margins\\nmemorization balancing\\nrobot\",\"273\":\"image reconstruction\\nadaptation models\\nestimation\\ncameras\\nrobot sensing systems\\nnavigation\\nimage colour analysis\\nimage sampling\\nrandom processes\\ndata-driven approach\\ndepth sampling\\ndepth acquisition\\nactive illumination\\nautonomous navigation\\nrobotic navigation\\nmechanical templates\\nsampling templates\\nautonomous vehicles\\nsolid-state depth sensors\\nadaptive framework\\nsimple reconstruction algorithm\\ngeneric reconstruction algorithm\\nsampling reconstruction algorithm\\nrandom sampling\\ndepth completion algorithms\\nsingle-pixel prototype sampler\\nrgb sampling strategies\\nsingle depth sampling strategies\\npiecewise planar depth model\\nsuperpixel sampler\\nsps\",\"274\":\"laser radar\\nmeasurement by laser beam\\nmathematical model\\nsensor phenomena and characterization\\nsurface emitting lasers\\nlaser modes\\ncalibration\\ncomputerised instrumentation\\ndifferentiation\\ngradient methods\\noptical radar\\noptical sensors\\noptical tracking\\noptimisation\\nradar receivers\\nparameter estimation\\n2d continuous-wave lidar sensors\\nlight detection and ranging sensors\\ndepth measurements\\nlocalization pipelines\\nautonomous robots\\nperception stack\\nphysics-based simulation\\nsensor measurements\\ngradient-based optimization\\nhokuyo urg-04lx lidar\\nsurface-light interactions\\nphysically plausible model\\nlaser light\\ntime-of-flight cameras\\ndepth sensors\",\"275\":\"image reconstruction\\nmultiplexing\\nthree-dimensional displays\\nsurface reconstruction\\nrobots\\nencoding\\nreliability\\nimage coding\\nimage filtering\\nimage matching\\nimage restoration\\nimage texture\\nobject recognition\\nrobot vision\\nstereo image processing\\nhigh reconstruction accuracy\\nfast image acquisition\\nspatial-temporal multiplexing method\\nmoving objects\\nthree-dimensional reconstruction\\nrobotic applications\\nrobotic recognition\\nspatial-multiplexing time-multiplexing structured-light techniques\\nimage acquisition time\\nspatial-temporal encoded patterns\\ndense 3d surface reconstruction\\ntexture map\\nimage blur\\nhigh-frequency phase-shifting fringes\\nspatial-coded texture\",\"276\":\"correlation\\nradar imaging\\nestimation\\nimage resolution\\nfeature extraction\\nsensors\\ndistance measurement\\nfourier transforms\\nimage matching\\nmotion estimation\\npharao\\ndirect radar odometry\\nscanning radar-based odometry methods\\nradar image\\nfeature-based methods\\nradar scans\\nlog-polar radar images\\nlarge-scale radar data\\nodometry estimation\\nradar-based navigation\\nfourier mellin transform\",\"277\":\"semantics\\nlaser radar\\nimage segmentation\\ntask analysis\\nthree-dimensional displays\\nneural networks\\nautomobiles\\nbayes methods\\ncontrol engineering computing\\nconvolutional neural nets\\nfiltering theory\\nimage sequences\\nlearning (artificial intelligence)\\nmobile robots\\nneural net architecture\\nobject detection\\noptical radar\\npath planning\\nrecursive estimation\\nrobot vision\\nslam (robots)\\ndeeptemporalseg\\ntemporally consistent semantic segmentation\\n3d lidar scans\\nsemantic characteristics\\nautonomous robot operation\\ndeep convolutional neural network\\ndcnn\\nlidar scan\\npedestrian\\nbicyclist\\ndense blocks\\ndepth separable convolutions\\ncurrent semantic state\\nisolated erroneous predictions\\nneural network architectures\\nbayes filter approach\\nkitti tracking benchmark\",\"278\":\"manipulators\\nforce\\ngrippers\\nplanning\\ngrasping\\npayloads\\nmotion control\\npayload limitations\\nsingle arm\\ngrasped object\\nwrenches\\npayload limits\\ndual-arm robot\\nrobot arms changes\\nwrench imbalance\\ndiscrete bimanual manipulation\\ngrasp points\\nbalanced configuration\\nrobot experiments\\ngrasping force\\nwrench balancing\",\"279\":\"encoding\\nneuromorphics\\ntactile sensors\\ncameras\\ntask analysis\\nbiomedical equipment\\nbiomedical optical imaging\\nbiomimetics\\nfeedback\\nhaptic interfaces\\nimage classification\\nimage texture\\nmedical image processing\\nprosthetics\\nskin\\ntouch (physiological)\\nneurotac\\ntexture recognition\\nartificial tactile sensing capabilities\\nrival human touch\\nrobotics\\nbiomimetic tactile sensors\\ngrasping\\nmanipulation tasks\\nbiomimetic hardware design\\ntactip sensor\\nlayered papillae structure\\nhuman glabrous skin\\nevent-based camera\\nspike trains\\ntexture classification task\\nspike coding methods\\nartificial textures\\nspike-based output\\nbiomimetic tactile perception algorithms\\nneuromorphic optical tactile sensor\",\"280\":\"force\\nuncertainty\\nrobot sensing systems\\ntask analysis\\ncalibration\\nhaptic interfaces\\nmanipulators\\npose estimation\\nregression analysis\\nrobotic assembly\\ntrees (mathematics)\\nsphere-tree representation\\nleast-uncertain estimate\\nrelative contact\\nmultiregion complex contacts\\ncontact types\\ncontact locations\\nobject shapes\\nobject poses\\ncomplex shapes\\nforce forecast\\nautonomous robotic manipulation\\nsimulated complex contacts\\nforce sensing\\nconstraint-based haptic simulation algorithm\\nthree-pin peg-in-hole robotic assembly tasks\\ncontact-rich two-pin peg-in-hole assembly tasks\\nregression model\",\"281\":\"grasping\\nforce\\nestimation\\ncameras\\nmathematical model\\nforce measurement\\ndexterous manipulators\\ngrippers\\nrobot vision\\nvisual servoing\\nfingernail imaging\\nvisual tracking system\\nforce collaboration\\nconstrained human grasp forces\\nunconstrained human grasp forces\\n3d fingertip forces\\nrobotic arms\",\"282\":\"robustness\\npose estimation\\ncameras\\ncost function\\nthree-dimensional displays\\ndistance measurement\\noptimisation\\nhigh generality\\nabsolute camera pose\\nmonocular visual odometry\\nbranch-and-bound\\nhigh outlier ratios\\nrobust estimation\\nefficient estimation\\n3d-to-2d point correspondences\\nprojection constraint\\nlocal optimizer\\neffective function bounds\\nreal-time applications\\nsynthetic datasets\\nreal-world datasets\",\"283\":\"collision avoidance\\nuncertainty\\ncameras\\nrobustness\\nsensors\\npredictive models\\nstate estimation\\nhelicopters\\nmobile robots\\npredictive control\\nrobot dynamics\\nrobot vision\\nrobust vision-based obstacle avoidance\\nmicroaerial vehicle\\ndynamic environments\\non-board vision-based approach\\nmoving obstacle\\nefficient obstacle detection\\ntracking algorithm\\ndepth image pairs\\nrobust collision avoidance\\nchance-constrained model predictive controller\\ncollision probability\\naccount mav dynamics\\nobstacle sensing uncertainties\\non-line collision avoidance\",\"284\":\"hardware\\nestimation\\nneural networks\\nmachine vision\\nfeature extraction\\nregisters\\nimage edge detection\\ncollision avoidance\\ncomputer vision\\nimage sensors\\nmicrocontrollers\\nmobile robots\\nrecurrent neural nets\\nrobot vision\\nsensor arrays\\nvlsi\\nneural network output\\nimage capture\\ncontrol system\\ntrained neural network\\nfully connected layer-recurrent\\ntraining data\\ninfrared proximity sensors\\nvision output\\nsparse feature description data\\nfeature algorithms\\npixel processor array chip\\nembedded 256\\u00d7256 processor simd array\\nimage sensor\\nrc model car\\nmicrocontroller\\nscamp-5 vision chip\\nvision system integrating\\nexperimental vehicle\\nblobs\\ncorner points\\nabstract features\\nmonocular vision based proximity estimation system\\nvision features computed\\nvelocity 0.64 m\\/s to 1.8 m\\/s\\ntime 4.0 ms\",\"285\":\"cameras\\noptimization\\ntransmission line matrix methods\\nland vehicles\\nmotion estimation\\nimage registration\\nreal-time systems\\ndistance measurement\\nfeature extraction\\nmobile robots\\noptimisation\\npath planning\\nrobot vision\\nsteering systems\\ntree searching\\nefficient globally-optimal correspondence-less visual odometry\\nplanar ground vehicles\\n2 dof ackermann steering model\\ndownward facing camera\\nsimple image registration problem\\n2-parameter planar homography\\nground-plane features\\nplane-based ackermann motion estimation\\ncorrespondence-based hypothesise\\ntest schemes\\nfronto-parallel motion\\nbranch-and-bound optimisation technique\\nlow-dimensional parametrisation\",\"286\":\"trajectory\\noptimization\\nnavigation\\nplanning\\nrobots\\ncollision avoidance\\ntopology\\naerospace navigation\\naerospace robotics\\ngraph theory\\nmobile robots\\nmonte carlo methods\\nmotion control\\noptimisation\\ntrajectory control\\ngrid-based representations\\noptimization graph\\nlocal planning map\\negoteb\\ntimed-elastic-bands\\nteb hierarchical planner\\nreal-time navigation\\ngoal directed motion\\nmultitrajectory optimization based synthesis method\\ntopologically distinct trajectory candidates\\nfactor graph approach\\negocentric perception space navigation\\negocentric perception space representations\\nmonte carlo evaluations\\nautonomous mobile robot\",\"287\":\"robots\\ntask analysis\\nadaptation models\\nvisualization\\nstacking\\ndata models\\ntraining\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nrobot vision\\nvisual robotic manipulation\\nrobotic visual data\\nreinforcement learning algorithms\\nrobotic learning\\nstate estimation\\nlatent state representation\\ndeep reinforcement learning\\nunlabeled real robot data\\nrobot experience\\ntime-contrastive techniques\\nlearned state representation\\nvision-based reinforcement learning agent\\nstandard visual domain adaptation techniques\\nself-supervised sim-to-real adaptation\\nsequence-based supervised objectives\\ncontrastive forward dynamics loss\",\"288\":\"adaptation models\\ntask analysis\\ntrajectory\\nrobots\\ntraining\\nlearning (artificial intelligence)\\nheuristic algorithms\\nmedical robotics\\ndynamic conditions\\ntask-specific trajectory generation model\\nkuka lbr 4+ robot\\nsim-to-real domain transfer\\nrobotic policy training\\nmeta reinforcement learning\",\"289\":\"hardware\\ndecoding\\ncomputational modeling\\nneural networks\\ntraining\\ntrajectory\\nbenchmark testing\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nvariational auto-regularized alignment\\nsim-to-real control\\ngeneral-purpose simulators\\nvariational autoencoder\\nblack-box simulation\\nlatent space\\nencoder training\\nsimulation parameter distribution\\nmatching parameter distributions\\nabb yumi robot hardware\",\"290\":\"measurement\\ntask analysis\\nheuristic algorithms\\nrobot sensing systems\\nrobust control\\ntrajectory\\ncontrol engineering computing\\nhelicopters\\nlearning (artificial intelligence)\\nmulti-robot systems\\nrobot programming\\nuser experience\\nrobot systems\\nmultisource inter-robot transfer learning\\nquadrotor experiments\\nreal source quadrotor\\nvirtual source quadrotor\\nexperience selection\\ndynamics similarity\\nrobotics literature\\nknowledge transfer\\nlearning process\\nrobust control theory\\ndata-efficiency algorithm\",\"291\":\"training\\nautomobiles\\nrobots\\ncomputational modeling\\ncameras\\nrobustness\\nnavigation\\nintelligent robots\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot dynamics\\nrobot vision\\nreality gap\\njoint perception\\non-demand compute architecture\\ntraining optimal policies\\nrobust evaluation\\ndeep reinforcement learning\\nrobotic control agent\\nraw camera images\\nrobust path planning\\ndeepracer\\nautonomous racing platform\\nsim2real reinforcement learning\\nend-to-end experimentation\\nrl\\nintelligent control systems\\nmonocular camera\\nphysical world\\nrobust reinforcement learning\\nmodel-free learning\",\"292\":\"prosthetics\\ntask analysis\\nwrist\\nergonomics\\nwires\\nrobots\\nmuscles\\nartificial limbs\\nbiomechanics\\nclosed loop systems\\nelectromyography\\nmedical signal processing\\nopen-loop scheme\\nergonomic posture\\ncontrol scheme\\nprostheses users\\nbody compensation\\nclosed-loop control\\nprosthetic level\\ncontrol loop\\ncorrecting errors\\nupper-limb prostheses control\\nprosthetic wrist rotation\\nergonomic control\",\"293\":\"magnetostatics\\nmagnetic separation\\nrobots\\nmagnetic sensors\\nmagnetic devices\\nkalman filters\\nmedical robotics\\noptimisation\\nprosthetics\\nsurgery\\ntelerobotics\\ntracking\\nlocalization algorithms\\noptimization\\nlevenberg-marquardt algorithm\\ntrust region reflective algorithm\\nrobotics applications\\nremote tracking\\nmultiple magnetic targets\\nmyokinetic control interface\\nmagnetic tracking algorithms\\nbiomedical applications\\nteleoperated surgical robots\\nupper limb prostheses\",\"294\":\"sociology\\nstatistics\\nrouting\\nreal-time systems\\npath planning\\nrobots\\nheadphones\\naugmented reality\\nemergency management\\noptimisation\\ncongestion-aware evacuation\\ncongestion-aware routing solution\\nindoor evacuation\\nreal-time individual-customized evacuation routes\\nmultiple destinations\\npopulation density map\\nobtained on-the-fly\\ncongestion distribution\\noptimal solution\\ntime-efficient evacuation route\\nar devices\\nuser-end augmented reality devices\",\"295\":\"virtual reality\\ntrajectory\\ncollision avoidance\\nmanipulators\\nprogramming\\nservice robots\\naugmented reality\\nhuman-robot interaction\\nrobot programming\\nrobotic manipulator programming\\nmixed reality\\ninteractive programming\\nhololens glasses\\nrobotic operation system\\nrobotic manipulators\\nrobot location\\npoint cloud analysis\\nvirtual markers\\nmenus\\npick-and-place operation\\ncontact operations execution\\nur10e robot\\nkuka iiwa robot\",\"296\":\"heart rate\\nrobot sensing systems\\nradar\\nmonitoring\\nestimation\\nlegged locomotion\\ncardiology\\nconvolutional neural nets\\nhealth care\\nmedical robotics\\nmedical signal processing\\npatient monitoring\\npatient rehabilitation\\ntelemedicine\\nmbeats features\\ndeep neural network predictor\\nrobust operation\\npost-operative rehabilitation\\nheart rate sensing\\nrobot mounted mmwave radar\\npost-operative recovery\\nnoncontact heart rate monitoring\\nstatic wall-mounted device\\nmillimeter wave radar system\\nperiodic heart rate measurements\\nusers daily activities\\nmmwave servoing module\",\"297\":\"training\\ninstruments\\nfoot\\nlegged locomotion\\nprediction algorithms\\nfootwear\\nneural networks\\nbiomedical measurement\\ngait analysis\\nmedical computing\\nneural nets\\npatient rehabilitation\\nencoder-decoder\\nrmse\\nroot mean square error\\ninertial measurement unit\\ninstrument standard footwear\\noverground walking\\ncontinuous gait cycle\\nwalking over-ground\\ngait rehabilitation\\ngait parameters\\ntraditional gait measurement systems\\ngait abnormalities\\ngait training\\nartificial neural networks\\ninstrumented shoes\\ngait cycle percentage\",\"298\":\"conferences\\nautomation\\nflow control\\nhumanoid robots\\nlegged locomotion\\npressure control\\nrobot dynamics\\nvalves\\nhydraulic direct-drive system\\nbiped humanoid robot\\nhydraulic direct drive system\\nsimple equipment configuration\\nsingle-rod cylinder\\nflow rate\\npassive flow compensation valve\\nvalve state\",\"299\":\"grippers\\nkinematics\\ngrasping\\nactuators\\nsprings\\nsteel\\nrobots\\nbending\\nminiature origami grippers\\nrobotic gripper design\\ncustomizable grasping tasks\\nminiature fingers\\nsingle actuator input\\ngrasping tasks\",\"300\":\"conferences\\nautomation\\nautonomous underwater vehicles\\nbiomechanics\\nbone\\nelasticity\\nmobile robots\\nmuscle\\nrobot dynamics\\nservomotors\\nunderwater vehicles\\nfish robots\\nbody stiffness\\nfish swimming\\nmechanical stiffness\\ntensegrity-based underwater robots\\ntensegrity class\\nelastic cables\\nrigid body segments\\nbody shape\\ntensegrity systems\\nfish-like robots\\nbio-inspired tensegrity fish robot\",\"301\":\"gaussian processes\\nbiological system modeling\\nspatiotemporal phenomena\\ngraphical models\\ndata models\\nrobots\\noceans\\nbayes methods\\nimage classification\\nmobile robots\\npath planning\\nrobot vision\\nvectors\\nlow dimensional vector observations\\ngaussian-dirichlet random fields\\nhigh dimensional categorical observations\\nspatio-temporal distribution\\nimaging sensor\\nimage classifier\\ndirichlet distributions\\nhigh dimensional categorical measurements\\ntaxonomic observations\\ninformative path planning techniques\",\"302\":\"prototypes\\nbifurcation\\nrobots\\nmathematical model\\nshape\\nreliability\\ntopology\\nactuators\\nmechanical stability\\nmobile robots\\nmotion control\\nnumerical analysis\\nrobot dynamics\\nrobot kinematics\\nvibration control\\nlocomotion characteristics\\nactuation strategy\\ncompliant tensegrity structure\\nmultistable tensegrity robot\\nmultiple stable equilibrium configurations\\ntilting locomotion system\",\"303\":\"soft robotics\\nmanipulators\\nforce\\nsprings\\nkinematics\\nshape\\nactuators\\nbiomechanics\\ndexterous manipulators\\nelasticity\\nmanipulator dynamics\\nposition control\\nservomotors\\nvariable stiffness\\nbistable structure\\nunstructured environments\\ndynamic environments\\nhighly dissipative nature\\nelastic materials results\\nload capability\\nrigid joints\\ncompliant bistable structures\\nbending stiffness\\narticulated soft robot\\nforce transmission\\nposition accuracy\\nmechanical constrain\\nconstruction method\\nservomotor\\nvariable workspace\\ndexterous manipulation\\ntip load\\nlocking function\",\"304\":\"manipulators\\nmuscles\\ngrippers\\nkinematics\\nhydraulic systems\\nactuators\\nbending\\nelasticity\\nmanipulator dynamics\\nmuscle\\npneumatic actuators\\nunderwater continuum manipulator\\ncompliant materials\\nmckibben water hydraulic artificial muscle\\nwham\\nmechanical properties\\nkinematics model\\nsoft grippers\\nbending procedure\\ndisgorging procedure\\nmouth-tongue collaborative soft robot\\nsingle-segment soft robot arm\\nswallowing procedure\",\"305\":\"mobile robots\\nwheels\\ngears\\nmanipulators\\ndc motors\\nkinematics\\nlegged locomotion\\nmotion control\\ncontinuously deformable slender body structure\\nsalamanderbot\\ncable-driven bellows-like origami module\\npowered wheels\\norigami structure\\nsoft-rigid composite continuum mobile robot\\nexploration applications\\nmobile soft robots\\nyoshimura crease pattern\\nvelocity 303.1 mm\\/s\\nradius 79.9 mm\",\"306\":\"joints\\nfasteners\\nthumb\\nellipsoids\\nrobots\\nprototypes\\nceramics\\nbiomimetics\\nbone\\ncontrol system synthesis\\ndexterous manipulators\\nend effectors\\nmanipulator kinematics\\nmotion control\\nposition control\\nsurgery\\nflexure hinge-based biomimetic thumb\\nrolling-surface metacarpal joint\\ngrasping\\ndexterous manipulation\\nkinematic multiplicity\\nrobotic hand\\nkinematic model\\nsurgical techniques\\nmotion capture data\\nend effector\\ntask-space velocities\\ntendon excursion velocity\\nhuman thumb state contribution\\ndata representation\\neffector velocity\",\"307\":\"force\\nwheels\\nsurface topography\\nsurface impedance\\nland vehicles\\nkinematics\\ndrag\\nfriction\\nmobile robots\\noff-road vehicles\\noptimisation\\nremotely operated vehicles\\nrobot dynamics\\nstability\\nvehicle dynamics\\nreconfigurable ground vehicle\\nadaptive terrain navigation capability\\nunmanned ground vehicle\\ndynamic wheelbase\\nadaptive thrust\\nfriction optimization\\nsteep slopes\\nslippery surfaces\\nimpedance-based stabilization module\\nmechanical oscillatory transients\\nibex\",\"308\":\"collaboration\\nthree-dimensional displays\\nsimultaneous localization and mapping\\ncameras\\nrobot vision systems\\ngroupware\\nimage fusion\\nmobile robots\\nmulti-robot systems\\npath planning\\nsensor fusion\\nslam (robots)\\ndynamic collaborative mapping\\nmultimodal environmental perception\\nheterogeneous sensor fusion model\\nlocal 3d maps\\nnight rainforest\\n3d map fusion missions\\nmultimodal sensors\\nlong-term operation\\ncollaborative robots\\ndynamic environment\\ndynamic objects\",\"309\":\"wheels\\ngravity\\nacceleration\\nmathematical model\\ntorque\\naxles\\nactuators\\nmobile robots\\nmotion control\\nmotor drives\\nroad vehicles\\nrobot dynamics\\nvehicle dynamics\\nsloped terrain\\nwheel rotation\\nplain centre hub drive\\nactive ride height selection\\nwheel radius manipulation\\nlocomotion generation\\nslope traversability\\nwheel pose control\\ncentre of gravity manipulation\",\"310\":\"hazards\\nnavigation\\nspace vehicles\\nautonomous robots\\ncomputer architecture\\ncameras\\ntrajectory\\naerospace navigation\\naerospace robotics\\ndistance measurement\\nmars\\nmobile robots\\noptimal control\\npath planning\\nplanetary rovers\\nrobot vision\\nslam (robots)\\ntrajectory control\\ngnc architecture\\nautonomous navigation\\nmars exploration missions\\nsample fetching rover\\nautonomous capabilities\\ntwo-level architecture\\nterrain\\nlocal path replanning\\nglobal localization\\nplanetary exploration\\nplanetary analog field test campaigns\\nguidance navigation and control architecture\\nhazard detection\\nvisual odometry\\nadaptive slam algorithm\\noptimal path planning\",\"311\":\"adaptation models\\nface recognition\\nsurveillance\\nrobot vision systems\\ntraining data\\nsmart homes\\nmedical services\\nfeature extraction\\nhome automation\\nunsupervised learning\\nvideo surveillance\\ntriplet network\\naugmentation network\\nidentity-aware features\\nface samples\\nidentity-irrelevant features\\nhome robot applications\\nface recognition system\\nsmart home environment\\nenvironment-specific face recognition model\\nself-augmentation\\nhealthcare application\\ncamera position\\nsurveillance video\\nidentity-aware feature extraction\\nspatiotemporal characteristic\\nface image disentanglement\",\"312\":\"activity recognition\\nhidden markov models\\nrobot sensing systems\\nfeature extraction\\nclustering algorithms\\nassisted living\\nmobile robots\\nrobot vision\\nactivity recognition solutions\\nhuman-object interaction\\nlow-level actions\\ngoal recognition algorithm\\nlow-cost robotics platform\\nassistive robots\\nmobile robot assistants\\ndaily living activities\\nrgb-d camera\\nplan recognition\\ncomputer vision\\nrobotic assistant\\nactivities for daily living\\nobject affordance\\nparticle filter\",\"313\":\"image edge detection\\nsemantics\\nimage segmentation\\nimage retrieval\\nfeature extraction\\ngeometry\\nedge detection\\nimage matching\\nimage texture\\nobject recognition\\nwavelet transforms\\nmultiseason environment-monitoring datasets\\nurban scenes\\nimage-based place recognition\\nbucolic environment\\nsemantic edge description\\nurban environments\\nnatural scenes\\nsemantic content\\nvegetation state\\nbucolic scene\\nglobal image description\\nsemantic information\\ntopological information\\nmatching two images\\nsemantic edge transforms\\nstate-of-the-art image retrieval performance\",\"314\":\"sensors\\nrobots\\nmuscles\\nfeature extraction\\nforce\\ncatheters\\nsurgery\\ncontrol engineering computing\\nhuman-robot interaction\\nmanipulators\\nmedical computing\\nmedical robotics\\nsensor fusion\\nmultilayer-multimodal fusion architecture\\npattern recognition\\nnatural manipulations\\npercutaneous coronary interventions\\nrobotic systems\\nrobot-assisted procedures\\nhuman-robot interfaces\\nguidewire manipulations\\nmultimodal behaviors\\nrule-based fusion algorithms\\nsinglelayer recognition architecture\\nrobot-assisted pci\\nhri\\nx-ray radiation reduction\\nmedical staff\",\"315\":\"simultaneous localization and mapping\\ncost function\\ngoogle\\njacobian matrices\\ngaussian distribution\\ngraph theory\\nleast squares approximations\\nmobile robots\\nnormal distribution\\nrobot vision\\nslam (robots)\\noccupancy grid map\\ngraph-based slam\\noccupancy normal distribution transforms\\nnormal distributions transforms\\nmobile robotics\\nleast squares problem\\nnonlinear optimizers\\nglobal ndt scan matcher\",\"316\":\"three-dimensional displays\\nskeleton\\nhidden markov models\\nstrain\\ntopology\\nsimultaneous localization and mapping\\nagriculture\\ncameras\\nimage registration\\nrobot vision\\nspatio-temporal nonrigid registration\\n3d point clouds\\nsensor data\\nagricultural robotics\\nplant science\\nagricultural tasks\\nautomated temporal plant-trait analysis\\nplant performance monitoring\\nplant registration\",\"317\":\"estimation\\nbundle adjustment\\nmotion measurement\\nvelocity measurement\\nuncertainty\\nsensor fusion\\ndistance measurement\\nimage fusion\\njacobian matrices\\nmotion estimation\\nvisual-inertial odometry\\ninertial measurement units\\nscale-aware estimation\\nsensor states\\nsensor motion\\nnoninformative inertial measurements\\nestimation modes\\nmotion characteristics\\nuncertainty-based adaptive sensor fusion\\nuncertainty-based sensor fusion\\nrelative motion estimation\\nobservability\",\"318\":\"laser radar\\nfeature extraction\\nthree-dimensional displays\\nmeasurement by laser beam\\nlaser noise\\nreal-time systems\\nspinning\\ndistance measurement\\nmobile robots\\noptical radar\\npath planning\\nrobot vision\\nslam (robots)\\nfov\\nautonomous vehicles\\nautonomous navigation\\nloam algorithm\\nlidar odometry and mapping\\nrobot pose localization\",\"319\":\"simultaneous localization and mapping\\nthree-dimensional displays\\nuncertainty\\nconferences\\nautomation\\nsonar\\nplanning\\ngraph theory\\nmobile robots\\nnavigation\\npath planning\\nrobot vision\\nslam (robots)\\nunderwater volumetric exploration\\nactive slam framework\\n3d underwater environments\\nmultibeam sonar\\nintegrated slam\\nvolumetric free-space information\\ninformative loop closures\\nnavigation policy\\n3d visual dictionary\\nsubmap saliency\\nsensor information\\npose-graph slam formulation\\nglobal occupancy grid map\\nuncertainty-agnostic framework\",\"320\":\"simultaneous localization and mapping\\nrobot kinematics\\ncameras\\nsynchronization\\ntrajectory\\nmobile robots\\npath planning\\npose estimation\\nrobot vision\\nservice robots\\nslam (robots)\\ndata sequences\\nrobotic autonomy\\nreal-world indoor scenes\\nopenloris-scene datasets\\nslam problems\",\"321\":\"inertial navigation\\nthree-dimensional displays\\nrobustness\\nestimation\\ntrajectory\\ntask analysis\\ngravity\\ncomputer vision\\nimage motion analysis\\nneural nets\\nrobust neural inertial navigation\\ndata-driven inertial navigation research\\nhorizontal positions\\nmoving subject\\nimu sensor data\\nground-truth 3d trajectories\\nnatural human motions\\nronin\",\"322\":\"knowledge engineering\\ncomputational modeling\\nreal-time systems\\ncomputer architecture\\nsemantics\\nuncertainty\\nvideos\\nconvolutional neural nets\\nimage classification\\nimage segmentation\\nobject detection\\nvideo signal processing\\nlightweight segementation models\\n2k-videos\\nhigh resolution videos\\nnfs-segnet\\ncomputation-efficiency\\nencoder network\\nnfs-net\\nsimple building blocks\\nmemory-heavy operations\\ndepthwise convolutions\\ncnn architectures\\nasymmetric architecture\\ndeeper encoder\\nuncertainty-aware knowledge distillation method\\nrealtime semantic segmentation network\\ncityscape benchmark\\ncomputer speed 24.3 gflops\",\"323\":\"video sequences\\ncameras\\nthree-dimensional displays\\nobservers\\ntask analysis\\ntraining\\ncomputer vision\\nconvolutional neural nets\\nimage reconstruction\\nimage sequences\\nlearning (artificial intelligence)\\nrecurrent neural nets\\nstereo image processing\\nvideo signal processing\\ngeometric feature\\nimage processing\\nscene understanding\\nautonomous vehicle navigation\\ndriver assistance\\nsemantic interpretation\\ndynamic environments\\nconvolutional neural networks\\nresidual convolutional lstm\\ntemporally consistent horizon line estimation\\n3d reconstruction\\ncnn architecture\\nadaptive loss function\",\"324\":\"sonar\\ntraining\\nsemantics\\ndata models\\ngallium nitride\\ntraining data\\nmarkov processes\\nacoustic signal processing\\nautonomous underwater vehicles\\ndata analysis\\nenvironmental factors\\nnaval engineering computing\\nneural nets\\nobject recognition\\nrealistic images\\nsonar imaging\\nstatistical analysis\\nbootstrapping atr systems\\nautonomous target recognition systems\\nrealistic synthetic sonar imagery\\nmarkov conditional generative adversarial networks\\ncontinuous synthetic sonar data generation\\nmarkov conditional pix2pix\\nacoustic sensors\",\"325\":\"search problems\\nimage edge detection\\napproximation algorithms\\nplanning\\nheuristic algorithms\\ndatabases\\npath planning\\nsampling methods\\ntrees (mathematics)\\nheuristic estimates\\nait*\\nasymmetric bidirectional search\\noptimal path planning\\ninformed sampling-based planning algorithm\\nadaptively informed trees\",\"326\":\"planning\\nmanifolds\\ntask analysis\\nlead\\nprobabilistic logic\\nrobot motion\\ncontinuous systems\\ndiscrete systems\\nmanipulators\\nmultimodal planning\\nrobotic manipulation problems\\ncontinuous infinity\\nobject grasping\\nmanipulation plan\\nsingle-mode motions\\nvalid transitions\\nmanipulation planners\\nmultimodal structure\\nmode-specific planners\\ngeneral layered planning approach\\npick-and-place manipulation domain\\nsynergistic discrete leads\\nspecific mode transitions\\nuseful mode transitions\\nbias search\",\"327\":\"planning\\nrobot sensing systems\\ncameras\\noctrees\\nthree-dimensional displays\\nsurface treatment\\nautonomous aerial vehicles\\nhierarchical systems\\nimage resolution\\nimage sampling\\nmobile robots\\npath planning\\nrobot vision\\ncomplex three-dimensional environment\\nnooks\\ncrannies\\ncoverage planning\\nmultiresolution hierarchical framework\\nthree-dimensional scenes\\nhierarchical coverage path planning\\nlightweight uav\\nlow-level sampling\\ncomplex 3d environments\",\"328\":\"trajectory\\ncameras\\nplanning\\ntask analysis\\naerodynamics\\nheuristic algorithms\\nnavigation\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\noptimisation\\npath planning\\nperception-aware time optimal path parameterization\\nquadrotors\\nperception-aware time optimal path parametrization\\nquadrotor systems\\non-board navigation\\nestimation algorithms\\nefficient time optimal path parametrization algorithm\\nquadrotor platform\\nvision-driven vehicles\",\"329\":\"trajectory\\nuncertainty\\nplanning\\nsafety\\nautonomous vehicles\\nsplines (mathematics)\\nmobile robots\\nmotion estimation\\nobject detection\\npath planning\\nroad traffic\\nroad vehicles\\nvehicles\\nautonomous vehicle\\nego vehicle\\nvisibility-aware planning\\nvisibility-aware trajectories\\nproactive motion planning\\ncooperative motion planning\\npartially-occluded intersection\\nemergent behavior\",\"330\":\"soft robotics\\nstrain\\ndeformable models\\nplanning\\nkinematics\\npneumatic systems\\nbending\\ncollision avoidance\\nmobile robots\\nmotion control\\nreliable robot-environment interaction models\\nobstacle-interaction model\\nrobot tip\\nobstacle-interaction planning method\\nactuated vine robot navigation\\nwrinkling deformation\\nbending deformation\",\"331\":\"topology\\ndecentralized control\\nprotocols\\nunmanned aerial vehicles\\ntracking loops\\nheuristic algorithms\\nattitude control\\nautonomous aerial vehicles\\ncontrol system synthesis\\ndecentralised control\\ndistributed control\\nmulti-robot systems\\nposition control\\ntracking\\ntrees (mathematics)\\nmultiple uavs\\nconstrained environment\\nconsensus problem\\nmultiple unmanned aerial vehicles\\nenvironmental constraints\\ngeneral communication topology\\ndirected spanning tree\\nposition transformation function\\ndynamic reference position\\nyaw angle\\nasymmetric topology\\nlocal tracking controller\\ndistributed consensus control\",\"332\":\"vehicle dynamics\\naerodynamics\\nneural networks\\nrotors\\nstability analysis\\ntraining\\nrobots\\naerospace robotics\\ncontrol system synthesis\\ndecentralised control\\nlearning (artificial intelligence)\\nmulti-robot systems\\nneurocontrollers\\nnonlinear control systems\\nparticle swarm optimisation\\nstability\\nclose-proximity multirotor control\\nlearned interactions\\nneural-swarm\\nnonlinear decentralized stable controller\\nclose-proximity flight\\nmultirotor swarms\\nclose-proximity control\\ncomplex aerodynamic interaction effects\\nsafety distances\\nnominal dynamics model\\nregularized permutation-invariant deep neural network\\nhigh-order multivehicle interactions\\nlarger swarm sizes\\nbaseline nonlinear\\nstable nonlinear tracking controller\",\"333\":\"roads\\ntask analysis\\nrobot sensing systems\\nrouting\\nheuristic algorithms\\npartitioning algorithms\\ncomputational complexity\\ngraph theory\\ninteger programming\\nlinear programming\\nmobile robots\\nmulti-robot systems\\npath planning\\nrobot tour generation\\nmultiple robots\\nline coverage problem\\nmixed integer linear program\\nnp-hard\\nmerge-embed-merge\\nmem algorithm\\ngraph simplification\\ngraph partitioning\",\"334\":\"visualization\\nmonitoring\\nrobot sensing systems\\nswitches\\nspace missions\\naircraft control\\nhelicopters\\nimage sensors\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nquadcopters\\nvisual sensors\\ncoverage holes\\ncoverage quality\\nsufficient conditions\\nnonsmooth barrier functions\\nvisual coverage maintenance\\ncoverage control\\nnecessary conditions\",\"335\":\"roads\\npredictive models\\ntrajectory\\ntopology\\ngeometry\\ntask analysis\\nautonomous vehicles\\nmobile robots\\nmotion estimation\\nprediction theory\\nroad safety\\nroad traffic\\nroad vehicles\\nrobot vision\\ntraffic engineering computing\\ncomplex road networks\\nmapped road topology\\ndynamic road actors\\nmapped lane geometry\\nmode collapse problem\\ngoal-directed occupancy prediction\\nlane-following actors\\nshared roads\\nsafe autonomous driving\\npossible vehicle behaviors\\npossible goal reasoning\\nlocal scene context multimodality\\nhigh-level action set\\nfuture spatial occupancy prediction\",\"336\":\"navigation\\ntrajectory\\nrobots\\nprediction algorithms\\ntraining\\ncollision avoidance\\nuncertainty\\nadaptive control\\ncontrol engineering computing\\nmobile robots\\npedestrians\\nroad traffic control\\ntraffic engineering computing\\npersonal space violation\\nadaptive navigation policy\\npedestrian motion\\nintent-aware pedestrian prediction\\nadaptive crowd navigation\\npedestrian rich environments\\nrobotic assistance\\npedestrian navigation\\nreal-world pedestrian datasets\",\"337\":\"cameras\\nsensors\\nglobal positioning system\\nglobal navigation satellite system\\nreceivers\\nlaser radar\\nsynchronization\\ninfrared detectors\\nmobile robots\\noptical radar\\nslam (robots)\\nwuxga cameras\\n3d lidar\\ninertial measurement unit\\ninfrared camera\\ndifferential rtk gnss receiver\\ncentimetre accuracy\\npublic dataset\\nsubmillisecond precision\\nautonomous driving\\nbrno urban dataset\\nself-driving agents\\nmapping tasks\\nbrno-czech republic\\nhttps:\\/\\/github.com\\/roboticsbut\\/brno-urban-dataset\",\"338\":\"planning\\ndecision making\\nuncertainty\\nsemantics\\nvegetation\\naerospace electronics\\nsafety\\ndecision theory\\nmarkov processes\\nmulti-agent systems\\nroad traffic\\ntrees (mathematics)\\ndense traffic scenarios\\nautomated vehicles\\nstochastic behaviors\\ntraffic participants\\nperception uncertainties\\npartially observable markov decision process\\nefficient uncertainty-aware decision-making\\nlongitudinal behaviors\\ncomplex driving environments\\nautomated driving\\nguided branching\\ndomain-specific closed-loop policy tree structure\\ndcp-tree\\nconditional focused branching mechanism\\ncfb\\ndomain-specific expert knowledge\",\"339\":\"learning (artificial intelligence)\\nmeteorology\\nrobustness\\ntask analysis\\nnavigation\\ntraining\\nautonomous vehicles\\ngeneralisation (artificial intelligence)\\nintelligent robots\\nroad vehicles\\nsensor fusion\\nsteering systems\\ntraffic engineering computing\\npretrained ipp model\\ncarla driving benchmark\\ngeneralization capability\\npure pursuit\\nautonomous urban driving navigation\\ntwo-stage framework\\nvisual information\\npure-pursuit method\\nsteering angle\\nimitation learning performance\\ndriving data\\nreinforcement learning method\\ndeep deterministic policy gradient\\nipp-rl framework\",\"340\":\"solid modeling\\nsemantics\\nthree-dimensional displays\\nautonomous vehicles\\ntraining\\ncameras\\npipelines\\naugmented reality\\ndriver information systems\\nimage segmentation\\nlearning (artificial intelligence)\\nobject detection\\npedestrians\\nroad vehicles\\ntraffic engineering computing\\nadversarial appearance learning\\naugmented cityscapes\\npedestrian recognition\\nautonomous driving area synthetic data\\ntraffic scenarios\\nautonomous vehicle\\ndata augmentation\\ncityscapes dataset\\nvirtual pedestrians\\naugmentation realism\\ngenerative network architecture\\ndata-set lighting conditions\\nvru\",\"341\":\"feature extraction\\nthree-dimensional displays\\nlaser radar\\nheuristic algorithms\\nvehicle dynamics\\nrobots\\nurban areas\\nbayes methods\\ndistance measurement\\nimage filtering\\nimage matching\\nimage registration\\nimage sampling\\nmonte carlo methods\\noptical radar\\npose estimation\\nrobot vision\\nroi-cloud\\nkey region extraction method\\nlidar odometry\\nlidar scan\\non-board imu\\/odometry data\\nbayes filtering\\nmonte carlo sampling\\nautonomous robot\\nlidar localization\\nvoxelized cube set\\npoint set registration\\nmassive point cloud data\",\"342\":\"cameras\\nvisualization\\nthree-dimensional displays\\npipelines\\npose estimation\\nimage retrieval\\nfeature extraction\\nimage representation\\nlearning (artificial intelligence)\\nneural nets\\nrobot vision\\nslam (robots)\\nvisual localization\\nscene-specific representations\\nrelative pose estimation\\nfeature-based approach\\ndeep learning\\ncamera\\nautonomous robots\",\"343\":\"databases\\nvisualization\\nfeature extraction\\nrobots\\npipelines\\nhistograms\\nimage fusion\\nmobile robots\\nobject recognition\\nrobot vision\\nhierarchical multiprocess fusion\\nmultiple complementary techniques\\nvisual localization\\nmultisensor fusion\\nvarying performance characteristics\\nhierarchical localization system\\nlocalization hypotheses\\nlocalization performance\\nfinal localization stage\\nparallel fusion\\nvisual place recognition\",\"344\":\"lighting\\ncameras\\nvisualization\\nindoor environments\\nestimation\\nmathematical model\\nadaptation models\\nfeature extraction\\nimage colour analysis\\nimage sequences\\nindoor environment\\nobject tracking\\nrendering (computer graphics)\\ndirect dense camera tracking approach\\nlighting adaptable map representation\\nlighting invariant map representations\\nlighting conditions\\nvisual localization methods\",\"345\":\"quantization (signal)\\nvisualization\\nindexes\\nrobots\\nprincipal component analysis\\nbenchmark testing\\nimage recognition\\nimage matching\\nimage representation\\nimage sequences\\nmobile robots\\nquantisation (signal)\\nrobot vision\\nstorage management\\nhashing overload approach\\ncompact place recognition\\noverloaded representations\\nstorage footprint\\nplace recognition system\\nultra-compact place representations\\nsublinear storage scaling\\nsublinear computational scaling\\nvisual place recognition\\nmatch selections\\nsequence-based matching\\nscalar quantization-based hashing\\nmobile robot\",\"346\":\"robot kinematics\\nrobot sensing systems\\nnoise measurement\\nprobabilistic logic\\ntask analysis\\ncameras\\naerospace robotics\\nmicrorobots\\nmobile robots\\nmulti-robot systems\\npose estimation\\nprobability\\nrobot vision\\nslam (robots)\\ntarget tracking\\nmultimav system\\nrobot team\\nvision based detection\\ndistance measurements\\ncoupled probabilistic data association filter\\nnonlinear measurements\\nvisual based robot to robot detection\\nvision based multimav localization\\nrobot localization\\nrobot pose estimation\\nmultiple microaerial vehicles\",\"347\":\"training\\nrobots\\ntask analysis\\nencoding\\ndecoding\\nheuristic algorithms\\nadaptation models\\nlearning (artificial intelligence)\\nrobot dynamics\\nmanga\\nmultiple environments\\ndynamics parameters\\nmotor noise variations\\npolicy learning\\nsystem identification\\nunknown environment\\ndynamics configurations\\ndynamics conditioned policies\\noff-policy state-transition rollouts\\ntraining method\\nmethod agnostic neural-policy generalization and adaptation\\ntransferring policies\",\"348\":\"navigation\\ncollision avoidance\\nbayes methods\\ntraining\\nrobot kinematics\\nmachine learning\\nlearning (artificial intelligence)\\nmobile robots\\nrobust control\\nfast adaptation\\ndeep reinforcement learning-based navigation skills\\nhuman preference\\nrobot navigation\\nrobustness\\nmaximum velocities\\nreward components\\noptimal choice\\nreal-world service scenarios\\ndeep rl navigation method\\nreward functions\\nbayesian deep learning method\\npreference data\\ndiverse navigation skills\\ndeep rl navigation agents\",\"349\":\"robots\\nmixture models\\nkinematics\\nbayes methods\\noptimization\\ntask analysis\\nplanning\\napproximation theory\\ncontrol engineering computing\\ninference mechanisms\\nstatistical distributions\\nvariational inference\\nmixture model approximation\\nrobot configurations\\nbayesian computation\",\"350\":\"microsensors\\nfluorescence\\nsemiconductor lasers\\nheating systems\\nmeasurement by laser beam\\nlaser excitation\\ntemperature measurement\\nbiomedical optical imaging\\ncellular biophysics\\ndyes\\ninfrared spectra\\nkidney\\nrefractive index\\nrhodamine b\\ntemperature-sensitive fluorescent dye\\npolystyrene particle\\ncell injection\\nlaser manipulation\\nmultiple wavelengths\\nfluorescent microsensor\",\"351\":\"legged locomotion\\nwheels\\nsynchronization\\ncouplers\\nrobot kinematics\\ngait analysis\\nmotion control\\nrobot dynamics\\nsprings (mechanical)\\nsynchronisation\\nextra robotic legs system\\nrobotic augmentation\\nhuman operator\\narticulated robot legs\\nhuman-xrl quadruped system\\nrear legs\\nquadrupedal robots\\nquadrupedal locomotion\\ncoupler design parameters\\npassive quadrupedal gait synchronization\\ndynamically coupled double rimless wheel system\\npoincar\\u00e9 return map\\nnumerical simulation\\nhuman augmentation\\nsupernumerary robotic limbs\\nexoskeletons\\nlocomotion\\nnonlinear dynamics\",\"352\":\"limit-cycles\\nlegged locomotion\\nperturbation methods\\nmathematical model\\noscillators\\nconvergence\\ntrajectory\\nlimit cycles\\nnumerical analysis\\nrobot dynamics\\nstability\\noptimal fast entrainment waveform\\nphase recovery\\nphase reduction theory\\nentrainment effect\\nlimit cycle walking\\nindirectly controlled limit cycle walker\\nexternal disturbances\\noccasional perturbation\\nclosed orbit\\nphase space\\nsuccessive perturbation\\naccumulated deviation\\ncontrol law\\ndisturbed phase\\nwobbling mass\\nwobbling motion\",\"353\":\"collaboration\\nrobot kinematics\\nobject recognition\\noptimization\\nrobot sensing systems\\nrobustness\\nconcave programming\\ngraph theory\\nimage matching\\nimage representation\\nmulti-robot systems\\nobject detection\\nrobot vision\\ncollaborative multirobot perception\\nnonconvex noncontinuous optimization problem\\nmultirobot coordination\\ncollaborative robot perception\\nhypergraph matching approach\",\"354\":\"sensors\\ntarget tracking\\nkalman filters\\nmicrosoft windows\\nestimation\\ntrajectory\\noptimization\\ncameras\\nmaximum likelihood estimation\\nvehicular ad hoc networks\\nwireless sensor networks\\nconsensus kalman filter\\nfixed communication bandwidth\\nhigh fidelity urban driving simulator\\nautonomous cars\\ntime-varying communication network\\ndistributed multitarget tracking\\nautonomous vehicle fleets\\nscalable distributed target tracking algorithm\\nalternating direction method of multipliers\\nvehicle-to-vehicle network\\nsensing vehicle\\nkalman filter-like update\\ncentralized maximum a posteriori estimate\\ncarla\\non-board cameras\",\"355\":\"batteries\\nswitches\\nlegged locomotion\\nswitching circuits\\naerodynamics\\nconnectors\\npropellers\\naerospace control\\nhelicopters\\nmobile robots\\nsecondary cells\\nin-flight battery switching\\nmultirotor flight time\\nmid-air docking\\nprimary battery\\nquadcopter flight\\ndocking platform\\nflying battery\\nsecondary battery\\narbitrary switching\",\"356\":\"springs\\nactuators\\ntorque\\npower demand\\nforce\\nrobots\\ngears\\nclutches\\nelectric motors\\nenergy consumption\\ninteger programming\\nmobile robots\\noptimal control\\npower consumption\\nquadratic programming\\nsprings (mechanical)\\nactuator power consumption\\nmobile robot design\\nelastic energy\\nelectrical energy consumption\\ngiven actuator design\\noptimized actuator energy consumption\\noptimized gear motor\\nsimulated energy-recycling actuator\\nmobile robotics applications\\noptimization and optimal control\\nforce control\\nprosthetics and exoskeletons\",\"357\":\"collision avoidance\\nrobots\\ntrajectory optimization\\nplanning\\ndynamics\\ncontinuous time systems\\nconvex programming\\nmobile robots\\nnonlinear control systems\\npredictive control\\ntrajectory control\\nnmpc approach\\ncontinuous time collision avoidance\\nkinodynamic feasibility\\nnonlinear model predictive control\\nconvex inner approximation\\nonline motion planning\\ncontinuous time collision free trajectories\",\"358\":\"grasping\\nrobot sensing systems\\nimage representation\\nproposals\\ngrippers\\ntask analysis\\nconvolutional neural nets\\nfeature extraction\\nimage colour analysis\\nlearning (artificial intelligence)\\nrobot vision\\nvectors\\naction image representation\\nzero real world data\\nend-to-end deep-grasping policy\\ngrasp quality\\nobject-gripper relationship\\ndeep convolutional network\\ncolor images\\ndepth images\\ncombined color-depth\\nscalable deep grasping policy learning\\nsalient feature extraction\",\"359\":\"predictive models\\nsolid modeling\\ngrippers\\nrobots\\nthree-dimensional displays\\nfeature extraction\\nimage edge detection\\nimage colour analysis\\nimage resolution\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\npose estimation\\nrobot vision\\nparallel plate gripper\\nneural network\\npixel location\\nnonmaximum suppression\\nsampling grasps\\nlearning-based grasp pose detection scheme\\nchannels images\\nresolution rgb image\\nrobot grasping success rate\\ncornell grasp dataset\\npredicted grasps\\ndense predictions\\ndense grasp predictions\\nranking procedures\\nclustering procedures\\nnms\\nnonmaximum suppression strategy\\ndetection model\\ngenerating grasp proposals\\nintermediate procedures\\ndetection accuracy\\nfine-tuning steps\\ndetection algorithms\",\"360\":\"grasping\\nclutter\\ncameras\\nimage segmentation\\nrobot vision systems\\nimage colour analysis\\nlearning (artificial intelligence)\\nmanipulators\\nrobot vision\\nstereo image processing\\ncluttered scenes\\nreinforcement learning framework\\n3d vision architectures\\nrgb-d cameras\\n3-stage transferable active grasping pipeline\\nreal embodied dataset\\nred\",\"361\":\"three-dimensional displays\\nfeature extraction\\ngrasping\\nmeasurement\\ntraining\\ncameras\\npipelines\\nedge detection\\nimage annotation\\nimage colour analysis\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nrobot vision\\npointnet++ grasping\\nsparse point clouds\\nrobot manipulation\\nlocal feature extractor\\ndeep learning\\nmultiobject scene\\nmultiobject dataset\\ngrasp generation algorithm\\nmultiobject grasp detection algorithm\\nferrari-canny metrics\\npointnet++ based network\",\"362\":\"three-dimensional displays\\ngeometry\\nestimation\\nsolid modeling\\ncameras\\nimage edge detection\\noptimization\\ncontrol engineering computing\\nconvolutional neural nets\\nimage colour analysis\\nimage reconstruction\\nimage representation\\nimage sensors\\nlearning (artificial intelligence)\\nmanipulators\\nrobot vision\\nstereo image processing\\ncleargrasp\\nmonocular depth estimation baselines\\ntransparent objects\\n3d shape estimation\\n3d geometry\\nrgb-d image\\ntransparent surfaces\\nocclusion boundaries\\nrobotic manipulation\",\"363\":\"three-dimensional displays\\npose estimation\\nfeature extraction\\nimage color analysis\\nsupervised learning\\nrotation measurement\\nquaternions\\nconvolutional neural nets\\nimage colour analysis\\nimage representation\\nobject detection\\nregression analysis\\nconvolutional neural networks\\ncolor information\\ntranslation regression\\ndeep learning\\nrotation regression\\n6d object pose regression\\ngeometry-based pose refinement\\naxis-angle representation\\ngeodesic loss function\\nquaternion representation\\nycb-video dataset\",\"364\":\"cameras\\nrobot vision systems\\nthree-dimensional displays\\npose estimation\\ntwo dimensional displays\\nfeature extraction\\nimage colour analysis\\nimage segmentation\\nobject detection\\nobject recognition\\nstereo image processing\\n2d bounding boxes\\n3d cameras\\nycb-m\\nmulticamera rgb-d dataset\\nestimation system\\n3d bounding boxes\\nground truth 6dof poses\\nycb object\\ncamera model\\nrobust algorithms\\nestimation algorithms\\n6dof pose estimation\",\"365\":\"three-dimensional displays\\npose estimation\\ncameras\\ntraining\\nmanipulators\\ngrasping\\nimage segmentation\\nintelligent robots\\nrobot vision\\nsupervised learning\\nrobot manipulation\\nself-supervised 6d object pose estimation\\nself-supervised learning\\nobject configuration\\npose estimation modules\\nobject segmentation\\n6d pose estimation performance\\nrobots skill teaching\",\"366\":\"prototypes\\nlight emitting diodes\\nfeature extraction\\nimage recognition\\nthree-dimensional displays\\nlighting\\ncoatings\\ncameras\\nconvolutional neural nets\\nimage classification\\nimage reconstruction\\nimage sequences\\nobject recognition\\nuv markings\\nuv ink markers\\nlow-cost gelsight sensor\\nnongelsight captured images\\noptical flow algorithm\\nconvolutional neural networks\\ncnn\\n2d image conversion\\nfeature recognition\",\"367\":\"force\\nhaptic interfaces\\ntask analysis\\nnoise measurement\\nvirtual environments\\nsignal to noise ratio\\nprobes\\nbrain-computer interfaces\\nfeedback\\nforce feedback\\nmedical computing\\nneurophysiology\\nprosthetics\\nexploratory action\\nconventional haptic interface\\nibci-based prostheses control strategies\\nneural prostheses\\nintracortical brain computer interface\\ninput signal\\nrobotic prostheses\\nparalysis\\nsignal-independent noise\\nnoncollocated force feedback\\nsignal-to-noise ratio\\nvirtual environment\\nnoncollocated haptic feedback\",\"368\":\"electron tubes\\nsea measurements\\ntactile sensors\\noceans\\ndexterous manipulators\\ngrippers\\nmotion control\\nneurocontrollers\\nrecurrent neural nets\\nsaltwater corrosion\\nlow-light conditions\\nrobust electrical parts\\nmechanical parts\\nunderwater robots\\nmobile manipulation tasks\\nsuction flow mechanism\\norifice occlusion\\nambient pressure\\ntactile sensing modality\\nautomated robotic behaviors\\nfingertip suction flow\\nsubmerged dexterous manipulation\\nrobotic systems\",\"369\":\"visualization\\nrobustness\\nsemantics\\ntask analysis\\nhistograms\\ncorrelation\\nsimultaneous localization and mapping\\nconvolutional neural nets\\nimage coding\\nimage matching\\nimage resolution\\nvisual databases\\nvgg16 cnn architecture\\nmatching cnn features\\nquery image\\nvgg16 convolutional neural network architecture\\nspatial matching visual place recognition\\nssm-vpr\\noptimal image resolutions\\nvisual place recognition\\nconvolutional neural networks\\nslam\\nloop closure\\nlife-long navigation\",\"370\":\"trajectory optimization\\nfunction approximation\\ntask analysis\\nlegged locomotion\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\npath planning\\nrobust control\\ntrajectory control\\nwearable robots\\nautonomous robots\\nonline trajectory planning\\nguided trajectory learning\\nself-balanced exoskeleton\\natalante\\nrobust control strategies\",\"371\":\"uncertainty\\nplanning\\ntask analysis\\nobject detection\\nrobot sensing systems\\nprobabilistic logic\\nmobile robots\\nobject recognition\\npath planning\\nbelief space\\nrobot localization\\ninterleaved acting\\nplanning technique\\nlow-level geometric features\\ntask planner\\nperception tasks\\nstate spaces\",\"372\":\"task analysis\\nresource management\\ngenetic algorithms\\nsociology\\nstatistics\\ncost function\\nmessage systems\\nmulti-agent systems\\nmin-time objective\\ndecentralized ga approach\\ntask execution\\nmultiagent collaborative search missions\\ndecentralized genetic algorithm\\nmultiagent systems\\ndecentralized task allocation problem\\ndecentralized evolutionary approaches\\nmin-time performance\\nmin-sum objective\",\"373\":\"planning\\ntask analysis\\nclutter\\ncollision avoidance\\nmanipulators\\nsearch problems\\nimage retrieval\\nmobile robots\\nobject detection\\nresilient manipulation planning\\ntask and motion planning\\nrobotic manipulator\\ntarget object retrieval\\ncollision-free path\\nobject rearrangement\\ntamp framework\\nstatic environments\\npick-and-place actions\\nbaseline methods\",\"374\":\"robots\\npermanent magnets\\nmagnetic resonance imaging\\nprogrammable logic arrays\\nfabrication\\nplastics\\nthree-dimensional displays\\nbending\\nmagnetic actuators\\nmagnetic flux\\nmicrorobots\\nmobile robots\\nmotion control\\npolymers\\nrobot kinematics\\nrods (structures)\\nuntethered soft millirobot\\nmagnetic actuation\\nscalable designs\\nmoulding technique\\nacrylonitrile butadiene styrene filaments\\nembedded permanent magnets\\nsoft-body\\nsoft-robots\\nexternal uniform magnetic field control system\\nmagnetic flux densities\\nsoft-robotic body\\nmagnetic field strength\\nmotion modes\\nmagnetic field inputs\\nhollow rod-like structures\\npolydimethylsiloxane\\n3d printed polylactic acid rings\\npivot walking\\nrolling motion\\ntumbling motion\\nwiggling motion\\nside-tapping motion\\nwavy motion\\ndeflection curve\\nnavigation\\nminimally invasive in vivo applications\\nbending angle\",\"375\":\"task analysis\\nrobots\\nelectroencephalography\\nnavigation\\nhafnium\\nlearning (artificial intelligence)\\ntraining\\nbrain\\nfeedback\\nmedical robotics\\nmedical signal processing\\naccelerated robot learning\\nhuman brain signals\\nreinforcement learning\\nrl algorithms struggle\\nlearning signal\\nrobot learning process\\nerror-related signal\\nmeasurable using electroencephelography\\nrobotic agents\\nsparse reward settings\\nhuman observer\\nrobot attempts\\nnoisy error feedback signal\\nsupervised learning\\nrl learning process\\nrobotic navigation task\",\"376\":\"conferences\\nautomation\\nbiomechanics\\nbrain\\nelectromyography\\nmedical signal processing\\nmuscle\\nneurophysiology\\nposition measurement\\nviscosity\\ncylindrical rotary controller manipulation\\nindex finger\\nviscosity characteristics differences\\nrotational manipulation\\nbrain activations\\nrotary manipulation\\nrotary motion\\nviscosity characteristics\\nbrain activity\\nmuscles activity\\nmuscle activity\",\"377\":\"electrooculography\\nelectromyography\\nreal-time systems\\ndecoding\\nelectrodes\\nservice robots\\nbiomechanics\\nbrain-computer interfaces\\nelectro-oculography\\ngrippers\\nhuman-robot interaction\\nman-machine systems\\nmedical robotics\\nmedical signal processing\\nneurophysiology\\nsignal classification\\ntime robot reach-to-grasp movement control\\ngrasping task\\nindustrial robot\\neye movements\\nelectromyography signals\\nreal-time human-robot interface system\\nhuman-controlled assistive devices\\ndexterous devices\\neog signals\\nrobot arms\\nreal-time control\\nhri systems\\nrobot control\\nemg signals\\nreal-time decoding\\nemg decoding\\nur-10 robot arm\",\"378\":\"electromyography\\ntorque\\ndecoding\\nwrist\\nlogic gates\\nneural networks\\nkinematics\\nbiomechanics\\nmedical signal processing\\nmuscle\\ntime series\\ndecoding method\\nlstm network\\ndecoding approach\\nwrist joint\\nlong-time span\\ncore processor\\nshort-term memory network\\ndecoding technique\\njoint angle\\nlearning emg signals\\nhuman-machine interaction\\nelectromyography (emg)\\nmachine learning\\nprosthesis\",\"379\":\"muscles\\nelectromyography\\ntask analysis\\nelectrodes\\nrobots\\ndecoding\\nfeature extraction\\nbiomechanics\\nbiomedical electrodes\\ndexterous manipulators\\nmanipulators\\nmedical robotics\\nmedical signal processing\\npatient rehabilitation\\nprosthetics\\ntelerobotics\\nemg signals\\nobject motion decoding\\nmuscle importances\\nmuscle importance results\\ndecoded motions\\nfingered robotic hand\\nrobotic devices\\ndexterous manipulation tasks\\nelectromyography-based interfaces\\nrobotics studies\\nteleoperation\\ntelemanipulation applications\\nemg-based control\\nprosthetic rehabilitation devices\\nassistive rehabilitation devices\\nrobotic rehabilitation devices\\ngrasping tasks\\nlearning scheme\\nhigh density electromyography sensors\\nin-hand manipulation motions\\nobject space\\nmyoelectric activations\\nhuman forearm\\nhand muscles\\nyaw motions\\ngeometric center\\nmyoelectric data\\nhigh-density electromyography-based control\\nhd-emg electrode arrays\",\"380\":\"cameras\\ntask analysis\\nrobot vision systems\\nrobot kinematics\\ntelepresence\\nteleoperators\\ncontrol system synthesis\\nmanipulators\\nrobot vision\\ntelecontrol\\ntelerobotics\\nvisual feedback\\nteleoperation assistance design\\ntelepresence camera selection\\nstandalone cameras\\nwearable cameras\\nmanipulation motions\\nactive perception control\\nhuman motor system\\nnatural perception-action coupling\\nautonomous camera selection\\nactive perception motions\\ncoordinated manipulation\\ntelepresence tele-action robots\\ntelepresence cameras\\nrobot teleoperation\\ntelepresence system\",\"381\":\"kinematics\\nmanipulators\\ntrajectory\\ncomputer architecture\\ncost function\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nmechanical variables control\\nmedical robotics\\nmobile robots\\nmotion control\\nredundant manipulators\\ntrajectory control\\ntechnical framework\\nmotion generation\\nautonomous anthropomorphic redundant manipulators\\nco-bots\\nindustrial settings\\npeople assistance\\nrobot motions\\nclassic solutions\\nanthropomorphic movement generation\\noptimization procedures\\nneuroscientific literature\\nlearning methods\\nmotion variability\\nhigh dimensional datasets\\nhuman upper limb principal motion modes\\nfunctional analysis\\nrobot trajectory optimization\\nredundant anthropomorphic kinematic architectures\\nhuman model\\nfunctional mode extraction\\nhuman trajectories\\nrobotic manipulator\\nadvanced human-robot interaction\\nindustrial co-botics\\nhuman assistance\",\"382\":\"robots\\njob shop scheduling\\ntask analysis\\nreal-time systems\\nschedules\\nadaptive scheduling\\nadaptation models\\nassembling\\ngenetic algorithms\\nmulti-agent systems\\nmulti-robot systems\\nrobotic assembly\\nscheduling\\nhuman-multirobot collaboration\\nhuman capability\\noptimal assembly scheduling\\nrobot adaptation\\nhuman-single-robot interaction\\nhuman-multirobot interaction\\nmultiagent interactions\\nreal-time adaptive assembly scheduling approach\\nformulated adaptive assembly scheduling problem\\nhuman-multirobot assembly tasks\",\"383\":\"robots\\nsurgery\\nfeature extraction\\niris\\ncameras\\nmirrors\\ncataracts\\neye\\nmedical computing\\nmedical robotics\\nophthalmic lenses\\nex-vivo porcine eyes\\nmicroscope-guided autonomous clear corneal incision\\nophthalmic microscope system\\nmultiaxes robot\\nself-sealing incision\\nautonomous robotic system\\ncataract surgery\",\"384\":\"needles\\nkinematics\\nmathematical model\\nactuators\\nmodulation\\nrobot sensing systems\\nmanipulator kinematics\\nmean square error methods\\nmedical robotics\\nposition control\\nsurgery\\nasynchronous control\\ndecoupled control\\nspatial rcm tensegrity mechanism\\nneedle manipulation\\nspatial remote center\\ndouble parallelogram system\\npercutaneous needle insertion\\ndecoupled modulation\\ncontrol methodology\\nposition tracking\\nneedle guide\\nrcm tensegrity mechanism\\nspatial remote center of motion\",\"385\":\"power cables\\nrobots\\nreal-time systems\\nkinematics\\npayloads\\nsafety\\nrobustness\\ncables (mechanical)\\npredictive control\\nrobot kinematics\\nrobust control\\ntrajectory control\\nmpc scheme\\nfully-constrained cable-driven parallel robots\\ncable tension distribution\\npick-and-place task\\nmaximum tension\\ntracking error minimization\\nredundancy resolution integrated model predictive control\\ncdprs\\ncable tension limits\\npayload mass\",\"386\":\"tendons\\nrobots\\ncomputational modeling\\nstrain\\nnumerical models\\nkinematics\\ndeformable models\\nactuators\\nbending\\nbiomechanics\\ninspection\\nmedical robotics\\nrobot kinematics\\ntendon actuated multisection continuum arms\\nbending deformations\\nhigh mechanical coupling\\nvariable length-based kinematic models\\ncontinuum arm curve parameter kinematics\\nrobot\",\"387\":\"dynamics\\nplanning\\nrobots\\ntrajectory optimization\\nchebyshev approximation\\ncables (mechanical)\\nmanipulator dynamics\\noptimisation\\npath planning\\nposition control\\ntrajectory control\\ncable-suspended parallel robot\\nstatic workspace\\ntrajectory optimization formulation\\ndynamic trajectories\\nsix-degree-of-freedom\\nlow-dimensional dynamic models\\nnarrow feasible state space\\ndynamic similarity\\npoint-mass cspr\\nfeasible force polyhedra\\ntransition trajectories\\nhighly dynamic motions\\nperiodic trajectories\\ndynamic trajectory planning\\noptimization and optimal control\\ncable-suspended parallel robots\",\"388\":\"spraying\\ncameras\\nsemantics\\nimage segmentation\\nvegetation\\ndecoding\\nmachine learning\\nagriculture\\nagrochemicals\\ncrops\\nimage capture\\nimage classification\\nlearning (artificial intelligence)\\nnozzles\\nsensor fusion\\npear orchard\\ndeep learning-based intelligent spraying system\\nfruit tree detection system\\nsegnet model\\nsemantic segmentation structure\\ndeep learning model\\nnozzle\\ndata fusion\\nrgb-d camera\\npesticides\\nenvironmental safety\",\"389\":\"planning\\ncameras\\nthree-dimensional displays\\nmanipulators\\nvegetation\\npipelines\\nagricultural products\\nimage colour analysis\\nindustrial manipulators\\nintelligent robots\\nrobot vision\\nmotion planning\\nsequence cut points\\nrobotic pruning\\ncontrol framework\\npruning fruit trees\\ndormant pruning\\nfresh market tree fruit production\\nindustrial manipulator\\neye-in-hand rgb-d camera configuration\\npneumatic cutter\",\"390\":\"navigation\\nrobots\\noptimization\\ntuning\\nrobustness\\ngenetic algorithms\\nmeasurement\\niterative methods\\nmobile robots\\nmotion control\\npath planning\\nsimulated environments\\ngenetic algorithm\\nresulting parameter sets\\nsubstantial performance improvements\\nagricultural robots\\ncontext dependant iterative parameter optimisation\\nrobust robot navigation\\nautonomous mobile robotics\\nrobust performance\\nrobot model\\nparameter tuning\\nunderlying algorithm\\nsubstantial combinatorial challenge\\nextensive manual tuning\\nnavigation actions\\nspatial context\\nnavigation task\\nrespective navigation algorithms\\niterative optimisation\\nperformance metrics\",\"391\":\"task analysis\\nmanifolds\\nmanipulator dynamics\\naerospace electronics\\nhumanoid robots\\nmeasurement\\ncollision avoidance\\nmanipulators\\nmobile robots\\nmotion control\\npendulums\\nrobot dynamics\\nwheels\\nrmp formalism\\nunderacutated systems\\nfully-actuated subsystem\\nresidual dynamics\\nmanipulation tasks\\n7-dof system\\nsecond-order motion policies\\nrmp-based approaches\\noperational space control\\nfully state-dependent\\ncollision avoidance bahaviors\\ncontrol input\\nriemmanian motion policies\\nunderactuated wheeled-inverted-pendulum humanoid robot\",\"392\":\"atmospheric modeling\\ndynamics\\nmathematical model\\nforce\\nasymptotic stability\\nbang-bang control\\nfeedback\\nfeedforward\\nmotion control\\nself-adjusting systems\\nuncertain systems\\naugmented controller\\nuncontrolled system\\nmechanical self-stability\\nuncertain environments\\nunstructured environments\\nexplicit state observation\\ncontrol law\\nperformance metrics\\nminimalistic approach\\nfeedforward bang-bang control\\nself-stabilizing dynamics\\nheight control\\nglobal asymptotic stability\\nself-stabilising systems\\nbernoulli ball\",\"393\":\"mathematical model\\ncouplings\\nintegrated circuits\\ntrajectory\\nkinematics\\nrobots\\ntensile stress\\nmanipulator dynamics\\nmatrix algebra\\nmotion control\\nnonlinear control systems\\npath planning\\nposition control\\nsingularity-free inverse dynamics\\nunderactuated system\\nrotating mass\\nconfiguration singularities\\nconfiguration space\\ninertial coupling\\nsmall-amplitude sine wave\\nnonlinear dynamics\\nrolling system\\nsingularity regions\\ncoupling singularities\\nrolling carrier\",\"394\":\"grippers\\nfriction\\ncouplings\\nstability analysis\\nforce\\ntorque\\nshape\\nactuators\\nadhesion\\nadhesives\\ndexterous manipulators\\nunder-actuated gripper\\nrobotic grippers\\nhigh-friction materials\\nscaling forces\\nadhesion-controlled friction\\nadhesion-based grippers\\nhigh-friction interfaces\\nrobust capture\\nsize 65.0 cm\",\"395\":\"roads\\nrobot sensing systems\\ncontext modeling\\nplanning\\nautomobiles\\ngeometry\\nkinematics\\ncontrol engineering computing\\nmulti-agent systems\\nroad traffic\\nroad vehicles\\ntelecommunication traffic\\ntraffic engineering computing\\nsummit\\nurban driving\\nmassive mixed traffic\\nunregulated urban crowd\\nhigh-speed traffic participants\\nhigh-fidelity simulator\\ncrowd-driving algorithms\\nopen-source openstreetmap map database\\nmultiagent motion prediction model\\nunregulated urban traffic\\nheterogeneous agents\\nautonomous driving simulation\\nrealistic traffic behaviors\\ncrowd-driving settings\",\"396\":\"training\\npredictive models\\nprocess control\\nadaptation models\\nprinting\\nrobots\\nthree-dimensional displays\\nlearning (artificial intelligence)\\nrapid prototyping (industrial)\\nrobotic welding\\nthree-dimensional printing\\nwelds\\nwires\\nintegrated learning-correction framework\\nmodel-based reinforcement learning\\nprocess parameters\\ninter-layer geometric digression\\nrobot arm\\n3d metallic objects\\nlayer by layer fashion\\nmultilayer multibead deposition control\\nrobotic wire arc additive manufacturing\\nweld beads\\nerror stacking\\nmaterial wastage reduction\\nmlmb print\",\"397\":\"planning\\nmonte carlo methods\\nthree-dimensional printing\\nsolid modeling\\nclustering algorithms\\nprinters\\ngeometry\\nmachine tools\\noptimisation\\nrapid prototyping (industrial)\\nsearch problems\\ntree searching\\n3d printing slice\\ndependency graph\\nmcts-based algorithm\\nlocal search\\ntoolpath quality\\nmonte carlo tree search\\noptimal fdm toolpath planning\",\"398\":\"kinematics\\nservice robots\\ncollision avoidance\\ntrajectory\\ntask analysis\\nheuristic algorithms\\nflexible manufacturing systems\\nindustrial robots\\nmobile robots\\nrobot dynamics\\nrobot kinematics\\nmodular robots\\nflexible manufacturing\\nmodule composition\\ncycle time\\nenergy efficiency\\nkinematic constraints\\ndynamic constraints\\nrandomly generated tasks\\nperformance metrics\\nmodular robot\\npromodular.1\\nobstacle constraints\\ncartesian space\",\"399\":\"task analysis\\nstacking\\nrobots\\nlearning (artificial intelligence)\\npoles and towers\\ntraining\\nthree-dimensional displays\\ngraph theory\\nmanipulators\\nmobile robots\\nneural nets\\nmultiobject manipulation\\nrelational reinforcement learning\\nlearning robotic manipulation tasks\\noutrageous data requirements\\ntask curriculum\\ngraph-based relational architectures\\nsimulated block stacking task\\nstep-wise sparse rewards\\nzero-shot generalization\",\"400\":\"robots\\ndata structures\\npeer-to-peer computing\\noverlay networks\\nrouting\\ndistributed databases\\ntopology\\ncontrol engineering computing\\ndata handling\\ndistributed processing\\nmobile robots\\nmulti-robot systems\\nstorage management\\ndata item position\\nnear-perfect data retention\\ndistributed data structure\\ncooperative multirobot applications\\ndistributed storage\\nshared global memory\\nexternal storage infrastructure\\nswarm topology\\ndata storage\\nswarmmesh\\ndata type\",\"401\":\"receivers\\ntransmitters\\ndrones\\nobservers\\ntrajectory\\nelectromagnetics\\nadaptive control\\nautonomous aerial vehicles\\nemergency management\\nmulti-robot systems\\nrescue robots\\nrobust control\\nsensor fusion\\navalanche victim search\\nrobust observers\\nvictim localization\\narva sensor\\nuavs\\nleast square identifier\\nsearch and rescue\",\"402\":\"optical feedback\\noptical sensors\\noptical imaging\\nplanning\\nnavigation\\nharmonic analysis\\nmathematical model\\ncollision avoidance\\ngraph theory\\nimage sequences\\nmobile robots\\nrobot vision\\noptic flow\\ninsect visuomotor system\\nobstacle detection\\ntopological graph\\nmetric-topological planning\\nautonomous navigation\\nrobotic platforms\\nbio-inspired reactive control\\nspatial decomposition\\nobstacle avoidance\\nfourier residual analysis\\nimage processing\\ncontinuous occupancy grid\\ngraph edge\\nexploration\\ncentering\\ncontrol\\nmapping\",\"403\":\"robot sensing systems\\ntrajectory\\nmeasurement uncertainty\\nstandards\\nuncertainty\\ndeterministic algorithms\\nmobile robots\\noptimisation\\npath planning\\nsensors\\ntree searching\\nrobot sensing trajectories\\nautonomous tsdf mapping\\ntsdf uncertainty\\nsensor measurements\\nefficient uncertainty prediction\\nlong-horizon optimization\\ndeterministic tree-search algorithm\\ninformation gain\\ntsdf distribution\\nefficient planning\\nuninformative sensing trajectories\\nactive tsdf mapping approach\\nsimulated environments\\ninformation theoretic active exploration\\noccupancy mapping\\nmobile robot\\ntruncated signed distance field\\nrobot motion primitive sequences\\nbranch-and-bound pruning\\ncomplex visibility constraints\",\"404\":\"safety\\nadaptation models\\nbayes methods\\nstochastic processes\\nuncertainty\\ncomputational modeling\\nswitches\\nadaptive control\\nbelief networks\\ncontrol engineering computing\\ngaussian processes\\nlearning (artificial intelligence)\\nlyapunov methods\\nmarkov processes\\nneural nets\\nprobability\\nsafety-critical software\\nstability\\nreal-time performance\\ndeep neural networks\\nmodel uncertainties\\nbayesian model learning\\nadaptive control framework\\ncontrol lyapunov functions\\ncontrol barrier functions\\ntractable bayesian model\\nsafety-critical high-speed mars rover missions\\nbayesian learning-based adaptive control\\ndeep learning\\nsafety-critical systems\\nhigh-speed terrestrial mobility\\nrobust\\/adaptive control of robotic systems\\nrobot safety\\nprobability and statistical methods\\nbayesian adaptive control\\nmars rover\",\"405\":\"laser radar\\nlegged locomotion\\nthree-dimensional displays\\nsimultaneous localization and mapping\\niterative closest point algorithm\\nfeature extraction\\ngraph theory\\nimage matching\\nimage registration\\nimage segmentation\\nlearning (artificial intelligence)\\nneural nets\\noptical radar\\noptimisation\\npose estimation\\nrobot vision\\nslam (robots)\\nstereo image processing\\nonline lidar-slam\\nlegged robot\\nrobust registration\\ndeep-learned loop closure\\n3d factor-graph lidar-slam system\\nindustrial environments\\npoint clouds\\ninertial-kinematic state estimator\\nicp registration\\nloop proposal mechanism\\ndeep learning method\\nodometry\\nloop closure factors\\npose graph optimization\\nslam map\\nrisk alignment prediction method\\ndeeply learned feature-based loop closure detector\",\"406\":\"simultaneous localization and mapping\\nthree-dimensional displays\\ncameras\\nvisualization\\ncognition\\nfeature extraction\\ntask analysis\\ncomputer vision\\nimage representation\\nimage retrieval\\nslam (robots)\\nvisual slam systems\\ncamera field-of-view\\nvoxel map representation\\nkeyframe map\\nmap points retrieval\",\"407\":\"task analysis\\nentropy\\nmeasurement\\ntraining\\nrobots\\ninterpolation\\nlearning (artificial intelligence)\\ninteractive video\\nrobot vision\\nvideo signal processing\\nadversarial skill-transfer loss\\ntask domain\\nlearned skill embeddings\\nentropy-regularized adversarial skill-transfer loss\\ntemporal video coherence\\nmetric learning loss\\nadversarial loss\\ntask context\\nunlabeled multiview videos\\ntask-agnostic skill embedding space\\nreinforcement learning agents\\nunsupervised robot skill learning\\nadversarial skill networks\\nlearned embedding\",\"408\":\"neurons\\nbiological neural networks\\nangular velocity\\nkernel\\ntask analysis\\ncomputer architecture\\nneuromorphics\\nimage resolution\\nimage sensors\\nneural nets\\nregression analysis\\ntemporal regression problem\\nrotating event-camera\\nsnn\\nirregular event-based input\\nasynchronous event-based input\\nhigh-temporal resolution\\nsynthetic event-camera\\nevent-based angular velocity regression\\nspiking networks\\ntemporal spikes\\nbio-inspired networks\\nbrightness change\\nspiking neural networks\\nspike-based computational model\\nasynchronous sensors\\nartificial neural networks\\n3-dof angular velocity\",\"409\":\"cameras\\nadaptive optics\\ngeometry\\noptical imaging\\nmachine learning\\nestimation\\ntwo dimensional displays\\nconvolutional neural nets\\ndistance measurement\\nlearning (artificial intelligence)\\npose estimation\\nslam (robots)\\nmonocular visual odometry algorithm\\ngeometry-based methods\\nmonocular systems\\nscale-drift issue\\ndeep learning\\nepipolar geometry\\nperspective-n-point method\\nframe-to-frame vo algorithm\\ndf-vo\\nscale consistent single-view depth cnn\",\"410\":\"cameras\\nthree-dimensional displays\\ntraining\\ngeometry\\nrobot vision systems\\ntransforms\\nconvolutional neural nets\\nfeature extraction\\nimage classification\\nimage motion analysis\\nimage reconstruction\\nimage segmentation\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npath planning\\npose estimation\\n3d scene geometry-aware constraint\\ncamera localization\\ndeep learning\\nfundamental component\\nautonomous driving vehicles\\nend-to-end approaches\\nconvolutional neural network\\n3d-geometry based traditional methods\\ncompact network\\nabsolute camera\\nimage contents\\nimage-level structural similarity loss\\nchallenging scenes\",\"411\":\"task analysis\\nrobots\\nlearning (artificial intelligence)\\ntraining\\nincentive schemes\\nbuffer storage\\ngames\\naugmented reality\\ncontrol engineering computing\\nlearning systems\\nmanipulators\\naugmented curiosity-driven experience replay\\nhindsight experience replay\\nsample-efficiency\\nautomatic exploratory curriculum\\ndynamic initial states selection\\ntask-relevant states\\ngoal-oriented curiosity-driven exploration\\naction space\\nhigh dimensional continuous state\\nlow exploration efficiency\\nrl agent\\nreinforcement learning\\nsparse feed-back\\nacder\\nmultistep robotic task learning\\nbasic tasks\\nchallenging robotic manipulation tasks\\nvaluable states\",\"412\":\"trajectory\\nrobots\\nkinematics\\ntask analysis\\ntraining\\nneural networks\\nlearning (artificial intelligence)\\nmotion control\\nneurocontrollers\\nrobot kinematics\\ntime optimal control\\ntrajectory control\\ntruerma\\nrobot trajectory learning\\nrecursive midpoint adaptations\\ncartesian space\\ndifferentiable path\\ninverse kinematics\\ntime optimal parameterization\\nreinforcement learning\\nrobot movement\\nneural network training\\nkuka iiwa robot\",\"413\":\"planning\\nparallel processing\\nfaa\\nrobot kinematics\\ncloud computing\\nprobabilistic logic\\ncomputational complexity\\ncontrol engineering computing\\nmanipulators\\nmobile robots\\nmotion control\\npath planning\\ndistributed motion planning\\nlambda serverless computing\\nmotion planning algorithms\\nrrt\\ncomputational load\\nlocal environment changes\\ncloud-based serverless lambda computing\\nparallel computation\\nserverless computers\\nlearned parallel allocation\\namazon lambda\\nsporadically computationally intensive motion planning tasks\\nfog robotics algorithms\",\"414\":\"distortion\\nrobot sensing systems\\nthree-dimensional displays\\nboundary conditions\\ntwo dimensional displays\\nmobile robots\\noptical radar\\nradar receivers\\nstereo image processing\\nterrain mapping\\nexploration approach\\npotential fields\\nuneven terrains\\nhigh declivity regions\\nground robot\\nelevation-based local distortions\\nnumerous outdoor tasks\\nmilitary applications\\n3d terrain exploration\\npatrolling application\\ndelivery application\\n2d lidar sensors\",\"415\":\"planning\\napproximation algorithms\\ntrajectory\\nheuristic algorithms\\nmeasurement\\nrobots\\nsilicon\\ncontrol engineering computing\\nformal verification\\nnonlinear control systems\\nreachability analysis\\nrobot dynamics\\nsampling methods\\ntrees (mathematics)\\nr3t\\nrandom reachable set tree\\noptimal kinodynamic planning\\nnonlinear hybrid systems\\nreachability-based variant\\nrapidly-exploring random tree\\nmultiple polytopes\\nnonlinear systems\\ncontact-rich robotic systems\",\"416\":\"uncertainty\\nthree-dimensional displays\\ntrajectory\\nsemantics\\nrobots\\nplanning\\nneural networks\\nbelief networks\\ncomputational geometry\\nimage reconstruction\\nmobile robots\\nneural nets\\npath planning\\nrobot vision\\ndeepsemantichppc\\nhypothesis-based planning\\nuncertain semantic point clouds\\ndeep bayesian neural network\\nflexible point cloud scene representation\\nsparse visual measurements\\nhypothesis-based path planner\\nuncertainty-aware hypothesis-based planner\",\"417\":\"planning\\nrobots\\nmeters\\ntask analysis\\nhardware\\nbuildings\\nspace exploration\\nmobile robots\\nmotion control\\npath planning\\nbalancing actuation\\nmotion planning problems\\nlow-energy robotic vehicles\\nhigh-endurance autonomous blimps\\ncomputing hardware\\nactuation hardware\\nceimp\\nanytime planning algorithm\\nactuation energy\\nasymptotic computational complexity\\nsampling-based motion planning algorithms\\ncompute energy included motion planning algorithm\",\"418\":\"planning\\nbayes methods\\nhistory\\nuncertainty\\ngeometry\\nlearning (artificial intelligence)\\nsearch problems\\ncollision avoidance\\ngraph theory\\nminimisation\\nmobile robots\\n7d planning problems\\nposterior sampling\\nanytime motion planning\\ncollision checking\\ncomputational bottleneck\\nlazy algorithms\\ncollision uncertainty\\nshortest path\\nreal-time applications\\nanytime performance\\nanytime lazy motion planning algorithm\\nedge collisions\\ninitial feasible path\\nshorter paths\",\"419\":\"rotors\\nattitude control\\nangular velocity\\nfault tolerance\\nfault tolerant systems\\nresource management\\ndrones\\nautonomous aerial vehicles\\ncascade control\\ncontrol system synthesis\\nhelicopters\\nmonte carlo methods\\nquadratic programming\\nupset recovery control\\nfault-tolerant controller\\narbitrary initial orientations\\nangular velocities\\ncontrol method\\npost-failure quadrotor\\nmonte-carlo simulation\\ncontrol allocator\\ncontrol allocation method\\nalmost-global convergence attitude controller\",\"420\":\"propellers\\naerodynamics\\natmospheric modeling\\npredictive models\\nblades\\nacceleration\\ncomputational modeling\\nautonomous aerial vehicles\\nmomentum\\nrotors (mechanical)\\nvehicle dynamics\\nforce model\\nmultirotor uav\\nmodel identification method\\nblade element theories\\nmomentum theory\\nactuation dynamics\",\"421\":\"springs\\nmanipulators\\npropellers\\ntask analysis\\ngrippers\\nactuators\\naerospace robotics\\nfeedback\\nmanipulator dynamics\\nvibration control\\nelastic suspension\\naerial manipulator\\ncontra-rotating propellers\\ncomputed torque control strategy\\nactive vibration canceling\\nfeedback linearization control strategy\\nrobotic carrier\",\"422\":\"cameras\\nrobot vision systems\\ntransforms\\nangular velocity\\nstate estimation\\nattitude control\\naircraft control\\nangular velocity control\\nautonomous aerial vehicles\\nclosed loop systems\\nfeedback\\nhelicopters\\nhough transforms\\nimage resolution\\nimage sensors\\nkalman filters\\npd control\\nrobot vision\\nattitude tracking\\nstate estimator\\nrotor thrusts\\nblack-and-white disk\\nroll angle\\nkalman filter\\nhough transform\\ndualcopter platform\\none-dimensional attitude tracking\\ndrones\\nquadrotors\\nlow-latency high-bandwidth control\\nevent-based feedback\\nevent-camera-driven closed loop control\\nproportional-derivative attitude control law\\nevent-based state estimation\\ntemporal resolution\\nsensor latency\\nhigh speed vision-based control\\nfrequency 1.0 khz\\ntime 12.0 ms\",\"423\":\"propellers\\ntask analysis\\nrobot sensing systems\\nreal-time systems\\nactuators\\nvehicle dynamics\\ntorque\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nhelicopters\\nmotion control\\nnonlinear control systems\\npredictive control\\nrotors (mechanical)\\nmotor-level nonlinear mpc\\ntilted-propeller\\nperception-constrained nonlinear model predictive control framework\\nreal-time control\\nmultirotor aerial vehicles\\nperceptive sensor\\nrealistic actuator limitations\\nrotor minimum\\nmaximum speeds\\nmultirotor platforms\\nunderactuated quadrotors\\ntilted-propellers hexarotors\\nmotor-torque level\",\"424\":\"manipulator dynamics\\nend effectors\\ntrajectory\\ntask analysis\\nplanning\\nvehicle dynamics\\ncontrol system synthesis\\nmanipulator kinematics\\npath planning\\nposition control\\nsimulated aerial videography task\\ndifferential flatness\\n6dof aerial manipulators\\ncoordinate-free formulation\\ncoupled dynamics\\n2dof articulated manipulator\\nend effector frame\",\"425\":\"trajectory\\ninterpolation\\nroads\\ntraining\\naerospace electronics\\ndata models\\nautonomous vehicles\\nbayes methods\\ndata analysis\\nmobile robots\\nroad safety\\nroad vehicles\\nsafety-critical software\\ntrajectory control\\ncmts\\nconditional multiple trajectory synthesizer\\nsafety-critical driving scenarios\\nnaturalistic driving trajectory generation\\nautonomous driving algorithms\\ncollision-free scenarios\\nsafety-critical cases\\nnear-miss scenarios\\noff-the-shelf datasets\\ngenerative model\\nconditional probability\\ntrajectory predictions\\nautonomous vehicle safety\\nsafety-critical data synthesizing framework\\nvariational bayesian methods\",\"426\":\"laser radar\\nestimation\\nrobustness\\nroads\\nwindows\\noptimization\\nautonomous vehicles\\ndistance measurement\\ngraph theory\\ninertial systems\\nmaximum likelihood estimation\\nmotion estimation\\noptical radar\\nlocalization estimation\\ninertial lidar intensity\\nmatching estimation\\nlidar localization system\\nenvironmental change detection method\\nkinematic estimation\\nframe-to-frame motion estimation\\nmultiresolution occupancy grid based lidar inertial odometry\\npose graph fusion framework\\napollo-southbay dataset\\nmap estimation problem\",\"427\":\"computer architecture\\nautonomous vehicles\\nlenses\\nlearning (artificial intelligence)\\ndecision making\\npredictive models\\nneural networks\\nconvolutional neural nets\\ngraph theory\\nimage representation\\nimage sequences\\nneural net architecture\\ntraffic engineering computing\\ndeepscene-q off-policy reinforcement learning algorithms\\ngraph-q\\ngraph convolutional networks\\nmultiple variable-length sequences\\nnovel deep scene architecture\\ncomplex interaction-aware scene representations\\ntraffic participants\\ntraffic signs\\nobject types\\nhigh-level decision making\\ndeep reinforcement learning\\nhigh-level decision component\\nperception component\\nautonomous driving systems\\ndynamic interaction-aware scene understanding\\ntraffic simulator sumo\",\"428\":\"automobiles\\ntrajectory\\nfeature extraction\\nhidden markov models\\npredictive models\\ncomputer architecture\\nroad transportation\\nconvolutional neural nets\\nfeedback\\nlearning (artificial intelligence)\\npath planning\\nrecurrent neural nets\\ntraffic engineering computing\\ncar\\nconvolutional recurrent neural networks\\nconvolutional long short term memory\\nconv-lstm\\ninteracting vehicle trajectory prediction\\nnovel feedback scheme\\ninteraction learning\\ntemporal learning\\nmotion learning\",\"429\":\"navigation\\ntask analysis\\ntrajectory\\nlearning (artificial intelligence)\\nautonomous vehicles\\nsmoothing methods\\ncurrent measurement\\ncontrol engineering computing\\nimage colour analysis\\nmobile robots\\nmulti-agent systems\\npath planning\\nroad traffic control\\nrobot vision\\nrobust control\\ntraffic engineering computing\\nsuboptimal policy\\ncarla driving benchmark\\nvision-based autonomous driving\\nimitative reinforcement learning\\nrobust driving policy\\nnonsmooth rewards\\nstate-action pairs\\nsmooth rewards\\nmatching measurer\\nnavigation rewards\\nnavigation command matching\\nattention-guided agent\\nsalient regions\\nrgb images\",\"430\":\"vehicles\\ntrajectory\\nheuristic algorithms\\neigenvalues and eigenfunctions\\nlaplace equations\\nclassification algorithms\\ntopology\\ncomputational complexity\\ndriver information systems\\ngraph theory\\nmulti-agent systems\\npattern classification\\nsupervised learning\\ngraphrqi\\ngraph spectrums\\nroad-agent trajectories\\ndriving traits\\naggressive driving\\nconservative driving\\nnearby road-agents\\ninteragent interactions\\nunweighted traffic graphs\\nundirected traffic graphs\\nsupervised learning algorithm\\ntraffic graph\\neigenvalue algorithm\\nautonomous driving datasets\\nprior driver behavior classification algorithms\",\"431\":\"measurement\\nradar imaging\\nrobot sensing systems\\nazimuth\\nfeature extraction\\ntrajectory\\nconvolutional neural nets\\ncw radar\\nfm radar\\nlearning (artificial intelligence)\\nnearest neighbour methods\\nradar tracking\\npolar nature\\nradar scan formation\\ncylindrical convolutions\\nanti-aliasing blurring\\nazimuth-wise max-pooling\\nrotational invariance\\nenforced metric space\\ntopological localisation system\\nrandom rotational perturbation\\nkidnapped radar\\ntopological radar localisation\\nrotationally-invariant metric learning\\nlarge-scale topological localisation\\nfrequency-modulated continuous-wave scanning radar\\nefficient learning-based approach\\nradar data\\npolar radar scans\\nnetvlad architectures\\nvisual domain\\nradar-focused mobile autonomy dataset\\ncnn architectures\\nreference trajectory\\nnearest neighbour\\nplace recognition\\nroot architecture\\nconvolutional neural network\\ndistance 280.0 km\\nradar\\nlocalisation\\ndeep learning\\nmetric learning\",\"432\":\"three-dimensional displays\\ntask analysis\\nlaser radar\\nfeature extraction\\nvisualization\\nrobots\\ntwo dimensional displays\\nimage recognition\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\noptical radar\\nrobot vision\\nslam (robots)\\nglobal visual localization\\nlidar-maps\\nplace recognition\\nautonomous driving field\\nvision-based approaches\\nimage database\\nhigh definition 3d maps\\ndeep neural network\\nshared embedding space\\n3d-lidar place recognition\\n3d dnn\\n2d-3d embedding space\\noxford robotcar dataset\\nimage w.r.t.\",\"433\":\"standardization\\nvisualization\\nlighting\\nunsupervised learning\\nprincipal component analysis\\ndimensionality reduction\\nclouds\\nconvolutional neural nets\\nimage matching\\nobject recognition\\nplace recognition performance\\nin-sequence condition changes\\nunsupervised learning methods\\nvisual place recognition\\nquery set\\nreference set\\nsingle distinctive condition\\nquery sequence\\ntraversal daytime-dusk-night-dawn\\ncnn-based descriptors\\nin-sequence changes\\ncontinuous changes\\nstatistical normalization\\npca\",\"434\":\"three-dimensional displays\\nlaser radar\\nsensors\\niterative closest point algorithm\\nimage color analysis\\ntrajectory\\nreal-time systems\\ndistance measurement\\nimage enhancement\\nimage matching\\nimage segmentation\\nobject detection\\nobject recognition\\noptical radar\\nlol system\\n3d point cloud maps\\nlidar-equipped vehicles\\n3d point segment matching method\\nlidar-only odometry and localization algorithm\\nkitti datasets\",\"435\":\"robots\\nneural networks\\nthree-dimensional displays\\ngaussian processes\\nlaser radar\\nmonte carlo methods\\nkernel\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\noptical radar\\npath planning\\nrecursive estimation\\nrobot vision\\nslam (robots)\\nprecise lidar-based robot localisation\\nlarge-scale environments\\nglobal localisation\\nmonte carlo localisation\\nmcl\\nfast localisation system\\ndeep-probabilistic model\\ngaussian process regression\\ndeep kernel\\nprecise recursive estimator\\ngaussian method\\ndeep probabilistic localisation\\nlarge-scale localisation\\nlargescale environment\\ntime 0.8 s\\nsize 0.75 m\",\"436\":\"sonar\\nstate estimation\\nrocks\\ntrajectory\\nrobot sensing systems\\nreliability\\nmobile robots\\nposition control\\nrobot vision\\nsensor fusion\\nslam (robots)\\ndeterministic approach\\ndata association\\nunderwater robot\\nsonar data\\nmembership state estimation\\nlocalization problem\\nindistinguishable landmarks\\ndiving phase\\nunknown initial position\",\"437\":\"trajectory\\nprobabilistic logic\\nrobots\\ntask analysis\\noptimization\\nkernel\\ngrasping\\nbayes methods\\nend effectors\\nlearning systems\\npredictive control\\ntrajectory control\\nimitation learning\\nconstrained skills\\nlinearly constrained optimization problem\\nnonparametric solution\\nlinearly constrained nonparametric framework\\nhuman skills learning\\nconstrained motor skills learning\\nrobotic systems\\nend-effector trajectory\\nlinearly constrained kernelized movement primitives\\nlc-kmp\\nprobabilistic properties\\nlocomotion tasks\\ngrasping tasks\\nhuman robot collaborations\",\"438\":\"lyapunov methods\\ntraining data\\nrobots\\nelectrostatic discharges\\nstability analysis\\ntrajectory\\ntask analysis\\nlearning (artificial intelligence)\\nmotion control\\nnonlinear dynamical systems\\nregression analysis\\nstability\\nenergy-based approach\\nlearned dynamical systems\\nreactive motion generation\\nstable motions\\naccurate motions\\nlearning problems\\ntraining time\\nsingle-step learning\\nregression technique\\nsingle-step approach\\nenergy considerations\\nlearned dynamics\\ndemonstrated motion\",\"439\":\"task analysis\\nrobots\\ntrajectory\\niris\\nlearning (artificial intelligence)\\niris recognition\\ngrasping\\nhuman-robot interaction\\nmanipulators\\nroboturk cans dataset\\noffline learning\\noffline robot manipulation data\\noffline task demonstrations\\nrobotics\\ngoal-conditioned low-level controller\\nhigh-level goal selection mechanism\\nlearning control\\nlearning from large-scale demonstration datasets\\nimplicit reinforcement without interaction at scale\\ncrowdsourcing\",\"440\":\"manifolds\\nrobots\\nsymmetric matrices\\nstandards\\nellipsoids\\nswitches\\nmeasurement\\ndifferential geometry\\nlearning (artificial intelligence)\\nmanipulators\\nmatrix algebra\\ngeometry-aware dynamic movement primitives\\nrobot control problems\\nmanipulability ellipsoids\\nsymmetric positive definite matrices\\ndmps\\nspd matrices\\neuclidean space\\nmathematically principled framework\\nspd manifold\\nriemannian metrics\",\"441\":\"visualization\\nloading\\nrobot sensing systems\\ntask analysis\\nfeature extraction\\nrobustness\\ncontrol engineering computing\\nearthmoving equipment\\nfoundations\\nimage representation\\nmobile robots\\nneural net architecture\\nrandom forests\\nvideo signal processing\\nhydrostatic driving pressure\\ncontrol signals\\napplication specific deep visual features\\nsiamese network architecture\\nrandom forest regressor\\nloading distance\\nautonomous robotic wheel loader\\nlearning-based pile loading controller\\ncontroller parameters\\nlow level sensor\\nboom angle\\nbucket angle\\ncross-entropy\\ncontrastive loss\\nsoil type\",\"442\":\"robots\\ncost function\\nnavigation\\nplanning\\nfeature extraction\\nheuristic algorithms\\ntask analysis\\ncost optimal control\\ndynamic programming\\nlearning (artificial intelligence)\\nlearning systems\\nmobile robots\\nobservability\\npath planning\\nprobability\\nstate-space methods\\ntrajectory control\\ninverse reinforcement learning\\nsafe navigation\\nautonomous navigation\\nunknown partially observable environments\\nnavigation behavior\\nstate-control trajectory\\ncost function representation\\nprobabilistic occupancy encoder\\nobservation sequence\\ncost encoder\\noccupancy features\\nrepresentation parameters\\ncontrol policy\\nvalue function\\nstate space\\nmotion-planning algorithm\\nrobot navigation\\nnavigation cost learning\\ndynamic\",\"443\":\"tools\\nforce\\nrobot sensing systems\\nretina\\nsurgery\\nveins\\nbiological tissues\\nbiomechanics\\nblood vessels\\neye\\nmanipulators\\nmedical robotics\\nphantoms\\nsurgical tools\\ndual force constraint controller\\nrobot-assisted retinal surgery\\ntool-to-sclera forces\\ncannulation tool\\ndual force-sensing capability\\nforce information\\nrobot controller\\nretinal vein cannulation\\ntarget vessel\\nrobotic manipulators\\ntool-to-tissue forces\\nretinal tissue injury\\nbimanual cannulation\\nbimanual robotic system\\nretinal vein occlusion\\noccluded vessel\\ninteraction forces\\nbimanual vein cannulation\\ntool-to-sclera force\\ntool-to-vessel force\\nsteady hand eye robot platforms\",\"444\":\"conferences\\nautomation\\nbiomechanics\\ndexterous manipulators\\ngrippers\\nmedical robotics\\nsurgery\\nmaster manipulator\\ncombined-grip-handle scheme\\npinch grip motion\\npower grip motion\\nrobotic surgery\",\"445\":\"catheters\\nforce\\nforce control\\nrobots\\nstability analysis\\nmagnetic resonance imaging\\nfriction\\nbiological tissues\\nbiomedical mri\\ncardiology\\nmedical image processing\\nmedical robotics\\ncontact stability analysis\\nmagnetically-actuated robotic catheter\\ncontact force quality\\nlesion formation\\nlesion size\\ngap-free lesion\\ntissue surface motion\\ncontact model\\ncontact force control schemes\\nheart surface motions\\nmagnetic resonance imaging-actuated robotic catheter\",\"446\":\"robots\\ninstruments\\nwrist\\nsurgery\\nforce\\nadaptation models\\nkinematics\\nadaptive control\\nbiomechanics\\nbiomedical equipment\\nforce control\\nmanipulators\\nmedical robotics\\nmotion control\\nposition control\\nanatomical orifice\\nminimally invasive surgery\\ninteraction forces\\npatient anatomy\\norifice behavior\\nadaptive control scheme\\nwrist velocity\\ntip velocity\\nintracorporeal targeting\\ninstrument tip positioning\\n3dof wrist center positioning problem\",\"447\":\"aggregates\\nmagnetic tunneling\\ncoils\\njunctions\\nmagnetic particles\\nblood vessels\\nmagnetic separation\\nbiomedical materials\\nbiomems\\nhaemodynamics\\nmedical robotics\\nmicrofluidics\\ntargeted embolization\\nswarm control technique\\nmean absolute error\\naggregation control approach\\nfluidic shear\\nmagnetic forces\\nmagnetic field\\nmagnetic swarm control strategy\\nclinical embolization\\nswarm control capability\\nfluidic flow environment\\nmagnetic aggregates\\nmagnetic swarms\\nrobotic control\\nrobotic swarm control\",\"448\":\"manipulators\\nsurgery\\nforce\\nfrequency modulation\\nkinematics\\nhaptic interfaces\\nswitches\\naugmented reality\\nend effectors\\nhuman-robot interaction\\nmedical robotics\\nmotion control\\nposition control\\nredundant manipulators\\nstability\\ntelerobotics\\nenergy tank model\\nhaptic feedback\\nkuka lwr4+ serial robot\\nsigma 7 haptic manipulator\\nredundant manipulator\\nrcm kinematic constraint\\nserial robot manipulator\\nremote center of motion constraint\\ndecoupled cartesian admittance control\\nend effector\\nbilateral teleoperation control stability\\nteleoperated surgery\",\"449\":\"legged locomotion\\nrobot kinematics\\ndynamics\\nnonlinear dynamical systems\\njacobian matrices\\ntrajectory\\ngait analysis\\nrobot dynamics\\ntrajectory control\\nfull-order dynamics\\nrapid generation\\nstepping-in-place gaits\\ndiagonally symmetric ambling gait\\ndynamic bipedal walking\\nquadrupedal locomotion\\nrapid gait generation\\nhybrid dynamics\\nthree-dimensional quadrupedal robot\\nhybrid zero dynamics framework\\nbipedal robots\\nbipedal walking gaits\\nquadrupedal trajectory\",\"450\":\"legged locomotion\\nfoot\\nforce\\nrobot sensing systems\\ncontrol systems\\nengines\\ncompliance control\\nforce control\\ngait analysis\\nimage motion analysis\\nposition control\\nrobot dynamics\\ntorque control\\ndirect force control\\nvicon motion capture system\\ncustom-designed platforms\\nlow-cost commercially-available hexapod robot\\nlegged robots\\ncustom-designed robotic platforms\\ncommercially-available robots\\nlow-cost research platforms\\nlow-cost joint actuators\\ntorque control capabilities\\nhierarchical control system\\nvirtual model control\\nsimple foot force distribution\\nwalking posture control system\",\"451\":\"robots\\ncorrelation\\natmospheric measurements\\nparticle measurements\\nextraterrestrial measurements\\ncollaboration\\nnavigation\\nautonomous underwater vehicles\\nfiltering theory\\nmobile robots\\nmonte carlo methods\\nmulti-robot systems\\npath planning\\nsensor fusion\\nmonterey bay\\nterrain relative navigation\\nfilter architecture\\ncollaborative multirobot localization\\nstandard trn\\nmonte carlo simulation\\ninter-vehicle range measurements\\nautonomous underwater vehicle\\nmultirobot information\\ntrn techniques\\ncovariance intersection\",\"452\":\"robot kinematics\\nmulti-robot systems\\ntime-varying systems\\ncollision avoidance\\nrobot sensing systems\\ntransforms\\ncomputational geometry\\nlinearisation techniques\\ntime-varying domains\\nnonconvex shape\\nnonconvex coverage problem\\ncontrol law\\ntime-varying density\\ntime-varying diffeomorphism\\nmultirobot control\\ntime-varying nonconvex domains\\ncoverage control\",\"453\":\"drones\\ntask analysis\\nresource management\\nrouting\\nurban areas\\nplanning\\nautonomous aerial vehicles\\ncomputational complexity\\ngraph theory\\nmulti-robot systems\\noptimisation\\nnear-optimal polynomial-time task allocation algorithm\\ndelivery sequences\\ntwo-layer approach\\nmultifaceted complexity\\nmaximum time\\ncomprehensive algorithmic framework\\npublic transit vehicles\\nefficient large-scale multidrone delivery\\ntransit network\\nbounded-suboptimal multiagent pathfinding techniques\",\"454\":\"robot sensing systems\\ntarget tracking\\ncovariance matrices\\nkalman filters\\nrobot kinematics\\nconvex programming\\ninteger programming\\nmobile robots\\nmulti-robot systems\\nmultirobot target\\nresource availability\\nnetworked multirobot system\\nsensing resources\\ncomputational resources\\ndistributed kalman filter\\nsensor measurement noise covariance matrix\\nsensing quality deteriorates\\nsystems communication graph\\nsensor quality\\nactive communication links\\nmixed integer semidefinite programming formulations\\nagent-centric strategy\\nteam-centric strategy\\ngreedy strategy\",\"455\":\"multi-robot systems\\nforce\\nmobile robots\\nforce feedback\\ncollision avoidance\\ndamping\\ngraph theory\\ntelerobotics\\nmotion pattern\\ngraph\\nmultirobot teleoperation\\ntopological constraints\",\"456\":\"robot sensing systems\\nbridges\\ngrippers\\nvibrations\\nself-assembly\\nhardware\\nmobile robots\\nmulti-robot systems\\nsearch problems\\nself-adjusting systems\\neciton robotica\\nself-assembling soft robot collective\\nsocial insects\\ncentralized control system\\narmy ants build bridges\\nflexible materials\\nrobotic collectives\\nflexible robots\\nself-assembling robotic systems\\nlattice-based structures\\nsoft robots\\namorphous structures\",\"457\":\"grasping\\nmuscles\\nrobot sensing systems\\ngold\\ntools\\nfeedback\\nfeedforward\\nlearning (artificial intelligence)\\nmanipulators\\nmaterials handling\\nsensors\\nfeedforward controls\\ninitial contact state\\npredictive network\\nsensor state transition\\nactual robot sensor information\\nfeedback control\\nstable tool-use\\nflexible musculoskeletal hands\\npredictive model\\nadaptability\\nimpact resistance\\nactuators\",\"458\":\"adaptation models\\nrobots\\ndata models\\nneural networks\\nanalytical models\\nfriction\\npredictive models\\ndexterous manipulators\\nlearning (artificial intelligence)\\nlyapunov methods\\nneural nets\\nunderactuated soft robotic hands\\ntransfer learning\\ndata limitations\\ndata collection\\nphysical robots\\ntransferred model\\ntrained transition model\\ndynamic model\\nchaotic behavior\\ndivergent behavior\\nupper bound\\nlyapunov exponent\",\"459\":\"oscillators\\ntrajectory\\nmanipulators\\nbiological system modeling\\nsoft robotics\\nmathematical model\\nend effectors\\nlearning (artificial intelligence)\\nmedical robotics\\nmotion control\\ntrajectory control\\ncyclic rhythmic patterns\\noscillatory signals\\nactuator\\ncentral pattern generator\\nperiodic motion\\nend-effector\\nmodel-free neurodynamic scheme\\ncpg model\\nsimulation model\\nlearning architecture\\nperiodic movement learning\\nmodular bio-inspired soft-robotic arm\\nreinforcement learning\\ncentral pattern generators\\nrhythmic movements\",\"460\":\"strain\\nactuators\\nadaptation models\\nstress\\nreal-time systems\\nrobots\\ncancer\\nbiomechanics\\ndeformation\\nelastic deformation\\nelasticity\\nelectric actuators\\nfinite element analysis\\nkinematics\\nmedical image processing\\nmedical robotics\\nradiation therapy\\nrobot kinematics\\nstress-strain relations\\nsoft robot\\nhead stabilization\\ncancer radiation\\nparallel robot mechanism\\nconstituent soft actuators\\nreal-time motion-correction\\ntreatment machine\\nstress-strain constitutive laws\\ninverse kinematics\\nradially symmetric displacement formulation\\nfinite elastic deformation framework\",\"461\":\"task analysis\\ncameras\\nrobot vision systems\\nthree-dimensional displays\\nrobot kinematics\\ncalibration\\nimage colour analysis\\nimage representation\\nimage sensors\\nlearning (artificial intelligence)\\npose estimation\\nrobot vision\\nstereo image processing\\nmultiple depth sensors\\nimperfect camera calibration\\nuncalibrated cameras\\ncamera-views\\nsingle view robotic agents\\nvoxel grid\\nrelative pose estimation\\n3d scene representations\\nregistered output\\nexplicit 3d representations\\nsensor dropout\\ninsertion tasks\\ntask performance\\nmulticamera approach\\nuncalibrated rgb camera\\nprecise manipulation tasks\\nclosed-loop end-to-end learning\\nmultiview approach\\nmultiple uncalibrated cameras\\nprecise 3d manipulation\",\"462\":\"task analysis\\ntrajectory\\npipelines\\ntorque\\nrobot sensing systems\\npredictive models\\nbiological tissues\\nclosed loop systems\\nimage classification\\nmanipulator dynamics\\nmedical robotics\\nrobot vision\\nsurgery\\ntorque measurement\\njoint torque measurements\\nclosed loop control law\\ngrapefruit cutting task\\ngrapefruit pulp\\nuncertain edge\\nprecision cutting\\nsoft tissue\\ntorque-based medium classification\\nvisibility constraints\\ncutting trajectory\\nbinary medium classifier\\nrobotics\",\"463\":\"task analysis\\nmanipulator dynamics\\ntrajectory\\ntextiles\\ndeformable models\\ndynamics\\nclothing\\nlearning (artificial intelligence)\\nsparse reward learning techniques\\ndeep reinforcement learning approach\\ndynamic cloth manipulation tasks\\nrigid objects\\nfollowed trajectory\\ngrasped points\\ngoal positions\\nnongrasped points\\nadequate trajectories\\ncontrol policy learning\\nsparse reward approach\\nengineering complex reward functions\\nstate representations\\ncontrol policy encodings\",\"464\":\"task analysis\\nrobots\\nplanning\\nlearning (artificial intelligence)\\ntraining\\ntrajectory\\nvisualization\\nimage motion analysis\\nmanipulators\\npath planning\\nrobot vision\\ndynamic scene changes\\nvisual inputs\\ntask-specific reward engineering\\nprevious limitations\\nreinforcement learning approach\\nprimitive skills\\nlearning methods\\nintermediate rewards\\ncomplete task demonstrations\\nvision-based task planning\\nbasic skills\\nsynthetic demonstrations\\ndata augmentation\\nmanipulation tasks\\nur5 robotic arm\\nversatile robotic manipulation\\nrobotics\\ntraditional task\\nmotion planning methods\\nstate observability\",\"465\":\"robots\\ntask analysis\\nrobustness\\ngrasping\\ndata models\\ncalibration\\nadaptation models\\nimage texture\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nprobability\\nrobot vision\\npixel-wise probability affordance map\\nimage space\\nworld space\\nviewpoints\\nmultiple-object pushing\\nmultiple-object grasping\\nphysical world\\nvision-based robotic object manipulation\\naffordance space perception network\\ndeep neural network\\n3d affordance space\\ntraining strategy\\ntask-agnostic framework\\nsingular-object pushing\\nsingular-object grasping\",\"466\":\"observability\\nmathematical model\\naerodynamics\\nstate estimation\\nglobal positioning system\\nmagnetometers\\npressure measurement\\ninertial systems\\nkalman filters\\nmagnetic sensors\\nnonlinear filters\\npressure sensors\\nremotely operated vehicles\\nsingular value decomposition\\nuav\\ncost-efficient onboard flight state estimation\\nrobustness\\nmems-based inertial system\\nstatic pressure sensors\\ndynamic pressure sensors\\nmagnetic sensor\\nweak magnetic field\\nnecessary condition\\nsystem state\\nin-depth observability analysis\\nsensor data\\ntest flights\\nekf\\nundisturbed estimates\\nwind state variable\\nobservable spaces\\nmultisensor extended kalman filter\\nsvd\\nglider\",\"467\":\"cameras\\ncurrent measurement\\njacobian matrices\\ncalibration\\ndocumentation\\nestimation\\nrobot sensing systems\\nestimation theory\\nimage filtering\\nkalman filters\\nrobot vision\\nslam (robots)\\nresearch platform\\nvisual-inertial estimation research\\nopen sourced codebase\\nvisual-inertial systems\\nvisual-inertial estimation features\\non-manifold sliding window kalman filter\\nconsistent first-estimates jacobian treatments\\nmodular type system\\nextendable visual-inertial system simulator\\ncompeting estimation performance\\nopenvins\\nonline camera intrinsic calibration\\nopen sourced algorithms\\nonline camera extrinsic calibration\\ninertial sensor time offset calibration\\nslam landmarks\\nstate management\",\"468\":\"cameras\\nsensors\\ncollaboration\\nstate estimation\\nthree-dimensional displays\\ncalibration\\nrobots\\ncommunication complexity\\ninertial navigation\\nkalman filters\\nnonlinear filters\\nposition measurement\\ncommunication links\\nversatile filter formulation\\nindependent state estimation\\nrelative position measurements\\naided inertial navigation\\nq-esekf\\nimu propagation\\ndecentralized collaborative state estimation\\nquaternion-based error-state extended kalman filter\\ncse concept\\nprobabilistic reinitialization\\nprominent benchmark datasets\",\"469\":\"optimization\\njacobian matrices\\nmaximum likelihood estimation\\ntime measurement\\ncameras\\nvisualization\\ncalibration\\ninertial navigation\\nmonte carlo methods\\noptimisation\\nrobot vision\\nsensor fusion\\nslam (robots)\\nanalytic combined imu integration\\nvisual inertial navigation\\nbatch optimization\\nvisual sensor fusion\\nrobotic tasks\\npartial-fixed estimates\\naci2\\ninertial measurement unit\\nmonte-carlo simulations\",\"470\":\"acceleration\\nrobot sensing systems\\nskin\\ngyroscopes\\naccelerometers\\ncalibration\\nestimation theory\\nfeedback\\nhumanoid robots\\nkalman filters\\nmanipulator kinematics\\nmotion control\\nredundant manipulators\\nfloating-base robots\\nredundant acceleration feedback\\nartificial sensory skin\\nestimation method\\nsecond-order kinematics\\nhighly redundant distributed inertial feedback\\nlinear acceleration\\nrobot link\\nskin acceleration data\\nlink level\\nstate dimensionality reduction\\nmain inertial measurement unit\\nsigma-point kalman filter\\njoint velocities\\nreem-c humanoid robot\\nacceleration feedback\\nartificial robot skin\",\"471\":\"synchronization\\nrobot sensing systems\\ncameras\\nlaser radar\\nclocks\\nvoltage control\\nhardware\\ndata acquisition\\nimage sensors\\nmicrocontrollers\\nmobile robots\\noptical radar\\nrobot vision\\nsynchronisation\\nmonocular camera\\nmobile robotic platform\\ntime synchronization architecture\\ntime synchronization approach\\naccurate time synchronization\\nevent-based camera dataset acquisition platform\\ndynamic visual sensor\\nnext-generation vision sensor\\nevent-based vision\\ndataset creation\\ntemporal accuracy\\nhigh temporal resolution\\nevaluation task\\nvisual data\\nevent camera\\nambient environment sensors\\nclock-based time synchronization\\nlidar\\npic32 microcontroller\",\"472\":\"robots\\nintegrated circuit modeling\\nimpedance\\nmathematical model\\ntask analysis\\ncollaboration\\nhuman-robot interaction\\npredictive control\\ncollaborative robotics\\nhigh performance control\\nmodel predictive impedance control\\nhuman robot compliant interactions\\nimpedance control\\nphysical human-robot interaction\",\"473\":\"kinematics\\nrobots\\nredundancy\\ntask analysis\\ncollaboration\\ntorque\\nsafety\\nactuators\\nbiomechanics\\ndesign engineering\\ndexterous manipulators\\nend effectors\\nmanipulator kinematics\\nmotion control\\nredundant manipulators\\nkinematic modeling\\ncompliance modulation\\nbracing constraints\\nlow torque actuators\\npassive safety reasons\\nhuman operator\\nin-situ collaborative robots\\nconflicting demands\\nlow torque actuation\\ndeep confined spaces\\nconstrained kinematics\\nendeffector compliance\\nredundancy resolution framework\\ndirectional compliance\\nend-effector dexterity\\nkinematic simulation results\\nredundancy resolution strategy\\nkinematic conditioning\\nbracing task\\nadmittance control framework\\ncollaborative control\\niscr\\nbracing\\nredundancy resolution\\nstiffness modulation\\ncompliance\\ncollaborative robots\",\"474\":\"impedance\\nbandwidth\\nforce\\nactuators\\ntorque\\nmanipulators\\nsprings\\nelasticity\\nflexible manipulators\\nforce control\\nhaptic interfaces\\nhuman-robot interaction\\nimpact (mechanical)\\nstability\\ntelerobotics\\ntime domain passivity approach\\nbandwidth control\\nseries elastic actuator based manipulators\\nelastic element\\nsystem durability\\nsea manipulator\\nsystem bandwidth\\nimpedance control\\nsystem stability\\nsuccessive stiffness increment approach\\nhaptic domain\\ntdpa\\nsystem passivity\\nimpact force\\nteleoperation\\ntwo-port electrical circuit network\\nsize 350.0 m\\nsize 120.0 m\",\"475\":\"surface impedance\\nrobot kinematics\\ntask analysis\\nforce\\nmanipulators\\nthumb\\nbiomechanics\\ndexterous manipulators\\nforce control\\nmotion control\\npath planning\\nregression analysis\\nsupport vector machines\\nunified motion-force control approach\\nhuman limb\\ncompliant robotic massage\\ndynamical system approach\\nskin surface\\nrobot fingers\\ncomplexity increases\\nmanipulation tasks\\ndynamical systems\\nnonflat surface\\nphysical interactions\\narm-hand motion-force coordination\\ndesired motion patterns\\nunknown surface\\nmannequin arm\\nallegro robotic hand\\nkuka iiwa robotic arm\\nrobotic fingers\\nds-based impedance control\\ndesired motions\\ndistance-to-surface mapping\",\"476\":\"exoskeletons\\ntorque\\nestimation\\nmuscles\\nimpedance\\nartificial neural networks\\nlegged locomotion\\nadaptive control\\nbiomechanics\\nelectromyography\\ngait analysis\\nmedical robotics\\nmedical signal processing\\nmotion control\\nneurocontrollers\\npatient rehabilitation\\nradial basis function networks\\nrobot dynamics\\ntorque control\\nunpredictable human body movements\\ncomplex body movements\\nelaborate control strategy design\\nuncertain dynamical parameters\\nhuman-exoskeleton interaction\\nlower limb exoskeleton\\nbio-signal enhanced adaptive impedance controller\\nrehabilitation lower-limb exoskeleton\\nexoskeleton track desired motion trajectory\\nradial basis function neural network enhanced adaptive impedance controller\\nsurface electromyogram signals\\nneural network-based torque estimation method\\njoint torque\\nhuman lower extremity dynamics\\nhuman operator walking\",\"477\":\"task analysis\\nvisualization\\nfeature extraction\\nneural networks\\nrobot kinematics\\nthree-dimensional displays\\ngradient methods\\nimage representation\\nlearning (artificial intelligence)\\nneural net architecture\\nparticle filtering (numerical methods)\\nrobot vision\\ndmn architecture\\nend-to-end differentiable\\nsparse visual localization\\nend-to-end learning\\ndifferentiable mapping network\\nspatially structured view-embedding map\\nsubsequent visual localization\\nlearning structured map representations\\nstreet view dataset\\nparticle filter\\ngradient descent\\nrobotics\\nneural network architecture\",\"478\":\"task analysis\\nmeasurement\\nrobots\\nfeature extraction\\ntraining\\nvisualization\\npipelines\\nconvolutional neural nets\\nimage representation\\nlearning (artificial intelligence)\\nvideo signal processing\\ntask-specific objects\\nintended task\\nimitation learning\\nvideo demonstration\\nend-to-end self-supervised feature representation network\\nvideo-based task imitation\\nmultilevel spatial attention module\\nspatial features\\nweighted combination\\nmultiple intermediate feature maps\\nrespective feature maps\\nmetric learning loss\\nmultiple view points\\nat-net features\\nreinforcement learning problem\\nattentive task-net\\nself supervised task-attention network\\nneural connections\\nlearning task-specific feature embeddings\\ntemporally consecutive frames\\npublicly available multiview pouring dataset\\nrl agent\\ngazebo simulator\\ncnn pipeline\",\"479\":\"task analysis\\nlighting\\nobject recognition\\ncameras\\nrobot vision systems\\nclutter\\ncontrol engineering computing\\ndata visualisation\\nlearning (artificial intelligence)\\nrobot vision\\nservice robots\\nlifelong deep learning\\nvisual algorithms\\nstandard computer vision datasets\\nadaptive visual perceptual systems\\nlifelong robotic vision dataset\\nlifelong object recognition algorithms\\nlifelong learning algorithms\\nopenloris-object dataset\\nobject recognition task\\nrobotic vision\\nimagenet dataset\\ncoco dataset\",\"480\":\"task analysis\\nestimation\\nvideos\\noptical imaging\\nadaptive optics\\ntraining\\ncameras\\ngeometry\\nimage classification\\nlearning (artificial intelligence)\\nvideo signal processing\\nmonocular depth estimation\\nimagenet-pretrained networks\\nsemantic information\\nspatial information\\nper-pixel depth estimation\\ngeometric-pretrained networks\\ngeometric-transferred networks\\nself-supervised geometric pretraining task\\nconditional autoencoder-decoder structure\",\"481\":\"robot sensing systems\\noptical fiber sensors\\nsubstrates\\noptical fiber theory\\nmedical robotics\\nsurgery\\nbragg gratings\\nclosed loop systems\\nendoscopes\\nfibre optic sensors\\nmanipulators\\nmotion control\\nclosed-loop motion control\\nrobotic endoscopic grasper\\nrobotic joint\\nautonomous robotic surgery\\nmotion hysteresis\\nendoscopic surgical robot\\njoint rotation angle sensing\\ntendon-sheath mechanisms\\nfiber bragg grating\\nflexible robots\\nsurgical robotics\\nfiber optics sensor\\ntendon sheath mechanism.\",\"482\":\"three-dimensional displays\\nrobot sensing systems\\nplastics\\ngeometry\\nlight emitting diodes\\ncameras\\ndexterous manipulators\\ntactile sensors\\ntactile fingertip sensors\\ndexterous robotic manipulation\\ndexterous multifingered hands\\nillumination geometry\\ndexterous manipulation tasks\",\"483\":\"robot sensing systems\\nforce\\nlegged locomotion\\nfoot\\nsensor arrays\\nforce sensors\\nbiomechanics\\ngait analysis\\npressure sensors\\nfoottile\\nrugged foot sensor\\npressure sensing\\nsoft terrain\\nsensor design\\nstandard biomechanical devices\\npressure plates\\npressure distribution\\nground reaction force estimation\\nsensing capabilities\\nwaterproof sensor\\nreaction force\\nforce plates\\nrough terrain\\ngranular substrate\\nliquid mud\\nmass 0.9 g\\nfrequency 330.0 hz\",\"484\":\"legged locomotion\\nperturbation methods\\nassistive devices\\nsensors\\nadaptation models\\nbiological system modeling\\nbiomechanics\\ngait analysis\\nhandicapped aids\\nhumanoid robots\\nassistive walking device\\nfall prevention\\ncontrol policy\\naugmented assistive device\\nmodels realistic human gait\\nrobust human walking policy\\nactuators\\nonboard sensors\\nrecovery policy\\nfall predictor\",\"485\":\"force\\nhip\\nlegged locomotion\\nmuscles\\nbelts\\ntorque\\nwindings\\ngait analysis\\nmedical control systems\\nmuscle\\nassistive force\\nphase shift factor\\nforce function\\nforce magnitude\\nassistive torque\\nmuscle force\\nbelt-type hip assist suit\\nswing leg\\nlifting\\nwalking\\nrectus femoris\\nankle joints\\nmid-swing phase\\nwalk ratio\",\"486\":\"sockets\\nbladder\\nvalves\\nfingers\\nprosthetics\\nlaser beam cutting\\nhuman-robot interaction\\nmedical robotics\\npneumatic systems\\npressure control\\nreal-time systems\\nwearable robots\\nhuman-socket interface\\nwearable device\\nsynthetic forearm model\\nreal time pressure regulation\\nautomated hands free donning\\nautomated underactuated donning mechanism\\nsoft pneumatic socket\\nrobotic prostheses\\ninterface pressure regulation\",\"487\":\"muscles\\nlegged locomotion\\nkinematics\\ntendons\\nreal-time systems\\nelectromyography\\ngait analysis\\nimage segmentation\\nimage sequences\\nmedical robotics\\nmuscle\\npatient rehabilitation\\nautomated routine\\nnormalized gait cycle\\nreal-time rates\\ngait cycle\\nmanual estimation\\nhealthy individuals\\npersons post-stroke walking\\ncomfortable walking speed\\nonset timing\\nautomated detection\\nsoleus concentric contraction\\nvariable gait conditions\\nindividualized assistance\\nchanging gait\\nexosuit control strategy\\nmuscle power\\nsoleus muscle\\npositive power\\nlow-profile ultrasound system\\nwalking individuals\\nbiological mechanisms\\nfrequency 130.0 hz\",\"488\":\"tracking\\nshoulder\\nelectrodes\\ncapacitive sensors\\nrobot sensing systems\\nstrain\\nbiomechanics\\nbiomedical measurement\\ncoaxial cables\\ninertial systems\\nkinematics\\nmean square error methods\\nmotion measurement\\npatient monitoring\\npatient rehabilitation\\nreadout electronics\\nregression analysis\\nstrain sensors\\nwearable motion tracking\\nground truth optical motion capture system\\nstrain sensor data\\njoint angle estimation\\nnormalized root mean square errors\\njoint velocity estimation\\nrecursive feature elimination-based sensor selection analysis\\nshoulder kinematics estimation\\nsoft strain sensors\\nunobtrusive approach\\nnoncyclic joint movements\\ncyclic arm movements\\nrandom arm movements\\nshoulder joint\\ncustomized readout electronics board\\nsewn microcoaxial cables\\ntextile-based capacitive strain sensors\\nmultidegree-of-freedom noncyclic joint movements\",\"489\":\"task analysis\\ntrajectory\\ncognition\\nplanning\\nmotion segmentation\\nhuman-robot interaction\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\nvideo signal processing\\ngoal-based imitation learning\\nthird-person video demonstration\\nhuman demonstrators\\nmotion reasoning\\nmotion planning\",\"490\":\"electroencephalography\\ndecoding\\nelectronic learning\\ntask analysis\\nhuman-robot interaction\\nrobot learning\\nlearning (artificial intelligence)\\nmedical robotics\\nmedical signal processing\\nlearned policy\\nlearning phases\\ncurrent control strategy\\nintrinsic human feedback\\nintrinsic interactive reinforcement learning approach\\nhuman-robot collaboration\\nflexible adaptation\\nreal-world robotic applications\\nreinforcement signals\\nflexible online adaptation\\nlearning progress\",\"491\":\"semantics\\nfeature extraction\\nobject oriented modeling\\nneural networks\\nconvolution\\nautonomous robots\\nconvolutional neural nets\\ngraph theory\\nlearning (artificial intelligence)\\nnatural language processing\\nobject detection\\nrecurrent neural nets\\nrobot vision\\nsequential scenes\\nrecurrent neural network\\ngraph convolutional network\\nobject-oriented semantic graphs\\nsemantic graph mapping\\nnatural question generation\",\"492\":\"task analysis\\ntraining\\nhazards\\nrobot sensing systems\\nservice robots\\nhuman-robot interaction\\nindustrial robots\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\noccupational safety\\nrisk management\\nhazard source\\nsafety engineers\\nrisk assessment processes\\ndeep rl agents\\nhuman-robot collaboration\\nhrc-productivity\\ndeep reinforcement learning\\nsystematic methodology\\ncore components\",\"493\":\"robots\\ntask analysis\\nplanning\\nnatural languages\\naerospace electronics\\ncognition\\nspace exploration\\nconvolutional neural nets\\nmobile robots\\nnatural language processing\\npath planning\\ndeep compositional robotic planners\\nnatural language commands\\nsampling-based robotic planner\\ncontinuous configuration space\\ncomplex command\\nsampling-based planner\\nrecurrent hierarchical deep network\",\"494\":\"task analysis\\nlattices\\ncost function\\nmobile robots\\nrobot motion\\nlearning (artificial intelligence)\\nhuman-robot interaction\\nlearning systems\\npath planning\\nrobot programming\\nstate lattices\\nautonomous mobile robots\\nmotion planning problem\\nrobot traffic\\nmotion features\\nlearned user preferences\\nlearning from corrections\\nalgorithm completeness proving\\nhuman robot interaction\",\"495\":\"agriculture\\nnavigation\\ncameras\\nrobot vision systems\\nvisualization\\nmonitoring\\nagricultural robots\\nagrochemicals\\ncrops\\nmobile robots\\npath planning\\nrobot vision\\nvisual servoing\\nvisual servoing-based navigation\\nautonomous navigation\\nfield robots\\nprecision agriculture tasks\\nvisual-based navigation framework\\ncrop-row structure\\nrow-crop fields monitoring\",\"496\":\"robots\\noptimized production technology\\nrouting\\nheuristic algorithms\\ntask analysis\\nirrigation\\nautomation\\ncomputational complexity\\ngraph theory\\nmobile robots\\noptimisation\\nscheduling\\ncop-fr\\noptimal solutions\\nhighly unbalanced rewards\\noptimal routing schedules\\naisle-structures\\nconstant-cost orienteering problem\\ntravel budget\\naisle-graph\\nloosely connected rows\",\"497\":\"planning\\nkinematics\\nmanipulator dynamics\\nacceleration\\nstability analysis\\ncomputational modeling\\nhumanoid robots\\nlegged locomotion\\nmobile robots\\nmotion control\\noptimisation\\npath planning\\nrobot kinematics\\nstability\\ntimber\\ntime optimal motion planning\\nzmp stability constraint\\ntimber manipulation\\ndynamic stability-constrained optimal motion\\ntimber harvesting machine\\nrough terrain\\nkinematics model\\noptimization problem\\ncomputation time\\nmotion plan\\ndynamic stability constraint\\nzero moment point stability measure\",\"498\":\"grippers\\nrobots\\nthree-dimensional displays\\ndrag\\ntrajectory\\nforce\\nradio frequency\\nagricultural robots\\ncollision avoidance\\ncontrol engineering computing\\nimage colour analysis\\nimage segmentation\\nindustrial robots\\nmobile robots\\nobject detection\\nrobot vision\\npoint cloud operation\\ndeep learning\\ncolor thresholding\\nimage processing\\nactive obstacle separation\\nlinear motions\\nzig-zag push\\nseparation motion\\ndrag motions\\nobstacle avoidance\\ntarget fruit\\nfruit harvesting robots\",\"499\":\"conferences\\nautomation\\ncalibration\\ncameras\\nimage filtering\\ninfrared imaging\\noptical radar\\n3d lidar\\ninfrared images\\ninfrared filter\\ncalibration method\\nsimultaneous location and mapping\\nvelodyne vlp-16 sensor\",\"500\":\"calibration\\ncameras\\nlaser radar\\nimage edge detection\\nrobot sensing systems\\nsemantics\\nrobustness\\ncomputer vision\\nimage colour analysis\\nimage motion analysis\\noptical radar\\noptimisation\\nsensor fusion\\nrgb camera\\nlight detection and ranging sensor\\nautonomous vehicles\\noutdoor environment\\nonline calibration technique\\noptimal rigid motion transformation\\nmutual information\\nperceived data\\noptimization problem\\nsemantic features\\ntemporally synchronized camera\\nautonomous driving tasks\\nonline camera-lidar calibration\\nsensor semantic information\\nsensor data fusion\\nsensor calibration\\ncomplex settings\\nsuboptimal results\\nextrinsic calibration\\nedge feature based auto-calibration\\ncutting-edge machine vision\\ncalibration quality metric\\nlidar sensor\",\"501\":\"three-dimensional displays\\nimage segmentation\\nrobots\\noptimization\\nimage edge detection\\ncalibration\\ncameras\\nfeature extraction\\ngaussian processes\\nimage reconstruction\\nimage registration\\nindustrial robots\\noptimisation\\npose estimation\\nproduction engineering computing\\nrobot kinematics\\nrobot vision\\nsemiconductor device manufacture\\nsemiconductor technology\\nprecise 3d calibration\\nvisual detection\\nelliptic-shape wafers\\n3d poses\\nrobust ellipse detection\\ntracking algorithm\\ncalibration parameters\\nrobot-camera system\",\"502\":\"cameras\\ncalibration\\npose estimation\\nrobustness\\ngeometry\\ntransforms\\nrobot vision systems\\nmotion estimation\\noptimisation\\ntree searching\\nselfie stick\\nvideo selfie\\nshort continuous video clip\\nselfie photos\\ncamera motion\\nfast branch-and-bound global optimization\\nglobally optimal relative camera pose estimation\\nspherical joint motion\\n3-dof search problem\",\"503\":\"cameras\\ncalibration\\noptimization\\nmotion estimation\\ngeometry\\nautomobiles\\nmirrors\\ncomputer vision\\npath planning\\nroad vehicles\\nlarge-scale outdoor experiments\\nindoor experiments\\nexterior orientation parameters\\nhighly practicable online optimisation strategy\\ncomplete online optimisation strategy\\ncamera-to-camera transformations\\ncamera-to-vehicle rotations\\ncamera positions\\nexterior orientation calibration\\nvision-based vehicle motion estimation\\nneighbouring views\\nextrinsic calibration\\nintelligent vehicle behaviour\\nexterior perception modality\\npassenger vehicles\\nvehicle-mounted surround-view camera system\\nonline calibration\",\"504\":\"cameras\\ncalibration\\nrobots\\ntraining\\nsensor systems\\nimage colour analysis\\nimage motion analysis\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nrobot vision\\ndeep convolutional neural network\\nsemisynthetic dataset generation pipeline\\nrgb cameras\\nvision sensors\\ndata-driven approach\\nrobotic platforms\\ncamera miscalibration detection\",\"505\":\"pipelines\\nrobot sensing systems\\nshape\\ngrippers\\nservice robots\\nmanipulators\\nindustrial manipulators\\nmaterials handling\\nmulti-robot systems\\nrobotic general parts feeder\\nmultiple objects\\nmanufacturing industry\\nmultirobot system\\nkitting\\nautomatic multiple parts feeding problem\\ncoarse-to-fine manipulation process\\nmultiple robot arms\\nmpph\\ntraditional parts feeder\\nvarious-shaped industrial parts\\nrobotic bin-picking system\\nautomatic parts feeding\\nmean picks per hour\",\"506\":\"planning\\nrobot sensing systems\\ntask analysis\\ndecision making\\ncollision avoidance\\nreal-time systems\\ncontrol engineering computing\\nindustrial manipulators\\ninference mechanisms\\nlearning (artificial intelligence)\\npath planning\\nproduction engineering computing\\nunloading\\nreasoning framework\\nindustrial manipulator robot\\nreal-time motion planning\\ncomplex robotic system\\nhigh-level decision-making\\nbelief space planning\\noffline learning\\nexecution module\\nrobot truck unloading\\nonline decision-making\",\"507\":\"robot sensing systems\\nmeasurement by laser beam\\ndelays\\nposition measurement\\ncollaboration\\nsensor systems\\nhumanoid robots\\nimage sensors\\noptical sensors\\noptical tracking\\nethercat channel\\nperception latency evaluation\\n3d vision-based sensor system\\nlaser-tracker system\\nhuman-robot collaborative environment\\nactuation system\",\"508\":\"grippers\\nfasteners\\nrobotic assembly\\nmanipulators\\nindustries\\nsensors\\ncost reduction\\ndexterous manipulators\\nindustrial manipulators\\nlegged locomotion\\nmaterials handling\\nposition control\\nrobot arm\\npeg-in-hole assembly\\nparallel-jaw gripper\\nindustry assembly lines\\nparting feeding machine\\nsorting process\\ngrasping process\\ntwo-fingered gripper\\ndesign engineering\\norientation control\\noffset position control\",\"509\":\"microphones\\nacoustics\\nrobot sensing systems\\ninterference\\nfrequency-domain analysis\\nacoustic signal processing\\nacoustic wave interference\\naircraft control\\ndistance measurement\\noscillations\\npropellers\\nbio-inspired distance estimation\\nacoustic signature\\nmotor-propeller system\\nsingle microphone\\naudible frequency band\\npower spectrum\\nbroadband oscillation\\nmps\\nbroadband constructive-destructive interference pattern\",\"510\":\"bars\\nsprings\\nbirds\\nneck\\nmanipulator dynamics\\ntrajectory\\nactuators\\nmanipulator kinematics\\nmotion control\\nlight-weight manipulator\\ntensegrity x-joints\\nmanipulators\\nanti-parallelogram joints\\ntensegrity one-degree-of-freedom mechanism\\n3-degree-of-freedom manipulator\\ntensegrity x-joint\",\"511\":\"torque\\nactuators\\nlegged locomotion\\nhysteresis\\nexoskeletons\\npneumatic systems\\nbending\\nbiomechanics\\nmotion control\\npneumatic actuators\\nrobot dynamics\\ntorque control\\ninput pressure\\nantagonistic soft chambers\\nmechanical performance\\nsafe compliant actuation\\nenhanced torque output\\nlobster leg joint\\nmusculoskeletal structure\\nbending module\\nlobster-inspired antagonistic actuation mechanism\\nmaximum torque output\\nfabricated module\\nstiffness tuning\\nangle control\\nbending angle\",\"512\":\"soft robotics\\nmimics\\nbiology\\nfabrication\\nstrain\\nmathematical model\\nbiomechanics\\necology\\nelastomers\\nmobile robots\\npneumatic actuators\\nzoology\\nsnake survival\\nsoft robots\\nfiber-reinforced elastomeric enclosures\\nanti-predator behaviors\\nlive snakes\\ncurvature values\\nsoft-robotic head\\nsoft robot motion durations\\ndistinct anti-predatory behavior\\nlive snake observations\\ncoral snake anti-predator thrashing\\nsoft-robotic platform\\nnonlocomotory movements\\nanimal-robot interactions\\ncoral snakes\\nsnake species\",\"513\":\"conferences\\nautomation\\ngait analysis\\nmuscle\\nneurophysiology\\nprosthetics\\ntorque\\nmuscle activity\\nhuman ankle function\\ndirectional mechanical impedance\\nactive muscles\\nstanding posture\\nreconstructed torque\\nankle states\\nankle angle\",\"514\":\"robot sensing systems\\nestimation\\nfriction\\nlegged locomotion\\nfoot\\nforce\\nhaptic interfaces\\ncontact surface estimation\\nhaptic perception\\nlegged systems\\ncontact force\\nsurface geometry\\nvision system\\nharsh weather\\nsurface information\\nhaptic exploration\",\"515\":\"robustness\\ntrajectory optimization\\nuncertainty\\nlearning (artificial intelligence)\\nrobots\\ncontrol engineering computing\\nmanipulators\\nnonlinear control systems\\nnonlinear programming\\nopen loop systems\\nlocal policy optimization\\ntrajectory-centric reinforcement learning\\nlocal stabilizing policy optimization\\ntrajectory-centric model-based reinforcement learning\\nglobal policy optimization\\nnonlinear systems\\nrobotic manipulation tasks\\nopen-loop trajectory optimization\\nlocal policy synthesis\\nsingle optimization problem\",\"516\":\"friction\\ndynamics\\nsnake robots\\nforce\\nadaptation models\\nheuristic algorithms\\noptimal control\\ndrag\\nmobile robots\\nmotion control\\npareto optimisation\\npredictive control\\nrobot dynamics\\ntrajectory control\\nautomatic snake gait generation\\nundulatory gaits\\nmovement pattern\\nserpenoid curve\\nmodel predictive control\\nlocomotion gaits\\ntrajectory optimization\\nsnake dynamics\\nanisotropic dry friction\\nviscous friction\\nfluid dynamic effects\\npareto-optimal serpenoid gaits\",\"517\":\"drones\\ncomputer crashes\\nreal-time systems\\nrobot sensing systems\\ncomputer architecture\\nneural networks\\ndata models\\naerospace computing\\nautonomous aerial vehicles\\nconvolutional neural nets\\nfault diagnosis\\nlearning (artificial intelligence)\\nneural net architecture\\npattern classification\\nrecurrent neural nets\\nsensor fusion\\nraw sensor data\\ndrone misoperations\\nunmanned aerial vehicle fault cause detection\\ndeep learning architectures\\nfault cause identification\\ndrone software cyberattack\\ndeep convolutional neural network\\nlong short term memory neural network\\nautoencoder\\nreal time sensor data classification\",\"518\":\"trajectory\\ngrippers\\nrobot sensing systems\\nplanning\\nmanipulators\\noptimization\\ncollision avoidance\\nconcave programming\\ndexterous manipulators\\nindustrial manipulators\\nmanipulator dynamics\\nmotion control\\nquadratic programming\\nwarehouse automation\\ngomp\\nbin-picking robot\\nrobot dynamics\\ngrasp planner\\nmotion planner\\nrobot bin picking\\npicks-per-hour\\npph\\ngrasp-analysis tools\\nrobot gripper\\ngrasp-optimized motion planning\\ndex-net\\ndegree of freedom\\nsequential quadratic programming\\nobstacle avoidance\\ntime-minimization\",\"519\":\"planning\\ntask analysis\\nsmoothing methods\\nresource management\\nmobile robots\\nwheels\\ncollision avoidance\\ninteger programming\\nlinear programming\\nmulti-robot systems\\nplanetary rovers\\ntravelling salesman problems\\ntrees (mathematics)\\njumping rover team\\nrobotic team\\nunmanned ground vehicles\\nhybrid operational modes\\nmultiple traveling salesman problem\\nmtsp\\nground surface\\njumping capability\\noptimal path\\nmixed-integer linear programming problem\\nrrt*\\nmultiple ugv\\noptimized motion\\ncustomized jumping rovers\\njumping robots\\npath planning\\nrapidly-exploring random tree\\nmixed-integer linear programming\",\"520\":\"three-dimensional displays\\nimage reconstruction\\nsurface reconstruction\\ncomputational modeling\\nsolid modeling\\ntrajectory\\ncameras\\nstereo image processing\\nlow-quality surfaces\\nexploration trial\\nactive 3d modeling\\nonline multiview stereo\\nlarge-scale structure monitoring\\nimage acquisition\\nreconstruction quality\\nview path-planning method\\nonline mvs system\\nonline feedbacks\\ncamera trajectory\\nreal-time three-dimensional model construction\",\"521\":\"convergence\\noptimization\\nrobots\\nplanning\\ncomplexity theory\\ntrajectory\\ngaussian distribution\\ncollision avoidance\\ncomputational complexity\\ngaussian processes\\ngraph theory\\nmanipulators\\nmobile robots\\noptimisation\\nsingle homotopy class\\nasymptotic convergence\\nsingle dof\\npath segments\\nrsc\\nzero-volume convergence region\\nadjustment method\\nhigh dof configuration spaces\\nreoriented short-cuts\\nlocally optimal path short-cutting\\nhigh degree of freedom problems\\ninformed gaussian sampling technique\\nigs\\ncollision checking computation\\nrobot manipulation\\nrotation oriented problems\\ntranslation oriented problems\",\"522\":\"uncertainty\\nnavigation\\nrobots\\ntask analysis\\ntraining\\nestimation\\nlearning (artificial intelligence)\\ncontrol engineering computing\\nmobile robots\\nneural nets\\nsafety-critical software\\ndiverse environments\\nenvironmental uncertainty\\nresilient behaviors\\ndeep reinforcement learning\\ncomplex behaviors\\nadaptive behaviors\\nuncertainty-aware predictor\\nuncertainty-aware navigation network\\nneural network\\nsafety-critical tasks\\nmobile robot\",\"523\":\"aircraft\\ncontrol systems\\nattitude control\\naerodynamics\\nposition measurement\\naircraft propulsion\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\ncascade control\\nhelicopters\\nmobile robots\\nnonlinear control systems\\ntrajectory control\\nnonlinear vector-projection control\\nagile fixed-wing aircraft\\nfixed-wing platforms\\nnonlinear control strategy\\nautonomous flight\\ncascaded control structure\\ninner attitude control loop\\nspecial orthornormal group\\nouter position control loop\\nthrust command\\nattitude references\\nlift forces\\nagile fixed-wing unmanned aerial vehicles\\nrotorcraft\",\"524\":\"force\\naerodynamics\\natmospheric modeling\\naircraft\\nrotors\\npropellers\\nadaptation models\\nadaptive control\\naerospace components\\naircraft control\\ncomputational fluid dynamics\\nmobile robots\\nnonlinear control systems\\ntracking\\nadaptive nonlinear control\\nairflow vector sensing\\ncomplex aerodynamic interactions\\nlinear force models\\nrotor forces\\nthree-dimensional airflow sensor\\ncustom-built fixed-wing vtol\\nforce prediction\\nbaseline flight controllers\\nfixed-wing vertical take-off and landing aircraft\",\"525\":\"robot sensing systems\\nrobot kinematics\\npayloads\\nshape\\nprototypes\\naerospace robotics\\ncontrol system synthesis\\nmobile robots\\nposition control\\narc-alpha prototype\\nmultilinked microaerial vehicles\\nreconfigurable aerial robotic chain\\nmultiple parallel angular controllers\\nmodel predictive position control loop\\ncontroller design\\nconnected aerial vehicles\\nsystem dynamics\\nsystem extendability\\ndistributed sensing\",\"526\":\"acceleration\\naccelerometers\\npropellers\\nrobustness\\nattitude control\\nzirconium\\nmathematical model\\nacceleration control\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nfeedback\\nhelicopters\\nmotion control\\nposition control\\nposition measurement\\nregression analysis\\nthree-term control\\ndirect feedback\\nnovel regression-based filter\\ncommanded propeller speeds\\nlow-latency acceleration measurements\\ncontrol feedback\\nlow frequency position measurements\\npid strategy\\nnoisy acceleration measurements\\nquadrotor aerial vehicles\\ndirect acceleration feedback control\",\"527\":\"rotors\\nresource management\\nactuators\\ntrajectory\\nforce\\nquaternions\\ntorque\\nautonomous aerial vehicles\\ncontrol system synthesis\\nmicrorobots\\nmobile robots\\nnonlinear control systems\\nobservers\\noptimal control\\npredictive control\\nrobot dynamics\\nrobot kinematics\\ntrajectory control\\ntrajectory tracking nonlinear model predictive control\\nomnidirectional microaerial vehicles\\nfree space\\nrigid body model based approach\\nreceding horizon\\noptimal wrench commands\\nmechanical design\\noptimal actuator commands\\ndisturbance observer\\noffset-free tracking\\n6dof trajectories\",\"528\":\"oscillators\\ndamping\\nmanipulators\\nrobot sensing systems\\ntask analysis\\ncranes\\ncontrol system synthesis\\nfeedback\\nlinear quadratic control\\nlinearisation techniques\\nnonlinear control systems\\noptimal control\\npendulums\\nrobust control\\nvibration control\\ncontrol action\\nsimplified sam model\\nmodel uncertainties\\noptimal oscillation damping control\\ncable-suspended aerial manipulator\\nsingle imu sensor\\ndouble pendulum\\noutput feedback linear quadratic regulation problem\\nminimal energy consumption\",\"529\":\"optimization\\nsociology\\nstatistics\\nvehicle dynamics\\nhardware\\nrobots\\nplanning\\nautomobiles\\nmobile robots\\noptimisation\\npath planning\\nrandom processes\\nsearch problems\\nautonomous vehicles\\nonline planning\\ntunercar\\nsuperoptimization toolchain\\nautonomous racing\\nvehicle parameters\\nautonomous racecar\\nsystems infrastructure\\nparallel implementation\\ncma-es\\nlap time\\nnaive random search\\nracing strategy\",\"530\":\"risk management\\nplanning\\nprediction algorithms\\nautonomous vehicles\\nrobot sensing systems\\nnavigation\\nprobabilistic logic\\ncollision avoidance\\nmobile robots\\npath planning\\nrisk analysis\\nroad safety\\nroad traffic control\\nroad vehicles\\nautonomous driving\\nsubsequent planning\\nrisk assessment algorithms\\nego vehicle\\nrisk-inducing factors\\nbidirectional reachability\\nrisk planning\",\"531\":\"games\\nautomobiles\\nmathematical model\\ngame theory\\nautonomous vehicles\\nrobot sensing systems\\ndecision making\\nlearning (artificial intelligence)\\nmobile robots\\nmonte carlo methods\\nroad vehicles\\nsensors\\nsensor data\\niterative multiplayer game\\ngame model\\nego-vehicle\\nvehicle-to-vehicle communication\\ntraffic simulator\\ngame theoretic decision making\\ncognitive hierarchy reasoning\\nmonte carlo reinforcement learning\",\"532\":\"roads\\nautonomous vehicles\\nlearning (artificial intelligence)\\ntask analysis\\nbenchmark testing\\ntrajectory\\ncollision avoidance\\npredictive control\\nroad traffic control\\ntraffic engineering computing\\ndense traffic\\nmodel-free reinforcement learning\\ncontrol methods\\nautonomous vehicle\\nobstacle-free volume\\ndeep reinforcement learning\\ncontinuous control policy\\ntarget road lane\\nmodel-predictive control-based algorithms\",\"533\":\"collision avoidance\\ntrajectory\\nrobots\\nacceleration\\nsafety\\nbicycles\\nautomobiles\\ncomputer games\\ngame theory\\ngame-theoretic autonomous car\\ncontrol barrier functions\\ntwo-player racing game\\nautonomous ego vehicle\\nopponent vehicle\\ntwo-car racing game\\ngame-theoretic control method hinges\\nsensitivity-enhanced nash equilibrium\",\"534\":\"image recognition\\nhidden markov models\\nrobots\\nsymmetric matrices\\ncameras\\nimage retrieval\\nintelligent transportation systems\\nnearest neighbour methods\\ntraffic engineering computing\\nscalable place recognition\\nenvironmental conditions\\nvisual place recognition\\nmobile robotics\\nimage information\\nimage acquisition process\\nk nearest neighbours\\ntemporal relations\\ninference time reduction\\ndatabases\\npublicly sourced data\\nsubgraph place recognition for intelligent transportation\\nsprint\",\"535\":\"three-dimensional displays\\nlaser radar\\nrobots\\nsensors\\nimage segmentation\\nneural networks\\nvisualization\\ncameras\\nfeature extraction\\nimage matching\\nimage sequences\\nlearning (artificial intelligence)\\nneural nets\\noptical radar\\npose estimation\\nrobot vision\\nlearning-based descriptors\\npoint cloud segments\\ndegree-of-freedom\\nlidar scan\\nneural network architecture\\nsegment retrieval rates\\nlidar-only description\\noneshot global localization\\nautonomous navigation tasks\\nlidar-visual pose estimation\\nlidar-only approach\\ndegree-of-freedom pose\\nnclt dataset\",\"536\":\"stability analysis\\neigenvalues and eigenfunctions\\nrecurrent neural networks\\ntraining\\nrobots\\nheuristic algorithms\\ntask analysis\\ncollision avoidance\\ncontrol engineering computing\\ngradient methods\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nneurocontrollers\\nrecurrent neural nets\\nmobile robot\\ntraditional robotic control suits\\nprofound task-specific knowledge\\nbuilding\\ntesting control software\\ndeep learning\\nend-to-end solutions\\nminimal knowledge\\nend-to-end linear dynamical systems\\nlds\\nrobotic domains\\nregularization loss component\\nlearning algorithm\\nlearned autonomous system\\nsimulated robotic experiments\\nstabilizing method\\nend-to-end robot learning scheme\\nrecurrent neural network compartment\\ngershgorin loss\",\"537\":\"task analysis\\ncouplings\\nforce\\ntrajectory\\nrobots\\neducation\\nautomation\\nexpectation-maximisation algorithm\\nintelligent robots\\nlearning (artificial intelligence)\\ntelerobotics\\nkinesthetic coupling\\nonline incremental learning approach\\ntask execution\\nteleoperation-based teaching\\npartial demonstration\\ndynamic authority distribution\\nmodified partial trajectory\\nrhythmic peg-in-hole teleoperation task\\npartial modification\\ntask operation\\nmini-batched online incremental learning\\nsupervisory teleoperation\\nexpectation-maximization algorithm\",\"538\":\"prosthetics\\nlegged locomotion\\npd control\\ntrajectory\\nreal-time systems\\nknee\\nartificial limbs\\ngait analysis\\nlearning (artificial intelligence)\\nmedical computing\\nmedical robotics\\nneurocontrollers\\nrecurrent neural nets\\nampro3 prosthetic leg\\nreal-time dynamical system\\nhuman-prosthesis system\\nprosthesis control\\nrecurrent neural network control\\nhybrid dynamical transfemoral prosthesis\\ncontrol methods\\nmodulating behaviors\\ndynamical robotic assistive devices\\nbehavioral cloning\\npowered transfemoral prostheses\\ngated recurrent unit\\ncustom hardware accelerator\\nprosthesis controller\\nrnn inference\\nnominal pd controller\\nedgedrnn accelerator\",\"539\":\"context modeling\\nrobots\\ntask analysis\\ninverse problems\\nvisualization\\npredictive models\\nfeature extraction\\nlearning (artificial intelligence)\\ngeneral imitation learning method\\nrobotic system\\ncontext translation model\\ndepth prediction model\\nmultimodal inverse dynamics model\\ndepth observation\\ninverse model maps\\nmultimodal observations\\ncross-context learning advantage\\ncross-context visual imitation learning\\ncolor observation\\nblock stacking tasks\",\"540\":\"wheelchairs\\ntraining\\nrobots\\nvehicles\\nhaptic interfaces\\nsensors\\nnavigation\\ngaussian processes\\ngeneralisation (artificial intelligence)\\nhandicapped aids\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nneural nets\\ngaussian process\\nlearning assistance by demonstration\\nhuman agent\\nmachine learning models\\ndimensionality reduction techniques\\nlearning system\\ncustom teleoperation\\nlad\\ngeneralisation capability\\nassistive power\\nlearned assistive policy\\ncustomised assistance\\nsmart wheelchairs\",\"541\":\"cost function\\ntrajectory\\nvehicles\\nroads\\nlearning (artificial intelligence)\\nsafety\\nplanning\\nbehavioural sciences computing\\ndriver information systems\\npath planning\\nroad safety\\nroad traffic\\nroad vehicles\\nhuman drivers\\ninteractive driving\\ncooperative behavior\\ndense traffic\\nautonomous vehicles\\nsafer interaction\\ncost function selection\\ncost function structures\\ninverse reinforcement learning\\nhuman driven trajectories\\nhuman driving behavior imitation\\nhuman driving behavior explanation\\ntheory of mind\\nautomated vehicles\\nimitation learning\\ncooperative motion planning\",\"542\":\"batteries\\nmagnetic sensors\\nmagnetic resonance imaging\\nmagnetization\\nmagnetic separation\\nstomach\\ncells (electric)\\ncoils\\nferromagnetic materials\\nmedical robotics\\nhall-effect sensors\\nlr44 button battery\\nmagnet-containing capsule\\nmagnetic sensor\\ntopographic localization\\ningested button battery\\nbattery lands\\ningestible magnetic robot\\nbutton battery retrieval\\nlocalization method\\ndirect magnetic field\\ntrilateration method\",\"543\":\"needles\\nrobots\\nmagnetic resonance imaging\\npain\\nforce\\nback\\nactuators\\nbiomedical mri\\nmedical image processing\\nmedical robotics\\nmobile robots\\noptical tracking\\nposition control\\ntelerobotics\\nfully actuated body-mounted robotic assistant\\nmri-guided low back pain injection\\nneedle alignment module\\nremotely actuated needle driver module\\nfully actuated robot\\nremote actuation design\\nactuation electronics\\nrobot frame\\nminimal visible image degradation\\nsize 0.53 mm to 1.45 mm\\nsize 0.7 mm\",\"544\":\"implants\\nfault tolerance\\nfault tolerant systems\\nrobot sensing systems\\nforce\\nbiological tissues\\nmedical robotics\\nredundancy\\nsurgery\\nshape-changing internal robots\\nhuman body\\nforeign body\\nin-vivo robot\\nimplantable robot\\nfault tolerant control\\nfault diagnosis\\nredundancy-based control\\nfault-tolerant capabilities\\nfault risks\\nimplantable robots\\nsurgical robots\",\"545\":\"cameras\\nrobot vision systems\\nsurgery\\nstereo image processing\\nendoscopes\\ntask analysis\\nmedical robotics\\nvisual perception\\ncamera baseline\\ndepth perception\\nsurgical robotics\\nrobot-assisted surgery\\nsurgical trocar\\nfixed baseline\\nstereoscopic pickup camera\\nda vinci surgical system\\nrobot-assisted surgical systems\\nflexible baseline design\\nadaptive baseline camera design\\nclinical stereoendoscopes\",\"546\":\"needles\\nrobots\\nwounds\\ncalibration\\nsurgery\\nimaging\\nthree-dimensional displays\\nbiological tissues\\nbiomedical optical imaging\\ncontrol engineering computing\\niterative methods\\nmedical image processing\\nmedical robotics\\nmobile robots\\noptical tomography\\npath planning\\nphantoms\\npose estimation\\nautonomous robotic microsuturing\\nprior autonomous suturing robots\\nplanned entry\\nsuture needle\\nanimal tissue\\ntissue phantoms\\nexit points\\niterative closest points\\nkeypoint identification\\nwound detection\\nrobot-needle transforms\\nrobot-oct\\nimaging feedback\\n3d optical coherence tomography system\\nrobotic suturing system\\nsoft tissue\\nsub-millimeter precision\\nhuman surgeons\\nrobotic automation\\noptical coherence tomography calibration\",\"547\":\"magnetic moments\\nmagnetic separation\\nobject tracking\\nmagnetic resonance imaging\\ninterpolation\\nactuators\\nfitting\\nbiological organs\\nbiomedical optical imaging\\nendoscopes\\nmedical image processing\\nphantoms\\nautonomous simultaneous magnetic actuation & localization\\nwireless capsule endoscopy\\ngastrointestinal examinations\\nclinical applications\\nrotating magnet\\nrobotic arm\\ninternal magnetic ring\\nmagnetic fields\\nexternal sensor array\\nspherical linear interpolation\\nactuation-localization loop\\nbezier curve gradient\\nnormal vector fitting\\nfrequency 25.0 hz\\nsimultaneous magnetic actuation and localization\\nmultiple objects tracking\",\"548\":\"legged locomotion\\nprobes\\nfoot\\nrobot sensing systems\\nforce\\naustralia\\ngait analysis\\nmobile robots\\nstability\\nprobe-before-step walking strategy\\nmultilegged robots\\nrough terrain\\nsafe footholds\\nhexapod robot\\nterrain probing approach\\nfollow-the-leader strategy\\nstabilisation\",\"549\":\"robots\\ntask analysis\\nmulti-robot systems\\nindexes\\norganizations\\nresource management\\nbiology\\ndirected graphs\\nmobile robots\\nunsupervised learning\\nmultirobot system\\nmultirobot structure\\nhuman-robot teaming\\nmultimodal graph embedding\\nrobot teams selection\\nasymmetrical relationships\\nphysical robots\\nmultifaceted internal structures\\ngraph embedding-based division methods\",\"550\":\"planning\\nhypercubes\\nsearch problems\\nlegged locomotion\\nwavelet transforms\\naerospace engineering\\nelectronic mail\\ngraph theory\\nmulti-agent systems\\npath planning\\nlocal search\\nlocal inconsistency conditions\\ncommon subgraph\\nprovably optimal path\\ninformative graph\\nsearch algorithm\\ndistributed agents\\nsingle-query shortest path\\nsearch space\\ncommon environment\\nmultiresolution graph\\nmultiagent multiscale a*\",\"551\":\"robots\\nlaplace equations\\nmulti-robot systems\\neigenvalues and eigenfunctions\\nmaintenance engineering\\ntask analysis\\ncontrol systems\\nmobile robots\\noptimal control\\noptimisation\\nconnectivity maintenance\\ncontrol barrier functions\\nmultirobot system\\nlocal connectivity\\nglobal connectivity\\nformation control\\ncontrol barrier function\\ncontrol strategy\",\"552\":\"robot sensing systems\\nrobot kinematics\\nlyapunov methods\\ntrajectory\\nshape\\ncontrol system synthesis\\ndecentralised control\\nmulti-robot systems\\nmultirobot team\\nnetwork structure\\ncontroller synthesis\\nformation control\\ncommunication requirements\\ndifferential-drive robots\\ninfinitesimally shape-similar formations\\nnetwork topology\",\"553\":\"robot kinematics\\nrobot sensing systems\\nestimation\\nnavigation\\nprobabilistic logic\\ndistributed control\\nmobile robots\\nmulti-robot systems\\nairborne chemicals\\nmobile sensing systems\\nintelligent systems\\nodor source localization\\nhomogeneous multirobot systems\\nmultiple mobile robots\\ndistributed system\\ndistributed source term estimation\",\"554\":\"navigation\\nrobot kinematics\\ncollision avoidance\\ngames\\nplanning\\nsafety\\ncomputational geometry\\ngame theory\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nselfish agent\\nrelative cell\\ncollision-free configuration\\negoistic weights\\naltruistic agents\\ndistributed semi\\nsemicooperative multiagent navigation policies\\ndynamic weights\\nlower relative weight\\nbuffered distance\\nagent weights\\nselfish behavior\\nprioritized behavior\\nweighted buffered voronoi tessellation\\nweighted buffered voronoi cells\",\"555\":\"load modeling\\nactuators\\nsoft robotics\\nkinematics\\ncalibration\\ntraining\\ntraining data\\nend effectors\\nlearning (artificial intelligence)\\ntapered arm configurations\\nsoft robotic actuators\\narm configurations\\nquasistatic model only control\\ncontroller baseline\\ncontrol reconfigurable staged soft\\nsystem load states\\nsoft arm configurations\\nstage approach\\nlstm calibration routine\\ncurrent load state\\ncontrol input generation step\\ngeneralized quasistatic model\\nlearned load model\",\"556\":\"springs\\ncoolants\\nactuators\\nmathematical model\\nforce\\nwires\\ncoils\\ncooling\\ndesign engineering\\nenergy consumption\\nindustrial robots\\nmuscle\\nshape memory effects\\nsprings (mechanical)\\nsma coil spring\\nflexible response soft actuator\\ncoolant circulation system\\nthermomechanical heat transfer model\\nmaximum actuation frequency\\nsma actuator embedded\\nstretchable coolant vascular pursuing artificial muscles\\nmuscle-like sma actuator\\nactive cooling system\\nheating phase\\nrobots structure\\nshape memory alloy\\nfrequency 0.5 hz\",\"557\":\"grasping\\nrobots\\ngrippers\\ntraining\\ntask analysis\\ngallium nitride\\nimage segmentation\\nlearning (artificial intelligence)\\nneural nets\\nobject recognition\\npose estimation\\nrobot vision\\nvisual servoing\\nsemicompliant objects\\ngrasping experiments\\ndeep learning\\ninaccurate agent gripper\\nvisual servoing grasping task\\ncyclegan\\ndrl\\ndeep reinforcement learning grasping agent\\ngenerative adversarial networks\",\"558\":\"task analysis\\nplanning\\nmanipulators\\nrobot kinematics\\ncontainers\\ntrajectory\\nmobile robots\\nmulti-robot systems\\npath planning\\ntask-agent assignment\\ncomplex tasks\\nmultiarm mobile manipulators\\nmultiple robotic agents\\nexpensive motion planning queries\\nspeed-up techniques\\nspatial constraint checking\\nconservative surrogates\\nsymbolic conditions\\nhigh-dof robotic agents\",\"559\":\"task analysis\\ntools\\nuncertainty\\nrobot sensing systems\\nrobustness\\nmotor drives\\nlearning systems\\nmanipulators\\nposition control\\nrobust control\\npeg insertion\\ninaccurate motor control\\nrobotic manipulation skills\\nshallow-depth insertion\\nwrench manipulation\\nrobot positions\\nlearning loops\\nlearning system\\nscaffold manipulation skill learning\\nrobot actions\\nrobust manipulation\",\"560\":\"planning\\ntask analysis\\nbayes methods\\nmanipulators\\naerospace electronics\\nuncertainty\\nmobile robots\\nmotion control\\npath planning\\nexecution system\\ndeterministic cost-sensitive planning\\nhybrid belief states\\npartially observable problems\\nonline replanning\\nbelief space\\nmultistep manipulation tasks\\nautonomous robot\",\"561\":\"manifolds\\nkalman filters\\ntwo dimensional displays\\ndispersion\\nrobots\\ncovariance matrices\\nlie groups\\nnonlinear filters\\npublic domain software\\nstate estimation\\nukf-m\\nstate estimation problems\\nunscented kalman filtering-manifolds\\nfiltering performance\\nindependent open-source\\npython frameworks\\nmatlab frameworks\\nonline repositories\\nhttps:\\/\\/github.com\\/caor-mines-paristech\\/ukfm\",\"562\":\"robot sensing systems\\ndelays\\ncurrent measurement\\nsensor fusion\\nestimation\\nlinear systems\\nmobile robots\\nnonlinear systems\\nstate estimation\\nprecise sensor fusion\\nout-of-sequence measurements\\nmobile robotics\\nprecise state estimation\\nsensor fusion algorithm\\nsensor fusion approaches\\nsuitable approaches\\nconsumer robot use-case\\nfusion process\\nestimation bias\\nestimation performance\",\"563\":\"uncertainty\\ndegradation\\ntraining\\nnoise measurement\\nrobot sensing systems\\nentropy\\nlearning (artificial intelligence)\\nneural nets\\nprobability\\nsensor fusion\\nuncertainty handling\\nmultimodal fusion\\nunanticipated input degradation\\nmultiple sensor modalities\\ndeep learning architectures\\nmodality-specific output softmax probabilities\\nuncertainty measures\\nuncertainty-scaled output\\nfusion architectures\\nprobabilistic noisy-fusion\\ndata-dependent spatial temperature scaling\\nuncertainty-aware fusion\\nuno\\nuncertainty-aware noisy fusion\",\"564\":\"global positioning system\\nrobot sensing systems\\ncalibration\\ncameras\\ncloning\\nrobustness\\nvisualization\\ndistance measurement\\nmonte carlo methods\\nsensor fusion\\ngps reference frame\\ngps signal\\nintermittent gps-aided vio\\nonline initialization\\nimu-camera data fusion\\nintermittent gps measurements\\nspatiotemporal sensor calibration\\nsensor reference frames\\nonline calibration method\\nreference frame initialization procedure\\ngps sensor noise\\ngps-vio system\\nvio reference frame\\nonline calibration\\nrobust gps-aided visual inertial odometry\\ngps-imu extrinsics\\nmonte-carlo simulations\\nlarge-scale real-world experiment\",\"565\":\"earth\\nmathematical model\\nforce\\nrobot kinematics\\nsensors\\nuncertainty\\ninertial navigation\\nmathematical analysis\\nmeasurement errors\\nmeasurement uncertainty\\nposition measurement\\nsensor fusion\\nunits (measurement)\\ninertial navigation equations\\nlog-linearity property\\nmathematical framework\\nimu error propagation\\ninertial measurement units\\nlie group se\\nrotating earth\\ncentrifugal force\\ncoriolis effect\",\"566\":\"robot sensing systems\\nradar measurements\\ndoppler radar\\ndoppler effect\\nestimation\\nvelocity measurement\\ninertial navigation\\nmillimetre wave radar\\nmobile robots\\noptimisation\\nradar imaging\\nradar receivers\\nvisual perception\\nmillimeter-wave radar-on-a-chip sensor\\ninertial measurement unit\\nbatch optimization\\nsliding window\\nradar-inertial ego-velocity estimation\\nvisually degraded environments\\nmobile robot body-frame velocity\\nradar-inertial velocity estimates\\nvisual-inertial approach\",\"567\":\"manipulators\\nforce\\ncleaning\\nbuildings\\nforce measurement\\nsea measurements\\nimpedance\\nactuators\\nball screws\\nbuildings (structures)\\ncompliance control\\nforce control\\nforce sensors\\nmobile robots\\nposition control\\nrobust control\\nservice robots\\nwalls\\nwet-type cleaning manipulator\\nsystem stability\\ncontrol performance\\nrobust force measurement mechanism\\nposition-based impedance control\\nfacade cleaning operation\\nseries elastic actuator\\nforce sensor\\ncontact force\\ncleaning performance\\nball screw mechanism\\n2-dof compliant manipulator\\nmanipulator stiffness\\nforce tracking\",\"568\":\"task analysis\\nuncertainty\\ngeometry\\nrobot sensing systems\\nrobustness\\nstate estimation\\nposition control\\nrobots\\nrobust control\\ntorque control\\nimpedance-controlled robots\\nrobust peg-in-hole\\nlocally guided peg-in-hole\\nautonomous execution\\npeg-in-hole problem\\nrobotic setup\\nrobust behavior\\nmesh data\\ntask-specific weight\\nmotion generation step\\njoint torque measurements\\nsampling-based state estimation framework\\npeg-in-hole assembly tasks\",\"569\":\"force control\\nfriction\\nindustrial manipulators\\nmechanical contact\\nrobotic assembly\\npolyhedral parts\\nfrictional contact\\nfrictionless contact\\nnonlinear equations\\nerror reduction function\\nfrictional cases\\nmisalignment-reducing conditions\\ncontact configurations\\nforce-guided assembly\",\"570\":\"actuators\\nelectrodes\\ndielectric liquids\\nforce\\ninsulators\\ndielectrics\\npermittivity\\nelectric actuators\\nmicroactuators\\nmicrosensors\\nsolenoids\\ncontraction variation\\nelectro-ribbon actuator\\nhigh-contraction electro-ribbon actuators\\nfluidically-driven technologies\\nelectrically-driven actuators\\nhigh power equivalent\\nmammalian muscle\\ndielectrophoretic liquid zipping\\nlow-power-consumption solenoids\\nvalves\",\"571\":\"actuators\\nstrain\\nforce\\nmathematical model\\nmuscles\\nrobots\\nyarn\\nbiomechanics\\ncoils\\nhysteresis\\nmagnetic hysteresis\\nmanipulator dynamics\\nmedical robotics\\nmuscle\\npneumatic actuators\\nrobotic muscles\\nforce generation\\nforce performance\\nforce production\\nhw-scp actuators\\npreisach hysteresis model\\npolynomial model\\nhigh-performance artificial muscles\\nhelically wrapped supercoiled polymer artificial muscles\\nstrain performance\",\"572\":\"robots\\njamming\\nforce\\ntendons\\nelectron tubes\\nwires\\nvalves\\nbellows\\nbending\\ncompressive strength\\ncontinuum mechanics\\ndesign engineering\\nmanipulator dynamics\\nmobile robots\\nrigidity\\nstructural design\\ncontrol systems\\npre-charged air\\nparticle jamming\\norigami structure\\ninternal chambers\\nspine-like chamber\\nidentical chambers\\nspine chamber\\npressurized air\\nair chambers\\nrobot expansion-contraction\\nstiffness variation mechanism\\nlateral stiffness\\naxial stiffness\\nprototype robot\\nvariable stiffness soft continuum robot\\nsoft continuum robot\\nvariable stiffness\\norigami\",\"573\":\"rails\\nwheels\\nrobot sensing systems\\nmanipulators\\nlayout\\nmobile robots\\nmulti-robot systems\\nrobotic manipulator arm overhead\\ncontinuous overhead manipulation\\nrail crossings\\nrobot swarm\\nmobile swarmrail units\\nsingle rail network\\nindoor transport\\nmobile manipulation\\nomnidirectional mobile platform\\nrail profiles\\nrail-structure\\noverhead robot system\",\"574\":\"trajectory\\nrobots\\nplanning\\naerospace electronics\\nmicrosoft windows\\nthree-dimensional displays\\nreal-time systems\\ncollision avoidance\\ngraph theory\\nmobile robots\\nmotion control\\nremotely operated vehicles\\nslam (robots)\\ntrajectory control\\noff-road terrain\\non-line mapping\\nplanning solution\\nobstacle detection\\nterrain gradient map\\nsimple cost map\\nadaptable cost map\\noptimal paths\\ncontrol input space\\nkinematic forward simulation\\ngenerated feasible trajectories\\noptimal trajectory\\ntime operation\\nfrequency 10.0 hz\\nfrequency 30.0 hz\",\"575\":\"task analysis\\nmathematical model\\nnavigation\\nautonomous robots\\nhardware\\npredictive models\\nmobile robots\\nmulti-robot systems\\ntelerobotics\\nutility function\\nreal-world mobile robot navigation\\nrobot fleets control\\nhuman operator\\nteleoperation\",\"576\":\"task analysis\\nrobot motion\\nrobot kinematics\\nbiological neural networks\\ntrajectory\\ntraining\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrecurrent neural nets\\nactor-critic approach\\nlegible robot motion planner\\nhuman-robot collaboration\\nhuman partners\\nmutual learning\\nlegibility evaluator\\npolicy network\\ndeep reinforcement learning\\nsequence model\\nseq2seq\\nmotion predictor\\nmaps motion\\nlegible reward\\nhuman-subject experiments\\nreal-human data\\nrecurrent neural networks based sequence\",\"577\":\"three-dimensional displays\\ndrones\\nrobot kinematics\\nshape\\nrobot sensing systems\\naerospace electronics\\naerospace computing\\naircraft control\\ncontrol engineering computing\\nhelicopters\\ninteractive devices\\nposition control\\nvirtual reality\\npointing gestures\\nquadrotor\\npointing ray\\npush button\\njoystick control\\nintuitive 3d control\\nuser proximity\\n3d piloting task\\nvirtual workspace surfaces\\nhuman robot interaction\",\"578\":\"robots\\ncognition\\nvisualization\\ngraphical models\\npsychology\\nnoise measurement\\nobject tracking\\nbelief networks\\ncognitive systems\\ngraph theory\\ninference mechanisms\\nlearning (artificial intelligence)\\nrobot knowledge\\nhuman interactions\\ngraphical model\\nobject states\\nparse graph\\nsingle-view spatiotemporal parsing\\nlearned representation\\ninference algorithm\\njoint pg\\neffective reasoning\\ninference capability\\nstates joint inference\\nhuman beliefs\\nsocio-cognitive ability\\nfalse-beliefs\\nindividual parse graph\\nsmall object tracking dataset\",\"579\":\"training\\nlearning systems\\ndeep learning\\npediatrics\\nconferences\\nhumanoid robots\\nrobot sensing systems\\ncognitive systems\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nneural nets\\nrobot vision\\naudiovisual cognitive architecture\\nautonomous learning\\nface localisation\\nhumanoid robot\\ndeep learning algorithms\\ncognitive framework\\naudiovisual attention\\nlearning generalization process\\nmachine learning\\nhri\",\"580\":\"feature extraction\\nextraterrestrial measurements\\nrobot kinematics\\nadaptation models\\nproductivity\\nmars\\naerospace robotics\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nplanetary rovers\\nplanetary rover exploration combining remote\\nactive spectroscopic mapping\\nplanetary rover missions\\nheavy reliance\\nground control\\nreal-time information\\nautonomous mapping\\nexploration approach\\nmachine learning model\\nrover measurements\\nspectroscopic data\\ninformation theory\\nnonmyopic path\\nexploration productivity\\nactual rover\",\"581\":\"robots\\nforce\\nmagnetic levitation\\ncomputer interfaces\\nshape\\nmagnetoelasticity\\ntask analysis\\naerospace robotics\\ngaussian processes\\nlearning (artificial intelligence)\\nmobile robots\\nposition control\\nregression analysis\\ncontrol capability\\nslider guide\\nrobot position error\\nmagnetic docking mechanism\\nfree-flying space robots\\nspherical surfaces\\nautonomous operation\\ninternational space station\\nfixed guide mechanism\\nmachine learning technique\\ngaussian process regression\\ngpr technique\",\"582\":\"wheels\\nrobot sensing systems\\nrocks\\ninstruments\\nmeasurement\\nsurface impedance\\ndc motors\\naerospace computing\\naerospace control\\naerospace robotics\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nplanetary rovers\\nplanetary surfaces\\nbarefoot rover\\nmachine learning\\n2d pressure grid\\nelectrochemical impedance spectroscopy sensor\\nin-situ sensing\\nsensor-embedded rover wheel\\nin-situ engineering\\nplanetary exploration missions\",\"583\":\"space vehicles\\nquaternions\\npose estimation\\nearth\\ncameras\\nthree-dimensional displays\\naerospace computing\\nimage classification\\nlearning (artificial intelligence)\\nmixture models\\nneural nets\\nrendering (computer graphics)\\nphotorealistic rendering\\non-orbit proximity operations\\n6d pose estimation\\nmonocular pose estimation\\nunreal engine 4\\nneural networks\\ndeep learning framework\\norientation soft classification\\norientation ambiguity\\nmixture model\\nurso\\nspacecraft pose estimation\",\"584\":\"task analysis\\nparameter estimation\\naerospace electronics\\nmanipulator dynamics\\nadaptive control\\naerospace control\\naerospace robotics\\njacobian matrices\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nconcurrent parameter identification\\nfree-floating robotic system\\non-orbit servicing\\nuncertain parameters\\nfast parameter identification method\\naccurate parameter estimates\\nsystem dynamic properties\\ncontrol scheme compensates\\nrobotic servicer base\\nparameter information\\ntransposed jacobian controller\\nrw angular momentum disturbance rejection\\noos tasks\",\"585\":\"quaternions\\nmathematical model\\nparameter estimation\\nrobots\\nlinear matrix inequalities\\nestimation\\nprogramming\\naerospace robotics\\nattitude control\\ncontrol system synthesis\\nmanipulator dynamics\\nmobile robots\\nmotion control\\nrecursive estimation\\nrobust control\\ndual quaternion-based discrete variational approach\\nonline inertial parameter estimation\\nfree-flying robots\\nmodel-based motion control\\nrigid body inertial parameter estimation\\ndiscrete dual quaternion equations\\nvariational mechanics\\nlinear parameter estimation problem\\nstandard localization algorithms\\nrotational inertia\\nlinear matrix inequality constraints\\npseudoinertia matrix\\nrecursive semidefinite programming\",\"586\":\"cameras\\ncalibration\\nlaser radar\\nthree-dimensional displays\\nimage edge detection\\ndetectors\\noptical radar\\nlidar calibration\\nintrinsic parameters\\nprobabilistic sense\\nprobabilistic formulation\\ncamera model\\nadditional lidar measurements\\nintrinsic camera calibration\\nstate-of-the-art calibration precision\\nextrinsic parameters\",\"587\":\"cameras\\ncalibration\\nrobot vision systems\\nsimultaneous localization and mapping\\nvehicle dynamics\\noptimization\\nmeasurement uncertainty\\nsensitivity analysis\\nslam (robots)\\ncalibration parameters\\ncalibration sensitivity analysis\\njoint angle noise\\njoint angle values\\ncalibration code\\ndynamic camera clusters\\nvisual slam\\ntime-varying set\\nextrinsic calibration transformations\\ndcc calibration accuracy\\nconfiguration space\\npixel re-projection error\\nfiducial target\\ndynamic camera cluster\\npose-loop error optimization\",\"588\":\"three-dimensional displays\\ncalibration\\ncameras\\nlaser radar\\nlinear programming\\nsolid modeling\\ntwo dimensional displays\\nfeature extraction\\noptical radar\\nstereo image processing\\nplanar feature correspondences\\nplane parameter covariance\\n3d corner points\\nplane measurement covariance\\nout-of-plane errors\\nanalytic plane covariances construction\\nplane parameter errors\\nplanarity-based extrinsic calibration\\nlidar\",\"589\":\"wrist\\ncollision avoidance\\nkinematics\\nredundancy\\njacobian matrices\\nservice robots\\nend effectors\\nindustrial robots\\nmotion control\\nredundant manipulators\\nend effector path tracking\\n6-dof robot\\n8-dof robot\\nredundant robot\\nkinematically redundant manipulation\\nindustrial arm-type robots\\ndexterity\\nroll-pitch-roll wrist configuration\\nsingularity free motion\\nend effector wrist module\\nwrist module\\nroll-pitch-yaw\\nwrist singularity\\ninverse kinematics\",\"590\":\"agricultural machinery\\nvehicle dynamics\\nplanning\\ntrajectory\\ndynamics\\nwheels\\nlead\\nnonlinear dynamical systems\\npath planning\\ntrajectory control\\nvehicle dynamics model\\nonline trajectory planning\\ncar-like tractor\\npassive full trailers\\nmotion planning\\ncomplex nonlinear dynamics\\nsimulation based prediction\\nindustrial tractor-trailers vehicle\\nobstacle free trajectories\",\"591\":\"robots\\ntransportation\\nbiology\\ncollision avoidance\\nscalability\\ntask analysis\\nexplosions\\nmobile robots\\nmulti-robot systems\\nswarm intelligence\\ninterrobot collisions\\nswarm robot foraging\\nscale-invariant swarm foraging algorithm\\nhierarchical branching transportation network\\nubiquitous fractal branching networks\\nbioinspired transportation network\",\"592\":\"legged locomotion\\ncerebellum\\nfoot\\nstability analysis\\nrobot kinematics\\ncompliant mechanisms\\nfeedback\\nmotion control\\npredictive control\\nrobot dynamics\\nrobust control\\nreflex based dynamic locomotion\\nstance control\\nlegged robotics\\ncentral pattern generators\\ncyclic motion\\nrobotic locomotion\\nreflex feedback\\nmusculoskeletal simulation models\\ncompliant quadruped robots\\ngravity compensation mechanism\\nhyq robot\\nstability module\\nrobust locomotion\",\"593\":\"neurons\\nrobots\\nsociology\\nstatistics\\nlight emitting diodes\\nneuromorphics\\nsynapses\\nerror correction\\nmobile robots\\nneural chips\\nneural nets\\npath planning\\npose estimation\\nslam (robots)\\nsnn mechanism\\nneuromorphic device\\nform-factor neuromorphic chip\\nspiking neural network\\nmap formation\\nneuromorphic hardware\\nneural networks\\nrobot control\\nerror estimation\\nsimultaneous localization and mapping\\nrobot pose estimation\\nsnn-based slam\\npath integration speed\",\"594\":\"strain\\nvisualization\\nforce\\nshock absorbers\\nmagnetic gears\\nend effectors\\nfeedback\\ngears\\nimpact (mechanical)\\nplastic deformation\\nposition control\\nrobot dynamics\\nvelocity control\\nadaptive visual shock absorber\\nvisual-based maxwell model\\nfeedback control\\nhigh-speed visual object tracking\\nobject noncontact state\\nobject contact\\nmagnetic gear response\\nplastic deformation control\",\"595\":\"kinematics\\nvehicle dynamics\\nbackstepping\\ntracking\\ndynamics\\ntires\\nconvergence\\ncompensation\\ncontrol nonlinearities\\nfeedback\\nfeedforward\\nmobile robots\\nmotion control\\nnonlinear control systems\\nobservers\\npath planning\\nposition control\\nrobot dynamics\\nrobot kinematics\\nrobust control\\nstability\\nsteering systems\\nvariable structure systems\\nkinematic controller\\nfeedforward slip compensation\\nvariable structure controller\\ngraceful motion\\nyaw rate commands\\nbackstepping dynamic controller\\nrobust steering commands\\noutput feedback control\\nautonomous ground vehicles\\nvehicle steering control\\ngraceful lateral motion\\ncouples yaw-rate based path\\nsteering angle\\nheading error\\nslip-based nonlinear recursive backstepping path following controller\\nobserver based sideslip estimates\\npath following accuracy\\nerror convergence\\nslip-based kinematic model\\ndynamic model\\npath following error\\nrobustness\\nhigh gain observer\\nstability analysis\",\"596\":\"robots\\ntrajectory\\nsafety\\nmeasurement\\nnavigation\\ncollision avoidance\\naerospace electronics\\ncontrol system synthesis\\nlyapunov methods\\npath planning\\nstability\\nsafe path-following control\\nfast navigation\\nsafe autonomous navigation\\ncontrol policy design\\nellipsoidal trajectory\\nellipsoidal bounds\\ncontrol design\\nlocal environment geometry\\nmedial obstacles\\nvirtual reference governor system\\nsystem safety\\nlyapunov-function-based designs\\nstate-dependent directional metric\\nquadratic state-dependent distance metric\\neuclidean-norm design\",\"597\":\"actuators\\ntorque\\nsprings\\nbrakes\\ndc motors\\nsea measurements\\nhysteresis motors\\nactive disturbance rejection control\\nclutches\\ncompensation\\nelasticity\\nforce control\\ngears\\nmanipulator dynamics\\nnonlinear control systems\\nposition measurement\\npower transmission (mechanical)\\nthree-term control\\nhybrid motor-brake-clutch series elastic actuator\\npositional measurement error\\nbacklash-compensated active disturbance rejection control\\nnonlinear multiinput series elastic actuators\\npassive compliance\\nforce-controlled robotic manipulators\\nelastic element\\ndedicated torque sensors\\ndeflection control\\nnonlinear deformation\\ntorque requirements\\nmechanical backlash\\nmultiinput active disturbance rejection controller\\nerror-based controllers\\nbacklash-compensated adrc\",\"598\":\"symmetric matrices\\ngenerators\\nlinear matrix inequalities\\nrobustness\\nclosed loop systems\\nlyapunov methods\\ncontrol system synthesis\\nfeedback\\nhelicopters\\nlinearisation techniques\\nnonlinear control systems\\ngeneralized homogenization\\nlinear quadrotor controller\\ngeneralized homogeneity\\nimplicit homogeneous feedback design\\ntuning rules\\nquadrotor qdrone\",\"599\":\"pose estimation\\nrobot sensing systems\\nphysics\\nheuristic algorithms\\ncost function\\ncoprocessors\\ngraphics processing units\\nmanipulators\\nobject tracking\\noptimisation\\nparticle filtering (numerical methods)\\nrobot vision\\ncomplex contact dynamics\\ngpu-accelerated parallel robot simulations\\nsample-based optimizers\\ncontact feedback\\nrobot-object interactions\\ngpu-accelerated robotic simulation\\nrobot hand\\nvision-based methods\\nparticle filters\\nstatic grasp setting\\nin-hand object pose tracking\\nmanipulation\\nphysics simulation\\nforward model\\npoint cloud distance error\",\"600\":\"three-dimensional displays\\npose estimation\\nrobot sensing systems\\nrobustness\\ncomputational modeling\\nsolid modeling\\nimage registration\\nocclusion-aware pose estimation\\nadaptive hands\\nmanipulation tasks\\nwithin-hand manipulation\\nrobot hand\\ndepth-based framework\\nrobust pose estimation\\nadaptive hand\\nefficient parallel search\\npoint cloud\\nrobust global registration\\nobject types\\nobject pose hypotheses\\nshort response times\\nin-hand 6d object pose estimation\",\"601\":\"feature extraction\\npose estimation\\nthree-dimensional displays\\nrobustness\\nuncertainty\\nimage segmentation\\nrobots\\nimage colour analysis\\nlearning (artificial intelligence)\\nmanipulators\\nobject detection\\noptimisation\\nregression analysis\\nrobot vision\\nvideo signal processing\\nrgb-d features\\nrobotic manipulation\\nlocal optimization approach\\ndistance between closest point pairs\\nrotation ambiguity\\nsymmetric objects\\nrotation regression\\nlocal-optimum problem\\nobject location\\npoint-wise vectors\\nrobust 6d object pose estimation\\ndiscrete-continuous formulation\\nlinemod\\nycb-video\",\"602\":\"grasping\\ntask analysis\\nclutter\\nimage segmentation\\nrobustness\\nservice robots\\ncollision avoidance\\ngrippers\\nlearning systems\\nmanipulators\\nneurocontrollers\\npolicy learning\\nsplit deep q-learning\\nrobust object singulation\\nrobotic manipulation\\nrobotic applications\\ngrasping techniques\\npushing policy\\nlateral pushing movements\\nreinforcement learning\\noptimal push policies\\nsplit dqn\\ntarget object extraction\",\"603\":\"clutter\\nthree-dimensional displays\\ngrasping\\ngrippers\\nrobots\\ncollision avoidance\\ngeometry\\nmanipulators\\nobject detection\\nrobot vision\\n6dof grasping\\ncluttered environments\\nmanipulator\\nrobotic platform\\ntarget-driven object manipulation\\npoint cloud observations\\ncollision checking module\\ngrasp sequence\",\"604\":\"three-dimensional displays\\nsolid modeling\\ndata models\\ntask analysis\\npose estimation\\nimage segmentation\\ntwo dimensional displays\\nconvolutional neural nets\\nobject detection\\nregression analysis\\nstereo image processing\\nrigid objects\\ndepth images\\nconvolutional neural network\\n3d input data\\nvolume elements\\noptimized end-to-end\\nmultiple objects\\nsingle shot 6d object pose estimation\\nsingle shot approach\\nobject pose network\\nregression task\\ngpu\\nsynthetic data\\npublic benchmark datasets\",\"605\":\"laser radar\\nradar imaging\\nthree-dimensional displays\\nurban areas\\nsimultaneous localization and mapping\\ngeophysical image processing\\ngeophysical techniques\\nimage recognition\\nmobile robots\\nobject recognition\\noptical radar\\nrobot vision\\nmultimodal range dataset\\nradio detection and ranging\\nlight detection and ranging\\nurban environment\\nrange sensor-based place recognition\\n6d baseline trajectories\\nplace recognition ground truth\\nimage-format data\\ntime-stamped 1d intensity arrays\\npolar images\\nimage data\\nradar place recognition method\\nlidar\\nlonger-range measurements\\nurban place recognition\\nmulran\",\"606\":\"cameras\\nsimultaneous localization and mapping\\noptimization\\nmatrix decomposition\\ntransmission line matrix methods\\nestimation\\nthree-dimensional displays\\noptimisation\\npose estimation\\nrobot vision\\nslam (robots)\\ngpo\\nglobal plane optimization\\nhomography estimation\\nhomography decomposition\\nmonocular slam initialization\\nmonocular simultaneous localization and mapping problem\\ncamera poses\\nchessboard dataset\",\"607\":\"three-dimensional displays\\nlaser radar\\nimage reconstruction\\ngraphics processing units\\nsensor fusion\\nweight measurement\\ncameras\\nimage colour analysis\\nimage representation\\noptical radar\\nlarge-scale 3d scene reconstruction\\nautonomous driving\\nvolumetric depth fusion\\nindoor applications\\ncommodity rgb-d cameras\\nhigh reconstruction quality\\nlidar sensors\\nautonomous cars\\nlarge-scale mapping\\nurban area\\nmeshed representation\\nreal world application\\nlarge-scale volumetric scene reconstruction\\ndistance 3.7 km\",\"608\":\"topology\\nnetwork topology\\nsimultaneous localization and mapping\\nneural networks\\noptimization\\nconvolutional neural nets\\ngraph theory\\nimage representation\\noptimisation\\nslam (robots)\\nmanhattan properties\\ntopological graph\\nunoptimized pose graph\\ntopological manhattan relations\\nground-truth pose graph\\nreal-world indoor warehouse scenes\\nmanhattan-like repetitive environments\\ntopological mapping framework\\nneighbouring nodes\\nindoor warehouse setting\\nwarehouse topological construct\\ndeep convolutional network\\nsiamese-style neural network\\nbackend pose graph optimization framework\\nmanhattan graph aided loop closure relations\",\"609\":\"cameras\\nrobustness\\noptimization\\ntracking\\niterative closest point algorithm\\nrobot vision systems\\nthree-dimensional displays\\nimage colour analysis\\nimage motion analysis\\nimage reconstruction\\nimage sequences\\ninteger programming\\ninterpolation\\niterative methods\\nmotion estimation\\nslam (robots)\\noptimal key-frame selection\\nvo method\\ncamera motion\\nelastic-fusion\\ndiscontinuous camera motions\\nrobust rgb-d camera tracking\\nadaptive visual odometry\\ntum benchmark sequences\\ncamera trajectory errors\\niterative closed point\",\"610\":\"trajectory\\noptimal control\\nstability analysis\\nneural networks\\ndelays\\ntraining\\ndrones\\naircraft control\\nautonomous aerial vehicles\\ncontrol engineering computing\\nhelicopters\\nmobile robots\\nneural nets\\noptimisation\\ntime optimal control\\npower optimality\\ntime optimality\\ndeep neural network\\nrobotic applications\\noptimality principles\\ndeep network representations\\naggressive online control\\ntime-optimal maneuvers\\noffline optimal control method\\naggressive quadrotor control\",\"611\":\"trajectory\\nplanning\\naerospace electronics\\nrobots\\ncollision avoidance\\ncost function\\nspace exploration\\nboundary-value problems\\nmotion control\\noptimal control\\npiecewise constant techniques\\nrobot dynamics\\ntrees (mathematics)\\nasymptotically-optimal kinodynamic planning\\nstate-cost space\\nao-rrt\\ntree-based planner\\nmotion planning\\nkinodynamic constraints\\noptimality proof\\npiecewise-constant control function\\ntwo-point boundary-value\\nlipschitz-continuity\",\"612\":\"robots\\nvehicle dynamics\\nrobustness\\nconvergence\\nlevel set\\nmathematical model\\nforce\\nautonomous aerial vehicles\\ncontrol system synthesis\\nhelicopters\\nmobile robots\\nmulti-robot systems\\nnonlinear control systems\\npath planning\\nposition control\\nrobust control\\ntime-varying systems\\nrobust quadcopter control\\nartificial vector fields\\npath tracking control strategy\\ncontrol laws\\nvector field\\ncontrolled second order integrator\\nquadcopter model\\ninput-to-state stable\\ncontrol inputs\",\"613\":\"training\\nvisualization\\nlearning (artificial intelligence)\\nsemantics\\nrobots\\nimage segmentation\\npredictive models\\nroad vehicles\\ntraffic engineering computing\\nsimulation-based reinforcement learning\\nreal-world autonomous driving\\ndriving system\\nreal-world vehicle\\ndriving policy\\nrgb images\\nsingle camera\\nsemantic segmentation\\nsynthetic data\\nreal-world data\\nsegmentation network\\nreal-world experiments\\nsim-to-real policy transfer\\nreal-world performance\",\"614\":\"planning\\nneural networks\\nentropy\\nkinematics\\nmachine learning\\ntuning\\ntraining\\nlearning (artificial intelligence)\\npath planning\\npredictive control\\nroad traffic control\\nsituational reward adaptation\\ngeneral-purpose planning algorithms\\nautomated driving\\nplanning algorithm\\ndriving kinematics\\nlinear reward function\\ndriving situation\\ndeep learning approach\\nsituation-dependent reward functions\\nsampled driving policies\\ndriving style\\nplanning cycle\",\"615\":\"predictive models\\ntrajectory\\nlegged locomotion\\nvehicle dynamics\\nvirtual environments\\nplanning\\ndynamics\\nbehavioural sciences computing\\nhuman-robot interaction\\nnavigation\\npath planning\\npedestrians\\nroad traffic control\\nroad vehicles\\ntraffic engineering computing\\nvirtual reality\\nautomated vehicle interactions\\nsafe navigation\\nautomated vehicles\\npedestrian interactions\\nhuman-driven vehicles\\nhdv\\nhybrid systems model\\nconstant velocity dynamics\\nlong-term pedestrian trajectory prediction\\nimmersive virtual environment\\nav interactions\\npedestrian crosswalk behavior analysis\\npedestrian crosswalk behavior prediction\\ngap acceptance behavior\\nav motion planning\\nive\",\"616\":\"robot sensing systems\\nlaser radar\\nthree-dimensional displays\\nazimuth\\ncalibration\\ncw radar\\ndistance measurement\\nfm radar\\nglobal positioning system\\nmillimetre wave radar\\noptical radar\\nroad vehicle radar\\noxford radar robotcar dataset\\nradar extension\\nmillimetre-wave fmcw scanning radar data\\ncentral oxford route\\ntruth optimised radar odometry\\nautonomous vehicles\\nenvironmental conditions\\nsensor modalities\\nad 2019 01\\nurban driving\\nweather condition\\ntraffic condition\\nlighting condition\\nnavtech cts350-x radar\\nvelodyne hdl-32e 3d lidar\\ngps-ins receiver\\nori.ox.ac.uk\\/datasets\\/radar-robotear-dataset\\nsize 280.0 km\\nmemory size 4.7 tbyte\",\"617\":\"laser radar\\nfeature extraction\\ncameras\\ntraining\\nautonomous vehicles\\nrobot sensing systems\\ncomputational complexity\\ncontrol engineering computing\\nexpert systems\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\nroad vehicles\\nsensory data\\nautonomous driving\\nmultistage training procedure\\nend-to-end learning\\nmultimodal experts network architecture\\ninference time step\\nmixed discrete-continuous policy\",\"618\":\"feature extraction\\ntransforms\\ncameras\\nsemantics\\ntwo dimensional displays\\ndata mining\\nthree-dimensional displays\\ncomputer vision\\nconvolutional neural nets\\nkalman filters\\nlearning (artificial intelligence)\\nnonlinear filters\\nobject detection\\nrobot vision\\ncommon environmental landmarks\\npoint features\\nhigher level information\\ncommon vision based approaches\\nlow level hand\\nekf framework\\npractical cnn\\ntypical suburban streets\\nlocaliser\\npmd\\ncnn based perception\\nurban streets\\nlocalisation scheme\\ncomplementary approaches\\noutdoor vision based localisation\\nconvolutional neural networks\\nnecessary perceptual information\\ncamera images\\nlane markings\\nmanhole covers\\nvector distance\\nbinary image\\nground surface boundaries\\ncnn based detection\\nnovel extended kalman filter\",\"619\":\"atmospheric measurements\\nparticle measurements\\nproposals\\npredictive models\\nfuses\\nlearning systems\\nmonte carlo methods\\nconvolutional neural nets\\nimportance sampling\\nlearning (artificial intelligence)\\nmobile robots\\nneurocontrollers\\nparticle filtering (numerical methods)\\npath planning\\nmotion model\\nconvolutional neural network\\ncnn\\nmonte carlo localization\\nparticle filter\\nhybrid localization method\\nlearning-based method\\nmodel-based method\\ne2e localization\\nmcl\\ncnn predictions\\nposterior distributions\",\"620\":\"cameras\\nimage registration\\nthree-dimensional displays\\nvisualization\\nearth\\nrobustness\\ngoogle\\nautonomous aerial vehicles\\ndistance measurement\\nglobal positioning system\\nimage filtering\\nimage sensors\\nmobile robots\\nmulti-robot systems\\npose estimation\\nrendering (computer graphics)\\nrobot vision\\ngoogle earth images\\ngeoreferenced rendered images\\ndense mutual information technique\\noutdoor gps-denied environments\\nimage registrations\\ngimballed visual odometry pipeline\\nvisual localization\\nrobust global pose estimation\\nmultirotor uav\\ntypical feature-based localizer\",\"621\":\"task analysis\\ntraining\\nrobots\\ntrajectory\\nlearning (artificial intelligence)\\nadaptation models\\nstacking\\ncomputer vision\\ncontrol engineering computing\\nshaped reward functions\\nacgd\\npolicy transfer\\nreal-world manipulation tasks\\nsim-to-real visuomotor control\\nreinforcement learning\\nadaptive curriculum generation\\nvision-based control policies\",\"622\":\"clutter\\nrobots\\ntraining data\\ntask analysis\\ntraining\\nencoding\\nvisualization\\ncomputer vision\\nlearning (artificial intelligence)\\nmanipulators\\ndata augmentation procedure\\nnetwork architectures\\nimplicit attention asor-ia\\nexplicit attention asor-ea\\nuncluttered environment\\ncluttered environments\\nend-to-end training\\nattentive deep visuomotor policies\\nend-to-end train multitask deep visuomotor policies\\nrobotic manipulation\\nreinforcement learning\\nend-to-end lfd architectures\\naccept synthetic objects as real\",\"623\":\"robot sensing systems\\ntask analysis\\ntrajectory\\nrobot kinematics\\ndatabases\\nstatistical learning\\nhumanoid robots\\nmobile robots\\nposition control\\nprincipal component analysis\\nrobotic assembly\\nlfd framework\\nexception strategies\\npeg-in-hole task\\nfranka-emika panda robot\\nassembly tasks\\nassembly policy\",\"624\":\"visualization\\ncomputational modeling\\nreal-time systems\\ntraining\\nrobots\\nfaces\\nthree-dimensional displays\\ncontrol engineering computing\\nforce feedback\\nhaptic interfaces\\nmanipulators\\nmedical computing\\nmedical robotics\\nsurgery\\ntelerobotics\\nvirtual reality\\nreal-time simulation\\ninteractive manipulation\\nhuman-readable front-end interface\\ncommercially available haptic devices\\ngame controllers\\nda vinci research kit\\nreal-time haptic feedback\\nmultiuser training\\nmanipulation problems\\nsoft-body manipulation\\nopen-source framework\\ninteractive soft-body simulations\\nreal-time training\\nmaster telemanipulators\",\"625\":\"magnetic resonance imaging\\nrobots\\nmagnetic devices\\ncoils\\nthree-dimensional displays\\nforce\\nbiomedical mri\\nmedical image processing\\nmedical robotics\\nmicrorobots\\npath planning\\nsurgery\\nuntethered magnetic millirobot\\nmri gradient coils\\nelectromagnetic field gradients\\nmagnetic resonance imaging devices\\npower untethered magnetic robots\\nmri devices\\nmagnetic pulling forces\\ndrug delivery\\nmri-powered untethered magnetic robots\\norientation control\\nthree-dimensional fluids\\n3-dof position control\\npath-planning-based 5-dof control algorithm\\noptimal controller\\nrobot manufacturing errors\\npitch angle\\nneutral pitching angle\\n3d bezier curves\\nworst-case path-tracking error\\nposition-tracking error\\norientation-tracking error\\npitch angles\\nfuture mri-powered active imaging\\nlaser surgery\\nbiopsy robots\\nminiature robots\\nmagnetic actuation\\noptimal control\",\"626\":\"humanoid robots\\nmathematical model\\nfriction\\ntask analysis\\ngravity\\napproximation theory\\nlegged locomotion\\nquadratic programming\\nmultilegged robots\\nmulticontact setting\\ndesired sliding-task motions\\ncenter-of-mass\\nadmissible convex area\\ncontact positions\\ncom support area\\ncsa\\nappropriate com position\\nmultiple fixed sliding contacts\\nhrp-4 humanoid robot\\nqp optimization problems\\nhumanoid and multi-legged robots\\nbalance\\nmulti-contacts\\nsliding contacts\",\"627\":\"legged locomotion\\nfoot\\nkinematics\\nhumanoid robots\\nhip\\ncomputational modeling\\ntask analysis\\nmotion control\\nposition control\\npredictive control\\nrobot dynamics\\nreduced five mass model\\nanalytical solution\\nmass distribution\\ninertial properties\\ndesired foot positioning\\ncrb inertia properties\\nmodel predictive control\\ndynamic kicking motion\\ninertia constraints\\nanalytical method\\nwhole-body motions\\nfast whole-body motion control\\nhumanoid open platform robot\\ndesired composite rigid body inertia\",\"628\":\"silicon\\nplanning\\nfoot\\nminimization\\nkinematics\\nlegged locomotion\\ninteger programming\\nlinear programming\\nminimisation\\ntrajectory control\\nkinematic reachability\\ncontact effectors\\nquasistatic com trajectory\\nquasiflat contacts\\ncontact surfaces\\nsl1m\\nuneven terrain\\ncombinatorial contact selection problem\\nmixed-integer optimization solvers\\nsparsity properties\\nl1 norm minimization techniques\\nonline contact replanning\\nsparse l1-norm minimization\",\"629\":\"trajectory\\ntask analysis\\nfoot\\nmanifolds\\npelvis\\nlegged locomotion\\nend effectors\\nhumanoid robots\\nmanipulator dynamics\\nmobile robots\\nmotion control\\npath planning\\nrobot programming\\nlocomanipulation plans\\nlocomotion constrained manifold\\nend-effector trajectory\\ninjective locomotion constraint manifold\\nlocomotion scheme\\nadmissible manipulation trajectories\\nweighted-a* graph search\\nplanner output\\ncontact transitions\\npath progression trajectory\\nwhole-body kinodynamic locomanipulation plan\\nlocomanipulability region\\nedge transition feasibility\\nnasa valkyrie robot platform\\ndynamic locomotion approach\\nexample locomanipulation scenarios\\ndivergent-component-of-motion\",\"630\":\"pelvis\\nlegged locomotion\\nfoot\\ndynamics\\nvehicle dynamics\\nrobot kinematics\\nforce control\\nhumanoid robots\\nmotion control\\npd control\\nposition control\\nsprings (mechanical)\\nbipedal control\\nminimal model information\\ndynamic impacts\\nwalking running controllers\\nmodeling information\\nforce-based control\\nbipedal balancing\\ntallahassee cassie robotic platform\\nbipedal robots\\nforce-based double support balancing controller\\ndynamic terrain scenarios\\nrobotic bipedal platform\\nminimal information\\nrobot model\\nindividual links\\npelvis-centric pelvis positions\\ncommanding pelvis positions\\nmodel-free pd controller\",\"631\":\"lattices\\nrobot kinematics\\ncommunication networks\\nrobustness\\nrobot sensing systems\\nenergy consumption\\nmulti-robot systems\\nnetwork theory (graphs)\\ncubic lattices\\ndense r-robust formations\\nrobot networks\\nmalicious robots\\ndefective robots\\nhigh energy consumption\\ncommunication network\\nrobot formations\\nsquare lattices\\ntriangular lattices\",\"632\":\"robots\\nprobabilistic logic\\nsecurity\\noptimization\\ntopology\\nobservers\\nmulti-robot systems\\ncombinatorial mathematics\\ncomputational complexity\\ngraph theory\\nmatrix algebra\\nmonte carlo methods\\noptimisation\\nset theory\\nstatistical distributions\\nmultirobot system\\nmrs\\nrobot interactions\\nprobability distribution\\noptimal solution\\nrooted k-connections problem\\ngraph transformations\\nweighted matroid intersection algorithm\\nedge set\\ninteraction graph\\noptimal security solution\\nsecure multirobot systems\",\"633\":\"robot kinematics\\nrouting\\nmulti-robot systems\\ncomplexity theory\\nrelays\\ncommunication networks\\npeer-to-peer computing\\nradiocommunication\\ncommunication routing\\nground-level communication\\nmultirobot coordination frameworks\\nmultirobot system\\nmultirobot networks\\npeer-to-peer radio communication\",\"634\":\"robot kinematics\\ntask analysis\\nmiddleware\\ncollision avoidance\\npython\\ncontrol engineering computing\\ncontrol system synthesis\\nlearning (artificial intelligence)\\nmobile computing\\nmobile robots\\nmulti-threading\\npath planning\\nprogram debugging\\nspecification languages\\nheterogeneous distributed coordination\\nlibraries\\ndevelopment tools\\napplication development processes\\nmachine learning\\ncyphyhouse\\ndebugging\\ndistributed mobile robotic applications\\ndistributed applications\\nkoord programming language\\ncontroller design\\ndistributed network protocols\\nplatform-independent middleware\\nmultithreaded simulator\\nkoord applications\\napplication code\\nheterogeneous agents\\nheterogeneous mobile platforms\\ndesign cycles\\nrobotic testbed\\ndistributed task allocation\\ndeployment toolchain\\nhardware-agnostic application\",\"635\":\"robots\\ntask analysis\\ncollision avoidance\\npath planning\\nresource management\\nrandom variables\\nplanning\\ndistributed algorithms\\ngraph theory\\nmulti-robot systems\\nprobability\\nstochastic processes\\nsimultaneous path planning\\nmultiple robots\\nstochastic path costs\\nstochastic edge costs\\nrobot team\\nstochastic travel costs\\nchance-constrained simultaneous task assignment\\ndeterministic simultaneous task assignment\\nshortest paths\\ntask locations\\nlinear assignment problem\\ncc-stap\",\"636\":\"robot kinematics\\ntopology\\nrobot sensing systems\\nmulti-robot systems\\nsymmetric matrices\\nlaplace equations\\ninteger programming\\nmathematical programming\\nmotion control\\nstable coordinated motion\\nrobot-to-robot interactions\\nasymmetric interaction topologies\\nmultirobot motion\\nmixed integer semidefinite programming\\nmultirobot systems\\nasymmetric interactions\\noptimal topology selection\",\"637\":\"robots\\nplanning\\nclutter\\naerospace electronics\\n1\\/f noise\\ntask analysis\\nautomation\\nmanipulators\\nmobile robots\\npath planning\\nhuman-operator guided planning\\nlow-level planner\\nfully autonomous sampling-based planners\\nhuman-in-the-loop\\nhigh-level plan\\ncontrol-based randomized planning\\npushing-based manipulation\\nnonprehensile manipulation\",\"638\":\"robots\\nkinematics\\nanalytical models\\nshape\\njacobian matrices\\naerospace electronics\\nlinear programming\\nmechanical stability\\nmobile robots\\nmotion control\\nrobot kinematics\\ntolerance analysis\\npuzzleflex\\nloose joints\\nfree motions\\nplanar assembly\\nrigid bodies\\nlocal distance constraints\\nconfiguration space velocities\\nlinear programming formulation\\nstructural stability perturbation analysis\",\"639\":\"shape\\npredictive models\\nplanning\\nanalytical models\\nrobots\\ncomputational modeling\\ntask analysis\\ncontrol engineering computing\\ninference mechanisms\\nmanipulators\\nneural nets\\nrobot vision\\nstate estimation\\nvision-based manipulation\\ncontact reasoning\\ncontact interactions\\nmotion optimization\\nstate representation\\nneural networks\",\"640\":\"robots\\nplanning\\ntrajectory optimization\\nskeleton\\nprogramming\\ngeometric programming\\nmanipulators\\npath planning\\nstochastic processes\\ntrajectory control\\nprobabilistic framework\\nconstrained manipulations\\nmotion planning\\nmanipulation planning framework\\nhierarchical structure\\nlogic rules\\nlarge-scale sequential manipulation\\ntool-use planning problems\\nlgp formulation\\nstochastic domain\\nposterior path distribution\\ngaussian path distribution\\nlogic-geometric programming\",\"641\":\"planning\\ncollision avoidance\\nmanipulators\\nphysics\\ncomputational modeling\\ntask analysis\\ncomputer simulation\\npath planning\\nplanning model\\nrobot manipulator\\nplanning with selective physics based simulation\",\"642\":\"trajectory\\nplanning\\ncontacts\\nheuristic algorithms\\nforce\\nconvergence\\nfriction\\nclosed loop systems\\ndynamic programming\\nlinear systems\\nmanipulators\\nstability\\nswitching systems (control)\\ntrajectory control\\nhybrid trajectories\\nplanar manipulation primitives\\nhybrid differential dynamic programming\\nclosed-loop execution\\nfrictional contact switches\\nhybrid ddp\\nfinite horizon trajectories\\nlinear stabilizing controllers\\nplanar pivoting\\nhybrid switches\\npose-to-pose closed-loop trajectories\\nplanar pushing\",\"643\":\"cameras\\nrobot vision systems\\nthree-dimensional displays\\ncolor\\nimage color analysis\\nprediction algorithms\\nimage colour analysis\\nimage fusion\\niterative methods\\nrobot vision\\nstereo image processing\\nblack objects\\nstereo cameras\\ndepth values\\nstructured-light camera\\nlight path\\ndepth fusion model\\nhigh-quality point clouds\\nshort-range robotic applications\\nfusion weights\\ndepth images\\nfused depth\\ndepth prediction\\nstereo model\\niterative closest point algorithm\\ndeep depth fusion\\ntransparent objects\\nreflective objects\",\"644\":\"laser radar\\ncameras\\npipelines\\nthree-dimensional displays\\nvisualization\\nrobustness\\nrobot sensing systems\\nimage enhancement\\nimage matching\\nimage texture\\nmotion estimation\\noptical radar\\nradar imaging\\nstereo image processing\\nrotating lidar\\nstereo camera pair\\nsensor motions\\nmaturing technique\\ninspection purposes\\nlidar-enhanced structure-from-motion estimation\\nlidar-enhanced sfm pipeline algorithms\\npipeline\",\"645\":\"cameras\\nthree-dimensional displays\\nobject detection\\nsensor fusion\\nrobot sensing systems\\ntwo dimensional displays\\nradar\\nbelief networks\\nimage fusion\\nsensor synchronization\\nmultiple sensors\\nlow-level sensor fusion\\nautomotive use-cases\\nprobabilistic low level automotive sensor fusion approach\\ncamera data\\nassociated data\\nsensor modalities\\nprobabilistic fusion\\nassociation method\\nbayesian networks\",\"646\":\"robot sensing systems\\nlegged locomotion\\ntask analysis\\ninstruments\\nrobot kinematics\\ngait analysis\\nkinematics\\nmedical computing\\nmobile robots\\nregression analysis\\nsupport vector machines\\nautonomous gait analysis system\\nmobile robot\\ncustom-engineered instrumented insoles\\non-board rgb-d sensor\\ninertial sensors\\nforce sensitive resistors\\nrobot companion\\nwalking exercises\\nsupport vector regression models\\nfundamental kinematic gait parameters\\noptical motion capture system\\nsvr models\\nautonomous mobile robots\\nout-of-the-lab gait analysis\\nrobot-assisted wearable sensor-mediated autonomous gait analysis\\nwearable technology\\ninstrumented footwear\\nassistive robotics\\nsportsole\",\"647\":\"force\\npose estimation\\nimpedance\\ntask analysis\\nthree-dimensional displays\\nrobots\\ndamping\\ncontrol engineering computing\\nforce control\\nimage colour analysis\\nimage sensors\\nindustrial manipulators\\nposition control\\nproduction engineering computing\\ncontrol framework definition\\nforce-controlled tasks\\nindustrial robots\\nincreasing autonomy\\nmanipulator\\nworking environment\\nrobust behavior\\nindustrial interaction tasks\\nuncertain working scenes\\n6d pose estimation\\nfeatureless parts\\nvariable damping impedance controller\\nadaptive saturation pi\\nouter loop\\nhigh accuracy force control\\nforce error\\novershoots avoidance\\ntask uncertainties\\npositioning errors\\nassembly task\\nforce-tracking task\\nforce overshoots\\nreference force\",\"648\":\"force\\nimpedance\\nforce control\\nprobes\\nsurface impedance\\nmanipulators\\nadaptive control\\nmedical robotics\\nmotion control\\nparameter estimation\\nposition control\\nsurgery\\nposition-based adaptive force controller\\ninteraction forces\\nnonstationary environments\\nfast parameter estimation\\ncompliant contact parameters\\nadmittance force modulation\\nnonstationary compliant surface\\nsafety-critical applications\\nmechanical probing strategy\\nenvironmental impedance parameters\\ncompliant environments\\nrobotic manipulator autonomous control\\nmanipulator controller design\\nsurgical tasks\\nmotion compensation\",\"649\":\"force\\nrobots\\ntask analysis\\nsurface impedance\\ndynamics\\nimpedance\\ntracking\\nadaptive control\\nforce control\\nlearning systems\\nmanipulator dynamics\\nmotion control\\nradial basis function networks\\nuncertain systems\\nonline adaptation\\nstate-dependent force correction model\\nforce error\\ncollaborative cleaning task\\ntask adaptation\\nadaptive force control\\nreactive behaviours\\nadaptive behaviours\\nforce adaptation\\ncontact tasks\\nrobot dynamics\\nforce tracking accuracy\\ncompensation model\\nadaptive framework\\nforce generation\\ntime-invariant dynamical system framework\\nradial basis functions\\nkuka lwr iv+ robotic arm\\ncompliance and impedance control\\nphysical human-robot interaction\",\"650\":\"semantics\\nimage segmentation\\ncameras\\ntraining\\ntask analysis\\nsatellites\\nestimation\\nfeature extraction\\nlearning (artificial intelligence)\\nobject detection\\nweakly supervised silhouette-based semantic scene change detection\\nnovel semantic scene change detection scheme\\nsemantic change detection network\\nlarge-scale dataset\\nspecific dataset\\nsemantic extraction\\nchange detection task\\nsiamese network structure\\npublicly available dataset\",\"651\":\"semantics\\ntask analysis\\nthree-dimensional displays\\nfeature extraction\\nlogic gates\\ntraining\\neuclidean distance\\nfeature selection\\nimage enhancement\\nimage segmentation\\nlearning (artificial intelligence)\\nrobust joint 3d semantic-instance segmentation\\nhuman scene perception process\\ncfsm\\ncoupled feature selection module\\nreciprocal semantic instance feature selection\\n3dcfs\\nrobust 3d point cloud segmentation framework\",\"652\":\"task analysis\\nbandwidth\\nsemantics\\ntraining\\ncollaboration\\nrobot sensing systems\\naircraft communication\\nautonomous aerial vehicles\\nimage segmentation\\nlearning (artificial intelligence)\\nmulti-agent systems\\nmulti-robot systems\\nneural nets\\nvisual perception\\ndegraded sensor data\\ncompressed request\\naerial robots\\nsemantic segmentation task\\ncollaborative perception\\nlearnable handshake communication\\nlocal observations\\nneighboring agents\\nmultiagent reinforcement learning\\nbandwidth-sensitive manner\\nscene understanding tasks\\ncommunication protocols\\nmultistage handshake communication mechanism\\nneural network\\nwho2com\\nairsim simulator\\nairsim-cp dataset\",\"653\":\"labeling\\nsemantics\\nthree-dimensional displays\\nsimultaneous localization and mapping\\ncameras\\nimage reconstruction\\nreal-time systems\\ndata visualisation\\nimage representation\\nlearning (artificial intelligence)\\nmobile robots\\nslam (robots)\\nview-based labelling\\nspatial ai systems\\nreal-time height map fusion\\nmap-based labelling\\ngenerated scene model\\ninput view-wise data\\nestimate labels\\nclear groups\\nlabelling scenes\\nsemantic labels\\ngeometric models\\npersistent scene representations\\nreal-time slam\\nmap-based semantic labelling\",\"654\":\"grammar\\nrobots\\nproduction\\ntesting\\ntraining\\nrandom variables\\nprobabilistic logic\\ngrammars\\ninference mechanisms\\nmobile robots\\nobject detection\\nprobability\\ntrees (mathematics)\\nvariational inference algorithm\\nobserved environments\\nnontrivial manipulation-relevant datasets\\nscene grammars\\ndiscrete variables\\ncontinuous variables\\nprobabilistic generative model\\nscene trees\\nhierarchical relationships\\nlabeled parse trees\",\"655\":\"visualization\\ncognition\\nbenchmark testing\\nrobots\\nimage color analysis\\ntask analysis\\nplastics\\ncontrol engineering computing\\nimage representation\\ninference mechanisms\\nmanipulators\\nnatural language processing\\nquery processing\\nrobot vision\\ntext analysis\\nobject perception\\nrobotics applications\\nobject grasping\\nobject properties\\nvisual text data\\nhousehold objects\\nnatural language descriptions\\nquestion-answer pairs\\nvisual reasoning queries\\nscene semantic representations\\nsymbolic program execution\\ndisentangled representation\\nvisual inputs\\ntextual inputs\\nsymbolic programs\\nreasoning process\\nshop-vrb\\nvisual reasoning benchmark\\nobject manipulation\",\"656\":\"grippers\\nforce\\nrobot sensing systems\\nstrain\\nbladder\\nmechanical sensors\\nclosed loop systems\\ndeformation\\ndexterous manipulators\\nfeedback\\ntactile sensors\\ndelicate object grasping\\nuniversal grasping\\nheavy bulky items\\nlightweight delicate objects\\nhighly flexible latex bladders\\nmagic ball origami gripper\\ntactile sensing\\nproprioceptive sensing\\nsensor feedback\\nclosed loop controller\\ncontinuum body gripper sensorization\\nhigh force object grasping\",\"657\":\"nails\\ngrippers\\ngrasping\\nthree-dimensional displays\\nactuators\\npayloads\\nfingers\\ndexterous manipulators\\nmanipulator dynamics\\npath planning\\npneumatic actuators\\nmanipulation tasks\\nnormal grasping forces\\ndelicate pinch grasps\\nrobotic grasping tasks\\nsoft gripper\\nadvanced grasping\\nretractable finger nails\\nreconfigurable palm\\nfinger nail mechanism\",\"658\":\"muscles\\nneurons\\nwrist\\ntraining\\nfeature extraction\\nneural networks\\nprincipal component analysis\\nbiomechanics\\nelectromyography\\ngesture recognition\\nimage motion analysis\\nlearning (artificial intelligence)\\nmedical signal processing\\nprosthetics\\nsemg data\\nhand motion measurement\\nautomated data labeling neural network\\nhand motion myoelectric decoding\\nhand motion labels\\nunsupervised neural network\\nunlabeled semg\\nhand motion signals\\nbio-signals\\nsurface electromyography array\\ndataset collecting\",\"659\":\"navigation\\nspace vehicles\\npredictive models\\nstrain\\ntask analysis\\ntrajectory\\nautonomous vehicles\\ncollision avoidance\\nmobile robots\\nmotion control\\npath planning\\nremotely operated vehicles\\nroad traffic control\\nroad vehicles\\ntrajectory control\\npedestrian-vehicle cooperation\\nautonomous vehicle\\nintelligent transportation\\nautonomous navigation research\\nsafe proactive navigation\\npedestrian-vehicle interaction behavioral model\\ninteraction scenario\\nquantitative time-varying function\\ncooperation estimation\\ncooperation-based trajectory planning model\",\"660\":\"navigation\\nbiological system modeling\\nrobot sensing systems\\nmobile robots\\ndesign methodology\\ntask analysis\\nhuman-robot interaction\\nmotion control\\nsocial robot navigation methods\\nsocial navigation methods\\nhuman sciences fields\\nmobile robot navigation\\nrobot behavior\\nsocial hierarchy\\nsocio-physical context\\nrobot motion\\nhuman behavior\",\"661\":\"task analysis\\nservice robots\\nnatural languages\\nhuman-robot interaction\\nrobot sensing systems\\nplanning\\nhuman computer interaction\\nmobile robots\\nnatural language interfaces\\npath planning\\nhuman instructions\\nservice environments\\nrobot plan model generation\\nnatural language interface\\ninteraction inconvenient\\nverbal interaction-based method\\nhuman involvement\\nhuman user\\nunclear instructions\\nreactive plan model\",\"662\":\"collision avoidance\\nnavigation\\ntraining\\nrobot sensing systems\\nlasers\\npath planning\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nrobot dynamics\\nrobot programming\\ntime-efficient path planning behavior\\ndynamic crowds\\nsocial-safety-awareness\\nreinforcement learning\\n2d laser scans\\nmapless collision-avoidance navigation\\nego-safety\\npedestrians\\nrobot tests\",\"663\":\"magnetic fields\\nmagnetosphere\\nmagnetic confinement\\nmagnetohydrodynamics\\npropulsion\\nnavigation\\nbiomechanics\\ncell motility\\ncomputational fluid dynamics\\nflow simulation\\nhydrodynamics\\nmagnetic actuators\\nmicrorobots\\nmobile robots\\nnavier-stokes equations\\nposition control\\nswirling flow\\ntrajectory control\\nvortices\\nprospective robotic agents\\nrotating magnetic field\\nmagnetized swimmer\\nhelical tail\\nhelical paths\\npusher-mode swimmers\\nrotating magnetic head\\nmicroswimmers\\nswimmer orientation\\nrender orientation-based methods\\nconfined swimmer\\ncontrol law\\nswimmer position\\nhelical pusher-mode trajectories\\nsteering control\\nmagnetic helical swimmers\\nhelical swimming\\nlow reynolds number\\nsteering\\ncontrol\\nstability\",\"664\":\"morphology\\nrobots\\nshape\\nfinite element analysis\\ncomplexity theory\\ncomputational modeling\\nmachine learning\\naerospace components\\naerospace control\\nlearning (artificial intelligence)\\nmobile robots\\nbiological exemplars\\nrobot design\\nmorphology-in-the-loop flapping wing design\\nrobot complexity\\nsim2real gap\\ndesign complexity\\nsim2real transfer\\nhigh performance robot morphologies\\nparameterised morphology design space\\nflapping wing flight\\nsimulation to reality\\nevolution\\nbio-inspired\",\"665\":\"numerical models\\nforce\\nmathematical model\\nhardware\\nsteady-state\\ncomputed tomography\\naerodynamics\\naerospace components\\nautonomous aerial vehicles\\nlinearisation techniques\\nposition control\\nvelocity control\\nlongitudinal gliding flight configuration\\naerodynamic forces\\nlinearized potential theory\\nflat plate\\nflapping-wing episodes\\nlinear potential theory\\nsteady-state descent\\nterminal velocity\\npitching\\ngliding angles\\ntail position\\nflapping-wing configuration\\nflight velocity\\nclimbing episodes\\nrealistic simulation tool\\nflapping frequencies\\nornithopter flight\\nlinearized model\\nflapping-wings uav\\nunreal engine 4\",\"666\":\"aerodynamics\\nmanipulator dynamics\\nmathematical model\\nrobot sensing systems\\nbirds\\naerospace control\\nangular momentum\\nbiomimetics\\nclosed loop systems\\ngears\\ngeometry\\nmobile robots\\nmotion control\\nremotely operated vehicles\\nrobot dynamics\\nbat-style perching maneuver\\naerial drone designs\\naerial flip turns\\nlanding surface\\nzero-angular-momentum turns\\ndetachable landing gear\\nclosed-loop manipulations\\nbiomimicry\\ninertial dynamics manipulation\\nbat flight characteristics\\ndynamical system\\ngeometric conservation properties\",\"667\":\"visualization\\nkernel\\noptical filters\\ndrones\\nband-pass filters\\nbiological system modeling\\ninsects\\naerospace robotics\\ncollision avoidance\\nfeedback\\nimage filtering\\nimage motion analysis\\nimage sequences\\nmicrorobots\\nobject detection\\nrobot vision\\nobject motion filters\\nobstacle negotiation\\nmicroaerial systems\\nbiological visual guidance\\nmachine vision system\\nmotion vision\\ndense optic flow map\\ninsect vision inspired object motion filter model\\nmicroracing drone\\nproximaldistal object separation\\nearly-stage motion detection\\nfeedback control loop\\nobstacle avoidance\\nvisual guidance\",\"668\":\"robots\\nfasteners\\nbrushless motors\\nskin\\npropulsion\\ntorque\\nsensors\\nmobile robots\\nrobot dynamics\\norientation control\\nversatile serpentine robot platform\\nscrew threads\\nmechanical design\\nelectrical design\\nreconfigurable serpentine robot\\nserpentine robots\\nscrew propulsion\\narcsnake robot\\nomni-wheel drive-like motions\\nnasa-jpl eels program\\nnasa-jpl exobiology extant life surveyor program\",\"669\":\"ground penetrating radar\\nthree-dimensional displays\\nimage reconstruction\\ndielectrics\\nfeature extraction\\nobject detection\\ninspection\\ngeophysical image processing\\ngeophysical techniques\\nlearning (artificial intelligence)\\nneural nets\\nradar detection\\nradar imaging\\nstereo image processing\\ngpr-based subsurface object detection\\ndepthnet\\nnondestructive evaluation devices\\nunderground scene\\ngpr based inspection\\nunderground targets\\npose information\\ngpr device\\ngpr image\\nb-scan data\\ngpr scan data\\nb-scan image\\ngeophysical survey system inc.\\nsynthetic gpr data\\nb-scan feature detection\\nunderground target depth prediction\\ngssi\\nvisual inertial fusion module\\nvif module\\ngprmax3.0 simulator\\nnde devices\\n3d gpr migration\\ndielectric prediction system\\ndeep neural network module\",\"670\":\"cameras\\nrobot vision systems\\nmanipulators\\nreal-time systems\\npipelines\\ntask analysis\\ncutting\\nend effectors\\ngardening\\nrobot vision\\nservice robots\\nstereo image processing\\nvisual servoing\\nmultiple cameras\\nsingle stereo camera\\nend effector\\nrobotic arm\\nreal time stereo visual servoing\\nautomated robotic rose cutter\\nrose bush pruning\\nrose pruning robots\",\"671\":\"wheels\\nmobile robots\\nangular velocity\\nrobot sensing systems\\nresource management\\nvelocity measurement\\nangular velocity control\\nangular velocity measurement\\ncentralised control\\nmotion control\\npower transmission lines\\nservice robots\\nwheel slippage\\nslip-limiting controller\\nredundant line-suspended robots\\nv-shaped wheels\\nwheel radius\\nwheel angular velocity measurements\\nslip limitation\\ncontrol allocation algorithm\\nhigh-level velocity controller\\ncentralized control\\nline ranger\",\"672\":\"manipulators\\nkinematics\\nsociology\\nstatistics\\ntrajectory\\ngenetic algorithms\\ntask analysis\\nend effectors\\nredundant manipulators\\nsearch problems\\ntrajectory control\\ntunnels\\ninterval search strategy\\nreference point strategy\\ncontinuous motion\\ninterval search genetic algorithm\\ninverse kinematics problem\\nparametric joint angle method\\npopulation continuity strategy\\nevolutionary generation\\nfitness function\\ntunnel shotcrete robot\\nend effector\",\"673\":\"jacobian matrices\\nmanipulators\\nkinematics\\nacceleration\\nfasteners\\napproximation theory\\nend effectors\\niterative methods\\nlie algebras\\nmanipulator kinematics\\nmanipulator jacobian\\nhigh-order derivatives\\nlie theory\\nhigher-order derivatives\\nhigher-order jacobian derivatives\\nserial manipulator kinematics\\njoint variables\\njoint-space coordinates\\nserial manipulator jacobians\\ntask-space cartesian coordinates\\ninertial-fixed frames\\nbody-fixed frames\\nkuka lrb iiwa7 r800 manipulator\",\"674\":\"kinematics\\noptimization\\nconferences\\nautomation\\nrobots\\ntask analysis\\nconvex programming\\nend effectors\\nmanipulator kinematics\\nmobile robots\\npolynomials\\nposition control\\ndegrees of freedom\\narticulated robots\\nglobally optimal solution\\nserial manipulators\\nkinematic constraints\\nhighly redundant serial kinematic chains\\njoint limit constraints\\ninverse kinematics problem\\nconvex optimization techniques\\nnumerical methods\\nnonlinear problem\\nfeasible joint configurations\\ntask-related workspace constraints\\nsum of squares optimization\",\"675\":\"task analysis\\nstability analysis\\nrobots\\nkinematics\\nthermal stability\\nnumerical stability\\nasymptotic stability\\nclosed loop systems\\ncontrol system synthesis\\ndiscrete time systems\\nhumanoid robots\\nlinear matrix inequalities\\nlyapunov methods\\nmathematical programming\\nmobile robots\\nrobot kinematics\\nstability\\nmultitask closed-loop inverse kinematics stability\\nmultiobjective task resolution\\nlocal stability problem\\nclosed-loop inverse kinematics algorithm\\nhighly redundant robots\\nsystem stability\\nclosed-loop control gains\\nsemidefinite programming problem\\ndiscrete-time lyapunov stability condition\\nsdp optimization problem\\nstability conditions\",\"676\":\"task analysis\\ncutting tools\\nmilling\\nforce\\nservice robots\\nsafety\\nindustrial robots\\nmachine tools\\nmobile robots\\nwood\\nindustrial operators\\ncollaborative robot\\ncarpentry task\\nrobotic assistance strategy\\nmachine-tool\\nwood milling\\naccidentogenic aspect\\nphysical model\\ntooling process\",\"677\":\"visualization\\nshape\\ncameras\\nsemantics\\nrobustness\\ngeometry\\nbuildings\\nconvolutional neural nets\\ndisasters\\nimage representation\\nimage texture\\nlearning (artificial intelligence)\\nnatural scenes\\npose estimation\\nshape recognition\\nshape-based representation\\nvisual localization\\nextremely changing conditions\\nconvolutional neural network\\nlayout changes\\napproximate scene coordinates\\nscene layout\\ncnn\\nstylized images\\nestimated dominant planes\\nquery images\\nsimulated disaster dataset\\nreliable camera pose predictions\",\"678\":\"trajectory\\nplanning\\nsafety\\nreachability analysis\\nrobustness\\noptimization\\naerospace engineering\\naircraft control\\ncollision avoidance\\nhelicopters\\nrobust control\\nset theory\\ntrajectory control\\nobstacle avoidance\\nrisk free flight\\nbackward reachable sets\\nforward reachable sets\\nrobust trajectory planning algorithm\\nsafety guarantee\\nmultirotor\\nhamilton-jacobi reachability analysis\",\"679\":\"predictive models\\nrobots\\nstochastic processes\\nplanning\\ndata models\\ncomputational modeling\\nrobustness\\nbayes methods\\nbelief networks\\ncontinuous time systems\\nmotion control\\npath planning\\npredictive control\\nprobability\\nreachability analysis\\nprobabilistic predictive models\\nhuman behavior\\nfuture motion\\nobservation models\\nstate predictions\\nrobot motion plan\\nhuman behavioral data\\nhuman motion prediction\\nhamilton-jacobi reachability problem\\ncontinuous-time dynamical system\\nmodel parameters\\nworst-case forward reachable set\\nfuture state distributions\\nrobust planning\\nhamilton-jacobi reachability-based framework\\nhuman motion analysis\\nsafe planning\\nreal-world autonomous systems\",\"680\":\"robot sensing systems\\nprivacy\\ndata privacy\\ncameras\\ntask analysis\\nlaw\\ndesign\\nrobots\\nsensors\\njudicious sensor selection\\nroboticists\\nrobot design\\nrobotics journals\\nprivacy preservation\\nrobot lifecycle\\nprivacy impact assessments\\nprivacy enhancement\\nprivacy by design\\nrobotics\\nsensor selection\\ncompliance\",\"681\":\"safety\\nrobustness\\nstochastic processes\\nrobots\\ntrajectory\\nnonlinear dynamical systems\\nheuristic algorithms\\nlearning (artificial intelligence)\\nmobile robots\\nnonlinear control systems\\npredictive control\\nprobability\\nrobust control\\nstochastic systems\\nbackup policy\\nlearned policy\\ncontrol policy\\nadditive stochastic disturbances\\nnominal dynamics\\nstochastic nonlinear dynamical systems\\nstochastic dynamics\\nsafe reinforcement learning\\nrobust model predictive shielding\\nstatistical learning theory\\nbackup controller\\ntube-based robust nonlinear model predictive controller\",\"682\":\"collision avoidance\\nrobot kinematics\\nheuristic algorithms\\ntopology\\nconvergence\\ndamping\\ndecentralised control\\nmobile robots\\nmulti-robot systems\\ndecentralized control strategy\\nheterogeneous robot swarms\\nformation control\\ncollision avoidance strategy\\nmultiple heterogeneous robots\\nheterogeneous swarm segregation\",\"683\":\"robots\\nestimation\\nshape\\ntask analysis\\nheuristic algorithms\\nrandom variables\\nclocks\\ncontrol engineering computing\\ndistributed algorithms\\nmulti-robot systems\\nparticle swarm optimisation\\npath planning\\ncounting swarm\\ndistributed algorithm\\nneighboring robots\\nrobot swarm\",\"684\":\"robot sensing systems\\nbayes methods\\ndecision making\\nclassification algorithms\\ntask analysis\\ncolor\\nmulti-robot systems\\ncollective bayesian decision-making\\ndecentralized robot swarms\\ndistributed bayesian algorithm\\nspatially distributed feature\\nfarm field\\nrobotics\\ndecentralized bayesian algorithms\\nsparsely distributed robots\\ndecision-making accuracy\\nbio-inspired positive feedback\\nfixed-time benchmark algorithm\\nbayes bots\\nbio-inspired approaches\",\"685\":\"collision avoidance\\nrobot sensing systems\\nmobile robots\\ngenerators\\nsupervisory control\\ndiscrete event systems\\nmulti-robot systems\\nrobot swarms\\npublic events\\nsupervisory control theory\\nformal framework\\ncorrect-by-construction controllers\\nswarm robotics systems\\nextended sct framework\\ne-puck robots\",\"686\":\"tools\\nsolid modeling\\nthree-dimensional displays\\nrobot sensing systems\\ngray-scale\\ncollision avoidance\\ncontrol engineering computing\\nlaser ranging\\nmobile robots\\nslam (robots)\\nsolid modelling\\ngazebo world construction\\ngrayscale image\\n3d solid model\\nrobot simulators\\nsimulated physical environment\\n2d image\\n2d laser range finder data\\ngazebo simulator\\n3d collada\\nsimultaneous localization and mapping\\nreal-time factor\\nslam missions\\nrtf\",\"687\":\"receivers\\ntransmitters\\nsensors\\nelectromagnetics\\nantennas\\nrobots\\nunmanned aerial vehicles\\naerospace communication\\nautonomous aerial vehicles\\ncontrol engineering computing\\noperating systems (computers)\\nradio transceivers\\nrescue robots\\nrobot programming\\nros gazebo plugin\\nforefront technology\\nsearch & rescue operations\\narva sensor simulation\\ntransceiver sensor\\nappareil de recherche de victims en avalanche\\nunmanned aerial vehicle\",\"688\":\"robots\\nsolid modeling\\ncognition\\nphysics\\nthree-dimensional displays\\ndata models\\ngeometry\\ncad\\ncameras\\nimage classification\\nlearning (artificial intelligence)\\nobject recognition\\npose estimation\\narticulated human body\\nobject affordances\\nphysical interactions\\nphysical simulations\\narbitrarily oriented object\\nphysical sitting interaction\\nobject affordance reasoning\\nobject classification\\nchair classification\\nappearance-based deep learning methods\\naffordances imagining\\nsynthetic 3d cad models\\ntraining data\\nfunctional pose predictions\\ndepth camera\",\"689\":\"grippers\\ngrasping\\ncameras\\ntraining\\nrobot vision systems\\ncontrol engineering computing\\nend effectors\\nimage colour analysis\\nlearning (artificial intelligence)\\nrendering (computer graphics)\\nrobot vision\\ndirectional semantic grasping\\ndeep reinforcement learning\\ndouble deep q-network\\nrobot simulator\\nrendering\\nmonocular rgb images\\nwrist mounted camera\\ncartesian robot control\\ncrossentropy method\\ndomain randomization\\nend effector\",\"690\":\"robots\\nfriction\\nnumerical models\\nbayes methods\\nmaterial properties\\ntask analysis\\ncalibration\\ngranular flow\\ngranular materials\\nindustrial robots\\nrolling friction\\nsliding friction\\ngranular media\\nrobotic tasks\\ncereal grains\\nplastic resin pellets\\nrobotics-integrated industries\\npharmaceutical development\\naccurate simulation\\nhardware framework\\nfast physics simulator\\nreal-world depth images\\ngrain formations\\nlikelihood-free bayesian inference\\ncalibrated simulator\\nunseen granular formations\\nsimulator predictions\",\"691\":\"tools\\ntask analysis\\nrobots\\nvisualization\\nforce\\nthree-dimensional displays\\nneural networks\\ncontrol engineering computing\\nimage representation\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nketo\\ntool manipulation\\ninformative representation\\ntask-specific keypoints\\n3d point clouds\\ntool object\\ndeep neural network\\ninformative description\\nself-supervised robot interactions\\ntask environment\\nmanipulation tasks\\ntask success rates\\nkeypoint prediction\\ntool generation\\nlearned representations\\nkeypoint representation learning\",\"692\":\"task analysis\\nrobots\\nvisualization\\npredictive models\\ngrasping\\nhead\\ndata models\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\nobject detection\\nrobot vision\\nvisual priors\\nvision-based manipulation\\ntransfer learning\\npassive vision task\\ndata distribution\\nactive manipulation task\\naffordance maps\\nvision networks\\nzero-shot adaptation\\nzero robotic experience\\nvisual pre-training\\nobject manipulation\\naffordance prediction networks\",\"693\":\"pose estimation\\ngrippers\\ncollision avoidance\\nrobot kinematics\\ntactile sensors\\nassembling\\nbayes methods\\ncalibration\\nforce sensors\\nindustrial manipulators\\nparticle filtering (numerical methods)\\nrobot vision\\nrobotic assembly\\nstate estimation\\nbayesian state estimation\\nparticle filtering\\nindustrial assembly tasks\\nforce sensor\\nrobotic gripper\\nrigid object\\ncontact based inhand pose estimation\",\"694\":\"robots\\ngrasping\\ncognition\\ntask analysis\\nneural networks\\ngrippers\\nobject detection\\nhumanoid robots\\ninference mechanisms\\nmanipulators\\nneural nets\\nrobot vision\\nrobotic grasp detection\\nseparate networks\\ntarget objects\\nsingle rgb-d camera\\nmultitask dnn\\naccurate detections\\nrelationship reasoning\\nstate-of-the-art performance\\nobject grasping tasks\\nhumanoid robot\\nsingle multitask deep neural network\\ndeep neural network based object\\nnetwork output\\nhigh-level reasoning\\nvmrd\\ncornell datasets\",\"695\":\"simultaneous localization and mapping\\nvisualization\\nestimation\\ncognition\\nprobability\\nfeature extraction\\nmobile robots\\nrobot vision\\nslam (robots)\\nstatic environments\\ndynamic environments\\npersistence filters\\norb-slam\\nvisual slam algorithm\\npersistence filtering\\npersistence reasoning\\nsemistatic environments\",\"696\":\"cameras\\ndynamics\\noptical imaging\\nsimultaneous localization and mapping\\nthree-dimensional displays\\ntwo dimensional displays\\nrobustness\\nimage colour analysis\\nimage motion analysis\\nimage reconstruction\\nimage segmentation\\nimage sequences\\nmobile robots\\nmotion estimation\\nrobot vision\\nslam (robots)\\ndynamic environments\\nvisual slam\\nmoving objects\\nstatic environment features\\nlead\\nwrong camera motion estimation\\ndense rgb-d slam solution\\ncamera ego-motion estimation\\nstatic background reconstructions\\noptical flow residuals\\ndynamic semantics\\nrgb-d point clouds\\ncamera tracking\\nbackground reconstruction\\ndense reconstruction results\\ndynamic scenes\\nstatic environments\\ndynamic dense rgb-d slam\",\"697\":\"uncertainty\\nsafety\\nautonomous vehicles\\nneural networks\\nautomobiles\\nprobabilistic logic\\nbayes methods\\nbelief networks\\ncollision avoidance\\ndecision making\\ninference mechanisms\\nneurocontrollers\\nprobability\\nroad safety\\nbayesian inference methods\\nuncertainty computation\\npointwise uncertainty measures\\nend-to-end bayesian controllers\\nautonomous driving scenarios\\nbayesian neural networks\\nsensor noise\\ncontroller behaviour\\nsafety guarantees\\nneural network controllers\\nend-to-end autonomous driving control\\nstatistical guarantees\\nuncertainty quantification\",\"698\":\"tools\\nretina\\nsurgery\\nnavigation\\ntask analysis\\ntrajectory\\nrobots\\nend effectors\\neye\\nlearning by example\\nmedical robotics\\nposition control\\nrobot programming\\nvisual servoing\\nretinal surgery\\nauditory feedback\\nautonomous navigation system\\nneedle surgical tool navigation\\nlearning from demonstration\\nhaptic feedback\\ndeep network training\\nsteady hand eye robot\\nsher surgical robot\\nend effector\",\"699\":\"actuators\\nfault tolerance\\nfault tolerant systems\\nvehicle dynamics\\nlearning (artificial intelligence)\\ntraining\\nsolid modeling\\naerospace computing\\nautonomous aerial vehicles\\ncontrol engineering computing\\nfault diagnosis\\nfault tolerant control\\nhelicopters\\nposition control\\nsecurity of data\\nstability\\nfault-tolerant control policy\\nactuator\\nstabilizing controller\\ndetection activation\\nsensor faults\\nlearn-to-recover\\nuavs\\nreinforcement learning-assisted flight control\\ncyber-physical attacks\\nquadcopter unmanned aerial vehicles\\nsensor attack\",\"700\":\"standards\\nprobability distribution\\nfuses\\nimage reconstruction\\nthree-dimensional displays\\nuncertainty\\nprobability density function\\ngeometry\\nlearning (artificial intelligence)\\nneural nets\\nprobability\\nstereo image processing\\nprobabilistic fusion\\nstandard pipelines\\ndeep learning\\nstandard 3d reconstruction pipelines\\nopen problem\\ndeep neural network\\nerror models\\nstandard 3d reconstruction system\\ndense depth maps\\ndiscrete probability distributions\\nnonparametric probability distributions\\nmultiview stereo approaches\\ngeometry- based systems\\nlearned single-view depth prior\",\"701\":\"attitude control\\nadaptation models\\npayloads\\nmimo communication\\nunmanned aerial vehicles\\nadaptive control\\ngravity\\naerospace robotics\\naircraft control\\nhelicopters\\nmimo systems\\nmodel reference adaptive control systems\\nnonlinear control systems\\nrobust control\\nstability\\nmultirotor aerial robot\\nflight controller\\nflight stability\\naerial robot system\\nmodel reference adaptive control\\nnonlinear multiple-input and multiple-output\\nmrac\",\"702\":\"collision avoidance\\ncameras\\nrobot vision systems\\nplanning\\nimage sensors\\nmobile robots\\nnavigation\\nrobot vision\\nslam (robots)\\nspace vehicles\\nautonomous microaerial vehicle\\nautonomous tiercel robots\\ncollision detector design\\nfisheye camera\\nreflective obstacles\\ntransparent obstacles\\ncollision-resilient robot\\ntiercel mav\\nautonomous navigation\\nautonomous flight\",\"703\":\"propellers\\nmathematical model\\nservomotors\\ngeometry\\nbrushless dc motors\\nblades\\nadaptive control\\naerospace propulsion\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nelectric propulsion\\npitch control (position)\\nstate-space methods\\nminimum-effort operation\\nunmanned aerial vehicles\\nuav\\nelectric propulsion systems\\ndisparate flight modes\\nforward-moving flight\\nflight mode dissimilarity\\nfixed-geometry propulsion systems\\nvariable-geometry systems\\nvariable pitch propeller\\npropulsion performance\\nvpp system control\\noperation state space\\nhovering\\nnear-minimum-electrical-effort propulsion system behavior\",\"704\":\"robots\\ntask analysis\\ntraining\\ncomputer architecture\\nlearning (artificial intelligence)\\ngrasping\\nfeature extraction\\ngrippers\\nneural net architecture\\nneurocontrollers\\nreactive neural networks\\nfully connected networks\\nreactive behaviours\\nactor-critic architecture\\nrobot environment\\nend-to-end reinforcement learning\\nrobotic learning\\npick and place task\\nbehaviour-based reinforcement learning\\nbrook subsumption architecture\\npick and place robotic task\\nactor-critic policy\\nactivation mechanisms\\ninhibition mechanisms\\ngripper\\ndegree-of-freedom\",\"705\":\"interpolation\\ntraining\\nlearning (artificial intelligence)\\ngaussian processes\\nmathematical model\\nrandom variables\\noptimization\\noptimisation\\nsmooth interpolation\\nreward function weights\\noptimal value function\\nmultiobjective reinforcement learning problems\\ngaussian process\\nvalue function transforms\\nmorl problems\",\"706\":\"navigation\\ncollision avoidance\\nrobustness\\nlighting\\nrobots\\noptical imaging\\nmachine learning\\nautonomous aerial vehicles\\ncontrol engineering computing\\nimage motion analysis\\nimage sequences\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nobject detection\\nrobot vision\\nslam (robots)\\nvisual perception\\ndeep reinforcement learning\\nuav obstacle avoidance\\nlearning-based reaction local planner\\nmicrouavs\\nimage moment\\nilluminance variation\\nmapless navigation\\nmoment-based lgmd\\nbioinspired monocular vision perception method\",\"707\":\"maintenance engineering\\nrobot sensing systems\\nestimation\\ntask analysis\\nlearning (artificial intelligence)\\ncomputer science\\nfeedback\\ninteractive systems\\nmobile robots\\ninteractive reinforcement learning\\nhuman teachers\\nsensor feedback\\nlearning process\\nnoninteractive rl\\npolicy feedback\\nfeedback source\\ninteractive rl methods\\nrevision estimation-from-partially incorrect resources\\nrepair\\nphysical robot\",\"708\":\"task analysis\\nuncertainty\\nrobot sensing systems\\nlearning (artificial intelligence)\\ncameras\\nswitches\\nlearning systems\\noptimisation\\nrobots\\nguided uncertainty-aware policy optimization\\nsample-efficient policy learning\\nrobust perception system\\nreinforcement learning\\nmodel-based methods\\nlearning-based methods\\nmodel-based strategies\\npeg insertion\\nguapo\\nmodel-based policy\",\"709\":\"task analysis\\nrobots\\ntrajectory\\nvideos\\npressing\\nbenchmark testing\\ncomplexity theory\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nuser experience\\ntask reproductions\\ndemonstration approaches\\nmultiple motion-based learning\\ntask complexity\\nskill learning\\nrobot executions\\ntask performance\\nstarting configuration\\nhuman demonstrator\\nphysical robot\\ntask models\\nmanipulation tasks\\nreal-world tasks\\nrelative strengths\",\"710\":\"robots\\ntask analysis\\neducation\\ntrajectory\\nprogramming\\nthree-dimensional displays\\nimpedance\\ncontrol engineering computing\\nend effectors\\nlearning (artificial intelligence)\\nrobot programming\\nteaching\\ntelerobotics\\nwearable consumer devices\\nprogramming tools\\nrobot teleoperation\\nsalient features\\noff-the-shelf soft-articulated robotic components\\ndynamic movement primitives\\nhuman trajectories\\nimpedance regulation skills\\n7-dof collaborative robots\\nanthropomorphic end-effectors\\nrobot teaching\",\"711\":\"legged locomotion\\nrobot sensing systems\\nbiomechanics\\npredictive models\\nprosthetics\\nprobabilistic logic\\ngait analysis\\nhumanoid robots\\nhuman-robot interaction\\nintelligent robots\\nlearning systems\\nman-machine systems\\nmedical robotics\\nhuman-robot symbiotic walking\\nprobabilistic framework\\nperiodic behavior\\nperiodic movement regimes\\ncustomized models\\nhuman walking\\nlatent variables\\nbiomechanical variables\\nrobotic prosthesis\\nimitation learning approach\\nhuman participants\\nankle angle control signals\\nrobotic prosthetic ankle\\npredictive modeling\\nperiodic interaction primitives\",\"712\":\"navigation\\ntrajectory\\nheart\\nblood\\nvalves\\nfrequency measurement\\nturning\\nbiomechanics\\nblood vessels\\ncardiology\\nhaemodynamics\\npatient diagnosis\\ncomplex 3d trajectory\\nblood-mimicking solution\\nmillimeter-scale magnetic helical swimmer navigating\\ncardiac structures\\nblood flow\\nrespiratory motions\\npulmonary embolus\\nrotational movement\\nmagnetic swimmers\\nhelical magnetic swimmer\\nagile 3d-navigation\",\"713\":\"task analysis\\nmanifolds\\nadaptation models\\nservice robots\\ngrasping\\ndata models\\ngeometry\\nhumanoid robots\\nindustrial robots\\nlearning (artificial intelligence)\\nrobot vision\\ngeometric nullspace\\nrobot skills\\nhuman demonstrations\\nfit geometric nullspaces\\ngeometric constraints\\npowerful mathematical model\\ngeometric skill description\\nskill model\\nlearnt skill\\nsimulated industrial robot\\nicub humanoid robot\\ngeometric constraint models\",\"714\":\"robot kinematics\\nlegged locomotion\\ngrasping\\ntask analysis\\nhumanoid robots\\nplanning\\ncompliance control\\ndexterous manipulators\\nposition control\\ndynamical system approach\\nadaptive grasping\\nicub humanoid robot\\nstate-dependent dynamical systems\\nrobots hands\\nintermediate virtual object\\nmotion generators\\nobject moves\\nwhole-body compliant control strategy\\nmanipulation tasks\\nbody manipulation\\nicub robots walk-to-grasp objects\",\"715\":\"symmetric matrices\\neigenvalues and eigenfunctions\\nprotocols\\nmatrix decomposition\\nlaplace equations\\ntwo dimensional displays\\nrobots\\ndistributed algorithms\\ndistributed control\\nmobile robots\\nmulti-robot systems\\nsubspace projectors\\nstate-constrained multirobot consensus\\nsubspace projection methods\\nconsensus value\\nconstrained 2d rendezvous\\nsingle-integrator robots\\ndiscrete-time agreement protocol\",\"716\":\"task analysis\\nautomata\\ncost function\\nplanning\\nresource management\\nswitches\\ndiscrete systems\\nentropy\\ngraph theory\\nmulti-agent systems\\nmulti-robot systems\\noptimisation\\nsearch problems\\nstochastic programming\\ntemporal logic\\ntask specification\\ndiscrete transition systems\\nfinite linear temporal logic specifications\\nstochastic optimization\\ngraph based search\\ncross entropy temporal logic optimization\\nmultiagent task allocation cross entropy algorithm\\nrobot team\",\"717\":\"task analysis\\nresource management\\ncost function\\nmobile robots\\nreal-time systems\\nminimization\\nadaptive systems\\nmulti-robot systems\\nadaptive task allocation\\ntask execution\\nrobot capabilities\\nheterogeneous multirobot teams\",\"718\":\"task analysis\\nad hoc networks\\nrouting\\nwireless networks\\nhardware\\nprobabilistic logic\\nmobile ad hoc networks\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\noptimisation\\ntelecommunication network routing\\nmobile relay nodes\\nnetwork team\\nwireless connectivity\\ntask agents\\nmobile wireless network infrastructure\\nmultirobot teams\\nprevious multiagent systems\\ncommunication infrastructure\\nend-to-end communication requirements\\ntask team\\narbitrary objective\\njoint optimization framework\\noptimal network routes\",\"719\":\"robot sensing systems\\nmonitoring\\nmutual information\\nspatiotemporal phenomena\\nkernel\\ncombinatorial mathematics\\ngaussian processes\\ngreedy algorithms\\nmatrix algebra\\nmonte carlo methods\\nmulti-robot systems\\noptimisation\\nmultirobot team\\nintermittent deployment problem\\nheterogeneous robots\\nenvironmental process\\nspatiotemporal process\\nintermittent deployment strategy\\nspatiotemporal gaussian process\\nmonte carlo simulations\\ngreedy algorithm\\nsubmodular optimization\\nmatroids\",\"720\":\"density functional theory\\nestimation\\ngaussian processes\\nrobot sensing systems\\nprediction algorithms\\nbayes methods\\ncomputational geometry\\nmobile robots\\nmulti-robot systems\\noptimisation\\nsampling methods\\nmultirobot coordination\\nunknown spatial fields\\nmultirobot coverage\\ninitially unknown spatial scalar field\\nbayesian optimization\\ncontrol law\\ncentroidal voronoi tessellation\\nadaptive sequential sampling method\\nsurrogate function\\ndensity function\",\"721\":\"task analysis\\nsoft robotics\\naerospace electronics\\nforce\\nrobotic assembly\\nlearning (artificial intelligence)\\nforce control\\nforce sensors\\nindustrial manipulators\\nmobile robots\\ntorque control\\nhigh frequency force-torque sensors\\nphysical softness\\ndata-driven approaches\\nhard robots\\nlearning robotic assembly tasks\\npeg-in-hole tasks\\nmodel-based reinforcement learning method\\nlower dimensional systems\\nenvironmental constraints\\nsoft robot\\nhigh frequency force-torque controllers\",\"722\":\"graphics processing units\\nrobots\\nlibraries\\nsprings\\ncomputational modeling\\nkernel\\nacceleration\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\noptimisation\\nparallel algorithms\\nparallel architectures\\ncuda-based c++ robotics simulation library\\nmultiagent robots\\nmassively parallel integration scheme\\nreinforcement learning iterations\\nrapid topology optimization\\nsimultaneous optimization\\ninnovative gpu architecture design\\nrobotics primitives\\ngpu-accelerated simulations\\nasynchronous computing model\\ngpu-accelerated interface\\ninteracting bodies\\nmultiagent robotics\\nintrinsically serial tasks\\nlow-dimensional tasks\\nrobotics simulation libraries\\nnvidia cuda\\nsoft-body robotics\\nparallel asynchronous library\\ntitan\",\"723\":\"data models\\nplanning\\npredictive models\\nadaptation models\\ntrajectory\\ntraining\\nuncertainty\\ndexterous manipulators\\noptimal control\\npath planning\\ncompetency-aware transition models\\nunderactuated adaptive hands\\nin-hand manipulation\\ndata-driven models\\nasymptotically optimal motion planner\\nmotion planning\\ngrasping tasks\\ndexterity\",\"724\":\"task analysis\\nplanning\\nrobots\\ntesting\\nfeature extraction\\ntrajectory\\nstandards\\ncollision avoidance\\ndecision making\\ndexterous manipulators\\ngrippers\\nlearning (artificial intelligence)\\nplanning (artificial intelligence)\\nrobot programming\\nrobot vision\\ntrajectory control\\nvirtual reality\\ndecision classifiers\\ncluttered environments\\nrobot planners\\nrandom sampling\\nobject manipulation plans\\ntrajectory optimisation\\nphysics based robot simulation\\nhuman-like planning\\ndepth camera\\nrobotiq two finger gripper\",\"725\":\"planning\\nmanipulators\\ncollision avoidance\\ntask analysis\\nclutter\\nkinematics\\ncomputational complexity\\ngraph theory\\nindustrial robots\\nmobile robots\\noptimisation\\nrobot vision\\nwarehousing\\nobject rearrangement\\ncluttered confined environments\\nrobotic manipulation\\ncluttered confined space\\nmotion planning\\nmanipulator\\nnonmonotone arrangement problems\\npick-and-place actions\\nbaseline methods\",\"726\":\"uncertainty\\nrobot sensing systems\\nphysics\\nshape\\nbuildings\\nbuilding materials\\ndexterous manipulators\\nfriction\\ngrippers\\nindustrial manipulators\\nmaterials handling equipment\\nmechanical contact\\npath planning\\nsensors\\nunseen objects\\nadaptive ramp building algorithms\\nirregularly shaped stones\\ncontact geometry\\nhigh-level algorithm\\nphysics-based planner\\npickup\\nrobotic system\\ncomplex grasp planning\\nautonomous modification\\nmanipulation\\nfound material\\nmotion support structures\\nspecialized construction algorithm\\nunstructured environments\\nphysics simulation\\nautonomous construction\\nrobotics\\nirregular building materials.\",\"727\":\"laser radar\\ntask analysis\\ntraining\\nfeature extraction\\nestimation\\nconvolution\\ncorrelation\\nimage matching\\nimage resolution\\noptical radar\\nstereo image processing\\ndense depth maps\\ndepth information\\nlight detection and ranging\\naccurate depth map\\nstereo imagery\\nstereo systems\\nhigh-quality dense depth maps\\nstereo matching algorithms\\nsparse depth map\\nhigh-resolution lidar\",\"728\":\"cameras\\nuncertainty\\nestimation\\nmathematical model\\nmotion estimation\\nintegrated circuits\\nrobots\\ncalibration\\ndistance measurement\\ngradient methods\\nimage filtering\\nimage texture\\ninertial systems\\ninterpolation\\nvectors\\nlow-textured environments\\nfully dense direct filtering approach\\nvisual texture\\ndirect photometric approaches\\nimage information\\ninformation propagation\\ncomplexity reduction approach\\nstate vector\\nmonocular visual-inertial odometry approaches\\nhigher order covariance propagation\\nstate handling improvement\",\"729\":\"impedance\\nadmittance\\nrobots\\nforce\\nkinematics\\nstability criteria\\nbond graphs\\nhaptic interfaces\\nhilbert spaces\\nhuman-robot interaction\\ninput-output stability\\nrobotic assembly\\nmechanical impedance\\nlinear spatial impedance representation\\nbond graph theory\\nideal model\\nidealized interaction\\nport functions\\nideal interaction model\\ncollaborative interactions\\ncompetitive interactions\\ninteraction models\\npassivity indices\\ninteraction stability analysis\\ninput-output viewpoints\\nrobot applications\\nhaptic devices\\nparts assembly\\ninteraction behaviours\\ninteraction dynamics\\ndalembert principle\\nnonsmooth mechanics\\nkinematic constraints\\nbond graph methodology\\nhilbert function space\\ncontinuous function\\ninput-output stability condition\",\"730\":\"antennas\\nestimation\\nvibrations\\nrobot sensing systems\\nantenna measurements\\ndelays\\nfiltering theory\\nmulti-robot systems\\nsignal processing\\nvariable structure systems\\ncontact instant detection\\nantenna devices\\nmimic insect antennae\\nmammal whiskers\\nrobotic systems\\nflexible antenna\\nimpact detection\\nimpact instant estimation\\nsuper-twisting algorithm\\ntime 5.0 ms\",\"731\":\"friction\\ngrippers\\nthree-dimensional displays\\nforce\\nsolid modeling\\ncomputational modeling\\nellipsoids\\nmanipulator dynamics\\npath planning\\nquadratic programming\\nrobot vision\\n6d friction cone\\napproximate compliant contacts\\nsoft point contact models\\narea contact model\\n6d friction limit surface\\n6dfc algorithm\\nsoft nonplanar area contact grasp planning\\n3d friction cones\\nabb yumi robot\\nquadratic program\",\"732\":\"predictive models\\nanalytical models\\nuncertainty\\ncomputational modeling\\ntrajectory\\ndynamics\\nstochastic processes\\ncomputer simulation\\nlearning (artificial intelligence)\\nmobile robots\\nself-supervised approach\\nrigid-body simulators\\ncontact models\\npredictive performance\\nhorizon prediction\\nuncertainty propagation\\nresidual point contact learners\\nrobotic tasks\",\"733\":\"vehicle dynamics\\nwheels\\nfriction\\nplanning\\ndynamics\\ntrajectory optimization\\ntires\\nautomobiles\\nmechanical contact\\nmobile robots\\noptimisation\\npath planning\\nrobot dynamics\\ntrajectory control\\ntyres\\ndynamic drift parking\\ndiscontinuous friction model\\ntire dynamics model\\ncost function\\nwheel skidding\\nversatile trajectory optimization framework\\nvehicle motion\\nanisotropic coulomb friction cone\\nmultirigid-body contact problems\\nlinear complementarity problem\\nrobotics community\\ncontact dynamics\\nreal world contact behavior\\nempirical friction model\\naggressive maneuvers\\ncar models\\ndynamic vehicle maneuvers\\nlcp wheel model\\nexecuting dynamic drift parking\\nplanning horizon\",\"734\":\"three-dimensional displays\\nfeature extraction\\nrobot sensing systems\\nclustering algorithms\\nreal-time systems\\ndata mining\\ngeometry\\nimage colour analysis\\npixelwise plane extraction\\nhighly parallelizable plane extraction\\norganized point clouds\\nspherical convex hull\\nregion growing algorithm\\nexplicit plane parameterization\\ngeometric constraints\\ngpu\\nrgb-d camera\",\"735\":\"cameras\\nsimultaneous localization and mapping\\nsemantics\\ntrajectory\\nlayout\\nrobustness\\nestimation\\ngeometry\\npose estimation\\nrobot vision\\nslam (robots)\\nview-invariant loop closure\\noriented semantic landmarks\\nmonocular semantic slam system\\nobject identity\\ninter-object geometry\\nview-invariant loop detection\\norb-slam\\nlocal appearance-based features\\nindoor scenes\\nobject orientation estimation\\ngeometrical detailed semantic maps\\nobject translation\\nobject scale\",\"736\":\"actuators\\nrobot sensing systems\\nacoustics\\nmicrophones\\nacoustic measurements\\nelastic constants\\nmanipulator dynamics\\npneumatic actuators\\nsoft pneumatic actuators\\nactive acoustic sensor\\ncontact sensors\\nsoft actuator\\nembedded speaker\\npneuflex actuator\\nactive sensors\\nactive acoustic contact sensing\\npanda robot arm\\nembedded microphone\",\"737\":\"conferences\\nautomation\\nactuators\\nbending\\ndata gloves\\ngrippers\\nmotion control\\npneumatic actuators\\nsensors\\nsolid modelling\\nsoft pneumatic actuator\\ngraphite-based flex sensor\\nversatile grasping\\n3d-printing approach\\nfabrication complexity\\nactuator dimensions\\nbidirectional actuators\\ngripper system\\nfunctional grasping tasks\\ndefault grasping width\\nbidirectional bending characteristic\\nbidirectional 3d-printed soft pneumatic actuator\",\"738\":\"activity recognition\\nreal-time systems\\noptimization\\nobject recognition\\nfeature extraction\\nrobot sensing systems\\nhuman-robot interaction\\nimage motion analysis\\nimage recognition\\nimage representation\\nlearning (artificial intelligence)\\noptimisation\\npose estimation\\nregression analysis\\nhuman activity categories\\nreal-time human activity recognition\\nhuman pose\\nobject cues\\nreal-world human-centered robotics applications\\nassisted living\\nhuman-robot collaboration\\nfrequency 104.0 hz\",\"739\":\"task analysis\\nrobot sensing systems\\npipelines\\nface detection\\nrecurrent neural networks\\nspeech recognition\\ngesture recognition\\nhuman-robot interaction\\ninteractive systems\\nlearning (artificial intelligence)\\nrecurrent neural nets\\nservice robots\\nhospital receptionist robot\\ntask-oriented dialogue system\\npipeline\\ndialogue states\\nend-to-end learning\\nsocial robot system\\nend-to-end dialogue system\\nrnn based gesture selector\\ndialogue efficiency\\ngestures\\nextended hybrid code network\",\"740\":\"educational robots\\nmediation\\nsensitivity\\nrobot sensing systems\\natmospheric measurements\\nparticle measurements\\nhuman-robot interaction\\nrobot mediation\\nsocially assistive robots\\ngroup dynamics\\nsocial settings\\ntrust dynamics\\nrobot mediated support group\\ndyadic trust scale\\ngeneral trust\\naverage interpersonal trust\\ngroup interaction session\\nmultiparty setting\",\"741\":\"shape\\nkinematics\\npotential energy\\nhip\\ntorso\\nlegged locomotion\\nbiomechanics\\nbone\\northopaedics\\ncpst\\ncoronal plane spine twisting composes shape\\nenergy landscape\\ngrounded reorientation\\nanimal locomotion\\nlegged robots\\nself-righting mechanics\\nfreedom coronal plane representation\\nbody shape affordance\\ncross-sectional geometries\\nkinematic model predictions\\nelliptical bodies\\nrectangular shaped bodies\\nquasistatic reorientation maneuvers\",\"742\":\"snake robots\\nwindings\\nshape\\nrobots\\njunctions\\nmodeling\\npins\\ncollision avoidance\\nmobile robots\\nmotion control\\npath planning\\npipes\\nmotion design\\nsnake robot\\nconstant diameter\\nmultiple pipe structures\\ntarget form\\nrolling motion\\ncomplicated pipe structures\",\"743\":\"robots\\nhydrocarbons\\nsoil\\nreservoirs\\nactuators\\noils\\nasphalt\\ncams (mechanical)\\ndata acquisition\\ndesign engineering\\ngeology\\nhydrocarbon reservoirs\\nmobile robots\\noil reservoirs\\nautonomous robots\\ntool transportation\\npetroleum reservoirs\\ncam-follower configuration worm robot\\nperistaltic displacement\\nsingle actuator peristaltic robot\\nsubsurface exploration\\ndevice emplacement\\ninitial testing\\nsingle actuator peristaltic motion robot\\nsubsurface geological exploration\\ndesign\\nnonconsolidated media\",\"744\":\"solid modeling\\nmanipulator dynamics\\nmathematical model\\nservice robots\\nfinite element analysis\\nflexible manipulators\\nrobotic manipulators\\nindustrial robots\\nmechanical stiffness\\nmultibody models\\nfinite element model\\nflexible link manipulator model\\nindustrial robot\\nstiffness parameters\\nrobot behavior\\nweight-reduced manipulator\",\"745\":\"trajectory\\nrobots\\nartificial neural networks\\nheuristic algorithms\\nmathematical model\\ntorque measurement\\nlearning (artificial intelligence)\\nlearning systems\\noptimal control\\nrobot dynamics\\nrobust control\\ntrajectory control\\nlearned dynamics models\\nmodel based reinforcement learning\\nrobust current dynamics learning\\nreal robot dataset\\ntransferability assessment\\n3 degrees of freedom robot trajectories\\nrobotic learning\",\"746\":\"mathematical model\\nmagnetic cores\\nmulti-agent systems\\nforce\\ntraining\\nsprings\\noscillators\\ndifferential equations\\nlearning (artificial intelligence)\\nneural nets\\nsynchronisation\\nagents change\\npoint-mass system\\nkuramoto phase synchronization dynamics\\npredator-swarm interaction dynamics\\nmultiagent interaction dynamics\\nneural network-based multiagent interaction model\\ngoverning dynamics\\ncomplex multiagent system\\nnonlinear network\\ngeneric ordinary differential equation based state evolution\\nneural network-based realization\\ntime-discretized model\\ncore dynamics\\nagent-specific parameters\\nmagnet\\ntraditional deep learning models\",\"747\":\"powders\\ncleaning\\nservice robots\\nthree-dimensional displays\\ntask analysis\\nforce control\\nindustrial robots\\nlearning (artificial intelligence)\\nneural nets\\npath planning\\nproduction engineering computing\\nthree-dimensional printing\\nrobotic decaking\\nautomated decaking\\n3d printed parts\\n3d printing based mass manufacturing\\nsmart mechanical design\\nmotion planning\\ndeep learning\\nmanipulation\\nsystem design\\n3d-printing\\ndecaking\",\"748\":\"solar panels\\nbrakes\\noceans\\ndc motors\\nsun\\nsolar energy\\nattitude control\\npower generation control\\nsolar cell arrays\\nsolar power stations\\nsolar tracker\\nocean environment\\nelectromagnetic brakes\\ndynamic model\\nangular acceleration\\ncontrol algorithm\\nreal water surface\\ntime 28.0 s\",\"749\":\"manipulators\\nblades\\nactuators\\nhydraulic systems\\ntorque\\nwires\\nemergency management\\nhydraulic actuators\\nmotion control\\nrescue robots\\nhydraulic-cable driven actuation modules\\n3dof manipulator\\nhydraulic actuation system\\ndisaster response mobile-manipulation\\ndisaster response operation\\nhydraulic-cable driven manipulator\",\"750\":\"mechatronics\\ninstruments\\nmusic\\nactuators\\nrobots\\nprototypes\\nsolenoids\\nhearing\\nmusical instruments\\ntechnical exploration\\ntimbral exploration\\nmechatronic chordophones\\nmusical robotics\\nstand-alone instruments\\nsound art installations\\nexpressive potential\\nplucked strings\\nexpressive mechatronic mono-chord\\npolystring chordophone\\nexpressive mechatronic chordophone\\nsound generation model\",\"751\":\"wheels\\nmobile robots\\ngears\\nfriction\\nmathematical model\\nenergy loss\\nmechanical stability\\nmotion control\\nnonlinear control systems\\npendulums\\nrobot dynamics\\nhuman environments\\nideal locomotion mechanism\\nomnidirectional balancing unicycle robot\\nmobility mechanism\\nomburo\\nagile mobility\\ncompact structure\\nactive omnidirectional wheel\",\"752\":\"tiles\\nrobot sensing systems\\nlattices\\nshape\\nautonomous robots\\nmobile robots\\npath planning\\nrobot vision\\nflexibility\\ncellular components\\nrobust robots\\ncellular building materials\\narbitrary cellular structures\\ncellular materials\\nlattice-based cellular structures\",\"753\":\"topology\\nrobots\\nplanning\\ngeometry\\nshape\\nkinematics\\nactuators\\ncollision avoidance\\nmobile robots\\npath planning\\nfast configuration space algorithm\\nvtt\\nself-reconfigurable robot\\ntruss shape\\nmotion planning\\nshape changing actions\\ntopology reconfiguration\\ngeometry reconfiguration actions\\ncell decomposition approach\\ncollision-free space\\nsimple shape-morphing method\\nvariable topology truss modular robot\",\"754\":\"automation\\npropellers\\nattitude control\\nconferences\\nrobots\\nactuators\\naerospace robotics\\nautonomous aerial vehicles\\ndrag\\nhelicopters\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nvehicle dynamics\\nmodquad-dof\\nmodular quadrotors\\nrobotic structure\\nenhanced capabilities\\nmodule design\\nfreedom relative motion\\nflying robot\\ncage\\ndocking mechanism\\nstructure control authority\\nstructure yaw control\\nyaw actuation method\",\"755\":\"fault tolerance\\nfault tolerant systems\\nrobots\\nthree-dimensional displays\\ncircuit faults\\nshape\\nplanning\\nactuators\\nmotion control\\nfault tolerant reconfiguration\\nself-folding robots\\nmodular system\\ncomplete actuation failure\\nactive modules\\nimprecise robotic motion\\nreconfiguration failure\\nintra-module connection\\nreconfiguration schemes\\nuser-specified fault tolerant capability\\narbitrary input initial pattern\\nrobotic platform\\nmodular origami robot\\nfault tolerant initial patterns\\nactuation fault tolerance approach\\nreconfiguration planning\\nmodular self-folding\",\"756\":\"navigation\\nhardware\\nrobot kinematics\\nshape\\ncameras\\nrobot vision systems\\ncomputational complexity\\nevolutionary computation\\nmotion control\\nrobots\\nstudied cubic modules\\nconvex motion primitives\\nrotating motion primitives\\nheterogeneous reconfiguration algorithm\\nparallel heterogeneous permutation method\\nfull-resolution reconfiguration algorithm\\nheterogeneous operations\\nspace saving\\nmodule hardware\\nsliding-only motion primitive\\ncubic modular robot\\ncubic module\\nsliding-only cubic modular robots\\nparallel permutation algorithm\\nheterogeneous sliding-only cubic modular\\nlinear full-resolution reconfiguration\\nlinear operating-time cost\\nsliding-only cubic modules\\nrobot structure\\nlinear operating time cost\",\"757\":\"cameras\\nc++ languages\\nlibraries\\nrobot vision systems\\ncalibration\\nimage edge detection\\nedge detection\\nfeature extraction\\nimage colour analysis\\nimage matching\\nlocation based services\\nobject detection\\nrobot vision\\nsoftware libraries\\nlocalization accuracy\\napriltag detection\\nfiducial markers\\nfreely available libraries\\napriltag 3\\naruco\\nopencv algorithm\\napriltags c++\\nrobotics\\nedge refinement\\ngrayscale camera image\\ntemplate matching\",\"758\":\"optimization\\nmeasurement\\nmanifolds\\nmathematical model\\nrobots\\ncovariance matrices\\ngaussian distribution\\ndecision making\\noptimisation\\nlinear least norm optimization problem\\nlinear least distance optimization problems\\nnonlinear least distance optimization\\noptimal value\\nminimum norm optimization\",\"759\":\"simultaneous localization and mapping\\nnavigation\\ncameras\\nperformance evaluation\\nrobot localization\\nimage motion analysis\\nmobile robots\\npath planning\\npose estimation\\nrobot vision\\nslam (robots)\\nresearch issue\\nmobile robotics\\nperformance assessment\\nrobot slam algorithms\\nlocalization accuracy\\nslam algorithm\\nbenchmark datasets\\nmotion capture\\nenvironment-specific\\nspatial coverage\\nslam performance evaluation\\ndistinctive markers\\nrobot navigation environment\\ngenerative latent optimization problem\\nlocal robot-to-marker\\nglobal robot\",\"760\":\"marine vehicles\\nsafety\\ngeometry\\ndecision making\\nnavigation\\nrisk management\\nplanning\\ncollision avoidance\\nmarine safety\\nmobile robots\\nremotely operated vehicles\\nships\\nperformance metrics\\nasv decision-making\\ncollision risk\\nasv planning strategies\\ninternational regulations for prevention of collisions at sea\\nquantified good seamanship\\ncolregs compliance\\nvessel interactions\\nautonomous surface vehicle decision-making\\nautonomous surface vessel performance evaluation\\nseamanship performance criteria\",\"761\":\"predictive models\\nmeasurement\\nvisualization\\nstochastic processes\\nplanning\\nrobot sensing systems\\nbayes methods\\nimage sequences\\nmobile robots\\npath planning\\nrobot vision\\nvideo coding\\nvideo signal processing\\naction-conditioned benchmarking\\nrobotic video prediction models\\nintelligent systems\\nvideo prediction systems\\nrobot actions\\nvideo prediction models\\nframe quality\\nrobot performs\\naction inference system\\nrobot planning systems\",\"762\":\"lyapunov methods\\ngrasping\\nfeature extraction\\ncomputer architecture\\ntask analysis\\npose estimation\\nvelocity control\\ncameras\\nclosed loop systems\\nconvolutional neural nets\\nimage colour analysis\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nneurocontrollers\\nrobot vision\\nvisual servoing\\ngtx 1080ti gpu\\ngrasping mugs\\nmultiinstance control\\nreal-time closed loop\\nlyrn\\nsingle shot rgb 6d pose estimation\\ncomplex multiinstance task\\nreaching action\\ncontrol lyapunov function\\nlearning principles\\nvisually guided reaching\\nlyapunov reaching network\\npose-based-visual-servo grasping system\\nclosed-loop control\\nover-the-shoulder monocular rgb camera\\nmultiinstance capability\\nvisual control\\ndeep convolution neural network\\nmanipulator joint angles\\nmonocular vision\\nreaching points\\nfrequency 85.0 hz\",\"763\":\"cameras\\ntask analysis\\nrobot sensing systems\\nsearch problems\\nclutter\\ndetectors\\nimage colour analysis\\nlearning (artificial intelligence)\\nmanipulators\\nobject recognition\\nrobot vision\\nobject finding\\ncases physical interaction\\ntarget object\\ncomplex environment\\nobject search\\ncluttered scenes interactions\\nreinforcement learning based active perception system\\nreinforcement learning based interactive perception system\\nrobotic manipulator\\nrgb\\ndepth camera\",\"764\":\"feature extraction\\ngrasping\\ntraining\\ntask analysis\\nrobots\\ndata mining\\ncorrelation\\ndexterous manipulators\\ngrippers\\nlearning (artificial intelligence)\\nrobot vision\\nquery image\\nsoft constraints\\nworkspace image\\ngrasp configuration\\nccan\\ninstance grasping\\nlearning-based method\\nconstraint co-attention module\\nconstraint co-attention network\\nrobotic grasping task\\nend-to-end instance grasping method\\ngrasp affordance predictor\",\"765\":\"three-dimensional displays\\nfeature extraction\\nproposals\\nobject detection\\nprediction algorithms\\ncorrelation\\nagriculture\\ndata analysis\\nimage motion analysis\\ninterpolation\\nlearning (artificial intelligence)\\nneural nets\\nobject tracking\\ndata streaming\\ntemporal information\\n3d streaming based object detection\\nnonkey frames\\nmotion based interpolation algorithm\\nframe-by-frame paradigm\\nkitti object tracking benchmark\\ndeep learning\",\"766\":\"three-dimensional displays\\nobject detection\\ntwo dimensional displays\\nlaser radar\\ndetectors\\ncameras\\nshape\\nimage matching\\noptical radar\\npose estimation\\nstereo image processing\\nlidar-based 3d object detector\\nobject point clouds\\nobject-centric stereo matching method\\n3d object detection\\nstereo cameras\\nlidar sensor\\nstereo 3d object detection\\npsmnet stereo matching network\\n6 dof pose\\n2d box association\",\"767\":\"robots\\nmeasurement\\ntask analysis\\nreliability\\ntools\\nlogistics\\nworkstations\\nimage classification\\nindustrial robots\\nmaterials handling equipment\\nrobot vision\\nwarehousing\\nrelative confusion matrix\\nmixed-product bin\\nrobot workstation\\nmanual picking station\\nbin picking robot\\nlogistics installations\\nwarehouse\\nimage dataset\",\"768\":\"feature extraction\\npose estimation\\nthree-dimensional displays\\ntraining\\nrendering (computer graphics)\\nimage reconstruction\\ndecoding\\ncomputer vision\\nimage colour analysis\\nneural nets\\nregression analysis\\npose estimation performance\\npose-irrelevant factors\\nencoding process\\nsuitable pose representation\\nregression approaches\\nsingle rgb image\\n6-dof object pose regression\\nfeature-based refinement\\npose-guided auto-encoder\\ndirect regression-based approaches\\npae\\nfeature-based pose refiner\",\"769\":\"three-dimensional displays\\nfeature extraction\\ninterpolation\\ntwo dimensional displays\\ntransforms\\ncomputational modeling\\nshape\\ncomputational geometry\\nhough transforms\\nobject detection\\ndata abstraction\\ngeometric primitives\\ncompact representation\\nprimitect\\nfast continuous hough voting\\nprimitive detection\\n3d point sets\\nsemiglobal hough voting scheme\\nrobotics applications\\nlocal low-dimensional parameterization\",\"770\":\"semantics\\nconvolution\\nimage segmentation\\nfeature extraction\\nreal-time systems\\nspatial resolution\\nconvolutional neural nets\\nimage resolution\\nimage sampling\\nobject detection\\nrobot vision\\nfeature space superresolution\\nfarsee-net\\nreal time semantic segmentation\\ncascaded factorized atrous spatial pyramid pooling\\nfeature maps\\nconvolutional neural networks\\nmultiscale context aggregation\\nobject scale variations\\nrobotic applications\\nsubsampled image\",\"771\":\"feature extraction\\nthree-dimensional displays\\nroads\\nhidden markov models\\nvehicles\\ntwo dimensional displays\\nconvolution\\ncomputer vision\\nconvolutional neural nets\\ndriver information systems\\ninteractive systems\\nlearning (artificial intelligence)\\nroad traffic\\nroad vehicles\\ngraph convolutional networks\\nintelligent automated driving systems\\ncomplicated driving situations\\nspatial-temporal interaction framework\\ngraph convolution networks\\ngcn\\ninteraction modeling\\nego-stuff interaction\\nhonda research institute driving dataset\\n3d-aware egocentric spatial-temporal interaction learning\\ntactical driver behavior annotations\\nego-thing interactions\",\"772\":\"skeleton\\nrobot motion\\nrobot kinematics\\nkinematics\\nzirconium\\ntorso\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nmulti-robot systems\\nhuman-robot motion retargeting\\nkinematic configurations\\nkinematic independent general solution\\nthree-phase optimization method\\ndeep reinforcement learning\\nmotion retargeting learning\\nmotion retargeting policy\\nmotion retargeting skill\\nhuman skeleton\\ncyclic-three-phase optimization\\nnao robot\\npepper robot\\nbaxter robot\\nc-3po robot\",\"773\":\"task analysis\\ninstruments\\ndecoding\\nreal-time systems\\ncomputational modeling\\noptimization\\nsurgery\\nendoscopes\\nimage segmentation\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nrobot vision\\nattention pruned multitask learning model\\nreal-time instrument detection\\nrobot-assisted surgery\\nimage-guided robotic surgery\\nreal-time robotic system\\nweight-shared encoder\\ntask-aware detection\\nasynchronous task-aware optimization\\nrobotic instrument segmentation dataset\\nend-to-end trainable realtime multitask learning model\\nglobal attention dynamic pruning\\nskip squeeze and excitation module\",\"774\":\"surgery\\ngesture recognition\\nrobots\\nhidden markov models\\nlearning (artificial intelligence)\\nfeature extraction\\ntask analysis\\nimage classification\\nimage segmentation\\nmedical robotics\\nneural nets\\ntree searching\\nvideo signal processing\\nvideo surveillance\\njoint surgical gesture segmentation\\ntree search algorithm\\nneural networks design\\nreinforcement learning framework\\nsurgical robotic applications\\nsurgical video classification\\nbaseline methods\\njigsaws dataset\\nsurgery surveillance\\nautomatic surgical gesture recognition\\nrobot-assisted surgery\\nsurgical gesture recognition\\ndeep reinforcement learning in robotics\\ntree search\\nrobotic surgery\",\"775\":\"image segmentation\\nconvolution\\nbiomedical imaging\\nimage resolution\\nconvolutional neural networks\\nthree-dimensional displays\\nrobots\\nbiomedical mri\\ncomputerised tomography\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nneural nets\\nsurgery\\nmedical image segmentation\\nrobot-assisted minimally invasive surgeries\\ncurrent dcnns\\nsampling layer\\nreceptive field\\nspatial dimension\\nfeature maps\\natrous convolutional layers\\nacnn\\nmagnetic resonance imaging\\ncomputed tomography image segmentation\\nsegmentation intersection\\natrous convolutional neural network\\nfull-resolution dcnn\\nu-net\\ndeeplabv3+\",\"776\":\"planning\\nrobustness\\nprivacy\\nautomata\\nmodel checking\\ntask analysis\\nformal specification\\npath planning\\nrobots\\ntemporal logic\\nhyperproperties\\nformal methods\\ntemporal logic objectives\\nhyper-temporal logics\\nmultiple paths\\nhyperltl specifications\\nplanning strategies\\nrobotic planning\\ndiscrete transition systems\",\"777\":\"robot sensing systems\\nplanning\\nsensor phenomena and characterization\\nuncertainty\\ncollision avoidance\\ncomputational complexity\\ncontrol engineering computing\\ndata structures\\ngraph theory\\nplanning (artificial intelligence)\\nrobots\\nsearch problems\\nsensors\\nrobotic sensors\\nplanning problem\\nsearch algorithms\\nsensor designs\\ndesign trade-offs\\nsensor maps\\npotential sensors\\nouter limits\\nsearch space\\nsingle special representative\\ntask domain knowledge\\nsensor technology\\nparticular problem instances\\nsensor characterization pairs\\nyielding solutions\",\"778\":\"automata\\nheuristic algorithms\\ntrajectory\\ntask analysis\\nrobot kinematics\\ncollision avoidance\\ngraph theory\\nmobile robots\\nnavigation\\nsearch problems\\ntemporal logic\\ntrajectory control\\nobstacle avoidance\\nheuristic search based path planning algorithm\\ntemporal logic path planning\\ngraph search problem\\npoint-to-point navigation\\ntemporal logic specifications\\ntemporal logic query\\noptimal trajectory\\ndijkstra's shortest path algorithm\",\"779\":\"trajectory\\ntools\\nrobots\\ncollision avoidance\\ntask analysis\\nsurgery\\nsplines (mathematics)\\nmedical robotics\\nmobile robots\\nmotion control\\nrobotic minimally invasive surgery\\ngeometric constraints\\ndesired task\\nfinal target\\nmoving obstacles\\ndeveloped motion planner\\ntwo-layer architecture\\nglobal level computes smooth spline-based trajectories\\ncollision free connections\\nrealistic surgical scenario\\nautonomous surgical\\ncollision-free trajectories\\nautonomous execution\\nassistive tasks\\ndynamical systems based obstacle avoidance\",\"780\":\"robot sensing systems\\nlabeling\\nuncertainty\\nsemantics\\nprobabilistic logic\\nmarkov processes\\nmobile robots\\nmotion control\\nneurocontrollers\\npath planning\\nprobability\\ntemporal logic\\nnoisy semantic observations\\nlinear temporal logic specifications\\nrobot sensing error\\nprobabilistic labels\\nlabeled transition system\\nrobot mobility\\nlabeled markov decision process\\nunknown transition probabilities\\nproduct-based model checkers\\nprobabilistic labeling functions\\nq-learning agent\\ndeep imitative reinforcement learning\\ntemporal logic robot motion planning\\ndeep imitative q-learning method\\ndiql\\ncontrol policies synthesis\\nltl\\nlmdp\\nsuboptimal instructions\",\"781\":\"three-dimensional displays\\ntwo dimensional displays\\nturning\\npath planning\\natmospheric modeling\\nspace vehicles\\noptimization\\noptimisation\\nvehicles\\nminimal 3d dubins path\\nbounded curvature\\npitch angle\\ncost-efficient three-dimensional paths\\ntwo-dimensional dubins curves\\nclosed-form solutions\\nlocal optimization\\ncost-efficient solution\\nlower bound estimation\\noptimal path\\nvehicle\",\"782\":\"object detection\\nvideos\\nvisualization\\ncameras\\ndetectors\\nsurveillance\\nautonomous aerial vehicles\\ncomputer vision\\nimage annotation\\nimage capture\\nimage colour analysis\\nvideo signal processing\\nvideo surveillance\\nmultimodal unmanned aerial vehicle dataset\\nlow altitude traffic surveillance\\nuavs\\nmounted cameras\\naerial image capture\\naerial visual data\\nobject detection algorithms\\ncomputer vision community\\nobject annotations\\nflying-cameras\\nmultipurpose aerial dataset\\nmultimodal sensor data\\nau-air dataset\\nmeta-data\\ntraffic-related object category\\nmobile object detectors\\nreal-time object detection\\nrobotics\\nreal-world outdoor environments\\nbounding box annotation\\nrgb videos recording\\nyolov3-tiny\\nmobilenetv2-ssdlite\\non-board computers\\nbird-view image\\ndata types recording\",\"783\":\"squids\\nelectron tubes\\naerodynamics\\ndrones\\nprototypes\\nthermal stability\\naircraft\\nautonomous aerial vehicles\\nballistics\\nhelicopters\\nmobile robots\\nrobot vision\\nemergency response\\nspace exploration\\ncritical situational data\\nonboard sensors\\nmultirotor prototype\\nonboard sensor suite\\nautonomy pipeline\\naerodynamic stability\\nactive stabilization\\nballistic launch\\nstreamlined quick unfolding investigation drone\\nvision-based autonomous transition\\nsquid\",\"784\":\"cameras\\nrobot vision systems\\ntracking\\nsurveillance\\nrobustness\\nautonomous aerial vehicles\\nimage sensors\\nobject tracking\\npattern clustering\\nrobot vision\\nfeature tracking\\nintrusion monitoring\\nuas\\nunmanned aerial systems\\nperception systems\\nillumination conditions\\nevent cameras\\nneuromorphic sensors\\nillumination changes\\nevent based vision\\nevent stream\\nintruder monitoring\\nevent clustering\\nevent-by-event processing\\nasynchronous event-based clustering\\nautomatic surveillance\\non-board hardware computational constraints\\nevent camera\\nasynchronous\\nclustering\",\"785\":\"cameras\\noptical imaging\\noptical sensors\\nadaptive optics\\noptical filters\\ntracking\\nintegrated optics\\naerospace control\\naircraft control\\nautonomous aerial vehicles\\nimage sensors\\nimage sequences\\nkalman filters\\nmotion estimation\\nnonlinear filters\\npose estimation\\nremotely operated vehicles\\nonboard monocular optical flow estimator\\nuav\\nflight control\\nunmanned aerial vehicles\\nautonomous operations\\nonboard sensors\\nmonocular camera\\nfree rotors\\nflight dynamics\\nfalling samara seed\\nconstantly rotating body frame\\noptical flow sensing\\noptimal images\\nrotation vectors\\noptical axis\\ntranslation vectors\\nflow field\\nshift estimation\\nselective heading image for translation\",\"786\":\"robots\\nmagnetometers\\nsensors\\nestimation\\nfrequency measurement\\nsaturation magnetization\\nangular velocity\\ngyroscopes\\nkalman filters\\nmobile robots\\nnonlinear filters\\noptical radar\\nslam (robots)\\nflydar\\nmagnetometer-based high angular rate estimation\\nslam\\nsimultaneous localisation and mapping\\nflying li-dar\\nekf-based algorithm\\nsinusoidal magnetometer measurement\\ncontinuously rotating airframe\\nimu sensors\\ngyro measurement\\ngyro bias\\ngyro saturation condition\\nrotating locomotion\\nrobot hovering angular velocity\",\"787\":\"actuators\\nresource management\\npropellers\\naerodynamics\\nquaternions\\nangular velocity\\nvehicle dynamics\\nautonomous aerial vehicles\\nhelicopters\\nkalman filters\\nnonlinear control systems\\nnonlinear filters\\npredictive control\\nprecise reference tracking\\ncrucial characteristic\\nmicroaerial vehicles\\nmav\\nexternal disturbances\\ncluttered environments\\nnonlinear model predictive control\\nnmpc\\nfully physics\\nnonlinear dynamics\\ncontrol inputs\\nfeasible actuator commands\\nsafe operation\\npotential loss\\nflight experiments\\nmotor failures\\nnonlinear mpc\\naggressive multicopter flight\\nextended kalman filter based motor failure identification algorithm\",\"788\":\"optical imaging\\nimage segmentation\\nsemantics\\nadaptive optics\\noptical filters\\nbayes methods\\noptical fiber networks\\nbelief networks\\nimage sequences\\nneural nets\\nprobability\\nvideo signal processing\\ntemporal information integration\\nvideo semantic segmentation\\ntemporal bayesian filter\\nvideo sequence\\ndiscrete probabilistic distribution function\\npossible semantic classes\\nbayesian filtering\\nprediction model\\nobservation model\\ndatadriven prediction function\\ndense optical flow\\ndeep neural network\\nobservation function\\nsemantic segmentation network\\ntemporal filtering\\ncityscapes\",\"789\":\"robots\\ntrajectory\\nplanning\\nnavigation\\ncollision avoidance\\nsafety\\ncognition\\nmobile robots\\npath planning\\npredictive control\\ntrajectory control\\nmap prediction\\ndata-driven method\\nautonomous navigation\\nhallway environments\\nna\\u00efve frontier pursuit method\\nheuristic methods\\nmap-predictive motion planning\\ndynamically-constrained robots\\ntrajectory planning\\nrobot position\\nfrontier selection heuristics\",\"790\":\"state estimation\\ngyroscopes\\nsensors\\ninertial navigation\\naccelerometers\\nmeasurement uncertainty\\nclosed loop systems\\nhelicopters\\nkalman filters\\nnonlinear filters\\nposition control\\nclosed-loop control\\nmean absolute position estimation error\\ntotal flight distance\\nstandard inertial navigation method\\ntrajectory tracking error\\nmultiple short hops\\nmulticopter navigation\\ngps systems\\nmulticopter localization\\ndirect integration\\ninertial navigation sensors\\naccelerometer\\nrate gyroscope\\nrapid error accumulation\\nmotion strategy\\ninertial navigation state estimation error\\nlong duration flight\\nmultiple short duration hops\\nzero-velocity pseudomeasurements\\nextended kalman filter\\nlidar\\nreal-world environment\\ndistance 5.0 m\\ndistance 10.0 m\",\"791\":\"robot sensing systems\\nmutual information\\ndistortion measurement\\ngain measurement\\ntime measurement\\ncomputational complexity\\ninformation theory\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\ninformation-theoretic exploration\\ncontinuous occupancy map framework\\n|\\u03b8| measurement beams\\nrecursive structure\\nrobotics applications\\nautonomous navigation task\",\"792\":\"navigation\\ncameras\\nsensors\\nuncertainty\\nimage segmentation\\nfeature extraction\\nrobots\\nautonomous underwater vehicles\\nbathymetry\\nmaximum likelihood estimation\\nmobile robots\\npath planning\\nremotely operated vehicles\\nmultiple perspective prior maps\\npath planning methodology\\nauv\\nshallow complex environments\\ncoral reefs\\naerial photographic survey\\nbathymetric information\\nprior map\\nnavigation graph\\ntest points\\nshortest paths\\ndestination points\\nmaximum likelihood function\\nmisclassified objects\\nphoto-realistic simulated environment\",\"793\":\"cameras\\ncalibration\\nlaser radar\\nthree-dimensional displays\\nestimation\\nmathematical model\\nrobustness\\ncomputer vision\\noptical radar\\nradar imaging\\nsurface topography measurement\\ncomputer vision applications\\nfully automatic extrinsic calibration\\nlidar extrinsic parameters\\nautomatic lidar-camera calibration\\nspherical target\\nlidar-camera imaging system\\npoint clouds\",\"794\":\"computer architecture\\nmicroprocessors\\nvehicle dynamics\\nmeasurement by laser beam\\ntime measurement\\nrecurrent neural networks\\ndynamics\\nimage filtering\\nimage motion analysis\\nmotion estimation\\noptical radar\\npath planning\\nprobability\\nradar imaging\\nrecurrent neural nets\\ntraffic engineering computing\\noccupancy grid maps\\ngrid cell\\noccupancy probability\\nmeasurement grid maps\\noccupancy probabilities\\nfiltered occupancy\\nnetwork architecture\",\"795\":\"three-dimensional displays\\nfrequency modulation\\nrobustness\\nunderwater vehicles\\ntwo dimensional displays\\ncultural differences\\nnonhomogeneous media\\ndivide and conquer methods\\nimage registration\\nlaser ranging\\nhigh-end laser range-finders\\ndivide-and-conquer method\\n3d registration method\\nfourier-mellin-soft\\nfms\\npartial overlaps\\nlarge-scale cultural heritage site\",\"796\":\"iterative closest point algorithm\\nuncertainty\\nbayes methods\\nthree-dimensional displays\\nrobot sensing systems\\nstandards\\nstochastic processes\\nimage fusion\\niterative methods\\nmarkov processes\\nmonte carlo methods\\nmotion estimation\\npose estimation\\nmotion uncertainty\\naccurate uncertainty estimation\\npose transformation\\nautonomous navigation\\ndata fusion\\niterative closest point\\npoint cloud pairs\\ndeterministic algorithm\\nprobabilistic manner\\ndata association errors\\nsensor noise\\noverconfident transformation estimates\\npose uncertainty\\nmarkov chain monte carlo algorithm\\nscalable bayesian sampling\\nstochastic gradient langevin dynamics\\ndata association uncertainty\\n3d kinect data\\nbayesian icp\",\"797\":\"robot sensing systems\\nservice robots\\nsolid modeling\\nplanning\\nlaser radar\\ncollision avoidance\\nimage representation\\nlegged locomotion\\nrobot vision\\nindustrial structure\\nmapping industrial structures\\ninformation gain-based planning\\nquadruped robot\\nonline active mapping system\\nvoxel representation\\nnbv\\nexpected information gain\\nterrain map\\nanybotics anymal robot\",\"798\":\"three-dimensional displays\\nvocabulary\\nimage reconstruction\\nimage retrieval\\nfeature extraction\\nrobustness\\ncameras\\nimage matching\\nimage motion analysis\\niterative methods\\nlarge-scale sfm\\nfeature matches\\nlarge-scale structure-from-motion\\nunordered image collections\\ntraditional feature matching method\\nregion covisibility\\noverlapping image pairs\\niterative matching strategy\\nunordered image datasets\\nrobust sfm\\nefficient image matching method\\ncovisibility-based image matching\",\"799\":\"uncertainty\\nthree-dimensional displays\\nprobabilistic logic\\nbayes methods\\npredictive models\\ncameras\\nmachine learning\\nbelief networks\\nimage colour analysis\\nimage fusion\\nimage reconstruction\\nlearning (artificial intelligence)\\nmonte carlo methods\\nneural nets\\ndepth prediction\\nrobust 3d reconstruction\\nbayesian deep learning framework\\nconventional bayesian deep learning\\nprobabilistic tsdf fusion\\ndense 3d reconstruction\\nglobal tsdf\\nsingle rgb camera\\n3d reconstruction problem\\nsingle rgb image\\ntraining environment\\ntest environment\\nlightweight bayesian neural network\",\"800\":\"lighting\\ntraining\\nmeasurement\\nbenchmark testing\\ntraining data\\nschedules\\ncameras\\ncomputer vision\\nfeature extraction\\nimage matching\\nimage retrieval\\nlearning (artificial intelligence)\\nobject detection\\nillumination-invariant feature network\\nfeature descriptor matching\\ncomputer vision applications\\nimage stitching\\nvisual localization\\nillumination variations\\ndescriptor learning\\nrobust descriptor\\ngeneric descriptor\\ndataset scheduling methods\\nroi loss\\nhard-positive mining strategy\\nillumination change conditions\\nif-net\",\"801\":\"uncertainty\\npredictive models\\nimage reconstruction\\nproposals\\ncomputational modeling\\nimage resolution\\nthree-dimensional displays\\ncameras\\nimage matching\\nlearning (artificial intelligence)\\nstereo image processing\\nmiddlebury dataset\\nnonlearning method\\ninfrastructure inspection\\ndownstream process\\nstereo reconstruction methods\\nsemiglobal block matching method\\n3d reconstruction error\\ninfrastructure inspection experiments\\ncustomized binocular stereo camera\\nhigh-resolution stereo images\\ndeep-learning assisted method\\npredicted disparity\\nperpixel searching range\\ndown-sampled stereo image pair\\ninitial disparity prediction\\ndeep-learning model\\naccurate stereo reconstruction\\nlearning-based model\\nresource demanding nonlearning method\\ntask-specific training data\\ngeneralization issue\\nlearning-based methods\\nhigh-resolution data\\ncomputational resource\\ninfrastructure inspections\\ndense stereo reconstruction\\nassisted high-resolution binocular stereo depth reconstruction\",\"802\":\"cameras\\ntransmission line matrix methods\\nrobot vision systems\\nestimation\\nautomobiles\\ngeometry\\ncalibration\\ncomputer vision\\nimage motion analysis\\nleast squares approximations\\noptimisation\\npolynomials\\nclosed-form solver\\npoint correspondences\\ncamera movement\\nmotion parameters\\nvehicle-mounted cameras\\nleast-squares optimal relative planar motion\\n6th degree polynomial\",\"803\":\"cameras\\ntransmission line matrix methods\\nmathematical model\\nestimation\\ngeometry\\nrobot vision systems\\nrobustness\\ncalibration\\ncomputer vision\\nimage motion analysis\\nimage sensors\\nrelative planar motion\\nvehicle-mounted cameras\\nsingle affine correspondence\\nextrinsic camera parameters\\ngeneral planar motion\\ncamera movement\\nimage plane\\nminimal solver\\nsemicalibrated case\\ncommon focal length\\nfully calibrated case\",\"804\":\"cameras\\nheuristic algorithms\\nobject detection\\nrobustness\\ntrajectory\\nvisual odometry\\nfeature extraction\\ndistance measurement\\nimage colour analysis\\nimage segmentation\\nimage sensors\\nmobile robots\\npose estimation\\nregression analysis\\nrobot vision\\ndynamic environment\\nsimple moving object detection algorithm\\ndense visual odometry\\nvo algorithms\\nocclusion accumulation\\ncolor images\\nrobotic navigation\\nreal-time rgbd data\\ndepth information\\nobstacle recognition\\ncamera pose estimate\\nbi-square regression weight\\nsegmentation accuracy\\npublic datasets\",\"805\":\"convex functions\\noptimization\\nmanifolds\\nxenon\\napproximation algorithms\\nclustering algorithms\\nnoise measurement\\napproximation theory\\ncomputational complexity\\nconcave programming\\nimage matching\\nmatrix algebra\\nsensor fusion\\nstochastic processes\\nmultisensory data association\\nmultiple visual sensors\\nconsistent visual perception\\nnoisy pairwise correspondences\\nmultiway matching problem\\nlow-rank matrix approximation problem problem\\nalternating direction method of multipliers\\nstochastic matrices\\nfisher information metric\\nadmm\",\"806\":\"friction\\ngrasping\\nforce\\nsensors\\ncomputational modeling\\nmathematical model\\nrobots\\nbiomechanics\\ncontrol engineering computing\\ndata visualisation\\nkinematics\\nmanipulators\\nmedical robotics\\nrendering (computer graphics)\\nsurgery\\ntelerobotics\\nparametric grasping methodology\\nmultimanual interactions\\ninteractive simulators\\ntraining simulators\\nteleoperated robotic laparoscopic surgery\\nstateof-art simulators\\nrealistic visuals\\naccurate dynamics\\nkinematic simplification techniques\\ntruly multimanual manipulation\\nactual task\\nrealistic grasping\\nrigid-body dynamics\\ncollision computation techniques\\nstate-of-the-art physics libraries\\nparametric approach\\nmultimanual grasping\\nreal-time dynamic simulation\\naccomplishing multimanual tasks\\nscrewdriver task\",\"807\":\"foot\\nlegged locomotion\\nshape\\ngeometry\\nmathematical model\\ndynamics\\ncomputational geometry\\nmotion control\\npose estimation\\npublic domain software\\nrobot dynamics\\nstability\\nankle trajectory\\nshape dependent foot kinetics\\nopenpose\\nopen source pose estimation system\\nrigid foot passive robot\\nwalking robot stability\\nfoot shape optimization\\nexact foot geometry\\ndynamic model\\nbiped robot\\npassive bipedal walking dynamics\",\"808\":\"legged locomotion\\nrobot sensing systems\\nfoot\\ngears\\nharmonic analysis\\nclosed loop systems\\nhumanoid robots\\nmodal analysis\\nposition control\\nrobot dynamics\\nvibrations\\nbiped using experimental modal analysis\\nstructural design\\nlow level position control\\nwalking control\\ncontrol design\\nstructural dynamics\\nlola's mechanical structure\\nbiped walking robot\\ncontrol algorithms\\nstructural vibration problems\\nstructural resonances\\ncontrol loop resonances\\nclosed-loop identification method\\nstructural modes\",\"809\":\"couplings\\nmathematical model\\nlegged locomotion\\naerodynamics\\nrobustness\\nrobot dynamics\\nrobust control\\ntrajectory control\\ndynamic coupling\\ngait robustness\\nvelocity decomposition\\nunderactuated mechanical systems\\ntwo link biped model\\nunderactuated biped robots\\ntrajectory optimization\",\"810\":\"humanoid robots\\nlips\\nrobustness\\nstability criteria\\ndynamics\\nlegged locomotion\\nmotion control\\npredictive control\\nrobot dynamics\\nrobust control\\nstability\\nrange amplitude\\ninternal stability\\nis-mpc method\\nconstraint modification\\nzmp constraint restriction\\nrobust gait generation\\nhumanoids\\nhumanoid gait generation\\nrobust performance\\nconsidered disturbance signals\\nmid-range value\\nsampling time\\nstability constraint\\ncurrent mid-range disturbance\\nappropriate restriction\\ncontrol horizon\",\"811\":\"legged locomotion\\nthree-dimensional displays\\ntrajectory\\nrobustness\\ntorso\\nrobot kinematics\\nfeedback\\nhumanoid robots\\nlearning systems\\nrobot dynamics\\nhybrid zero dynamics inspired feedback control policy design\\n3d bipedal locomotion\\nmodel-free reinforcement learning framework\\nfeedback control policies\\n3d bipedal walking\\nrl algorithms\\nreference joint trajectories\\npolicy structure\\nhybrid nature\\nwalking dynamics\\nrl framework\\nlightweight network structure\\nshort training time\\n3d bipedal robot\\nstable limit walking cycles\\nwalking speed\",\"812\":\"task analysis\\nlegged locomotion\\nreduced order systems\\nlips\\ntrajectory optimization\\nnonlinear control systems\\npendulums\\nrobot dynamics\\nsprings (mechanical)\\nfive-link model\\ncassie bipedal robot\\noptimal reduced-order modeling\\nbipedal locomotion\\nlip\\nspring-loaded inverted pendulum\\nslip\\nagile maneuvers\\nhigh-dimensional system\\nground inclines\",\"813\":\"semantics\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nrobustness\\nvisualization\\nbandwidth\\nfeature extraction\\nimage colour analysis\\nimage matching\\nimage representation\\nmobile robots\\nmulti-robot systems\\nobject detection\\nrobot vision\\nslam (robots)\\nparticular communication bandwidth\\nlimited communication bandwidth\\nrelative object positions\\n2step decentralized loop closure verification\\ncompact semantic descriptors\\nbandwidth requirements\\ncommunication aware place recognition\\ninterpretable constellations\\nrobot networks\\nmultiple robots\\nmapping environments\\ncapricorn\\nexploring environments\\n3d points\\ncompact spatial descriptors\\nmatching robots\\ngeometric information\\nglobal image descriptors\\ntum rgb-d slam sequence\",\"814\":\"collision avoidance\\nplanning\\nrobot kinematics\\nvegetation\\ntrajectory\\nreachability analysis\\naerospace control\\nhelicopters\\nmulti-robot systems\\npath planning\\nposition control\\nrobot dynamics\\ntrees (mathematics)\\ncollision-free geometric solution guarantees\\nonline planning\\naerial robots\\nquadrotor teams\\ncluttered 3d workspaces\\nkinodynamic multirobot planning problem\\nposition invariant geometric trees\\nkinodynamically feasible trajectories\\nmultirobot team\\nnonstationary initial states\",\"815\":\"drones\\nstate estimation\\ncameras\\nsensors\\nglobal positioning system\\nreal-time systems\\nautonomous aerial vehicles\\ndecentralised control\\nmobile robots\\nrobot vision\\ndecentralized visual-inertial-uwb fusion\\nunmanned aerial vehicles\\nmultiple uavs\\nvisual-inertial-uwb fusion framework\\nextensive aerial swarm flight experiments\\nmotion capture system\\nvision based method\\nestimation consistency\\nrelative state estimation framework\\naerial swarm applications\\ndecentralized relative state estimation method\",\"816\":\"automobiles\\ncollision avoidance\\nrobots\\ntrajectory\\nplanning\\nturning\\nkinematics\\nmobile robots\\npath planning\\nrobot kinematics\\ntrajectory control\\ndubins curves\\nholonomic robots\\nseparation distance\\ntrajectory planning\\ncollision-free trajectories\\ndubins cars\\nconcurrent assignment\\ndc-capt\",\"817\":\"trajectory\\nagricultural machinery\\ncomputational modeling\\nvehicle dynamics\\ntracking\\ndynamics\\nlinear approximation\\nfeedback\\nlinearisation techniques\\noptimal control\\npredictive control\\nroad vehicles\\nstability\\ntrajectory control\\ncorrective term\\ntracking term\\ninput-output linearization\\nnonminimum-phase systems\\nis-mpc\\nantijackknifing control\\nfeedback control law\\nreference cartesian trajectory\\ntrailer hitch angle\\ntractor-trailer vehicles\\nintrinsically stable mpc scheme\",\"818\":\"agricultural machinery\\naxles\\nsensors\\nkinematics\\nreliability\\nestimation\\ntrajectory\\ncontrol system synthesis\\nmobile robots\\nmotion control\\npath planning\\nposition control\\npredictive control\\nroad traffic control\\nroad vehicles\\nstability\\nvehicle dynamics\\nsensing-aware model predictive path\\ncar-like tractor\\ncontroller-design problem\\njoint-angle kinematics\\nbackward motion\\nvehicle segments\\njackknife state\\njoint-angle estimation problem\\npath-following controller\",\"819\":\"robots\\nsnake robots\\nruntime\\ntraining\\nentropy\\nmotion control\\nlinear regression\\nadaptive control\\nbiomimetics\\nfeedback\\nmobile robots\\nregression analysis\\nrobot dynamics\\nbiomorphic hyperredundant robots\\nlocomotion gait\\nautonomous motion control\\nruntime training framework\\noffline practising\\nsnake robot\\ndynamic feedback\",\"820\":\"wheels\\nmobile robots\\nkinematics\\ntask analysis\\nmanifolds\\ntrajectory\\ncontrollability\\nfeedback\\ngeometry\\nposition control\\nrobot kinematics\\nmotion task scenarios\\nvirtual geometry constraint\\ntransverse function\\nfour-dimensional configuration manifold\\nsmall time local controllability\\ntwo-wheeled nonholonomic robot\\nnonstandard motion tasks\\nrestricted wheels rotation\",\"821\":\"mathematical model\\nrobot kinematics\\nadaptation models\\nprobability distribution\\noscillators\\nlegged locomotion\\nclosed loop systems\\ngraph theory\\nmotion control\\nneurocontrollers\\noptimisation\\npath planning\\nreal-time systems\\nsampling methods\\noptimal behaviors\\ngradient free optimization\\nclosed loop control\\nunderactuated robots\\nlegged robot control\\nreal time applications\\nprobabilistic graphical model\\nlocomotive behaviors\\ntask space central pattern generator\\nsampling based motion planner\\nneural oscillator network\",\"822\":\"shape\\ngrippers\\nthree-dimensional displays\\ntask analysis\\nplanning\\nrobots\\nuncertainty\\ndexterous manipulators\\ngraph theory\\nlearning (artificial intelligence)\\nunknown shape\\ngrasp configurations\\ndeep generative models\\nobject shapes\\npartial visual sensing\\nobject shape uncertainty\\nmanipulation actions\\nin-hand manipulation tasks\\nunknown objects\\ndexterous manipulation graph method\",\"823\":\"robustness\\ntask analysis\\nforce\\nrobot kinematics\\ntorque\\nrobot sensing systems\\ncomputational complexity\\nlearning (artificial intelligence)\\nmanipulators\\nhierarchical control\\nrobotic in-hand manipulation\\nfinger motion\\ncomplex manipulation sequences\\nlow-level controllers\\nmodel-free deep reinforcement learning\\nhierarchical method\\ntraditional model-based controllers\\nmanipulation primitives\\nelongated objects\\nobject models\",\"824\":\"tactile sensors\\ntrajectory\\nforce\\nfriction\\nperturbation methods\\nclosed loop systems\\ndexterous manipulators\\nend effectors\\nforce control\\nmanipulator dynamics\\npath planning\\nperturbation techniques\\nrobot vision\\nrobot trajectories\\nmanipulation primitives\\nabb yumi dual-arm robot\\ntactile dexterity\\ntactile feedback\\nclosed-loop tactile controllers\\ndexterous robotic manipulation\\ndual-palm robotic system\\ntactile control\\ntactile-based tracking\\nend-effector\",\"825\":\"grasping\\nprototypes\\nkinematics\\ntask analysis\\nrobot sensing systems\\nthumb\\ncontrol system synthesis\\ndexterous manipulators\\ngrippers\\nmanipulator kinematics\\nmotion control\\ncontinuous rotation capability\\nfingertips\\nobject manipulation\\nroller-based dexterous hand design\\ntwo-finger manipulation\\nnonholonomic spatial motion\\nrobotic hands\\nthree-finger manipulation\\nactively driven rollers\\nnonanthropomorphic robot hand\\nwithin-hand manipulation\\nobject grasping\",\"826\":\"shape\\nfiber gratings\\noptical sensors\\nrobot sensing systems\\nspatial resolution\\nbending\\nbragg gratings\\ndexterous manipulators\\nfibre optic sensors\\noptical fibres\\nreflectometry\\nflexible medical instruments\\ncontinuum dexterous manipulators\\nminimally invasive surgery\\naccurate cdm shape reconstruction\\nfiber bragg grating sensors\\nsensing locations\\nbasic shapes\\noptical frequency domain reflectometry\\nhigher spatial resolution\\ncomplex shapes\\nultraviolet laser exposure\\northopedic surgeries\\nmaximum tip position error\\nofdr reconstruction\\nfbg reconstruction\\nmore accurate alternative\\nfbg sensors\\ncomplex cdm shapes\\ncontinuum robots\\nhigh-resolution optical fiber shape sensing\\nrandom optical gratings\\nsize 35.0 mm\\nsize 3.4 mm\",\"827\":\"trajectory optimization\\nmanipulator dynamics\\ntask analysis\\nplanning\\napproximation theory\\ndexterous manipulators\\nfeedback\\nlinear programming\\nlinearisation techniques\\nnonlinear control systems\\nstability\\npiecewise affine approximations\\ndexterous robotic manipulation\\nnonsmooth nonlinear system\\nlocal multicontact dynamics\\npiecewise affine system\\nlinearization\\nfeedback controller\\nlinear programs\\nlocal trajectory stabilization\\ndexterous manipulation\\nfeedback policy design\",\"828\":\"cameras\\nlaser radar\\nthree-dimensional displays\\nvisualization\\nrendering (computer graphics)\\nsimultaneous localization and mapping\\ngeophysical image processing\\nobject tracking\\noptimisation\\nphotometry\\npose estimation\\nsolid modelling\\nmonocular direct sparse localization\\nprior 3d surfel map\\nmonocular camera\\nprior surfel map\\nvertex\\nnormal maps\\nglobal planar information\\nsparse tracked points\\nimage frame\\ndirect photometric errors\\ncamera localization\\npose tracking\\nrendering\\noptimization\\nglobal 6-dof camera poses\",\"829\":\"feature extraction\\nlaser radar\\nthree-dimensional displays\\nkalman filters\\nreal-time systems\\noptimization\\nnavigation\\ndistance measurement\\ninertial navigation\\niterative methods\\nmotion estimation\\noptical radar\\nstate estimation\\nground vehicles\\n6-axis imu\\niterated error-state kalman filter\\nfeature correspondences\\nfilter divergence\\nlins\\nstate-of-the-art lidar-inertial odometry\\nlightweight lidar-inertial state estimator\\nreal-time ego-motion estimation\\nrobust navigation\\n3d lidar\\nrobocentric formulation\",\"830\":\"calibration\\ndh-hemts\\nrobot kinematics\\nthree-dimensional displays\\nkinematics\\noptimization\\nindustrial manipulators\\nrobot vision\\ndh parameters\\nhigh stitching errors\\nlong-term routine industrial use\\nrobot-scanner calibration approach\\nlow data stitching error\\nlong-term continuous measurement\\n2d standard calibration board\\nlow stitching error\\nvirtual arm-based robot-scanner kinematic model\\ntrajectory-based robot-world transformation calculation\\ncumbersome marker-based method\\nlower system downtime\\nautomated eye-in-hand robot-3d scanner calibration\\nindustrial robot\\ncomplete measurement\\ndata stitching process\\nsingle coordinate system\\nmarker-free stitching\\ncumbersome traditional fiducial marker-based method\\nalign multiple fov\",\"831\":\"cameras\\nfeature extraction\\ntwo dimensional displays\\nrobustness\\npose estimation\\nthree-dimensional displays\\nvisual odometry\\ndistance measurement\\nimage reconstruction\\nlearning (artificial intelligence)\\nstereo image processing\\nmonocular visual odometry\\nhybrid scheme\\ncamera pose estimation\\npredicted repeatability maps\\npatch-wise 3d-2d association\\nlocal feature parameterization\\nadapted mapping module\\nlocal reconstruction accuracy\\nmonocular vo system\\nlearned repeatability\\nlearned description\\npublic datasets\\nrobust backend\\nlightweight backend\",\"832\":\"feature extraction\\nautomobiles\\nvideos\\nconvolution\\nestimation\\nvisualization\\ndecision making\\ndriver information systems\\ngraph theory\\nlearning (artificial intelligence)\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\ninteraction graph\\nobject importance estimation\\non-road driving videos\\nhuman driving behavior\\nautonomous driving systems\\nego-vehicle\",\"833\":\"inspection\\nsurface treatment\\ncameras\\nsurface topography\\nrobots\\nautomobiles\\nsurface reconstruction\\nautomobile industry\\nflaw detection\\nmobile robots\\npainting\\nquality control\\nvibrations\\nsemispecular painted automotive surfaces\\nreal-time robotics system\\ntolerate varying lighting conditions\\ninspected surface\\ndefect tracking mechanism\\nrobotics inspection system\\ndetecting defects\\npainted surface defect detection\\nsmall inherent vibrations\\nmanufacturing operations\\ntopographical information\\nspectral analysis\",\"834\":\"force\\ngrasping\\nsensors\\nestimation\\nshape\\nfasteners\\ntorque\\nactuators\\ncontrol system synthesis\\ndexterous manipulators\\nend effectors\\nhuman-robot interaction\\nmotion control\\ntorque control\\nunderactuated end-effector\\nplanar sequential grasping\\nmultiple objects\\nunderactuated end-effector design\\nautonomous grasp\\ncircular objects\\nsequential grasps\\nhuman-robot hand-off interactions\",\"835\":\"thumb\\nrobots\\njoints\\nmuscles\\ngrasping\\nelectronics packaging\\ndexterous manipulators\\ngrippers\\nmanipulator kinematics\\nmotion control\\nsynergy-inspired hands\\nbiomechanical characteristics\\nhuman hand synergy\\nrobot hands\\nsynergy characteristics\\nanthropomorphic hands\\nsynergy-inspired design\",\"836\":\"grasping\\ngrippers\\nmultiplexing\\nforce\\nbelts\\nmanipulators\\ncompliant mechanisms\\ndexterous manipulators\\nmotion control\\nmultiplexed manipulation\\nhybrid soft gripper\\nhybrid suction\\nparallel jaw grippers\\nmultimodal grippers\\nsoft robotic manipulators\\nsoft fingers\\nmultimodal grasping\\namazon robotics\\/picking challenge\\ncomplaint handed shearing auxetics actuators\",\"837\":\"grippers\\nforce\\ngrasping\\npulleys\\nadhesives\\nactuators\\ntendons\\nadhesion\\ncontrollable activation\\nminimal disturbance\\nform closure\\nrobotic grasping\\nversatile grasp\\nunderactuated gecko adhesive gripper\\nresulting gripper grasp force\\nadhesive contact area\\nsimple tendon-driven mechanism\\nunderactuated gecko-inspired adhesive gripper\\nmultiple activation steps\\ncomplex activation mechanism\",\"838\":\"strain\\njacobian matrices\\ndeformable models\\nshape\\ntask analysis\\nadaptation models\\nrobots\\ncontrol engineering computing\\ndeformation\\neigenvalues and eigenfunctions\\nend effectors\\nleast squares approximations\\nrobot vision\\nvisual servoing\\nsoft objects\\nonline estimation\\ndeformation jacobian\\nrobot end-effector\\ndeformation behavior\\nadvised method\\nmodel-free methods\\nmarker-based active shaping task\\nshape preservation tasks\\nactive deformation through visual servoing method\\nmodel-free deformation servoing method\\nweighted least-squares minimization\\nsliding window\\neigenvalue-based confidence criterion\\nmarker-less active shaping\\nmodel-based methods\",\"839\":\"task analysis\\nkernel\\nrobustness\\nfeature extraction\\nrobot kinematics\\nthree-dimensional displays\\ncontrol engineering computing\\ndata visualisation\\nentropy\\nfeature selection\\ngraph theory\\nlearning (artificial intelligence)\\nmanipulators\\nregression analysis\\nvideo signal processing\\nvisual geometric skill inference\\nmanipulation skills\\nhuman demonstration video\\nassociation relationships\\neye-hand coordination tasks\\ngeometric control error\\ngraph based kernel regression method\\nassociation constraints\\nhuman readable task definition\\ncontrol errors\\nfeature-based visual ser-voing\\nincremental maximum entropy inverse reinforcement learning\\nrobust feature trackers\",\"840\":\"visual servoing\\ncameras\\nconvergence\\nvisualization\\nadaptive optics\\ntask analysis\\nfeature extraction\\nimage sensors\\nimage sequences\\nlearning (artificial intelligence)\\nneural nets\\npose estimation\\nrobot vision\\nstereo image processing\\noptical flow\\nvisual features\\ndeep neural network\\ndiverse scenes\\nvisual servoing approaches\\nrobust servoing performance\\ncamera transformations\\ndeep flow guided scene agnostic image\\ndeep learning\\nrelative camera pose\\nphoto-realistic 3d simulation\\naerial robot\\ndfvs\\ninteraction matrix\",\"841\":\"visualization\\nnavigation\\ncameras\\nvisual servoing\\nmanipulators\\npath planning\\nrobot vision\\nphotometric path planning\\nvision-based navigation system\\nvisual memory\\ntopological map\\nvirtual camera\\nnavigability\\nvisual path\\nnavigation stage\\nonboard camera\\ntop view image\\nlearning stage\\nurban scene\",\"842\":\"visualization\\nmicrosoft windows\\ntrajectory\\noptimization\\ntask analysis\\ncomputational modeling\\ncameras\\nmanipulators\\noptimal control\\noptimisation\\npredictive control\\nregression analysis\\nrobot vision\\nvisual servoing\\nvisual predictive control tasks\\nregression techniques\\ncontrol optimization process\\n7-axis manipulator\\nimage-based visual servoing\",\"843\":\"end effectors\\nrobot kinematics\\ntask analysis\\nmathematical model\\ntopology\\nactuators\\ndesign engineering\\nelasticity\\nmanipulator dynamics\\nmanipulator kinematics\\nposition control\\nbin picking\\nvariable stiffness link\\nlow dof serial robot\\n2-dof malleable robot\\nworkspace categories\\nserial robot arms\",\"844\":\"grippers\\nsprings\\nshape\\nsoft robotics\\ngrasping\\nforce\\nmathematical model\\nbending\\ndexterous manipulators\\nelastomers\\nforce control\\npneumatic actuators\\nposition control\\nsprings (mechanical)\\npreprogrammed grasp\\nconstant-curvature wrap\\nfinger-sized round objects\\nflat objects\\nsmall objects\\nadaptable tri-stable\\ngrasped object\\nbi-stable springs\\nstable positions\\nfinger bending\\ngrasping performance\\ncontrol gripper\\nsoft grippers\\nwrap grasps\\nsoft robotic pneumatic grippers\\ndelicate objects\\nfluidic elastomer grippers\\ninextensible gripping surface\\nextensible pneumatic chambers\\nextensibility results\\nfinger curling\\nsimple fingers\\ntri-stable soft robotic finger\\npinch grasps\",\"845\":\"electron tubes\\ntendons\\nshape\\neducational robots\\nmanipulators\\npneumatic systems\\nbiomechanics\\ndexterous manipulators\\nmobile robots\\nposition control\\ndexterous tip-extending robot\\nvariable-length shape-locking\\ntip-extending vine robots\\ndistal end\\ninextensible tip-extending\\npressurized tip-extending\\nrobot body\\nlocked sections\\nfree sections\\nshape-locking mechanism\\nshape-locking concept\\nsoft robotics\\ndexterous workspace\",\"846\":\"iron\\nactuators\\nmagnetic cores\\npowders\\nelectromagnetics\\nmagnetic liquids\\ndexterous manipulators\\nelectromagnetic actuators\\ngrippers\\nliquid metal ion sources\\npneumatic actuators\\ncompliant electromagnetic actuator architecture\\nsoft robotics\\nsoft materials\\ncompliant actuation concepts\\nsoft robotic systems\\ngallium-indium liquid metal conductors\\nsoft actuator\\nxenia soft corals\\ncompliant permanent magnetic tips\\nrobotic actuator\\nfrequency 7.0 hz\\nsize 6.0 mm\",\"847\":\"valves\\njamming\\nactuators\\nlaser beams\\nsoft robotics\\nshape\\nbeams (structures)\\nbuckling\\ncantilevers\\nelectromagnets\\nelectromechanical actuators\\nmotion control\\nrigidity\\nrobot kinematics\\ntip-everting robots\\ntendonsteering\\ncantilevered loads\\nelectromechanical device\\nelectromagnet\\npassive valves\\npressure layer jamming\\nbuckle point locations\\ncompressive loads\\ndiscrete distributed stiffness control\\ninflated beam robot body\\ninflated continuum robots\",\"848\":\"legged locomotion\\nexoskeletons\\nadaptation models\\nlearning (artificial intelligence)\\ntrajectory\\nextremities\\noptimal control\\nadaptive control\\nartificial limbs\\nhandicapped aids\\niterative methods\\nmedical robotics\\nneural nets\\npatient rehabilitation\\nwearable robots\\ndata-driven reinforcement learning\\nlower limb exoskeleton\\nhemiplegic patient\\nrehabilitation scenario\\naffected leg\\nunaffected leg\\nexoskeleton system\\nddrl strategy\\npolicy iteration algorithm\\nonline adaptation control\\nwalking assistance control\\nwalking assistance scenario\\nstrength augmentation scenario\\nactor-critic neural network\\nacnn\\ndata-driven control\\nreinforcement learning\\nleader-follower multi-agent system\\nhemiplegic patients\",\"849\":\"legged locomotion\\nvisualization\\nmuscles\\nrobot sensing systems\\nperturbation methods\\nelectromyography\\nbiomechanics\\nfeedback\\ngait analysis\\nmechanoception\\nmedical robotics\\nmuscle\\nneurophysiology\\npatient rehabilitation\\npatient treatment\\nvirtual reality\\npoststroke gait rehabilitation\\nvariable stiffness treadmill\\nrobot-assisted gait therapies\\nfeedback mechanisms\\nvisual feedback\\nsurface stiffness changes\\nrepeatable muscle activation patterns\\npredictable muscle activation patterns\\nsurface changes\\nproprioceptive feedback\\nmanipulated visual feedback\\nvirtual environment\\nreal-world compliant surfaces\\nwalking surface stiffness\\nsensorimotor mechanisms\\nrobotic rehabilitation device\\nvirtual reality experience\\nrobot-assisted interventions\\nrehabilitation method\\nrobot assistance\\nmodel-based robot-assisted rehabilitation\\nhuman gait\\nfloor compliance changes\\nvisual anticipation\",\"850\":\"cameras\\nrna\\nfeature extraction\\nvisualization\\npose estimation\\nnavigation\\nthree-dimensional displays\\ncollision avoidance\\ndistance measurement\\ngraph theory\\nhandicapped aids\\nmobile robots\\nparticle filtering (numerical methods)\\nrobot vision\\nvisual positioning system\\nindoor blind navigation\\nvps\\nrobotic navigation aid\\nassistive navigation\\ndepth-enhanced visual-inertial odometry\\nrgb-d camera\\ninertial measurement unit\\ndvio method\\ngeometric feature\\nfloor plane\\nmeasurement residuals\\ninertial data\\ngraph optimization framework\\nsampson error\\nnear-range visual features\\nknown depth\\nfar-range visual features\\nestimation accuracy\\nparticle filter localization method\\npfl\\nvisually impaired person\\nheading error\\naccurate pose estimation\",\"851\":\"robot sensing systems\\nfootwear\\nforce\\nforce measurement\\ninstruments\\nlegged locomotion\\nclosed loop systems\\nforce sensors\\ngait analysis\\nmedical robotics\\nmuscle\\npatient rehabilitation\\nfoot-mounted sensors\\noutsole-embedded optoelectronic sensor configuration\\nbiaxial shear grfs\\ntraditional strain-gauge based solutions\\noptoelectronic sensors\\nfootwear structure\\noutsole-embedded sensor\\nshear ground reaction forces\\nonline estimation\\n3d ground reaction forces\\nclosed-loop control\\nlower-extremity robotic exoskeletons\\nin-verse dynamics\\noptimization models\\nnet joint torques\\nmuscle forces\\ninstrumented footwear\\nvertical grfs\\nwearable technology\\noptoelectronics\\nshear force sensor\",\"852\":\"brushless motors\\nperturbation methods\\nforce\\nforce sensors\\nshafts\\nforce control\\nlegged locomotion\\nclosed loop systems\\ngait analysis\\ngeriatrics\\ninjuries\\nmechanoception\\nmedical computing\\nmedical control systems\\nopen-source bump-emulation system\\nrobotic rope-driven system\\nhuman gait\\nfall-inducing perturbations\\nlaboratory-based perturbation systems\\naging population\\nfall-related injury\\nhuman balance\\nopen-loop system\\nclosed-loop force control\\nopen-loop force control\\ntransverse plane\\nforce-fields\",\"853\":\"actuators\\nthumb\\nexoskeletons\\ntendons\\nrobots\\npneumatic systems\\ngrasping\\ndexterous manipulators\\ndiseases\\nforce feedback\\ngrippers\\nhandicapped aids\\nmotion control\\npatient rehabilitation\\nquality assessment experiments\\ninflatable thumb\\ngrasp stability\\nhybrid assistive glove\\ngrasping capabilities\\nhand exoskeletons\\nneurological diseases\\nmusculoskeletal diseases\\nwearable gloves\\nexoskeleton glove\\npneumatic telescopic extra thumb\\nforce exertion experiments\\nactivities of daily living\",\"854\":\"exoskeletons\\nelectromyography\\ntorque\\nrobots\\nmuscles\\ngravity\\ntask analysis\\nbiomechanics\\nforce sensors\\nhuman-robot interaction\\nmedical robotics\\nmedical signal processing\\nmuscle\\nwireless emg\\nmovement direction\\nintensity estimation\\ngravity compensation\\nemg armband\\nemg signal\\ntraditional gravity compensation\\nintuitive control law\\nhuman-robot collaboration\\nfreedom upper-limb exoskeleton\",\"855\":\"clothing\\nrobots\\ngrasping\\ncollision avoidance\\nrails\\nneural networks\\ntraining\\nassisted living\\ndexterous manipulators\\nend effectors\\ngrippers\\nhandicapped aids\\nlearning (artificial intelligence)\\nlearning systems\\nneurocontrollers\\nposition control\\nrobot vision\\nrobot-assisted dressing system\\ndressing activities\\ngrasping point estimations\\nbaxter robot\\nrobot-garment collision avoidance\\norientation computation\\ngrasping point prediction\\ndepth images\\nsupervised deep neural network\\nrobotic manipulation\\nrobot end-effector\\nrobot configuration\\nelderly people\\ndisabled people\\nassistive robots\\ngarment manipulation\",\"856\":\"task analysis\\ncollision avoidance\\nskin\\nrobot sensing systems\\nsafety\\nhaptic interfaces\\nhuman-robot interaction\\nindustrial robots\\nmobile robots\\nmotion control\\nposition control\\nrobot vision\\nvisual servoing\\nreactive skin control\\n6 dof industrial robot\\nphysical human-robot interaction\\nindustrial scenarios\\nhierarchical task approaches\\nlow priority tasks\\nstandard hierarchical fusion\\ntactile interaction\\nsafety task\\n6 dof position-based visual servoing task\\ninteractive task-reconfiguring approach\\ntacto-selector\\npbvs\",\"857\":\"pallets\\ntask analysis\\ncollaboration\\nrobots\\nimpedance\\ntorque\\nresource management\\ngraphical user interfaces\\ngroupware\\nhuman-robot interaction\\nindustrial robots\\nmobile robots\\nmulti-robot systems\\noptimisation\\npalletising\\nproduction engineering computing\\nvisual perception\\ncollaborative palletizing tasks\\nintelligent collaborative robotic system\\nmixed case palletizing\\nvisual perception algorithms\\nhigh-level optimisation\\ngraphical user interface\\nmobile collaborative robotic assistant\\nhuman-robot collaborative framework\\nmoca\\npacking density maximisation\",\"858\":\"perturbation methods\\nforce\\nlegged locomotion\\nmuscles\\npulleys\\ntracking\\nforce control\\ngait analysis\\nhuman-robot interaction\\nmedical robotics\\npatient rehabilitation\\nstatistical testing\\nthree-term control\\nvirtual reality\\nwalking running subjects\\ntreadmill\\nt-test\\nauditory warnings\\n3 dof parallel cable system\\nutah's treadport active wind tunnel\\ngait algorithms\\npid force controller\\nthree tether parallel robot\\nnexus vicon motion capture\\nsports related concussions\\nhemiparetic rehabilitation\\nimmersive virtual reality locomotion system\",\"859\":\"collaboration\\ntask analysis\\nimpedance\\nmanipulators\\nservice robots\\nforce\\nhuman-robot interaction\\nmanipulator dynamics\\ntrajectory control\\nobject dynamical properties\\nrobot impedance control\\nhuman-robot collaboration\\nhuman-robot object co-manipulation\\nphri\\n7-dof kuka lbr iiwa 14 r820 robot\\nhuman trajectory\\nrobot control law\\nhuman forces\\ninteraction quality metrics\\ninteraction comfort\\nhuman safety\\nphysical human-robot interaction\\ncontinuous physical interaction\",\"860\":\"task analysis\\nmanipulators\\nhaptic interfaces\\ncameras\\nnull space\\nrobot vision systems\\nautonomous aerial vehicles\\ndelays\\nend effectors\\nforce feedback\\nmanipulator dynamics\\nmobile robots\\nposition control\\nredundant manipulators\\nrobot vision\\ntelerobotics\\nvideo cameras\\nredundant aerial manipulator\\nrobotic manipulator\\nflying base\\nreachability\\nmanipulation task\\nhuman capabilities\\ntelemanipulation tasks\\nvisual feedback\\ntask-dependent\\nvideo camera\\nend-effector motion\\nbase position\\nstable bilateral teleoperation\\ntime-delayed telemanipulation\\nwhole-body bilateral teleoperation\\nnull-space wall\\nhaptic concept\\nkinematic structure\\ntask-dependent optimal pose\",\"861\":\"task analysis\\nfatigue\\nmuscles\\ngrasping\\ncameras\\nrobot vision systems\\nhumanoid robots\\nmanipulators\\nmobile robots\\nmotion control\\ntelerobotics\\nmobile humanoid robot\\ngeneral-purpose assistive tasks\\nmotion mapping\\nhuman motion\\nrobot teleoperation\\nautonomous interface\\nteleoperation interfaces\\ntask completion time\\nassistance function\\nteleoperator\\nautonomous grasping function\",\"862\":\"tracking\\nthree-dimensional displays\\ntask analysis\\nrobot sensing systems\\ncameras\\nneural networks\\ndexterous manipulators\\nrobot vision\\ntelerobotics\\nvision-based teleoperation\\ndexterous robotic hand-arm system\\nrobotic systems\\nreasoning skills\\ndepth-based teleoperation system\\ndoa robotic system\\ndexpilot\\ndegree-of-actuation\\nmultifingered robots\\npick-and-place operations\",\"863\":\"robots\\ndecision making\\nprotocols\\nheuristic algorithms\\nindexes\\nforce\\nmulti-robot systems\\nlyapunov methods\\nstability\\ntelerobotics\\nteam cohesion\\ndynamic decision-making protocol\\ndecision variable\\nslave robots\\ndecision-making algorithm\\n3-masters-11-slaves teleoperation\\ndistributed winner-take-all teleoperation\\nmultirobot system\\nmultimaster-multislave teleoperation system\\nlyapunov stability analysis\",\"864\":\"task analysis\\ntrajectory\\nrobots\\ntraining\\nsupport vector machines\\nmanuals\\ndrones\\nautonomous aerial vehicles\\nlearning (artificial intelligence)\\nmobile robots\\ntelerobotics\\nenhanced teleoperation\\nautocomplete\\nremote location\\nskilled teleoperators\\ntraining time\\nnovice teleoperators\\nhuman input\\ndesired motion\\nmachine learning\\nmotion primitives\\nunmanned aerial vehicle\",\"865\":\"path planning\\ntask analysis\\ngeometry\\nbuildings\\nplanning\\nmeasurement\\nsolid modeling\\nassembling\\ncollision avoidance\\ncomputational geometry\\nindustrial control\\ndistance computation algorithms\\ncontact-based assembly tasks\\ncollision queries\\ndistance queries\\nbounding volume hierarchy\\nbroad phase proximity query algorithm\\ncontact-based hierarchy for assembly tasks\\nchat tasks\",\"866\":\"ip networks\\ncovariance matrices\\nthree-dimensional displays\\ntensile stress\\ncost function\\npath planning\\ngeometry\\ncollision avoidance\\ncomputational geometry\\ncomputer graphics\\nmanufacturing industries\\nmesh generation\\ntrees (mathematics)\\nbvh node\\nips cdc\\nsplit axis\\nsplit position\\nips path planner\\ncollision-free disassembly paths\\nbounding volume hierarchies\\nmixed face sizes\\nbvh\\ncomplex 3d geometries\\ntriangle meshes\\ncollision and distance computation module\\ntighter-fitting bounding volumes\",\"867\":\"robots\\ncollision avoidance\\naerospace electronics\\nnavigation\\nmathematical model\\nkinematics\\nautomobiles\\nglobal positioning system\\nlyapunov methods\\nmobile robots\\nmotion control\\npath planning\\nroad traffic control\\nnarrow lanes\\nhigh- density parking solution\\ncar-like robots\\nhard constraints\\nrobot motion\\nrobot localization\\nconfiguration space formulation\\nstanley robotics robots\\nautomated dense parking\\nlyapunov- based control strategy\\ngps orientation\",\"868\":\"trajectory\\npredictive models\\nhidden markov models\\nacceleration\\ngeometry\\nshape\\nurban areas\\nfeature extraction\\nobject detection\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\nautonomous driving systems\\ntraffic behavior\\nurban environments\\nroad geometries\\nlane-based multimodal prediction network\\narbitrary shapes\\ntraffic lanes\\nfuture trajectory\\nlane geometry\\nlane feature\\ngeneralized geometric relationships\\nvehicle state\\nvehicle motion model constraint\\nprediction method\\nmultimodal trajectory predictions\\nsafe driving systems\\nlamp-net\",\"869\":\"collision avoidance\\ntrajectory\\nsafety\\nmanipulators\\nrobot sensing systems\\nservice robots\\nindustrial manipulators\\nmotion control\\noccupational safety\\npath planning\\npredictive control\\nonline optimal motion generation\\nguaranteed safety\\nshared workspace\\nsafer manipulator robots\\nserious injury\\nequip robots\\nonline motion generation\\npartially unknown dynamic environment\\nindustrial manipulator robots\\nmodel predictive control scheme\",\"870\":\"eigenvalues and eigenfunctions\\nnonlinear dynamical systems\\nrobots\\naerospace electronics\\nheuristic algorithms\\nvehicle dynamics\\naircraft control\\nhelicopters\\nlearning (artificial intelligence)\\nnonlinear control systems\\noptimal control\\npredictive control\\nrobot dynamics\\nmodel predictive control\\nnonlinear diffeomorphism\\nmultirotor landing\\nnonlinear robot dynamics\\nepisodic koopman learning\",\"871\":\"coils\\nmagnetic resonance imaging\\nthree-dimensional displays\\ncameras\\nmagnetic devices\\nvisual servoing\\nmagnetic separation\\nmarine systems\\nmicrorobots\\nmobile robots\\nrobot vision\\nrefraction-rectified location algorithm\\ncoil module\\nmotor module\\neye-in-hand stereo-vision module\\nmedical applications\\nspacial movement\\ncontrol methods\\nmagnetic actuation systems\\nnarrow space\\nmagnetic field\\nmagnetic helical microswimmers\\nparallel mobile coils\\neye-in-hand 3d visual servoing\\ncylindrical workspace\\nprototype system\\nlong-distance 3d path\\ntriple-loop stereo visual servoing strategy\\ndynamic magnetic fields\\nmobile-coil system\",\"872\":\"shape\\nstrain\\nmagnetic resonance imaging\\nnanoparticles\\nvirtual private networks\\nmicromagnetics\\nmagnetic anisotropy\\ncontrol nonlinearities\\ndeformation\\nfuzzy control\\nmicrorobots\\nmobile robots\\nmulti-robot systems\\nrobot dynamics\\nepns\\nmobile paramagnetic nanoparticle swarm\\nautomatic shape deformation control\\nswarm control\\nactive shape deformation\\nelliptical rotating magnetic fields\\nswarm pattern\\nelliptical paramagnetic nanoparticle swarm\\nstrength ratio\\nelliptical field\\nshape ratio\\nlength ratio\\ndeformation dynamics\\nfuzzy logic-based control\\nnanorobot\\nmicrorobotics\\nfield ratio\\nnonlinearity\\nplanar rotational locomotion\\nplanar translational locomotion\",\"873\":\"robots\\npropulsion\\nmagnetosphere\\nprototypes\\nmicroorganisms\\nforce\\nmathematical model\\nbiomechanics\\nmicrorobots\\nmobile robots\\nposition control\\nmagnetic miniature swimmers\\nmultiple rigid flagella\\nmultiple rigid tails\\nrotating magnetic field\\nrobot rotation\\ntail distribution\\ntail height\\nmultitailed swimmer robots\\nspherical helices\\n2-tailed swimmer\\nangular position\",\"874\":\"probes\\nforce\\nforce measurement\\noptical sensors\\nforce sensors\\nsensor phenomena and characterization\\ncalibration\\nclosed loop systems\\nelectromagnetic actuators\\nmicrosensors\\nmicroelectro mechanical systems\\nforce measurements\\nmeso-scale robotics\\nmeso-scale active force sensor\\nnovel meso-scale sensor\\nnil-stiffness guidance\\nloop control\\nnil-stiffness characteristic\\ninfinite stiffness\\nsensor architecture\\nlow frequency forces\\ncutoff frequency\\nlarge-range nil-stiffness electro-magnetic active force sensor\\nactive force sensors\\nmeasurement range\\nconventional passive force sensors\\nquasiinfinite stiffness\\nfrequency 73.9 hz\",\"875\":\"saturation magnetization\\nelectromagnets\\ncurrent measurement\\nmagnetic cores\\nmagnetostatics\\nmagnetic resonance imaging\\nmagnetic separation\\nelectromagnetic devices\\nlearning (artificial intelligence)\\nmean square error methods\\nmedical computing\\nneural nets\\nsurgery\\nnonlinear regions\\nhigher magnetic fields\\nrandom forest\\nrf\\nartificial neural network\\nemns\\nstate-of-the-art linear multipole electromagnet model\\nmpem\\nann model\\nfield magnitude\\ncurrent range\\nhigh current regions\\nfield-magnitude rmse improvement\\nerror reduction\\nmachine learning\\nmedical applications\\ncomplex nonlinear behavior\\naccurate field\\nmagnetic navigation\\nmodeling electromagnetic navigation systems\\nmultiscale devices\\nhuman body\\nremote surgery\\nlinear behavior\\nsignificant modeling errors\",\"876\":\"fitting\\ntracking\\nhead\\nmicroscopy\\nmathematical model\\nreal-time systems\\ncurve fitting\\nbiology computing\\nccd image sensors\\nfeature extraction\\nimage motion analysis\\nobject tracking\\nautomated tracking system\\ntime-lapse observation\\ntail recognition\\nc. elegans\\nautomated platform\\nbehavioral analysis\\nlong-term tracking\\nresponse speed\\ntracking time\",\"877\":\"feature extraction\\nuncertainty\\nbiological system modeling\\nbayes methods\\ndata models\\nneural networks\\nbackscatter\\nbathymetry\\ngeophysical image processing\\nneural nets\\noceanographic techniques\\nremotely operated vehicles\\nseafloor phenomena\\nsonar\\nunderwater vehicles\\nhabitat model\\nauv systems\\nseafloor imagery\\nefficient auv surveys\\nvisually-derived habitat classes\\nbroad-scale bathymetric data\\nfewer samples\\nbenthic surveys\\nadaptive benthic habitat\\nautonomous underwater vehicles\\nbenthic habitat mapping\\nbroadscale bathymetric data\\nremotely-sensed acoustic data\",\"878\":\"image recognition\\nrobot sensing systems\\ntask analysis\\nsemantics\\nthermal sensors\\nfeature extraction\\nimaging\\nimage colour analysis\\nimage retrieval\\nimage segmentation\\ninfrared imaging\\nspectral analysis\\nmultispectral place recognition task\\nmultispectral semantic segmentation\\nmultispectral domain invariant image\\nretrieval-based place recognition\\nmultispectral recognition\\nthermal image\\nrgb domain-based tasks\\nmultispectral domain invariant framework\\nunpaired image translation method\\nsemantic image\\ndiscriminative invariant image\",\"879\":\"robots\\nplanning\\nprobabilistic logic\\ncognition\\nsemantics\\ntask analysis\\npredictive models\\nfailure analysis\\nhumanoid robots\\nmobile robots\\nplanning (artificial intelligence)\\nprobability\\nprobabilistic effect prediction\\nsemantic augmentation\\nexact outcome\\nfailure situations\\nfailure tolerance\\nrobot actions\\naugmenting collected experience\\nsemantic knowledge\\nrealistic physics simulations\\noutcome probabilities\\nunknown tasks\\nsimulated experience\\naction success probabilities\\nworld experiments\\nhumanoid robot\\nplanning trials\\nrollin justin\",\"880\":\"robots\\nplanning\\ntask analysis\\ncomputational modeling\\ncollision avoidance\\nstochastic processes\\ntrajectory\\nintelligent robots\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\nplanning (artificial intelligence)\\nmultiple execution-time contingencies\\nmotion policies\\nstochastic settings\\nstochastic situations\\nabstract models\\nmotion planning\\nabstract planning\\nstochastic environments\\nanytime integrated task\",\"881\":\"bridges\\npayloads\\nforce\\nwheels\\nwinches\\nrobots\\ninspection\\nbridges (structures)\\ncables (mechanical)\\ngrippers\\nmobile robots\\nrobot dynamics\\nwires\\npalm-based gripper\\nccrobot-iii\\ncable climbing robot\\ncable-stayed bridge inspection\\nmainbody frame\\nsteel wires\\nclimbing precursor\\nsplit-type wire-driven design\\nmass 40.0 kg\\nsize 90.0 mm to 110.0 mm\",\"882\":\"robots\\nangular velocity\\nturning\\nshafts\\nelbow\\ncrawlers\\nkinematics\\ncontrol engineering computing\\nmobile robots\\nmotion control\\npipes\\nrobot kinematics\\nomnidirectional tractable three module robot\\nomnidirectional modules\\nholonomic motion\\nmotion singularity region\\nmotion capabilities\\nclosed-form kinematic model\\nmsc adams\",\"883\":\"bridges\\nsteel\\nmobile robots\\ninspection\\nadhesives\\nforce\\nbridges (structures)\\ndesign engineering\\nmaintenance engineering\\nstructural engineering\\nara lab robot\\nsteel structures\\npractical climbing robot\\nsteel bridge inspection\\ncutting edge steel inspection robots\\nunified design\\nadvanced robotic and automation lab\",\"884\":\"wheels\\nmobile robots\\nmagnetic levitation\\nmagnetic forces\\nwelding\\nadhesives\\nadhesion\\nelasticity\\nindustrial manipulators\\nmanipulator kinematics\\npermanent magnets\\nrobotic welding\\nsteel\\nnonelastic suspension mechanism\\narbitrary curved shape\\nwheeled wall-climbing robot\\ncurved ferromagnetic surfaces\\nmagnetic force direction\\nmagnetic wheels\\nspherical shape\\n2 dof rotational magnetic adhesion\\nshape-adaptive magnetic adhesion\",\"885\":\"fault detection\\njacobian matrices\\nmathematical model\\nkernel\\ntime measurement\\nrobot kinematics\\nfault diagnosis\\nmanipulators\\nnonlinear control systems\\npolynomial approximation\\nscara\\northonormal jacobi polynomials\\nnonlinear mechanical systems\\nrigid robots\\nalgebraic fault detection\",\"886\":\"rotors\\ntorque\\nforce\\nservomotors\\nfault tolerance\\nfault tolerant systems\\nattitude control\\naircraft control\\nhelicopters\\nfault tolerance analysis\\nreconfigurable tilted rotor\\nmultirotor vehicles\\nyaw maneuverability\\nhexarotor vehicle\\nhexagon-shaped multirotor\\naltitude control\\ntilt angle\\nrotor reconfiguration\",\"887\":\"testing\\ntools\\ninstruments\\nclustering algorithms\\nservice robots\\nsafety\\nfault diagnosis\\npattern clustering\\nrobot programming\\nsystem monitoring\\nexecution anomaly detection\\nautonomy software robustness\\nclustering algorithm\\nautonomy systems\\nrobotics systems\\nreal-world industrial system\\nautonomous systems\\nfault identification\",\"888\":\"testing\\ntools\\ntracking\\nneurons\\ncameras\\nfeature extraction\\nreliability\\nimage filtering\\nimage motion analysis\\nkalman filters\\nlearning (artificial intelligence)\\nneural nets\\nobject tracking\\nreal-world learning-enabled system\\ndynamic vehicle tracking\\nhigh-resolution wide-area motion imagery input\\nsymbolic components\\nkalman filter\\nneural networks\\nsystem-level reliability\\ncoverage-guided neural network testing tool\\nvehicle tracking system\\nadversarial examples\\ndeep learning components\\nvalidation methods\\nlearning-enabled systems\\nneural network components\\ndeepconcolic tool\",\"889\":\"proposals\\ngrasping\\ntask analysis\\nrobot sensing systems\\nfeature extraction\\nkernel\\nconvolutional neural nets\\nimage classification\\nlearning (artificial intelligence)\\nobject detection\\nrobot vision\\nhighly accurate robotic grasp detection\\nfully convolutional neural network\\nrotation ensemble module\\nrotation invariance\\ncomputer vision tasks\\nrotation anchor box\\nmultiple objects\\n4-axis robot arm\\ncornell dataset\\nrem\",\"890\":\"task analysis\\nshape\\nvisualization\\ntraining data\\nthree-dimensional displays\\nsolid modeling\\ntraining\\ncad\\nlearning (artificial intelligence)\\nobject recognition\\npose estimation\\nrobotic assembly\\nkit assembly task\\nshape matching problem\\nshape descriptor\\nobject surfaces\\nself-supervised data-collection pipeline\\n3d cad models\\ntask-specific training data\",\"891\":\"task analysis\\nvisualization\\nrobots\\nstrain\\ntraining\\nvideos\\ndata models\\nintelligent robots\\nlearning (artificial intelligence)\\nmanipulators\\nrobot vision\\nropes\\nrope manipulation policies\\ndense object descriptors\\nsynthetic depth data\\nrobotic manipulation\\ndeformable 1d objects\\nhigh-fidelity analytic models\\nconfiguration spaces\\nend-to-end manipulation policies\\nphysical interaction\\ninterpretable deep visual representations\\nrobot manipulation\\ninterpretable policies\\ntransferable geometric policies\\nvisual reasoning\\npoint-pair correspondences\\ninitial goal rope configurations\\ngeometric structure\\nsynthetic depth images\\ndense depth object descriptors\\nabb yumi robot\\ninterpretable geometric policies\",\"892\":\"simultaneous localization and mapping\\nstrain\\njacobian matrices\\noptimization\\ncameras\\ndeformable models\\ngeometry\\ncomputational complexity\\nembedded systems\\ngraph theory\\nhessian matrices\\nrobot vision\\nslam (robots)\\nstereo image processing\\nparameter estimation\\ncomputation complexity\\ntwo step optimization\\ndeformable geometry\\nstereo camera\\nslam applications\\nlarge scale embedded deformation graph\\nhessian matrix\",\"893\":\"cameras\\nrobot vision systems\\nrobot kinematics\\ncalibration\\ntwo dimensional displays\\ntraining\\nimage colour analysis\\nimage sensors\\nmanipulators\\nneural nets\\npose estimation\\nrobot vision\\ncamera-to-robot pose estimation\\nsingle rgb image\\ndeep neural network\\nperspective-n-point\\nrobot manipulator\\nclassic hand-eye calibration systems\\ncamera sensors\\nclassic off-line hand-eye calibration\\nrobot sensors\",\"894\":\"cameras\\ncalibration\\nimage segmentation\\nsemantics\\nheating systems\\naluminum\\nthree-dimensional displays\\ncontrol engineering computing\\nconvolutional neural nets\\nimage colour analysis\\ninfrared imaging\\nlearning (artificial intelligence)\\nmobile robots\\npst900\\nsegmentation network\\nlong wave infrared imagery\\nrgb-thermal camera calibration\\nthermal image pairs\\ndarpa subterranean challenge\\nrgb imagery\\nsemantic segmentation\\ncnn architecture\",\"895\":\"three-dimensional displays\\nlaser radar\\nimage segmentation\\ntwo dimensional displays\\nencoding\\nsemantics\\nfeature extraction\\noptical radar\\nradar imaging\\nstereo image processing\\nsingle-shot instance prediction\\nlidar instance segmentation\\nlidar point cloud dataset\\npoint-wise labels\\nrobust baseline method\\nlarge-scale outdoor lidar point clouds\\ndense feature encoding technique\\nprecise 3d bounding box\",\"896\":\"cameras\\nrobot vision systems\\nimage segmentation\\nvisualization\\ncoherence\\ntracking\\nimage sequences\\nmobile robots\\nobject detection\\nobject recognition\\nrobot vision\\nvideo signal processing\\nmobile robot\\npan-tilt monocular camera\\ncamera movements\\nrobot operating indoors\\nobject candidates\",\"897\":\"three-dimensional displays\\ntask analysis\\nsemantics\\nnetwork architecture\\nimage segmentation\\ntwo dimensional displays\\nconferences\\nconvolutional neural nets\\nimage classification\\nimage representation\\nneural net architecture\\nsolid modelling\\ndilated point convolutions\\nreceptive field size\\n3d point cloud\\npoint convolutional networks\\nsemantic segmentation\\nobject classification\\n3d data representations\\nnetwork architectures\",\"898\":\"decoding\\nvisualization\\nfeature extraction\\nimage segmentation\\nsemantics\\nimage edge detection\\ntask analysis\\ncollision avoidance\\nedge detection\\nimage fusion\\ninertial navigation\\nmarine vehicles\\nmobile robots\\nneural net architecture\\nremotely operated vehicles\\nrobot vision\\nrefinement network\\nunmanned surface vehicles\\nobstacle detection\\nwater reflections\\nwakes\\ninertial information fusion\\nvisual features\\ndeep encoder decoder architecture\\nwater obstacle separation\\nsemantic segmentation\\nautonomous navigation\\nwater edge estimation\\ninertial measurement unit\\nloss function\\nwater features\\nsensor fusion\\nseparation function\",\"899\":\"feature extraction\\ndetectors\\ncomputer architecture\\ntask analysis\\nspatial resolution\\nhead\\nobject detection\\ncomputer vision\\nneural nets\\ndanet\\nsingle-stage object detectors\\ndynamic anchor selection\\nanchor boxes\\nobject localization candidates\\nms coco dataset\",\"900\":\"radar\\nmeasurement\\ntask analysis\\nestimation\\nrobot sensing systems\\ncomputer architecture\\ndistance measurement\\nfeature extraction\\nimage colour analysis\\nimage sensors\\nimage sequences\\nmobile robots\\nmotion estimation\\nobject recognition\\nobject tracking\\nradar computing\\nrobot vision\\nslam (robots)\\nsupervised learning\\npredict robust keypoints\\nodometry estimation\\nmetric localisation\\nself-supervised framework\\ndifferentiable point-based motion estimator\\nlocalisation error\\noxford radar robotcar dataset\\npoint-based radar odometry\",\"901\":\"forecasting\\nlaser radar\\nthree-dimensional displays\\nneural networks\\nobject detection\\npredictive models\\ntrajectory\\nbelief propagation\\nconvolutional neural nets\\ngaussian processes\\ngraph theory\\nintelligent transportation systems\\niterative methods\\nlearning (artificial intelligence)\\nmessage passing\\nprobability\\nself-driving\\nprobabilistic predictions\\niterative actor state update\\nspatially-aware graph neural network\\nconvolutional neural network\\nsensor data\\nrelational behavior forecasting\\nspagnn\\nmodel uncertainty\\ngaussian belief propagation\",\"902\":\"three-dimensional displays\\ntensile stress\\nvehicle dynamics\\ntransforms\\nlaser radar\\nestimation\\nobject detection\\ncomputational geometry\\ncomputer vision\\nfeature extraction\\nimage sequences\\nintelligent transportation systems\\nlearning (artificial intelligence)\\nmotion compensation\\nmotion estimation\\noptical radar\\nparameter estimation\\nreal-time systems\\nroad safety\\ntraffic engineering computing\\nlidar point clouds\\nurban environment\\nmotion detection\\n3d point cloud sequence\\nobject categories\\ntemporal context aggregation\\nclass-agnostic scene dynamics\\nmotion parameters estimation\\nself driving vehicle navigation safety\\nmotion parameter estimation\\nreal time inference\\nroad participant motion\\nneural network\\ncamera images\",\"903\":\"trajectory\\nrobots\\npredictive models\\ncameras\\ntarget tracking\\ndata models\\npipelines\\ncontrol engineering computing\\nhuman-robot interaction\\nimage resolution\\nrecurrent neural nets\\nrobot vision\\nspatiotemporal phenomena\\nicub robot\\nevent-based spatiotemporal trajectory prediction\\nnonlinear trajectories\\nframe-based cameras\\nasynchronous information stream\\nlow latency information stream\\nhigh temporal resolution\\nlong short-term memory networks\\nevent-cameras spatial sampling\\nencoder-decoder architecture\\nspatial trajectory points\\nhuman-to-robot handover trajectories\",\"904\":\"painting\\npaints\\nrobots\\nthree-dimensional displays\\nsolid modeling\\ntwo dimensional displays\\ngeometry\\nimage texture\\nindustrial robots\\nmixing\\npath planning\\nrecurrent neural nets\\nrobot vision\\nsupervised learning\\nrobotic texture painting\\npainting textures\\nsurface geometry\\npaint mixing\\nself-supervised learning framework\\npainting process\\npaint simulation environment\\nrobot executes\\ndata-driven planning framework\\npaint delivery tool flow rate\\n3d surfaces\\nrecurrent neural network\\nrnn\",\"905\":\"planning\\nrobots\\ntrajectory\\nprobabilistic logic\\ntraining\\ncomplexity theory\\nneural networks\\ngraph theory\\nmobile robots\\nmotion control\\npath planning\\nsampling methods\\nimplicit graph representation\\nstate space\\nsolution trajectories\\ngraph-theoretic techniques\\nhierarchical graph\\nuniform sampling\\nsampling-based motion planning\\nrobotic motion planning\\nmotion planning techniques\\nlearned critical probabilistic roadmaps\\ncritical prm\\ncritical probabilistic roadmaps\",\"906\":\"heuristic algorithms\\ntraining\\npath planning\\nbiological neural networks\\nrobots\\nnavigation\\ngraph theory\\nlearning (artificial intelligence)\\nneural nets\\noptimisation\\nsearch problems\\npath planning problem\\ncomputation load\\nneural network\\noptimal paths\\noptimal cost\\nglobal optimality\\nadmissible heuristic function\\nefficient graph search\\nlearning heuristic a*\\nlha*\\nsuboptimality bound\\nmaze-like map\",\"907\":\"planning\\ntask analysis\\nrobots\\ncollision avoidance\\nheuristic algorithms\\nfeature extraction\\nconvolution\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nsampling methods\\nstereo image processing\\ntrees (mathematics)\\nts-rrt\\ntask-space rapidly-exploring random trees\\n3d-cnn\\nheuristic guided task-space planner\\nfully convolutional neural networks\\nheuristic map\\nrandom trees\\nsampling-based planner\\ncollision-free path\\nrobotic manipulation\\nmotion planning\",\"908\":\"navigation\\nplanning\\nsemantics\\ntrajectory\\nmathematical model\\noptimization\\nrobots\\ngeometry\\nimage representation\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\npath planning\\nrobot vision\\nlinear regression\\nefficiency planning\\nsampling distribution learning\\nsampling-based planners\\nobject-level semantics\\nmyopic behavior\\ngeometric information\\nrobotic agent\\nobject-level representations\\nhybrid geometric\",\"909\":\"planning\\ntask analysis\\nrobot sensing systems\\nneural networks\\ngrasping\\nsearch problems\\ninteger programming\\nlearning (artificial intelligence)\\nmanipulators\\nneurocontrollers\\npath planning\\nrobot vision\\ndeep visual heuristics\\nmixed-integer program\\ndeep neural network\\nvisual input\\nrobot manipulation planning\\nmotion planning\\nlearning algorithm\\ngoal encoding\\noptimization problems\",\"910\":\"planning\\noctrees\\nentropy\\nrobot sensing systems\\nmeasurement\\ntask analysis\\nautonomous aerial vehicles\\ncollision avoidance\\nmicrorobots\\nmobile robots\\nnavigation\\nprobability\\nrobot vision\\nmav\\ncollision-free navigation\\nautonomous robots\\nmicroaerial vehicles\\nmap entropy\\noccupancy probabilities\\nutility function\\nfrontier extraction\\nfrontier-based exploration\\nfrontier voxels\\nmap frontiers\\nfrontier-based information-driven autonomous exploration\\nexploration planner\\noctree map representation\\nvisual-based navigation\\nmotion planning\\noctree-based occupancy mapping\\nsampling-based exploration\\naerial systems: perception and autonomy\",\"911\":\"trajectory\\ncameras\\nvehicle dynamics\\nplanning\\nglobal positioning system\\nvisualization\\nacceleration\\naircraft landing guidance\\nautonomous aerial vehicles\\nhelicopters\\nkalman filters\\nmobile robots\\nnonlinear filters\\nrobot dynamics\\nrobot vision\\nrobust control\\nturbulence\\nvariable structure systems\\ndynamic landing\\nautonomous quadrotor\\nmoving platform\\nturbulent wind conditions\\nautonomous landing\\nfast trajectory planning\\nwind disturbance\\nfully autonomous vision-based system\\nquadrotor-platform distance\\nlanding trajectory\\nreceding horizon control\\nboundary layer sliding controller\\nextended kalman filter\\ngps measurements\\nprecise control\\nunmanned aerial vehicles\\nautonomous vehicles\\nlanding on a moving platform\\ndisturbance compensation\",\"912\":\"aerodynamics\\ntrajectory\\natmospheric modeling\\nreal-time systems\\ncomputational modeling\\nplanning\\nsplines (mathematics)\\naerospace components\\nautonomous aerial vehicles\\nfeedback\\nmobile robots\\nmotion control\\nnonlinear control systems\\npath planning\\nposition control\\npredictive control\\nvehicle dynamics\\nrotary-wing uavs\\nnonlinear control approach\\nfixed-wing uavs\\nfull-state direct trajectory optimization\\nrepresentative aircraft model\\nnonlinear aircraft model\\nfixed-wing trajectories\\nrandomized motion planning\\nlocal-linear feedback\\ndirect nmpc\\npost-stall motion planning\\nfixed-wing unmanned aerial vehicles\\nfrequency 5.0 hz\",\"913\":\"iron\\naircraft\\nnumerical models\\natmospheric modeling\\napproximation algorithms\\nmathematical model\\naerospace control\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\nconvex programming\\nnonlinear control systems\\npredictive control\\nremotely operated vehicles\\nflight envelope determination\\nprotection system\\nfixed-wing uav\\ngeneric model\\nnonlinear numerical model\\nmodel predictive controller\\nflight envelope constraints\\ntrim flight envelope\\nmpc\",\"914\":\"forecasting\\npredictive models\\nroads\\nuncertainty\\ncomputer architecture\\ntensile stress\\nprobability density function\\nposition control\\nprobability\\nroad vehicles\\ntracking\\njoint forecast\\nmultimodal probability density functions\\nvehicle position tracks\\nmultimodal joint vehicle motion forecasting\\nmaneuver definitions\\nroad scene\\nlong short-term memory layers\\nspatial grid\",\"915\":\"sonar\\nimage reconstruction\\nimaging\\nthree-dimensional displays\\nsensors\\nnonlinear optics\\nsurface reconstruction\\nautonomous underwater vehicles\\noptimisation\\nrobot vision\\nsolid modelling\\nsonar imaging\\nstereo image processing\\n3d imaging sonar reconstruction\\nobject-level 3d underwater reconstruction\\nimaging sonar sensors\\nnonline-of-sight reconstruction\\nconvex linear optimization problem\\nalternating direction method of multipliers\\nadmm\\nsonar elevation apertures\\nvolumetric albedo framework\",\"916\":\"simultaneous localization and mapping\\nlasers\\nnavigation\\nfeature extraction\\nthree-dimensional displays\\nimage registration\\niterative methods\\nmobile robots\\nrobot vision\\nslam (robots)\\nlink-points\\nmultiple indoor areas\\noutdoor areas\\nmap quality\\nsingle map approaches\\nsemantic map management approach\\nmultiple maps\\nmodular map structure\\nutilized slam method\\nlaser scan data\\nappropriate slam configuration\\nsingle independent maps\\nappearance-based method\\niterative closest point registration\\npoint clouds\\nsimultaneous localization and mapping configurations\",\"917\":\"semantics\\ncollaboration\\nthree-dimensional displays\\nrobot sensing systems\\ngeometry\\nrobot kinematics\\nbayes methods\\nexpectation-maximisation algorithm\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nsingle robot semantic mapping\\ncollaborative geometry mapping\\nsemantic point cloud\\nheterogeneous sensor fusion model\\ncollaborative robots level\\n3d semantic map fusion algorithm\\nhierarchical collaborative probabilistic semantic mapping framework\\nbayesian rule\\nprobability\\nexpectation-maximization\\nmathematical modeling\",\"918\":\"support vector machines\\nkernel\\ntraining\\nrobot sensing systems\\ncollision avoidance\\ntrajectory\\nimage classification\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\ntrajectory control\\ndecision boundary\\nkernel perceptron classifier\\nonline training algorithm\\npiecewise-polynomial robot trajectories\\nautonomous navigation\\nackermann-drive robot\\nsparse kernel-based occupancy\\nreal-time occupancy mapping\\nautonomous robot\\nmap representation\\npiecewise-polynomial robot trajectory\\npiecewise-linear robot trajectory\\ncollision checking algorithm\",\"919\":\"three-dimensional displays\\nmeasurement\\nrobots\\nsemantics\\ntwo dimensional displays\\nindoor environments\\npath planning\\nimage representation\\nindoor navigation\\nmobile robots\\nrobot vision\\nslam (robots)\\nstereo image processing\\ntopology\\ntopological global representations\\n3d dense submaps\\nhybrid global map\\nautonomous exploration\\nautonomous navigation\\ndense 3d maps\\n3d dense representations\\n3d dense mapping systems\\nhybrid topological mapping\\nmetric 3d maps\\nstandard cpu\",\"920\":\"cameras\\npipelines\\nmachine-to-machine communications\\nimage edge detection\\npose estimation\\nsimultaneous localization and mapping\\nhistograms\\ncomputer vision\\nobject detection\\noptimisation\\npath planning\\nrobot vision\\nslam (robots)\\nlifted algorithm\\ncombinatorial complexity\\nheuristic criterion\\nplanar pose estimation\\nmarker-based mapping\\nhighly ambiguous inputs\\nppe ambiguities\\npossible marker orientation solutions\\nrotation averaging formulation\\nmarker corners\\nplanar markers\\nclique constraints\\nrobust rotation averaging\\nmarker pose ambiguity\",\"921\":\"training\\nembedded systems\\ncosts\\nservice robots\\ncomputational modeling\\nconferences\\nrobot vision systems\\ncameras\\nface recognition\\nfeature extraction\\nimage colour analysis\\nimage motion analysis\\nimage sensors\\nimage sequences\\nlearning (artificial intelligence)\\npose estimation\\nuser interaction\\nservice robot\\nproactive service\\npotential visitors\\nvisitor\\nearly anticipation\\nhuman pose information\\npublicly available jpl interaction dataset\\naccurate anticipation performance\\ncnn-lstm-based model\\nhuman verification\",\"922\":\"three-dimensional displays\\ncameras\\nrobot vision systems\\ntrajectory\\nquaternions\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nsport\\nspin detection\\nrobotic table tennis\\nrotation\\ntable tennis match\\nhuman player\\nreturn stroke\\nhigh-speed camera\\nframe rate\\ncircular brand logo\\nbackground difference\\nspin types\\ntable tennis rally\\nfrequency 380.0 hz\",\"923\":\"navigation\\nvisualization\\ntask analysis\\nrobot sensing systems\\nvisual perception\\nacoustics\\nfeature extraction\\nacoustic signal processing\\naudio signal processing\\naudio-visual systems\\nhuman computer interaction\\nmobile agents\\npath planning\\naudio-visual embodied navigation\\nmobile intelligent agents\\nmultiple sensory inputs\\nsound source\\nindoor environment\\nraw egocentric visual data\\naudio sensory data\\naudio signal\\nvisual environment\\nvisual pieces\\naudio pieces\\nvisual perception mapper module\\nsound perception module\\naudio-visual observations\\nsimulated multimodal environment\\nvisual-audio-room dataset\",\"924\":\"tools\\ndata models\\nrobots\\ntask analysis\\ntraining\\nsolid modeling\\nneural networks\\nconvolutional neural nets\\ngraph theory\\nlearning (artificial intelligence)\\nrobot vision\\ngated graph neural network\\nautonomous tool construction\\nreference tool\\nrobotics\\nggnn\\nrcnn-like structure\\ntc-grcnn\\nlarge-scale training data generation\\nlarge-scale testing data generation\\ntool construction graph rcnn\\nregion based convolutional neural network\\ncandidate part pairs\",\"925\":\"training\\nunmanned aerial vehicles\\ncorrelation\\nreliability\\nreal-time systems\\noptimization\\nvisualization\\nautonomous aerial vehicles\\ncorrelation methods\\nimage filtering\\nimage motion analysis\\nminimisation\\nmobile robots\\nobject detection\\nobject tracking\\nrobot vision\\ntraining-set distillation\\nuav object tracking\\ncorrelation filter\\nvisual object tracking\\nunmanned aerial vehicle\\nenergy minimization function\\nscoring process\\ntime slot-based distillation approach\",\"926\":\"decoding\\npropagation losses\\nestimation\\ntraining\\nimage reconstruction\\ntask analysis\\nscattering\\ncomputer vision\\nconvolutional neural nets\\ncorrelation methods\\nimage coding\\nimage colour analysis\\nimage denoising\\nimage representation\\nimage sensors\\nlearning (artificial intelligence)\\nspatial variables measurement\\nsingle hazy rgb input\\nsingle dense encoder\\nencoded image representation\\ndehazing image depth estimation\\nsingle image depth estimation\\nimage dehazing\\nconvolutional neural networks\\ncnn\\ndehazing depth estimation algorithms\\ntraditional haze modeling\\ndepth estimation network\\nfully scaled depth map\\ndepth-transmission consistency loss\\nseparate decoders\",\"927\":\"collision avoidance\\nmanipulators\\nsurgery\\ntask analysis\\nforce\\ntools\\ncontrol engineering computing\\nend effectors\\nhuman-robot interaction\\ninternet of things\\nmedical robotics\\nmotion control\\nphantoms\\nredundant manipulators\\ntelerobotics\\nthings-based collaborative control\\nteleoperated minimally invasive surgeries\\nrobot-assisted minimally invasive surgery scenario\\nhierarchical operational space formulation\\n7-dofs redundant manipulator\\nmultiple operational tasks\\nmotion constraint\\nundergoing surgical operation\\ninternet of robotic things\\ncompliant swivel motion\\nhtc vive pro controllers\\nrobot elbow\\nsmooth swivel motion\\nkuka lwr4+ slave robot\\nsigma 7 master manipulator\\ninternet of things-based human-robot collaborative control scheme\\npriority levels\",\"928\":\"legged locomotion\\ndynamics\\nnonlinear dynamical systems\\nrobot sensing systems\\nhardware\\ngait analysis\\nmechanical stability\\npendulums\\nrobot dynamics\\npassive dynamic balancing\\npassive dynamic systems\\ndynamic behaviors\\nactuated robots\\nrobotic assistive devices\\nrobotic systems\\npassive system\\npassive bipedal robot\\ndynamically stable periodic walking gaits\\npassive dynamic walking\",\"929\":\"mathematical model\\nlips\\nthree-dimensional displays\\nfeedback control\\nlegged locomotion\\ntrajectory\\nfeedback\\nhumanoid robots\\nnonlinear control systems\\npendulums\\nrobot dynamics\\nstability\\nstabilization\\nlinear feedback\\nvariable-height\\npendulum model\\nbalancing strategy\\nheight variations\\nwell-known ankle strategy\\nbiped stabilizer\\ninput feasibility\\nstate viability constraints\\nresulting stabilizer\",\"930\":\"conferences\\nautomation\\ngait analysis\\nhumanoid robots\\nlegged locomotion\\nposition control\\nrobot dynamics\\nstability\\nsteppable unbalanced state boundary\\nfull-body systems\\ndouble support contact configurations\\nsteppable states\\nbiped system\\nfull-order nonlinear system dynamics\\ndarwin-op humanoid robot\\nbalance stability boundaries\\nds contact configuration\\nhuman gait\",\"931\":\"humanoid robots\\nlegged locomotion\\nfriction\\nrobot sensing systems\\nwrist\\nforce\\nmaterials handling\\nmotion control\\nrobot dynamics\\nrobot self balance\\nzero moment point pattern\\nwalking pattern dynamic model\\narm compliance\\nfriction compensation\\ncapture point method\\ncart pushing\\nhumanoid robot\\nmaterial handling\",\"932\":\"legged locomotion\\npotential energy\\ndamping\\ntransmission line matrix methods\\nmathematical model\\nrobustness\\ncontrol system synthesis\\ngait analysis\\nmotion control\\nrobot dynamics\\nrobot kinematics\\nrobust control\\ninterconnection and damping assignment passivity-based control\\ngait generation\\ncompass-like biped robot\\ndynamic parameter\\nport-hamiltonian framework\\ncontroller discretization\\nparametric uncertainties\",\"933\":\"robot kinematics\\ncollision avoidance\\nplanning\\ntrajectory\\nheuristic algorithms\\ncouplings\\nmobile robots\\nmulti-robot systems\\npath planning\\npath prospects\\nprioritization rule\\nheterogeneous robot teams\\nmultirobot path deconfliction\\nconflict-free path planning\\nconflict-free path plans\\nprioritization heuristics\",\"934\":\"prediction algorithms\\nrobot sensing systems\\nplanning\\nuncertainty\\ncomputational modeling\\ngaussian distribution\\nmarkov processes\\nmobile robots\\nmotion control\\nmulti-robot systems\\npath planning\\nprobability\\ninfinite power series expansion theorem\\nfinite-term approximation\\ncomputational feasibility\\nadverse impacts\\nhigher order series terms\\nactive cl\\nleader-follower architecture\\nmarkov decision process\\none-step planning horizon\\noptimal motion strategy\\nmdp model\\nconnectivity-prediction algorithm\\nmultirobot systems\\nfuture connectivity\\nrange-limited communication\\nactive motion planning\\nquadratic forms\\nrandom normal variables\",\"935\":\"collision avoidance\\nrobot kinematics\\ntask analysis\\nsafety\\nmulti-robot systems\\nreal-time systems\\ndistributed algorithms\\nmobile robots\\ntrees (mathematics)\\nlarge-scale multirobot systems\\nrobot team\\nconnected communication graph\\nminimum inter-robot connectivity constraints\\nactivated connectivity constraints\\nbehavior mixing controllers\\ndistributed minimum connectivity constraint spanning tree algorithm\\nprovably minimum connectivity maintenance\\nsubgroup connectivity maintenance\\nminimum global connectivity maintenance\\ndistributed mccst algorithm\",\"936\":\"grasping\\nforce\\nrobots\\ndynamics\\nacceleration\\njacobian matrices\\ntask analysis\\ncooperative systems\\ndecentralised control\\nforce control\\nmanipulator dynamics\\nmobile robots\\nmulti-robot systems\\nposition control\\ninternal-force regulation\\noptimal cooperative robotic manipulation problem\\nenergy resources\\nrigid cooperative manipulation systems\\nrigid grasping contacts\\nenergy-optimal conditions\\narising internal forces\\ninter-agent forces\\nclosed form expression\\nstandard inverse dynamics\\nforce distribution\\nrobotic agents\\nnonzero inter-agent internal force vector\\ninternal force minimization\",\"937\":\"robot sensing systems\\nend effectors\\ntask analysis\\neducation\\nservice robots\\ncollaboration\\nhaptic interfaces\\nhuman-robot interaction\\nindustrial manipulators\\nindustrial robots\\nsensors\\nrobot control\\ndangerous industrial robots\\ncollaborative robots\\nteaching pendant\\ndirect teaching\\nnovel robot interaction technique\\nrobot arm\\nunimanual hand gestures\\nbimanual hand gestures\\nrobot telekinesis\\nbimanual object manipulation technique\\nunimanual object manipulation technique\\nproduction lines\",\"938\":\"task analysis\\nrobot kinematics\\njacobian matrices\\naerospace electronics\\nmanipulators\\nsafety\\noptimisation\\nredundant manipulators\\nsafety-critical software\\nset theory\\ntime-varying systems\\nconstrained optimization problem\\nredundant robotic manipulator\\nset theoretic approach\\nmultitask execution\\nsafety critical tasks\\nrobotic system\\noptimization based task execution\\nset based tasks\\ntime varying priorities\\nmultitask prioritization\\ncontrol barrier functions\",\"939\":\"aerospace electronics\\ntask analysis\\ntrajectory\\njacobian matrices\\nmanipulators\\nservice robots\\ncollision avoidance\\nfeedback\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmobile robots\\nmotion control\\nvariable impedance control\\ncartesian latent space\\nnull space\\nassistive robots\\ncartesian impedance control\\njoint control\\ndesired interaction\\nenvironmental feedback\\nrobot arm\\noperational space\\nvariable stiffness\\nprecision requirements\\ndimensionality reduction\\nfreedom relevant\\nredundant ones\\ntask-redundant dof\\nhuman head\",\"940\":\"grasping\\nthree-dimensional displays\\nneurons\\ncameras\\nrobot vision systems\\ndexterous manipulators\\ninfrared spectra\\nobject detection\\nrobot vision\\ntarget object\\ngrasping poses\\ngrasp strategies\\nmagichand system\\ncontext-aware dexterous grasping\\nrobotic grasping\\ncontext-aware anthropomorphic robotic hand grasping system\\nnir spectra\\nmolecular level\\nrgb-d images\\ndexterous grasping\\ncharacteristics of objects recognition\\nnir spectrum\",\"941\":\"grasping\\nrobustness\\ntraining\\ngrippers\\nsensors\\nend effectors\\nfeedback\\nhumanoid robots\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nneural nets\\nrobot vision\\nungraspable poses\\nrobotic grasping\\nmodel-free deep reinforcement learning\\nfeedback control policies\\nvisual information\\nrobot arm\\nobject-table clearance\\npregrasp manipulation learning\\nhuman bimanual manipulation\",\"942\":\"grippers\\nshape\\nsolids\\nsurface treatment\\nhardware\\nrobot sensing systems\\ndexterous manipulators\\nend effectors\\nindustrial manipulators\\nmanipulator kinematics\\nrobot vision\\nconvex polygonal objects\\nbin picking scenarios\\nmanipulation process\\nrobotic dexterous in-hand manipulation\\ntilt-and-pivot manipulation\\nthin object picking\\ntilt-and-pivot kinematics\\ntilt-and-pivot planning\",\"943\":\"instruments\\nconvolution\\nsemantics\\ncomputational efficiency\\ndecoding\\nsurgery\\nreal-time systems\\nconvolutional neural nets\\nedge detection\\nimage capture\\nimage coding\\nimage segmentation\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nrobot vision\\nrobotic surgical instruments\\nrobot-assisted surgery\\ndeep learning models\\nattention-guided lightweight network\\nlwanet\\nlightweight network mobilenetv2\\ndepthwise separable convolution\\ntransposed convolution\\nsurgical instrument\\nencoder-decoder architecture\\nattention fusion block\",\"944\":\"breast\\nprobes\\ntrajectory\\nsafety\\nrobot kinematics\\nskin\\nbiological tissues\\nbiomedical ultrasonics\\nfeedback\\nimage registration\\nmedical image processing\\nmedical robotics\\nphantoms\\nsurgery\\nvisual servoing\\nrobotic 3d breast us acquisitions\\nus feedback\\nvisual servoing algorithm\\npatient specific scans\\ntissue deformations\\nus probe\\nultrasound feedback\\nautomated robotic breast ultrasound acquisition\",\"945\":\"three-dimensional displays\\nsurgery\\nrobustness\\nprobes\\nprobabilistic logic\\nrobots\\nbiomedical imaging\\nbiomechanics\\nimage reconstruction\\nimage registration\\nmedical image processing\\nimage-guided surgery\\npre-to-intraoperative registration\\nintra-operative 3d data\\ntangent vectors\\nsparse intraoperative data points\\npre-operative model points\\nprobabilistic distribution\\nmultidimensional point\\nmaximum likelihood problem\\nrigid registration\\nintraoperative point\\nmean target registration error value\\nsize 0.6795 mm\",\"946\":\"instruments\\nrobots\\npose estimation\\nsurgery\\nshafts\\nendoscopes\\ncollision avoidance\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nneural net architecture\\nrobot vision\\nsingle-shot pose estimation\\nmonocular endoscopic images\\nminimally invasive surgery\\ncollision-avoidance algorithm\\nonline estimation\\nmonocular endoscope\\nart vision-based marker-less\\nposition estimation\\nsurgical robot instrument shafts\\nannotated training dataset\\nimproved pose-estimation deep-learning architecture\",\"947\":\"catheters\\nimage segmentation\\nx-ray imaging\\nreal-time systems\\nmachine learning\\nmotion segmentation\\nfeature extraction\\nimage sequences\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nneural nets\\nsurgery\\ndeep learning framework\\nsegmentation network\\nencoder-decoder architecture\\nflow network\\noptical flow information\\nframe-to-frame temporal continuity\\ncatheter segmentation\\nrobot-assisted endovascular intervention\\nflow-guided warping function\\noptical flow-guided warping\",\"948\":\"three-dimensional displays\\natmospheric modeling\\ntraining\\ntwo dimensional displays\\ncomputed tomography\\nimage segmentation\\nsolid modeling\\ncomputerised tomography\\nlung\\nmedical image processing\\nneural nets\\ncascaded 2d-3d model\\npathological ct scans\\n3d adversarial training model\\nnovel 2d neural network\\nairway tree\\npathological abnormalities\\npreoperative chest ct scans\\npatient-specific airway maps\\nperipheral airways\\nenhanced visualisation\\n3d airway maps\\nrobotic bronchoscopic intervention\\nbronchoscopic navigation\\ncascaded neural networks\\npathological airway segmentation\",\"949\":\"gears\\nrobots\\npins\\nshape\\nthree-dimensional displays\\nthumb\\nprinters\\nassembling\\ndesign engineering\\nplastics\\nthree-dimensional printing\\npart-joining quality\\ndesign method\\nrobotic hand\\n3d-printed assembly mechanisms\\nrobotic systems\\nplastic materials\\njapanese wooden joinery techniques\\nassembling 3d-printed parts\",\"950\":\"grippers\\ngears\\nnails\\nthumb\\nforce\\npiezoelectric transducers\\nforce control\\nimpact (mechanical)\\ngripping force\\nproduct height\\nimpact force\\nstacked rack-and-pinion system\\nmechanism verification\\nparallel gripper\\ngripper displacement-magnification mechanism\\nextendable finger mechanism\\nsize 95.0 mm\\nsize 110.0 mm\\nsize 214.0 mm\\nmass 1.36 kg\\nsize 60.0 mm\\nvelocity 100.0 mm\\/s\",\"951\":\"switched mode power supplies\\ngrippers\\nadhesives\\nheating systems\\nshape\\nrough surfaces\\nsurface roughness\\nadhesion\\ncontrol system synthesis\\nmanipulators\\npolymers\\nshape memory effects\\nshape memory polymer adhesive gripper\\nsmart adhesive applications\\npick-and-place applications\\nreversible dry adhesion\\ngecko grippers\\nhigh adhesion strength\\nsmp adhesive mechanics\\nreversible dry adhesive properties\\nsingle surface contact grippers\\nsmp adhesive gripper\",\"952\":\"uncertainty\\nprobabilistic logic\\nadaptive optics\\noptical imaging\\npose estimation\\noptical sensors\\npredictive models\\nimage matching\\nmonte carlo methods\\nneural nets\\nprobability\\nmultiperson pose tracking\\nsequential monte carlo\\nprobabilistic neural pose predictor\\nuncertainty-aware modeling\\ncritical tracking errors\\ntracking scheme\\nmultiple predictions\\nprediction errors\\nproposal distribution\\nepistemic uncertainty\\nheteroscedastic aleatoric uncertainty\\nneural modeling\\nmota score\\nposetrack2018 validation dataset\\npose matching\\ntime-sequence information\",\"953\":\"proposals\\nelectron tubes\\nthree-dimensional displays\\nimage segmentation\\nvideo sequences\\nmotion segmentation\\nprobabilistic logic\\nfeature extraction\\nimage motion analysis\\nobject detection\\nstereo image processing\\nvideo signal processing\\nstereo video\\n4d-gvt\\ndata-driven object instance segmentation\\nneural networks\\nspatio-temporal object proposals\\n4d generic video tubes\\nobject categories\\n4d generic video object proposals\",\"954\":\"strain\\nelasticity\\nestimation\\ndeformable models\\nforce measurement\\nforce\\nforce sensors\\ndeformation\\nfinite element analysis\\nimage colour analysis\\nmanipulators\\nparameter estimation\\nrobot vision\\nvisual information\\nsimulated object\\nelasticity parameter estimation\\ntracked object\\nsoft objects\\ndeformable object\\nsimultaneous tracking\\ninteractive finite element method simulations\\nrgb-d sensor\\nrobotic manipulation\",\"955\":\"object tracking\\nobject detection\\nvisualization\\nneural networks\\nmachine learning\\nfeature extraction\\nstreaming media\\naudio signal processing\\naudio-visual systems\\nlearning (artificial intelligence)\\nrobot vision\\ntracking\\nmultiple objects\\nvisually based trackers\\nobject collisions\\naudio-visual object tracking neural network\\ntracking error\\navot end\\naudio-visual inputs\\nvisually based object detection\\ntracking methods\\nopencv object tracking implementations\\ndeep learning method\\naudio-visual dataset\\nsingle-modality deep learning methods\\naudio onset\\nmultimodal object tracking\\nstate-of-the-art object tracking\",\"956\":\"cameras\\ntracking\\nrobot vision systems\\ninspection\\nskeleton\\npipelines\\nagricultural robots\\nconvolutional neural nets\\ndistributed processing\\nfarming\\nimage filtering\\nimage matching\\nimage sensors\\nobject detection\\nobject tracking\\nrobot vision\\nefficient pig counting\\nlarge-scale pig farming\\nautomated pig counting method\\npig movements\\npig grouping houses\\nreal-time automated pig counting system\\npig detection algorithm\\ndeformable pig shapes\\npig body part\\nkeypoints tracking\\nspatial-aware temporal response filtering\\npig occlusion\\npig overlapping\\nmonocular fisheye camera\\ninspection robot\\ndeep convolution neural network\\nkeypoints association\\nefficient on-line tracking method\\ntracking failures\\nedge computing device\",\"957\":\"three-dimensional displays\\npose estimation\\nrobustness\\nrobots\\nreal-time systems\\ntracking\\nvisualization\\nclosed loop systems\\nimage colour analysis\\nlearning (artificial intelligence)\\nobject detection\\nobject tracking\\nrobot vision\\nstereo image processing\\nhttps:\\/\\/sites.google.com\\/view\\/6packtracking\\nphysical robot\\ninterframe motion\\n3d keypoints\\nreal time novel object instances\\nrgb-d data\\nnocs category-level 6d pose estimation benchmark\\nkeypoint matching\\nobject instance\\nknown object categories\\ndeep learning approach\\nanchor-based keypoints\\ncategory-level 6d pose tracker\\n6-pack\\nsimple vision-based closed-loop manipulation tasks\",\"958\":\"soft magnetic materials\\noptimization\\ntopology\\nmagnetic domains\\nmagnetoacoustic effects\\nlevel set\\nshape\\nactuators\\ndeformation\\ndesign engineering\\nelastomers\\ngrippers\\noptimisation\\nsensitivity analysis\\nshape sensitivity analysis\\ngripper\\nflytrap structure\\nmaterial layout\\ninnovative structures\\nbionic medical devices\\ncompliant actuators\\nlevel-set-based multiphysics topology optimization method\\nadjoint variable method\\nmaterial time derivative\\nsub-objective function\\narchitect ferromagnetic soft active structures\\ndesign domain\\nstructural topology optimization\\nferromagnetic soft elastomers\\nexternal magnetic field\\nshift morphology\\nsoft elastomer matrix\\nferromagnetic particles\\nflexible electronics\\nsoft machines\\nexternal environmental stimulus\\nchange configurations\\nflexible locomotion\\nsoft active materials\\nferrosoro\\ndesigning ferromagnetic soft robots\",\"959\":\"soft robotics\\ncameras\\nrobot vision systems\\nink\\nactuators\\nconvolutional neural nets\\ndexterous manipulators\\ngrippers\\nintelligent sensors\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\ntactile sensors\\nhigh-resolution proprioceptive sensing\\nrich tactile sensing\\nhighly underactuated exoskeleton\\nrobotic gripper\\ntactile information\\nproprioception cnn\\nhuman finger proprioception\\nproprioceptive state\\nperipheral environment\\nvision-based proprioception\\nrigid-body robots\\nsoft robots\\naccurate proprioception\\nelasticity\\ntactile sensor\\nprevious gelsight sensing techniques\\nexoskeleton-covered soft finger\\nsize 0.77 mm\",\"960\":\"soft robotics\\nsprings\\npotential energy\\nshape\\nactuators\\nplastics\\ndeformation\\ndesign engineering\\nflexible manipulators\\ngrippers\\nmobile robots\\npneumatic actuators\\nenergy landscape\\nsoft body structures\\nfast motion\\nstrong motion\\nsoft module\\nsoft bistable module\\nfast robots\\nstrong soft robots\\nsoft gripper\\nsoft jumping robot\\nsoft actuator\\ntwisted-and-coiled actuator\",\"961\":\"potential energy\\ncomputational modeling\\nenergy measurement\\nsprings\\nservomotors\\nrobots\\nfaces\\ndeformation\\nfinite element analysis\\nlegged locomotion\\npneumatic actuators\\nshear modulus\\nsprings (mechanical)\\ncustom release mechanisms\\nquick compression\\nfold pattern\\ncontrollable jump height\\njumping maneuvers\\ncontrol strategies\\nrobot body\\nmodel fold patterns\\npotential energy storage\\nparametric origami tessellation\\nface deformations\\nnonlinear spring\\npseudorigid-body model\\nmechanical testing system\\nstored potential energy\\nreconfigurable expanding bistable origami pattern\\nuntethered origami jumping robot\\nrebound robot\",\"962\":\"muscles\\ncrosstalk\\nfeature extraction\\nwrist\\npattern recognition\\nmicrosoft windows\\nelectrodes\\nelectromyography\\ngesture recognition\\nmedical signal processing\\nmuscle\\npattern classification\\nsemg signals\\nmotion intensity feature\\ngrasping motions\\nmotion intensity extraction scheme\\nsurface electromyography\\nmuscular information representing gestures\\nsemg-based motion recognition methods\",\"963\":\"task analysis\\nprobes\\ntraining data\\nhidden markov models\\ntorso\\nreal-time systems\\nfeature extraction\\nbiomechanics\\nimage motion analysis\\nlearning (artificial intelligence)\\npatient rehabilitation\\nsynergy probe\\nwhole-body exercise\\nsimultaneous online motion discrimination\\nhome rehabilitation sessions\\nreconstructed movement\\nonline data\",\"964\":\"belts\\nlegged locomotion\\npelvis\\nkinematics\\ntraining\\nforce\\nforce control\\ngait analysis\\nmedical robotics\\nmobile robots\\nmotion control\\npatient rehabilitation\\nrobot kinematics\\nmobile tpad frame\\ntreadmill walking\\nmotor control patterns\\nopen loop controller\\ntreadmill based robotic trainer\\npelvic force augmentation\\nmobile tethered pelvic assist device\\nforward kinematics controller\\nmobile device controller\\nrehabilitation robotic devices\\noverground gait training robotic devices\",\"965\":\"muscles\\nwrist\\niron\\nforce\\nrobots\\nground penetrating radar\\ndata models\\nbiomechanics\\nbiomedical electrodes\\nfeedback\\nfeedforward\\ngaussian processes\\nhandicapped aids\\nlearning (artificial intelligence)\\nmedical control systems\\nmedical robotics\\nneuromuscular stimulation\\nneurophysiology\\npatient rehabilitation\\nregression analysis\\nparalyzed human arm\\nfunctional electrical stimulation\\nrestoring reaching ability\\ntetraplegia\\nshoulder-arm complex\\nfull-arm 3d reaching tasks\\ngaussian process regression model\\nfeedforward-feedback control structure\\nparalyzed upper limb\\nfes-driven 3d reaching controller\",\"966\":\"transient analysis\\nrobots\\nsteady-state\\ntask analysis\\ndynamics\\nforce\\nhaptic interfaces\\nfeedback\\nfeedforward\\nhuman-robot interaction\\nmanipulator dynamics\\nmotion control\\nvirtual reality\\nrobot control\\ninternal dynamics\\ntransient behavior\\npredictable dynamics\\nvirtual object\\nrobotic manipulandum\\npredictable steady state\\nfeedforward controller\\ninverse dynamics\\nhaptic feedback\",\"967\":\"admittance\\ntask analysis\\nrobot sensing systems\\ncollaboration\\nstability criteria\\naugmented reality\\ncontrol engineering computing\\ndrilling\\ngroupware\\nhuman-robot interaction\\nindustrial robots\\nproduction engineering computing\\nuser interfaces\\nphri\\nlabor intensive tasks\\nfractional order control\\nfractional order variable admittance controller\\ninteger order variable admittance controller\\nrealistic drilling task\\ntransparent interaction\\naugmented reality headset\\nhuman sensory capabilities\\nvariable-fractional order admittance controller\\nautomation driven manufacturing environments\\ncollaborative robots\\naugmented reality interfaces\\nproduction workflow\\ncognitive skills\\nphysical human robot interaction\\ncobots\\nstability\\ndrilling depth\",\"968\":\"task analysis\\nmanipulators\\nphysics\\ntools\\nhuman-robot interaction\\nmobile robots\\nlearning (artificial intelligence)\\nmedical robotics\\nrobot programming\\nservice robots\\nphysics simulation framework\\nautonomous robots\\nphysical interaction\\nphysics simulations\\nphysical assistance\\nopen source physics\\nassistive robots\\nsimulated environments\\nrobotic manipulator\\nassistive gym models\\ncommercial robots\\nassistive robotics research\\nadl\\nactivities of daily living\",\"969\":\"robot sensing systems\\nhaptic interfaces\\nforce\\nspatiotemporal phenomena\\ntraining\\ncontrol engineering computing\\nforce sensors\\nhuman-robot interaction\\nlearning (artificial intelligence)\\ntelerobotics\\nwhole-body human-robot haptic interaction\\nlearning-from-demonstration framework\\nhuman-robot social interactions\\nhuman-robot contact\\nlfd framework\\nteleoperated bimanual robot\",\"970\":\"damping\\nmanipulators\\ncollaboration\\njacobian matrices\\nkinematics\\ncollision avoidance\\nhuman-robot interaction\\nmanipulator kinematics\\nmobile robots\\nkinematic singularities\\ndamping-based strategy\\nhuman operator\\nhuman preferences\\nphysical human-robot collaboration\\nrobot manipulator\\nkinematic singular configuration\\ndouble-blind a\\/b pairwise comparison testing protocol\\nsingularities handling\",\"971\":\"admittance\\ntask analysis\\ncollaboration\\nrobot sensing systems\\nclamps\\nmanipulators\\ngesture recognition\\nhuman-robot interaction\\nmedical robotics\\nmobile robots\\nmulti-robot systems\\nmobile manipulators\\nsupernumerary limbs\\nreconfiguration potential\\nmobile collaborative robot assistant\\nsupernumerary body\\nmoca-man\\nhand gesture recognition system\\nmobile base\\nlong distance co-carrying tasks\\nmanipulating tools\\nconjoined actions\\nperforming heavy manipulation tasks\\nprolonged manipulation tasks\\nclose-proximity manipulation\\nmobile robot assistant\\nreconfigurable collaborative robot assistant\\nconjoined human-robot actions\\ncollaborative robotic system\",\"972\":\"force\\niron\\nforce measurement\\nrobots\\nstability analysis\\ndelays\\ntask analysis\\nforce control\\nhuman-robot interaction\\nstability\\ntelerobotics\\nmaster-slave teleoperation system\\nbilateral controllers\\nforce transparency\\nforce loop\\ntime delayed teleoperation\\nkuka lightweight robots\\ntime domain passivity\",\"973\":\"exoskeletons\\ntask analysis\\ncomputer architecture\\nstability analysis\\nrobots\\ndelays\\nhaptic interfaces\\nmotion control\\nstability\\ntelerobotics\\nsimulated time delay\\ncontrol loop frequency\\nmultidofs devices\\ntdpa\\ntime domain passivity approach\\nexoskeletal master\\nbimanual teleoperation system\\ncommunication delay\\nbilateral teleoperation\\nhaptic feedback\\nrobotic platforms\\nrescue robotics\\nindependently passivated slave devices\\nexoskeleton-based bimanual teleoperation architecture\",\"974\":\"haptic interfaces\\ndrones\\ntask analysis\\nrobot sensing systems\\nhardware\\nautonomous aerial vehicles\\ndata gloves\\nhuman-robot interaction\\nmobile robots\\nmotion control\\nrobot vision\\ntelerobotics\\ntrajectory control\\ndrone teleoperation\\nremote radio controllers\\nwearable interface\\ndrone trajectory\\nhand motion\\nhaptic system\\nrobotic systems\\nteleoperation performance\\nremote controllers\\nhuman-robot interfaces\\nhand-worn haptic interface\\ndata glove\\nline of sight\\nsearch-and-rescue missions\",\"975\":\"task analysis\\ntraining\\ndynamics\\njoints\\nrobot motion\\nmanipulators\\ncontrol engineering computing\\nhuman-robot interaction\\nmobile robots\\nservice robots\\ntelerobotics\\nvirtual reality\\nremotely-operated robot\\nremote telepresence\\nbaxter robot\\nxbox one controller\\njbj\\nlimb\\nmultiple joints\\nsuccessfully completed tasks\\njoint-by-joint method\\nchoreography-inspired method\\nperformance data\\nstatic tasks\\nrcc method\\ndynamic tasks\\nhuman-likeness\\nrobotic motion\\nteleoperated robot motion\\nrapid pose selection\\narticulated robots\\nrobot choreography center\",\"976\":\"robots\\ngrasping\\ntask analysis\\nthree-dimensional displays\\nsolid modeling\\nvirtual reality\\npipelines\\ncontrol engineering computing\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmulti-robot systems\\nrobot programming\\ntelerobotics\\ngrasping task\\nhuman perception\\nhuman-robot master-apprentice model\\nvirtual reality teleoperation\\nartificial intelligence\\nself-supervised learning\",\"977\":\"tools\\nfeature extraction\\ntask analysis\\nrobots\\ndetectors\\nthree-dimensional displays\\nhaptic interfaces\\nfeature selection\\ntelecontrol\\nvirtual reality\\nunstructured environments\\nhuman operator performance\\nremote environment\\ntask execution\\nuser interface\\ncamera images\\ninteractive selection\\ninteractive virtual fixture generation\\nshared teleoperation\\n6-dof haptic feedback\",\"978\":\"safety\\ncollision avoidance\\nmobile robots\\nlead\\npath planning\\nrobot kinematics\\nautomatic guided vehicles\\ncurve fitting\\nnavigation\\noff-road vehicles\\npredictive control\\nstability\\nsteering systems\\nvehicle dynamics\\nlocal obstacle-skirting path planning\\nobstacle avoidance\\noff-road mobile robots\\nglobal reference path\\nsmooth path\\nlateral stability\\ndouble-steering rover\\nonline cubic bezier curves\\nbi-steerable rover\\nconstrained model predictive control\\nautonomous guided vehicles\",\"979\":\"collision avoidance\\nrobot sensing systems\\ntask analysis\\nmanipulators\\nskin\\nhuman-robot interaction\\nmotion control\\nquadratic programming\\nrepulsive motions\\ninstantaneous optimal joint velocities\\nquadratic optimization problem\\nproximity sensing skins\\nlow-latency perception\\nproximity sensors\\nfastreacting motions\\nsafe human-robot interaction\\nredundant serial robot manipulators\\nproximity servoing\",\"980\":\"two dimensional displays\\ncollision avoidance\\nimage segmentation\\ntraining\\nrobot sensing systems\\nconvolutional neural nets\\nlaser ranging\\nlearning (artificial intelligence)\\nmobile robots\\nnavigation\\nrobot vision\\nindoor robot localization\\nobstacle avoidance\\nlaser scanners\\ncollision events\\n2d occupancy maps\\nobstacle footprint prediction\\nphysical interaction learning\\nhorizontal scanning 2d laser range finders\\nconvolutional neural network\",\"981\":\"robots\\npredictive models\\npath planning\\ndecoding\\ncollision avoidance\\ntraining\\ndynamics\\nlearning (artificial intelligence)\\nmobile robots\\nmonte carlo methods\\nrecurrent neural nets\\ntree searching\\nmonte carlo tree search\\ngenerative recurrent neural networks\\nintegrated path\\nmotion models\\ntraffic\\nrobotic path planning\\ndynamic environments\\neffective path planning\\nmotion prediction accuracy\\nplanned robotic actions\\ngenerative rnns\\naction space\\ncrowd dynamics\\nsocial response\\nlearnt model\",\"982\":\"safety\\ncollision avoidance\\nthree-dimensional displays\\ndrones\\ntrajectory\\nvehicle dynamics\\nplanning\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\nsensors\\nuncertain systems\\nhigh-speed flight\\nuncertain environments\\ncontroller level\\nstate uncertainty\\nnonlinear system dynamics\\nhigh-fidelity simulation\\ncave environment\\nsafety-critical rapid aerial exploration\\naerial vehicles\\nunknown environments\\nquadrotor\\nonboard sensors\",\"983\":\"ultrasonic imaging\\nmagnetic resonance imaging\\nconvection\\ncoagulation\\ncoils\\nbiochemistry\\nbiological tissues\\nbiomedical materials\\nbiomedical ultrasonics\\nblood\\nmagnetic particles\\nmicrorobots\\nnanomedicine\\nnanoparticles\\npatient treatment\\nreconfigurable magnetic microswarm\\nthrombolysis\\nultrasound imaging\\nmagnetic nanoparticle microswarm\\ntissue plasminogen activator\\noscillating magnetic field\\naspect ratio\\nout-of-plane fluid convection\\nlysis rate\\nmicroswarm-induced fluid convection\\nmicro\\/nanorobot\\nmagnetic control\\ncollective behavior\",\"984\":\"optical feedback\\noptical sensors\\nforce\\nhaptic interfaces\\noptical imaging\\nthree-dimensional displays\\nhigh-speed optical techniques\\ncontrol engineering computing\\nforce feedback\\nmicromanipulators\\nradiation pressure\\nrobot vision\\ntelerobotics\\nhaptic device\\ntransparent force feedback\\nuser dexterity\\nmicrosized shapes\\ncontact forces\\noptical micromanipulation\\nforce-feedback bilateral coupling\\ninteractive approaches\\nvisual feedback\\nhaptic feedback teleoperation systems\\noptical tweezers platform\\n2d image\",\"985\":\"robots\\nresonant frequency\\nactuators\\nvibrations\\nsteady-state\\nwires\\nmathematical model\\nmicroactuators\\nmicrorobots\\npiezoelectric actuators\\nsteering systems\\nresonance-based steering mechanism\\ndifferential steering\\non-board piezoelectric actuator\\nfrequency-controlled locomotion\\nsteering mechanism\\nmicrobristle robots\\nactuation frequency control\\nprincipal actuation frequency components\\nsize 6.0 mm\\nsize 400.0 mum\\nsize 12.0 mm\\nsize 8.0 mm\",\"986\":\"legged locomotion\\nactuators\\nresonant frequency\\nheat-assisted magnetic recording\\nfabrication\\nrobot sensing systems\\ngait analysis\\nmicrorobots\\nmotion control\\ninsect-size microrobot\\nmechanically dexterous legged robot\\nhamr-jr's open-loop locomotion\\nhamr-vi microrobot\\ndesign process\\nfabrication process\\nindependently actuated degrees of freedom\\nmass 320.0 mg\\nfrequency 1.0 hz to 200.0 hz\\nsize 22.5 mm\",\"987\":\"robot sensing systems\\nsoftware\\nsensor systems\\nmobile robots\\nemulation\\ncollision avoidance\\nhuman-robot interaction\\nmulti-robot systems\\nfundamental limits\\nphysical demonstration\\nrobot behavior\\npotential mismatches\\nconvincing illusion\\nsystem simulation\\nsimulated systems\\nsimple multirobot experiment\\nrobot navigating\\nrobot illusions\",\"988\":\"task analysis\\nmaintenance engineering\\ngames\\nrobot control\\nmanipulators\\nsafety\\nrobots\\ntemporal logic\\nkuka iiwa arm\\nbaxter robot\\nltl specifications\\ncorrect-by-construction robot control\\nltl synthesis\\nhigh-level robot tasks\\nlinear temporal logic\",\"989\":\"runtime\\nswitches\\ngames\\nrobots\\nmeasurement\\nsafety\\nplanning\\ncontrol system synthesis\\nmobile robots\\noptimisation\\npath planning\\nsuboptimal control\\nmission specification\\ndynamic environment\\nperformance metric\\ntask-critical information\\nstrategy synthesis\\ntime-varying information\\nonline re-synthesis\\npre-specified representative information scenarios\\nperformance suboptimality\\nruntime information\\nnear-optimal reactive synthesis\\nswitching mechanism\\nrobotic motion planning\",\"990\":\"learning (artificial intelligence)\\nautomata\\nplanning\\nmarkov processes\\ncomputational modeling\\ntask analysis\\ncontrol system synthesis\\ndecision theory\\nmobile robots\\npath planning\\nprobability\\ntemporal logic\\nmotion planning\\nmdp\\nrl-based synthesis approach\\ndiscount factors\\nmodel-free rl algorithm\\ntotal discounted reward\\noptimal policy\\ntransition probabilities\\nltl formula\\nmarkov decision process\\nunknown stochastic environment\\ncontrol policy synthesis\\nreinforcement learning frame-work\\nmodel-free reinforcement learning\\nlinear temporal logic specifications\\ncontrol synthesis\",\"991\":\"collision avoidance\\ncollaboration\\nprototypes\\nparallel robots\\nrobot sensing systems\\nsprings\\nend effectors\\nhuman-robot interaction\\nsprings (mechanical)\\ntrajectory control\\nparallel manipulator\\npick-and-place operations\\nhuman operator\\nacceleration\\nplanar five-bar mechanism\\npassive joints\\nplanar seven-bar mechanism\\nsupplementary passive leg\\ncollaborative parallel robot\\npick-and-place trajectory\\nr-min\\ncollaborative underactuated parallel robot\\ntension spring\\nend-effector\\ndegrees of freedom\\nimpact force\",\"992\":\"legged locomotion\\nwheels\\ntorso\\nforce\\nkinematics\\nmobile robots\\nmotion control\\nrobot kinematics\\nhigh-flexibility locomotion\\nwhole-torso control\\nchallenging terrain\\nsix-wheel-legged robot\\nirregular terrain\\nheavy-duty work\\nstewart platforms\\ndiverse degrees\\ntraversability\\nrough terrain\\nsand-gravel terrain\\nparallel suspension system\",\"993\":\"meters\\nlength measurement\\nwinches\\npowders\\nkinematics\\nrobot kinematics\\nexhibitions\\nmobile robots\\ntrajectory control\\ncable length estimation\\nrobot position\\nglass powder\\ncable-driven parallel robot\\nartistic exhibition\\npositioning control\\ncdpr geometry\\nexhibition place\\nprince tears\\ntime 3.0 d\\ntime 174.0 hour\\ntime 32.0 d\\nmass 1.5 ton\\nart\",\"994\":\"manipulators\\nkinematics\\ntransmission line matrix methods\\nforce\\ngeometry\\nprototypes\\nmanipulator kinematics\\nreconfiguration mode\\n3-crs parallel manipulator\\noriginal parallel mechanism\\nmotorized cylindrical joint\\nparallel robotics community\\ndimensional synthesis\\ngeometric approach\\nrelative geometric configurations\\nsingularity analysis problem\",\"995\":\"kinematics\\nmobile robots\\ntrajectory optimization\\nmobile agents\\ntask analysis\\nactuators\\ncollision avoidance\\nend effectors\\nmanipulator kinematics\\nmulti-robot systems\\noptimisation\\nposition control\\nconstrained collaborative mobile agents family\\nground mobile bases\\nclosed-loop kinematic chains\\nrevolute joints\\nclosed- loop kinematic chains\\nstandalone trajectory optimization method\\nccma system\\nfixed design parameters\\ncontrol policy optimization\\nmanipulation capabilities\\ntracked mobile bases\\nparallel robots\\noptimization and optimal control\",\"996\":\"mobile robots\\nwheels\\nforce\\ngears\\nfriction\\nrobot sensing systems\\ncollision avoidance\\ntraction\\nstep-obstacle climbing\\nbody rotational wheeled robot\\nscattered obstacles\\ndriving environment\\nstep-type obstacle\\nmain body rotation mechanism\\nrobot wheels\\nwheel-drive robot\\nbody mass\\nslope traveling\\ndownhill wheel\\nmechanical effect\\nrobot platform\",\"997\":\"radar\\nrobot sensing systems\\nradar antennas\\nchirp\\ncomputational modeling\\ncollision avoidance\\ncw radar\\nfm radar\\nfrequency modulation\\nimage sensors\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nradar computing\\nroad vehicle radar\\nsensors\\ncollaborative robotics\\nradar systems\\nrobot systems\\noptimization\\nmachine learning approaches\\nrealistic simulation models\\nradar sensor simulations\\nrelative velocities\\nlambertian reflectance model\\nreflection estimates\\nfrequency modulated continuous wave radar\\nsimulation environments\",\"998\":\"task analysis\\nplanning\\nrobot sensing systems\\ngrounding\\nrobustness\\nfeature extraction\\nlearning systems\\nmanipulators\\nneurocontrollers\\nrobot vision\\nsymbolic operators\\nmanipulation tasks\\ntransferable task execution\\nvisual input\\nsymbolic planning methods\\npartially-observable world\\nhierarchical model\\nhigh-level model\\ndeep planning domain learning\\nsymbolic world state\\ndpdl\\nstrips\\nlogical predicates\\nlow-level policy learning\\nphotorealistic kitchen scenario\",\"999\":\"estimation\\nuncertainty\\nrobot sensing systems\\ntraining\\ngrasping\\nend effectors\\nimage colour analysis\\nmanipulator kinematics\\nmean square error methods\\nmobile robots\\nneural nets\\noptical radar\\nrobot vision\\nunsupervised learning\\nrobotic manipulation\\nneural network\\nrgb-d images\\nphysical interactions\\nautonomous grasping policy\\nend effector position labels\\nforward kinematics\\nmanipulation systems\\nstructured light sensors\\nunsupervised deep learning\\nself-supervised grasping\\ndepth estimation\\nlidar sensors\\nroot mean squared error\",\"1000\":\"videos\\nrobots\\ntraining\\nmeasurement\\nobject recognition\\nvideo sequences\\ntask analysis\\nimage representation\\nlearning (artificial intelligence)\\nonline learning\\nobject representations\\nappearance space feature alignment\\nmonocular videos\\nself-supervised model\\nocn\\nleverage self-supervision\\nonline adaptation\\nonline model\\nobject identification error\\noffline baseline\\nrobotic pointing task\\nobject adaptation\\nobject-contrastive network\",\"1001\":\"robots\\nvisualization\\ntraining\\nkinematics\\ngaussian processes\\noptimization\\nkernel\\nfeature extraction\\nintelligent robots\\nmobile robots\\nobject detection\\nvisual servoing\\nexploratory behavior\\nvisual features learning\\ncontextual multiarmed bandit\\nparameterized action space\\narticulated object interaction\\ncontextual prior prediction\",\"1002\":\"grasping\\nrobots\\nobject detection\\nsemantics\\ntask analysis\\ndeconvolution\\nfeature extraction\\nconvolutional neural nets\\nimage segmentation\\nlearning (artificial intelligence)\\nmanipulators\\nmt-dssd\\nsemantic object segmentation\\ngrasping point detection\\nmultitask learning\\ngrasping operation\\nmultitask deconvolutional single shot detector\\nrobot manipulation\\namazon robotics challenge dataset\",\"1003\":\"shape\\ngrasping\\nimage segmentation\\nthree-dimensional displays\\ntask analysis\\nrobot sensing systems\\nconvolutional neural nets\\ndexterous manipulators\\nlearning (artificial intelligence)\\nmobile robots\\nobject recognition\\npath planning\\nrobot vision\\nshape recognition\\nsynthetic data\\ndeep networks\\nprimitive shape\\nobject grasping\\nsegmentation-based architecture\\nmonocular depth input\\nbackbone deep network\\nparametrized grasp families\\nshape primitive region\\ntask-free grasping\\nshape primitives\\ntask-relevant grasp prediction\\nranking algorithm\\ntask-free grasp prediction\",\"1004\":\"training\\nrendering (computer graphics)\\nrobots\\nengines\\npipelines\\ntask analysis\\ncameras\\nimage segmentation\\niterative methods\\nlearning (artificial intelligence)\\nneural nets\\npose estimation\\nrobot vision\\nrealistic scene synthesis\\nrobotics\\ntraining data\\ndeep learning\\nsynthesis pipeline\\ncluttered scene perception tasks\\nsemantic segmentation\\nobject detection\\nphysically realistic scenes\\nhigh-quality rasterization\\nmaterial parameters\\ncamera sensors\\ndeep neural network\\ntraining frames\\niterative render-and-compare approaches\\nycb-video dataset\\nstillleben\",\"1005\":\"training\\nimage color analysis\\nplastics\\ngallium nitride\\ntask analysis\\nshape\\ndecoding\\nimage classification\\nlearning (artificial intelligence)\\nobject detection\\nsupport vector machines\\ndata scarcity problems\\nunderwater image datasets\\nvisual detection\\nmarine debris\\ntwo-stage variational autoencoder\\ngenerated imagery\\ntwo-stage vae\\nbinary classifier\\nmulticlass classifier\\naugmentation process\\ntrash images\\nunderwater trash classification problem\\ndata-dependent task\\nquality images\",\"1006\":\"spatiotemporal phenomena\\ngallium nitride\\ntask analysis\\ntraining\\ngenerative adversarial networks\\nrobots\\nfeature extraction\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nlearning systems\\nunstructured environments\\nunsupervised representation learning architecture\\nunderlying representation\\nhigh-dimensional raw video inputs\\nspatiotemporal representation learning\\nlower-dimensional latent space\\ntwo-stage learning approach\\nconvolutional neural network\\nlong short-term network\\nlstm-lstm cells\\nhierarchical representation learning\\nlow-dimensional representation\\nvideo prediction task\\ngan trained lstm-lstm networks\\nrobot behavior learning\\nlayered spatiotemporal memory long short-term memory\\ngenerative adversarial network\\nconvnet\",\"1007\":\"robots\\nphysics\\nengines\\npredictive models\\nhistory\\nneural networks\\ntrajectory\\nbackpropagation\\ngraph theory\\nneural nets\\nrobot programming\\ngroups of articulated objects\\nlearning action effects\\ncomplex robotic systems\\ngraph neural networks\\nbelief regulated dual propagation nets\\nobject interaction\\nobject trajectory level\\nbelief regulator\\nphysics predictor\\npropnets\\ngeneral-purpose learnable physics engine\\nbrdpn\\nrobotics domain\",\"1008\":\"predictive models\\nkinematics\\ntrajectory\\nhidden markov models\\nradar tracking\\ndata models\\ninterpolation\\nintelligent transportation systems\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nroad safety\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\ntrajectory control\\nvehicle dynamics\\ndeep learning\\ndeep convnets\\ndeep kinematic models\\nkinematically feasible vehicle trajectory predictions\\nself driving vehicles\\ntraffic safety\\nautonomous technology\\nkinematically feasible motion prediction\\nvehicle kinematics\\nphysically grounded vehicle motion models\",\"1009\":\"trajectory\\nautonomous vehicles\\ndata collection\\nautomobiles\\nroads\\ndrones\\ndecision making\\ndriver information systems\\nmobile robots\\nroad safety\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\ntransportation\\nhuman driver behavior prediction\\npublic transportation systems\\nfully automatic transportation environments\\nautonomous vehicle decision making\\nplanning\\nlstm-based trajectory prediction method\\nurban scenario\",\"1010\":\"robots\\ndata models\\noceans\\nneural networks\\nconvolution\\nnetwork architecture\\nlogic gates\\nhandwritten character recognition\\nlearning (artificial intelligence)\\nmobile robots\\nneural net architecture\\nrobot vision\\nenvironment prediction\\nsparse samples\\nrobotics applications\\nneural network architecture\\nspatially correlated data fields\\nspatially continuous samples\\nbiased loss functions\\nreconstruction error\\nrobotic information gathering trials\\nmnist hand written digits dataset\\nocean monitoring\\nregional ocean modeling system ocean dataset\\nroms\",\"1011\":\"predictive models\\ntwo dimensional displays\\nrobots\\ndata models\\nphysics\\nthree-dimensional displays\\nanalytical models\\ngraph theory\\nhumanoid robots\\nlearning (artificial intelligence)\\nmanipulators\\nspatial object relations\\nlearning internal prediction models\\nrobot tasks\\npossible action consequences\\naction parameters\\ndesired goal states\\nparametrizing pushing actions\\nhigh-level planner\\nobject-centric graphs\\nsynthetic data set\\ngoal state\\npossible pushing action candidates\\nhigh prediction accuracy\\nhumanoid robot armar-6\\nlearned internal model\\npushing action effects\",\"1012\":\"planning\\ntrajectory\\ntorque\\nlegged locomotion\\njacobian matrices\\nknee\\ncollision avoidance\\nhumanoid robots\\nlearning (artificial intelligence)\\nmotion control\\nneural nets\\npath planning\\npose estimation\\nrobot vision\\nrobust control\\ntransfer functions\\nlocomotion\\nuneven terrain\\nmulticontact motion planning\\npose evaluation\\nneural network\\nactivation function\\nrobust robotics system\\ndeep learning\\ndepth image\",\"1013\":\"planning\\nconferences\\nautomation\\ngaussian processes\\nartificial intelligence\\ntrajectory optimization\\nrobots\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\noptimisation\\nrobotics tasks\\ntrajectory optimization algorithms\\ndifferentiable extension\\ngpmp2 algorithm\\nlearning-based approach\\ngaussian process motion planning algorithm\\nmotion planning\",\"1014\":\"planning\\nprobabilistic logic\\nrobots\\ndensity measurement\\ntask analysis\\nbuildings\\nconvolutional neural networks\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nneurocontrollers\\npath planning\\nsampling methods\\nsampling-based planners\\nplanning time\\nmotion planning problems\\nopen motion planning library\\nsampling-based algorithms\\nuniform sampling\\nsampling-based motion planners\",\"1015\":\"target tracking\\ncameras\\nunmanned aerial vehicles\\nvisualization\\ntask analysis\\nsurface cleaning\\nthree-dimensional displays\\nautonomous aerial vehicles\\nimage sequences\\npose estimation\\nslam (robots)\\npose-estimate-based target tracking\\nhuman-guided remote sensor mounting\\nautonomous aerial manipulation\\nunstructured environments\\nuav localization\\npbtt method\\ntarget point\\nfully on-board computation\\nrgb-d camera\\ndownward-facing optical flow camera\\nhorizontal localization\\nautonomous flight tests\\ninteracting-boomcopter uav platform\\nuav position estimator\",\"1016\":\"cameras\\ncollision avoidance\\nmotion segmentation\\nmachine learning\\noptical imaging\\nrobot vision systems\\nimage segmentation\\ncontrol engineering computing\\nhelicopters\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\nrobot vision\\ndeep dynamic obstacle dodging\\ndynamic obstacle avoidance\\nquadrotor\\ndeep learning\\nsingle event camera\\nshallow neural networks\\nego-motion\\nlow light testing scenario\\nevdodgenet\",\"1017\":\"cameras\\ntraining\\nobservers\\nvisualization\\nimage color analysis\\nglobal navigation satellite system\\nposition measurement\\nautonomous aerial vehicles\\nimage classification\\nlearning (artificial intelligence)\\nmicrorobots\\nmobile robots\\nobject detection\\ntraining datasets\\nmachine learning-based visual relative localization\\nmicroscale uavs\\nrelative microscale unmanned aerial vehicle localization sensor uvdar\\nautomatically annotated dataset midgard\\nmavs\\nvisual object detection\\ncarefully crafted training dataset\\nannotated camera footage\",\"1018\":\"dynamic programming\\nprogramming\\nrobots\\nlearning (artificial intelligence)\\nheuristic algorithms\\ntask analysis\\ntraining\\nmobile robots\\nscalable safe reinforcement learning\\nreal-world robots\\ncomplex strict constraints\\nsafe reinforcement learning algorithms\\nhigh-dimensional systems\\ndaap\\nsample efficiency\\ndynamic actor-advisor programming\\ndynamic policy programming framework\\nconstraint violation risk\",\"1019\":\"training\\nnavigation\\nrobot kinematics\\nrobot sensing systems\\noptimization\\nmachine learning\\ndiscrete systems\\ngradient methods\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\nstate-space methods\\nmapless navigation\\ndiscrete state space algorithms\\ncontinuous alternatives\\ndouble deep q-network\\nparallel asynchronous training\\ntraining time\\nproximal policy optimization algorithms\\noriginal discrete algorithm\\ncontinuous algorithms\\ncontinuous deep deterministic policy gradient\\nmultibatch priority experience replay\\ndiscrete deep reinforcement\",\"1020\":\"robot kinematics\\ntraining\\ntools\\ntask analysis\\nmachine learning\\nhistory\\ndecentralised control\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nrecurrent neural nets\\nmultirobot decentralized macro-action-based policies\\ncentralized q-net\\ndecentralized control\\ndecentralized multiagent reinforcement learning\\ndecentralized q-net\\ndecentralized exploration\\nmacro-action based decentralized multiagent double deep recurrent q-net\\nparallel-macdec-maddrqn\",\"1021\":\"robustness\\noptimization\\ntraining\\ncontrol theory\\ncomputational modeling\\nlearning (artificial intelligence)\\nbayes methods\\nneural nets\\noptimisation\\npendulums\\nrobust model-free reinforcement learning\\nmultiobjective bayesian optimization\\nautonomous agent\\nexogenous reward signal\\ntest conditions\\npure reward maximization\\nmodel-free case\\nrobust model-free rl problem\\nmultiobjective optimization problem\\nrobustness indicators\\nrobust formulation\",\"1022\":\"semantics\\nmotion segmentation\\nimage reconstruction\\nmotion estimation\\npipelines\\nsilicon\\ndynamics\\nimage motion analysis\\nimage segmentation\\nimage sequences\\nobject detection\\ndense piecewise semantic reconstruction\\nsuperpixel relations\\nstructure-from-motion\\nsuperpixel relation analysis\\nmotion relations\\nsemantic instance segmentation\\ndynamic scenes\\nmoving objects\\nspatial relations\",\"1023\":\"ellipsoids\\nthree-dimensional displays\\nrobot sensing systems\\ncameras\\ntwo dimensional displays\\nuncertainty\\ngraph theory\\nimage colour analysis\\nimage sensors\\nmobile robots\\nnormal distribution\\nrobot vision\\nslam (robots)\\ncamera origin\\npose graph\\nndt-om\\nkeyframe-based dense mapping\\nkeyframe-based mapping system\\nrgb-d sensor\\n2d view-dependent structures\\nuncertainty model\\nrgb-d cameras\\nview-dependent local maps\\nnormal distribution transform maps\\nglobal map\\nloop closure detection\\nautonomous robots\\nslam\",\"1024\":\"uncertainty\\nplanning\\nrobot sensing systems\\ntrajectory\\nmanuals\\nrobot localization\\ngaussian processes\\nmobile robots\\npath planning\\ninformative path planning\\nactive field mapping\\nlocalization uncertainty\\ninformation gathering algorithms\\nefficient data collection\\nfundamental problem\\nimplicit requirement\\nhigh-quality maps\\ninformative planning framework\\nactive mapping\\ngaussian process model\\ntarget environmental field\\nutility function\\nfield mapping objectives\\ngp-based mapping scenarios\\nmean pose uncertainty\\nmap error\\nindoor temperature mapping scenario\",\"1025\":\"surface treatment\\ndata models\\npredictive models\\ncovariance matrices\\ngaussian processes\\ncomputational modeling\\nmeasurement uncertainty\\nmobile robots\\npath planning\\nregression analysis\\nrobot vision\\nslam (robots)\\nsparse gaussian process experts\\nimplicit surface mapping\\nstreaming data\\ncreating maps\\nrobotics\\nnavigation\\ncompact surface map\\ncontinuous implicit surface map\\nrange data\\napproximate gaussian process experts\\ngp models\\nmodel complexity\\nprediction error\\nreal-world data sets\\ncompact surface models\\naccurate implicit surface models\\nexact gp regression\\nsubsampled data\",\"1026\":\"three-dimensional displays\\nvehicle dynamics\\noctrees\\nobject detection\\nlaser modes\\nfeature extraction\\nimage capture\\nimage filtering\\nimage registration\\nimage representation\\noptical radar\\nrobot vision\\n3d point cloud maps\\ndynamic object removal\\nlaser scans\\nlidar scans\\nvoxel traversal method\",\"1027\":\"semantics\\nfeature extraction\\ntask analysis\\nestimation\\nthree-dimensional displays\\nimage segmentation\\ncomputer architecture\\nimage matching\\ninference mechanisms\\nneural nets\\nsemantic networks\\nstereo image processing\\naugmented reality\\ndeep neural networks\\nsemantic segmentation\\ninference\\nsemantic stereo image matching\\ncoarse-to-fine estimations\\nembedded devices\\ngpu\\nembedded jetson tx2\",\"1028\":\"image segmentation\\nestimation\\ntask analysis\\ntraining\\nfeature extraction\\nneural networks\\nimage reconstruction\\ncomputer vision\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nregression analysis\\nsingle image depth estimation\\nunsupervised network\\ndeep neural networks\\ncomputer vision tasks\\nencoder-decoder-based interactive convolutional neural network\\nmultitask learning framework\\ncnn\\npixel depth regression\",\"1029\":\"legged locomotion\\nrabbits\\nsafety\\nfoot\\npredictive models\\ncontrol systems\\ncontrol system synthesis\\npredictive control\\nreduced order systems\\nrobot dynamics\\nsafety-preserving controllers\\nmodel predictive control\\n5-link rabbit model\\nanchor framework\\nonline robotic gait design\\nonline control design\\nbipedal robot\\nreduced-order model\\ncontrol synthesis\\ntemplate framework\\nsafe robotic gait design\\nbipeds\\nunderactuated system\\nsafety guarantee\",\"1030\":\"modulation\\nrobots\\nhip\\nstability criteria\\nfoot\\nforce\\nhumanoid robots\\nlegged locomotion\\nmechanical stability\\nmotion control\\nposition control\\npredictive control\\ntime optimal control\\nunified push recovery fundamentals\\nstepping strategies\\nbalance strategies\\nminimum jerk controller\\nhuman behaviour\\nmodel-predictive control\\nrecovery motions\\nrobotic systems\\nhuman balance recovery\\nlegged machines\\ntime-optimal performance\",\"1031\":\"uncertainty\\ntrajectory\\nmathematical model\\ncomputational modeling\\nnumerical models\\nstochastic processes\\ncost function\\nbayes methods\\ncontrol system synthesis\\ndifferential equations\\nmonte carlo methods\\npredictive control\\nprobability\\nrobust control\\nsampling methods\\nstochastic systems\\ntransforms\\nuncertain systems\\ndouble likelihood-free inference stochastic control\\ncomplex physical systems\\ncontrol strategies\\nanalytical tractability\\nprobabilistic inference\\nsimulation parameters\\nlikelihood function\\nmodern simulators\\nnonanalytical model\\nclassical control\\nmodel parameters\\ndisco\\nnumerical solvers\\nuncertainty assessment\\nbayesian statistics\\nlikelihood-free inference\\ncontrol framework design\\nunscented transform\\ninformation theoretical model predictive control\\nmonte carlo sampling\\nrobotics tasks\\nposterior distribution\",\"1032\":\"task analysis\\noptimization\\ndata models\\nadaptation models\\nheuristic algorithms\\nneural networks\\nplanning\\nlearning (artificial intelligence)\\ncontrol algorithms\\nprimal-dual method\\nsufficiently accurate models\\ntraditional control\\nerror characteristics\\ninaccurate physical measurements\\nplanning algorithms\\nrobot\\naccurate model learning\",\"1033\":\"task analysis\\nplanning\\nruntime\\nmanipulators\\ntransforms\\ncomplexity theory\\nmobile robots\\npath planning\\nservice robots\\ncleaning tasks\\nmobile manipulation plans\\nplan transformations\\nmobile fetch and place\\nrobot behavior\\ntable setting tasks\",\"1034\":\"manipulators\\ndatabases\\ntask analysis\\nrobustness\\ngrasping\\nuncertainty\\nplanning\\ncollision avoidance\\nmobile robots\\nmotion control\\nredundant manipulators\\nmobile manipulator\\nplanned base positions\\nrobust base sequence\\nprecomputed reachability database\\nbase positioning uncertainty\\ncollision free inverse kinematics solutions\\nmultiple pick and place tasks\\nkinematic redundancy\",\"1035\":\"gears\\nnavigation\\ntask analysis\\nmanipulators\\nthree-dimensional displays\\nrobot sensing systems\\nassembling\\ncontrol engineering computing\\nindustrial manipulators\\nmachining\\nmobile robots\\nproduction engineering computing\\nsoftware architecture\\nirregular objects\\nmechanical parts\\ncomplex task sets\\nintegrated task sets\\nieee international conference on robots and automation\\nfetchit! mobile manipulation challenge\\nmobile multitask manipulation\\nconfined environment\\nintegrated environment\\nconfined space\\nassembly\",\"1036\":\"mobile robots\\nfriction\\ntask analysis\\ndynamics\\nmathematical model\\nforce\\ncollision avoidance\\nlinear systems\\nmanipulators\\nmotion control\\npredictive control\\ntime-varying systems\\ntrajectory control\\nwheels\\nlinear time-varying mpc\\nnonprehensile object manipulation\\nnonholonomic mobile robot\\nnonprehensile manipulation motion primitive\\nunilateral constraint\\nmanipulated object\\nlinear time-varying model predictive control\\npushing manipulation\",\"1037\":\"task analysis\\nrobot kinematics\\nrobustness\\nvisualization\\naerospace electronics\\neducation\\ncontrol engineering computing\\nforce control\\nhumanoid robots\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nmotion control\\nposition control\\nvirtual reality\\nmobile manipulation system\\none-shot teaching\\nmobile manipulation hardware\\nsoftware system\\nhuman-level tasks\\nsingle demonstration\\nhighly capable mobile manipulation robot\\nparameterized primitives\\nrobust learned dense visual embeddings representation\\ntask graph\\ntaught behaviors\",\"1038\":\"three-dimensional displays\\ntwo dimensional displays\\ncameras\\ncost function\\nrobot sensing systems\\ntransforms\\nsymmetric matrices\\ncalibration\\nimage registration\\ninteger programming\\niterative methods\\nmobile robots\\nrobot vision\\niterative nearest-neighbor\\nmixed-integer program\\ndata association\\ninteger variables\\n3d line-based registration\\nmixed-integer programming\\nrigid-body transformation\\n3d point cloud data\\nmobile robotics\\nsensor calibration\\nlinear line-based 2d-3d registration\",\"1039\":\"cameras\\neigenvalues and eigenfunctions\\ngravity\\npose estimation\\ntransmission line matrix methods\\nsymmetric matrices\\nmotion estimation\\naccelerometers\\ninertial systems\\npolynomial matrices\\ngravity direction\\ncamera motion estimation\\ncamera-imu systems\\ncamera-inertial measurement unit systems\\n1dof\\ndegree of freedom\\ngr\\u00f6bner basis\\npolynomial eigenvalue problem\\n3-point algorithm\",\"1040\":\"training\\ntask analysis\\npredictive models\\nroads\\ndata models\\nautonomous systems\\ndecision making\\ncomputer vision\\nlearning (artificial intelligence)\\nroad safety\\nroad traffic\\nsafety-critical software\\ntask-aware novelty detection\\nself-driving cars\\ntrustworthy prediction\\nadversarial attacks\\nlife-threatening decisions\\nlearning framework\\nprediction model\\nnetwork saliency\\nlearning architecture\\nsaliency map\\nin-house indoor driving environment\\nadversarial attacked images\\ntarget prediction\\ndeep-learning driven safety-critical autonomous systems\",\"1041\":\"shape\\nthree-dimensional displays\\ntwo dimensional displays\\nautomobiles\\ncurrent measurement\\nsolid modeling\\nimage reconstruction\\nimage segmentation\\nneural nets\\nobject detection\\npose estimation\\nshape recognition\\nstereo image processing\\nstate-of-the-art deep learning based 3d object detectors\\nprevious geometric approach\\nadaptive sparse point selection scheme\\nsilhouette alignment term\\ndense stereo reconstruction\\nstereo image pair\\n3d rigid-body poses\\n3d bounding boxes\\ninstance segmentations\\nsimple bounding boxes\\nobject level\\nautonomous driving\\nscene understanding\\nshape estimation\\nvisual vehicle pose\\nshape priors\\ndirect photometric alignment\",\"1042\":\"videos\\ntext recognition\\nvehicles\\nimage recognition\\ntask analysis\\nroads\\nsemantics\\nintelligent transportation systems\\nroad traffic\\ntext detection\\ntraffic engineering computing\\nvideo signal processing\\ndriver assistance\\nroadtext-1k dataset\\ntext bounding boxes\\ndriving videos\\nintelligent systems\",\"1043\":\"estimation\\nthree-dimensional displays\\ncameras\\noptical imaging\\ntwo dimensional displays\\nfeature extraction\\nneural networks\\ndriver information systems\\nimage sequences\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nobject detection\\nroad safety\\nrobot vision\\nvideo signal processing\\nend-to-end learning\\ninter-vehicle distance\\nadas\\nmonocular camera\\nadvanced driver-assistance systems\\nrelative velocity estimation method\\nmultiple visual clues\\ntime-consecutive monocular frames\\ndeep feature clue\\nscene geometry clue\\ntemporal optical flow clue\\nvehicle-centric sampling mechanism\\nlight-weight deep neural network\",\"1044\":\"haptic interfaces\\nautoregressive processes\\nforce\\nacceleration\\npredictive models\\nsolid modeling\\ndiscrete fourier transforms\\nfeedback\\nhuman-robot interaction\\nimage texture\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\ntactile sensors\\ntelerobotics\\nvirtual reality\\nhaptic texture generation\\nhaptic sensory feedback\\nuser interactions\\nimmersive virtual reality\\nmaterial properties\\nhaptic vibration feedback\\npenn haptic texture toolkit\\naction-conditional model learning\\ngelsight measurements\\nteleoperation system\\nautonomous robot\\ngelsight image texture\",\"1045\":\"torque\\nmuscles\\nexoskeletons\\nelbow\\nskin\\nforce\\ntraining\\nbiomechanics\\nfeedback\\nhaptic interfaces\\ntorque control\\nvisual perception\\njoint torque feedback\\nkinesthetic feedback\\nexternal torques\\npreload torques\\ntest stimulus torques\\nstall torque\\naverage torques\\nstatic poses\\ninterweaving staircase method\\nextension direction\\nflexion direction\\nsize 1.27 nm\\nsize 0.27 nm\\nsize 3.0 nm\\ntime 120.0 s\\nsize 0.54 nm\",\"1046\":\"gravity\\nhaptic interfaces\\njacobian matrices\\ntorque\\nmanipulators\\nkinematics\\nbars\\ncompensation\\ncouplings\\ndesign engineering\\nforce feedback\\nmanipulator kinematics\\nmotion control\\nopen loop systems\\nvirtual reality\\nsystem weight\\nfour-bar-linkage mechanism\\nlinear motion\\nring-type gimbal mechanism\\ngravity force\\nconceptual mechanical design\\nfour-bar mechanism\\ngravity compensation\\nopen-loop force display performance\\nghap\\nparallel haptic device\\n6 degree of freedom manipulator\\nforward kinematics\",\"1047\":\"cameras\\nfeature extraction\\nbundle adjustment\\nvisual odometry\\nlighting\\ntracking\\ndistance measurement\\nimage matching\\nmotion estimation\\nrobot vision\\nstereo image processing\\nvisible spectrum\\nelectromagnetic spectrum\\nextreme illumination conditions\\ncamera setups\\nmultimodal monocular visual odometry solution\\nmultimodal tracking framework\\nstereo matching techniques\\nlong wave infrared spectral bands\\nlwir\\nmms-vo\\nwindowed bundle adjustment framework\\nmotion estimation process\\nvisible-thermal datasets\\nfeature tracking\\nvisual odometry trajectory\",\"1048\":\"robots\\ntracking\\nfeature extraction\\ncollaboration\\nunmanned underwater vehicles\\ntask analysis\\nautonomous underwater vehicles\\ncontrol engineering computing\\nconvolutional neural nets\\nhuman-robot interaction\\nmobile robots\\nobject detection\\nobject tracking\\ncustom cnn\\ndeep sort algorithm\\nrealtime tracking-by-detection\\nrealtime diver detection\\ninitial diver detection\\nappearance metric\\nsimple online realtime tracking\\nhuman divers\\nautonomous underwater robots\\nunderwater human-robot collaboration\\nrealtime multidiver tracking re-identification\\non-board tracking\\non-board autonomous robot operations\\nmultiperson tracking\",\"1049\":\"three-dimensional displays\\nprobes\\ntracking\\nrobots\\ncameras\\ntrajectory\\nbiological tissues\\nbiomedical optical imaging\\nmedical image processing\\nmedical robotics\\nsurgery\\nvisual servoing\\nautonomous tissue scanning\\nfree-form motion\\nintraoperative tissue characterisation\\nimaging probes\\ntissue surface\\nrobot-assisted local tissue scanning\\nmotion stabilisation\\nperiodic motion\\nfree-form tissue motion\\nscanning trajectory\\nultrasound tissue scanning\",\"1050\":\"valves\\nerbium\\nthree-dimensional displays\\nsoft robotics\\nelectrodes\\nactuators\\ndesign engineering\\nelectrorheology\\nhuman-robot interaction\\nhydraulic actuators\\nhydraulic systems\\nindustrial robots\\n3d-printed electroactive hydraulic valves\\nsoft robotic applications\\nalternative locomotion techniques\\nopen-source method\\nhigh-pressure electrorheological valves\\nelectrorheological fluid-based control\\ndeformable actuators\\nsafety areas\\npressure 230.0 kpa\\ntime 1.0 s to 3.0 s\",\"1051\":\"damping\\nstability analysis\\nservice robots\\nacceleration\\nsafety\\nimpedance\\nend effectors\\nhuman-robot interaction\\nstability\\nvariable structure systems\\nfixed damping controllers\\nvariable robotic damping controller\\nrobotic arm\\nphysical human robot interaction\\ndual sided logistic function\\nend effector\\n7 degree-of-freedom robot\\nroot mean squared interaction forces\",\"1052\":\"torque\\njoints\\nneural networks\\npredictive models\\nrobot sensing systems\\nstochastic processes\\ncognitive systems\\nhumanoid robots\\nhuman-robot interaction\\nmobile robots\\ntorque feedback\\nhumanoid torobo\\nbio-inspired study\\ncognitive compliance\\nhuman environments\\nadaptive robotics\\nnatural cognition\\nsubjective experience\\nintentional human-robot interaction\\nmotor compliance\",\"1053\":\"bayes methods\\nrobots\\nuncertainty\\ntask analysis\\nmeasurement uncertainty\\nresource management\\nnoise measurement\\ndelays\\nmobile robots\\nstability\\ntelerobotics\\nadaptive authority allocation\\nbayesian filter\\ncontrol framework\\nautonomous system\\nhuman operator\\ntime-varying measurement noise characteristics\\nsystem-driven adaptive shared control framework\\nstability proof\\nteleoperation\\nshared control\\nkalman filter\\nbayesian filters\",\"1054\":\"robot sensing systems\\nhaptic interfaces\\ntask analysis\\nforce\\nmanipulators\\nelectrodes\\ncomputational complexity\\ndexterous manipulators\\ntelerobotics\\nfirst-generation telerobot\\ntask complexity\\ntactile telerobots\\ninaccessible tasks\\nhighly-dexterous bimanual tactile telerobot\\nbare human hands\\nanthropomorphic robot hands\\nbiomimetic tactile sensors\\nin-hand manipulation\\nautonomous robotic hands\\nrobotic dexterity\",\"1055\":\"manipulators\\nindexes\\nhaptic interfaces\\nkinematics\\nperformance evaluation\\nforce\\nphantoms\\ncontrol system synthesis\\ndexterous manipulators\\nmanipulator kinematics\\noptimal systems\\nperformance index\\ntelerobotics\\nremot-arm\\nkinematic design\\ndexterous workspace\\nrelative orientability index\\ntarget workspace\\nperformance indices\\nhaptic master device dexterity\\n6-dof haptic master device\\nworkspace matching degree\",\"1056\":\"tools\\ntask analysis\\nrodents\\ndelay effects\\ndelays\\nforce\\nrobots\\naerospace instrumentation\\nbiocontrol\\nmanipulators\\nmedical robotics\\nmobile robots\\nspace research\\nspace vehicles\\nsurgery\\ntelerobotics\\nzero gravity experiments\\nteleoperated microgravity rodent dissection\\ninternational space station\\niss\\nbiological effects\\nspaceflight\\nrodent habitat\\nmicrogravity science glovebox\\nteleoperation\\nraven ii\\nrudimentary interaction force estimation\\nonboard dissection robot\\nraven-s prototype design\\ncommunications time delay\\nrobot design\\nrobot simulation\",\"1057\":\"collision avoidance\\nrobots\\nmarkov processes\\nnavigation\\ngames\\nrobustness\\ntraining\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\ncollision-free navigation\\nhuman-centered robots\\nmarkov games\\nrobot navigation\\nsingle-agent markov decision process\\nstatic environment\\nmultiagent formulation\\nprimary agent\\nremaining auxiliary agents\\npath-following type adversarial training strategy\\nrobust decentralized collision avoidance policy\\nreal-world mobile robots\\nhuman-centered robotics\\ndeep reinforcement learning\\nmulti-agent system\\nadversarial training\",\"1058\":\"collision avoidance\\nnavigation\\ntrajectory\\nrobot sensing systems\\nrobustness\\ntracking\\nlearning (artificial intelligence)\\nmobile robots\\npedestrians\\ntrajectory control\\ndensecavoid\\nreal-time navigation\\ndense crowds\\nanticipatory behaviors\\npedestrian behaviors\\nvisual sensors\\npedestrian trajectory prediction algorithm\\ninput frames\\ncompute bounding boxes\\npedestrian positions\\nfuture time\\nhybrid approach\\ndeep reinforcement learning-based collision avoidance method\\nrobust trajectories\\nstatic scenarios\\ndynamic scenarios\\nmultiple pedestrians\\nrobot freezing\\ntrajectory lengths\\nmean arrival times\",\"1059\":\"three-dimensional displays\\ntrajectory\\ncameras\\nvideos\\ntracking\\nvehicle crash testing\\ndata mining\\ninternet\\npublic domain software\\nroad safety\\nroad vehicles\\ntraffic engineering computing\\nvideo signal processing\\nreal-world collision scenarios\\nautonomous vehicles\\nuncalibrated monocular camera source\\ndeepcrashtest\\nvirtual crash tests\\nautomated driving systems\\ndashcam crash videos\\n3d vehicle trajectories\\nopen-source implementation\",\"1060\":\"coils\\nmagnetic devices\\nmagnetic separation\\nmagnetic resonance imaging\\nmagnetic particles\\nsignal to noise ratio\\nmagnetic nanoparticles\\nbiochemistry\\nbiomedical materials\\nbiomedical optical imaging\\ncellular biophysics\\ndyes\\nfluorescence\\nmedical robotics\\nmicromanipulators\\nnanomedicine\\nnanoparticles\\nph\\nfluorescent dyes\\nbiochemical measurements\\nion concentrations\\nsignal-to-noise ratios\\ndye-coated magnetic nanoparticles\\nmagnetic micromanipulation systems\\ngenerated swarm\\nmagnetic micromanipulation system\\nposition control accuracy\\nintracellular ph mapping\\nglobal dye treatment\\nfluorescent dye concentration\\nintracellular measurement results\\nrobotic control\\non-demand intracellular measurement\\nph sensitive fluorescent dye-coated magnetic nanoparticles\",\"1061\":\"transducers\\nacoustics\\nforce\\nfluids\\nthree-dimensional displays\\nstreaming media\\nhydrodynamics\\nacoustic streaming\\nmicrofabrication\\nmicrofluidics\\nmicromanipulators\\nposition control\\nhigh-speed acoustic streaming\\nmanipulation velocity\\ntrapped particle\\nparticle manipulation\\ncentimeter distance\\ntransducer surface\\nstreaming flow field\\nhydrodynamic force\\nmicrofabricated gigahertz transducer\\nthree-dimensional space\\ndynamic position control\\nspatial distance\\nmicroscale objects\\nmicrorobotics\\nnoncontact manipulation\\nmicroparticle\\n3d manipulation\\nacoustofluidic tweezers\",\"1062\":\"task analysis\\nergonomics\\nrobot kinematics\\nstrain\\ncollaboration\\nscheduling algorithms\\nhuman-robot interaction\\nlogistics\\noccupational health\\noccupational safety\\nproductivity\\nrobotic assembly\\nscheduling\\nwarehousing\\nonline scheduling algorithm\\nhuman-robot collaborative kitting\\nassembly line\\nkey logistic task\\nhuman operators\\nwork-related musculoskeletal disorders\\npicking operations\\noffline scheduler\\nwarehouse\\nproductivity analysis\",\"1063\":\"learning (artificial intelligence)\\nautonomous systems\\nplanning\\nheuristic algorithms\\ncomputational modeling\\nreal-time systems\\nuncertainty\\nmobile robots\\noptimisation\\nautonomous system\\nreal-time planning problems\\nmodel-free approach\\nanytime algorithms\\ncomputation time\\nmeta-level control technique\\nmeta-level control problem\\nreinforcement learning methods\\nmobile robot domain\",\"1064\":\"task analysis\\nrobot kinematics\\nplanning\\ncollision avoidance\\njob shop scheduling\\nresource management\\ncutting\\nindustrial manipulators\\nmotion control\\nmulti-robot systems\\noptimisation\\nrapid prototyping (industrial)\\nscheduling\\nspot welding\\ntime-varying portion\\ngeneric optimization method\\nvarying complexity\\ndual-arm robot\\nrobot arm\\nmotion scheduling\\nmultiple robot coordination\\nsimultaneous task allocation\\nadditive manufacturing\\nbolt tightening\\nbolt inserting\\ntask scheduling\\ndual-arm manipulation\\nmotion planning\",\"1065\":\"planning\\ncollision avoidance\\nrobot sensing systems\\nlibraries\\nsafety\\ntrajectory\\naerospace navigation\\nair safety\\nautonomous aerial vehicles\\ngraph theory\\ninfinite horizon\\nmicrorobots\\nmobile robots\\nprobability\\nrobot vision\\nsearch problems\\nhigh-speed mav flight\\nonline sparse topological graphs\\nsafe high-speed autonomous navigation\\nlocal planning grid\\ncomputationally-efficient planning architecture\\nsafe high-speed operation\\nlonger-term memory\\nmotion primitive-based local receding horizon planner\\nmemory-efficient sparse topological graph\\nplanning system\\ncomplex simulation environments\\nrobot decision making\\nprobabilistic collision avoidance\\nsafe rerouting\",\"1066\":\"training\\nadaptation models\\ntrajectory\\ngames\\nlearning (artificial intelligence)\\nrobots\\nswitches\\nmulti-agent systems\\nneural nets\\ndifferentiated sub-policies\\nhierarchical controller\\nadaptation performance\\nhierarchical deep reinforcement learning\\npolicy performance\\nconfidence- based training process\",\"1067\":\"task analysis\\nrobot sensing systems\\nplanning\\nfires\\nunmanned aerial vehicles\\nautonomous aerial vehicles\\ncontrol system synthesis\\ndiscrete event systems\\nmobile robots\\npath planning\\ntemporal logic\\ntask specifications\\nuniversally quantified locations\\nconstant time\\nhybrid control\\ndiscrete event controller\\nsynthesised plan\\niterator-based temporal logic task planning\\nrobotic systems\\nstate explosion\\ndiscrete locations\\nfixed-wing unmanned aerial vehicle\",\"1068\":\"robot sensing systems\\nplanning\\ntask analysis\\nheuristic algorithms\\nautomata\\nmobile robots\\nmulti-robot systems\\npath planning\\nrobot dynamics\\ntemporal logic\\nmultiple robots\\nreactive mission\\nunknown environment\\ntemporal logic planning approaches\\nknown environments\\nabstraction-free ltl planning algorithm\\ncomplex mission planning\\ncomplex planning tasks\\nco-safe linear temporal logic formulas\\nreactive temporal logic planning\",\"1069\":\"three-dimensional displays\\nimage reconstruction\\nplanning\\ncameras\\nsurface reconstruction\\ninspection\\nshape\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\nrobot vision\\nshape recognition\\nsolid modelling\\nstereo image processing\\nhigher order function networks\\nmultiview reconstruction\\nvisual inspection\\nneural network\\nshape information\\ndeep learning\\ncomplete 3d reconstruction\\nhigher order functions\\nreconstruction quality\\nmultiview hof network\\nimage acquisition\\nview planning\\nvisibility quality\\nshape representation\",\"1070\":\"navigation\\nrobots\\nuncertainty\\ntraining\\ntask analysis\\nlearning (artificial intelligence)\\nmachine learning\\nmobile robots\\npath planning\\ntrajectory control\\nlearned navigation strategies\\nresidual reinforcement learning framework\\nrobotic manipulation literature\\nresidual control effect\\nsub-optimal classical controller\\ndata efficiency\\ncluttered indoor navigation tasks\\nresidual reactive navigation\",\"1071\":\"trajectory\\ngrasping\\ngrippers\\nrobot sensing systems\\ncomputational modeling\\nservice robots\\ndexterous manipulators\\ngaussian processes\\npath planning\\nquality control\\nregression analysis\\nsheet materials\\nonline grasp plan refinement\\nrobotic layup\\ncomposite prepreg\\nhigh-performance composites\\nsheet layup\\ncomposite components\\ndeformable sheets\\nrobotic cell\\nlayup process\\nmanual layup\\nonline refinement\\nenvironmental factors\\ngaussian process regression model offline\\ngrasp plans\\ngpr\",\"1072\":\"three-dimensional displays\\noptimization\\ncollision avoidance\\ngrasping\\ngeometry\\nrobot sensing systems\\ndexterous manipulators\\ngrippers\\nimage reconstruction\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nrobot vision\\nshape recognition\\nsolid modelling\\ncontinuous 3d reconstructions\\ngeometrically aware grasping\\ndeep learning\\ngrasp synthesis\\nunseen objects\\npartial object views\\nindirect geometric reasoning\\nexplicit geometric reasoning\\ngrasping system\\nreconstruction network\\ngrasp success classifier\\ncontinuous grasp optimization\\ngrasp metrics\\n96 robot grasping trials\"},\"Benchmark Setup\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":-1,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1,\"1022\":-1,\"1023\":-1,\"1024\":-1,\"1025\":-1,\"1026\":-1,\"1027\":-1,\"1028\":-1,\"1029\":-1,\"1030\":-1,\"1031\":-1,\"1032\":-1,\"1033\":-1,\"1034\":-1,\"1035\":-1,\"1036\":-1,\"1037\":-1,\"1038\":-1,\"1039\":-1,\"1040\":-1,\"1041\":-1,\"1042\":-1,\"1043\":-1,\"1044\":-1,\"1045\":-1,\"1046\":-1,\"1047\":-1,\"1048\":-1,\"1049\":-1,\"1050\":-1,\"1051\":-1,\"1052\":-1,\"1053\":-1,\"1054\":-1,\"1055\":-1,\"1056\":-1,\"1057\":-1,\"1058\":-1,\"1059\":-1,\"1060\":-1,\"1061\":-1,\"1062\":-1,\"1063\":-1,\"1064\":-1,\"1065\":-1,\"1066\":-1,\"1067\":-1,\"1068\":-1,\"1069\":-1,\"1070\":-1,\"1071\":-1,\"1072\":-1},\"Experimental Results\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":-1,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1,\"1022\":-1,\"1023\":-1,\"1024\":-1,\"1025\":-1,\"1026\":-1,\"1027\":-1,\"1028\":-1,\"1029\":-1,\"1030\":-1,\"1031\":-1,\"1032\":-1,\"1033\":-1,\"1034\":-1,\"1035\":-1,\"1036\":-1,\"1037\":-1,\"1038\":-1,\"1039\":-1,\"1040\":-1,\"1041\":-1,\"1042\":-1,\"1043\":-1,\"1044\":-1,\"1045\":-1,\"1046\":-1,\"1047\":-1,\"1048\":-1,\"1049\":-1,\"1050\":-1,\"1051\":-1,\"1052\":-1,\"1053\":-1,\"1054\":-1,\"1055\":-1,\"1056\":-1,\"1057\":-1,\"1058\":-1,\"1059\":-1,\"1060\":-1,\"1061\":-1,\"1062\":-1,\"1063\":-1,\"1064\":-1,\"1065\":-1,\"1066\":-1,\"1067\":-1,\"1068\":-1,\"1069\":-1,\"1070\":-1,\"1071\":-1,\"1072\":-1},\"Code Link\":{\"0\":null,\"1\":\"'We do not assume IMU biases to be zero, instead we code the known information about them as probabilistic priors that are exploited by our MAP estimation.'\",\"2\":null,\"3\":\"'https:\\/\\/github.com\\/dytrong\\/Descriptor-Fusion-Model'\\n\\n'Keypoint Description by Descriptor Fusion Using Autoencoders'\\n\\n'Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo conditional changes such as those in illumination and viewpoint. In this paper, a descriptor fusion model (DFM) is proposed to create a robust keypoint descriptor by fusing CNN-based descriptors using autoencoders. Our DFM architecture can be adapted to either trained or pre-trained CNN models. Based on the performance of existing CNN descriptors, we choose HardNet and DenseNet169 as representatives of trained and pre-trained descriptors. Our proposed DFM is evaluated on the latest benchmark datasets in computer vision with challenging conditional changes. The experimental results show that DFM is able to achieve state-of-the-art performance, with the mean mAP that is 6.45% and 6.53% higher than HardNet and DenseNet169, respectively.'\\n\\n'The details of our descriptor fusion model (DFM). The descriptor fusion steps are as follows. First, we use a keypoint detector (SIFT in our experiments) to detect K keypoints and then extract 64 \\u00d7 64 image patches around these keypoints. Secondly, each of the image patches is fed into a trained CNN model and a pre-trained CNN model to generate two keypoint descripors, for all the K keypoints. Subsequently, we use a convolutional auto-encoder to compress the descriptor from the pre-trained model. Finally the descriptor from the trained CNN model and the compressed pre-trained descriptor are fused by a fully-connected autoencoder. For either autoencoder, it is trained with the standard reconstruction loss and, after training only the encoder half is used.'\\n\\n'The steps of generating a keypoint descriptor in DFM are as follows. First, we use a keypoint detector (SIFT in our experiments) to detect K keypoints and then extract a 64 \\u00d7 64 image patch around each keypoint. Secondly, each image patch is fed into a trained CNN model and a pre-trained CNN model, respectively, to generate two keypoint descripors. Subsequently, we use a convolutional autoencoder (CAE) to compress the descriptor from the pre-trained model. Finally the descriptor from the trained CNN model and the compressed pre-trained descriptor are fused by a fully-connected autoencoder (FCAE). The two autoencoders are trained independently by minimizing reconstruction loss and, during online operation, only the encoder half is used. The details of the CAE and FCAE are described in Section II-B.'\\n\\n'We use a CAE to reduce the dimensions of the descriptors extracted from the pre-trained CNN model. The use of the CAE allows us to reduce the memory requirement of the descriptor generated by the pre-trained CNN, which is typically on the order of a few hundred thousand, and improve the discriminating power of the descriptor at the same time, as we have found in our experiments. In addition, instead of using one autoencoder to perform both dimension reduction and fusion, we choose to use two separate autoencoders, one for compressing the pre-trained descriptor (CAE in Fig. 1) and the other for fusion (FCAE in Fig. 1), as this separation allows each autoencoder to specialize and provides an overall optimal result.'\\n\\n'B. Convolutional autoencoder (CAE) and fully connected autoencder (FCAE)'\\n\\n'The design of the CAE and the FCAE in our DFM is based on the traditional autoencoder architecture, and they each contains three encoders and three decoders.'\\n\\n'1) Convolutional autoencoder (CAE)'\\n\\n'Among the di-mensionality reduction methods, autoencoder has proven to enhance the accuracy in [24]\\u2013 [26] at the same time, although the autoencoder models may differ in design details. In our DFM, we use a CAE to compress the descriptors from a pre-trained CNN model. Each block of the encoder is composed of a convolutional layer, a batch normalization layer and a parametric rectified linear unit as the activation function. The use of convolutional layers allow us to preserve spatial information in the feature maps of the pre-trained CNN models. Similarly, each block of the decoder includes a deconvolutional layer, a batch normalization layer and a parametric rectified linear unit as the activation function.'\\n\\n'Since there is little spatial information to preserve among the two input descriptors, we use a fully connected autoencoder (FCAE) after concatenation to fuse the trained descriptor and the compressed pre-trained descriptor. This decision allows us to handle two source descriptors of different dimensions and is supported by [18] and [19], which used fully connected layer to fuse multiple descriptors after concatenation. For the choice of a trained descriptor in our DFM, we adopt HardNet [7] because of its robustness to viewpoint changes [13]. We should note, however, that our proposed DFM architecture is able to handle any pre-trained and trained descriptor models.'\\n\\n'Similar to CAE, each block of our FCAE is composed of a fully connected layer, a batch normalization layer and a parametric rectified linear unit as the activation function. The input sizes of three encoder blocks are 4224, 512 and 256, respectively. The output sizes of the first two encoder blocks are 512 and 256. The output size of the third encoder is 128 or 256. The hyper-parameters of the decoders mirror the encoders. After the FCAE is trained offline, its encoder half is used online to perform descriptor compression.'\\n\\n'For training the FCAE model, we also use a reconstrution loss function in terms of the mean squared error, and the Adam optimization algorithm to learn the model parameters. The learning rate is set at 0.0001 and the batch size we choose 64. The model is trained 200 epochs and all codes run on a computer with 16 CPUs (2.1 GHz), 128 GB memory and a Nvidia TITAN Xp GPU. We concat all compressed pre-trained descriptors and trained descriptors as the FCAE dataset, of which 80% are used for training and 20% are used for testing.'\\n\\n'In this section, we first introduce the experimental dataset and evaluation metric. Second, we discuss the effect of the keypoint patch sizes on matching performance, and choose a suitable patch size for all the compared descriptor extractors. Then, we compare convolutional autoencoder (CAE) as a dimension reduction method with principal component analysis (PCA) and random projection (RP) on the pre-trained CNN descriptors. Next, we compare the fully connected autoencoder (FCAE) with product, summation and concatenation as alternative techniques to fuse the trained and the pre-trained descriptors. Finally, we compare our proposed DFM descriptor with state-of-the-art descriptors and discuss the experimental results on keypoint matching .'\\n\\n'Comparison of three dimensionality reduction methods in terms of mean average precision (mAP) on Hpatches datasets. We compare convolutional autoencoder (CAE) with principal component analysis (PCA) [20] and random projection (RP) [21], two popular dimensionality reduction methods. The methods are applied to the descriptors computed by pool3 of DenseNet169 with dimensions of 184320. The red lines represent the mAP of the descriptor before dimension reduction. We reduce the dimensions of the DenseNet169-pool3 to 128, 256, 512, 1024, 2048 and 4096. We find the CAE to perform the best among the three dimensionality reduction methods on both the illumination and viewpoint datasets. In our final DFM, we use 4096 as the output dimension of the CAE.'\\n\\n'We use the fully connected autoencoder (FCAE) introduced in II-B to fuse the trained descriptor and the com-pressed pre-trained descriptor. In order to choose a fusion method, we compare FCAE with product, summation and concatenation, which are proven to enhance performance in [14] and [15]. For the trained descriptor, we select HardNet [7] which performs the best in terms of viewpoint changes [13]. For the pre-trained descriptors, in order to be able to compare the four fusion methods, we use the 128-dimension DenseNet169 so that summation and multiple of two source descriptors can be performed.'\\n\\n'TABLE I Comparision of different fusion methods in terms of mean average precision (mAP) [%] on Hpatches datasets. In order to choose a fusion method, we select product, summation, concatenation and autoencoder on HardNet and DenseNet169. In order to compare these four fusion methods fairly, we compress the descriptor from DenseNet169 to a dimension of 128 with CAE. We find the FCAE method is the most accurate among the four fusion methods. The dim means the descripor dimension, and illum and view represent two sub-datasets of Hpatches. Bolder numbers represent the best performance among all the compared fusion methods.'\\n\\n'In this paper, a descriptor fusion model (DFM) is proposed to fuse two leading keypoint descriptors from the trained and pre-trained CNN models using two autoencoders (CAE and FCAE), leading to a keypoint description method that is superior to the state-of-the-art. We have shown that, (a) our fusion-based descriptor combines the advantages of trained and pre-trained descriptors and improves the descriptor ac-curacy in handling both illumination and viewpoint changes (b) the compressed pre-trained descriptors are better than the uncompressed descriptors from convolution layers of pre-trained CNN models (c) the CAE is a better dimension reduction technique than principal component analysis (PCA) and random projection (RP) in terms of matching performance, and (d) the FCAE is a better fusion method than product, summation and concatenation. Although our proposed DFM uses pre-trained and trained CNN models to generate the input descriptors to the fusion process, it is not limited to these two classes. In fact, any existing descriptor models can be used within our DFM framework in constructing an alternative competing descriptor with improved performance. Our DFM is available at https:\\/\\/github.com\\/dytrong\\/Descriptor-Fusion-Model.'\\n\\n'https:\\/\\/github.com\\/dytrong\\/Descriptor-Fusion-Model'\",\"4\":null,\"5\":\"'The lidar front-end is in charge of using the raw lidar data to obtain relative pose measurements between consecutive robot poses (odometry) and non-consecutive poses (loop closures). The front-end builds upon an existing open-source implementation [24] and proceeds along the same lines as modern lidar-SLAM systems such as LOAM [9].'\",\"6\":\"'When incorporating deep neural networks into robotic systems, a major challenge is the lack of uncertainty measures associated with their output predictions. Methods for uncertainty estimation in the output of deep object detectors (DNNs) have been proposed in recent works, but have had limited success due to 1) information loss at the detectors nonmaximum suppression (NMS) stage, and 2) failure to take into account the multitask, many-to-one nature of anchor-based object detection. To that end, we introduce BayesOD, an uncertainty estimation approach that reformulates the standard object detector inference and Non-Maximum suppression components from a Bayesian perspective. Experiments performed on four common object detection datasets show that BayesOD provides uncertainty estimates that are better correlated with the accuracy of detections, manifesting as a significant reduction of 9.77%-13.13% on the minimum Gaussian uncertainty error metric and a reduction of 1.63%-5.23% on the minimum Categorical uncertainty error metric. Code will be released at https:\\/\\/github.com\\/asharakeh\\/bayes-od-rc.'\",\"7\":\"'In the first stage of our approach, we encode the input RGB image together with an object and a subject attention mask to classify them into a set of spatial relations with an auxiliary convolutional neural network (CNN). We denote the RGB image, the object and subject attention masks as\\\\nx\\\\ni\\\\n,\\\\na\\\\ni\\\\no\\\\n,\\\\na\\\\ni\\\\ns\\\\nrespectively and yi corresponds to the relation label in one-hot encoding - i.e., yi \\u2208 {0,1}|C| is a vector of dimensionality C (the number of relations). We model relations for a set of commonly used natural language spatial prepositions C = {inside, left, right, in front, behind, on top}. Let\\\\nD={(\\\\nx\\\\n1\\\\n,\\\\na\\\\n1\\\\no\\\\n,\\\\na\\\\n1\\\\ns\\\\n,\\\\ny\\\\n1\\\\n),\\u2026,(\\\\nx\\\\nN\\\\n,\\\\na\\\\nN\\\\no\\\\n,\\\\na\\\\nN\\\\ns\\\\n,\\\\ny\\\\nN\\\\n)}\\\\nbe the labeled data available for training our auxiliary classification network, which we name RelNet, see Figure 2a. Let \\u03b8RelNet be the parameters of the network. We denote the mapping of RelNet as\\\\nf(\\\\nx\\\\ni\\\\n,\\\\na\\\\ni\\\\no\\\\n,\\\\na\\\\ni\\\\ns\\\\n;\\\\n\\u03b8\\\\nRelNet\\\\n)\\u2208\\\\nR\\\\n|C|\\\\n. The attention masks are calculated as a Gaussian distance transform\\\\na(u,v)=\\\\n1\\\\n\\u03c3\\\\n2\\u03c0\\\\n\\u221a\\\\ne\\\\n\\u2212\\\\n1\\\\n2\\\\n((1\\u2212\\\\nd\\\\nuv\\\\n)\\/\\u03c3)\\\\n2\\\\nwith duv being the distance transform between (u, v) and the bounding box center, based on the L2 norm and with \\u03c3 = 2.'\",\"8\":\"'Deep neural networks (DNNs) have achieved great success in the area of computer vision. The disparity estimation problem tends to be addressed by DNNs which achieve much better prediction accuracy in stereo matching than traditional hand-crafted feature based methods. On one hand, however, the designed DNNs require significant memory and computation resources to accurately predict the disparity, especially for those 3D convolution based networks, which makes it difficult for deployment in real-time applications. On the other hand, existing computation-efficient networks lack expression capability in large-scale datasets so that they cannot make an accurate prediction in many scenarios. To this end, we propose an efficient and accurate deep network for disparity estimation named FADNet with three main features: 1) It exploits efficient 2D based correlation layers with stacked blocks to preserve fast computation; 2) It combines the residual structures to make the deeper model easier to learn; 3) It contains multi-scale predictions so as to exploit a multi-scale weight scheduling training technique to improve the accuracy. We conduct experiments to demonstrate the effectiveness of FADNet on two popular datasets, Scene Flow and KITTI 2015. Experimental results show that FADNet achieves state-of-the-art prediction accuracy, and runs at a significant order of magnitude faster speed than existing 3D models. The codes of FADNet are available at https:\\/\\/github.com\\/HKBU-HPML\\/FADNet.'\",\"9\":null,\"10\":null,\"11\":\"'https:\\/\\/robotic-esp.com\\/code\\/'\\n\\n'https:\\/\\/robotic-esp.com\\/code\\/'\",\"12\":\"'We now give a detailed description of the proceeding. Also consider the pseudocode in the right column.'\",\"13\":\"'Movements encoded by DMP are designed to be executed towards a stationary goal known before the start of the motion although it can be perturbed to another stationary goal during motion. Few research efforts have been focused on DMP utilization with a moving goal. In [13], the authors are modifying the DMP, to follow a moving goal in a leader-follower framework. In [14] by augmenting a modified DMP formulation with velocity feedback the authors allow the system to follow moving goals. In both these works position and velocity trajectories are scaled towards the moving goal so that the preset time duration of the task is preserved. The latter may induce either higher or lower velocities depending on the current goal position. It is therefore possible that the generated velocity exceeds safety boundaries. The use of saturation on the robot\\u2019s velocity does not resolve this problem as it generates a completely different velocity profile than that of the demonstration. However, in robot human coexistence, safety, trust and robot motion predictability is of outmost importance. It is therefore essential to maintain these human-like motion characteristics encoded during the demonstration.'\\n\\n'A different solution to the problem of high velocities is to not require a fixed motion duration. This is possible in models that do not encode temporal characteristics of the motion [15]. Alternatively one could adapt the motion duration as in [16]; this adaptation is based on the utilization of a model for the goal dynamics which predicts the reaching location. However, such a model may not be available when for example the robot interacts with agents of unknown dynamic behaviour. Such agents could be other robots or humans whose motion patterns may not be necessarily known. In [17] a DMP formulation has been proposed in the context of hitting a moving ball with a specified velocity. This method has been extended in [18] utilizing the probabilistic DMP formulation proposed in [19]. Both formulations require again a predictor to estimate the reaching location and the moving goal\\u2019s velocity at this point. Utilizing a predictor however causes the system to heavily depend on the prediction errors which can lead to execution failure.'\\n\\n'Dynamic movement primitives (DMP) are used to encode trajectories, through the augmentation of a unique point attractor linear dynamical system with a non-linear term that encapsulates arbitrary complex shape modulations known as the transformation system of the DMP. For encoding an n degrees of freedom task, the transformation system is given by [5]:'\\n\\n'In case the new goal is not stationary but moving, the system may be eventually able to track the goal but the trajectory does not follow the encoded motion pattern after a preset time duration since the forcing term goes to zero when the phase variable vanishes. This is happening as the phase variable evolution (4) does not depend on g. For example when tracking a goal that moves away from the initial position, the phase variable will reach zero before reaching the goal, which implies a zero forcing term, thus removing all encoded motion patterns from the trajectory.'\\n\\n'In order for the system to generate a trajectory with the encoded motion pattern while reaching a moving goal, the temporal scaling parameter \\u03c4 must be continuously adjusted. If the robot keeps the motion pattern of the demonstrated motion without adjusting its time scaling [13], [14] then, a faraway target would induce high velocities that could exceed the robot\\u2019s capabilities. Hence, in the case of a moving target, the temporal scaling parameter should be modified in real time. We propose to augment the DMP system with the following temporal scaling adaption law:'\",\"14\":null,\"15\":\"'https:\\/\\/youtu.be\\/fVhFc41T88I.'\\n\\n'https:\\/\\/youtu.be\\/fVhFc41T88I'\",\"16\":null,\"17\":null,\"18\":null,\"19\":\"'https:\\/\\/github.com\\/vision4robotics\\/KAOT-tracker'\\n\\n'https:\\/\\/youtu.be\\/jMfmHVRqv3Y'\\n\\n'Speed: The speed of KAOT is around 15 fps with a GPU and can be used in real-time applications. However, KAOT tracker is implemented on MATLAB platform and the code is not optimized, so the speed can be further improved.'\\n\\n'https:\\/\\/github.com\\/vision4robotics\\/KAOT-tracker'\",\"20\":\"'https:\\/\\/fanshi14.github.io\\/me\\/icra20.html'\\n\\n'https:\\/\\/fanshi14.github.io\\/me\\/icra20.html'\",\"21\":\"'As our language model is not constrained to any map region or landmarks, it is necessary to encode goals and constraints of the natural language command in a domain-independent way. To accomplish this, we turn to LTL, a domain-independent formalism whose syntax can encode goals and constraints of the robot\\u2019s path. By allowing for encoding of both the present and future states of the robot, LTL supports the inherent non-Markovian nature of unconstrained natural language commands, such as \\\"Move to the medicine store without going over the red bridge.\\\" We use LTL to determine if a discrete trajectory satisfies the goals and constraints of the natural language command. LTL has the following grammatical syntax:'\\n\\n'Commanding mobile robot movement based on natural language processing with RNN encoder\\\\xaddecoder'\\n\\n'OSM is a global open-source map where any user can add landmarks and information about the landmarks. Critically, this information can be semantic in nature, such as the type of cuisine for a restaurant or the function of a building. We leverage OSM\\u2019s extensive semantic database as the foundation of our language model, enabling groundings of semantic referring expressions to landmarks.'\",\"22\":null,\"23\":null,\"24\":\"'https:\\/\\/youtu.be\\/bEQi4eglH2Y'\\n\\n'Several factors complicate the problem of lane detection. First, the competition track can contain very sharp turns, making the underlying geometry hard to capture. Further, the accuracy of the perception subsystem\\u2019s detections can be affected by weather, lighting conditions, and sensor quality, resulting in a large number of false positives (15-25%) in the cone detections. The competition track does have color-coded left and right boundaries, but the color information provided by the perception subsystem can be inaccurate and incomplete. Further, the MIT\\/DUT team aimed to have complete perception redundancy using both lidar and camera pipelines, therefore the boundary detection system had to operate in scenarios (for the lidar-only system) where the problem of false-positives was exacerbated by having a small look-ahead and wide field of view, often including detections from other parts of the track or other nearby objects. In many cases, like the one shown in Fig. 3, given no other information, the number of false positives and inaccurate detections make the track geometry hard to decipher even for a human in some locations.'\",\"25\":null,\"26\":\"'The perception component of our system consists of a deep encoder-decoder similar to multitask segmentation and monocular depth [22], trained to reconstruct RGB, depth and segmentation. For driving, we use the encoded features, which contain compressed information about the appearance, the semantics, and distance estimation. In principle, such representations could be learned simultaneously with the driving policy, for example, through distillation or by adding perception labels. However, to improve data efficiency and robustness when learning control, we pretrain the perception network on several large, heterogeneous, research vision datasets [23], [24], [25], [26], [27].'\",\"27\":\"'https:\\/\\/www.youtube.com\\/watch?v=oGNz-ECspXY'\\n\\n'This paper has presented a new approach to map relative localization that fuses visual lane level and topological map matching with GPS and dead reckoning. Both matching algorithms described are based on common features that are used in assistance for the navigation of the vehicle. The proposed method has been validated using readily available, open source map. Three use cases were studied, to compare different levels of sensor fusion: with both the map matching algorithms, only lane level map matching algorithm and neither of them. It was shown that two algorithms together can robustly localize the vehicle within the lane. Finally, accuracy of the localization of the vehicle was established at four points of interest using visual tags.'\",\"28\":null,\"29\":null,\"30\":null,\"31\":null,\"32\":null,\"33\":null,\"34\":null,\"35\":\"'Robotic manipulators are reaching a state where we could see them in household environments in the following decade. Nevertheless, such robots need to be easy to instruct by lay people. This is why kinesthetic teaching has become very popular in recent years, in which the robot is taught a motion that is encoded as a parametric function - usually a Movement Primitive (MP)-. This approach produces trajectories that are usually suboptimal, and the robot needs to be able to improve them through trial-and-error. Such optimization is often done with Policy Search (PS) reinforcement learning, using a given reward function. PS algorithms can be classified as model-free, where neither the environment nor the reward function are modelled, or model-based, which can use a surrogate model of the reward function and\\/or a model for the dynamics of the task. However, MPs can become very high-dimensional in terms of parameters, which constitute the search space, so their optimization often requires too many samples. In this paper, we assume we have a robot motion task characterized with an MP of which we cannot model the dynamics. We build a surrogate model for the reward function, that maps an MP parameter latent space (obtained through a Mutual-information-weighted Gaussian Process Latent Variable Model) into a reward. While we do not model the task dynamics, using mutual information to shrink the task space makes it more consistent with the reward and so the policy improvement is faster in terms of sample efficiency.'\",\"36\":\"'https:\\/\\/youtu.be\\/0cXGagMbBtg'\",\"37\":null,\"38\":null,\"39\":\"'Surgical robots have been introduced to operating rooms over the past few decades due to their high sensitivity, small size, and remote controllability. The cable-driven nature of many surgical robots allows the systems to be dexterous and lightweight, with diameters as low as 5mm. However, due to the slack and stretch of the cables and the backlash of the gears, inevitable uncertainties are brought into the kinematics calcu-lation [1]. Since the reported end effector position of surgical robots like RAVEN-II [2] is directly calculated using the motor encoder measurements and forward kinematics, it may contain relatively large error up to 10mm, whereas semi-autonomous functions being introduced into abdominal surgeries require position inaccuracy of at most 1mm. To resolve the problem, a cost-effective, real-time and data-driven pipeline for robot end effector position precision estimation is proposed and tested on RAVEN-II. Analysis shows an improved end effector position error of around 1mm RMS traversing through the entire robot workspace without high-resolution motion tracker. The open source code, data sets, videos, and user guide can be found at \\/\\/github.com\\/HaonanPeng\\/RAVEN Neural Network Estimator.'\\n\\n'To compensate for the inaccuracy, an intuitive approach is to take additional sensor measurements by applying a motion tracker at the surgical tooltip or a joint encoder on each robot joint depending on whether to resolve the problem in cartesian or joint level. Drawbacks occur in both solutions. In the former case, complications occur during the required high heat sterilization procedure [10]. The latter, however, introduces complexity keeping the sensor wires and robot cables compact. Alternatively, real-time video streams from endoscopes are used as an additional cue for end effector pose estimates. But extracting pose information from vision alone can be challenging with the highly dynamic and reflective surgical scenes in real-world operations [11]. Recently, online estimation systems are proposed to provide a robust and precise end effector position prediction.'\\n\\n'Comparison of actual position, motor encoder based forward kinematics position, and neural network estimation. The neural network estimator decreases the positional RMS error by 83.6% and the standard deviation of error by 59.4%.'\\n\\n'Due to compliance and losses in the transmission mechanism (cable\\/pulley links in the RAVEN-II), indirect measurement of joints and other external uncertainties, estimation of the precise end effector position is challenging. In this work, a cost-effective online RAVEN-II position precision estimator is implemented and tested on a 140-minute trajectory set. The system entails a vision-based ground truth position measurement system and an online data-driven position estimator based on a neural network. Although the total cost of the measurement is around one hundred dollars (mostly the cost of four webcams), the sub-millimeter accuracy achieved is more than 10 times better than the RAVEN-II position accuracy based on motor-mounted encoders. The neural network estimator decreases the positional RMS error by 83.6% and the standard deviation of error by 59.4%. Furthermore, the estimator requires no additional sensors or information other than the RAVEN-II built-in \\u2018ravenstate\\u2019 ROS topic (updated at 1000Hz), which contains kinematic and dynamic information of RAVEN-II. Finally, the proposed cost-effective position estimator can be generalized to other RAVEN-II sites, as well as other robots with accuracy affected by compliance and losses in transmission elements between motors and joints. Although robotic surgeons today readily compensate for imperfect position control, as commercial surgical robots incorporate human augmentation and autonomous functions, the need for accurate position estimate and control will increase.'\",\"40\":\"'The encoder-decoder TCN network that hierarchically models vision or kinematics data to states.'\",\"41\":\"'https:\\/\\/arxiv.org\\/abs\\/1909.09674'\\n\\n'https:\\/\\/arxiv.org\\/abs\\/1909.09674.'\\n\\n'Learning Latent Actions through Autoencoders. We learn latent actions using different autoencoder models, and compare how these models perform with respect to our desired properties. We find that models which are conditioned on the robot\\u2019s current state accurately reconstruct high-DoF actions from human-controllable, low-DoF inputs.'\\n\\n'Learning Latent Actions through Autoencoders.'\\n\\n'Here, we leverage autoencoders to learn a consistent and controllable latent representation for assistive robotics. Previous teleoperation literature has explored principal component analysis (PCA) for reducing the user\\u2019s input dimension [23], [24]. We will compare our method to this PCA baseline.'\\n\\n'autoencoders'\\n\\n'encoder'\\n\\n'decoder'\\n\\n'Model Details. We examine models such as PCA, AE, VAE, and state conditioned models such as cAE and cVAE. The encoders and decoders contain between two and four linear layers (depending on the task) with a tanh(\\u2022) activation function. The loss function is optimized using Adam with a learning rate of 1e\\u22122. Within the VAE and cVAE, we set the normalization weight < 1 to avoid posterior collapse.'\\n\\n'Summary. We focused on assistive robotics settings where the user interacts with low-Dof control interfaces. In these settings, we showed that intelligent robots can embed their high-DoF, dextrous behavior into low-DoF latent actions for the human to control. We determined that autoencoders conditioned on the system state accurately reconstructed the human\\u2019s intended action, and also produced controllable, consistent, and scalable latent spaces.'\",\"42\":null,\"43\":\"'http:\\/\\/youtu.be\\/w1Sx6dIqgQo'\\n\\n'Our quadruped robot has 12 active Degrees-of-Freedom (DoF) and is equipped with a stereo camera, an IMU, joint encoders and torque sensors (see Table I for the specifications). We aim to estimate the history of the robot\\u2019s base link pose and its velocity (linear and angular) over time. In contrast to previous works, we propose to estimate velocity biases (in addition to IMU biases) to compensate for leg odometry drift, as detailed in the following section.'\",\"44\":\"'The robot uses a rotating 2D lidar to perceive the surrounding terrain information. The scanned data are reconstructed as 3D point cloud which are used for generating the elevation map. An open source elevation mapping Robot Operating System (ROS) package [21] is used in our project. The traversability map is created based on the elevation map. Once the feasible footholds are found based on the traversability map, the COG trajectory and the collision free swing-leg trajectory can be planned. The footholds together with the trajectories in cartesian space are transformed into leg motor\\u2019s position in joint space which are used to control the legs to form in energy-saving stance posture.'\",\"45\":\"'https:\\/\\/youtu.be\\/7_c82NYmk4Q'\\n\\n'Heuristic-based legged locomotion control has shown success in traversing rough terrains, particularly with degraded perception. Since experts already have knowledge about locomotion, it is often a better choice to simply encode intelligent heuristics rather than leave it to an optimization solver. Focchi at al. demonstrate a heuristic planning framework on the HyQ robot to overcome uncertain conditions [1].'\\n\\n'As opposed to traditional MPC techniques, RPC directly exploits simple heuristics to simplify complex cost spaces and find feasible solutions quickly. Rather than treating the problem of robot locomotion as a black box optimization, simple physics-based heuristics are encoded into the cost function and constraints to bias the optimization towards a sensible solution while remaining free to explore the surrounding cost space for the possibility of a better result.'\",\"46\":\"'This reward function penalizes the movement of the center of mass in any direction. For each primitive cycle, we assign a desired orientation for the robot. Lastly, we penalize high joint velocity for the safety of our robots. Intuitively, this reward functions encodes that the optimal turning behavior is to turn on the spot at a constant speed. The parameters for reward functions for training are shown in Table I.'\\n\\n'For testing our algorithm on the Daisy robot, we designed a similar setup as simulation, where Daisy was commanded to go to goals up to 12m away from its start point. While our method can generalize to arbitrarily far away locations, currently our hardware setup is limited by the sensing of Vive tracking system for global position of Daisy; our goals are limited to be in the region covered by the base stations. Despite this, sometimes the robot loses tracking during the experiments, and the high-level action is hard-coded to stand still until the tracking is recovered. We test two experimental settings on hardware:'\",\"47\":\"'The motion capabilities of SoRX were evaluated through five experiments: running, step climbing, and traversing rough terrain, steep terrain, and unstable terrain. A modified version of an open-source pneumatic control board [34] was used in all experiments. In our board, every air output channel is connected to two pairs of valves and pumps to allow for both pressurization and depressurization. The primary experimental testbed is shown in Fig. 10. At this stage the robot runs in open loop (i.e. without steering control); hence, two acrylic panels were used to ensure the robot does not fall off from the platform. The length of the platform is 1.2 m. A 12-camera VICON motion capture system was used to collect position and velocity data of the center-of-mass (CoM) of SoRX.'\",\"48\":null,\"49\":\"'https:\\/\\/github.com\\/qwerty35\\/swarm_simulator'\\n\\n'The source code will be released in https:\\/\\/github.com\\/qwerty35\\/swarm_simulator.'\\n\\n'https:\\/\\/github.com\\/qwerty35\\/swarm_simulator'\",\"50\":\"'http:\\/\\/www.cyberbotics.com\\/'\\n\\n'For brevity, we do not include the pseudocode of the entire\\\\nA\\\\n\\u2217\\\\nSC\\\\nalgorithm. However, algorithm 3 shows the pseudocode of GET_HEURISTIC_COST(P,p,v), a subroutine that computes the four element heuristic cost\\\\nc\\u2208\\\\nR\\\\n4\\\\n+\\\\nof a path p. This custom heuristic cost determines the order in which\\\\nA\\\\n\\u2217\\\\nSC\\\\nwill expand search states within the algorithm. Ties are broken in cascading fashion. This cascaded tiebreaking behavior is the key to the efficiency of our pathplanner in computing make-span optimal route plans while simultaneously avoiding conflicts.'\",\"51\":null,\"52\":null,\"53\":null,\"54\":\"'In this paper, we showed that non-holonomic trajectory optimization has some niche mathematical structures which have been overlooked in the existing literature. We proposed some intelligent reformulations around these structures and reduced non-holonomic trajectory optimization to a problem of solving a sequence of globally valid convex QPs. We performed extensive comparisons with state of the art open-source optimizers and showed either similar or improved performance in terms of achieved optimal cost but with more than 20x reduction in computation time.'\",\"55\":\"'Hall Effect based absolute encoders located on the hip motors provide the motor positions as feedback signals to facilitate closed-loop control of the leg position. An aluminum tube of length 1.15 m holds the leg at the u-shape bracket and allows it to do spherical movement around the base freely, as shown in Figure 1(c). With large radius and small polar angle, the leg is assumed to be moving in the sagittal plane along the wooden track. Two Accu-Coder model 15s encoders are located at the base of the boom, which operate in the quadrature phase and measure the azimuth and polar angles of the tube. All the data are recorded via a Teensy 3.6 microcontroller at 500 Hz, while the control frequency for the leg is set at 1000 Hz. The major physical parameters of the robot are shown in Table I. Note that the nominal leg rest length lo is determined when the proximal links are placed in horizontal configuration, and the nominal leg stiffness ko is set to the value scaled from human data (mass = 80 kg, nominal leg rest length = 1 m, nominal leg stiffness = 19 600 N\\/m) [7].'\\n\\n'To implement the optimal force profile on the physical platform, several issues were addressed. First, the optimal force profile is generated separately from the main controller board\\u2019s Arduino 1.6.1 code and must be uploaded prior to each experimental run. The force profile data array is transmitted via USB-TTL serial communication from MATLAB 2018b to the main controller board where it is stored. The controller code later calls the stored array to apply the correct force input when stance phase is triggered. Second, real-time datalogging is needed to simultaneously collect data from the physical platform and main controller board that is communicated via serial communication. A Teensy 3.6 microcontroller stores the data to a micro SD card to allow for post-processing. This dataset includes: the platform\\u2019s two angular encoders located at its base and boom arm, current sensing of the motor controllers\\u2019 inputs, and the virtual-leg calculations that result from the main controller board\\u2019s access to motor encoders located at the hip axis.'\",\"56\":null,\"57\":null,\"58\":null,\"59\":\"'Fig. 9 provides the results of shear displacement estimation accuracy. The horizontal axis represents the applied displacement, which was measured using the stage encoder, and the vertical axis represents estimated shear displacement. R2 represents the coefficient of determination of the linear regression line. Although the estimation error is observed at 15% and 3% for translational and rotational displacement respectively, the estimated displacement shows strong linear correlation with the applied displacement. A possible cause of error is the approximation error of CoP.'\",\"60\":\"'https:\\/\\/github.com\\/swchui\\/Grasping-state-assessment\\/graspingdata'\\n\\n'https:\\/\\/github.com\\/swchui\\/Grasping-state-assessment\\/graspingdata'\",\"61\":\"'Current end-to-end grasp planning methods propose grasps in the order of seconds that attain high grasp success rates on a diverse set of objects, but often by constraining the workspace to top-grasps. In this work, we present a method that allows end-to-end top-grasp planning methods to generate full six-degree-of-freedom grasps using a single RGBD view as input. This is achieved by estimating the complete shape of the object to be grasped, then simulating different viewpoints of the object, passing the simulated viewpoints to an end-to-end grasp generation method, and finally executing the overall best grasp. The method was experimentally validated on a Franka Emika Panda by comparing 429 grasps generated by the state-of-the-art Fully Convolutional Grasp Quality CNN, both on simulated and real camera images. The results show statistically significant improvements in terms of grasp success rate when using simulated images over real camera images, especially when the real camera viewpoint is angled. Code and video are available at https:\\/\\/irobotics.aalto.fi\\/beyond-topgrasps-through-scene-completion\\/.'\",\"62\":null,\"63\":\"'Top: example input images of a challenging indoor environment. Bottom: Densely reconstructed map of a duplex building with estimated trajectory. The colors of the trajectory encode heights.'\",\"64\":\"'Experimental setup: The whole model is coded in Python 3.5 using the framework Tensorflow v1.13.1. All experiments were conducted on a single Nvidia GeForce GTX 1080 GPU.'\",\"65\":\"'encoder-decoder'\",\"66\":\"https:\\/\\/github.com\\/albert100121\\/360SD-Net\",\"67\":\"'For building the depth network, we propose to use a spherical feature transform layer at the end of encoder to reduce the difficulty of feature learning, and extend the CSPN [13] to inverse gnomonic projection CSPN (IG-CSPN) and deformable CSPN (D-CSPN), which better recover the structural details of estimated depths.'\\n\\n'The input OmniImage and the partial dense depth map, when available, are separately processed by their initial convolutions. The convolved outputs are concatenated into a single tensor, which acts as input to the encoder. The encoder consists of five residual blocks which downsample the spatial resolution of the feature map 16\\u00d7 w.r.t. the image resolution. On the other side, the decoder has the reverse structure, where transposed convolution layers are adopted here to upsample the feature map to the original resolution for dense depth estimation. In the network, all convolutional layers are followed by batch normalization [42] and ReLU [43].'\\n\\n'To alleviate the distortion problem of OmniImage, at the end of encoder, we adopt a SFTL to obtain spherical neighborhood in the equirectangular image with inverse gnomonic projection or deformable convolution. In addition, to recover the structure detail of the estimated depth map, at the end of decoder, we adopt the module of CSPN [13], and modify its propagation neighborhood to be dynamically changing w.r.t. to the pixel location. In the following sections, we elaborate the difference of both modules.'\",\"68\":null,\"69\":\"'Humans also use proprioception to gather kinematic in-formation to control the impedance of their limbs, but what set them apart is that they are endowed with a rich sense of touch, mediated by a collection of mechanoreceptors, densely populated in the fingertips. This wide-ranging array of mechanoreceptors encodes the complex mechanical interaction that occurs at the contact between the skin and the object. The sense of touch captures surface features [5], [6], compliance of materials [7], [8] or the presence of edges [9]. Of importance for the present work, an estimate of the curvature of an object can be extracted from a single press [10] and guide the timing of motor commands required for grasping and object manipulation [11], [12].'\",\"70\":null,\"71\":null,\"72\":null,\"73\":null,\"74\":null,\"75\":\"'We implement the proposed algorithm in MATLAB. For an event tracker, we process events through the open-source MATLAB script of [21] using only 1st part of the EM algorithm. In particular, Harris corners are detected in a synthesized event frame, namely\\\\nI(\\u03be)=\\\\n\\u2211\\\\ni\\\\n\\u03b4(\\u03be\\u2212\\\\n\\u03be\\\\ni\\\\n)\\\\n. Throughout all experiments, we set the maximum number of event feature as 30, the upper limit number of the spatiotemporal window as 30,000 events and the next temporal size as 3 times 65% percentile of the lifetime. Chi-squared test rejects optical flow outliers with 95% belief. For intensity feature tracker, 50 of Shi-Tomasi features are tracked by KLT tracker with 8-pt RANSAC. For the inequality constraint in (12), we let m = 0.25m\\u22121 and M = 10m\\u22121.'\",\"76\":null,\"77\":null,\"78\":null,\"79\":null,\"80\":null,\"81\":null,\"82\":null,\"83\":\"'All signals were sampled at 200Hz. The angular position of each hip joint was directly measured by the embedded encoders. Angular velocity was calculated by filtering the position signals offline. An FIR filter of order 50 with 20Hz passband frequency and 30Hz stopband frequency was designed using the designfilt function in MATLAB (The Mathworks, Natick, MA, USA) to approximate an ideal low-pass filtered differentiator. The filtered velocity signal was shifted in time by the group delay of the filter to compensate for delay introduced by the filter. Hip extension and flexion were defined as positive and negative, respectively (Fig. 1a).'\",\"84\":\"'Fully assembled and open view of the SCSA knee. Image shows (1) hybrid actuator, (2) load cell, (3) covered absolute encoder, (4) covered PCB, and (5) enclosed battery pack'\",\"85\":\"'The main components of the pneumatic AFO are an output pneumatic cylinder, a metallic slider-crank mechanism, custom thermoplastic braces, two 2-way solenoid valves, a pressure sensor, a magnetic encoder and two custom optical type ground reaction force (GRF) sensors as shown in Fig. 3-(a).'\\n\\n'The two 2-way solenoid valves (EC-2-12, Clippard, USA) are used to supply and discharge pressurized air to the output cylinder (SD16N75-B, Taiyo, Japan) and the pressure sensor (PSE540, SMC, Japan) is used to monitor the pressure of the output cylinder. An absolute magnetic encoder (I2A systems, Rep. of Korea) was installed to monitor the ankle flexion angle and the two custom optical GRF sensors were installed on the foot brace at the toe and heel for gait phase detection [20], [21]. The thermoplastic braces were designed so that the pneumatic AFO can be worn within the shoe and pants.'\",\"86\":null,\"87\":\"'The prototype is composed of two identical actuation mechanisms mounted symmetrically on a compliant body harness. Each half of the exoskeleton features a thigh mount to support joint pulley, a BLDC motor (Maxon 24V 170W EC40 with a 2000-CPT encoder without gear), motor driver (ESCON Module 50\\/8 HE), an 4096-CPT absolute optical encoder (AMT 203-V) measuring the hip joint pulley rotation angle, hollow-type loadcell (Futek LTH300) installed at the motor side and capable of measuring up to 110 kg compression force, and 2 Dyneema strings (LIROS) used in parallel. A 96-Mhz ARM-type micro-controller (Teensy 3.2) runs control and data acquisition software.'\",\"88\":\"'Mechanism model of the exoskeleton. (a) Front view. (b) Rear view. 1-Gear 1, 2-Pinion 1, 3-Wirerope 1, 4-Tension spring 1, 5-Adjusting mechanism 1, 6-Gear 2, 7-Pinion 2, 8-Wirerope 2, 9-Tension spring 2, 10-Adjusting mechanism 2, 11-Encoder sensor 1, 12-Auxiliary link, 13-Length adjusting mechanism, 14-Encoder sensor 2.'\",\"89\":null,\"90\":null,\"91\":null,\"92\":null,\"93\":\"'Leg Mode: The MiniRHex open source software is used to operate in leg mode [25]. A tripod gait pattern was used for legged locomotion.'\",\"94\":null,\"95\":null,\"96\":\"'Our design addresses propulsion efficiency by using a tunable resonant frequency that is achieved through anisotropic flexure stage stiffness. The device is operated at resonance in all of its possible excitation orientations and thereby minimizes current input into the VCM regardless of the thrust generated at the output of the diaphragm pump mechanism. Our design also enhances AUV localization capability through a monotonic relationship between operating frequency and the thrust vector profile. When acoustically coupled to the exterior surface of the AUV, the instantaneous resonant frequency of pump operation directly broadcasts the AUV\\u2019s thrust vectors to neighboring AUVs or stationary acoustic receivers. This technique avoids the standard digital communication paradigm and encodes what would normally be multiple bytes of thrust information into a single frequency. Finally, our proposed design achieves high reliability in multiple ways: 1) reduced part count compared to standard propeller designs; 2) mechanical component isolation using actuated diaphragms; 3) a high cycle flexure-based linear bearing mechanism; and 4) standard off-the-shelf actuators and components.'\\n\\n'We have described a bidirectional resonance-based diaphragm pumping mechanism that can be used to propel AUVs using a small number of readily fabricated components. In addition to operating efficiently at resonance, the device operating frequency can encode the thrust vector configuration of the AUV so that neighboring AUVs can better estimate relative and absolute positions. Our design minimizes the component count and surface interfaces associated with rotary and linear bearings by using a flexure based suspension and deforming diaphragms. Reduced component count and flexure-based mechanisms are expected to increase AUV propulsion system reliability. Our future work will focus on implementing and testing specific diaphragm designs for optimal thrust generation at the scale of typical AUVs. The performance of the localization system may also exhibit depth dependence and issues attendant with traditional acoustic sensing, which we also plan to assess in future work.'\\n\\n'Given the monotonic relationship between thrust vectors and operating frequency, useful information is encoded in the operating frequency of the pumps themselves. In its current conceptual form, the localization principle described in Section IV requires only simple hydrophone sensors. If we assume the AUV has identical pitch and yaw pump mechanisms, our approach would be limited to scenarios in which multiple AUVs are performing maneuvers in one plane of motion (e.g. pitching up or down; turning left or right) at a time so as to avoid confounding between frequencies producing pitch motions or frequencies producing yaw motions. However, this limitation can be overcome by designing the yaw and pitch pumps to operate within non-overlapping frequency ranges. Such designs form the basis for ongoing and future work. Overall, the bidirectional resonant pump mechanism addresses multiple challenges within AUV design and provides a promising foundation for further development.'\",\"97\":null,\"98\":null,\"99\":\"'https:\\/\\/github.com\\/xahidbuffon\\/srdrm'\\n\\n'http:\\/\\/irvlab.cs.umn.edu\\/resources\\/usr-248-dataset'\\n\\n'https:\\/\\/github.com\\/xahidbuffon\\/srdrm'\",\"100\":null,\"101\":\"'Two experiments were performed, with impedance modulation switched off and on, respectively. In both cases, the load was brought out of its equilibrium position by approx. \\u00b12rad, and then released. Due to limitations in the software implementation the sample rate was set to 200Hz; together with filtering of the encoder signals, this necessitated lowering the nominal stiffness to\\\\nK=\\\\n1\\\\n2\\\\nk\\\\nto ensure satisfactory performance. Higher order derivatives were neglected as the relatively low encoder resolution makes numerical approximation infeasible.'\\n\\n'Although some issues were observed in the experimental results, they validate the effectiveness of the energy-based safety concept. While Sec. III-B.1 showed that rendering the desired (modulated) impedance in principle requires higher order derivatives of system states, the results indicate that satisfactory performance can be achieved without them. Indeed, the system was able to enforce the safety limits to a large extent. Control performance was mainly limited by sample rate and encoder resolution, which are straightforward avenues to increase performance further.'\",\"102\":null,\"103\":null,\"104\":null,\"105\":null,\"106\":\"'Many more trajectories were recorded than appear in this description. A faulty encoder on the Husky rendered odometry unusable on Configuration A runs at the tunnel circuit. These runs were therefore not evaluated here as most of the techniques we evaluate are configured to rely upon odometry. One run with poor odometry, ex B route2.bag, was included in the evaluation to see how well these techniques could perform with poor odometry. The runs in Configuration A with poor odometry are expected to be released in the future or can be provided by request, as they could still be useful to evaluate visual SLAM.'\\n\\n'To support testing of SLAM systems independent of object recognition, artifact locations are coded into run files with the extension \\u0227rtifacts. These artifact locations are generated through the subt_scoring_node with the coding_mode parameter set to true. In coding mode, the scoring node generates the artifact file based upon user input. The location of AprilTags corresponding to the ground truth frame origin are generated through the use of the AprilTag library, which is provided as an entry in the included rosinstall file. As the AprilTags for the fiducial landmarks are passed, the best measurement as reported by the tag detection is kept and recorded into the artifacts file. The entry in the artifacts file consists of the\\u02d9 fiducial tag, the ROS time when the detection was made, the image coordinates of the detection (center), the camera frame ID, and 3D coordinates of the tag (using stereo for depth) and the robot base frame ID and 3D coordinates in the base frame. When the user sees that the robot is observing an artifact that should be coded into the file, they first pause the bag playback, and then click the button on the right of the coding window corresponding to the artifact type. The user then clicks on the center of the artifact in the image shown in the window. The same type of entry as described for fiducial landmarks is made in the artifacts file, except using the artifact type string.'\\n\\n'This dataset could be used to additionally test an object recognition and localization mechanism; this would require an extension to the scoring node to substitute artifact reports processed from the coded artifacts file with a report callback (which would have to be added to the scoring node). Care should be taken to limit the number of reports made on a single artifact; the artifacts were coded only on one appearance in a given \\\"loop\\\" (i.e. the robot had to travel a significant distance before revisiting the artifact for it to be subsequently recoded).'\\n\\n'Though this initial analysis used hand-coded artifact detections, this dataset has the potential for evaluating object recognition and localization alongside the current map accuracy evaluation if users wanted to run their own perception algorithms. A mechanism could be implemented in the scoring node that would accept global frame object reports and score them directly against the ground truth; replacing the coded artifact reports. We invite the community to submit pull requests to the repository that implement support for this mode of evaluation.'\\n\\n'This paper presents an approach and introduces new open-source tools that can be used to evaluate robotic mapping algorithms. Also described is an extensive subterranean ...'\\n\\n'This paper presents an approach and introduces new open-source tools that can be used to evaluate robotic mapping algorithms. Also described is an extensive subterranean mine rescue dataset based upon the DARPA Subterranean (SubT) challenge including professionally surveyed ground truth. Finally, some commonly available approaches are evaluated using this metric.'\\n\\n'We provide a set of open-source support tools to enable researchers to easily evaluate their own mapping approaches against this metric, including an approach for aligning their map coordinate frames with the global frame without using GPS.'\\n\\n'The first SLAM algorithm evaluated with this dataset is an approach from our previous work that was developed based upon the open-source OmniMapper [9]. This mapping system is a modular framework for integrating sensory measurements from potentially many modalities. It is configured for this analysis to utilize platform wheel odometry, gyro data for orientation only, and LiDAR data from the Ouster OS1-64. OmniMapper builds a pose graph along the robot\\u2019s trajectory, with connections between adjacent poses coming from Iterative Closest Point (ICP) [7] when a match is made with low residual error, and wheel odometry is substituted when no match is found. Wheel odometry is used as an initial guess to bootstrap the ICP iterations, which greatly accelerates convergence. When the robot revisits a location previously seen along its trajectory, the ICP procedure is used to find loop closures, where additional constraints are added to the graph. This graph of relative frame transformations is continuously optimized via iSAM2 [5] in the GTSAM package [2]. The dataset evaluations are performed in real time; however, only partially degraded performance is still achieved at 400% speed.'\\n\\n'We have selected a set of modern SLAM approaches that were available in the open source in addition to our own technique. These approaches were selected to provide some examples of using this dataset with different sensory modalities, including LiDAR and stereo vision. In addition, the raw proprioceptive odometric trajectory estimate is evaluated by itself to establish how necessary mapping is to achieving a high score on this challenge.'\\n\\n'We have presented a dataset, a metric, and analysis tools for evaluating mapping algorithms applied to underground tunnel environments. Thanks to the efforts of the DARPA Subterranean Challenge support team, surveyed ground truth landmark artifact positions are provided that extend deep below the earth in three mines which has enabled us to challenge researchers to test their own mapping systems. We have performed an initial evaluation on our own mapping system, and two options available in the open source, as well as an evaluation of how well the robot\\u2019s proprioceptive sensing alone could be used in place of mapping.'\",\"107\":null,\"108\":\"'As an external manifestation of human emotions, expression recognition plays an important role in human-computer interaction. Although existing expression recognition methods performs perfectly on constrained frontal faces, there are still many challenges in expression recognition in natural scenes due to different unrestricted conditions. Expression classification belongs to a pattern recognition problem where intra-class distance is greater than the inter-class distance, which leads to severe over-fitting when using neural networks for expression recognition. This paper proposes a novel net-work structure called Dimension Reduction Network which can effectively reduce generalization error. By adding a data dimension reduction module before the general classification network, a lot of redundant information is filtered, and only useful information is left. This can reduce the interference by irrelevant information when performing classification tasks and reduce generalization error. The proposed method does not require any modification to the classification network, only a small dimension reduction module needs to be added in front of the classification network. However, it can effectively reduce generalization error. We designed big and tiny versions of Dimension Reduction Network, both exceeds our baseline on AffectNet data set. The big version of our proposed method surpassed the state-of-the-art methods by more than 1.2% on AffectNet data set. Our code will open source 3 when the paper is accepted.'\",\"109\":\"'Hand Pose Estimation for Hand-Object Interaction Cases using Augmented Autoencoder'\\n\\n'Hand pose estimation with objects is challenging due to object occlusion and the lack of large annotated datasets. To tackle these issues, we propose an Augmented Autoencoder based deep learning method using augmented clean hand data. Our method takes 3D point cloud of a hand with an augmented object as input and encodes the input to latent representation of the hand. From the latent representation, our method decodes 3D hand pose and we propose to use an auxiliary point cloud decoder to assist the formation of the latent space. Through quantitative and qualitative evaluation on both synthetic dataset and real captured data containing objects, we demonstrate state-of-the-art performance for hand pose estimation with objects, even using only a small number of annotated hand-object samples.'\\n\\n'In this work, we propose a novel deep learning framework using Augmented Autoencoder to tackle hand-object interaction problem in hand pose estimation tasks. Our method takes 3D occluded hand point cloud as input, which is obtained by a random data augmentation process from clean hand samples. The encoder extracts point-wise features and fuses them to a latent vector. Addressing the problem of object occlusion in hand-object interaction cases, we use an auxiliary decoder to reconstruct the clean hand point cloud from the latent vector, and another decoder estimates simultaneously the 3D hand pose from the same latent vector. To the best of our knowledge, this is the first work that uses 3D point cloud data to tackle object occlusion problem in hand-object interaction tasks (Fig. 1).'\\n\\n'We propose an auxiliary clean hand reconstruction decoder to improve the quality of the latent space, which in turn improves the hand pose accuracy.'\\n\\n'In the following, we first review some hand pose estimation works on both clean hand and hand-object interaction cases. Then we briefly introduce the backbone of our framework, Augmented Autoencoder and the utilized point cloud reconstruction method, FoldingNet.'\\n\\n'C. Augmented Autoencoder and 3D Shape Reconstruction'\\n\\n'Augmented Autoencoder is the backbone of our method, which is firstly proposed by Sundermeyer et al. [28] in their real-time RGB-based pipeline for object detection and 6D pose estimation. In order to remove the effects of object occlusions and background clutters, they use an augmentation process to generate input data, which superimposes artificial occlusions and clutters to the clean data. Their work demonstrates that this training procedure is able to enforce the invariance of the encoded latent variable against a variety of different input augmentations. Encouraged by the idea of augmentation invariance, we apply a random augmentation process on clean hand samples of existing datasets to generate our input, and recover corresponding clean hand samples with an auxiliary 3D shape reconstruction decoder.'\\n\\n'The motivation behind our Augmented Autoencoder based hand pose estimation framework is to control what the latent vector encodes and which properties are ignored. To take advantages of current large-scale clean hand dataset, we apply a random augmentation process by superimposing random objects from ShapeNet [40] on clean hands to simulate hand-object interaction scenarios in reality. Simultaneously, the clean hand point cloud also serves as the ground-truth for reconstructed points by the auxiliary Decoder 1. Through this approach, we make the latent representation invariant against object occlusions when a hand is in contact with an object.'\\n\\n'Overview of our method (left) and the structure of the encoder (right). The input of our network is occluded hand point cloud, which is obtained by a random augmentation process from clean hand point cloud. The encoder encodes the input hand to a latent vector. The obtained latent vector is then used to reconstruct clean hand point cloud by the auxiliary Decoder 1 and predict 3D hand pose by Decoder 2. There are three losses in our VAE based framework, which are the KL loss, reconstruction loss and pose loss. (Brightness in point cloud indicates depth, i.e. darker denotes further.)'\\n\\n'B. Residual Permutation Equivariant Layer based Encoder'\\n\\n'In order to extract complex features, we use a 5-layer perceptron to encode F3 to the final K-dimensional latent vector, which consists of a latent mean vector \\u00b5 \\u2208 \\u211dK, and a latent standard deviation vector \\u03c3 \\u2208 \\u211dK.'\\n\\n'C. Decoder and Training Loss'\\n\\n'Decoder 1 is a FoldingNet [37] that transforms (\\\"folds\\\") 2d grid points of a square into 3D point cloud with two folding operations. In the folding operation, each grid point\\u2019s coordinate is concatenated with the latent vector z and fed into a 4-layer perceptron to construct a more complex shape compared to the input. The final reconstructed points\\\\nP\\\\n^\\\\nare evaluated by Chamfer Distance (CD) and Earth Mover\\u2019s Distance (EMD) [39] with respect to the ground-truth clean hand point cloud P \\u2208 \\u211dN\\u00d73. Note that the number of points in\\\\nP\\\\n^\\\\nis required to be the same as P.'\\n\\n'For 3D hand pose prediction, Decoder 2, which consists of 5 fully-connected layers, takes the reparameterized latent vector as input and outputs the vectorized 3D hand pose\\\\ny\\\\n\\u02c6\\\\n\\u2208\\\\nR\\\\nJ\\\\n, where J = 3\\u00d7#joints. The training loss between predicted hand pose\\\\ny\\\\n^\\\\nand ground-truth pose ygt \\u2208 \\u211dJ is the L2 loss:'\\n\\n'In this paper, we propose a novel deep learning framework using Augmented Autoencoder to handle hand pose estimation tasks for hand-object interaction cases. Our method consumes 3D hand point cloud and predicts accurate 3D hand pose. The proposed augmentation process and auxiliary clean hand reconstruction decoder implicitly force the latent representation of the pose only to be correlated to clean hand and the reconstructed clean hand despite the object occlusion in hand-object interaction cases. Furthermore, the proposed hand pose estimation training strategy is able to utilize existing clean hand datasets to tackle hand-object interaction cases. Quantitative and qualitative evaluation results show that our framework is capable of achieving low joint errors on both clean hand input (\\u223c 9 mm) and interacting hand input (\\u223c 14 mm). In the future work, more aspects of joint hand-object case will be investigated such as object pose estimation [44] and physical constraints. Another interesting aspect will be evaluating the grasp quality of reconstrcuted hand pose.'\",\"110\":null,\"111\":null,\"112\":\"'We compared our SSD-MR with representative models, specifically IDT, two-stream CNN, and SSD with LSTM. Basically, we employ the original tunings from IDT (HOG\\/HOF\\/MBH, codeword vector and support vector machine), two-stream CNN, and SSD with LSTM [34]. The two-stream CNN is trained on the PNM dataset in addition to the UCF101 pre-trained spatial- and temporal-stream. Additionally, the LSTM is assigned on behalf of the temporal convolution. Table III lists the results obtained on the PNM dataset.'\",\"113\":\"'Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.'\\n\\n'Illustration of the proposed worst-case graph matching approach for long-term place recognition with newly appearing landmarks. Given an image with detected landmarks, our approach constructs a graph representation that encodes visual appearances, distance relationships, and angular relationships of the landmarks in order to represent the place. Then, our approach formulates place recognition as a graph matching problem under the worst-case scenario. It matches places by computing the similarities of distance and angular spatial relationships of the landmarks with the least similar appearances (i.e., worst-case).'\\n\\n'The main novelty of this paper focuses on the proposal of the worst-case graph matching approach that integrates both landmark appearances and spatial relationships. Specifically, we design a unified graph representation that simultaneously encodes landmarks\\u2019 appearance cues as well as distance and angular relationships, which improves the expressiveness of the representation to encode places. Second, we introduce a novel formulation of long-term place recognition as a worst-case graph matching problem, which addresses the challenge caused by newly appearing and disappearing landmarks, and is able to compute the matching score directly from the graph representations of the query and template images, instead of requiring a separate matching procedure as in most existing methods using vector-based place representations.'\\n\\n'Most existing methods only used visual feature or simple spatial relationships of the landmarks and did not considered high order spatial relationships between the landmarks. In this paper, the proposed approach can explicitly encode visual feature and various spatial relationships of the landmarks.'\\n\\n'Given an input image, we extract landmarks to generate a graph representation, which encodes the spatial relationships of the landmarks to represent a place. Assume n landmarks are detected from the input image. Then, the positions of the landmarks in the image space are represented by the node set\\\\nP={\\\\np\\\\n1\\\\n,\\\\np\\\\n2\\\\n,\\u22ef,\\\\np\\\\nn\\\\n},where\\\\np\\\\ni\\\\n=[x,y]\\\\ndenotes the central position of the i-th landmark in the image at coordinate [x, y]. Given the position information, we construct the spatial relationships of landmarks, which are divided into two categories, including distance spatial relationship and angular spatial relationship. The distance spatial relationships are represented by a distance set\\\\nE={\\\\ne\\\\ni,j\\\\n}\\\\n, where ei,j denotes the distance of an edge constructed by nodes pi and pj. The angular spatial relationships are denoted by a angular set\\\\nT={\\\\nt\\\\ni,j,k\\\\n},where\\\\nt\\\\ni,j,k\\\\n=[\\\\n\\u03b8\\\\ni\\\\n,\\\\n\\u03b8\\\\nj\\\\n,\\\\n\\u03b8\\\\nk\\\\n],i,j,k=1,2,\\u2026,n,i\\u2260j\\\\n\\u2260k\\\\nrepresents the three angles of a triangle constructed by nodes pi, pj and pk. The angular relationship is robust to scale change, since angles of a triangle is invariant to scale change. Given the node set, distance set, and angular set, we can represent an input image as a graph\\\\nG=(P,T,E)\\\\n.'\\n\\n'We propose the novel worst-case graph matching approach that integrates spatial relationships of landmarks with appearance cues to perform long-term place recognition. Our approach employs graph representations to encode appearances and spatial relationships of landmarks in order to represent places. Then, our approach formulates place recognition as a worst-case graph matching problem, which maximizes the spatial similarity of the landmarks with the worst appearance similarity in order to address challenges caused by appearing and disappearing landmarks. In addition, the matching score of two places is directly computed by our approach without requiring further matching procedures. Experimental results on two public benchmark datasets have shown that our approach obtains promising long-term place recognition performance, with a changing number of landmarks.'\",\"114\":null,\"115\":null,\"116\":null,\"117\":null,\"118\":\"'All 6 desired trajectories used in closed-loop navigation experiments. Each desired trajectory is color-coded to show the direction of travel.'\\n\\n'Actual trajectories the robot traveled for each desired trajectory, color-coded by method. Desired velocity is 1.0m\\/s and IMU is high-end.'\\n\\n'Actual trajectories the robot traveled for each desired trajectory, color-coded by method. Desired velocity is 1.0m\\/s and IMU is low-end.'\\n\\n'While the ultimate use case of VI-SLAM in robotics is closed-loop navigation, traditional benchmarking of VISLAM employs open-loop analysis, i.e., the SLAM output doesn\\u2019t affect actual robot actuation and future sensory input. Though reflecting the estimation drift level of VI-SLAM, open-loop evaluation fails to fully address the coupled impact of navigation and VI-SLAM estimation during closed-loop operation. For targeted closed-loop navigation, it is hard to gain insights on VI-SLAM from published open-loop benchmark scores. To address this benchmarking gap, we present an open-source [1], reproducible benchmarking simulation for closed-loop VI-SLAM evaluation, and the outcomes from evaluating several VI-SLAM methods using it. Reproducible, closed-loop benchmarking should serve to guide future VISLAM research for mobile robotics.'\",\"119\":\"'PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous Convolution for Unorganized 3D Points'\\n\\n'Motivated by the success of encoding multi-scale contextual information for image analysis, we propose our PointAtrousGraph (PAG) - a deep permutation-invariant hierarchical encoder-decoder for efficiently exploiting multi-scale edge features in point clouds. Our PAG is constructed by several novel modules, such as Point Atrous Convolution (PAC), Edgepreserved Pooling (EP) and Edge-preserved Unpooling (EU). Similar with atrous convolution, our PAC can effectively enlarge receptive fields of filters and thus densely learn multi-scale point features. Following the idea of non-overlapping maxpooling operations, we propose our EP to preserve critical edge features during subsampling. Correspondingly, our EU modules gradually recover spatial information for edge features. In addition, we introduce chained skip subsampling\\/upsampling modules that directly propagate edge features to the final stage. Particularly, our proposed auxiliary loss functions can further improve our performance. Experimental results show that our PAG outperform previous state-of-the-art methods on various 3D semantic perception applications.'\\n\\n'Owing to the effectiveness in capturing spatially-local correlations of convolution operations, deep convolution neural networks (CNNs) have yielded impressive results for many image-based tasks. In order to encode multi-scale contextual information, CNNs probe incoming image features with filters or pooling operations at multiple rates and multiple effective fields-of-view [1], such as Atrous Spatial Pyramid Pooling (ASPP) [2] and Pyramid Pooling Module [3], [4]. Both strategies effectively capture the contextual information at multiple scales, hence significantly improving the capabilities of CNNs. Unorganized point cloud is a simple and straight-forward representation of 3D structures, which is frequently applied by modern intelligent robotics applications, such as autonomous driving and human-robot interactions. However, the unorderedness and irregularity of 3D point clouds make the conventional convolution operation inapplicable. Despite novel filtering kernels are recently proposed, limited studies have been carried out on designing deep hierarchical encoder-decoder architectures to learn multiscale point features for 3D semantic perception.'\\n\\n'In this paper, we propose the PointAtrousGraph (PAG) - a deep permutation-invariant hierarchical encoder-decoder to exploit multi-scale local geometric details with novel PAC modules for point cloud analysis. To address the overlapped neighborhood graph problem (elaborated in Sec. III), we apply our edge-preserved pooling (EP) operation to preserve critical edge features during subsampling. Therefore, our PAG can exploit and preserve multi-scale local geometrical details hierarchically. In a similar fashion, our edge-preserved unpooling (EU) operation is applied to recover the spatial information of sparse high-dimensional point features. Furthermore, we introduce tailored chained skip subsampling\\/upsampling modules to directly propagate point features from each hierarchy. Additionally, we propose novel auxiliary loss functions, maximum mean discrepancy (MMD) loss and deeply supervised loss, which further increases our inference accuracy. Our PAG also requires less training memory consumption and shorter training time than most existing networks that highly rely on neighborhood graphs in 3D points. Experiments show that our PAG achieves better performance than previous state-of-theart methods in various point cloud applications, including 3D object classification, object-part segmentation and 3D semantic segmentation.'\\n\\n'Hierarchical Encoder-Decoder.'\\n\\n'Typically, deep hierarchical encoder-decoder architectures contain: (1) an encoder module that progressively reduces the feature resolution, enlarges the receptive fields of filters and captures higher semantic information; (2) a decoder module that gradually recovers the spatial information'\\n\\n'an encoder module'\\n\\n'a decoder module'\\n\\n'Our PointAtrousGraph (PAG) is focused on learning multiscale edge features by applying a deep hierarchical encoderdecoder architecture. To maintain the permutation-invariant property, our PAG is made up of symmetric functions, such as shared mlp, max-pooling and feature concatenation. In particular, the PAC module is applied as a fundamental building block, which can effectively learn multi-scale dense edge features in 3D points. Furthermore, we propose our edge-preserved pooling (EP) operation, which benefits in constructing deep hierarchical networks by preserving critical edge features during subsampling processes. Our EP operation also enlarges the receptive fields by decreasing 3D point feature density. In a similar manner, our edgepreserved unpooling (EU) operation gradually recovers the high-dimensional point feature density by considering 3D point spatial locations. We also directly propagate point features from different hierarchies to the final stage by tailored chained skip subsampling\\/upsampling modules. In addition, we propose novel auxiliary losses to further increase our inference accuracy.'\\n\\n'In view of this, we propose our edge-preserved pooling (EP) module, which effectively captures local geometrical details while maintaining preserving respective features of each point. In line with the idea of non-overlapping max-pooling operations, we encode local edge features by considering neighboring point features in the original 1,024 points. Due to the absence of regular grids in 3D point clouds, we select neighboring point features by constructing neighborhood graphs in metric spaces. To preserve both distinctive individual point features and local geometrical edge features, our EP module is designed as:'\\n\\n'To recover spatial information for image features, lowlevel image features from encoder are often applied to refine the high-level features in decoder, especially when using a symmetric hierarchical encoder-decoder architecture [16], [25]. Our edge-preserved unpooling (EU) module also considers the point features of centroids and their local neighboring point features searched in metric spaces. Unlike PointNet++, our EU module does not need to consider the \\\"d-dim coordinates\\\" associated with each point:'\\n\\n'D. Deep Hierarchical Encoder-Decoder'\\n\\n'Based on our PAC, EP and EU modules, we construct the deep hierarchical encoder-decoder architecture PointAtrousGraph (PAG) to learn multi-scale features for 3D point classification and segmentation tasks (shown in Fig. 4). The same encoder architecture is applied by our classification and segmentation networks, which consists three hierarchies to gradually reduce the point feature density and meanwhile enlarge the receptive fields for learning higher semantic point features. Within each hierarchy, we successively lay out two PAC layers with increasing sampling rates to gradually exploit larger local geometric details. Our decoder architectures are designed differently with respect to different applications. In addition, we also propose different skip connection modules, chained skip subsampling and chained skip upsampling, for classification and segmentation, respectively.'\\n\\n'Our PointAtrousGraph (PAG) architecture (better view in color). Our classification and segmentation networks have the same designed network encoder architecture. Our classification network is enclosed by red dashed lines, and our segmentation network is enclosed by golden dashed lines. The chained skip subsampling module is applied in our classification network, which is enclosed by blue dash-dotted lines. Likewise, our chained skip upsampling module is enclosed by pink dash-dotted lines in our segmentation network.'\\n\\n'Hierarchical Encoder-Decoder Architecture. Inspired by the success of encoding rich contextual image information at multiple scales for image-based applications, we focus on capturing multi-scale local geometrical details in 3D points by a hierarchical encoder-decoder architecture. Our PAG contains: (1) an encoder module that progressively decreases point feature density, enlarges the field of view of filters and learns higher semantic edge features; (2) a decoder module that gradually recovers the density for high-dimensional point features. Deep hierarchical CNNs commonly use the non-overlapping max-pooling operations, which decrease the image feature resolution while summarizing local-spatial contextual information. To resolve the overlapped neighborhood graph problem, we propose our EP operation, which concatenates both \\\"centroid\\\" and \\\"neighbors\\\" features for hierarchically propagating edgeaware features. In our ablation studies, we observe that the \\\"neighbors\\\" features benefit in generating global features for classification tasks. The \\\"centroid\\\" features are beneficial for propagating respective information of each individual point, hence leading to better segmentation results. Following spatial pyramid pooling operations [57], [58], we can also perform our EP operations at several scales for future studies.'\\n\\n'Hierarchical Encoder-Decoder Architecture.'\",\"120\":null,\"121\":\"'Inspired by the developments in natural language processing and by the advancements in attention modeling, image captioning algorithms have relied on the combination of an image encoder and a natural language decoder. The interaction between vision and language has been modeled either using Recurrent Neural Networks or exploring more recent alternatives \\u2013 like one-dimensional convolutions or fully-attentive models such as the Transformer [10], [11]. Most of the last advancements in the field, however, are due to approaches which rely on complex forms of attention and of interactions between the visual and the textual do-main [12], [13], [11]. This is often done at the expense of the computational demands of the algorithm, thus limiting the applicability of these results to embedded agents and robots. Further, for these approaches to be adapted in robotics, they need to be re-thought in terms of efficiency, memory and, power consumption as well as in terms of their adaptability in real contexts.'\\n\\n'Our fully-attentive captioning model consists of an encoder module, in charge of encoding image regions, and a decoder module, which is conditioned on the encoder and generates the natural language description. In contrast to previous captioning approaches, which employed RNNs as the language model, we propose to use a fully-attentive model for both the encoding and the decoding stage, building on the Transformer model [10] for machine translation. In addition, we propose a self-attention region encoder with memory vectors to encode learned knowledge and relationships on the visual input. A summary of our approach and of its components is visually presented in Fig. 1.'\\n\\n'Both encoder and decoder consist of a stack of Transformer layers which act, respectively, on image regions and words. In the following, we revise their fundamental features. Each encoder layer consists of a self-attention and feed-forward layer, while each decoder layer is a stack of one self-attentive and one cross-attentive layer, plus a feed-forward layer. Both attention layers and feed-forward layers are encapsulated into \\\"add-norm\\\" operations, described in the following.'\\n\\n'In the encoding stage, the sequence of image regions is used to infer queries, keys and values, thus creating a self-attention pattern in which pairwise region relationships are modelled. In the decoder, we instead apply both a cross-attention and a masked self-attention pattern. In the former, the sequence of words is used to infer queries and image regions are used as keys and values. In the latter, the left-hand side of the textual sequence is used to generate keys and values for each element of the sequence, thus enforcing causality in the generation.'\\n\\n'Overview of our image captioning approach. Building on a Transformer-like encoder-decoder architecture, our approach includes a memory-aware region encoder which augments self-attention with memory vectors. Further, our approach is shallow, as it requires only two encoding and decoding layers.'\\n\\n'B. Memory-augmented region encoder'\\n\\n'C. Fully-attentive decoder'\\n\\n'The language model of our approach is composed of a stack of two decoder layers, each performing self-attention and cross-attention operations. As mentioned, each cross-attention layer uses the decoder output sequence to infer keys and values, while self-attention layers rely exclusively on the input sequence of the decoder. However, keys and values are masked so that each query can only attend to keys obtained from previous words, i.e. the set of keys and values for query qt are, respectively, {ki}i\\u2264t and {vi}i\\u2264t.'\\n\\n'At training time, the input of the encoder is the ground-truth sentence {BOS, w1, w2, ..., wn}, and the model is trained with a cross-entropy loss to predict the shifted ground-truth sequence, i.e. {w1, w2, ..., wn, EOS}, where BOS and EOS are special tokens to indicate the start and the end of the caption.'\\n\\n'TABLE I Captioning performance (without memory) as we vary the number of encoder and decoder layers.'\\n\\n'While at training time the model jointly predicts all output tokens, the generation process at prediction time is sequential. At each iteration, the model is given as input the partially decoded sequence; it then samples the next input token from its output probability distribution, until a EOS marker is generated.'\\n\\n'Specifically, given the output of the decoder we sample the top-k words from the decoder probability distribution at each timestep, and always maintain the top-k sequences with highest probability. We then compute the reward of each sentence wi and backpropagate with respect to it. The final gradient expression for one sample is thus:'\\n\\n'As mentioned, we use two layers in both the encoder and the decoder. The dimensionality of all layers, d, is set to 512 and we use H = 8 heads. The dimensionality of the inner feed-forward layer, df, is 2048. We use dropout with keep probability 0.9 after each attention layer and after position-wise feed-forward layers. Input words are represented with one-hot vectors and then linearly projected to the input dimensionality of the model, d. We also employ sinusoidal positional encodings [10] to represent word positions inside the sequence, and sum the two embeddings before the first encoding layer.'\\n\\n'We then evaluate the role of using persistent memory vectors in the encoder. Table II reports the performance obtained by our model with two layers and a number of memory slots per head varying from 0 to 100. As it can be seen, using 40 memory slots for each head further increases the CIDEr metric from 128.9 to 129.7.'\",\"122\":\"'Augmented Reality and mobile robots are gaining increased attention within industries due to the high potential to make processes cost and time efficient. To facilitate augmented reality, a calibration between the Augmented Reality device and the environment is necessary. This is a challenge when dealing with mobile robots due to the mobility of all entities making the environment dynamic. On this account, we propose a novel approach to calibrate Augmented Reality devices using 3D depth sensor data. We use the depth camera of a Head Mounted Augmented Reality Device, the Microsoft Hololens, for deep learning-based calibration. Therefore, we modified a neural network based on the recently published VoteNet architecture which works directly on raw point cloud input observed by the Hololens. We achieve satisfying results and eliminate external tools like markers, thus enabling a more intuitive and flexible work flow for Augmented Reality integration. The results are adaptable to work with all depth cameras and are promising for further research. Furthermore, we introduce an open source 3D point cloud labeling tool, which is to our knowledge the first open source tool for labeling raw point cloud data.'\\n\\n'Development of an open source 3D annotation tool for 3D point clouds'\\n\\n'The next step in the pipeline is the annotation of the acquired point cloud data. To the time of this work, no reliably, open source tool could be found dealing with the direct annotation of 3D point cloud. On this account, we propose a 3D point cloud labeling tool for annotating point cloud data. In the following, the tool will be explained briefly. The tool takes as input raw point cloud data which will be visualized with the python library PPKT. In order to annotate the robot in the visualizer, the user has to select 3 points that represent 3 corners of the base of the robot, which are used by the tool to compute the fourth corner together with the base plane on which the robot is situated. Each point inside the point cloud is projected onto the base plane. All points situated inside the calculated base are considered as part of the object. Out of these points, the highest one which is smaller than a predefined threshold is picked and the coordinates will be used to compute the height of the bounding box. The center is computed by choosing the point on the normal of the base plane that starts from its center, it is at a distance of height\\/2 from the base plane and it is situated inside the robot. The tool goes through each point cloud, renders it, waits for the user to select the robot inside the visualizer by choosing 3 points and then computes the center of the bounding box corresponding to that selection together with its size and rotation according to above explanations.'\\n\\n'Recent research focus on automated calibration methods rather than manual ones. Most ubiquitous are marker based approaches. Open source libraries and toolkits like Aruco or Vuforia make it possible to deploy self created fiducial markers for pose estimation. The marker is tracked using computer vision and image processing. Especially when working with static robots, marker based approaches are widely used due to the simple setup and competitive accuracy. Several work including Aoki et al. [15], Ragni et al. [16] or [17] et al. relied on a marker based calibration between robot and AR device. However, Baratoff et al. [18] stated, that the instrumentation of the real environment, by placing additional tools like markers, sensors or cameras is a main bottleneck for complex use cases especially in navigation that are working with dynamically changing environments. Furthermore, deploying additional instruments require more time to setup and are not able to react to changes within the environment. Especially for industrial scenarios this is the main concern [18].'\",\"123\":\"'https:\\/\\/youtu.be\\/NADwzcm_FmQ.'\",\"124\":null,\"125\":null,\"126\":null,\"127\":\"'https:\\/\\/yixingluo.github.io\\/SCMP.github.io\\/'\\n\\n'https:\\/\\/yixingluo.github.io\\/SCMP.github.io\\/'\",\"128\":null,\"129\":\"'Radu Corcodel'\",\"130\":\"'https:\\/\\/www.youtube.com\\/watch?v=YcEaFTjs-a0'\\n\\n'Benchmark comparisons and real-world experiments that validate the performance of our proposed method. The source code of our implementation will be released.'\\n\\n'Gradient-based trajectory optimization (GTO) has gained wide popularity for quadrotor trajectory replanning. However, it suffers from local minima, which is not only fatal to safety but also unfavorable for smooth navigation. In this paper, we propose a replanning method based on GTO addressing this issue systematically. A path-guided optimization (PGO) approach is devised to tackle infeasible local minima, which improves the replanning success rate significantly. A topological path searching algorithm is developed to capture a collection of distinct useful paths in 3-D environments, each of which then guides an independent trajectory optimization. It activates a more comprehensive exploration of the solution space and output superior replanned trajectories. Benchmark evaluation shows that our method outplays state-of-the-art methods regarding replanning success rate and optimality. Challenging experiments of aggressive autonomous flight are presented to demonstrate the robustness of our method. We will release our implementation as an open-source package 1 .'\",\"131\":\"'Comparison of exploration directions proposed by LBPlanner and GBPlanner in test simulation environments. Both planners were triggered at each point shown, but the LBPlanner paths were executed. The color code for each planner iteration is determined by the dot product between the exploration directions proposed by each planner. Subfigure a) shows high correlation between the proposed exploration directions at all points (average 19\\u00b0 absolute angle difference). Subfigure b) shows the GBPlanner identifying the dead-end (4) before the LBPlanner (5), as well as the LBPlanner\\u2019s failure to explore the other branch.'\",\"132\":\"'https:\\/\\/www.youtube.com\\/watch?v=onOc05Ymxzs'\",\"133\":null,\"134\":null,\"135\":\"'This work suggests an integrated approach for a drone (or multirotor) to perform an autonomous videography task in a 3-D obstacle environment by following a moving object. The proposed system includes 1) a target motion prediction module which can be applied to dense environments and 2) a hierarchical chasing planner. Leveraging covariant optimization, the prediction module estimates the future motion of the target assuming it efforts to avoid the obstacles. The other module, chasing planner, is in a bi-level structure composed of preplanner and smooth planner. In the first phase, we exploit a graph-search method to plan a chasing corridor which incorporates safety and visibility of target. In the subsequent phase, we generate a smooth and dynamically feasible trajectory within the corridor using quadratic programming (QP). We validate our approach with multiple complex scenarios and actual experiments. The source code and the experiment video can be found in https:\\/\\/github.com\\/icsl-Jeon\\/traj_gen_vis and https:\\/\\/www.youtube.com\\/watch?v=_JSwXBwYRl8.'\",\"136\":null,\"137\":null,\"138\":null,\"139\":\"'TABLE I: Evaluation on the TRAF dataset with MOTDT [53] and MDP [54]. MOTDT is currently the best online tracker on the MOT benchmark with open-sourced code. Bold is best. Arrows (\\u2191, \\u2193) indicate the direction of better performance. Observation: RoadTrack improves the accuracy (MOTA) over the state-of-the-art by 5.2% and precision (MOTP) by 0.2%.'\\n\\n'(Sparse) MOT & KITTI-16 Datasets: There are now several popular open-source tracking benchmarks available on which researchers can test and compare the performance of tracking algorithms. The current state-of-the-art benchmark is the MOT benchmark [8], which contains a mix of pedestrians and traffic sequences. However, the MOT benchmark is a general tracking benchmark dataset. Therefore, we additionally conduct experiments exclusively on the KITTI-16 traffic sequence [50]. It should be noted that the KITTI-16 sequence is sparse, consisting of mostly cars, and does not contain road-agent interactions.'\\n\\n'Due to the open-source nature of the MOT benchmark, there are a large number of methods available in the MOT benchmark (80 and 87 on 2D MOT15 and MOT16, respectively). To demonstrate the superiority of RoadTrack, it is therefore sufficient to select state-of-the-art methods from all the methods, and compare RoadTrack against this set of methods. We define a state-of-the-art method as one that satisfies all of the following criteria simultaneously:'\",\"140\":\"'For multi target tracking, the main challenge is that the association between measurements and targets is unknown. If the associations were known because transmissions contain individual codes, the problem can simply be reduced to single target tracking, separately for every target. Multi target tracking can be treated with the Joint Probabilistic Data Association Filter [11], but this requires enumerating all possible association hypotheses. Other methods like the probability hypothesis density filter avoid the measurement associations in their formulation [12], [13]. Further interesting possibilities are symmetric and association-invariant transformations like the SME Filter [14], the Kernel-SME Filter [15], [16], or methods based on association-invariant set distance measures [17], [18]. Most methods however have been proposed for linear measurement models or measurements with the full dimensionality only, and cannot easily be applied to nonlinear subspace measurements like TOA for emitter localization.'\\n\\n'Together with Frequentis Comsoft GmbH we analyzed real data from a 20 min national air traffic control data set based on the Mode A\\/C radio frequency communication protocol. Overall, it provided 440 532 TOA events with 4090 different Mode C target identification codes. All targets actually had individual codes and the targets thus could also be localized without association-free MLAT, but this information was used for ground truth only. We focused on four specific targets with the codes 2415, 2416, 4442, 5403. This restricted the data set to 157 585 TOAs from 14 receivers covering roughly 30 000 km2, where the four considered targets stayed within an area of about 3500 km2. Frequentis Comsoft Quadrant interrogators always transmit in a specific A-C-A-A pattern of four Mode A and Mode C requests, in order to be able to recognize Mode A\\/C messages that were triggered by own interrogations. Furthermore, the four answers in combination provide a more accurate single TOA measurement. The data set has been specifically chosen for testing our association-free multi target localization method. Targets perform quite complex maneuvers simultaneously, two of them in close formation. On the other hand, the environment is chosen such that there are no other major difficulties like reflections present. We can thus assume that measurements are free of clutter, but with missed detections.'\\n\\n'The Mode C transmissions contain pressure altitude, but this information was not used - only the TOA measurements of the transmissions together with the known receiver positions. Mode A transmissions contain a 12-bit code to identify the targets and all considered targets had unique IDs, but this information was only used as ground truth in order to evaluate the proposed multi target localization method. The maximum distance between two receivers was 233 km, so pauses of 0.78 ms and more could be seen as a guaranteed separation between dense transmission events. The A-C-A-A interrogation sequence was emitted in intervals of at least 10 ms, therefore 10 711 dense transmission events were clearly separated. According to the Mode A codes from ground truth, 9873 of the dense transmission events actually contained messages from more than one target. In order to be able to apply our method without further modifications, we determined the number of targets\\\\nP\\\\nfrom ground truth for every dense transmission event. For the MLAT algorithm, the actual number of targets behind a set of TOA is however not known in reality. This will be addressed in future works.'\\n\\n'Code'\\n\\n'Code & Datasets'\",\"141\":\"'A toy case study on MNIST is carried out to validate our hypothesis. Besides, experiments are taken on two public datasets and show good performance. We also open the source code for reproduction 1.'\\n\\n'Under the hypothesis that an image is composed of place and appearance, our method uses the convolutional network as a feature extractor to disentangle place and appearance features. Specifically, autoencoder is used as the backbone. An image is embedded into feature space, and the feature is transformed back to image space. In this paper, part of feature encodes place and other encodes appearance, which are disentangled into place space S and appearance space A respectively.'\\n\\n'As in many self-supervised feature learning literature, encoders (\\\\nE\\\\nS\\\\nand\\\\nE\\\\nA\\\\n) and decoder G are used to re-construct original input image cooperatively. To measure reconstruction quality, L2 distance is used, thus the overall reconstruction loss is expressed as:'\\n\\n'Pure autoencoder cannot guarantee that the place feature only captures domain-unrelated information. It may contain appearance content, which is not penalized by reconstruction loss. To overcome this problem, we use adversarial learning, where place domain discriminator Dpla is introduced to constrain place features to lie in the same latent space. In each training iteration, two place features are extracted from two images, which may come from the same or different domains. Dpla tries to tell whether they are from the same domain, as shown in Fig. 2 (top right). For example, given place feature s1 from domain\\\\nD\\\\n1\\\\n, we sample the other two place features\\\\ns\\\\n\\u2032\\\\n1\\\\nand s2 from\\\\nD\\\\n1\\\\nand\\\\nD\\\\n2\\\\nrespectively. The loss function for this case can be written as'\\n\\n'Simultaneously, place encoder\\\\nE\\\\nS\\\\nare encouraged to confuse Dpla by outputting place features following the same distribution across domains, so that they are invariant against varied appearance. The autoencoder is trained in an adversarial paradigm against Dpla, and its loss function can be expressed as:'\\n\\n'The autoencoder in our method is shared across domains. Besides, discriminators use information between two domains, and they are domain-unrelated. Thus, it is easy to extend to multiple domains. Assume that there are N domains, denoted by\\\\nD\\\\n1\\\\n,\\\\nD\\\\n2\\\\n,\\u2026,\\\\nD\\\\nN\\\\n. In each training iteration, two domains\\\\nD\\\\ni\\\\n and \\\\nD\\\\nj\\\\nare randomly drawn, where i, j = 1, 2,\\u2026, N and i \\u2260 j. Then images from\\\\nD\\\\ni\\\\n and \\\\nD\\\\nj\\\\nare sampled respectively from training set, which are input data of encoders (Eq. (3)). In testing phase, images from different domains are fed into\\\\nE\\\\nS\\\\nto obtain place features.'\\n\\n'Network Architecture Encoders (\\\\nE\\\\nS\\\\nand\\\\nE\\\\nA\\\\n), decoder G and discriminators (Dapp and Dapp) are all convolutional networks. They are shared among different domains. The autoencoder is bottleneck architecture, where encoders downsample an image as two feature maps and the decoder jointly upsamples them back to the original resolution. The appearance feature is a vector of dimension\\\\nn\\\\nA\\\\n. Input features of discriminators, (si, sj) or (si, aj), are concatenated together before going into the discriminators, and they are downsampled to one dimension as output.'\\n\\n'Place Recognition To demonstrate the discriminative ability of the learned features, we use place features as descriptors in place recognition. Specifically, given a sequence of already observed images, we extract their place features by feeding those images into place encoder\\\\nE\\\\nS\\\\n, constituting the database feature set {sDB,i}, where i = 1,\\u2026, NDB and NDB is number of database images. When a new query image comes, the query feature sQ is extracted from\\\\nE\\\\nS\\\\n. Then, the best-matched image xm in the database is determined by'\\n\\n'To understand better what has place feature learned, the vector of appearance feature is set to zero, and the out-put of the decoder is displayed in Fig. 3c (called zero-appearance image). One can see that when the appearance feature is fixed, all decoded images have the same color. It illustrates that the decoupled place feature contains only place information (digit) of any input digit image. Fig. 3c also presents reconstructed images decoded from place and appearance features of different domains (called translated image). Specifically, the decoder combines the place feature from the first input image and appearance feature from the second input image to reconstruct digit. Results show that the digits of translated images are determined by place features, while colors are controlled by appearance features.'\\n\\n'In this paper, we propose a self-supervised feature learning method to disentangle the place and appearance features, and the place feature is leveraged in the place recognition task. An autoencoder, including place encoder, appearance encoder, and decoder, is trained as a self-supervised feature extractor. To make place feature domain-unrelated, place domain discriminator is proposed. Besides, appearance compatibility discriminator is used to eliminate dependency between place and appearance features. We start from a toy case to illustrate the disentanglement effect. Experiments on real datasets show that the disentangled place feature is suitable for the place recognition task. It achieves comparable results to several existing methods. We also present the generalization ability of our method. An extension strategy is shown to prove that our method is easy to add new domains, which is also found to be beneficial to reduce model complexity without sacrificing place recognition performance.'\",\"142\":null,\"143\":null,\"144\":\"'Number of feature detections and found associations versus time. Color code: Pose estimate classified as valid (green), otherwise (red). For zero associations, the prior pose is propagated using odometry measurements.'\\n\\n'Localization accuracy in longitudinal and lateral direction as well as orientation. Color code: valid solution (blue), invalid solution (red).'\",\"145\":null,\"146\":null,\"147\":\"'https:\\/\\/github.com\\/vflorence\\/RSLOS.'\\n\\n'An overview of our method. After collecting images of an object (left), the robot uses encoder readings and active depth sensing to segment the foreground (manipulator or manipulator + grasped object) of each image (middle-left, grey). Using its self-recognition network (middle, blue), the robot isolates the object from the rest of the foreground (middle-right, red). Thus, the robot generates densely-labeled annotations of newly encountered objects (right).'\\n\\n'In-hand object segmentation methods use robot encoder feedback to locate a grasped object and various methods to reason about robot-object occlusion. Omr\\u010den et al. [9] incorporate many data sources such as color distribution, a disparity map, and a pretrained Gaussian Mixture Model (GMM) of the hand to segment unknown objects, but their method does not extract pixelwise object labels. Welk et al. [10] use Eigen-backgrounds, disparity mapping, and tracking on a model of the robot manipulator to isolate objects. Krainin et al. [8] take a self-recognition based approach to object learning by matching a robot manipulator to its 3D mesh model in order to isolate and model an inhand object. However, their method requires a 3D geometric model of the robot manipulator and focuses on modeling non-deformable objects from multiple viewpoints. Browatzki et al. [6], [7] use a GMM trained on pixel values around a bounding box to isolate a grasped object. This method focuses on viewpoint selection and data association, but their object isolation technique is not robust to pixel-level similarity between the object, background, and robot. While these systems are able to isolate unseen objects for visual learning and oftentimes model the occlusion of the robot manipulator by these objects, they are limited by the need for custom manipulator models, environment-specific heuristics, and parameter tuning.'\\n\\n'Next, we use the robot\\u2019s encoder readings and kinematic model to get approximate 3D coordinates of link positions and project them into the depth image. We take the 3D link position of each ith link relative to the camera\\u2019s coordinate frame, Pi := [xi,yi,zi]\\u22a4, and find the projected depth-image coordinates using the transform'\\n\\n'https:\\/\\/github.com\\/vflorence\\/RSLOS.'\",\"148\":null,\"149\":null,\"150\":null,\"151\":\"'Ordered classification: Standard classification considers independent categories and does not penalize major ordering mistakes. In order to represent the succession of progress classes, we thus encoded the target vectors with the ordinal formulation of [36] as represented in Fig. 3, and substituted the categorical MCE loss with the Mean Binary Cross Entropy (MBCE) loss (i.e. Sigmoid activation function and MCE loss). MBCE sets up an independent binary classifier for each class and, in combination with the ordinal target encoding, generates a larger loss the further the prediction is from its ground truth. After model training, progress predictions are obtained from the output yp of this classifier by finding the first index k where\\\\ny\\\\np\\\\nk\\\\n<0.5\\\\n.'\\n\\n'We used the open source TensorFlow implementation of the Bidirectional LSTM presented in [12] as our baseline (A). We also relied on the provided training parameters, since they were carefully tuned on the same dataset. Given the stochastic nature of the optimization process, all experiments were performed three times and results were averaged. All runs were trained on NVIDIA Tesla V100-DGXS GPU, with training time of about 1 hour per run.'\",\"152\":null,\"153\":\"'https:\\/\\/youtu.be\\/LKFDB_BOhl0'\",\"154\":null,\"155\":\"'In terms of code, we merely condense the local grid coordinates and the corresponding image pixel coordinates into a 2D vector array of length (2 \\u00d7 Spp)2 and run vector search in the indices corresponding to the grid rows of the wheel pairs, making the search less cumbersome. Hence, we have all the image coordinates, corresponding image pixels, and corresponding local grid coordinates of all the pixels near the two pairs of wheels.'\",\"156\":\"'https:\\/\\/www.youtube.com\\/watch?v=htI8202vfec'\",\"157\":null,\"158\":null,\"159\":null,\"160\":\"'We encode this problem with running costs gi (2) expressed as weighted sums of the following:'\",\"161\":null,\"162\":\"'For long exposure photography, it does not matter at what exact time the system passes through each waypoint. This renders hard-coded activations such as in Eq. (2) unnecessary and motivates a more flexible approach. Namely, instead of pre-specifying the activations \\u03b1t, we treat them as optimization variables. More concretely, we parameterize the coefficients \\u03b1t by Radial Basis Functions (RBFs) of the form'\",\"163\":null,\"164\":null,\"165\":null,\"166\":null,\"167\":\"'Gripper workspace spheres (right), for the Franka panda gripper (upper) and the Allegro right hand (lower), featuring 10 spheres per finger with the color code: thumb(red), index(green), middle(blue), and pinky(grey). The real hardware shown to the left, fitted with the Intel Realsense d435 depth camera.'\",\"168\":null,\"169\":\"'On joint optimization of motion compensation, quantization and baseline entropy coding in H.264 with complete decoder compatibility'\",\"170\":null,\"171\":null,\"172\":\"\\\"Many species have evolved advanced non-visual perception while artificial systems fall behind. Radar and ultrasound complement camera-based vision but they are often too costly and complex to set up for very limited information gain. In nature, sound is used effectively by bats, dolphins, whales, and humans for navigation and communication. However, it is unclear how to best harness sound for machine perception.Inspired by bats' echolocation mechanism, we design a low- cost BatVision system that is capable of seeing the 3D spatial layout of space ahead by just listening with two ears. Our system emits short chirps from a speaker and records returning echoes through microphones in an artificial human pinnae pair. During training, we additionally use a stereo camera to capture color images for calculating scene depths. We train a model to predict depth maps and even grayscale images from the sound alone. During testing, our trained BatVision provides surprisingly good predictions of 2D visual scenes from two 1D audio signals. Such a sound to vision system would benefit robot navigation and machine vision, especially in low-light or no-light conditions. Our code and data are publicly available.\\\"\\n\\n'To the best of our knowledge, our BatVision is the first work that generates scene depth maps from binaural sound only. Our code, model, and data are available at https: \\/\\/github.com\\/SaschaHornauer\\/Batvision.'\\n\\n'We use an encoder-decoder network architecture to turn the audio clip into the visual image, and further improve the quality of generated images using an adversarial discriminator to contrast them against the ground-truth (Fig. 4).'\\n\\n'Our sound to vision network architecture. The temporal convolutional audio encoder A turns the binaural input into a latent audio feature vector, based on which the visual generator G predicts the scene depth map. The discriminator D compares the prediction with the ground-truth and enforces high-frequency structure reconstruction at the patch level.'\\n\\n'A. Our Audio Encoder A'\\n\\n'Encoder for Waveforms.'\\n\\n'Encoder for Spectrograms.'\\n\\n'Our audio encoder for the raw waveform.'\\n\\n'TABLE I Layer configuration of our waveform audio encoder'\\n\\n'The generator decodes the latent audio feature vector and expands it into visual scene image. For raw waveforms, successive deconvolutions yield the best results, whereas for spectrograms, a UNet-type encoder-decoder network [19] yields best results. We investigate several resolutions for reconstructed images, from 16 \\u00d7 16 to 128 \\u00d7 128.'\\n\\n'Decode by A UNet. To transform the output of our audio encoder to a 2D image representation suitable for a UNet, we reshape the 1024-dimensional feature vector into a 32\\u00d732\\u00d71 tensor. For spectrograms, where the audio encoder outputs a 1 \\u00d7 f \\u00d7 1024 vector and f = 1, we first apply two fully connected linear layers before reshaping it into a 32 \\u00d7 32 \\u00d7 1 tensor. The output of this generator depends on the target resolution, e.g. 128 \\u00d7 128 \\u00d7 1.'\\n\\n'Decode by A UNet.'\\n\\n'The encoder of the UNet downsamples the 32\\u00d732\\u00d71 input through several layers of double convolutions followed by batch normalization and ReLU, whereas the decoder of the UNet upsamples the input through double de-convolutions followed by batch normalization and ReLU. Skip connections are utilized wherever possible.'\\n\\n'Decode from Direct Upsampling.'\",\"173\":null,\"174\":\"'Despite the utility and benefits of omnidirectional images in robotics and automotive applications, there are no datasets of omnidirectional images available with semantic segmentation, depth map, and dynamic properties. This is due to the time cost and human effort required to annotate ground truth images. This paper presents a framework for generating omnidirectional images using images that are acquired from a virtual environment. For this purpose, we demonstrate the relevance of the proposed framework on two well-known simulators: CARLA Simulator, which is an open-source simulator for autonomous driving research, and Grand Theft Auto V (GTA V), which is a very high quality video game. We explain in details the generated OmniScape dataset, which includes stereo fisheye and catadioptric images acquired from the two front sides of a motorcycle, including semantic segmentation, depth map, intrinsic parameters of the cameras and the dynamic parameters of the motorcycle. It is worth noting that the case of two-wheeled vehicles is more challenging than cars due to the specific dynamic of these vehicles.'\\n\\n'This paper presented a general framework to generate datasets of omnidirectional images from virtual environments, and provided the OmniScape dataset. We demonstrated the relevance of this framework by generating fisheye and catadioptric images with depth map, semantic segmentation and dynamic parameters. Two simulators were investigated with success, GTA V and open-source CARLA Simulator.'\\n\\n'In complement to the images given in this paper, more examples from CARLA Simulator and GTA V can be found in OmniScape GitHub.'\",\"175\":\"'Training was conducted on a graphics processing unit (a single 12 GB NVIDIA GeForce GTX Titan), and the code was written with Keras. The mean squared error was used as the loss, and the Adam optimizer was utilized for fast and robust training performance [32]. To obtain generalizable results, Gaussian noise with a measured standard deviation was added to the input tensor during training. The imposition of the simulated noise takes a role of regularization [33], and using the exact value of the measured standard deviation circumvents heuristic selection of hyperparameters. The minibatch size was 360. In each epoch, the network was validated on the test dataset. The validation loss decreased and eventually converged to 6.42e-4, while the training loss converged to 6.17e-4. The performance of the trained network will be validated with experimental data in Section IV.'\",\"176\":null,\"177\":\"'FE analysis codes developed in MATLAB (Mathworks, USA) and based on linear elastic formulation. Each layer is discretized using 8-noded isoparametric brick elements that are assigned isotropic material properties. To increase the calculation accuracy, the FE mesh was designed with our predefined test points in mind. The nodes at the bottom of the sensor are fixed to the rigid support. Details of the adopted FE formulation are explained by Liu and Quek [22].'\",\"178\":null,\"179\":\"'Throughout this section, we use the point deformation models trained with Auto-ET and data from 10 poking locations. All implementation is performed in Python code with the Numpy package for linear algebra, and all timing is conducted on a desktop PC with an Intel i7 3.8GHz processor.'\",\"180\":\"'We present a novel type of six-axis force-torque sensor using fiducial tags and a webcam. The design is fast to fabricate and simple to use, and is also strong enough to survive drops and crashes common in contact-rich tasks such as robotic grasping. With only 3D-printed custom components, the design needs minimal technical expertise to adapt to applications ranging from manipulation to human-computer interaction research. The open-source design also allows for direct integration in designs for tasks such as grasping where sensor size is important. This fiducial-based sensor is less accurate than commercial force-torque sensors, but is also orders-of-magnitude less expensive \\u2013 commercial sensors can cost thousands of dollars, while the parts cost of our sensor is under $50 (see Table II). These combined advantages of our prototype sensor validates the general design principle of using 3D pose estimates from printed fiducials to create a six-axis force-torque sensor. Future work on improving the Fz and Mz axes could allow for an inexpensive, user-friendly, and robust alternative to current commercial sensors, opening up a new range of use cases for six-axis force-torque sensors.'\\n\\n'Commercial six-axis force-torque sensors suffer from being some combination of expensive, fragile, and hard-touse. We propose a new fiducial-based design which addresses all three points. The sensor uses an inexpensive webcam and can be fabricated using a consumer-grade 3D printer. Open-source software is used to estimate the 3D pose of the fiducials on the sensor, which is then used to calculate the applied force-torque. A browser-based (installation free) interface demonstrates ease-of-use. The sensor is very light and can be dropped or thrown with little concern. We characterize our prototype in dynamic conditions under compound loading, finding a mean R 2 of over 0.99 for the F x , F y , M x , and M y axes, and over 0.87 and 0.90 for the F z and M z axes respectively. The open source design files allow the sensor to be adapted for diverse applications ranging from robot fingers to human-computer interfaces, while the sdesign principle allows for quick changes with minimal technical expertise. This approach promises to bring six-axis force-torque sensing to new applications where the precision, cost, and fragility of traditional strain-gauge based sensors are not appropriate. The open-source sensor design can be viewed at http:\\/\\/sites.google.com\\/view\\/fiducialforcesensor.'\",\"181\":null,\"182\":null,\"183\":null,\"184\":\"'We compare several different models and learning signals for training the commander. The most basic model does not utilise the attention mechanism and simply sums over all the encoded guidance for command prediction. We term this AllSum. Then, since soft attention is reported to have similar performance as AllSum in [16], we consider the hard attention model as described in Sec IV-A which is purely optimised with the command loss learning signal and is termed as HardAtt. Next, we add the REINFORCE (REINFORCE) or metric learning (Metric) learning signal. We also combine three learning signals together (Combined).'\\n\\n'The Euclidean distance matrix of the encoded images in a sampled video with two direction changing actions. Note that REINF indicates the REINFORCE algorithm.'\",\"185\":\"'While we limit the discussion for space reasons, it is worth mentioning that Kimera also provides an open-source suite of evaluation tools for debugging, visualization, and benchmarking of VIO, SLAM, and metric-semantic reconstruction. Kimera includes a Continuous Integration server (Jenkins) that asserts the quality of the code (compilation, unit tests), but also automatically evaluates Kimera-VIO and Kimera-RPGO on the EuRoC\\u2019s datasets using evo [71]. Moreover, we provide Jupyter Notebooks to visualize intermediate VIO statistics (e.g., quality of the feature tracks, IMU preintegration errors), as well as to automatically assess the quality of the 3D reconstruction using Open3D [72].'\\n\\n'(a) Kimera\\u2019s 3D mesh color-coded by the distance to the ground-truth point cloud. (b) Ground-truth point cloud color-coded by the distance to the estimated cloud. EuRoC V1_01 dataset.'\\n\\n'We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual ...'\\n\\n'We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.'\\n\\n'Kimera is an open-source C++ library for real-time metric-semantic SLAM. It provides (a) visual-inertial state estimates at IMU rate, and a globally consistent and outlier-robust trajectory estimate, computes (b) a low-latency local mesh of the scene that can be used for fast obstacle avoidance, and builds (c) a global semantically annotated 3D mesh, which accurately reflects the ground truth model (d).'\\n\\n'TABLE I: Related open-source libraries for visual and visual-inertial SLAM (top) and metric-semantic reconstruction (bottom).'\\n\\n'Table II compares the Root Mean Squared Error (RMSE) of the Absolute Translation Error (ATE) of Kimera-VIO against state-of-the-art open-source VIO pipelines: OKVIS [73], MSCKF [74], ROVIO [75], VINS-Mono [24], and SVO-GTSAM [76] using the independently reported values in [77] and the self-reported values in [24]. Note that these algorithms use a monocular camera, while we use a stereo camera. We align the estimated and ground-truth trajectories using an SE(3) transformation before evaluating the errors. Using a Sim(3) alignment, as in [77], would result in an even smaller error for Kimera: we preferred the SE(3) alignment, since it is more appropriate for VIO, where the scale is observable thanks to the IMU. We group the techniques depending on whether they use fixed-lag smoothing, full smoothing, and loop closures. Kimera-VIO and Kimera-RPGO achieve top performance across the spectrum.'\\n\\n'Kimera is an open-source C++ library for metric-semantic SLAM. It includes state-of-the-art implementations of visual-inertial odometry, robust pose graph optimization, mesh reconstruction, and 3D semantic labeling. It runs in real-time on a CPU and provides a suite of continuous integration and benchmarking tools. We hope Kimera can provide a solid basis for future research on robot perception, and an easy-to-use infrastructure for researchers across communities.'\",\"186\":\"'A new approach to sample-efficient RL training for goal-driven navigation tasks. We use VPR and deep learning models to encode our sensory input images which, when combined with goal destination signals, can generate compact, bimodal representations, from which a navigation policy can be learned to generalize across extreme visual changes such as day to night or summer to winter cycles.'\\n\\n'Our goal is to train a policy network to perform goal-driven navigation tasks. To enable sample-efficiency, we use either off-the-shelf VPR or deep learning models to encode our sensory input images and obtain multi-dimensional feature vectors. Then, using RL, we combine these features with compact goal destinations, resulting in compact, bimodal representations that can then be used to train our policy using a single traversal in our CityLearn environment.'\\n\\n'We encode our sensory input images \\u2013 which are either 1920\\u00d71080 RGB for the Nordland dataset or 1280\\u00d7960 RGB for the Oxford RobotCar dataset \\u2013 using either off-the-shelf VPR (NetVLAD) or deep learning (ResNet-50) models. For NetVLAD [6], we use their best performing network, based on VGG-16 [72] with PCA plus whitening, to encode our images into a range of visual observations consisting of 4096-d, 2048-d, 512-d and 64-d feature vectors. For ResNet-50 [57], we use a network trained on ImageNet [73] to extract image representations of 2048-d, which we then reduce to more compact representations such as 512-d and 64-d using the algorithm provided in NetVLAD for dimensionality reduction.'\\n\\n'Visual place recognition: VPR performance using our encoded visual observations are reported via area under the curve (AUC) metrics across a number of feature dimensions (Fig. 6). We train a classifier on each reference traversal using a single MLP that receives our encoded visual observations. We then use this trained classifier to evaluate the remaining query traversals. Once we have the scores for both query and reference, we compute the precision-recall curves from where we can obtain the overall AUC performance.'\\n\\n'We conducted comprehensive experiments applying VPR and RL techniques to examine the value of using visual and self-motion (in terms of the agent\\u2019s previous actions) sensory feedback to learn navigation policies on diverse robotic datasets. To enable efficient RL training, we use VPR models to encode real sensory data that, when combined with the goal destination, generates compact bimodal representations. Once trained, we showed that smaller visual representations such as 64-d generalized better than larger features over a range of environmental transitions, while being around 2 orders of magnitude faster and requiring a small fraction of the amount of experience in terms of training time.'\",\"187\":null,\"188\":null,\"189\":\"'After that, we encode the curve using the velocity field to make the assistance time-independent, which is inspired by a contour following application [25]. The curve parameter represents the desired target to reach on the parametric curve. Once a position is given, we can obtain a velocity from the field which aid the subject perform the correct motion pattern. In addition, with the help of the curve parameter, our designed method can even deal with a general multi-dimension task and the repeated and overlapped motions.'\",\"190\":null,\"191\":null,\"192\":null,\"193\":null,\"194\":\"'Prior work on motion prediction in robotics has made use of graphical models. For example, Kuli\\u0107 et al. encoded full-body motion primitives using Hidden Markov Models and applied the model to motion imitation [8]. Koppula and Saxena focused on movement prediction using conditional random fields [9]. While these approaches are sound they generally do not scale to large databases of motion capture.'\\n\\n'Recent work on human motion prediction for short-term motion has focused on recurrent neural network architectures (RNN). Fragkiadaki et al. proposed a RNN based model that incorporates nonlinear encoder and decoder networks before and after recurrent layers [1]. Their model is able to handle training across multiple subjects and activity domains. Jain et al. introduced a method to incorporate structural elements into a RNN architecture [12]. Autoencoders also can be used for denoising the prediction [2]. Martinez et al. introduced a gated recurrent unit (GRU) based approach with a residual connection in the loop function and showed that this outperforms prior RNN based methods [3]. Pavllo et al. further improved the RNN-based prediction by changing the joint angle representation to quaternions [4], [13]. However, this comes at the cost of additional normalization layers and normalization penalty. Recently Wang and Feng introduced a position-velocity recurrent encoder-decoder model (VRED) [5]. Their model adds an additional velocity connection as an input to the GRU cell in the recurrent structure.'\\n\\n'For prediction of the human we base our model on the position-velocity recurrent encoder-decoder neural network (VRED) [5]. We model the kinematic state of a human as a vector consisting of base position, base rotation and joint angles: s = (p,r,j) which in our case is a 66 dimensional vector for full-body data. The joints are toes, ankles, knees, hips, pelvis, torso, neck, head, inner shoulders, shoulders, elbows and wrists.'\",\"195\":null,\"196\":null,\"197\":null,\"198\":null,\"199\":\"'The clamping force can be predicted by the compression length of the spring of the IB Magnet after its frame contacts the ferromagnetic surface. Since the springs are fixed to the control rod, which is separated from the frame, the compression length is adjusted independently from the size of the target object and the distance between the fingers. The clamping force changes the thickness of the target object reduction of thickness during clamping. The gripper can still produce enough clamping force by shifting the control rod until the attractive force sufficiently exceeds the elastic force produced by the deformation. This is because the deformation width does not change after the frames of facing the IB Magnets have contacted each other. Since the attractive force is a known function of the displacement, a variable can be measured and controlled by the encoder of the linear actuator. A direct measurement of the attractive force is not required.'\",\"200\":null,\"201\":null,\"202\":null,\"203\":\"'Figure 5 shows the result of the 3D semantic mapping for the 10-minute trajectory. The reconstruction of coral species was denser and more precise owing to the better bounding box prediction of the improved detector. The color-codes for each coral classes used in the reconstruction are given in Table IV.'\\n\\n'TABLE IV Color codes used in semantic mapping for different corals.'\",\"204\":\"'Algorithm 1: DOB-Net - pseudocode for each thread'\",\"205\":null,\"206\":\"'https:\\/\\/umich.box.com\\/v\\/psfdataset-uwdomedcameras'\",\"207\":\"'In the PAM identification experiment, the PAM was first inflated to a certain initial pressure (with rope being slack). The initial pressure ranges from 200 kPa to 600 kPa. Then a force controller was applied to stretch the rope (desired force of 10 N). The motor encoder data in this condition were used to calculate the PAM rest length. Finally, the motor velocity was controlled with a randomized profile for 200 seconds.'\",\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":\"'https:\\/\\/youtu.be\\/uWWJ5aV6ScE'\",\"213\":null,\"214\":\"'Detection For a virtual scene, each instance has been assigned a unique color. In other words, given an RGB image, we can find the area of all instances based on the panoptic segmentation map. Then we can calculate the minimum horizontal bounding box (HBB) and minimum oriented bounding box (OBB) using methods provided by many third-party open-source image processing libraries.'\",\"215\":\"'Re-identification on the local descriptor usually requires key-point extraction and massive local geometry calculation. In comparison, matching on global descriptor is more efficient in place recognition. Rizzini introduces a novel descriptor named GLAROT that encodes the relative geometric position of key-point pairs into a histogram [17]. The experiment result achieves very satisfactory recall precision and recall rate. However, building key-point relation is still computationally expensive. Kim et al. propose scan context which projects laser scan into global descriptor. The matching of scan context only requires element-wise multiplication so that the query speed is fast. However, the matching precision is not high enough and false positive occurs during public datasets test.'\",\"216\":null,\"217\":\"'Firstly, we change the input size of this network from [3\\u00d7436\\u00d71024] to [1\\u00d7240\\u00d7320]. RGB images should be transformed into grey images before feeding them into the shrunken network. Secondly, we remove one coding block and two pooling operations from the encoder, and the output size of the last encoding layer is [15\\u00d730]. Finally, we remove one decoding block and reduce the correlation radius from 4 to 3, as the correlation operation of decoding block is computationally expensive. The size of the predicted flow is [112\\u00d7160]. Our encoding and decoding blocks are identical to the encoding and decoding blocks of PWC-Net. The shrunken network architecture is shown in the supplementary video.'\\n\\n'As BA-Net does not public codes, we construct a motion tracking version of it. The constructed BA-Net is trained on our training\\/validating dataset. Similar to FlowNorm DSO, we also use the predicted flow guide for the convergence of BA-Net. We use the remaining part of the SceneNN dataset to build a challenging image pair dataset. Then, we generate initial poses by adding rotation noise and translation noise to the ground truth poses of these image pairs. The final number of image pairs is 60126. We compare how many pairs are successfully aligned by BA-Net and its FlowNorm version. The results are 48261 and 37218 for FlowNorm version and the original version respectively, which proves our method can further expand the convergence range of the tracker based on minimizing the feature metric residual.'\",\"218\":\"'To remove the dependence on sensor-specific assumptions and heuristics, we resort to adaptive and more principled solutions. First, instead of using hard-coded rules, we propose an adaptive initialization scheme that analyzes the geometric relation among all cameras and selects the most suitable initialization method online. Second, instead of engineering heuristics, we choose to characterize the uncertainty of the current pose estimate with respect to the local map using the information from all cameras, and use it as an indicator of the need for a new keyframe. Third, instead of relying on the covisiblity graph, we organize all the landmarks in a voxel grid and sample the camera frustums via an efficient voxel hashing algorithm, which directly gives the landmarks within the FoVs of the cameras. These methods generalize well to arbitrary camera setups without compromising the performance for more standard configurations (e.g., stereo).'\\n\\n'Our initialization method has no hard-coded assumptions regarding the camera configuration. For any multi-camera setups with known intrinsic and extrinsic calibrations, it is able to select the proper initialization method accordingly, without the need to change the algorithm settings manually. Specifically, it utilizes an overlapping check between the camera frustums to identify all the possible stereo camera pairs. If there exists stereo pairs, the initial 3D points are created from the stereo matching of these stereo pairs. Otherwise, the 5-point algorithm is run on every camera as in a standard monocular setup, and the map is initialized whenever there exists a camera that triangulates the initial map successfully (i.e., enough parallax, and the camera is not undergoing strong rotation).'\",\"219\":null,\"220\":\"'codes'\\n\\n'There is a large body of work in deep learning-based SLAM systems. For example, CodeSLAM [15], SceneCode [16], and DeepFactors [17] represent scenes using compact codes that that can be decoded into 2.5D depth maps. DeepTAM [18] trains a tracking network and a mapping network, which learn to reconstruct a voxel representation from a pair of images. CNN-SLAM [19] extends LSD-SLAM [20], a popular monocular SLAM system, to use single-image depth predictions from a convnet.'\",\"221\":\"'A historical sequence of poses is encoded by a bi-directional LSTM layer [20],'\",\"222\":null,\"223\":null,\"224\":\"'For the consistency throughout the experiments, we set the dimension of hidden states of all of the encoder and decoder LSTMs to 32. We iteratively train the generators and discriminator with batch size 32, i.e., the number of target agent for each batch, for 500 epochs using the Adam [31] optimizer with learning rate of 0.001.'\",\"225\":null,\"226\":\"'Learning meaningful visual representations in an embedding space can facilitate generalization in downstream tasks such as action segmentation and imitation. In this paper, we learn a motion-centric representation of surgical video demonstrations by grouping them into action segments\\/subgoals\\/options in a semi-supervised manner. We present Motion2Vec, an algorithm that learns a deep embedding feature space from video observations by minimizing a metric learning loss in a Siamese network: images from the same action segment are pulled together while pushed away from randomly sampled images of other segments, while respecting the temporal ordering of the images. The embeddings are iteratively segmented with a recurrent neural network for a given parametrization of the embedding space after pre-training the Siamese network. We only use a small set of labeled video segments to semantically align the embedding space and assign pseudo-labels to the remaining unlabeled data by inference on the learned model parameters. We demonstrate the use of this representation to imitate surgical suturing kinematic motions from publicly available videos of the JIGSAWS dataset. Results give 85.5% segmentation accuracy on average suggesting performance improvement over several state-of-the-art baselines, while kinematic pose imitation gives 0.94 centimeter error in position per observation on the test set. Videos, code and data are available at: https:\\/\\/sites.google.com\\/view\\/motion2vec.'\\n\\n'Table I summarizes the performance comparison of Motion2Vec with different combinations of supervised and unsupervised approaches to metric and sequence learning. We use segmentation accuracy on the test set as the performance metric defined by the percentage of correct segment predictions in comparison to the ground-truth segments annotated by human experts. We observe that svTCN performs better among the unsupervised metric learning approaches without using any of the action segment labels. The unsupervised metric learning approaches perform well with RNN, but other sequence models including HMMs\\/HSMMs and CRFs find it difficult to encode the action segments. On the other hand, Motion2Vec representation with triplet loss performs well with both the supervised and the unsupervised sequence learning approaches by better grouping the action segments with the use of labeled demonstrations. Triplet loss with RNN gives better performance among all the compared approaches. Moreover, M2V-T, a variant of Motion2Vec with a combination of labeled triplet and svTCN loss, better aligns the images temporally with nearest neighbour imitation accuracy of 84.4% in the embedding space.'\",\"227\":\"'RRT expands a graph toward a random-sampled point. The graph extends rapidly due to random-sampling, but the position uncertainty increases easily because of rotations in a terrain with big motion uncertainty. To solve the problem, the proposed method expands a graph from nodes with small position uncertainty. The position uncertainty of a node is expressed by particles. The same number of particles is assigned to each node. The uncertainty propagation is described by moving particles with the expansion of nodes. The pseudo code of the proposed RRT with consideration of motion uncertainty, MU-RRT, is shown in Algorithm 1. The overview of Algorithm 1 is as follows:'\",\"228\":null,\"229\":null,\"230\":\"'In this part, the Gaussian Mixture Model is presented to encode the trajectories from teaching. Gaussian Mixture Model is a probability-based statistical model which can describe the probability density distribution of high-dimensional dataset by the sum of different weights of multiple Gaussian models [22]. In this paper, the GMM is used to describe the position density in Cartesian space and obtain nonlinear item in in DMP by regression from each GMM. The DMP framework of multi-demonstrations is reformulated as by K component Gaussian model,'\\n\\n'For multi-demonstrations from teaching, the GMM \\u0398 encodes the set of trajectories from robot in Cartesian space. The k component of Gaussian mixture model is defined as,'\\n\\n'Then, dynamic movement primitive is applied to encode the multi-demonstrations from human teaching. The regression results are shown in Figs. 3\\u20135. Fig. 3 shows that all the reproductions can pass through the original RCM and converge to the goal point even from random initial position.'\",\"231\":\"'Note that for the case studies, due to the size of the environment state space, we could not obtain a ground truth for the planning problem, and thus had to use the approximated approaches. The algorithms were coded in Python 3.6 on a computer with 2.3 GHz i5 quadcore processor and 16 GB memory.'\",\"232\":null,\"233\":null,\"234\":null,\"235\":\"'The main focus of this work was to implement a control architecture to enable human control of a multi-agent system. The results of the experiment demonstrate that, for the case of multiple quadcopters and a cable-suspended payload, it can be achieved by controlling the cables\\u2019 and payload\\u2019s attitude. Notably, the use of CAC and PAC will be tested further with multi-follower quadcopters in future work. Further, a state feedback controller is used in this study but there exists the possibility of other control designs which could enable better performance. The incorporation of human in the control loop can facilitate the navigation of quadcopters-payload system through unknown environments with potential applications in agriculture, warehouses, and tasks requiring impromptu payload transport. Therefore, the future work will focus on achieving the cables\\u2019 and payload\\u2019s attitude computation using portable onboard sensors, such as potentiometers, encoders, IMUs, etc., to eliminate the need of an indoor motion capture system and the ground station.'\",\"236\":\"'https:\\/\\/github.com\\/I2RDL2\\/ASTAR'\\n\\n'https:\\/\\/github.com\\/I2RDL2\\/ASTAR'\",\"237\":\"\\\"3D vehicle detection based on point cloud is a challenging task in real-world applications such as autonomous driving. Despite significant progress has been made, we observe two aspects to be further improved. First, the semantic context information in LiDAR is seldom explored in previous works, which may help identify ambiguous vehicles. Second, the distribution of point cloud on vehicles varies continuously with increasing depths, which may not be well modeled by a single model. In this work, we propose a unified model SegVoxelNet to address the above two problems. A semantic context encoder is proposed to leverage the free-of-charge semantic segmentation masks in the bird's eye view. Suspicious regions could be highlighted while noisy regions are suppressed by this module. To better deal with vehicles at different depths, a novel depth-aware head is designed to explicitly model the distribution differences and each part of the depth-aware head is made to focus on its own target detection range. Extensive experiments on the KITTI dataset show that the proposed method outperforms the state-of-the-art alternatives in both accuracy and efficiency with point cloud as input only.\\\"\\n\\n\\\"Methods exploiting point cloud only as input have also been explored in many works. Similar to 2D detection, these methods can be mainly divided into two categories: the one-stage method and the two-stage method. Generally, one-stage methods are faster than two-stage ones. Usually, these methods encode the point cloud as a bird's eye view (BEV) [7], [9] representation or to the irregular pillars [10], and directly predicts the 3d bounding boxes and their scores. [11], [5] firstly group point cloud into voxels and then extracts features using 3D convolution from voxels which is used in region proposal network (RPN) [12]. The two- stage method usually generates bounding box proposals in the first stage and refine them in the second stage [13]. Recently, [14] achieves impressive 3D detection results on the KITTI [15] benchmark by introducing [16] as a two- stage encoder for canonical 3D bounding box refinement. However, these methods often need a separate model for each stage which is time-consuming.\\\"\\n\\n'A. Voxel Feature Encoder'\\n\\n'The Voxel Feature Encoder (VFE) is applied to the raw point cloud to obtain voxelized feature representation for the following SCE module. It consists of two steps, i.e., point cloud voxelization and voxel feature extraction.'\\n\\n'An overview of the proposed network SegVoxelNet, which consists of a Voxel Feature Encoder (VFE), a Semantic Context Encoder (SCE) and a depth-aware head. VFE encodes the raw point cloud into voxels and further converts them into bird\\u2019s eye view feature maps. SCE predicts the semantic segmentation masks and encodes the semantic context information in the feature maps for better detection. And the depth-aware head is designed to have multiple parts to predict 3D bounding boxes for vehicles in different depth level.'\\n\\n'B. Semantic Context Encoder'\\n\\n'The Semantic Context Encoder (SCE) in Fig. 2 takes the BEV feature maps from the Voxel Feature Encoder as input, and output the semantic context encoded feature maps for detection. The proposed SCE consists of two branches sharing the input VFE feature maps. The first branch is modified from SECOND [5]. It consists of a U-Net structure with a downsampling and an upsampling layer for obtaining larger receptive field. The output of the U-Net is the same size as the input, so the it can be concatenated to the input VFE feature map to generate the main feature map. The second branch learns to predict the BEV semantic masks, which is then used to enhance the feature maps from the first branch by a fusion module.'\\n\\n'Then, we fuse the two branches to obtain the semantic context encoded feature maps for the detection head. The fusion can be done by re-weighting the feature maps by the probability map as attention residual learning in [28], Formally, given the probability map M and the feature map of the first branch F, the re-weighted feature R can be calculated by'\\n\\n'Visualization of the influence of integrating semantic segmentation information into detection feature maps. Top is the 2D image, bottom consists of a) bird\\u2019s eye view LiDAR point clouds, b) origin detection feature map, c) semantic segmentation feature map and d) semantic context encoded feature map. Green bounding boxes in the image and BEV denote the ground truth 3D vehicles. The red bounding boxes on the semantic segmentation feature map enhances the car-existing region on the detection feature map, while the yellow ones gives a suppressive effect on it.'\\n\\n'Effect of different fusion between semantic segmentation feature maps and detection feature maps: In Semantic Context Encoder(SCE), Semantic context information is introduced to highlight the existing vehicle region and suppress the background. We further evaluate the quantitative effects of the proposed fusion methods. We compare the proposed attention residual learning with simple concatenation along the channel axis. Tabel. III shows the 3D vehicle detection performance with re-weight and concatenation. It can be seen that these both methods are better than the baseline method without the semantic branch, which proves the effectiveness of the semantic context information for detection. While the performance gap between these two variants is very small, the proposed re-weight method results in fewer parameters and is more efficient than the concatenation alternative.'\",\"238\":null,\"239\":null,\"240\":\"'Driveable area detection is a key component for various applications in the field of autonomous driving (AD), such as ground-plane detection, obstacle detection and maneuver planning. Additionally, bulky and over-parameterized networks can be easily forgone and replaced with smaller networks for faster inference on embedded systems. The driveable area detection, posed as a two class segmentation task, can be efficiently modeled with slim binary networks. This paper proposes a novel binarized driveable area detection network (binary DAD-Net), which uses only binary weights and activations in the encoder, the bottleneck, and the decoder part. The latent space of the bottleneck is efficiently increased (\\u00d732\\u2192\\u00d716 downsampling) through binary dilated convolutions, learning more complex features. Along with automatically generated training data, the binary DAD-Net outperforms state-of-the-art semantic segmentation networks on public datasets. In comparison to a full-precision model, our approach has a \\u00d714.3 reduced compute complexity on an FPGA and it requires only 0.9MB memory resources. Therefore, commodity SIMD-based AD-hardware is capable of accelerating the binary DAD-Net.'\\n\\n'Efficiently optimized various blocks in the model\\u2019s encoder, bottleneck and decoder, combining structural and local binary approximation schemes. A detailed ablation study is provided investigating different variants in binary DAD-Net.'\\n\\n'The proposed driveable area detector is inspired by autoencoder-based networks with skip connections, i.e. DeepLabV3 [9]. As the name implies, binary DAD-Net has binary representations in all three parts of the model: the encoder, bottleneck (latent space) and decoder. The modules are detailed in Sec. III-A-III-C. Binary DAD-Net adopts the binarization scheme of Rastegari et al. [12] as discussed in Sec. III-D. The structure of binary DAD-Net is given in Fig. 2.'\\n\\n'A. Binary Encoder for Feature Extraction'\\n\\n'Overview of binary DAD-Net. The binarized network consists of three parts, namely an encoder, a bottleneck and a decoder. Binary dilated convolution in the bottleneck ensures an extended feature resolution. The feature response is efficiently increased from 32 to 16 neurons through binary dilated convolutions.'\\n\\n'The bottleneck layers in the segmentation architecture retain the lowest spatial dimensions obtained from the encoder, likewise to an auto-encoder. Similar to the binary convolutional layer, weights and activations are binarized for the bottleneck, see central building block of Fig. 2.'\\n\\n'C. Binary Decoder for Semantic Predictions'\\n\\n'Third, to best of our knowledge binary DAD-Net is the first work also binarizing the decoder for driveable area detection, see right building block of Fig. 2. Employing only binary convolutions enlarges the output of the bottleneck to the size of the original input image I generating pixel-wise predictions for the task of driveable area detection. The binary decoder also consists of bilinear upsampling and a binary score layer. In detail, after the binary dilated convolution, described in the previous section, linear combination (binary 1 \\u00d7 1 convolution) of the ASPP feature maps and the encoder skip connection (after the first residual block) is computed. Next, the feature maps are fused in two consecutive binary refinement blocks. The binary refinement blocks consist of 3\\u00d73 kernels, which is similar to the binary convolutional layer, described above. Instead of transpose convolutions, bilinear up-sampling enlarges the feature maps to the size of the input I. This is important as the binary transpose convolution would introduce additional operations and would lead to an accuracy degradation, see Sec. IV-B.'\\n\\n'The requirement of the following analysis is to determine appropriate modules for binary DAD-Net (encoder and decoder), a local binary approximation (XNOR|Compact|BNN|ABC), structural approximation schemes of the bottleneck (BPAC|Dilation|ASPP) and good initialization scheme(ImageNet|Automatic annotations). Analysis is performed on CityScapes dataset.'\\n\\n'Encoder\\/Decoder Selection:'\\n\\n'TABLE I Selection of a SotA encoder\\/decoder configuration.'\\n\\n'TABLE II Local binary approximation of the encoder and decoder.'\\n\\n'In this section, the structure of the proposed binary DAD-Net is analyzed and evaluated against SotA models for semantic segmentation on both public datasets. Also a variety of encoder models with different binarization methodologies proposed in [12], [13] and [14] are employed.'\\n\\n'This paper introduces a novel binary driveable area detector (binary DAD-Net) required in the field of autonomous driving. Binary DAD-Net is fully binarized, including the encoder, the bottleneck and the decoder. An elaborate study is performed to explore various components of Binary DAD-Net, namely the model structure, the binarization scheme and the ground-truth annotations for training. Along with automatically generated training data, binary DAD-Net achieves state-of-the-art semantic segmentation results 96.60%(-0.7%) on the CityScapes dataset. The proposed driveable area detector is very memory efficient, with only 0.9MB parameters (-15.9\\u00d7). Moreover, Binary DAD-Net shows its superior performance w.r.t. an embedded implementation, by drastically reducing the computational complexity (14.3\\u00d7) compared to previous work.'\",\"241\":\"'A few state of the art SLAM algorithms were evaluated with our dataset. For the tested open-source LIDAR and camera based methods, the results were less satisfactory in the our highly urbanized scenarios. A quantitative evaluation is given in Table 3, and a collection of 2D trajectory plots are shown in Figure 4. The dynamic objects in the moving scene largely contributed to the failures of SLAM algorithms, as most of these algorithms use static feature points to estimate the pose of the vehicle. Map CA_20190928173350, collected during the rush hour on the one of the busiest street in San Francisco, is the most distorted map produced with current SLAM algorithms.'\\n\\n'As for visual odometer estimation, we used the open-source VINS-MONO [25] algorithm developed by Qin et al. VINS-MONO tightly couples visual odometry with IMU estimation to output an optimized localization result. As pointed out in [25], the algorithms out-performed most existing visual odometry methods. During the experiment, we noticed that the visual odometry was very sensitive to changes in light conditions: the algorithm failed when entering-exiting a tunnel. After using half-real time play-back rate, we generated the continuous trajectory successfully. For performance, while the algorithm slightly out-performed the LIDAR based method in translation, the error in roll\\/pitch\\/yaw angle estimation was worse. The performance further deteriorated when the path is filled with sharp maneuvers (Figure 4(d)). Since VINS-MONO takes time for initialization, a rigid body transformation [28] was applied to the constructed map before evaluation.'\",\"242\":\"'We show the detailed algorithm in the pseudo-code below, where is pixel-wise multiplication and N\\u22121 is the pixelwise inverse of matrix N.'\",\"243\":null,\"244\":\"'The encoder network is composed of five blocks (in blue); each consists of a (stride 2) 2D convolution layer, a ReLU activation, and a batch normalization layer. We avoid the use of pooling layers, as they lead to spatial invariance, which would be detrimental for visual localization tasks. At the bottleneck, we branch the network into depth, explainability mask, and pose correction subnetworks. For the depth and explainability mask, we upsample from the bottleneck using decoder blocks which consist of a 2D transposed convolution layer [24] followed by a ReLU activation. Our depth prediction layer is a 2D (stride 1) convolution that reduces the channel layer to one, and a ReLU activation which ensures the output is positive.4 The final explainability mask layer is a sigmoid activation, which compresses the pixel values to lie within (0, 1).'\\n\\n'the release of an open source implementation of our method in PyTorch [9].1'\",\"245\":null,\"246\":\"'Trajectory of a real-world experiment with a differential wheel robot, Duckiebot. The robot only has IMU and UWB sensors. A UWB anchor is placed on (0,0) in the right bottom. There is no encoder or other velocity sensor used in the robot. The robot is controlled manually to moving a \\\"TUM\\\" like trajectory in a basketball pitch in TUM Garching campus.'\",\"247\":null,\"248\":null,\"249\":\"'https:\\/\\/github.com\\/vision4robotics\\/BiCF-Tracker'\\n\\n'https:\\/\\/youtu.be\\/fS12kosv37s'\\n\\n'A novel tracker is proposed, which can learn the object\\/background appearance changes more Efficiently. By using inter-frame information to analyze and re-solve the response-based bidirectional incongruity error, it helps to encode the filter with high accuracy and robustness. Moreover, multiple features (HOG and CN) are used to assist in the object\\/background expression.'\\n\\n'https:\\/\\/github.com\\/vision4robotics\\/BiCF-Tracker'\",\"250\":null,\"251\":\"'https:\\/\\/github.com\\/GPaolo\\/taxons'\\n\\n'Performing Reinforcement Learning in sparse rewards settings, with very little prior knowledge, is a challenging problem since there is no signal to properly guide the learning process. In such situations, a good search strategy is fundamental. At the same time, not having to adapt the algorithm to every single problem is very desirable. Here we introduce TAXONS, a Task Agnostic eXploration of Outcome spaces through Novelty and Surprise algorithm. Based on a population-based divergent-search approach, it learns a set of diverse policies directly from high-dimensional observations, without any task-specific information. TAXONS builds a repertoire of policies while training an autoencoder on the high-dimensional observation of the final state of the system to build a low-dimensional outcome space. The learned outcome space, combined with the reconstruction error, is used to drive the search for new policies. Results show that TAXONS can find a diverse set of controllers, covering a good part of the ground-truth outcome space, while having no information about such space.'\\n\\n'High level schematic of Task Agnostic eXploration of Outcome space through Novelty and Surprise (TAXONS). It consists of two processes operating in parallel. The first one, the search process (gray arrows), generates a set of new policies, evaluates them in the environment and then stores in the repertoire the best ones according to the selection criteria. The second process (red arrow), is the training of the autoencoder (AE) on the observations collected during the search and evaluation of the policies.'\\n\\n'In the rest of the paper we will formally describe TAXONS, introduce three experiments on which it will be evaluated, compare it to other baseline methods and discuss the results in detail. The related code is available at: https:\\/\\/github.com\\/GPaolo\\/taxons.'\\n\\n'B. AutoEncoded Novelty and Surprise'\\n\\n'NT: a novelty search algorithm in which the outcome descriptor is given by the features extracted by a random autoencoder from oT.'\\n\\n'https:\\/\\/github.com\\/GPaolo\\/taxons'\",\"252\":\"'http:\\/\\/tiny.cc\\/cost-model-learning'\\n\\n'http:\\/\\/tiny.cc\\/cost-model-learning'\\n\\n'http:\\/\\/tiny.cc\\/cost-model-learning'\",\"253\":null,\"254\":null,\"255\":\"'Finally, the pseudo-code of one step of the inverse simulation is shown in the algorithm 1. With regards to [16], this formulation requires to compute 6 additional inverse simulation steps (12 steps) to construct the centered Jacobian.'\",\"256\":null,\"257\":\"'Algorithm 1 A3C - pseudocode for each actor-learner thread.'\",\"258\":null,\"259\":null,\"260\":\"'https:\\/\\/youtu.be\\/MEXK8XVso2M'\",\"261\":null,\"262\":\"'The world-frame pointcloud is then used to update a 2.5 dimension heightmap where the half-dimension refers to the fact that the third dimension is encoded as the value in an matrix of the other two dimensions. More specifically, the heightmap is a representation of the world in which the X and Y dimensions are discretized into 1.5 cm\\u00d71.5 cm cells. For each pointcloud received, the value of a given cell in the heightmap, representing the height at that location, is calculated by taking the Z component of the most recent pointcloud point in the corresponding X-Y cell range.'\",\"263\":\"'Gazebo simulation setup: 10 aerial robots and 50 ground mobile targets: (a) Gazebo environment; and (b) Rviz environment. Each robot is color-coded, along with its coverage region. All robots in the same clique have the same color. The targets are depicted as white cylindrical markers.'\",\"264\":null,\"265\":null,\"266\":null,\"267\":null,\"268\":null,\"269\":\"'We do not determine all start points at the beginning but use an iterative approach that allows to continually supervise the improvement of the policy, select new start points based on progress achieved until then, and add only relevant data. The optimal control policy is approximated by a Gaussian process (GP), retrained in each iteration with data points on the optimal trajectories. The main steps of the algorithm are outlined as pseudo-code in Algorithm 1. They are motivated and described in more detail in the following subsections.'\",\"270\":\"'We introduce Crocoddyl (Contact RObot COntrol by Differential DYnamic Library), an open-source framework tailored for efficient multi-contact optimal control. Crocoddyl e...'\\n\\n'We introduce Crocoddyl (Contact RObot COntrol by Differential DYnamic Library), an open-source framework tailored for efficient multi-contact optimal control. Crocoddyl efficiently computes the state trajectory and the control policy for a given predefined sequence of contacts. Its efficiency is due to the use of sparse analytical derivatives, exploitation of the problem structure, and data sharing. It employs differential geometry to properly describe the state of any geometrical system, e.g. floating-base systems. Additionally, we propose a novel optimal control algorithm called Feasibility-driven Differential Dynamic Programming (FDDP). Our method does not add extra decision variables which often increases the computation time per iteration due to factorization. FDDP shows a greater globalization strategy compared to classical Differential Dynamic Programming (DDP) algorithms. Concretely, we propose two modifications to the classical DDP algorithm. First, the backward pass accepts infeasible state-control trajectories. Second, the rollout keeps the gaps open during the early \\\"exploratory\\\" iterations (as expected in multipleshooting methods with only equality constraints). We showcase the performance of our framework using different tasks. With our method, we can compute highly-dynamic maneuvers (e.g. jumping, front-flip) within few milliseconds.'\",\"271\":\"'https:\\/\\/vsislab.github.io\\/gsdrl\\/'\\n\\n'https:\\/\\/vsislab.github.io\\/gsdrl\\/'\",\"272\":\"'Semantic grasping is the problem of selecting stable grasps that are functionally suitable for specific object manipulation tasks. In order for robots to effectively perform object manipulation, a broad sense of contexts, including object and task constraints, needs to be accounted for. We introduce the Context-Aware Grasping Engine, which combines a novel semantic representation of grasp contexts with a neural network structure based on the Wide & Deep model, capable of capturing complex reasoning patterns. We quantitatively validate our approach against three prior methods on a novel dataset consisting of 14,000 semantic grasps for 44 objects, 7 tasks, and 6 different object states. Our approach outperformed all baselines by statistically significant margins, producing new insights into the importance of balancing memorization and generalization of contexts for semantic grasping. We further demonstrate the effectiveness of our approach on robot experiments in which the presented model successfully achieved 31 of 32 suitable grasps. The code and data are available at: https:\\/\\/github.com\\/wliu88\\/railsemanticgrasping.'\\n\\n'Other works have used more abstract semantic representations, but leveraged rule-based inference mechanisms that must be encoded manually. Antanas et al. [13] encoded suitable grasp regions based on probabilistic logic descriptions of tasks and segmented object parts. Works on object affordance detection [19], [20], [14], [21], [22] have leveraged the affordances of object parts to define the correspondences between affordances and grasp types (e.g., rim grasp for parts with contain or scoop affordance.) Detry et al. [23] built on these works by training a separate detection model using synthetic data generated by predefined heuristics to detect suitable grasp regions for each task. Kokic et al. [24] assigned preferences to affordances in different tasks, which were then used to determine grasp regions to use or avoid. Instead of manually specifying semantic grasps based on semantic representations, we learn the relations between these semantic features and suitable grasps from data.'\\n\\n'In this section, we discuss in detail how to create the object embedding that encodes semantic information of object parts. A naive approach is to simply concatenate embeddings of all parts. However, this approach can lead to features that are difficult to generalize because objects consist of different numbers of parts and a canonical order of parts for concatenation is undetermined.'\",\"273\":null,\"274\":null,\"275\":\"'Three-dimensional reconstruction of dynamic objects is important for robotic applications, for example, the robotic recognition and manipulation. In this paper, we present a novel 3D surface reconstruction method for moving objects. The proposed method combines the spatial-multiplexing and time-multiplexing structured-light techniques that have advantages of less image acquisition time and accurate 3D reconstruction, respectively. A set of spatial-temporal encoded patterns are designed, where a spatial-encoded texture map is embedded into the temporal-encoded three-step phase-shifting fringes. The specifically designed spatial-coded texture assigns high-uniqueness codeword to any window on the image which helps to eliminate the phase ambiguity. In addition, the texture is robust to noise and image blur. Combining this texture with high-frequency phase-shifting fringes, high reconstruction accuracy would be ensured. This method only requires 3 patterns to uniquely encode a surface, which facilitates the fast image acquisition for each reconstruction step. A filtering stereo matching algorithm is proposed for the spatial-temporal multiplexing method to improve the matching reliability. Moreover, the reconstruction precision is further enhanced by a correspondence refinement algorithm. Experiments validate the performance of the proposed method including the high accuracy, the robustness to noise and the ability to reconstruct moving objects.'\\n\\n'To provide a solution for the accurate 3D surface reconstruction of moving objects, in this paper, we develop a novel spatial-temporal multiplexing method. The proposed method combines the techniques of spatial multiplexing and temporal multiplexing methods by embedding a spatial-coded texture map into the high-frequency phase-shifting fringes. The methodology and experiments are presented as follows.'\\n\\n'Fig. 2 shows the system configuration of the proposed 3D surface reconstruction method, which consists of a projector for pattern projection, stereo cameras for triangulation, and a computing unit for image and data processing. For each reconstruction step, the designed spatial-temporal patterns are sequentially projected onto the surface of the target object, and the distorted patterns are then synchronously captured by the stereo cameras from different viewpoints. Through decoding, each pixel on the images can obtain two codewords: a temporal codeword and a spatial codeword, which are reliable clues for stereo matching, because the correspondence between image pairs are double checked in the filtering matching procedure. The matching results are finally refined by the refinement algorithm to further enhance the reconstruction precision.'\\n\\n'The spatial codes and temporal codes should be retractable from the spatial-temporal encoded patterns, because they are separately used in the procedure of stereo matching;'\\n\\n'A. Spatial-temporal Encoded Patterns'\\n\\n'Next, we will prove that the spatial codes and temporal codes could be separately retracted from the spatial-temporal encoded patterns. As demonstrated in Fig. 3, after projecting the above patterns onto the object surface, images acquired by the stereo cameras can be written as'\\n\\n'Procedures of generating the spatial-temporal encoded patterns and retracting spatial codeword and temporal codeword from the acquired images.'\\n\\n'Since an arctangent function is utilized in the procedure of deriving temporal codeword, the obtained phase map has the problem of 2\\u03c0 ambiguity. The ambiguity can be removed by the spatial codeword so that unique codeword is assigned to each pixel. The requirements for the spatial codeword are listed as follows.'\\n\\n'The spatial codeword should be able to encode every period of phase-shifting fringes uniquely.'\\n\\n'Since noise and image blur are very common in the structured-light systems, the spatial codeword should be robust to them so that the decoding reliability could be enhanced.'\\n\\n'Considering the two requirements, a novel spatial-encoded pattern is designed as follows.'\\n\\n'B. Design of The Spatial-Encoded Pattern'\\n\\n'The function of the spatial-encoded pattern is to distinguish different phase periods. To improve the contrast between the spatial encoded texture and the phase-shifting fringes, the designed pattern is a binary image. Prior to design of the spatial-encoded pattern, we first clarify the key parameters of the pattern:'\\n\\n'Illustration of the procedure of designing the spatial-encoded pattern. (a) Generation of m\\u00d71 vectors. Distance between any two vectors is more than d0. (b) Different combinations of the vectors generated in (a). Size of the combined cell is m \\u00d7 T. (c) The obtained m \\u00d7 M block. Any two m \\u00d7 m windows in the block are different. (d) The obtained pattern by repeating the m \\u00d7 M block horizontally and vertically.'\\n\\n'The objective of designing the spatial-encoded pattern is to maximize the minimal difference between any two m \\u00d7 m windows in an m \\u00d7 M block, as illustrated in Fig. 4 (c). Mathematically, the minimal difference is described as:'\\n\\n'Illustration of the filtering matching process. First, correspondence candidates are found whose temporal codeword is similar to (ul,vl). Second, the spatial codeword of the candidates are compared, and the pixel whose spatial codeword is most similar to that of (ul,vl) is the final correspondence.'\\n\\n'In the last subsection, a spatial codeword and a temporal codeword is obtained, as shown in Fig. 3 (b). Then, in this part, the two codewords are utilized to establish correspondence between the images acquired from different cameras, as shown in Fig. 6. The matching process consists of two steps. First, correspondence is roughly built using the temporal codeword. Due to the 2\\u03c0 ambiguity, the correlation is not unique, i.e. a pixel (ul,vl) on the left image may find several corresponding pixels in the epipolar line of the right image which are called \\\"correspondence candidates\\\". In the second step, precise correspondence is established within the candidates using the spatial codeword. As illustrated in Fig. 6, for the pixel on the left image, although all the correspondence candidates have the same temporal codeword, their surrounding windows on the spatial codeword map are very different. The pixel whose surrounding window is most similar to the window of the left pixel is the estimated corresponding point.'\\n\\n'Since the matching algorithm filters the correspondence, the procedure is called \\\"filtering matching\\\". The advantages of the proposed filtering matching are from two aspects: (1) the correspondence is double checked by the two codeword, which largely enhances the matching reliability; (2) the filtering matching method avoid the procedure of phase unwrapping.'\\n\\n'In this paper, we have presented a 3D surface reconstruction method for moving objects. Combination of the spatial multiplexing technique and the temporal multiplexing technique is realized by designing a set of special patterns that are encoded spatially and temporally. The patterns are generated by embedding a texture map into the high-frequency phase-shifting fringes. Only three patterns are required for accurate 3D reconstruction, which makes it suitable for reconstructing moving objects. The spatial-encoded texture has robustness to noise and image blur. Then, a filtering stereo matching method was proposed that has the advantage of high matching accuracy. Moreover, we proposed a correspondence refinement method to further enhance the matching precision. Various experiments were carried out and the results validated the effectiveness and performance of the proposed method. Especially, comparative experiments demonstrated the advantages of the proposed method for reconstructing moving objects.'\",\"276\":null,\"277\":\"'http:\\/\\/deep-temporal-seg.informatik.uni-freiburg.de\\/'\\n\\n'Our proposed semantic segmentation framework. In the first step we project a LiDAR scan onto five 2D images and encode the following modalities: depth, surface reflectance intensity, 3D coordinates(x, y, z). These images are then stacked together, fed into the proposed CNN architecture, and the output is the predicted segmentation mask (bottom left). The segmentation information is then projected back to the scan to infer the semantic labels for each point in the scan. In the encoder we have two convolution layers (conv_0 and conv_1), two max-pooling layers and four dense blocks (db_0, db_1, db_2 and db_3). In the decoder we use two up-convolution layers, two dense blocks (db_4, db_5) with depth separable convolution and one convolution layer (conv_2).'\\n\\n'Using depth separable convolution layers instead of convolution layers for dense blocks in the decoder. This helps in reducing the parameters from 3.6M to 2.8M.'\\n\\n'We trained two models, where we use depth separable convolution only in the last dense block of the encoder (db_3) and then in last two dense blocks together (db_3 + db_2). In both cases performance decreases, especially for the second case the decrease is substantial. Even though depth separable convolution is an ingenious way of reducing parameters but excessively using it can decrease performance as well.'\",\"278\":null,\"279\":\"'Here we consider the topology of the sensor, and sum the spike counts separately for each of the sensor\\u2019s 49 taxels. The resulting array of 49 spike rates is used as an encoded representation of the texture'\\n\\n'The spatiotemporal encoded representation can be considered the convolution of the multi-taxel spike train with an exponential kernel used in the Van Rossum distance calculation (see Section III-D).'\\n\\n'cos\\u03b8 = 0. Labelled line code: each neuron is considered independent and their distances are summed.'\\n\\n'cos\\u03b8 = 1. Summed population code: spike trains are superimposed before calculating the distance metric.'\\n\\n'The neuroTac sensor developed here is a neuromorphic optical tactile sensor, with a data output consisting of multi-taxel spike trains. The spike trains were encoded using four different bio-inspired encoding mechanisms: Intensive, Spatial, Temporal and Spatiotemporal. We validated the sensor through a texture classification task, in which 11 3d-printed textures (grid sizes 0-5 mm in steps of 0.5 mm) and 20 natural textures were discriminated using a KNN classification algorithm.'\\n\\n'It has also been suggested in a recent study that the specific timing of spikes carries significance for texture discrimination, as it could encode spatial frequency features of the textures being contacted [28]. The spatiotemporal coding method described here could capture these precise spike timings, with a resolution dependent on the time constant parameter \\u03c4. Here \\u03c4 was optimized to be 76 ms, which could indicate an approximate timescale for the frequency features of the textures considered.'\\n\\n'We presented a neuromorphic optical tactile sensor and demonstrated its performance on a texture classification task. Four bio-inspired spike encoding mechanisms were investigated, which suggested information about texture coarseness is encoded in the timing of spikes. The neuroTac\\u2019s fast spike-based output could lead to a step forward in the areas of robotic manipulation and prosthetics.'\",\"280\":null,\"281\":null,\"282\":null,\"283\":\"'https:\\/\\/youtu.be\\/nZaR-8Z515s'\",\"284\":null,\"285\":null,\"286\":\"'Moving to an egocentric representation permits the incorporation of concepts from the family of gap-based local planning approaches. Gap-based local planners generally operate on raw laser-scan data, making them perception space approaches for 2D data. Range information implicitly encodes the navigable free space immediately around the robot, which is analyzed for openings or gaps. In general, local navigation is performed by selecting a suitable gap (based on various criteria) and generating commands to steer through it [20], [21], [22]. Rather than using gaps to directly generate navigation commands, we propose to use them as a heuristic to inform more efficient sampling of candidate trajectories residing in distinct topologies.'\\n\\n'This paper described a modification to the Timed-Elastic-Band (TEB) navigation system whose primary characterization is that of modifying the internal, local planning representation from being world-centric to being egocentric. Exploring the numerical and computational issues associated to TEB and their negative consequences provides guidance on how to incorporate the egocentric representation with the explicit goal of remedying these issues. The final egoTEB implementation nicely unifies alternative local navigation strategies and modern sensor-based navigation methods for unknown environments. The net result is a more efficient implementation with equal or improved collision avoidance properties that also shows significant potential for low computational power devices. This implementation is open source software [27]. As future work, it would be interesting to explore how egoTEB might improve TEB for the case of moving objects.'\",\"287\":\"'Table I shows the quantitative results from evaluating task success on the real robot. These experiments show that DANN improves on top of the domain randomization baseline by a small margin. However, end-to-end adaptation with the TCN loss results in degradation of performance. This is likely due to insufficient sharing of the encoder between the self-supervised objective using simulated data and real data. On the other hand, the two-stage self-supervised domain adaptation with TCN significantly improves over the end-to-end variant and domain randomization baselines. This reconfirms that modality tuning used in the two-stage training method results in significantly better sharing of the encoder. Finally, the two-stage self-supervised adaptation with our CFD objective, which utilizes both the temporal structure of the observations and the actions, performs significantly better when compared to all other methods, yielding a 62 % task success.'\",\"288\":\"'We obtain the generative model by training a variational autoencoder (VAE) on a set of trajectories which are suitable for the given task, in the sense that they accomplish a particular version of the task under particular conditions, and are safe to be executed on the physical robot. The trajectories used for our experiments are described in Section III-B. A VAE consists of two parts\\u2014the encoder and the decoder. The encoder outputs a probability distribution representing the low-dimensional latent representation of the input. During training, a sample is drawn from this distribution and passed to the decoder, which reconstructs the original input based on the low-dimensional representation. The decoder part of the VAE, on its own, can be used to map vectors in the latent space to the output domain, which in our case represents useful trajectories.'\",\"289\":\"'General-purpose simulators can be a valuable data source for flexible learning and control approaches. However, training models or control policies in simulation and then directly applying to hardware can yield brittle control. Instead, we propose a novel way to use simulators as regularizers. Our approach regularizes a decoder of a variational autoencoder to a black-box simulation, with the latent space bound to a subset of simulator parameters. This enables successful encoder training from a small number of real-world trajectories (10 in our experiments), yielding a latent space with simulation parameter distribution that matches the real-world setting. We use a learnable mixture for the latent prior\\/posterior, which implies a highly flexible class of densities for the posterior fit. Our approach is scalable and does not require restrictive distributional assumptions. We demonstrate ability to recover matching parameter distributions on a range of benchmarks, challenging custom simulation environments and several real-world scenarios. Our experiments using ABB YuMi robot hardware show ability to help reinforcement learning approaches overcome cases of severe sim-to-real mismatch.'\\n\\n'We propose to view simulators as regularizers towards the \\u2018latent space\\u2019 of simulator parameters. Instead of constructing low-dimensional representations from scratch, we show how variational autoencoders (VAEs) can benefit from simulation. Our approach starts by pre-training VAE\\u2019s decoder to match the output of a black-box simulator, given the current state, control action and key simulator parameters as input. This pre-training is done with simulation data. Following that, we provide a small number of hardware observations to train the VAE. Then, we iterate these two training stages. The resulting approach is our contribution: DET2STOC algorithm. It enables automatic regularization to the space of hypotheses consistent with simulator dynamics, but avoids overfitting to a specific simulation setting. Our approach leverages a small amount of hardware data to convert a given deterministic simulator to an approximate stochastic model, hence the name: DET2STOC. It provides a data-efficient solution to identify the parameter posterior for the simulator to match reality, and to learn a generative model whose output matches the real-world dynamics.'\\n\\n'A. Variational Autoencoders'\\n\\n'Autoencoders learn to reconstruct a given input x, passing it through a bottleneck that restricts representational capacity. This facilitates learning a lower-dimensional embedding. VAE [3] is a probabilistic generative formulation of this concept. It assumes that input is generated by a random process involving a latent variable z (unobserved). z is generated from a prior distribution p(z), while x is generated from a conditional distribution p(x|z), called likelihood. It is assumed that integrating marginal likelihood p(x) = p(z)p(x|z)dz and finding the exact posterior p(z|x) = p(x|z)(z)\\/p(x) is intractable. To approximate p(z|x), variational inference maximizes evidence lower bound (ELBO) to learn parameters \\u03d5 of an approximate posterior q\\u03d5(z|x):'\\n\\n'encoder'\\n\\n'decoder.'\\n\\n'Conditional Variational Autoencoder (CVAE) can condition on auxiliary input. CVAE [2] defines an encoder q\\u03d5(z|y, x), a prior p\\u03b8(z|x) and a decoder p\\u03b8(y|z, x). ELBO for CVAE with the output variable y is reformulated as:'\\n\\n'Our approach builds on CVAE architecture, since we are interested in aligning the decoder with a forward dynamics model, which is usually defined as: p(st+1|st,at). Here, st is the state of the system at time t; at is the | control action. st, at in our case are used for conditioning (as x for the case of CVAE), and st+1 is viewed as an output (as y in CVAE).'\\n\\n'We propose to use simulators as \\u2018informed\\u2019 regularizers. Instead of starting from random encoder and decoder weights, we will first align the decoder function fdec with the output of an existing general-purpose simulator fsim. This implies that we put a soft restriction on the class of models for the decoder neural network function. Hence, we focus the search on a subspace that aligns more closely with a class of functions expressed by simulation (a class, since we consider distributions over simulator parameters).'\\n\\n'\\u03d5mix: a learnable mixture \\u2018prior\\u2019 that can be used as aggregate posterior (instead of using encoder activations)'\\n\\n'A single iteration of DET2STOC can be enough to obtain a good initial fit, even with a small number of real trajectories. This high data efficiency arises because decoder gradients are informative immediately after the pre-training step. Hence, encoder can be trained from few real samples and still meaningfully shift the posterior. Further iterations are beneficial for more complex scenarios. We ran 1-20 iterations for scenarios in this work. DET2STOC can be used in fully iterative mode: updating the posterior as soon as new hardware data arrives, potentially even after every timestep. DET2STOC can be also run in batch mode: first collecting a set of hardware trajectories, then using them for all iterations.'\\n\\n'For both the encoder and decoder networks we use fully connected 3-layer MLPs with 3 with 64 hidden units, ReLU activations and layer normalization [18]. The number of latent variables is set to the number of simulation parameters we aim to infer. \\u03c6mix is parameterized by a Gaussian mixture, with full or diagonal covariance components (we experimented with both). Decoder outputs parameterize multivariate Gaussian distributions with diagonal covariances. Decoder alignment and ELBO are optimized with Adam [19], using learning rate 1e-4 with cosine decay.'\\n\\n'To evaluate predictive performance, we compare approximate forward model from DET2STOC\\u2019s decoder pdec(st+1|st) to the true dynamics. For this, we initialize the simulator with a randomly chosen state st, sample simulator parameters from the \\u2018real\\u2019 distribution \\u03c8true and simulate passive dynamics to get st+1. This yields a probability distribution\\\\np\\\\n\\u03c8\\\\ntrue\\\\n(\\\\ns\\\\nt\\\\n+1\\\\n|\\\\ns\\\\nt\\\\n)\\\\nthat we can compare with. We run multiple iterations of DET2STOC and evaluate the log likelihood of the test set \\u03bereal after each iteration. Table II shows these results and comparison with a CVAE baseline.'\",\"290\":\"'http:\\/\\/tiny.cc\\/similarity'\",\"291\":\"'DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1\\/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub2.'\",\"292\":null,\"293\":\"'The performance of the three algorithms was evaluated using three metrics: the computation time (CT), the localization error, and the robustness to the initial guess. In particular, operating system functions in the C++ source code were used to measure the CT needed by the algorithm to localize the MMs. Unfortunately, the CT of the TRRA algorithm could not be calculated, as it was implemented in a different platform (i.e. MATLAB). However, since Trust Region algorithms and the LMA are both based on the Newton Step method [32], they should exhibit similar convergence speeds, hence justifying the quantification of the CT for only one of the two optimization methods in this preliminary assessment.'\",\"294\":null,\"295\":null,\"296\":\"'https:\\/\\/github.com\\/zhaoymn\\/mbeats'\\n\\n'For the purpose of reproducing our approach, we release a novel dataset of mmWave heart rate measurement and our source code of neural networks: https:\\/\\/github.com\\/zhaoymn\\/mbeats.'\\n\\n'https:\\/\\/github.com\\/zhaoymn\\/mbeats'\",\"297\":\"'https:\\/\\/github.com\\/antoniopradom\\/Gait_Cycle_Percentage.'\\n\\n'Gait training is widely used to treat gait abnormalities. Traditional gait measurement systems are limited to instrumented laboratories. Even though gait measurements can be made in these settings, it is challenging to estimate gait parameters robustly in real-time for gait rehabilitation, especially when walking over-ground. In this paper, we present a novel approach to track the continuous gait cycle during overground walking outside the laboratory. In this approach, we instrument standard footwear with a sensorized insole and an inertial measurement unit. Artificial neural networks are used on the raw data obtained from the insoles and IMUs to compute the continuous percentage of the gait cycle for the entire walking session. We show in this paper that when tested with novel subjects, we can predict the gait cycle with a Root Mean Square Error (RMSE) of 7.2%. The onset of each cycle can be detected within an RMSE time of 41.5 ms with a 99% detection rate. The algorithm was tested with 18840 strides collected from 24 adults. In this paper, we tested a combination of fully-connected layers, an Encoder-Decoder using convolutional layers, and recurrent layers to identify an architecture that provided the best performance.'\\n\\n'In this paper, we present an algorithm which predicts the percentage of the gait cycle using an ANN. This novel algorithm uses an Encoder-Decoder RNN architecture that combines the filtering features of a CNN with the time series processing features of an RNN to predict the gait cycle percentage in real-time. We show that this model handles the raw data from 3 different sensed features (3 pressures, 3 accelerations, and 3 rotation angles) and accurately predicts the gait. To validate the performance of the model and the effects of the different layers, an analysis was performed with data recorded from 24 healthy adults wearing the DeepSole system [8] shown in Fig. 2. Using leave-one-out crossvalidation (LOOCV) [30], we tested the performance on subjects that were not included in the training. LOOCV can show the performance of a pretrained model on a new subject without any calibration or baseline recording.'\\n\\n'The programming code for all modules and models evaluated is freely available at https:\\/\\/github.com\\/antoniopradom\\/Gait_Cycle_Percentage.'\\n\\n'Graphical overview of the neural network modules in the Encoder-Decoder RNN model. ERM is an encoder-decoder RNN that maps the 9 signals collected by the DeepSole system into the predicted gait cycle percentage. A 0 value corresponds to gait cycle start and a 1 corresponds to the end.'\\n\\n'For the Encoder-Decoder (ED) module, three 1D convolutional layers with kernel sizes of 20, 10, and 5 were used to encode signals from each channel independently. The length of the tensors was fixed throughout the convolution by using the number of features of the input tensor as the number of filters. The convolution output was fed into the next module and then fed to a dense layer with a filter value of one, and then three 1D convolutional layers with kernel sizes of 20, 10, and 5 to decode the output. For all convolutions, a padding was done to keep the dimensionality of the input and a Relu activation was applied after each layer.'\\n\\n'The Encoder-Decoder Dense model (EDM) uses the Encoder module, then the dense module, then the decoder module, and finally the last module. By adding the Encoder module to DM, we can test if the Encoder-Decoder properties can substitute the sequencial processing of the RNN.'\\n\\n'The Encoder-Decoder RNN model (ERM) uses the Encoder module, then the RNN module, followed by the dense module, Decoder module, and finally the last module. This architecture is shown in Fig. 3. Our hypothesis is that the model which combines all 3 modules in a single model will have the best prediction performance. This is similar to the Encoder-Recurrent-Decoder model proposed by Fragkiadaki et al. to predict human body pose from video data [36].'\\n\\n'Adding the Encoder-Decoder module to the RNN and Dense modules (ERM) significantly reduces the RMSE of the predicted percentage of gait compared to DM, RM, and Linear. The ERM has the smallest RMSE and the lowest variability between subjects. Combining the Encoder-Decoder with the other modules also reduces the HS identification lag, as the ERM lag is significantly lower than that of RM. Moreover, it has the lowest number of false positives and has significantly fewer false negatives than RM, EDM, and Linear. This model has the best overall performance.'\\n\\n'The Linear model outperforms the model without Encoder-Decoder layers, except for the percentage of false negatives. This is because the Linear model is not able to consistently reach the extremes of the prediction. As shown in Fig. 5, it is usually bounded between 20% and 80% due to inter-subject variability.'\\n\\n'This work shows that the combination of all modules into a convolutional encoder and decoder can be used to accurately learn the temporal correlation across a time sequence. The RNN was used to learn the temporal dynamics from multichannel time series signals. This model can be used to predict the gait cycle percentage within 7%. Even though the subjects in the study had different cadences, the models were able to continuously predict the gait of the subject for both left and right sides.'\\n\\n'https:\\/\\/github.com\\/antoniopradom\\/Gait_Cycle_Percentage.'\",\"298\":null,\"299\":null,\"300\":null,\"301\":null,\"302\":null,\"303\":null,\"304\":null,\"305\":\"'Each separate module contains embedded electronic system, including a 32-bit ARM Cortex processor as the main controller and three sets of motor control chips. Salamanderbot\\u2019s circuit boards have shrunk in size by nearly half compared with their predecessors, which makes the entire body smaller. Motor sets are placed triangulated on bottom side of the circuit board. By adjusting the motor to pull or release the cable, the module can bend in different directions, thus controlling the movement direction of the module. Specifically, the main controller reads the value of motor encoders, then calculates and outputs the corresponding PWM for the motors according to the codes. This low-level control mechanism has been illustrated in our prior work [15], either. In addition, a SPI interface is extended on the circuit board, which enables each module to be easily configured into slave mode and receive master board\\u2019s instruction for high-level tasks.'\\n\\n'Before the experiment, the module is programmed with basic motion primitives so that Salamanderbot can adjust its posture to achieve steering. Although primitives are already encoded, the robot is remote controlled. Thus, the reaction time of human being likely slows down the speed. In future work, tactile sensors and\\/or a camera can help the robot automatically analyse the environment and execute the proper motion. In this case, we believe that the traversal speed would be comparable to the characterized linear speed values.'\",\"306\":\"'Each link\\u2019s position and orientation is measured via magnetic field distortion-based motion\\/position sensors running at 240 Hz (Polhemus, Model Liberty). Simultaneously, discrete time-step data, which is ultimately re-scaled to 240 Hz intervals for data processing, is measured from the thumb tendon excursion via four 5V Linear Optical Encoders on 300DPI optical rails (USDigital, Model 30035N), read by a Saleae Logic Analyzer (Saleae, Model 0160) and synchronized with the motion sensor data.'\\n\\n'The tendons are actuated externally so they cannot slacken at any point in the thumb end effector\\u2019s workspace. Approximately 500g is vertically loaded at the end of each of the tendons, after they each tie-in to one of four 3D printed blocks on horizontal linear rails, enabling data collection from our linear encoders as shown in Figure 4(a). For each run, this data is compiled as the time-series response of tendon excursion associated with each end effector position throughout the workspace, shown in Figure 5.'\",\"307\":\"'The developed impedance controller has been implemented on the system and the following results were obtained. The angular position of the front links was obtained using an optical encoder mounted at the motor shaft that produces 3690 pulses per revolution and has a resolution of 0.1\\u02da. Fig. (7) shows the modified dynamic behavior of the front links under the influence of external torque.'\",\"308\":null,\"309\":\"'The rotation of each wheel is recorded using a quadrature rotary encoder at 100Hz and resolution of 600 increments per full rotation. This high resolution was needed as the large wheel radius results in large ground travel increment for each rotational increment.'\\n\\n'Chassis used to perform the experiments. A caster wheel was used to stabilise the platform, rotary encoders on each wheel to record rotation and time of flight sensors to record ride height.'\",\"310\":\"'Schematic concept of the two-level GNC architecture for Autonomous Navigation of Planetary Rovers. Orange coloured boxes refer to sensors, green to developed components and greyed components to re-used from open-source community.'\",\"311\":\"'https:\\/\\/github.com\\/YiLunLee\\/Unsupervised-Face-Recognition'\\n\\n'In this paper, we propose a face recognition model which unsupervisedly learns the identity-aware feature representation of face images in the unconstrained videos by leveraging the spatiotemporal characteristics in a video as well as the augmented training data provided by our augmentation network. Two key ideas behind motivate our model designs: First, an identity-encoder projects face images into an embedding space where the euclidean distance between samples in this space represents the semantic of identity similarity, which is learned by exploiting the algorithm of triplet-based metric learning. In other words, the faces of the same identity should be close to each other, while those of different identities ought to be as far away from each other as possible. In order to eliminate the expensive effort of manually annotating groundtruth data as supervision, we refer to the physical constraints in a video to produce the training data of negative pairs for the metric learning, i.e. the persons appear at the same frame are definitely with different identities. While for the positive pairs, we extract the feature of face images by a pre-trained VGG- based network, take the nearest neighbors of every face, and assume they are of the same identity. According to this idea, we thus achieve unsupervised learning. Second, in order to let face recognition model have better adaptability to the changes in the unconstrained videos (examples as illustrated in Figure 1) and avoid getting over-fitted to the face samples drawn for metric learning, we proposed an augmentation network to enrich the data distribution used for training the identity-aware feature representation. This augmentation network is realized by extending the CVAE- GAN [3] architecture into the CVAE-InfoGAN one, which we are going to detail later. With taking identity-aware features as the condition, our CVAE-InfoGAN disentangles out the features that are irrelevant to the identity, hence the augmentation network is capable of synthesizing face images of various appearance but the same identity. With the richer data obtained by disentanglement and self-augmentation, our identity-encoder model can be further refined and in results gets better performance in face recognition.'\\n\\n'Identity Encoder'\\n\\n'A. Stage-1: Training Identity-Encoder Eid'\\n\\n'The identity-encoder Eid aims at extracting the identity- aware feature of face images and is trained to discover an embedding space where the euclidean distance between embedded features stands for the similarity of identities (i.e. smaller distance leads to a higher chance of belonging to the same person). This is achieved by using the metric learning technique (i.e. triplet network [22] in our approach), where we need to provide it the training data composed of negative and positive pairs (i.e. two face images of different identities or of the same identity respectively). We then exploit the spatiotemporal characteristic of the face samples within a video so as to build up the training data, as described below.'\\n\\n'Overview of our proposed method. The training procedure is composed of three stages. First, we train identity-encoder Eid based on the triplet loss, where the training data is automatically discovered from the spatiotemporal characteristics of the target video. Second, upon being conditioned on the identity-aware feature ec obtained from Eid, our augmentation network stemmed from CVAE-InfoGAN learns to disentangle a face image into identity-aware ec and identity-irrelevant ez features and is able to synthesize augmented data according to the given condition ec. Finally, with having more triplet produced by the augmented data, Eid is further fine-tuned to achieve better performance in face recognition.'\\n\\n'With the positive and negative pairs, we construct the triplets to train our identity-encoder Eid for learning the identity-aware feature of face images. Here we adopt the symmetric triplet loss [1] as our objective function. Given a triplet\\\\nT=(\\\\nx\\\\ni\\\\nk\\\\n,\\\\nx\\\\ni\\\\nl\\\\n,\\\\nx\\\\nj\\\\nk\\\\n)\\\\n, where\\\\n(\\\\nx\\\\ni\\\\nk\\\\n,\\\\nx\\\\ni\\\\nl\\\\n)\\u2208\\\\nP\\\\n+\\\\n,\\\\n(\\\\nx\\\\ni\\\\nk\\\\n,\\\\nx\\\\nj\\\\nk\\\\n)\\u2208\\\\nP\\\\n\\u2212\\\\n, there are three distances in the embedding space produced by\\\\nE\\\\nid\\\\n:d(\\\\nE\\\\nid\\\\n(\\\\nx\\\\ni\\\\nk\\\\n),\\\\nE\\\\nid\\\\n(\\\\nx\\\\ni\\\\nl\\\\n))\\\\n,\\\\nd(\\\\nE\\\\nid\\\\n(\\\\nx\\\\ni\\\\nl\\\\n),\\\\nE\\\\nid\\\\n(\\\\nx\\\\nj\\\\nk\\\\n))\\\\n, and\\\\nd(\\\\nE\\\\nid\\\\n(\\\\nx\\\\ni\\\\nk\\\\n),\\\\nE\\\\nid\\\\n(\\\\nx\\\\nj\\\\nk\\\\n))\\\\n, where the first one is a positive pair and the last two forms negative pairs, and d represents the euclidean distance. The symmetric triplet loss \\u2112s is defined as'\\n\\n'https:\\/\\/github.com\\/YiLunLee\\/Unsupervised-Face-Recognition'\",\"312\":null,\"313\":\"'https:\\/\\/github.com\\/abenbihi\\/wasabi.git'\\n\\n'The contribution of this paper is a novel global image descriptor based on semantics edge geometry for image-based place recognition in bucolic environment. Experiments show that it is also suitable for urban settings. The descriptor\\u2019s and the evaluation\\u2019s code are available at https:\\/\\/github.com\\/abenbihi\\/wasabi.git.'\\n\\n'Early methods build global descriptors with statistics of local features over a set of visual words. A first step defines a codebook of visual words by clustering local descriptors, such as SIFT [7], over a training dataset. Given an image to describe, the Bag of Words (BoW) [8] method assigns each of its local features to one visual word and the output statistics are a histogram over the words. The Fisher Kernels [12] refine this statistical model fitting a mixture of Gaussian over the visual words and the local features. This approach is simplified in VLAD [13] by concatenating the distance vector between each local feature and its nearest cluster, which is a specific case of the derivation in [12]. These methods rely on features based on pixel distribution that assumes that images have strong textures, which is not the case for bucolic images. They are also sensitive to variations in the image appearance such as seasonal changes. In contrast, we rely on the geometry of semantic elements and that proves to be robust to strong appearance changes.'\\n\\n'This section details the experimental setup and presents results for our approach against methods for which public code is available: BoW[8], VLAD[27], NetVLAD[5], DELF[6], Toft et al. [16], and VLASE [17]. We demonstrate the retrieval performance on two outdoor bucolic datasets: CMU-Seasons[2] and Symphony[1], recorded over a period of 1 year and 3 years respectively. Although existing methods reach SoA performance on urban environments, our approach proves to outperform them on bucolic scenes, and so, even when they are finetuned. It also shows better generalization as it achieves near SoA performance of the urban slices on the CMU-Seasons dataset.'\\n\\n'A new 64-words codebook is generated for BOW and VLAD, using the CMU park images from slices 18-21. The NetVLAD training requires images with ground-truth poses, which is not the cases for these slices. So it is trained on three slices from 22-25 and evaluated on the remaining one. On Symphony, images together with their ground-truth poses are sampled from the east side of the lake that is spatially disjoint from the evaluation traversals. The DELF local features are not finetuned as the training code is not available even though the model is. The segmentation model [21] used for Toft et al.\\u2019s descriptor is the same as for WASABI and was trained to segment the CMU park across seasons. The CaseNet model used by VLASE is not finetuned.'\\n\\n'https:\\/\\/github.com\\/abenbihi\\/wasabi.git'\",\"314\":\"'From the recognition results presented above, we can see there are some differences in terms of recognition performance among different classifiers. Compared with other models, the BPNN, SVM, and RF are more competitive to encode natural manipulations with corresponding behaviors. This is because they have more complicated structures, providing better capability to model the relationship between natural manipulations and behavioral data.'\",\"315\":\"'While occupancy grid maps only consist of occupancy probabilities, NDT maps are originally also designed as a regular grid, but instead of an occupancy value, the cells encode the measurement points falling into them as a multi-variate sample distribution [2]. Consequently, a coarser grid resolution is sufficient. NDT-OM(FG) [3], [4] and ONDT [5] are combinations of these two possibilities and the involved grid cells contain both information types.'\\n\\n'We solve the matching function in a derivation-free manner using fast open source solvers. In contrast to hand-crafted Jacobian- and Hessian-based methods, our approach is easy to implement and very fast.'\",\"316\":null,\"317\":null,\"318\":\"'https:\\/\\/youtu.be\\/WHbbtU-Q9-k'\\n\\n'LiDAR odometry and mapping (LOAM) has been playing an important role in autonomous vehicles, due to its ability to simultaneously localize the robot\\u2019s pose and build high-precision, high-resolution maps of the surrounding environment. This enables autonomous navigation and safe path planning of autonomous vehicles. In this paper, we present a robust, real-time LOAM algorithm for LiDARs with small FoV and irregular samplings. By taking effort on both frontend and back-end, we address several fundamental challenges arising from such LiDARs, and achieve better performance in both precision and efficiency compared to existing baselines. To share our findings and to make contributions to the community, we open source our codes on Github1.'\\n\\n'While most of the previous work were based on spinning LiDARs, in this work, we focus on the odometry and mapping with solid-state LiDARs of small FOVs. Our contributions are: (1) we develop a complete LOAM algorithm for LiDARs with small FOVs. The algorithms is carefully engineered and made open source to benefit the community; (2) we further increase the accuracy and robustness of the LOAM algorithm by considering the low-level physical properties of LiDAR sensors in the front-end processing; (3) we propose a picewise processing technique to overcome the motion blur problem, and parallelize its implementation. Experiments show that the piecewise processing outperforms linear interpolation in terms of accuracy and running efficiency.'\",\"319\":null,\"320\":\"'Wheel encoder-based odometry data are also provided, as they are widely available in wheeled robots. The odometry data from the Segway robot are fused from wheel encoders and a chassis IMU by proprietary filtering algorithms along with the robot.'\\n\\n'The OpenLORIS-Scene datasets and the proposed metrics are tested with open-source SLAM algorithms. The algorithms are chosen to cover most data types listed in Table I, and to represent a diverse set of SLAM techniques. ORBSLAM2 is a feature-based SLAM algorithm [16]. It can optimize poses with absolute scale by using either stereo features or depth measurements. DSO, on the contrary, tracks the camera\\u2019s states with a fully direct probabilistic model [17]. DS-SLAM improves over ORB-SLAM2 by removing features on moving objects [18]. VINS-Mono provides robust pose estimates with absolute scale by fusing pre-integrated IMU measurements and feature observations [19]. InfiniTAM is a dense SLAM system based on point cloud matching with an iterative closed point (ICP) algorithm [20]. ElasticFusion combines the merits of dense reconstruction and globally consistent mapping by using a deformable model [21].'\\n\\n'The capability of continuous self localization is fundamental to autonomous service robots. Visual Simultaneous Localization and Mapping (SLAM) has been proposed and studied for decades in robotics and computer vision. There have been a number of open source SLAM systems with careful designs and heavily optimized implementations. Do they suffice for deployment in real-world robots? We claim there is still a gap, coming from the fact that most SLAM systems are designed and evaluated for a single operation.'\\n\\n'Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot\\u2019s long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io\\/dataset\\/scene.'\",\"321\":\"'http:\\/\\/ronin.cs.sfu.ca\\/'\\n\\n'This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of horizontal positions and heading direction of a moving subject from a sequence of IMU sensor measurements from a phone. In contrast to existing methods, our method can handle varying phone orientations and placements.More concretely, the paper presents 1) a new benchmark containing more than 40 hours of IMU sensor data from 100 human subjects with ground-truth 3D trajectories under natural human motions; 2) novel neural inertial navigation architectures, making significant improvements for challenging motion cases; and 3) qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks. We share the code and data to promote further research. (http:\\/\\/ronin.cs.sfu.ca).'\\n\\n'We share the code and data to promote further research in a hope to establish an ultimate anytime anywhere navigation system for everyone\\u2019s smartphone.'\\n\\n'IONet: We use our local implementation, as the code is not publicly available. As in RIDI method, we train a unified model on RoNIN dataset, and a separate model for each placement type for RIDI and OXIOD datasets.'\",\"322\":\"'We propose a fast and lightweight end-to-end convolutional network architecture for real-time segmentation of high resolution videos, NfS-SegNet, that can segement 2K-videos at 36.5 FPS with 24.3 GFLOPS. This speed and computation-efficiency is due to following reasons: 1) The encoder network, NfS-Net, is optimized for speed with simple building blocks without memory-heavy operations such as depthwise convolutions, and outperforms state-of-the-art lightweight CNN architectures such as SqueezeNet [2], Mo- bileNet v1 [3] & v2 [4] and ShuffleNet v1 [5] & v2 [6] on image classification with significantly higher speed. 2) The NfS- SegNet has an asymmetric architecture with deeper encoder and shallow decoder, whose design is based on our empirical finding that the decoder is the main bottleneck in computation with relatively small contribution to the final performance. 3) Our novel uncertainty-aware knowledge distillation method guides the teacher model to focus its knowledge transfer on the most difficult image regions. We validate the performance of NfS-SegNet with the CITYSCAPE [1] benchmark, on which it achieves state-of-the-art performance among lightweight segementation models in terms of both accuracy and speed.'\\n\\n'System overview: Our network is composed of a fast encoder and has an asymmetric architecture, where the encoder is heavier than the decoder, and is trained with uncertainty-aware knowledge distillation. The fast encoder network (NfS-Net) and real-time segmentation network (NfS-SegNet) are described in Section III-A and Section III-B. Section IV-A and IV-C introduces our uncertainty-aware knowledge distillation to utilize the knowledge of the larger teacher network (GD-Net) and unlabeled data from CITYSCAPES [1].'\\n\\n'We propose an end-to-end trainable architecture for real-time scene segmentation (NfS-SegNet) that uses NfS-Net as an encoder, which obtains the fastest speed on the CITYSCAPES [1] challenge and significantly outperforms state-of-the-art lightweight achitectures.'\\n\\n'To expedite the speed of the deep network for real-time semantic segmentation, we need a fast encoder architecture.'\\n\\n'A. NfS-Net: Fast Encoder'\\n\\n\\\"In this section, we propose a fast and lightweight convolutional network architecture for the encoder network. We optimize the network for speed by using some of the known techniques. Specifically, we perform all convolutions without bias terms and aggressively downsample in early stage. and promote reuse of the features as much as possible by using the DenseNet [21] structure. We use only four types of layers (convolution, parametric relu, pooling and concatenation). We minimize memory access which is the main bottleneck of the inference. Ma et al. [6] also suggests that excessive group convolution increases memory access cost and network fragmentation reduces degree of parallelism. Fig 3 shows the runtime of NfS-Net at each feature map resolution, against that of well-known fast encoders. We see that NfS-Net does not have any distinctive bottleneck, while MobileNet or ShuffleNet has bottleneck at either the lower layer or upper layer of the network. MobileNet's bottleneck is on use of depthwise convolution that requires heavy memory access, and ShuffleNet is inherently deep in its architecture.\\\"\\n\\n'For ADAS and autonomous driving, input videos need to have high resolutions to detect objects as early as possible, in order to have sufficient time to react to unexpected events or avoid collisions. To this end, we propose an architecture that can process 2K-videos in real-time. We start by adding a new encoder with a stride at the end of NfS-Net, where the input video resolutions are reduced at each step, to obtain a feature map that is as small as 1\\/256 of the original image. This allows us to lower the amount of computation while dramatically increasing the resolution of the videos. As for the decoder, we made it lightweight, and made it to simply double the result of the decoder. Thus our network architecture is highly asymmetric with most of the computations and parameters allocated for the encoder, with a very shallow decoder. This is in contrast to existing encoder-decoder segmentation architectures, such as UNet [27], which is mostly symmetric. This design choice is based on our empirical findings. First, the encoder has a short runtime even with high computation cost. Figure 4 shows that while the encoder takes up most of the comptuations, the actual runtime is marginal compared to the decoder. Secondly, the increase in the decoder complexity does not contribute much to the performance. Figure 5 shows that increasing the complexity of the decoder results in large reduction in the speed (36.4 FPS \\u2192 27.5) while obtaining diminishing return on the accuracy (IoU 73.1 \\u2192 73.4). Based on these findings, we made the encoder relatively heavier while slimming down the decoder.'\\n\\n'Now we validate our fast, lightweight semantic segmentation network, NfS-SegNet on real-time video segmentation of first-person dashcam videos, as well as perform experiments to analyze each of its part (fast CNN encoder, uncertainty- aware knowledge distillation).'\\n\\n'We proposed a fast and lightweight end-to-end CNN architecture for real-time scene segmentation of high-resolution videos. Our model, NfS-SegNet, is composed of a very fast encoder (NfS-Net), and is designed asymmetrically to allocate the parameters and computation more on the encoder that can be computed fast, with less focus on the decoder which is the bottleneck in inference. We further proposed a novel uncertainty-aware knowledge distillation method, that focuses more on the difficult part of the image when distilling knowledge of the teacher network, and significantly improved the accuracy of our network. We validate our method on CITYSCAPES benchmark, on which it outperforms all other lightweight real-time semantic segmentation models in both the accuracy and the speed.'\",\"323\":\"'We obtained the source code of recent single image algorithms [19], [21], [22], [30], [3]. In addition, we compare our own single image algorithm along with these methods. Our single image algorithm is based on a CNN, similar to the regression approach presented in [30]. We parametrise the horizon line h by offset \\u03c9 and slope \\u03b8. With image width W, it is defined in homogeneous coordinates as:'\",\"324\":null,\"325\":\"'https:\\/\\/robotic-esp.com\\/code\\/'\\n\\n'https:\\/\\/robotic-esp.com\\/code\\/'\",\"326\":null,\"327\":null,\"328\":null,\"329\":null,\"330\":null,\"331\":null,\"332\":\"'For data collection, we use the uSD card extension board and store binary encoded data roughly every 10ms. Each dataset is timestamped using the on-board microsecond timer and the clocks are synchronized before takeoff using broadcast radio packets. The drift of the clocks of different Crazyflies can be ignored for our short flight times (less than 2min).'\",\"333\":null,\"334\":null,\"335\":\"'In this work, we propose a new method for predicting a distribution over the actor\\u2019s future behavior that addresses the limitations of previous methods. Our approach relies on having a pre-cached map of the environment that encodes the precise topology of the road network. This topological representation captures information about the spatial relationships between lanes, thereby encoding important semantics on where and how to drive. In many existing approaches, this road topology is either ignored altogether or converted into a 2D bird\\u2019s-eye view of the scene, which we posit leads to significant information loss. Prior approaches thus under-utilize the map information and struggle with the basic task of predicting that actors will follow lanes, spurring the need for auxiliary loss functions to prevent actors from being predicted to drive off the road [7].'\\n\\n'In this section, we describe our approach for predicting vehicle occupancy over the lane network. In order to focus on the prediction problem, we assume we already have an object detection and tracking system which operates on sensor data to produce a list of actors in the scene along with their state estimates. We further assume that we have access to a high-resolution map of our surroundings, which encodes road boundaries, lane boundaries, lane connectivity, lane directions, crosswalks, traffic signals, and other details of the scene geometry. In this approach, we consider each actor in turn and generate predictions for the actor of interest in the context of all other actors in the scene, including the self-driving vehicle (SDV) itself. An overview of our approach is shown in Figure 2.'\",\"336\":\"'Our pedestrian prediction network architecture. The generator network consists of a recurrent encoder network, a variational autoencoder, an intent prediction module and recurrent decoder network. The discriminator network consists of a recurrent encoder network to distinguish between real and fake trajectories.'\\n\\n'Encoder Network'\\n\\n'The recurrent encoder network consists of a linear spatial embedding layer with a ReLU activation layer, \\u03b1(\\u2022), followed by an LSTM layer, where We and Wlstme are weights of the embedding layer and LSTM, respectively.'\\n\\n'In this section, we describe our probabilistical model of intent recognition and how navigation intent can be combined with latent embeddings from the recurrent encoder network to improve longer term prediction of pedestrian trajectory.'\\n\\n'Decoder Network'\\n\\n'The goal of the decoder network is to use the latent embedding from the encoder network combined with the intent distribution to generate prediction samples. The decoder network used is similar to that in [18] and consists of linear and recurrent layers used to generate pedestrian predictions.'\\n\\n'The hidden state of the LSTM decoder,\\\\np\\\\nt\\\\ni\\\\nis initialized to\\\\nz\\\\nt\\\\ni\\\\n. Here Wd, Wlstmd and Wo are weights, and \\u03c6 is a fully connected layer.'\\n\\n'During training, we use a combination of the mean squared error (MSE) between the ground truth trajectory and the predicted trajectory, adversarial loss, as well as KL-divergence loss from a unit Gaussian distribution for the variational autoencoder as our loss function. Similar to [18], we also experiment with variety loss during training where for each scene we generate k possible outputs and choose the lowest MSE to generate diversity in the samples.'\\n\\n'The dimension of the hidden state for the encoder, p is 16. The dimension of the decoder network\\u2019s hidden state, z is 32 including the 16 dimensions representing probability of navigation intent. We trained for 200 epochs with a batch size of 64 using the Adam optimizer with the initial generator network learning rate set to 0.0001 and the initial discriminator network learning rate set to 0.001.'\",\"337\":\"'https:\\/\\/github.com\\/RoboticsBUT\\/Brno-Urban-Dataset-Calibrations'\\n\\n'https:\\/\\/github.com\\/RoboticsBUT\\/Brno-Urban-Dataset-Tools'\\n\\n'https:\\/\\/github.com\\/RoboticsBUT\\/Brno-Urban-Dataset'\\n\\n'Autonomous driving is a dynamically growing field of research, where quality and amount of experimental data is critical. Although several rich datasets are available these days, the demands of researchers and technical possibilities are evolving. Through this paper, we bring a new dataset recorded in Brno - Czech Republic. It offers data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy which, to the best knowledge of the authors, is not available from any other public dataset so far. In addition, all the data are precisely timestamped with submillisecond precision to allow wider range of applications. At the time of publishing of this paper, recordings of more than 350 km of rides in varying environment are shared at: https:\\/\\/github.com\\/RoboticsBUT\\/Brno-Urban-Dataset.'\\n\\n'https:\\/\\/github.com\\/RoboticsBUT\\/Brno-Urban-Dataset-Calibrations'\\n\\n'https:\\/\\/github.com\\/RoboticsBUT\\/Brno-Urban-Dataset-Tools'\\n\\n'https:\\/\\/github.com\\/RoboticsBUT\\/Brno-Urban-Dataset'\",\"338\":\"'Decision-making in dense traffic scenarios is challenging for automated vehicles (AVs) due to potentially stochastic behaviors of other traffic participants and perception uncertainties (e.g., tracking noise and prediction errors, etc.). Although the partially observable Markov decision process (POMDP) provides a systematic way to incorporate these uncertainties, it quickly becomes computationally intractable when scaled to the real-world large-size problem. In this paper, we present an efficient uncertainty-aware decision-making (EUDM) framework, which generates long-term lateral and longitudinal behaviors in complex driving environments in real-time. The computation complexity is controlled to an appropriate level by two novel techniques, namely, the domain-specific closed-loop policy tree (DCP-Tree) structure and conditional focused branching (CFB) mechanism. The key idea is utilizing domain-specific expert knowledge to guide the branching in both action and intention space. The proposed framework is validated using both onboard sensing data captured by a real vehicle and an interactive multi-agent simulation platform. We also release the code of our framework to accommodate benchmarking.'\\n\\n'open-source'\\n\\n'In this paper, we proposed the EUDM framework for automated driving in dense interactive scenarios, by introducing two novel techniques, namely, the DCP-Tree and CFB mechanism. The complete framework is open-sourced and comprehensive evaluations are conducted using both real-world data and simulation. In the future, we will conduct closed-loop field test for the EUDM framework.'\",\"339\":\"'In this section, we describe the whole pipeline of our IPP-RL framework. Our IPP-RL works in two stages: IPP model pretraining and retraining in reinforcement learning. In the IPP pretraining stage, the training data is a set of driving videos cellected by the expert. It consists of high-level commands, location and RGB image information. We use ego-centric RGB image, measured speed along with steering angle calculated by PP as inputs, and experts\\u2019 action as ground truth, to train the neural network. In the reinforcement learning stage, we select the model-free algorithm, DDPG, which consists of an actor-network and a critic-network. We build the actor-network as the same as IPP-network and initialize it with the all weights from pretrained IPP model to predict reasonable action instead of exploring randomly. Moreover, we design a preferable reward function to guide the Actor to predict action, and use the n-step return to optimize the critic-network. All networks mentioned above are subject to the multi-branch mechanism introduced in the work by Codevilla et al. [2] and each branch is trained separately.'\",\"340\":null,\"341\":null,\"342\":\"'For each image pair, we compute the essential matrix E that encodes the relative pose between the query and database image. Next, we extract the four relative poses (R,t), (R, \\u2212t), (R\\u2032,t), (R\\u2032,\\u2212t) corresponding to E [25], where R and R\\u2032 are related by a 180\\u00b0 rotation around the baseline [25]. Traditionally, a cheirality test based on feature matches is used to find the correct relative pose among the four candidates [25]. However, methods that directly regress the relative pose typically do not provide such matches.'\",\"343\":\"'The source code for this paper is available online1.'\",\"344\":null,\"345\":\"'Visual place recognition algorithms trade off three key characteristics: their storage footprint, their computational requirements, and their resultant performance, often expressed in terms of recall rate. Significant prior work has investigated highly compact place representations, sub-linear computational scaling and sub-linear storage scaling techniques, but have always involved a significant compromise in one or more of these regards, and have only been demonstrated on relatively small datasets. In this paper we present a novel place recognition system which enables for the first time the combination of ultra-compact place representations, near sub-linear storage scaling and extremely lightweight compute requirements. Our approach exploits the inherently sequential nature of much spatial data in the robotics domain and inverts the typical target criteria, through intentionally coarse scalar quantization-based hashing that leads to more collisions but is resolved by sequence-based matching. For the first time, we show how effective place recognition rates can be achieved on a new very large 10 million place dataset, requiring only 8 bytes of storage per place and 37K unitary operations to achieve over 50% recall for matching a sequence of 100 frames, where a conventional stateof-the-art approach both consumes 1300 times more compute and fails catastrophically. We present analysis investigating the effectiveness of our hashing overload approach under varying sizes of quantized vector length, comparison of near miss matches with the actual match selections and characterise the effect of variance re-scaling of data on quantization. Resource link: https:\\/\\/github.com\\/oravus\\/CoarseHash.'\",\"346\":null,\"347\":\"'learn a good latent space by encoding observations through appropriate regularizations and explicitly concatenate to it an encoding of the vector of dynamics parameter configurations; condition the policy decoder on this latent representation'\\n\\n'Our basic model consists of an encoder for observations and a policy (or \\u2018action\\u2019) decoder. The Markov chain corresponding to the model is X \\u2192 Z \\u2192 A, where X is the input state (which can either be fully observable or partially observable). A is the action space, a sample from which is what the model outputs. We consider A to be a normal distribution whose mean and variance are predicted by the decoder from latent Z. Our training scheme is end-to-end and hence we do not need intermediate supervision for latent Z. In the subsequent sections, we denote the observation encoder by f\\u03d5(\\u2022) and the action decoder by g\\u03b8(\\u2022). Later, we also introduce the dynamics encoder, inverse dynamics model, and state (reconstruction) decoder respectively denoted by M\\u03b6(\\u2022), ginv(\\u2022), and frec(\\u2022). The Dynamics Conditioned Policy module in Fig. 1 describes the basic architecture of MANGA. All the model components are realized by feed-forward neural networks.'\\n\\n'We condition our policy decoder both on the current observation frame in the environment and on an encoding of dynamics parameters of the agent and the environment. Many other previous papers [5], [3] considered the raw dynamics parameters as input to the policy model (i.e. without encoding them separately from input observations), however, it is important to consider a separate encoding of the parameters so that they scale well and are in sync with the latent encoding of input observations. This is also important because the observations change in each time-step while the dynamics parameter vector, and thus the dynamics encoding, remains fixed within each episode of training.'\\n\\n'We postulate that the auxiliary modules, namely the inverse dynamics model and the state-reconstruction decoder are needed to learn a good latent space Z for effective transfer. MANGA refers to the proposed approach with all components present. For reference, we compare MANGA with an Oracle. Oracle refers to an untrained agent of the same architecture as MANGA, that has access to ground-truth system parameters, and is trained from scratch directly in the test environment. In Fig. 2, we show results by selectively ablating different components of the proposed model when the dynamics parameters are perturbed in the range of \\u00b15% of base values. There is clearly a drop in performance on the test environment when we remove either or both the auxiliary modules. Interestingly, removing the inverse dynamics model causes a very sharp decrease in performance across all the three MuJoCo domains. Hence, it is clear that ignoring nuisance correlates between states and actions is important for quick and effective transfer.'\",\"348\":null,\"349\":\"'This process seems overly complex compared to directly encode p(x), for example as a mixture, a non-parametric density or with a variational autoencoder. However, it offers several advantages, especially when we have access only to small datasets, and\\/or when very good generalization capabilities are required, namely:'\\n\\n'We proposed the use of variational techniques with mixture models as a general tool to represent distributions of robot configurations satisfying several forms of objectives. We emphasized its advantages over sampling methods, showing that computational and memory requirements can be significantly reduced in high dimension space, which can be crucial in many applications. The developed approach, and associated source codes, can be exploited in a wide range of applications. It is particularly promising for training unnormalized density (e.g., energy-based model or PoE) as it can handle multimodality better than sampling approaches. This direction will be further developed and presented in future studies.'\",\"350\":null,\"351\":null,\"352\":null,\"353\":null,\"354\":null,\"355\":null,\"356\":null,\"357\":null,\"358\":\"'This paper introduces Action Image, a new grasp proposal representation that allows learning an end-to-end deep-grasping policy. Our model achieves 84% grasp success on 172 real world objects while being trained only in simulation on 48 objects with just naive domain randomization. Similar to computer vision problems, such as object detection, Action Image builds on the idea that object features are invariant to translation in image space. Therefore, grasp quality is invariant when evaluating the object-gripper relationship; a successful grasp for an object depends on its local context, but is independent of the surrounding environment. Action Image represents a grasp proposal as an image and uses a deep convolutional network to infer grasp quality. We show that by using an Action Image representation, trained networks are able to extract local, salient features of grasping tasks that generalize across different objects and environments. We show that this representation works on a variety of inputs, including color images (RGB), depth images (D), and combined color-depth (RGB-D). Our experimental results demonstrate that networks utilizing an Action Image representation exhibit strong domain transfer between training on simulated data and inference on real-world sensor streams. Finally, our experiments show that a network trained with Action Image improves grasp success (84% vs. 53%) over a baseline model with the same structure, but using actions encoded as vectors.'\",\"359\":\"'The oriented bounding box is decoded to 6 channels images (Score Map Channel, d1 Channel, d2 Channel, d3 Channel, d4 Channel, \\u03b8 Channel). For each di Channel that represent the location of the edges, the lighter the color is, the closer the pixel is to the corresponding edges. Both of the oriented bounding box and the 6 channels images represent the same ground truth label.'\\n\\n'Our model contains three components, the feature extractor, unpooling layer and the output unit. The green blocks denote the feature extractor, which is consist of 4 fully convolutional layers. The red blocks represent the unpooling layers, which are used to decode the higher layer features. The output unit uses 1 \\u00d7 1 kernel to predict the 6 channels images. The blue and yellow arrows represent the process of enlarging and concatenating feature maps, respectively. The right image shows the detail of concatenating. We first double the feature map size di, and then concatenate it with the ResNet50 feature map f4\\u2212i in the last channel, which is followed by two conv1\\u00d71 and conv3\\u00d73 kernels to fuse these two feature maps.'\",\"360\":\"'https:\\/\\/github.com\\/cxy1997\\/Transferable-Active-Grasping'\\n\\n'https:\\/\\/github.com\\/cxy1997\\/Transferable-Active-Grasping'\",\"361\":\"'https:\\/\\/github.com\\/pyni\\/PointNet2_Grasping_Data_Part'\\n\\n'https:\\/\\/github.com\\/pyni\\/PointNet2_Grasping_Data_Part'\",\"362\":\"\\\"Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp - a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to real-world images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms' performance on transparent objects. Code, data, and benchmarks will be released. Supplementary materials: https:\\/\\/sites.google.com\\/view\\/cleargrasp.\\\"\\n\\n'Our primary contributions are twofold. First, we propose an algorithm for estimating accurate 3D geometry of transparent objects from RGB-D images. Second, we construct a large-scale synthetic dataset of over 50,000 RGB-D images as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. Our experiments demonstrate that ClearGrasp is capable of generalizing not only to transparent objects in the real-world, but also to novel objects unseen in training. ClearGrasp is substantially better than monocular depth estimation baselines, and our ablative studies show the importance of critical design decisions. We also demonstrate that ClearGrasp can be applied out-of-the-box with state-of-the-art manipulation algorithms to achieve 86% and 72% picking success rates with suction and parallel-jaw grasping respectively on a real-world robot platform. Code, data, pretrained models, and benchmarks will be released.'\",\"363\":null,\"364\":null,\"365\":\"'We conduct experiments to demonstrate how the self- supervised learning improves the system. We first show that by using the self-annotated images to fine-tune the semantic segmentation network, we can significantly improve the segmentation accuracy compared to the original network trained with synthetic data only. Semantic segmentation suffers from the domain gap severely for objects with complex material properties, such as shininess. Fine-tuning the network can bridge the domain gap. Second, we show that fine-tuning the auto-encoders used in PoseRBPF [9] can improve their reconstruction quality, and consequently improve the pose estimation accuracy. Finally, we conduct robot grasping experiments to illustrate that our fine-tuned system can achieve a high success rate in model-based grasping based on 6D object pose estimation.'\\n\\n'Due to the noises from the joint encoders of the robot or the bias in the hand-eye calibration, simply applying forward kinematics might often lead to inaccurate object poses. Similar to the pose initialization process, we perform 1 step filtering with the propagated particles using the current RGB- D image for all the objects. To ensure the quality of the generated data for self-supervised learning, we only save data when the two pose evaluation metrics are above certain thresholds (Sec. III-B.4). Once the system decides to save the data, the object poses are further refined with the SDF-based pose refinement algorithm as described in Sec. III-B.3.'\\n\\n'With the collected images and estimated object poses, we can fine-tune the neural networks in our system to improve the pose estimation performance. Initially, both the network for segmentation and detection in PoseCNN and the autoencoders used in PoseRBPF (see Fig. 3) are trained only with synthetic data. As a result, the system cannot even segment some textureless objects in our experiments such as a red foam brick. Therefore, we employ the curriculum learning idea [39] to bootstrap the system sequentially. We start with simple scenes that contain only one object. If the network cannot segment the object, we perform a global initialization in PoseRBPF, where particles are uniformly sampled in the image. This process collects training data with single objects in images. We can then fine-tune the segmentation network to make it segment all objects. Fine-tuning with single objects also enables the network to segment objects in cluttered scenes. In the next stage, we move to cluttered scenes with multiple objects, and collect training data with different object poses and occlusions to fine-tune the segmentation network and the auto-encoders. During fine-tuning, we mix synthetic and real data to prevent the networks from overfitting. We can iterate between data collection and fine-tuning. Because when the networks become better, the system is more efficient in collecting new data.'\\n\\n'Fig. 6 presents the 6D pose estimation accuracy in terms of ADD and ADD-S using different percentages of real images for fine-tuning or number of particles. We can see that 1) both methods improve consistently with more real training data, and the gap between synthetic models and fine-tuned ones is obvious. 2) Using depth is important to achieve high accuracy, and our SDF-based pose refinement algorithm significantly boosts the accuracy. 3) The performance of our method improves consistently with more particles in PoseRBPF. However the improvement becomes less obvious when performing refinement. 4) It is interesting that our RGB based particle filter with SDF refinement achieves the best performance, which is even better than the RGB- D particle filter with refinement. This is because the autoencoders are trained only with RGB images. Using RGB images in the particle filter obtains more accurate rotation, and then SDF refinement can fix errors in translation, and adjust rotation locally. Fig 7 shows an example on how the fine-tuned segmentation network and autoencoders improve segmentation and pose estimation.'\",\"366\":null,\"367\":\"'Systems for closed-loop control of iBCIs operate under unique input-output conditions. One such condition is that their output command signals typically contain decoder noise from neural measurements [14]. This decoder noise is signal independent and contributes to the violation of Fitts\\u2019 Law when tasks are performed over a large enough range of target radii and command signal gains [5]. This is opposed to the signal-dependent noise commonly associated with physical movements [15][16] or sEMG measurements [17].'\\n\\n'In a closed-loop iBCI system, the introduction of signal-independent noise in the command signal results in feedback that also contains signal-independent noise. An analysis of the signal characteristics of iBCI command signals found that the magnitude of noise in the decoded signal had a strong effect on user performance in a targeting task with visual feedback [18]. A non-BCI related kinesthetic force discrimination study found that noise alters a user\\u2019s perception as measured by Weber fraction [19].'\",\"368\":null,\"369\":\"'We revise, improve and extend the system previously introduced by us and named SSM-VPR (Semantic and Spatial Matching Visual Place Recognition), largely boosting its performance above the current state of the art. The system encodes images of places by employing the activations of different layers of a pre-trained, off-the-shelf, VGG16 Convolutional Neural Network (CNN) architecture. It consists of two stages: given a query image of a place, (1) a list of candidates is selected from a database of places and (2) the candidates are geometrically compared with the query. The comparison is made by matching CNN features and, equally important, their spatial locations, selecting the best candidate as the recognized place. The performance of the system is maximized by finding optimal image resolutions during the second stage and by exploiting temporal correlation between consecutive frames in the employed datasets.'\",\"370\":null,\"371\":\"'We conduct experiments on a simulated environment that allows to collect statistically-significant data and compare our approach with simple solutions. We made the source code available online for reproducibility.1 We also conducted experiments on a real robot to show the applicability of our approach in the real world. In all experiments, we set the desired probability\\\\nP\\\\n~\\\\n\\u225c0.95\\\\n. We made available online a video of these experiments and additional ones.2'\\n\\n'A grid world encodes the simulated environment. Each cell is divided into four triangles to approximate the robot\\u2019s pose. Each triangle represents a physical state in S of Definition 1.'\",\"372\":\"'The pseudocode for the Task Sequence Execution thread is provided in Algorithm 1. This thread periodically queries the GA thread for its current best solution and uses that solution to determine the current task sequence for the agent to execute regardless the agent\\u2019s current task sequence. The Task Sequence Execution thread also exchanges with the other agents information such as tasks that have been completed, the agent\\u2019s current location, and the agent\\u2019s current best solution. This thread shares any solutions received with the GA thread, which incorporates them into its population. When the number of remaining tasks (locations) becomes less than the number of agents, this thread uses the min-max nearest neighbors algorithm to construct a task sequence for the agent (instead of using the GA\\u2019s solution, which has at least one task for every agent).'\\n\\n'Algorithm 1: Pseudocode for task sequence execution run in a separate thread on agent i.'\",\"373\":null,\"374\":null,\"375\":\"'http:\\/\\/crlab.cs.columbia.edu\\/brain_guided_rl\\/'\\n\\n'In reinforcement learning (RL), sparse rewards are a natural way to specify the task to be learned. However, most RL algorithms struggle to learn in this setting since the learning signal is mostly zeros. In contrast, humans are good at assessing and predicting the future consequences of actions and can serve as good reward\\/policy shapers to accelerate the robot learning process. Previous works have shown that the human brain generates an error-related signal, measurable using electroencephelography (EEG), when the human perceives the task being done erroneously. In this work, we propose a method that uses evaluative feedback obtained from human brain signals measured via scalp EEG to accelerate RL for robotic agents in sparse reward settings. As the robot learns the task, the EEG of a human observer watching the robot attempts is recorded and decoded into noisy error feedback signal. From this feedback, we use supervised learning to obtain a policy that subsequently augments the behavior policy and guides exploration in the early stages of RL. This bootstraps the RL learning process to enable learning from sparse reward. Using a simple robotic navigation task as a test bed, we show that our method achieves a stable obstacle-avoidance policy with high success rate, outperforming learning from sparse rewards only that struggles to achieve obstacle avoidance behavior or fails to advance to the goal.'\",\"376\":null,\"377\":\"'https:\\/\\/github.com\\/spebern\\/eog-emg.'\\n\\n'In this paper, we propose a real-time human-robot interface (HRI) system, where Electrooculography (EOG) and Electromyography (EMG) signals were decoded to perform reach-...'\\n\\n'In this paper, we propose a real-time human-robot interface (HRI) system, where Electrooculography (EOG) and Electromyography (EMG) signals were decoded to perform reach-to-grasp movements. For that, five different eye movements (up, down, left, right and rest) were classified in real-time and translated into commands to steer an industrial robot (UR-10) to one of the four approximate target directions. Thereafter, EMG signals were decoded to perform the grasping task using an attached gripper to the UR-10 robot arm. The proposed system was tested offline on three different healthy subjects, and mean validation accuracy of 93.62% and 99.50% were obtained across the three subjects for EOG and EMG decoding, respectively. Furthermore, the system was successfully tested in real-time with one subject, and mean online accuracy of 91.66% and 100% were achieved for EOG and EMG decoding, respectively. Our results obtained by combining real-time decoding of EOG and EMG signals for robot control show overall the potential of this approach to develop powerful and less complex HRI systems. Overall, this work provides a proof-of-concept for successful real-time control of robot arms using EMG and EOG signals, paving the way for the development of more dexterous and human-controlled assistive devices.'\\n\\n'Overview of the real-time developed HRI system. EMG and EOG signals were decoded in real-time and the decoded information was translated intol commands to control a UR-10 robot in order to perform complex reach-to-grasp movements.'\\n\\n'Decoder'\\n\\n'In this work, we propose a human-robot interface based on EMG and EOG decoding to control reach-to-grasp movements of a UR-10 robot arm. Overall, our results show that we could successfully classify both signals with more than 90% accuracy across three different subjects and we underpin the real-time aspect by showing the successful online control of the robot arm. To the best of our knowledge, this experiment is among the very few, if not the only one, to demonstrate the real-time control of robot arms using decoded information from EMG and EOG signals. Nonetheless, the proposed system still has some limitations that should be addressed in future work, in order to investigate to what extent this approach could be generalized across many subjects, including elderly and disabled people, to further verify the robustness of such a system when used in real-life scenarios, as well as to extend the proposed approach to classify more complex reach-to-grasp movements.'\\n\\n'This paper proposes a human-robot interface system based on the real-time decoding of EOG and EMG signals to control reach-to-grasp movements using an industrial UR-10 robot arm. The system was tested with three different healthy subjects in the validation phase and one subject during the online phase. Overall, we obtained a mean validation accuracy of 93.62% and 99.50% for EOG and EMG classification, respectively, and online accuracy of 91.66% and 100% during the online phase. We demonstrate the real-time capability of the proposed system by showing successful control of the robot arm using decoded information from both signals.'\\n\\n'Source code and documentation'\\n\\n'All source code of this work is released under the MIT license and is made publicly available under https:\\/\\/github.com\\/spebern\\/eog-emg.'\\n\\n'EOG and EMG signals were recorded from three healthy subjects (two males and one female) using the g.USBamp system (G.tec medical engineering GMBH, Austria) with a sampling frequency of 1200 Hz. Subjects were between 22 and 27 years old and claimed to have no record of neurological disorders. The three subjects were recruited to perform four different eye movements (up, down, left, and right) by following a pointing arrow on the screen during the training phase (EOG task), as well as to perform one hand movement, namely power-fist followed by a relaxation phase. The pointing arrow (cue) was randomly displayed and was selected from a discrete uniform distribution. All recording sessions took place in the lab and yielded a total of 150 and 400 trials for EOG and EMG signals, respectively. For EOG signals, 30 trials for each direction were recorded (including the rest phase\\/no movement) forming the total number of 150 trials. For EMG, 200 trials for each condition (power-fist vs rest) were collected yielding to the total number of 400 trials. One pair of electrodes, which cover the muscles of the forearm (flexor and extensor carpi radialis) in a ring-like fashion were used, and the reference electrode was placed on the elbow bone. It should be noted that liquid-gel ECG electrodes (model: 5048 mm) were used for EMG recording. The open source library mushu [26] was used for data acquisition and both EMG and EOG signals were thereafter processed using the gumpy.signal module in the gumpy BCI toolbox [21]. For that, EOG signals were band-pass filtered between 1 and 22 Hz using a 4th order zero-phase Butterworth filter, whereas EMG was band-pass filtered between 30 and 500 Hz using the same filter. Both signals were notch filtered at 50 Hz to remove the power line interference [21]. The designed experimental paradigms, electrode placement and recorded signals for EOG and EMG are shown in Figure 2 and Figure 3, respectively.'\\n\\n'https:\\/\\/github.com\\/spebern\\/eog-emg.'\",\"378\":\"'We evaluate the performance of the proposed technique in estimating the wrist angle and torque based on surface EMGs of a pair of wrist flexor and extensor muscles. While several magnitudes of resistance are imposed on the wrist joint by a robot, participants are asked to move their wrist joint according to given trajectories. To overcome the resistance by the robot, the participants need to generate a sufficient magnitude of torque at the joint through contraction of the agonist muscles. As well, they are required to modulate joint stiffness to follow the given trajectory as accurately as possible through co-contraction of the agonist-antagonist muscles. By imposing different magnitudes of resistance, we assess the performance of the proposed technique to decode joint torque as well as joint angle, and accordingly, joint stiffness. To examine the efficacy of the LSTM neural network in learning time series with long-time spans using varying time lags for prediction of the angle and torque of the wrist joint, we present different trajectories with irregularity and aperiodicity to the participants for the training and evaluation phases of the decoding procedure, respectively. This effort would provide an insight into the predictability of unlearned motions by the proposed technique based on learned motions. A comparative study with a decoding method that possesses no device for time dependency of data illuminates the proposed method\\u2019s advantageous capability of learning a long time-span data for EMG-based decoding of kinematics and kinetics.'\\n\\n'A one degree-of-freedom robot for the wrist joint was developed. The motor (LS Mecapion, APM-SA01ACN-8) embedded an encoder of a resolution of 2048 pulses per revolution and was connected with a Harmonic Drive gear with a ratio of 50:1. A torque sensor (Transducer Techniques, TRT-200) was installed between the Harmonic Drive and the hand plate. A custom Advanced Motion Controls (AMC) drive was used to convert the motor command into the current to the motor.'\\n\\n'The architecture of the proposed decoding strategy. The proposed decoder consists of multiple steps between the input (EMG features) and output (angle and torque); EMG features go through the sequence input layer, LSTM neural network, fully-connected layer, dropout layer, again fully-connected layer, and regression layer in turn. Angle and torque estimations are achieved through this procedure.'\\n\\n'The EMG-based decoder was developed and performed utilizing Matlab Deep Learning Toolbox\\u2122 (MathWorks, Matlab 2018b) in a secondary personal computer (PC). EMG features calculated in the LabVIEW program were exchanged with estimated angle and torque produced in Matlab through UDP (User datagram protocol) communication.'\\n\\n'D. Decoder Development'\\n\\n'1) Decoder Architecture'\\n\\n'Fig. 2 depicts the architecture of the proposed decoder that estimated the profiles of joint angle and torque according to the features (RMS values and coefficients of the AR model) that were extracted from EMG signals with a time window. The profiles of joint angle and torque were averaged over the time window. The EMG features went through the sequence input layer, LSTM neural network, fully-connected layer, dropout layer, again fully-connected layer, and regression layer in turn, and then the decoder was trained with the profiles of joint angle and torque as the output. The sequence input layer transferred the sequence data (EMG features) to the LSTM network. The two fully-connected layers linked every output element in the previous step with every input element in the next step. The dropout layer set inpu t elements to zero randomly, with a probability of 0.5. The regression layer dealt with the regression problem between the actual and predicted values of angle and torque.'\\n\\n'The decoder was trained with EMG features as the input and with joint angle and torque data as the output acquired during the training phase for each stage. The performance of the proposed decoding strategy was evaluated by examining how accurately the joint angle and torque were predicted, respectively, by the trained decoder based on EMG signals during the evaluation phase. Respective variance accounted for (VAF) between the measured and predicted joint angle and between the measured joint torque and predicted joint torque was computed.'\\n\\n'For the ease of data analysis, we finished collecting all data first and simulated the real-time circumstance to evaluate the online performance of the decoding technique. For evaluation, the acquired EMG data during the evaluation phase were read and processed to the EMG features in the LabVIEW program. The EMG features were then transferred via UDP to the decoder that had been trained in the secondary PC with the data acquired during the training phase, and the decoder sent out the predicted values back to the LabVIEW program. A comparison between the actual and predicted values (angle and torque) was made then.'\\n\\n'Results (angle and torque) by the proposed decoder trained with reordered data sets of Subject 3 for CASE 3 with a chunking window of (b) 0.1 s, (c) 1 s, (d) 5 s, and (e) 15 s. (a) Results with the original data.'\\n\\n'The proposed decoding method employed a machine learning technique. LSTM neural networks are able to automatically determine the optimal time lags and learn the time series with long-time spans. The proposed method estimated participants\\u2019 wrist joint angle and torque with an excellent agreement, though the decoder did not take the advantage of overfitting by being trained with the similar trajectories used for evaluation. As observed in Fig. 5, the two trajectories for the training and evaluation phases, respectively, are highly irregular, aperiodic and different from each other. The results imply the strength of the proposed method that is based on what the decoder is trained using varying time lags in estimating the angle and torque with new inputs resulting from a series of different trajectories of motion [29]. This feature is advantageous in that unlearned trajectories of motion can be predicted from learned trajectories of motion. A more systematic analysis will be present in a follow-up study.'\\n\\n'EMG signals are of high time dependency. To examine the efficacy of the proposed decoding method that employed the LSTM in dealing with the time dependency of EMG signals, we trained the decoder with reordered data. The original data set used for training was chunked with several fixed time windows (0.1 s, 1 s, 5 s, and 15 s) and randomly reordered to train the decoder. Fig. 7 exhibits the original training data set as well as chunked and reordered data sets of Subject 3 for CASE 3. Fig. 8 shows decoding results of the decoders trained with each data set. We found that a smaller chunking window led to worse estimation accuracy, implying that as the chunking window is greater, the estimation accuracy approaches to the original one (Table 2). This suggests that while EMG features as the input to decoder are of time dependency, the proposed decoding strategy is capable of coping with the data with a long-time span. It comes more obvious through a comparative study with a decoder based on a linear regression model [23]. Overall, the linear regression method showed lower estimation accuracy, especially in angle estimation. We emphasize that nearly the same accuracy was produced regardless of how reordered the data set used for the decoder was, as seen in Table 3, indicating the lack of the capability in the regression-based method to deal with the time dependency of EMG signals.'\",\"379\":\"\\\"Electromyography (EMG) based interfaces have been used in various robotics studies ranging from teleoperation and telemanipulation applications to the EMG based control of prosthetic, assistive, or robotic rehabilitation devices. But most of these studies have focused on the decoding of user's motion or on the control of the robotic devices in the execution of simple tasks (e.g., grasping tasks). In this work, we present a learning scheme that employs High Density Electromyography (HD-EMG) sensors to decode a set of dexterous, in-hand manipulation motions (in the object space) based on the myoelectric activations of human forearm and hand muscles. To do that, the subjects were asked to perform roll, pitch, and yaw motions manipulating two different cubes. The first cube was designed to have a center of mass coinciding with the geometric center of the cube, while for the second cube the center of mass was shifted 14 mm to the right (off-centered design). Regarding the acquisition of the myoelectric data, custom HD-EMG electrode arrays were designed and fabricated. Using these arrays, a total of 89 EMG signals were extracted. The object motion decoding was formulated as a regression problem using the Random Forests (RF) technique and the muscle importances were studied using the inherent feature variables importance calculation procedure of the RF. The muscle importance results show that different subjects use different strategies to execute the same motions on same object when the weight is off-centered. Finally, the decoded motions were used to control a five fingered robotic hand in a proof-of-concept application.\\\"\\n\\n'Example of manipulation motions performed during the experiment. Subfigure A) shows the Pitch motion, subfigure B) shows the Roll motion, and subfigure C) shows the Yaw motion. The axes are color coded and the colored \\u2018Circles\\u2019 and the \\u2018X\\u2019 marks at the origins indicate that the axis is orthogonal (outward or inward) to the page. The thick white colored arrows show the direction of the motion about the out-of-plane axis.'\\n\\n'In order to demonstrate the EMG based control capabilities of the proposed motion decoding framework, the decoded object motion was used to derive appropriate motor trajectories for the NDX-A* adaptive robot hand (see Fig. 4), so as to execute the desired manipulation task. NDX-A* is an under-actuated, tendon-driven robot hand based on the original NDX hand [28] and it has been developed by the New Dexterity research group at the University of Auckland. The NDX-A* has 15 Degrees Of Freedom (DOF) and 6 actuators, one actuator per finger to control finger flexion and one for thumb opposition. The pitch motion was selected as it is a relatively simple motion that can be implemented even with under-actuated robot hands.'\\n\\n'As a demonstration of the proposed framework, pitch motion was decoded from myoelectric activations of a subject to derive appropriate motor trajectories for executing the motion on an adaptive robot hand (see Fig. 7). To map the object motion to hand motions, all feasible manipulation motions of the hand were experimentally derived and used to develop a regression model that acts as the inverse of the hand object system Jacobian. The Jacobian can be used to map object trajectories to robot hand motor trajectories. More details can be found in [38].'\\n\\n'The decoded dexterous manipulation motions (in the object space) are used to derive the required motor trajectories so as for a humanlike, tendon-driven robot hand to replicate the desired dexterous manipulation task (execute an object pitch motion). Subfigure A and subfigure B show the two end-points of the object motion trajectory.'\",\"380\":null,\"381\":\"'https:\\/\\/youtu.be\\/FHOsVVXBV74'\",\"382\":null,\"383\":null,\"384\":null,\"385\":null,\"386\":null,\"387\":null,\"388\":\"'The encoder-decoder architecture of the semantic segmentation networks. The size of input and output RGB image is 1280\\u00d7256.'\\n\\n'Upsampling using max-pooling indices in decoder network. The a, b, c, d correspond to values in a feature map.'\\n\\n'Each encoder convolves with a filter bank to produce a set of feature maps. Max-pooling is used to achieve translation invariance over small spatial shifts in the image, which combined with sub-sampling leads to each pixel governing a larger input image context (spatial window). These methods achieve better classification accuracy but reduce the feature map size, leading to a lossy image representation with blurred boundaries, which is not ideal for segmentation. It is desired that the output image resolution is the same as the input image. To achieve this, SegNet performs an upsampling in its decoder, which requires it to store some information. Thus, it is necessary to capture and store boundary information in the encoder feature maps before sub-sampling. To utilize space efficiently, SegNet stores only the max-pooling indices while the locations of maximum feature value in each pooling window is memorized for each encoder map. Only 2 bits are needed for each window of 2 \\u00d7 2, resulting in a slight loss of precision, as a trade-off.'\\n\\n'For each of the 13 encoders, there is a corresponding decoder which upsamples the feature map using memorized max-pooling indices. During upsampling, the max-pooling indices at the corresponding encoder layer are assembled to upsample, as shown in Fig. 3. These feature maps are then convolved with a decoder filter bank to provide dense feature maps. A batch normalization step is then applied to each of these maps. Finally, a K-class soft-max classifier is used to predict the class of each pixel where K is the number of classes.'\",\"389\":null,\"390\":\"'https:\\/\\/rasberryproject.com\\/'\",\"391\":null,\"392\":null,\"393\":null,\"394\":null,\"395\":\"'https:\\/\\/youtu.be\\/dNiR0z2dROg'\\n\\n'Autonomous driving in an unregulated urban crowd is an outstanding challenge, especially, in the presence of many aggressive, high-speed traffic participants. This paper presents SUMMIT, a high-fidelity simulator that facilitates the development and testing of crowd-driving algorithms. By leveraging the open-source OpenStreetMap map database and a heterogeneous multi-agent motion prediction model developed in our earlier work, SUMMIT simulates dense, unregulated urban traffic for heterogeneous agents at any worldwide locations that OpenStreetMap supports. SUMMIT is built as an extension of CARLA and inherits from it the physics and visual realism for autonomous driving simulation. SUMMIT supports a wide range of applications, including perception, vehicle control and planning, and end-to-end learning. We provide a context-aware planner together with benchmark scenarios and show that SUMMIT generates complex, realistic traffic behaviors in challenging crowd-driving settings.'\",\"396\":null,\"397\":\"'The pseudocode for finding a set of HDS is shown in Alg. 1. Initially, we compute the dependency depth for all contours in dependency graph D. The dependency depth is the number of edge transitions from root contours. For example, the dependency depth for contour i in Fig. 2a is 1 and 2 for all dependees. Given all contours at same depth, we enumerate each pair ij and find subgraphs H where the degree of connectivity is greater than \\u0393. For every adjacent subgraphs hi \\u2208 H and hj \\u2208 H, we compute the degree of connectivity. If the degree is greater than \\u0393, then this pair is merged. We repeat this process until no further merging is possible. We refer to the resulting dependency graph as the clustered dependency graph. Figure 2 shows how contours are clustered into three different subgraphs, where Fig. 2b shows the final form of the clustered dependency graph. In Fig. 3, we demonstrate a various models with HDS.'\",\"398\":null,\"399\":\"https:\\/\\/github.com\\/richardrl\\/rlkit-relational\",\"400\":null,\"401\":null,\"402\":\"'https:\\/\\/www.youtube.com\\/watch?v=74JrzXksx9o'\\n\\n'In many of the previously stated methods, optic flow is extracted from camera image streams, which requires additional processing. Similar to optic flow, depth scans also encode information about a vehicle\\u2019s orientation and lateral displacement relative to the center-line of a corridor. One benefit of using depth scans is that they can be measured directly using any number of COTS scanning lidar devices, such as the RPLIDAR A3\\/S1 used in this work. This paper departs from [25], [26], and [27] by applying the same WFI method used on optic flow signals to depth scans measured by laser scanners.'\\n\\n'Our original efforts at finding a reliable open-source lo-calization solution did not lead to viable for deployment. Finding a solution that relied on a minimal number of sensors, particularly sensors whose performance is unaffected by light, and had minimal dependence on state estimation became a priority after initial testing. In our solution only a single scanning depth sensor, the RPLIDAR A3, is needed for generating the low-level control commands for navigation between junctions or during exploration.'\\n\\n'Planning a path, like when using the open source package Btraj [30], requires a choice of a start and goal point as well as an environment to plan through. Choosing intelligent goal points and generating a continuous representation of the environment can be computationally expensive processes. If the environment in between the start and goal point is too complex, the solver may be unable to generate a viable trajectory within the limits of imposed constraints. Tuning constraints on a trajectory is a nontrivial process, and a solution is not guaranteed for every environment. The proposed control law has a very simple pipeline for quickly and efficiently generating control commands using solely the instantaneous raw sensor data. Tuning the steering controller gains for the proposed controller is a simple and intuitive process.'\",\"403\":null,\"404\":null,\"405\":\"'https:\\/\\/ori.ox.ac.uk\\/lidar-slam'\\n\\n'We discuss the performance of this algorithm, as well as its computational complexity in the experiment section of this paper. Pseudo code of the algorithm is available in Algorithm 1.'\",\"406\":null,\"407\":\"'To solve this, our method introduces a novel entropy regularization by jointly training two networks in an adversarial manner: an encoder network E and a discriminator D. Given two sequential frames (v, w), which are separated by a temporal stride \\u0394t, we define an unlabeled skill embedding x = (E(v), E(w)) and collection of skills\\\\nX={\\\\nx\\\\n1\\\\n,\\u2026,\\\\nx\\\\nN\\\\n}\\\\nrepresenting the different tasks. The encoder network embeds single frames of the dimension d1 \\u00d7 d2 into a lower-dimensional representation of size n, i.e.\\\\nE:\\\\nR\\\\nd\\\\n1\\\\n\\u00d7\\\\nd\\\\n2\\\\n\\u2192\\\\nR\\\\nn\\\\n. We compute Euclidean distances in the embedding space to compare the similarity of frames. The discriminator network takes two concatenated embedded frames that define an unlabeled skill x as input and outputs yc, the probability of the skill being originated from task c. Formally, we require D(x) \\u2208 \\u211dC to give rise to a conditional distribution over tasks\\\\n\\u2211\\\\nC\\\\nc=1\\\\np(\\\\ny\\\\nc\\\\n=c|x,D)=1\\\\n. Although we define this hyper-parameter a priori as the number of tasks contained in a training set, we observed minor performance drops setting it to a value with small deviation from the true number of tasks. Most importantly, ASN does not need a task label for the demonstration videos.'\\n\\n'The encoder parameters are updated using a metric learning loss and maximization of the entropy of the discriminator output. In order to capture the temporal task information, we use a modified version of the lifted structure loss [29]. Given two view-pairs (v1, v2), synchronized videos from different perspectives, we attract frames that represent the same temporal task state and repulse temporal neighbors, given a constant margin \\u03bb, i.e.'\\n\\n'This leads to the following objectives for the encoder E and discriminator D:'\\n\\n'We therefore optimize the discriminator and the encoder according to:'\\n\\n'The encoder network is inspired by Time-Contrastive Networks (TCN) [6]. We use an Inception network as a feature extractor [31], which is initialized with ImageNet pre-trained weights. The feature extractor is followed by two convolutional layers and a spatial softmax layer for dimension reduction. Finally, after a Fully Connected (FC) layer, the model outputs the embedding vector for a frame. For all experiments, we use \\u03b1 = 0.1, \\u03b2 = 1.0, \\u03bb = 1.0 and an embedding size of 32. The discriminator consists of two FC layers to estimate \\u00b5 and \\u03c3 of a Gaussian distribution, followed by two layers to output the task ID. We use dropout for regularization.'\\n\\n'We train the encoder and discriminator networks with the Adam optimizer and a learning rate of 0.001. A training batch contains 32 frames from n = 4 different view pairs. We load real-world data from video files and sample the simulated data from uncompressed image files. Training directly on images, ensures that our model is not learning any bias introduced by video compression techniques. For frames from the training set, we randomly change brightness, contrast and saturation and randomly mirror frames horizontally. For real-world data, additional training frames are cropped randomly. We train on images of the size 299\\u00d7299 3 pixels. For simulated data the discriminator network is only\\u00d7 updated with successful task demonstrations, since only they contain the skills we want to transfer. After data augmentation, the frames of a batch are normalized on each RGB channel using the \\u00b5 and \\u03c3 of the ImageNet dataset.'\\n\\n'Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http:\\/\\/robotskills.cs.uni-freiburg.de.'\",\"408\":null,\"409\":\"'Depending on the availability of data (monocular\\/stereo videos, ground truth depths), different depth models can be trained. Throughout the work, we use a standard fully convolutional encoder-decoder network with skip-connections [38] [39] to predict depths.'\\n\\n'In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO\\/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue. Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system. Code is available at here: DF-VO.'\",\"410\":null,\"411\":null,\"412\":null,\"413\":null,\"414\":null,\"415\":null,\"416\":\"'https:\\/\\/deepsemantichppc.github.io\\/'\\n\\n'https:\\/\\/deepsemantichppc.github.io'\",\"417\":\"'https:\\/\\/lean.mit.edu\\/projects\\/CEIMP'\",\"418\":\"'https:\\/\\/arxiv.org\\/abs\\/2002.11853'\",\"419\":\"'https:\\/\\/youtu.be\\/hrr2BzPLaMg)'\",\"420\":null,\"421\":\"'https:\\/\\/github.com\\/jacqu\\/teensyshot'\\n\\n'https:\\/\\/youtu.be\\/Nbs62fteUgs'\\n\\n'The velocity of the propellers is controlled externally with an anti-windup PID digital loop. A single MCU (PJRC Teensy 3.5) is controlling the velocity of 6 ESCs. So two MCUs are required to control the 12 motors of the AWG. Recent ESC firmware (KISS and BLHeli for example) can send telemetry data on a serial link that contains among other the velocity of the electromagnetic field. Since synchronous motors are used, this velocity is proportional to the rotor mechanical velocity. Our custom code running on the MCU can robustly retrieve this measurement at a frequency of 500 Hz, calculate a throttle value and send it to the ESC using the DSHOT600 digital protocol. The 6 control loops are running synchronously on one MCU thanks to a Direct Memory Access (DMA) mechanism used to generate the DSHOT signal. The ESCs are configured to work in a bidirectional mode (called 3D mode). In this mode the inverter within the ESC works in a 4-quadrant mode. We tuned the PID to obtain a step response shaped like a first-order system with a 60 ms time constant. Since this response time is many orders of magnitude faster than the mechanical oscillation modes of the AWG, we consider that it is negligible in the remainder of this paper.'\\n\\n'Each propulsion unit is made of a pair of coaxial contrarotating propellers (Graupner 3D 6\\\") in order to cancel unwanted reaction torque and gyroscopic effects. Each propeller is driven by a brushless DC motor (T-Motor F40 Pro II Kv2600) which is connected to an Electronic Speed Controller (ESC, KISS 32A). Since the thrust is directly linked to the squared rotational velocity of the propeller, a speed regulation is needed for the torque control of the robot. However, to the best of the authors\\u2019 knowledge, to this day, the only ESCs offering speed regulation are designed for helicopters and they lack responsiveness. The open-source Teensyshot firmware (https:\\/\\/github.com\\/jacqu\\/teensyshot) is developed in our lab for the project and implements an outer fast PID speed regulation loop using real-time ESC telemetry data. This regulation is running on an external micro-controller (MCU, Teensy 3.5) at a sampling rate of 500 Hz acquiring telemetry data through a 115 200 bps serial link and sending ESC throttle control input data using the DSHOT600 protocol.'\\n\\n'The AWG is fully autonomous: it carries its own energy source, a 1550mAh, 11.1V lithium polymer battery pack (TATTU 3S1P). It also has an on-board CPU (Raspberry Pi 4B) running high-level control algorithms and communicating with a ground station through Wi-Fi TCP\\/IP sockets thanks to the open-source Simulink toolbox RPIt [13] developed in our lab. The Raspberry Pi is connected by USB to 2 MCUs regulating the velocity of a total of 12 motors.'\\n\\n'https:\\/\\/github.com\\/jacqu\\/teensyshot'\",\"422\":\"'The dualcopter and disk are both equipped with CUI AMT22 Modular Absolute Encoders4 that measure the ground truth roll angle. The encoders have an accuracy of 0.2\\u00b0 and a precision of 0.1\\u00b0. The encoder transfers angle measurements to the UP board via Serial Peripheral Interface (SPI) with a maximum data transmission rate of almost 20 kHz. In this project we used an update rate of 1 kHz.'\\n\\n'Bode plot of encoder-driven PD controller with gains kp = 0.705 and kd = 0.024. The data points are shown in blue. The black lines show the best-fit third-order transfer function, a result of the second-order control plant with the effects of first-order motor dynamics observed as well.'\\n\\n'We evaluated the performance of the vision-driven controller by comparing it to the encoder-driven controller, the best performing controller on the current platform. The vision-driven controller uses the event-based state estimator to determine the roll angle and angular velocity relative to the horizon marked by the disk, which is stationary here, while the encoder-driven controller uses the angular measurements from the encoder as feedback to achieve the commanded angle. The state update rate is 1 kHz for both controllers, and both use a PD attitude controller with the same gains as in (6).'\\n\\n'Bode plot showing the frequency response of the final encoder-driven controller (kp = 0.705, kd = 0.141). The data points are shown in blue. The black lines show the best-fit third-order transfer function with a delay determined to be 5.9 ms.'\\n\\n'In the future, we intend to use the dualcopter platform to explore event-based control, in which events directly encode control signals, thereby removing the state estimation step altogether. Such algorithms have been investigated in [11] and [12], and they have yet to be tested on a physical platform. The experimental setup developed in this work provides a platform to which various event-based control algorithms can be adapted for testing.'\",\"423\":null,\"424\":null,\"425\":\"'https:\\/\\/wenhao.pub\\/publication\\/rare-supplement.pdf'\\n\\n'The proposed CMTS consists of three modules: a sequential trajectory encoder (red), a conditional map encoder (green), and a decoder that generates trajectory (blue). See supplementary material for the dimensions of the layers.'\\n\\n'A. Variational Autoencoder'\\n\\n'We use linear interpolation during the training stage to obtain the composed feature. We use VAE as our basic model because its prior distribution of the latent code z is a multivariate Gaussian distribution, which is naturally disentangled. Assuming the latent code of the safe data and the collision data are zs and zc respectively, the linear interpolation in latent space becomes:'\\n\\n'Next, we measure the reconstruction error of composed latent code zf. For zs and zc, we directly calculate the difference of x and q\\u03d5(x|z). Since there is no reference for zf in the dataset. To solve this problem, we propose a consistent regularization from the mutual information view that is similar to [51]. We ensure the reconstruction of zf by building a unique mapping from xf to zf with the encoder p\\u03b8(x|z). From the view of mutual information, We note there is a unique mapping from x to z if and only if the entropy H(z|x) = 0. However, it is intractable to access all data pairs to calculate the entropy, so we use variational inference to obtain the upper bound of H(z|x):'\\n\\n'We encode each dataset into a latent space and use the low-dimensional codes to train four non-parametric Bayesian models. We set the concentrating parameter \\u03b1 to 1 for all models and use the output cluster number K as an indicator of the complexity of the datasets. Table. II shows that the dataset generated by CMTS has larger numbers of clusters than OD and CD. The combination of the CMTS dataset and OD has the highest number of clusters among all datasets, which means the CMTS dataset contains clusters unseen in OD and CD, otherwise, CMTS+OD will have similar K as OD.'\",\"426\":null,\"427\":\"'In this paper, we propose Graph-Q and DeepScene-Q, interaction-aware reinforcement learning algorithms that can deal with variable input sizes and multiple object types in the problem of high-level decision making for autonomous driving. We showed, that interaction-aware neural networks, and among them especially GCNs, can boost the performance in dense traffic situations. The Deep Scene architecture overcomes the limitation of fixed-sized inputs and can deal with multiple object types by projecting them into the same encoded object space. The ability of dealing with objects of different types is necessary especially in urban environments. In the future, this approach could be extended by devising algorithms that adapt the graph structure of GCNs dynamically to adapt to the current traffic conditions. Based on our results, it would be promising to omit graph edges in light traffic, essentially falling back to the Deep Sets approach, while it is beneficial to model more interactions with increasing traffic density.'\\n\\n'We use the open-source SUMO [30] traffic simulation to learn lane-change maneuvers. All agents are trained off-policy on datasets collected by a rule-based agent with enabled SUMO safety module integrated, performing random lane changes to the left or right whenever possible.'\",\"428\":\"'Recently, the trajectory prediction problem has been addressed as a sequence-to-sequence problem based on an encoder-decoder architecture [12], [13], [14], [15], adding the surrounding cars in the input sequence to encode interaction. However, it is difficult to consider all the surrounding cars in the decoder as this is designed only to learn future movement from the encoded features. Therefore, the interaction predicted in the future horizon may not be fully exploited.'\\n\\n'We compare several existing models (defined below) with our proposed model, either recoding (CV,V-LSTM), downloading public code (CS-LSTM), or using published results (D-LSTM).'\\n\\n'TABLE I: RMSE comparison on NGSIM dataset [20] between CV, V-LSTM, Dual-LSTM, CS-LSTM and the proposed technique at different time horizons. The Dual-LSTM results are from the original paper [25] as the code or model is not publically available.'\\n\\n'Within the US driving code, the overtake maneuver should only be performed using the left vacant lane; in the case when the left lane is occupied the driver should wait until it is clear. In addition, once the overtake maneuver is completed the vehicle should cut back in to the rightmost vacant lane. Contradictory trajectories, overtaking using the right lane and not pulling back into the right lane after successful overtaking, are shown in Fig. 9(c) and Fig. 9(d) respectively. Fig. 9(c) shows the violation of the first code, where vehicle 469\\u2019s ego lane and both the left lanes are congested. Due to the availability of a significant gap in the rightmost lane, our model predicted a dangerous accelerated right lane trajectory as opposed to the decelerated left lane change ground truth trajectory. Fig. 9(d) shows the violation of the second code, where vehicle 65\\u2019s ego lane is empty and there is no need for an immediate lane change in terms of lane congestion. Our model predicted a follow lane trajectory as opposed to the right lane change (ground truth) trajectory, pulling back into the rightmost vacant lane. We think the reason behind these violations is missing traffic code information in the model. In future, we will consider adding rule-based layers in our model to handle these types of case.'\\n\\n'In this paper, we propose a novel Conv-LSTM based architecture and an interactive feedback based prediction scheme to forecast the future trajectory of traffic participants from the current scene. Several experiments show that our proposed method can achieve comparable prediction accuracy with the state-of-the-art models during the long term prediction horizon, even without using maneuver information. Further work will be carried out to remove that underlying accumulated error to make the prediction more accurate and investigate how to benefit from driving code rules and road topology.'\",\"429\":null,\"430\":\"'We give the pseudo-code of our eigenvalue algorithm in Algorithm 1. At each time-step t, we compute k eigenvectors of \\u2112t. For each eigenvector, we perform an iterative process. We begin by initializing a random vector. Next, we iteratively perform the following update rule until it converges to an eigenvector. For the jth eigenvector, the update rule is given as:'\\n\\n'We evaluate the performance of our algorithm on two open-source datasets, TRAF and Argoverse, described in Section V-A. In Section V-B, we list other classification algorithms that are used to compare GraphRQI using standard classification evaluation metrics. We report our results and analysis in Section V-C. We perform exhaustive ablation experiments to motivate the advantages of of GraphRQI in Section V-D. Finally, in Section V-E we show how driver behavior knowledge can be useful and applied to road-agent trajectory prediction.'\\n\\n'We present a novel algorithm, GraphRQI, that can classify driver behaviors of road-agents from their trajectories. Our classification of behaviors is based on prior studies in traffic psychology, and our approach is designed for trajectory data from videos or other visual sensors. We compute the traffic matrices from the road-agent positions and use an eigenvalue algorithm to compute the spectrum of the Laplacian matrix. The spectrum comprises of the features used to train a multilayer perceptron for driver behavior classification. We also present a faster algorithm to compute the eigenvalue and demonstrate the performance of our algorithm on two open-source traffic datasets. We observe 25% improvement in classification over prior methods.'\",\"431\":\"'This paper presents a system for robust, large-scale topological localisation using Frequency-Modulated Continuous-Wave scanning radar which extends the state-of-the-art by an efficient, learning-based approach to handle radar data for localisation. We learn a metric space for embedding polar radar scans using CNN and NetVLAD architectures traditionally applied to the visual domain. However, we tailor the feature extraction for more suitability to the polar nature of radar scan formation using cylindrical convolutions, anti-aliasing blurring, and azimuth-wise max-pooling; all in order to bolster the rotational invariance. The enforced metric space is then used to encode a reference trajectory, serving as a map, which is queried for nearest neighbour for recognition of places at run-time. We demonstrate the performance of our topological localisation system over the course of many repeat forays using the largest radar-focused mobile autonomy dataset released to date, totalling 280 km of urban driving, a small portion of which we also use to learn the weights of the modified architecture. As this work represents a novel application for radar, we analyse the utility of the proposed method via a comprehensive set of metrics which provide insight into the efficacy when used in a realistic system, showing improved performance over the root architecture even in the face of random rotational perturbation.'\\n\\n'Generation of these embeddings is delegated to an encoder network (c.f. Section V) which extracts information from the radar measurements and compresses them within the multidimensional space. The training procedure enforces that the network will learn this transformation.'\\n\\n'Because of the geographical scale of the environment which must be encoded for representation (large urban centres), exploitation of common data structure techniques to discretise this space for fast lookups is essential to reduce the NN search complexity. In the results discussed in Section VII, a k-dimensional tree [21] structure is used. The kd-tree is built with a number of nodes equal to the number of radar scans along the trajectory although it is possible to decimate the radar stream (e.g. by distance travelled or time elapsed). This choice guarantees the exactness of the search result, thus not influencing our discussion of the accuracy of the learned representation. Other CPU- or GPU-based methods allow for faster, although approximate, searches [22], [23].'\\n\\n'We then apply the learned metric space to encode an entire trajectory from the dataset (c.f. Section VI-C), including data from all splits (train, valid, and test). This encoded trajectory is used as a static map along which all other trajectories in the dataset are localised against. We exclude the pair of trajectories which we use to train the network. Figure 4 shows average PR curves with one standard-deviation bounds. The corresponding AUC are 0.75 \\u00b1 0.06 for OURS as compared to 0.72 \\u00b1 0.05 for VGG-16\\/NETVLAD. This experiment serves to indicate that our proposed modifications result in measurable performance improvements over the baseline system.'\\n\\n'Convolutional Autoencoder Based Feature Extraction in Radar Data Analysis'\",\"432\":null,\"433\":null,\"434\":\"'https:\\/\\/github.com\\/RozDavid\\/LOL'\\n\\n'https:\\/\\/youtu.be\\/ektGb5SQGRM'\\n\\n'The source code of the proposed algorithm is publicly available at: https:\\/\\/github.com\\/RozDavid\\/LOL A video demonstration is available at: https:\\/\\/youtu.be\\/ektGb5SQGRM'\\n\\n'Our main contribution is the integration and adaptation of the LOAM and SegMap algorithms into a novel solution, creating thus a Lidar-only Odometry and Localization (LOL) method that eliminates the need of any other supplementary sensor, e.g., IMU, wheel encoder, and satellite-based Global Positioning System (GPS). Furthermore, we included some additional improvements in the acceptance of correct matches, by applying further geometrical constraints complementing the feature similarity ones. Namely, we apply a RANSAC based geometric verification, and once a good match is detected between the online measurements end the target map, we only search for similar 3D Lidar segments (with relaxed similarity constraints) in the neighbourhood of the current location defined by the location uncertainty. Also, we only use the shift between the target map and the online source segments centroids as a prior, and we refine the final transformation by applying a fine-grained ICP matching between the two point clouds. We tested the proposed algorithm on several Kitti datasets, c.f. Fig. 2 bottom row, and found a considerable improvement in term of precision without a significant computational cost increase.'\\n\\n'We publicly release the source code of the proposed system.'\\n\\n'https:\\/\\/github.com\\/RozDavid\\/LOL'\",\"435\":\"'https:\\/\\/youtu.be\\/5wQXrpzxHNk'\",\"436\":null,\"437\":null,\"438\":\"'An alternative approach is to encode the motion into a state-dependent DS. This idea is exploited by the stable estimator of dynamical systems (SEDS) in [3], where the parameters of a Gaussian mixture regression (GMR) are constrained to ensure global stability. However, SEDS exploits quadratic stability constraints that limit the reproduction accuracy. Researchers in the field have realized that, in some cases, accuracy and stability are conflicting objectives to achieve. This is known as the accuracy vs stability dilemma [5] and several approaches have been proposed to improve the accuracy while preserving the stability.'\",\"439\":\"'We further break the high-level mechanism into 2 parts. The first part is a conditional Variational Autoencoder (cVAE) [19] that tries to model the full distribution of states p(st+T|st) that are T timesteps away from a given state st. It is used to sample a set of goal proposals. The second part is a value function V(s) that is used to select the most promising goal proposal. The low-level controller is a Recurrent Neural Network (RNN) that outputs an action at at each timestep, given a current observation st and goal sg.'\",\"440\":null,\"441\":null,\"442\":\"'This paper focuses on inverse reinforcement learning (IRL) to enable safe and efficient autonomous navigation in unknown partially observable environments. The objective is to infer a cost function that explains expert-demonstrated navigation behavior while relying only on the observations and state-control trajectory used by the expert. We develop a cost function representation composed of two parts: a probabilistic occupancy encoder, with recurrent dependence on the observation sequence, and a cost encoder, defined over the occupancy features. The representation parameters are optimized by differentiating the error between demonstrated controls and a control policy computed from the cost encoder. Such differentiation is typically computed by dynamic programming through the value function over the whole state space. We observe that this is inefficient in large partially observable environments because most states are unexplored. Instead, we rely on a closed-form subgradient of the cost-to-go obtained only over a subset of promising states via an efficient motion-planning algorithm such as A* or RRT. Our experiments show that our model exceeds the accuracy of baseline IRL algorithms in robot navigation tasks, while substantially improving the efficiency of training and test-time inference.'\\n\\n'Practical applications of autonomous robot systems increasingly require operation in unstructured, partially observed, unknown, and changing environments. Achieving safe and robust navigation in such conditions is directly coupled with the quality of the environment representation and the cost function specifying desirable robot behavior. Designing a cost function that accurately encodes safety, liveness, and efficiency requirements of navigation tasks is a major challenge. In contrast, it is significantly easier to obtain demonstrations of desirable behavior. The field of inverse reinforcement learning [1]\\u2013[3] (IRL) provides numerous tools for learning cost functions from expert demonstration.'\\n\\n'map encoder'\\n\\n'cost encoder'\\n\\n'cost encoder.'\\n\\n'The map encoder incrementally updates a hidden state ht using the most recent observation zt obtained from robot state xt. For example, a Bayes filter with likelihood function parameterized by \\u03c8 can convert the sequential input (x1:t,z1:t) into a fixed-sized hidden state ht+1:'\\n\\n'The cost encoder uses the latent environment map ht to define the cost function estimate ct(x,u) at a given state-control pair (x,u). A convolutional neural network (CNN) [21] with parameters \\u03d5 can extract cost features from the environment map:'\\n\\n'A. Map Encoder'\\n\\n'We encode the occupancy probability of different environment areas into a hidden state ht. In detail, we discretize X into N cells and let m\\u2217 \\u2208 {\\u22121,1}N be the vector of true occupancy values over the cells. Since m\\u2217 is unknown to the robot, we maintain the occupancy likelihood\\\\nP(\\\\nm\\\\n\\u2217\\\\n=1\\u2223\\\\nx\\\\n1:t\\\\n,\\\\nz\\\\n1:t\\\\n)\\\\ngiven the history of states x1:t and observations z1:t. See Fig. 3 for an example of a depth measurement zt and associated occupancy likelihood over the map m\\u2217. The representation complexity may be simplified significantly if one assumes independence among the map cells\\\\nm\\\\n\\u2217\\\\nj\\\\n:'\\n\\n'In summary, the map encoder starts with prior occupancy log-odds h0, updates them recurrently via:'\\n\\n'B. Cost Encoder'\\n\\n'Ours-HCE stands for hard-coded cost encoder. This simple variant of our model uses Eqn. (16) with s = 1 and l = 100 set explicitly as constants.'\\n\\n'Ours-SCE stands for soft-coded cost encoder and has s and l in Eqn (16) as learnable parameters.'\\n\\n'Ours-CNN is our most generic variant using a convolutional neural network as cost encoder. The network architecture is the same as in DeepMaxEnt for fair comparison.'\\n\\n'We proposed an inverse reinforcement learning approach for infering navigation costs from demonstration in partially observable enviroments. Our model introduces a new cost representation composed of a probabilistic occupancy encoder and a cost encoder defined over the occupancy features. We showed that a motion planning algorithm can compute optimal cost-to-go values over the cost representation, while the cost-to-go (sub)gradient may be obtained in closed-form. Our work offers a promising model for encoding occupancy features in navigation tasks and may enable efficient online learning in challenging operational conditions.'\",\"443\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/33692911'\",\"444\":null,\"445\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34123481'\",\"446\":null,\"447\":null,\"448\":null,\"449\":\"'On the boundary of domain D we impose discrete-time dynamics to encode the perfectly inelastic impact dynamics as toe0 and toe3 impact the ground (and suppressing the dependence of D and J* on q and\\\\nq\\\\n\\u02d9\\\\n):'\\n\\n'Without sacrificing the model fidelity of the full-body dynamics of the quadruped, the ability to exactly decompose these dynamics into equivalent bipedal robots makes it possible to rapidly generate gaits that leverage the fullorder dynamics of the quadruped. Importantly, this allows for the rapid iteration of different gaits necessary for bringing quadrupeds into real-world environments. Moreover, the fact that these gaits can be generated on the order of seconds suggests that with code optimization on-board and realtime gait generation may be possible soon. The goal is to ultimately use this method to realize a variety of different dynamic locomotion behaviors on quadrupedal robots.'\",\"450\":\"'Although open source packages are used for sensor drivers1 and non-posture components of locomotion control2 \\u2013 including a gait engine (generating a sinusoidal tripod gait) and an inverse kinematics (IK) engine \\u2013 the posture control systems designed in this study are implemented in custom C++ software developed to run under the Robot Operating System (ROS).'\",\"451\":null,\"452\":\"'So far, we have finished addressing the derivation of our control strategy, the pseudo code is provided in Algorithm 1. In this section, simulations of teams with 100 robots are conducted to validate the proposed control scheme (8) with the convex coverage control law (3). Under the control strategy, the distribution of the robots in the real environment is shown in Fig. 2i, and the coverage visualization in the transformed convex region is shown in Fig. 2ii. The simulation is tested under four scenarios: control law with both feedforward term\\\\n\\u2202\\\\nc\\\\n~\\\\n\\u2202t\\\\nand\\\\n\\u2202\\u03c6\\\\n\\u2202t\\\\n, control law with only one of the feedforward terms\\\\n\\u2202\\\\nc\\\\n~\\\\n\\u2202t\\\\nor\\\\n\\u2202\\u03c6\\\\n\\u2202t\\\\n, and control law without any feedforward term.'\",\"453\":\"'We ran simulations with two large-scale public transit networks in San Francisco (SFMTA) and the Washington Metropolitan Area (WMATA). We used the open-source General Transit Feed Specification data [1] for each network. We considered only the bus network (by far the most extensive), but our formulation can accommodate multiple modes. We defined a geographical bounding box in each case, of area 150 km2 for SFMTA and 400 km2 for WMATA (illustrated in the appendix), within which depots and package locations were randomly generated. For the transit network, we considered all bus trips that operate within the bounding box. The size of the time-expanded network, |VTN|, is the total number of stops made by all trips; |VTN | = 4192 for SFMTA and |VTN | = 7608 for WMATA (recall that edges are implicit, so |ETN | varies between queries, but the full graph GO can be dense). The drone\\u2019s flight range constraint is set (conservatively) to 7 km and average speed to 25 kph, based on the DJI Mavic 2 specifications [14]. In this section, we evaluate the two main components \\u2014 the task allocation and multi-agent path finding layers. In the appendix we compare the performance of two replanning strategies for when a drone finishes its current delivery, and two surrogate travel time estimates for coupling the layers.'\",\"454\":null,\"455\":null,\"456\":null,\"457\":null,\"458\":null,\"459\":null,\"460\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/33133751'\",\"461\":\"'https:\\/\\/www.youtube.com\\/watch?v=02dhUfTJNK4'\",\"462\":\"'Precision cutting of soft-tissue remains a challenging problem in robotics, due to the complex and unpredictable mechanical behaviour of tissue under manipulation. Here, we consider the challenge of cutting along the boundary between two soft mediums, a problem that is made extremely difficult due to visibility constraints, which means that the precise location of the cutting trajectory is typically unknown. This paper introduces a novel strategy to address this task, using a binary medium classifier trained using joint torque measurements, and a closed loop control law that relies on an error signal compactly encoded in the decision boundary of the classifier. We illustrate this on a grapefruit cutting task, successfully modulating a nominal trajectory t using dynamic movement primitives to follow the boundary between grapefruit pulp and peel using torque based medium classification. Results show that this control strategy is successful in 72 % of attempts in contrast to control using a nominal trajectory, which only succeeds in 50 % of attempts.'\\n\\n'We address the challenge above using a learning strategy where the desired operational region is compactly encoded in the decision boundary of a binary medium classifier. Here, the estimated likelihood of sensor readings associated with either medium guides the movement execution in the form of online trajectory correction. In summary, we 1) use the DMP framework to encode a nominal scooping trajectory, 2) learn probabilistic classification of sensor readings associated with operation in either mediums, and 3) construct a control scheme that corrects the nominal scooping trajectory according to the estimated posterior distribution over either medium, as illustrated in Fig. 3.'\",\"463\":\"'The demonstrations for all the tasks are generated by a hard-coded python script. The actuator follows points in the cloth reference frame that are pre-selected by a human in just one instance of the tasks. A uniform Gaussian noise of 10% is added to introduce randomness in the demonstration data. The generated demonstrations are imperfect and we use 20 episodes of demonstration data to train our agent.'\\n\\n'In order to assess the importance of task-relevant motion features, we introduce randomization in the speed and the trajectory of manipulator while keeping initial and final positions intact in our hard-coded demonstration generation script. In Table I we report success rates under different randomization for all three tasks averaged over 3 epochs of 100 episodes each.'\",\"464\":\"'Our simulation environments and the code used in this work is publicly available on our website [15].'\\n\\n'RLBC algorithm. The pseudo-code for the proposed approach is shown in Algorithm 1. The algorithm can be divided into three main steps. First, we collect a dataset of expert trajectories\\\\nD\\\\nk\\\\nfor each skill policy\\\\nk\\\\nk\\\\ns\\\\n. For each policy, we use an expert script that has an access to the full state of the environment. Next, we train a set of skill policies\\\\n{\\\\n\\u03c0\\\\n1\\\\ns\\\\n,\\u2026,\\\\n\\u03c0\\\\nK\\\\ns\\\\n}\\\\n. We sample a batch of state-action pairs and update parameters of convolutional layers \\u03b8 and the skills linear layer parameters \\u03b7. Finally, we learn the master \\u03c0m using the pretrained skill policies and the frozen parameters \\u03b8. We collect episode rollouts by first choosing a skill policy with the master and then applying the selected skill to the environment for n time-steps. We update the master policy weights \\u00b5 to maximize the expected sum of rewards.'\",\"465\":\"'We also reproduce some state-of-the-art methods and conduct experiments in the same setting to evaluate the grasping success rate. We use the open-source code provided by the paper authors to retrain the model on the same dataset with the original paper. In particular, Morrison et al. [16] proposes an open-loop method for static grasping and a closed-loop method for dynamic grasping. We reproduce its open-loop method with a stationary top-down camera in order to compare with other methods in the same setting. We use depth images in experiments for [16] to ensure the model we tested has the same performance with the original paper. The results are shown in Table. II. It indicates that our approach outperforms current state-of-the-art methods without calibration and using only RGB image. We also compare our inference time with [4] and [16]. It takes us 6 ms to predict an affordance map while 219 ms for [4] and 11 ms for [16], which means our method is much faster.'\",\"466\":null,\"467\":\"'In this paper, we present an open platform, termed OpenVINS, for visual-inertial estimation research for both the academic community and practitioners from industry. The open sourced codebase provides a foundation for researchers and engineers to quickly start developing new capabilities for their visual-inertial systems. This codebase has out of the box support for commonly desired visual-inertial estimation features, which include: (i) on-manifold sliding window Kalman filter, (ii) online camera intrinsic and extrinsic calibration, (iii) camera to inertial sensor time offset calibration, (iv) SLAM landmarks with different representations and consistent First-Estimates Jacobian (FEJ) treatments, (v) modular type system for state management, (vi) extendable visual-inertial system simulator, and (vii) extensive toolbox for algorithm evaluation. Moreover, we have also focused on detailed documentation and theoretical derivations to support rapid development and research, which are greatly lacked in the current open sourced algorithms. Finally, we perform comprehensive validation of the proposed OpenVINS against state-of-the-art open sourced algorithms, showing its competing estimation performance.'\\n\\n'Developing a working VINS algorithm from scratch has proven to be challenging, and in the robotics research community, this has shown to be a significant hurdle for researchers due to the lack of VINS codebases that have comprehensive documentation and detailed derivations for which even users with little background can learn and extend a current state-of-the-art work to address their problems at hand. While there are several open sourced visual-inertial codebases [2]\\u2013[8], they are not developed for extensibility and lack proper documentation and evaluation tools, which, in our experience, are crucial for rapid development and deep understanding, thus accelerating VINS research and development in the field. Moreover, these systems have many hard-coded assumptions or features that require an intricate understanding of the codebases in order to adapt them to the sensor systems at hand. This, along with inadequate documentation and support, limits their wide adoption in different applications.'\\n\\n'To fill the aforementioned void in the community and to promote the VINS research in robotics and beyond, in this paper, we present an extendable, open sourced codebase that is particularly designed for researchers and practitioners with either limited or extensive background knowledge of state estimation. We provide the necessary documentation, tools, and theory for those who are even new to visual-inertial estimation, and term this collection of utilities as OpenVINS (OV). This codebase has been the foundation of many of the recent visual-inertial estimation projects in our group at the University of Delaware, which include multi-camera [9], multi-IMU [10], visual-inertial moving object tracking [11], [12], Schmidt-based visual-inertial SLAM [13], [14], pointplane and point-line visual-inertial navigation [15], [16], among others [17]\\u2013[19]. We summarize the key functionality of the different components in OpenVINS as follows:'\\n\\n'It is our belief that the documentation of this work in itself is one of the main contributions to the research community. Both researchers and practitioners with little background in estimation may struggle to grasp the core theoretical concepts and important implementation details when it comes to visual-inertial estimation algorithms. To bridge this gap the documentation of this codebase takes as much of a priority as new features that could improve the estimation performance. As compared to existing open sourced systems with limited documentation, we focus on providing additional dedicated derivation pages on how different parts of the code are derived and interact. The in-code and page documentation is automatically generated from the codebase using Doxygen [36] which is then post-processed using m.css [37] to provide high quality search functionality and mobile friendly layout. This tight-coupling of our documentation and derivations within the codebase also ensures that the documentation is up to date and that developers can easily find answers.'\\n\\n'In what follows we describe our generalized modular on-manifold EKF-based estimator which, in its simplest form, estimates the current state of a camera-IMU pair. We then introduce the implemented features that provide the foundation for researchers to quickly build and extend on. Note that what we present here is only a brief introduction to the feature set and readers are referred to our thorough documentation website. We also provide an evaluation of the proposed EKF-based solution in simulations and then on real-world datasets, clearly demonstrating its competing performance against other open sourced algorithms.'\\n\\n'E. Codebase Documentation'\",\"468\":null,\"469\":null,\"470\":\"'In this work, we propose a new estimation method for second-order kinematics for floating-base robots, based on highly redundant distributed inertial feedback. The linear acceleration of each robot link is measured at multiple points using a multimodal, self-configuring and self-calibrating artificial skin. The proposed algorithm is two-fold: i) the skin acceleration data is fused at the link level for state dimensionality reduction; ii) the estimated values are then fused limb-wise with data from the joint encoders and the main inertial measurement unit (IMU), using a Sigma-point Kalman filter. In this manner, it is possible to estimate the joint velocities and accelerations while avoiding the lag and noise amplification phenomena associated with conventional numerical derivation approaches. Experiments performed on the right arm and torso of a REEM-C humanoid robot, demonstrate the consistency of the proposed estimation method.'\\n\\n'encoder'\\n\\n'In this work, we proposed a new derivation-free estimation method for second-order kinematics on floating-base robots. Relying on highly-redundant distributed acceleration measurement from an artificial sensory skin covering our robot, our method is two-fold. The first step consists in fusing the skin accelerations link-wise using a dedicated Maximum-Likelihood observer. The second step then involves fusing these measurements with that of the encoders and floatingbase IMU. The results show that the motion derivative estimates have equivalent noise levels to those of the reference signals while maintaining a low lag. Future work will consist, on the one hand, of extending the developed method to an entire humanoid robot, thereby verifying whether it can be executed on-line and, on the other hand, of evaluating the propensity of the estimated acceleration signal to be used in a high-gain control loop.'\",\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":\"'In this work, we proposed a framework based on Dynamical System and Support Vector Regression to perform contact tasks that require robotic arm-hand coordination. We learned the desired motion-force pattern from human demonstrations and encoded them using dynamical systems. Furthermore, we modeled the surface using SVR where the approximation of the distance to the surface and its gradient helped us with the coordinated motion planning for the robot arm and hand\\/finger. Using the impedance-based dynamical system approach, we delivered compliant interaction with the environment for both arm and the hand. Furthermore, this control approach allowed for motion-control (in the tangential plane) and force exertion (in the orthogonal direction) on a nonlinear surface.'\",\"476\":null,\"477\":\"'We introduced the differentiable mapping network that learns a map for sparse visual localization, and demonstrated strong performance across simulated and real-world domains. We believe these results provide useful insights for applications beyond the sparse localization domain. Future work may explore extensions of our view-based latent map structure, e.g., having multiple view-embeddings at learned locations. A particularly interesting direction is to extend the proposed work to visual SLAM, where new observations could be treated as additional context, and uncertain context poses could be encoded in particles.'\",\"478\":\"'Overview of the proposed AT-Net. The proposed method has two main modules, namely convolutional feature encoder and multi-level attention module. The convolutional encoder takes three images as input, the positive image, anchor image and negative image and generates task specific feature representations with the aid of the multi-level attention module. These feature representations are then trained using metric loss objective.'\\n\\n'To generate an embedding vector which can act as a generalized and nuisance-invariant state representation for training any reinforcement learning agent, it is necessary to encode only those pixels from the video demonstration which are relevant to the imitation task. In a standard CNN pipeline, each layer contains a diverse range of image features and as we go deeper into the network, these features tend to possess more contextual information than spatial information [33]. Our multi-level spatial attention module takes advantage of this behavior by allowing image patches from shallow layers (local feature vectors\\\\nl\\\\ns\\\\ni\\\\n) to directly contribute to the final embedding vector in proportion to its compatibility with the last layer feature map (global feature vector g). This also means that we are incentivizing the shallow layers to focus on learning those features which are contextually relevant to the imitation task so that they will be embedded in the final representation vector. In addition, similar to the case in [22], there is a greater benefit of using layers relatively late in the network as they are \\u2018relatively mature\\u2019 and more specific to the task. The use of a multi-level module therefore allows us to access the diversity of information available at different spatial resolutions in the pipeline so that we can generate a more comprehensive and detailed representation vector.'\",\"479\":\"https:\\/\\/github.com\\/lifelong-robotic-vision\",\"480\":\"'ImageNet-pretrained networks have been widely used in transfer learning for monocular depth estimation. These pretrained networks are trained with classification losses for which only semantic information is exploited while spatial information is ignored. However, both semantic and spatial information is important for per-pixel depth estimation. In this paper, we design a novel self-supervised geometric pretraining task that is tailored for monocular depth estimation using uncalibrated videos. The designed task decouples the structure information from input videos by a simple yet effective conditional autoencoder-decoder structure. Using almost unlimited videos from the internet, networks are pretrained to capture a variety of structures of the scene and can be easily transferred to depth estimation tasks using calibrated images. Extensive experiments are used to demonstrate that the proposed geometric-pretrained networks perform better than ImageNet-pretrained networks in terms of accuracy, few-shot learning and generalization ability. Using existing learning methods, geometric-transferred networks achieve new state-of-the-art results by a large margin. The pretrained networks will be open source soon 1 .'\\n\\n'In this paper, we propose a novel pretraining task that uses wild videos from the internet to capture both semantic and spatial information. Based on the fact that the optical flow between two images is determined solely by the geometric structure and motions (both camera and object motion), we use a conditional encoder-decoder to separate structure information and motion information from uncalibrated videos. The structure information of the scene is encoded from a single image, and motion information is estimated from two adjacent images. The optical flow between two images is finally reconstructed by using the estimated structure information conditioned on the motion vector. Since optical flow can be supervised without camera intrinsic or extrinsic parameters, the pretraining task can utilize unlimited videos from the internet and learn a variety of structures. After the pretraining stage, the encoder network can be easily transferred to depth estimation tasks.'\\n\\n'Overview of the proposed geometric pretrain and further transfer learning. (a), the proposed geometric pretraining uses a simple but effective conditional encoder-decoder structure. Input during the pretraining is two-frame image pairs. Structure information is estimated from the reference image and the motion information is estimated from two images. Using both the encoded structure and motion, the optical flow between two frames is reconstructed. Since the encoder only sees the reference image, it is forced to learn motion-invariant structure information. (b), pretrained encoder networks can be further transferred for depth estimation using current methods (e.g., monodepth2).'\\n\\n'The core of the proposed geometric pretraining task is to separate the structure information from the optical flow. Using a conditional encoder-decoder, the optical flow is reconstructed using the structure information from a single image conditioned on the motion information from two images. By compressing the motion information through a low-dimensional bottleneck, the structure encoder network is forced to captures motion-invariant structure information so that the optical flow can be correctly estimated.'\\n\\n'1) Structure Encoder:'\\n\\n'The structure encoder takes the source image as the input and outputs feature maps for the optical flow decoder. Since the trained structure encoder will be used as the backbone network for depth estimation, no specific architecture is required. In this work, we use standard ResNet-18 [37] for most of the experiments to be consistent with monodepth2.'\\n\\n'2) Motion Encoder: The motion encoder takes two images as the input and outputs a compact motion vector. The purpose of the motion encoder is similar to that of pose networks in GeoNet [11] that estimate the motion information. Different from pose networks, the proposed motion encoder generates motion information implicitly using latent vectors. We follow monodepth2 and use a modified ResNet [37] network with bottleneck layers to generate the latent motion vectors with the dimension of only 128. Compared to the dimensions of recovered optical flow, for example, 640 \\u00d7 192 \\u00d7 2, the motion vector only accounts for less than 0.1 %o of the data, such that motion vectors only encode the necessary motion information.'\\n\\n'2) Motion Encoder:'\\n\\n'3) Flow Decoder:'\\n\\n'The flow decoder is designed to fuse the information from the structure encoder and the motion encoder. The motion vector is upsampled and concatenated with the feature map from the structure encoder. The decoder consists of several nearest-upsample layers and uses skip connections to reconstructs the optical flow between It and It+1. Compared to the encoder network, the decoder contains 14 layers and is relatively small. To speed up the training convergence, the optical flow f is estimated in a coarse-to- fine manner.'\\n\\n'A number of encoders are trained using different combinations of the datasets. Networks are also initialized differently to study the effect of second-order transfer learning (e.g., ImageNet\\u2192Geometric-pretraining\\u2192depth-learning). Details are listed in Table III-D. ImageNet indicates the model is trained using ImageNet-pretrained networks as the initialization. The models are implemented in PyTorch [41] and optimized using Adam [42]. The batch size is selected to maximize the usage of a single GTX-TITAN Xp: 20 images for the resolution of 640 \\u00d7 192, and 8 images for 1024 \\u00d7 320. The learning rate is 10\\u22124 throughout the training for simplicity.'\\n\\n'We transfer the pretrained structure encoders into depth estimation networks using monodepth2 [18] without any modification. Specifically, we use the Eigen full split to train stereo-supervised depth estimation and use the Zhou split to train monocular-supervised or monocular-stereo-supervised depth estimation. To evaluate the performance of trained networks, we follow the standard approach that caps depth values to 80 m. Monocular-supervised depth maps are median scaled [8] before evaluation to correct the unknown scale in monocular sequences. We evaluate recent state-of- the-art methods including our pretrain-transferred models in Table II. We also evaluate the transferred performance of the ResNet50 from object detection for driving scenes [43] (denoted as obj in the table).'\\n\\n'In short, using geometric pretrained networks as the initialization for depth learning brings more accuracy compared to ImageNet-initialized models. To overcome the small size of pretraining datasets, second-order transfer learning (ImageNet\\u2192Geometric-pretraining\\u2192depth-learning) can be adopted. We also achieve the new state-of-the-art results by a large margin using geometric-pretrained ResNet50 as the encoder in a higher resolution.'\\n\\n'Since encoder networks learn the structure information during the geometric pretraining, depth maps can be learned using small size training data. To prove the ability of few- shot learning, we train a number of stereo-supervised depth estimating networks using different sizes (100%, 10%, and 5%) of the KITTI dataset. Since training with small datasets may lead to overfitting, in Figure 4, we report the best performance of each model among the training epochs.'\",\"481\":\"'During rotation, the rotation angle of the joint was recorded by an encoder (Scancon 2RM3600D), and the wavelength data was collected by an interrogator (Micron Optics SI-255) at a scan rate of 1000 Hz in real-time. The wavelength shift \\u0394\\u03bbB and the rotation angle \\u03b1 for these three cycles are plotted in Fig. 7. As can be seen, the wavelength shift follows the pattern of the rotation angle.'\",\"482\":null,\"483\":null,\"484\":\"'Our method can be used to inform the selection of sensors and actuators when designing a walking device with the capability of fall prevention. We test two versions of actuators: the 2D hip device can actuate the hip joints only in the sagittal plane while the 3D device also allows actuation in the frontal plane. We also consider three different configurations of sensors: an inertial measurement unit (IMU) that provides the COM velocity and acceleration, a motor encoder that measures hip joint angles, and the combination of IMU and motor encoder. In total, we train six different recovery policies with three sensory inputs and two different actuation capabilities. For each sensor configuration, we train a fall predictor using only sensors available to that configuration.'\\n\\n'Figure 9 shows the stability region for each of the six design configurations. The results indicate that 3D actuation expands the stability region in all directions significantly comparing to 2D actuation, even when the external force lies on the sagittal plane. We also found that the IMU sensor plays a more important role than the motor encoder, which suggests that COM information is more critical than the hip joint angle in informing the action for recovery. The recovery policy performs the best when combining the IMU and the joint encoder, as expected.'\\n\\n'We validate the proposed framework using the open-source physics engine DART [38]. Our human agent is modeled as an articulated rigid body system with 29 degrees of freedom (dofs) including the six dofs for the floating base. The body segments and the mass distribution are determined based on a 50th percentile adult male in North America. We select the prototype of our assistive walking device as the testbed. Similar prototypes are described in [39]\\u2013[41]. It has two cable-driven actuators at hip joints, which can exert about 200 Nm at maximum. However, we limit the torque capacity to 30, beyond this value, the torque saturates. Sensors, such as Inertial Measurement Units (IMU) and hip joint motor encoders, are added to the device. We also introduce a sensing delay of 40 to 50 ms. We modeled the interaction between the device and human by adding positional constraints on the thigh and anchor points. For all experiments, the simulation time step is set to 0.002s.'\",\"485\":null,\"486\":null,\"487\":null,\"488\":\"'(A) 2D illustrations of the shirt design with main components color coded and insets detailing sensor-to-cable and cable-to-board connections. (B) Data flow schematic.'\",\"489\":null,\"490\":\"'Figure 1-a shows the observation scenario, which is used to record training data to build an ErrP decoder. In this scenario, the subjects do not interact with the robot, since human gestures and robot\\u2019s action selections are already preprogrammed. A hand gesture is displayed to the subjects as a word (left, right, forward, or upward) on the monitor, which is located on the left side of the robot. A feature vector of the displayed gesture is sent to the pseudo-learning algorithm, where action selections are preprogrammed (1). The selected action is sent to the robot (2) that executes the selected action (3). The subjects observe the robot\\u2019s action (4) and evaluate the correctness of the robot\\u2019s action in form of EEG (5). Two different kinds of markers were written in EEG data for offline post-hoc analysis: gesture markers and action markers. The comparison between gesture markers (e.g., gesture: left) and robot\\u2019s action markers (e.g., action: left) enables to evaluate the correctness of robot\\u2019s actions (e.g., correct action), which served as actual label (Tab. I-a) for evaluation on predicted label of an ErrP decoder (e.g., ErrP is detected \\u2192 wrong action, Tab. I-c, i.e., false positive, Tab. I-e). Note that positive class refers to wrong action. The reason for preprogramming is to reduce the recording time of EEG data.'\\n\\n'2) ErrP decoder:'\\n\\n'For each subject, we recorded six datasets to train an ErrP decoder and three datasets for online learning. Six datasets that were recorded from the observation scenario (Fig. 1a) were used for training a classifier to detect two distinct classes (ErrP\\/no ErrP), which are correlated with each correct and wrong action of the robot. Each dataset contained 80 correct and 10 wrong actions. The trained classifier (Fig 1b) was used to detect ErrPs for each single action of the robot during the online learning session (Fig 1c). In the online learning session, each dataset contained 90 trials for all subjects except for two subjects who performed 120 trials and 60 trials respectively. The subject-specific differences between the total numbers of trials were taken into account for evaluation and the results are reported in percentage. In online learning, the ratio between correct and wrong actions varied between subjects, since learning performance depends on the quality of reward, i.e., ErrP-detection performance between subjects.'\\n\\n'Experiment setup: (a) observation scenario, (b) ErrP decoder, and (c) human-robot interaction scenario. Details, see text.'\\n\\n'The outputs of ErrP decoder were analyzed both in the whole learning process (e.g., Fig. 2-b1) and in each learning phase (e.g., Fig. 2-b2). As performance metric, we used the number of false positive (FP), false negative (FN), true negative (TN), and true positive (TP), and ErrP misclassifications (FP \\u222a FN).'\",\"491\":null,\"492\":\"'Safety in Human-Robot Collaboration (HRC) is a bottleneck to HRC-productivity in industry. With robots being the main source of hazards, safety engineers use over-emphasized safety measures, and carry out lengthy and expensive risk assessment processes on each HRC-layout reconfiguration. Recent advances in deep Reinforcement Learning (RL) offer solutions to add intelligence and comprehensibility of the environment to robots. In this paper, we propose a framework that uses deep RL as an enabling technology to enhance intelligence and safety of the robots in HRC scenarios and, thus, reduce hazards incurred by the robots. The framework offers a systematic methodology to encode the task and safety requirements and context of applicability into RL settings. The framework also considers core components, such as behavior explainer and verifier, which aim for transferring learned behaviors from research labs to industry. In the evaluations, the proposed framework shows the capability of deep RL agents learning collision-free point-to-point motion on different robots inside simulation, as shown in the supplementary video.'\\n\\n'The main goal of the pre-learning phase is to encode the task and safety requirements and create an instrumented training environment that reflects such requirements and the context of applicability. The main components in the pre-learning phase are mentioned as follows.'\\n\\n'Currently, expert knowledge is required to provide an instrumented training environment that encodes the task and the context of applicability into a RL setting. The expert knowledge is reflected in: (1) encoding the context of applicability by providing instrumented training environments, (2) encoding the task and safety requirements in reward functions [18] and constraints [21], [22], (3) defining the action space by providing existing robot skills for the RL agents to learn on [11], (4) defining the state space by providing the necessary perception skills (e.g., object recognition), (5) and optionally providing demonstrations to increase the learning efficiency of RL agents [23].'\\n\\n'The reward function can either be sparse or dense. Sparse reward functions encode RL tasks in the form of providing a non-negative reward signal +r or 0, only when a task or sub-task is achieved correctly, and negative reward signal \\u2212r otherwise. Sparse reward functions are one of the most trivial and convenient ways to define tasks in the RL setting. However, sparse reward functions can easily lead the RL agent to learn unintended behavior [16], due to the delayed reward signals. On the other hand, a dense reward function provides a state-dependent reward signal that better describes the task, e.g., Euclidean distance in case of goal-based tasks. Defining a well-shaped dense reward function requires reward engineering and limits the power of RL by incurring a possible sub-optimal expected behavior that the RL agent learns.'\\n\\n'In this perspective we test the performance changes of agents against different punish weights, since we use punish weight to encode safety in the reward functions (see Eq. 5, 6 and 7). Fig. 2-a and 2-c show the safe success rate vs. punish weight in fetch and UR5 environments with three static or dynamic obstacles. Results show that all agents are sensitive to the punish weight and robot type. Especially HER success rate starts to collapse with punish weights exceeding 10, compared to more consistent performance of HRA-2 and HRA-n.'\\n\\n'Since we train the RL agents to output only the Cartesian positions to an existing IK solver, the trained agent should gain similar performance on different types of robots. However, results show that an agent trained on UR5 and executed on Fetch achieves a performance degradation from 85.0% safe success rate on UR5 to 52.0% safe success rate on Fetch. An agent trained on Fetch and executed on UR5 achieves performance degradation from 71.7% on Fetch to 64.1% on UR5. The performance degradation comes from the different shapes and DoF of the robots encoded in the agents\\u2019 latent space. It can be noticed that training on a redundant robot and executing the trained policy on a non-redundant robot results in a better transferrability.'\\n\\n'Safety in HRC is a bottleneck to productivity due to over-emphasized safety constraints, and expensive risk assessment process. With robots being the main source of hazards, we proposed a novel framework that uses deep RL to teach robots safe and intelligent behavior. The framework maps HRC task and safety requirements into a RL setting, and uses preliminary work in the HRC eld, namely, CARA to encode the context of applicability in the respective RL setting. In the evaluations, we compared different RL agents on learning and solving the task of safe point-to-point robot arm motion. Results showed that off-policy DDPG combined with HER and HRA are more sample-efficient compared to on-policy PPO. Moreover, HRA combined with HER did not largely improve the safety performance over using HER alone. However, HRA is hypothesised to reduce the complexity of the representation of the learnt behavior, that can simplify the Verifier and Explainer. Overall, off-policy agents achieved up to \\u2248 85% safe motion in challenging scenarios. The results are encouraging and expected to be further improved when enhancing individual components of the framework. As future work, we are planning to introduce hard constraints instead of soft penalties to further increase the safe success rate. Another foreseen improvement is to include realistic human models for training and evaluating the RL agents, and addressing the verification and explanation for transferring the learnt behavior from simulation to a real-world scenario.'\",\"493\":\"'We augment a sampling-based planner, RRT, with a hierarchical recurrent network that encodes the meaning of a natural-language command the robot must follow. Just as with a traditional planner, the robot mentally explores the space around a\\\\nstartlocation\\\\nbuilding a\\\\nsearchtree\\\\nto find a good path in its configuration space. Unlike a traditional planner, we do not specify a goal as a location, but instead rely on a neural network to score how likely any position in the configuration space is to be an end state while considering the past history of the robot\\u2019s actions and its observation of the environment. The structure of the RNNs mirrors that of the search tree, with each splitting off as different decisions are considered. At each time step, the RNNs\\\\nobservetheenvironment\\\\n, and can adjust the sampling process of the planner to avoid moving in undesirable locations (in this case, the tree is not expanded toward the red circle, and instead adjusted to go down the passageway through the green circle). See fig. 2 for details on the structured RNNs and how they encode the structure of sentences as relationships between recurrent models.'\\n\\n'We ensure that the model provides a level of interpretability in two ways. First, the structure of the sentence is encoded explicitly into the structure of the recurrent network; see fig. 2. Inspecting the network reveals which subnetworks are connected together and the topology of the connections mirrors that of natural language. Second, the internal reasoning of the model is highly constrained to operate through attention maps. Rather than allowing each component the freedom to pass along any information up the hierarchy in order to make a decision, we constrain all components to communicating via a grayscale map that is multiplied by the current observation of the environment. Inspecting these attention maps reveals information about which areas each network is focused on and can provide a means to understand and explain failures. In addition, this constrained representation is easy to learn and does not require a large number of examples. We find that adding these interpretable computations also increases performance relative to more opaque representations, likely because words which have never co-occurred at training time have an easier time understanding the output of other word models when the representations are interpretable.'\\n\\n'Success rate of executing natural language commands with two concepts, the same number as the models saw at training time, and five or six concepts, more complex sentences than were seen at training time. All models sampled 500 nodes in the configuration space of the robot. Our model generalizes well and faithfully encodes the meaning of commands. While the BoW model is also novel it lacks the internal structure to represent many sentences and significantly underperforms our hierarchical model.'\\n\\n'We have demonstrated that a hierarchical recurrent network can work in conjunction with a sampling-based planner to create a model that encodes the meaning of commands. It learns to execute novel commands in challenging new environments that contain features not seen in the training set. We demonstrated that our approach scales to real-world sentences produced by users.'\\n\\n'Our model provides a level of interpretability. The structure of the model overtly mirrors that of the parse of a sentence making it easy to verify if a sentence has been incorrectly encoded. Attention maps are used throughout the hierarchical network to allow component parts to communicate with one another. These provide another means by which to understand which components caused a failure; see fig. 7 for example attention maps for failed commands and the level of explanation possible along with its limitations. In many cases, this provides both reassurances that errors will be pinpointed to the responsible part of the model and confidence in the chosen model. This level of transparency is unusual for end-to-end models in robotics.'\\n\\n'Commanding mobile robot movement based on natural language processing with RNN encoder\\\\xaddecoder'\",\"494\":null,\"495\":\"'http:\\/\\/github.com\\/PRBonn\\/visual-crop-row-navigation'\\n\\n'Autonomous navigation is a pre-requisite for field robots to carry out precision agriculture tasks. Typically, a robot has to navigate along a crop field multiple times during a season for monitoring the plants, for applying agrochemicals, or for performing targeted interventions. In this paper, we propose a visual-based navigation framework tailored to row-crop fields that exploits the regular crop-row structure present in fields. Our approach uses only the images from on-board cameras without the need for performing explicit localization or maintaining a map of the field. Thus, it can operate without expensive RTK-GPS solutions often used in agricultural automation systems. Our navigation approach allows the robot to follow the crop rows accurately and handles the switch to the next row seamlessly within the same framework. We implemented our approach using C++ and ROS and thoroughly tested it in several simulated fields with different shapes and sizes. We also demonstrated the system running at frame-rate on an actual robot operating on a test row-crop field. The code and data have been published.'\\n\\n'The source code of our navigation system, the data from the real-world experiments as well as the simulated environment are available at: http:\\/\\/github.com\\/PRBonn\\/visual-crop-row-navigation.'\\n\\n'http:\\/\\/github.com\\/PRBonn\\/visual-crop-row-navigation'\",\"496\":\"'We now introduce the Opt Full-row (OFr) algorithm that optimally solves COP-FR. The pseudo-code of OFr is provided in Algorithm 1.'\",\"497\":null,\"498\":\"'https:\\/\\/drive.google.com\\/file\\/d\\/15BO2_4aaR5KHxbgOJQ9fV76zf0_i4GJs\\/view?usp=sharing'\",\"499\":null,\"500\":null,\"501\":null,\"502\":null,\"503\":null,\"504\":\"\\\"Self-diagnosis and self-repair are some of the key challenges in deploying robotic platforms for long-term real-world applications. One of the issues that can occur to a robot is miscalibration of its sensors due to aging, environmental transients, or external disturbances. Precise calibration lies at the core of a variety of applications, due to the need to accurately perceive the world. However, while a lot of work has focused on calibrating the sensors, not much has been done towards identifying when a sensor needs to be recalibrated. This paper focuses on a data-driven approach to learn the detection of miscalibration in vision sensors, specifically RGB cameras. Our contributions include a proposed miscalibration metric for RGB cameras and a novel semi-synthetic dataset generation pipeline based on this metric. Additionally, by training a deep convolutional neural network, we demonstrate the effectiveness of our pipeline to identify whether a recalibration of the camera's intrinsic parameters is required or not. The code is available at http:\\/\\/github.com\\/ethz-asl\\/camera_miscalib_detection.\\\"\",\"505\":null,\"506\":\"'The hardcoded strategy for deciding between Pick and Sweep actions uses three parameters hpile,hmin and dmin (highlighted in blue). The right figure shows the side view of a possible configuration of boxes. Boxes are categorised as either \\\"pile\\\" or \\\"wall\\\" based on the threshold height hpile. The decision tree in the left explains the logic. If the depth of the pile dpile is greater than the threshold depth dmin, it chooses to sweep. Otherwise it checks whether the height of the first wall hW is greater than the threshold height hmin and if so, performs a Pick. Pick action is instantiated by choosing a pick point preferring boxes that are closer to the robot and as high as possible. Sweep action is instantiated by choosing the depth of the sweep to be dpile.'\\n\\n'hardcoded strategy'\\n\\n'For the real world experiment, we ran the motion planner and the executor modules with the hardcoded strategy for 10 minutes and 32 seconds. The real world environment is shown in Fig. 8e. The boxes in the scene vary in sizes. We extract the pile and the walls by fitting planes to the depth values from the point cloud obtained using our sensors. Once we extract the pile and the walls, we use the hardcoded strategy as described in Fig. 6. A total of 128 boxes were unloaded in the entire run which results in an unloading rate of 0.2 boxes\\/sec (see video.)'\\n\\n'Hardcoded strategy:'\\n\\n'In this work, we proposed a planning, learning, and reasoning framework that accounts for uncertainty in the world, generates robust motion plans offline that are adapted for online real-time execution in previously unseen environments for automated truck unloading. Our real world experiments show the real-time performance of our motion planning and execution modules, while the simulation experiments show the capabilities of our framework in learning robust offline strategies that generalize online with better throughput when compared to hardcoded strategy designed by a human expert.'\",\"507\":null,\"508\":null,\"509\":null,\"510\":\"'As for the single joint experiments, the encoder information is used to compute the effective cable length lri at each iteration (only on the right here because the long cable does not provide additional information) and the corresponding values of \\u03b1.\\\\n\\u03b1\\\\n\\u02d9\\\\nare computed numerically.'\",\"511\":null,\"512\":null,\"513\":null,\"514\":\"'https:\\/\\/youtu.be\\/SnafS_5361g'\",\"515\":\"'The ball-and-beam system used for the experiments. There is an RGB camera above that measures the location of the ball. The encoder (seen in the figure) measures the angular position of the beam.'\",\"516\":null,\"517\":\"\\\"With the increase in use of Unmanned Aerial Vehicles (UAVs)\\/drones, it is important to detect and identify causes of failure in real time for proper recovery from a potential crash-like scenario or post incident forensics analysis. The cause of crash could be either a fault in the sensor\\/actuator system, a physical damage\\/attack, or a cyber attack on the drone's software. In this paper, we propose novel architectures based on deep Convolutional and Long Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder) and classify drone mis-operations based on real-time sensor data. The proposed architectures are able to learn high-level features automatically from the raw sensor data and learn the spatial and temporal dynamics in the sensor data. We validate the proposed deep-learning architectures via simulations and realworld experiments on a drone. Empirical results show that our solution is able to detect (with over 90% accuracy) and classify various types of drone mis-operations (with about 99% accuracy (simulation data) and upto 85% accuracy (experimental data)).\\\"\\n\\n'The reason for choosing data-driven approach (such as deep learning techniques) over traditional model-based approaches are as follows. Deep learning techniques\\u2014(a) have ability to learn complex patterns especially non-linear functions; the sensor data of a UAV at times of potential crash events (such as broken propeller) is highly non-linear and complex in nature; (b) have no requirement to manually design the features from the data\\u2014the layers in a deep network learn meaningful features on their own during the training process. This also translates to another advantage of not needing domain expertise to extract the features; (c) can work with unlabeled data in unsupervised fashion to generate features. This is very much beneficial for crash-like scenarios due to scarce availability of labelled data. To this end, we propose a novel Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (Bi-LSTM) deep neural network based autoencoder for detection of faults\\/anomalous patterns followed by a CNN-LSTM deep network for their classification\\/identification.'\\n\\n'We propose a novel Convolutional Neural Network (CNN) and Bidirectional Long Short Term Memory (Bi-LSTM) based deep autoencoder network architecture for real-time detection of anomalous patterns in UAV IMU sensor data.'\\n\\n'In this section, we first explain our CNN Bi-LSTM autoencoder network to detect anomalies followed by CNN-LSTM network classifier to identify anomalies\\/crash scenarios.'\\n\\n'CNN and Bi-LSTM based Detector (\\u2018AutoEnc\\u2019): Anomaly detection using unsupervised learning consists of two steps. In step 1, the system is trained with several normal examples to learn representations of the input data e.g., GMM clustering. Because we are dealing with temporal data, a sliding window approach needs to be adopted to learn these representations. In step 2, given a test data point, we define an anomaly score based on the learned representations, e.g., distance from the mean of the cluster. In an LSTM autoencoder, input time series data, {x0,x1,\\u2026xn}, of size n+1 (corresponding to one window of data segmented from full data) is fed to the encoders which consist of n + 1 LSTM cells. The output of the last LSTM cell\\u2014called the embedding\\u2014is fed as input to a series of n+1 LSTM cells to generate an output, {a0,a1,\\u2026an}. The autoencoder is trained by minimizing the reconstruction error, |x \\u2212 a|2.'\\n\\n'Convolutional Bi-LSTM Encoder: The basic encoder in an LSTM autoencoder does not perform sufficiently well as it not does take into account: (i) inter channel\\/modal correlations (ii) directionality of data. We design an encoder that addresses these issues as shown in Fig. 3(top). It consists of a series of 1-dimensional (1D) convolutional layers followed by bi-LSTM layers. The convolutional layers help in capturing inter-channel spatial correlations, while the LSTM layers help in capturing inter- and intra-channel temporal correlations. The number of filters, the size of filter kernel and the type of padding (with a default stride length of 1) is indicated in Fig. 3(top). For example, in the first convolution step, 48 filters of size 5 \\u00d7 1 are applied to the input data of size, 25 \\u00d7 1 \\u00d7 6 (assuming a 6-channel input data), to result in an output of size, 25\\u00d71\\u00d748. Unidirectional LSTM layers capture temporal patterns only in one-direction, while the data might exhibit interesting patterns in both directions. Hence to capture these patterns, we have a second set of LSTM cells for which the data is fed in the reverse order. Further, we have multiple layers of these bi-LSTM layers to extract more hierarchical information. All the data that has been processed through multiple convolutional and bi-LSTM layers is available in the cell states of final LSTM cells. This is the output of the encoder which will be fed as input to our decoder.'\\n\\n'Convolutional Bi-LSTM Encoder:'\\n\\n'encoder'\\n\\n'decoder'\\n\\n'Convolutional Bi-LSTM Decoder: The decoder performs encoder operations in reverse order so as to reconstruct the input data (Fig. 3(bottom)). It first consists of bi-LSTM layers which take the final cell states from encoder as one of the inputs (the other input being zero). Other input (other than the previous cell state) can be either zero or the output of the previous LSTM cell. The outputs of LSTM layers are fed as input to a series of 1D de-convolutional layers which perform reverse of convolution (also called transposed convolution) to generate data with same shape as that of input data to encoder (6-channel 1D data of length 25).'\\n\\n'Convolutional Bi-LSTM Decoder'\\n\\n'Proposed deep CNN and Bi-LSTM architecture for fault classification. For 1D Conv. and Bi-LSTM layers, please refer the encoder in Fig. 3.'\\n\\n'Inducing Crash: In order to simulate a broken propeller, we make its RPM to zero by supplying zero current to its motor. However, for this to work successfully, it is necessary to constantly provide zero current to the effected motor. A one-time operation would not be sufficient as the currents to the motors are generated in a high-frequency update loop using a PID controller and hence correct current values (which do not induce crash) are provided to the motors in subsequent update loops. Hence, we modified the firmware code to make the effected propeller\\u2019s motor current to zero inside the update loop itself so that its RPM is continuously zero, resulting in a crash. Fig. 8(c) shows the accelerometer data (x,y,z axes) after the crash is induced by making the RPM of one of the propellers to zero. By comparing with the actual drone crash data (accelerometer) from 3DR Solo drone in Fig. 5, we can see that the simulation data is more complex and hence difficult to learn than the former. We successfully simulated all 15 crash scenarios (classes). In all these scenarios, we collected 18-channel data viz., linear and angular versions of acceleration, velocity, and position along all the three axes from the start of the crash until the end. We repeated this experiment 300 times to account for noises and gather sufficient data. We have included the video demo of the crash experiments run in AirSim simulator along with this submission. For results below, we used only 3-channel linear acceleration data (instead of all 18 channels), unless otherwise specified, as there is a need to limit the amount of data to process on a real drone.'\",\"518\":null,\"519\":null,\"520\":null,\"521\":null,\"522\":\"'The proposed navigation framework is mainly divided into two modules, the predictor and the policy network. Our predictor is a recurrent network that captures the uncertainty and motion information in the environment from a sequence of laser scan data. After that, the uncertainty and motion information are encoded as part of the input of the policy network to predict the action distribution. In particular, feeding the input through convolutional layers and fully-connected layers, the mean value of the action distribution can be obtained. Besides, the variance of the distribution is modeled as an uncertainty-dependent vector estimated by a uncertainty mapping function in order to enhance the connection between the uncertainty and the navigation behavior.'\",\"523\":null,\"524\":null,\"525\":\"'The Aerial Robotic Chain concept focuses on reconfigurability to facilitate advanced sensing, processing, and endurance capabilities combined with the ability of shape morphing in order to navigate through complex and possibly confined environments. In comparison to monolithic aerial robot designs, it provides an alternative to the typical trade-off between payload\\/endurance and the minimum environment cross section that the system can navigate through. To achieve this goal, ARC consists of multiple quadrotor MAVs (ARC-units) rigidly connected with each other via lightweight rods and respective 3DoF rotational joints at the connection points. The rigid link serves to facilitate a) def-inite geometric configurations, b) communications between the robots (through the integration of respective wiring), and c) the (future) ability to estimate the pose of one of the robot from the other assuming the integration of encoders within each joint. Figure 2 visualizes the ARC concept.'\\n\\n'In the particular prototype realization of the aerial robotic chain, a system consisting of two ARC-units and a connecting rod was developed. This prototype, ARC-Alpha, is built around a collision-tolerant quadrotor design with 5inch 3-blade propellers (Lumenier 5\\u00d75\\u00d73) and high\\u2013torque motors (Lumenier LX2205-12 2400kV), integrating a PX4 autopilot responsible for attitude and thrust control, and an UP Core Plus board with an Atom x7-E3950 2GHz responsible for high-level control and further autonomy functionalities. The first ARC-unit of ARC-Alpha further integrates a Realsense T265 visual-inertial Tracking system. On the other hand, the second ARC-unit integrates a FLIR Boson LWIR thermal camera. The distribution of the required sensing payload to two different ARC-units follows the principle design goal of a distributed and reconfigurable system that at the same time can have the payload of a larger unit with the navigation capacity of smaller systems in terms of flying through narrow cross sections. The ARC-units are connected through a 3DoF rotational joint that integrates optical encoders (US digital E4T), while the connecting rod further integrates wiring that allows the UP boards of the robots to be connected with each other. As such, the design of the ARC-Alpha allows for distributed computation among the two robotic units and sharing of information. In the future, the encoders on the joints will be used to enable the estimation of the pose of the second robot from the first, while the distributed computing will be exploited to allow the control to run on either both of onboard computers or on any of them, while also allowing \\\"hot-swap\\\". The design of ARC-Alpha is a milestone in the direction of realizing the extendable aerial robotic chain.'\",\"526\":null,\"527\":null,\"528\":null,\"529\":\"'While the method as described in this paper has not been rigorously tested in head-to-head racing, the online pathplanning component is capable of navigating the vehicle in the presence of other agents. Future work should investigate the feasibility of deploying this methodology utilizing an expanded search space that encodes the behaviors of other racers. A clear limitation of the presentation of this work is a lack of comparisons to other black-box optimizations, we mitigate this deficiency by measuring the toolchains performance against a variety of hand-tuned solutions entered in F1\\/10 competitions. Given that the experimental results show that we can find significant performance improvements over the other competition entries, it is important to explore whether other search strategies can create even faster agents.'\\n\\n'TUNERCAR includes a complete set of system components which are utilized to demonstrate the approach. In this section we outline the vehicle hardware, simulator, and vehicle software. The target hardware, an open-source 1\\/10th scale autonomous vehicle created by the authors, is mapped to a validated, deterministic simulator capable of modeling both the dynamics and sensors included on the vehicle [f1tenth.org]. The simulator includes a wrapper which enables a distributed approach to optimization of the source program with low communication overhead. In addition, the toolchain provides performance oriented implementations of the core algorithms (see Section IV-C) and a method to update their parameters.'\",\"530\":null,\"531\":\"'https:\\/\\/youtu.be\\/tix7LhzVlNE'\",\"532\":\"'https:\\/\\/github.com\\/honda-research-institute\\/DenseTrafficEval'\\n\\n'The simulation scenario we consider is implemented using an open-source simulator 2. In order to obtain diverse on-road behaviours from other vehicles, we make a few modifications to well-known rule-based models - Intelligent Driver Model (IDM) for lane following [33], and MOBIL for lane changing [34]. IDM is modified to include a stop-and-go behaviour that cycles between a non-zero and zero desired velocity in regular time intervals. This behaviour is intended to simulate real-world driving behaviours seen in heavy-traffic during rush-hour. MOBIL is allowed to randomly change lanes (if safe) with probability p = 0.04. Our benchmark scenario has two key components - dense traffic and a deadend in front of the ego-vehicle. The most important parameters that control an instantiation of this benchmark are the number of vehicles, gaps between them, their desired velocities and the ego-vehicle\\u2019s distance to the deadend. A detailed list of parameters is given in Table I.'\\n\\n'https:\\/\\/github.com\\/honda-research-institute\\/DenseTrafficEval'\",\"533\":\"'Even more so than in everyday traffic situations, interactions between vehicles are crucial in racing scenarios. In fact, as traffic code does not severely limit operations of vehicles, a wider range of possible behaviors becomes possible\\u2014 a representative example of this being the leveraging of potential collisions in order to overtake an opponent vehicle during the course of a race [8]. Both in public roads and racing scenarios, the model of the opponent vehicle, whether this is driven by a human or not, plays a central role in the development of motion planning strategies. Following the line of inquiry of [8], in this work we propose a control framework that explicitly accounts for uncertainties in the model used for collision avoidance, which constitutes the main source of interaction with opponent vehicles.'\",\"534\":\"'Sparsity: The transition matrix contains two types of connections: the first are introduced by the nature of image collection and encode the belief that if the vehicle was at a location p at time t, the next image observed is highly likely to be in the temporal vicinity of p. To be robust to variations in vehicle speed, in addition to connecting neighbouring nodes, we connect the current node to a fixed number of nodes which we call the adjacency of p, denoted by\\\\nW(p)\\\\n. This represent the set of directed connections from which the current state can be reached in a single hop. This gives the transition matrix a Toeplitz structure, as seen from Fig. 3, which is highly sparse.'\",\"535\":\"'Our global localization pipeline which uses a single sparse 3D LiDAR scan at a time is outlined in blue. Segments are extracted from a LiDAR scan, described using a neural network and matched against a database. The correspondences are then verified and used to estimate the global 6 DOF pose of the robot. Optionally, depicted in purple, we propose a method to augment point cloud segments with the visual information encoded in overlapping camera images, in order to enhance the segment descriptor performance.'\\n\\n'Our approach requires that a camera has significant overlap with the field of view of the LiDAR sensor. Once an incoming point cloud has been segmented, the resulting segments are projected onto the camera image. Following this, the bounding box of the projection is computed and the corresponding part of the image is extracted. The resulting image patch represents the visual appearance of the corresponding segment and serves to complement the structural information encoded in the LiDAR data.'\",\"536\":\"'https:\\/\\/youtu.be\\/mhEsCoNao5E'\\n\\n'https:\\/\\/youtu.be\\/MIUGkGPxCdY'\\n\\n'https:\\/\\/youtu.be\\/mhEsCoNao5E'\\n\\n'https:\\/\\/youtu.be\\/mhEsCoNao5E'\\n\\n'In the second experiment, we perform real-world learning from demonstrations of a small set of obstacle avoidance scenarios performed by a mobile robot (Pioneer 3-AT). The objective is to detect obstacles using LIDAR input data and execute avoidance motions. To compare performance with existing RNN models, we carry out the same experiments with Long short-term memory networks (LSTM), standard continuous-time RNN, an unconstrained LDS and our proposed LDS with Gershgorin circle loss regularization. The code and training data are provided in the supplementary materials.'\",\"537\":\"'This section describes how we remotely teach a robot a task through teleoperation for generating an agent\\u2019s behaviour. In this work, we encode the agent\\u2019s initial behaviour using Gaussian Mixture Model (GMM), from a single teleoperated demonstration. A GMM with k components is parameterized by\\\\n\\u0398\\\\n(k)\\\\n=\\\\n{\\\\n\\u03c0\\\\nm\\\\n,\\\\n\\u03bc\\\\nm\\\\n,\\\\n\\u03a3\\\\nm\\\\n}\\\\nk\\\\nm=1\\\\nwhere \\u03c01, \\u2026, \\u03c0k are mixing coefficients (priors), \\u00b51, \\u2026, \\u00b5k are mean vectors and \\u03a31, \\u2026, \\u03a3k are covariance matrices. The probability density function of X is said to follow the k-component GMM if it can be written as\\\\nP(X\\u2223\\\\n\\u0398\\\\n(k)\\\\n)=\\\\n\\u2211\\\\nk\\\\nm=1\\\\n\\u03c0\\\\nm\\\\nN(X;\\\\n\\u03bc\\\\nm\\\\n,\\\\n\\u03a3\\\\nm\\\\n)\\\\n, subject to the constraints 0 < \\u03c0m < 1 and\\\\n\\u2211\\\\nk\\\\nm=1\\\\n\\u03c0\\\\nm\\\\n=1\\\\n. The dataset for each Degree-of-Freedom (DOF) is created by concatenating the teleoperated demonstration trajectory with the phase variable:'\",\"538\":null,\"539\":\"'https:\\/\\/vsislab.github.io\\/ccvil\\/'\\n\\n'Fig. 2 illustrates our context translation model. Given the observations from source context, two streams of Encoder1 extract the features referred to as\\\\n\\u03d5\\\\n1\\\\n(\\\\no\\\\n1\\\\ns\\\\n)\\\\nand\\\\n\\u03d5\\\\n1\\\\n(\\\\no\\\\n1\\\\ne\\\\n)\\\\n, while the Encoder2 extracts the feature of initial observation from target context referred to as\\\\n\\u03d5\\\\n2\\\\n(\\\\no\\\\n2\\\\ns\\\\n)\\\\n. Then\\\\n\\u03d5\\\\n2\\\\n(\\\\no\\\\n2\\\\ns\\\\n)\\\\nand\\\\n(\\\\n\\u03d5\\\\n1\\\\n(\\\\no\\\\n1\\\\ns\\\\n)\\u2212\\\\n\\u03d5\\\\n1\\\\n(\\\\no\\\\n1\\\\ne\\\\n))\\\\nare concatenated as the inputs of a translation function\\\\nz=F(\\\\n\\u03d5\\\\n1\\\\n(\\\\no\\\\n1\\\\ns\\\\n)\\u2212\\\\n\\u03d5\\\\n1\\\\n(\\\\no\\\\n1\\\\ne\\\\n),\\\\n\\u03d5\\\\n2\\\\n(\\\\no\\\\n2\\\\ns\\\\n))\\\\n. Finally, a decoder is employed to decodes the feature z into prediction goal observation\\\\no\\\\n^\\\\n2\\\\ne\\\\n. Encoder1 and Encoder2 are built using the first 3 blocks of ResNet50 backbone but have different weights. The model is trained by supervised learning with pairs of demonstrations data {Ds,Dt}, where Ds comes from random contexts (the source context) and Dt comes from the agent\\u2019s own context (the target context). The pixel-level image loss\\\\nL\\\\nimage\\\\nprovides supervision signal for our training, denoted as follow:'\\n\\n'Illustration of our depth prediction model. The network uses an encoder-decoder architecture to predict the goal depth observation. Is and Ie are initial and goal color observations, Ds is the initial depth observation and\\\\nD\\\\n^\\\\ne\\\\nis the prediction goal depth observation.'\\n\\n'https:\\/\\/vsislab.github.io\\/ccvil\\/'\",\"540\":\"'Learning Assistance by Demonstration (LAD) is concerned with using demonstrations of a human agent to teach a robot how to assist another human. The concept has previously been used with smart wheelchairs to provide customised assistance to individuals with driving difficulties. A basic premise of this technique is that the learned assistive policy should be able to generalise to environments different than the ones used for training; but this has not been tested before. In this work we evaluate the assistive power and the generalisation capability of LAD using our custom teleoperation and learning system for smart wheelchairs, while seeking to improve it by experimenting with different combinations of dimensionality reduction techniques and machine learning models. Using Autoencoders to reduce the dimension of laserscan data and a Gaussian Process as the learning model, we achieved a 23% improvement in prediction performance against the combination used by the latest work on the field. Using this model to assist a driver exposed to a simulated disability, we observed a 9.8% reduction in track completion times when compared to driving without assistance.'\\n\\n'However, due to the natural characteristics of the readings generated by a moving laser scanner, this simplistic feature selection mechanism can deteriorate the quality of input data. As a more powerful approach, we propose the use of Autoencoders [21]. We experimented with this using unsupervised training and up to four fully-connected layers, resulting in encodings of sizes 256, 64, 16 or 4. For all layers, logistic sigmoid was used as the encoding and decoding transfer functions and both weight and sparsity regularisation was applied. As an alternative, Principal Component Analysis (PCA) [22] was also considered, keeping enough principal components to explain 99, 90, 75 and 50% of the observed variance of the scan data. Post-analysis showed that this corresponded to respectively 229, 59, 14 and 5 components.'\\n\\n'This study was financed in part by the Coordena\\u00e7\\u00e3o de Aperfei\\u00e7oamento de Pessoal de N\\u00edvel Superior - Brasil (CAPES) - Finance Code 001 - and by CEFET-MG as a PhD scholarship to VS, and a Royal Academy of Engineering (RAEng) Chair in Emerging Technologies to YD.'\",\"541\":null,\"542\":\"'In the automatic dislocation experiments, the control algorithm first actuates the magnetization coil to produce a vertical magnetic field, such that the location of the battery can be calculated through the sensory data based on the proposed trilateration localization method. Due to the weak magnetic field produced by the electromagnetic coil system, direct actuation of the magnetized button battery is impossible; therefore, a magnet-containing capsule is used to dislocate the capsule. Next, the system switches to the navigation mode, and the algorithm proceeds with the real-time tracking of the capsule and controls the navigation coils to navigate the capsule to the button battery and dislocate the battery. The pseudo code of the control algorithm is shown in Algorithm 1.'\",\"543\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34422445'\",\"544\":\"'Fig 5 (a) shows a fault in the main rail force sensor (FS1). The system was commanded to move from a starting force of 0.2 N to 0.4 N; once it reached that point, a fault in FS1 was simulated by lifting the associated tissue attachment ring, thus making the sensor value drop (90 s). This fault was recognized as an out-of-boundary fault. Consequently, the system immediately retracts to a safe position as indicated by the encoder whose signal shows a displacement from 4 to 0 mm. In a second experiment, we show the force incongruency fault. Fig 5 (b) shows a force reading fault in FS1. We simulated this fault by manually pressing on FS1. The initial force set point was 0.45 N and, once this was achieved, FS1 was pressed until a large difference was created between the two force sensors (70s). The system recognized the large difference as a force incongruency fault and retracted to safe position.'\\n\\n'Fault detection and retraction reflex: (a) out-of-boundary error, (b) incongruency error. FS1, E1 are main unit\\u2019s force sensor and encoder, respectively. FS2, E2 are the mirror unit\\u2019s force sensor and encoder, respectively. FG is the target force.'\",\"545\":null,\"546\":null,\"547\":\"'https:\\/\\/youtu.be\\/-g1p3VZfqt0'\\n\\n'https:\\/\\/www.bilibili.com\\/video\\/av89922671\\/'\",\"548\":null,\"549\":\"'Multi-robot systems of increasing size and complexity are used to solve large-scale problems, such as area exploration and search and rescue. A key decision in human-robot teaming is dividing a multi-robot system into teams to address separate issues or to accomplish a task over a large area. In order to address the problem of selecting teams in a multi-robot system, we propose a new multimodal graph embedding method to construct a unified representation that fuses multiple information modalities to describe and divide a multi-robot system. The relationship modalities are encoded as directed graphs that can encode asymmetrical relationships, which are embedded into a unified representation for each robot. Then, the constructed multimodal representation is used to determine teams based upon unsupervised learning. We per-form experiments to evaluate our approach on expert-defined team formations, large-scale simulated multi-robot systems, and a system of physical robots. Experimental results show that our method successfully decides correct teams based on the multifaceted internal structures describing multi-robot systems, and outperforms baseline methods based upon only one mode of information, as well as other graph embedding-based division methods.'\\n\\n'To address the selection of multi-robot teams, we propose a novel multimodal graph embedding approach to encode diverse relationships of multiple robots as graphs and integrate these multiple graphs into a unified representation that is applied to divide a multi-robot system into teams. We model each internal relationship of the robots using a directed graph as an information modality. Given a set of member relationships, we construct multiple graphs that are applied as the input to our approach. Then, we propose a new multimodal Katz index to integrate multiple graphs of robot relationships and embed them into a unified representation for each robot. Then, the constructed representation is used to identify multi-robot teams through unsupervised learning. Our multimodal graph-embedded robot division approach is capable of fusing diverse robotic relationships and identifying team divisions without requiring explicit knowledge of tasks.'\\n\\n'We propose a new multimodal graph embedding approach to integrate multiple directed graphs that encode relationships in a multi-robot system into a unified representation that is used to identify multi-robot teams.'\\n\\n'In real-world deployment, robotic members within a sys-tem typically have multiple various relationships (e.g., spatial relationships, communication connectivity, and organization hierarchy). When multiple robotic relationships are available, we encode the system with M graphs, where\\\\nG\\\\nm\\\\nis the graph describing the m-th relationship between robots. Each graph\\\\nG\\\\nm\\\\nis described by an adjacency matrix\\\\nA\\\\nm\\\\n\\u2208\\\\nR\\\\nN\\u00d7N\\\\n, where each element aij is the weight of the edge connecting vertex vi to vertex vj. The proposed method is able to represent both directed (e.g. communication from one member to others) and undirected (e.g., spatial distances) relationships. In the case of representing undirected relationships, the weight of edges satisfies aij = aji.'\\n\\n'The spatial relationships, communication capabilities, and structured hierarchy of these formations are defined by the field operations teaming protocol. We encode each of these relationship modalities as a separate graph in order to com-pare our approach to previous methods. Table I reports the clustering accuracy for our approach versus these baseline approaches. Out of a possible 306 agents in the six different formations, our approach clusters 96.73% of the agents correctly. The second best is the concatenated combination of HOPE embeddings, clustering 93.14% of agents correctly, showing that extending existing graph embedding methods to leverage multiple information modalities can significantly improve their performance from using a single modality.'\",\"550\":\"'https:\\/\\/github.com\\/DCSLgatech\\/mams-astar'\\n\\n'https:\\/\\/youtu.be\\/aKr5JQd-qAM.'\\n\\n'https:\\/\\/github.com\\/DCSLgatech\\/mams-astar'\",\"551\":null,\"552\":null,\"553\":null,\"554\":\"'This paper introduces the Weighted Buffered Voronoi tessellation, which allows us to define distributed, semicooperative multi-agent navigation policies with guarantees on collision avoidance. We generate the Voronoi cells with dynamic weights that bias the boundary towards the agent with the lower relative weight while always maintaining a buffered distance between two agents. By incorporating agent weights, we can encode selfish or prioritized behavior among agents, where a more selfish agent will have a larger relative cell over less selfish agents. We consider this semi-cooperative since agents do not cooperate in symmetric ways. Furthermore, when all agents start in a collision-free configuration and plan their control actions within their cells, we prove that no agents will collide. Simulations demonstrate the performance of our algorithm for agents navigating to goal locations in a position-swapping game. We observe that agents with more egoistic weights consistently travel shorter paths to their goal than more altruistic agents.'\\n\\n'We wish to consider multi-robot systems where different agents have different social preferences, ranging from egoistic to altruistic. One solution is to assign each agent preference using Social Value Orientation (SVO), a metric from social psychology that relates how an individual weights their reward to self versus the reward to others in social dilemmas. By assigning agents varying social preferences, we can design systems that are heterogeneous and incorporate semicooperative agents. In this paper, we encode these social preferences into the definition of a Voronoi tessellation, such that more egoistic and selfish agents have larger relative cells than the more altruistic agents in the environment.'\\n\\n\\\"For distributed multi-agent systems operating in cluttered environments, it is important to design control policies that allow for collision avoidance and scale with the number of agents. Voronoi cells can guarantee collision-free maneuvers of the group when all agents restrict planning to within their cell. To encode varying levels of cooperation, we present the Weighted Buffered Voronoi Cell (WBVC), which generates buffered cells with asymmetric boundaries depending on the relative cooperation between the agents. Using these weighted cells, we allow for semi-cooperative planning, where some agents may be more selfish than other agents. In this weighted tessellation, a more selfish agent has a larger cell than its less selfish neighbors. We present a control policy wherein agents navigate towards their goal location by choosing the closest point within their cell, with a righthanded heuristic for avoiding deadlock. Under this policy, the selfish agent's larger cell creates a larger available space for maneuvers, thus it is less likely to yield. We demonstrate this behavior in a position-swapping game between multiple agents with varying cooperation. Consistently, we observe more selfish agents take shorter paths to their goal than less selfish agents.\\\"\\n\\n'We encode Social Value Orientation (SVO) preferences in into a Voronoi tessellation using linear weights. (a) Unweighted vs (b) Weighted Buffered Voronoi tessellation, with egoistic (red), prosocial (purple), and altruistic (blue) agents. By encoding SVO into the Voronoi tessellation, egoistic agents receive a larger relative piece of the environment, which translates to priority when navigating.'\\n\\n'This paper presents the Weighted Buffered Voronoi tessellation, which allows us to encode social preferences and varying degrees of cooperation into a buffered Voronoi cell. For agents navigating through an environment, we guarantee collision avoidance when all agents choose control policies within their cells. Additionally, agents with more egoistic SVO preferences travel shorter paths when navigating towards their goal than more altruistic agents. We avoid deadlock in our position swapping games by employing a right-hand rule, consistent with other distributed multiagent policies. Future work will example how the social preferences of the individual might be optimized to achieve optimal group performance.'\",\"555\":\"'We would like to thank Scott Chow and Christopher Bollinger for their help troubleshooting code.'\",\"556\":null,\"557\":null,\"558\":\"'Once detected, we symbolically encode an infeasible task-agent assignment. The symbolic constraint then enables pruning similar task-agent assignment in the future. If we have a potentially feasible assignment, then we explicitly generate motion plans and cache them for the task-agent pair. We cache every computed motion plan and keep a record of the objects that will influence the motions. For example, extracting the part with the left-arm is only influenced by the door. In the future, if there is a similar query for part extraction from a compartment with an open door, then our method avoids recomputing the motion plan and recycles the cached trajectory. Moreover, instead of planning motion for all the agents together, our method uses the cached motion for a task-agent pair and uses that as a constraint for generating the motion for another task-agent pair. For example, while one arm is extracting the part with the help of the base, the second arm that is holding the door will also need to have joint motion such that the door can be kept at the open state during the part extraction. We can generate the motion for an arm to extract the part, and then use this motion as a constraint to generate the motion of the other arm and base for holding the door during part extraction.'\",\"559\":null,\"560\":\"'https:\\/\\/github.com\\/caelan\\/SS-Replan'\\n\\n'https:\\/\\/arxiv.org\\/abs\\/1911.04577'\\n\\n'https:\\/\\/youtu.be\\/IOtrO29DFUg'\\n\\n'Our key representational insight is that we can encode the Bayesian filtering process by defining sampling and inference streams that operate on distributions. The sample-obs stream samples ?obs from the distribution of observations using the observation model for ?o and a pose belief ?pb. The test-vis stream returns true if object ?o2 at belief ?pb2 prevents the robot from observing ?obs with probability less than , a value described in Section V. The probability of occlusion is estimated by performing ray-casting along ?obs using poses sampled from ?pb2.'\\n\\n'Algorithm 1 gives the pseudocode for our online replanning policy. The inputs to POLICY are the prior belief b, goal set of beliefs B*, and maximum cost c*. POLICY maintains a set of previously proven static facts fprev as well as the tail of the previous plan\\\\na\\\\n\\u20d7 \\\\nprev\\\\n. On each iteration, first, the procedure DETERMINIZE models the belief SSPP as a deterministic planning problem with actions A, initial state s, and goal set of states S*. If the prior plan\\\\na\\\\n\\u20d7 \\\\nprev\\\\nexists, POLICY applies the plan constraints using the CONSTRAINPLAN procedure described in algorithm 2. If the PDDLStream planner PLAN is unable to solve the constrained problem within a user-provided timeout, the constraints are removed, and planning is reattempted. If successful, PLAN returns not only a plan \\u00e3 but also the certified facts f within the preimage of\\\\na\\\\n\\u20d7 \\\\nthat prove that\\\\na\\\\n\\u20d7 \\\\nis a solution. Then, POLICY executes a1, the first action of\\\\na\\\\n\\u20d7 \\\\n, receives an observation o, and updates its current belief b. Finally, it extracts the subset of constant facts in f, static facts that only involve constants, and sets\\\\na\\\\n\\u20d7 \\\\nprev\\\\nto be remainder of\\\\na\\\\n\\u20d7 \\\\nthat was not executed.'\\n\\n'Algorithm 2 gives the pseudocode for the constraint transformation. It creates a new set of action schemata A\\u2032, each of which have modified preconditions and effects, using the previous plan \\u00e3. The fact (Applied i) is a total-ordering constraint that enforces that action ai\\u22121 be applied before action ai. For each argument v of action ai, if v is a constant, the new action is forced to use the same value. The fact (Bound v) is true if symbol v has already been assigned to some value in the action sequence. If (Bound v) is true, the fact (Assigned v ?p) is true if free variable v has been assigned to new value ?p. Each free variable v must either be unbound or assigned to action argument ?p.'\\n\\n'https:\\/\\/github.com\\/caelan\\/SS-Replan'\",\"561\":\"'https:\\/\\/github.com\\/CAOR-MINES-ParisTech\\/ukfm'\\n\\n'The present paper introduces a novel methodology for Unscented Kalman Filtering (UKF) on manifolds that extends previous work by the authors on UKF on Lie groups. Beyond filtering performance, the main interests of the approach are its versatility, as the method applies to numerous state estimation problems, and its simplicity of implementation for practitioners not being necessarily familiar with manifolds and Lie groups. We have developed the method on two independent open-source Python and Matlab frameworks we call UKF-M, for quickly implementing and testing the approach. The online repositories contain tutorials, documentation, and various relevant robotics examples that the user can readily reproduce and then adapt, for fast prototyping and benchmarking. The code is available at https:\\/\\/github.com\\/CAOR-MINES-ParisTech\\/ukfm.'\\n\\n'Besides providing a comprehensive code, our main contribution in terms of methodology is to introduce a novel and general framework for UKF on manifolds that is simpler than existing methods, and whose versatility allows direct application to all manifolds encountered in practice. Indeed, [7], [8] proposes UKF implementations based on the Levi-Civita connection but mastering differential geometry is difficult. [7], [13], [20], [21] are reserved for SO(3) and SE(3), while [23] is reserved for Lie groups and requires more knowledge of Lie theory than the present paper.'\\n\\n'We see that the belief is encoded using only a mean estimate\\\\n\\u03c7\\\\n^\\\\n, and a covariance matrix P that encodes the extent of dispersion of the belief around the estimate.'\\n\\n'In the code, we implement the frameworks on relevant vanilla robotics examples which are listed as follows:'\\n\\n'We finally enhance code framework, documentation and examples with filter performance comparisons: for each example we simulate Monte-Carlo data and benchmark UKFs and EKFs based on different choices of uncertainty representation (4) through accuracy and consistency metrics.'\\n\\n'We have released both open source Python package and Matlab toolbox UKF-M implementations of our method at https:\\/\\/github.com\\/CAOR-MINES-ParisTech\\/ukfm. Both implementations are wholly independent, and their design guidelines pursue simplicity, intuitiveness and easy adaptation rather than optimization. We adapt the code to the user preferences as follow: the Python code follows class-object paradigm and is heavily documented through the Sphinx documentation generator, whereas the Matlab toolbox contains equivalent functions without class as we believe choosing well function names is best suited for the Matlab use as compared to class definition. The following code snippets are based on the Python package that we recommend using.'\\n\\n'https:\\/\\/github.com\\/CAOR-MINES-ParisTech\\/ukfm'\\n\\n'A Code for Unscented Kalman Filtering on Manifolds (UKF-M)'\",\"562\":\"'IMU measurements are available with a fixed sampling time of 10ms and a typical delay below 0.5ms and a maximum delay of up to 10ms; wheel encoder measurements are available with a sampling time between 15ms and 65ms and typical delay below 0.5ms and a maximum delay of up to 11ms; global pose information from SLAM is available with a sampling time between 30ms and 340ms. The delay of the global pose information is mainly distributed between 10ms and 60ms with a median delay in all experiments ranging from 20ms to 25ms. However, some pose data have a considerably higher delay. In (R1a-c), (R2), (R4) measurements include outliers with delays up to 200ms. The experiment (R3) contains considerably more global pose data with delays above 35ms than the other experiments, also including some outliers with delays over 200ms.'\",\"563\":\"'TempNet is a shallow version of SegNet and uses 2 convolution and pooling\\/upsampling blocks for the decoder and encoder. Unlike prior methods, the scores produced by TempNet are conditioned on the data. The input to the temperature network is the same as for the segmentation network, and the output is a single-channel spatial temperature map,\\\\nT\\u2208\\\\nR\\\\nd\\\\no\\\\n\\u00d7\\\\nd\\\\no\\\\n. The average temperature deviation ratio (Ave.Temp) uses the average of the test spatial temperature map and applying Eq. (4).'\",\"564\":null,\"565\":null,\"566\":null,\"567\":\"'Fig. 3 shows a kinematic model of the proposed 2-DOF compliant manipulator. The length of the contact point at the end of the manipulator can be obtained as follows from the linear encoder and the rotary encoder attached to the manipulator.'\\n\\n'In order to obtain the displacement of the spring, it is necessary to know the location of the joints that are connected with a spring. Those also are calculated from encoders attached to the manipulator. The following constraint equation can be derived from the kinematic model.'\\n\\n'As described above, the posture of the robot is estimated by using the linear encoder and the rotary encoder to obtain the displacement of spring. Thus, using the Jacobian, the contact force of the manipulator contact point can be obtained. In most cases, encoders have a higher resolution and are more robust to signal noise than the load cell, and it is expected that more stable and precise force control will be enabled through the contact force estimated by SEA.'\\n\\n\\\"Fig. 9 shows the effect of sensor noise on the force tracking performance when applying SEA. The sensor noise measured in the load cell is within \\u00b1 0.5 N, but since the frequency is more than 10Hz and the bandwidth of the actuator is narrower than the noise, the sensor noise negatively affects the force tracking performance. On the other hand, the force can be measured without a noise with a resolution under 0.02 N when using the displacement of the SEA, because the resolution of the linear encoder used to calculate the manipulator's posture is 20 um and the resolution of the rotary encoder is 1\\/12000. Therefore, more stable force control is achieved when using SEA displacement data. In addition, as shown in Fig. 9(c), the performance of the controller using load cell data is not stable and even vibrates at the maximum amplitude of 42 N in the 5-degree slope disturbance case. This is attributed to the larger angle and longer distance between the wall and the end effector as well as the greater singularity of the manipulator, as shown in Fig. 10. Although the noise does not change, the noise is amplified near the singularity to increase the control input error and degrade the controller performance due to the characteristics of the parallel robot mechanism.\\\"\",\"568\":null,\"569\":null,\"570\":null,\"571\":null,\"572\":null,\"573\":\"'The four integrated geared motors of type 37Dx68L are DC motors with a rotary encoder that can be used to deter-mine the position of the shaft. Their maximum rotation speed is 350 rpm with which theoretically a maximum driving speed of 0.88 m\\/s can be achieved with the selected wheels. This first demonstrator is currently powered by an external power supply via a cable but could also use rechargeable batteries, which is the desired solution for the SwarmRail system. The electronics also comprise a Bluetooth communication module that enables the mobile unit being controlled from a smartphone and an Arduino Mega which runs the control algorithm that is described in the following lines.'\",\"574\":null,\"575\":null,\"576\":\"'https:\\/\\/sites.google.com\\/view\\/actor-critic-legible-planner'\\n\\n'Figure 3 shows the network architecture of the motion predictor and the policy network of motion planner. A Seq2Seq network is used for motion prediction. Both encoder and decoder have 2 layers, within each layer containing 8 GRU units. The policy network maps the latent space of encoder and environment observation to robot actions through two fully connected layers. The policy network is optimized using Proximal Policy Optimization [14].'\\n\\n'The architecture of the legible motion planning network. The input of the moiton predictor is the robot end-effector positions in the last 10 time-steps. The policy network has the environment observation and encoder state\\\\no=[\\\\no\\\\n\\u03b6\\\\nt\\\\n,\\\\no\\\\ng\\\\n,\\\\no\\\\nd\\\\n,\\\\no\\\\nsenc\\\\n]\\\\nas inputs, and outputs the mean of joint velocity a as the action command.'\",\"577\":null,\"578\":\"'Since there is no publicly available code on this task, we compare our inference algorithm with a random baseline model as the reference for future benchmark; it simply returns an object with the same attributes as the query object at tq. The result shows that our system achieves 81% accuracy, while the baseline model only has 39% accuracy.'\",\"579\":\"'For the training of the model, we split the data gathered during the experiment into a training and validation set using a leave one out policy took randomly. The resulting training set was composed of 1938 data points and 112 for the validation set. The model was trained with data augmentation (horizontal flip, random crop) and we stopped the training when the validation loss stabilized. We used open-source, pre-trained models on the COCO dataset [40] from the Tensorflow object detection API.'\",\"580\":\"'encoder'\\n\\n'decoder'\",\"581\":null,\"582\":\"'An ESCON 50\\/5 maxon motor controller controls a 48 V Brushed DC motor, that is used to propel the test cart forward\\/backward, and provides motor current\\/encoder data. The rig leaves the Z-Axis of the tactile wheel\\/motor assembly unconstrained, and then has an optical encoder to measure the Z-height displacement of the wheel which is read into an Arduino UNO. The entire motor assembly is mounted using a 6-axis ATI force-torque Omega160 sensor to measure any forces generated by the wheel\\/media interaction, read in by an NI-DAQ.'\",\"583\":\"https:\\/\\/github.com\\/pedropro\\/UrsoNet\",\"584\":null,\"585\":null,\"586\":null,\"587\":\"'https:\\/\\/github.com\\/TRAILab\\/AC-DCC'\\n\\n'In order to relate information across cameras in a Dynamic Camera Cluster (DCC), an accurate time-varying set of extrinsic calibration transformations need to be determined. Previous calibration approaches rely solely on collecting measurements from a known fiducial target which limits calibration accuracy as insufficient excitation of the gimbal is achieved. In this paper, we improve DCC calibration accuracy by collecting measurements over the entire configuration space of the gimbal and achieve a 10X improvement in pixel re-projection error. We perform a joint optimization over the calibration parameters between any number of cameras and unknown joint angles using a pose-loop error optimization approach, thereby avoiding the need for overlapping fields-of-view. We test our method in simulation and provide a calibration sensitivity analysis for different levels of camera intrinsic and joint angle noise. In addition, we provide a novel analysis of the degenerate parameters in the calibration when joint angle values are unknown, which avoids situations in which the calibration cannot be uniquely recovered. The calibration code will be made available at https:\\/\\/github.com\\/TRAILab\\/AC-DCC.'\\n\\n'In addition to a more accurate calibration, we identify the parameters when joint angle values are not available, that cause the system to enter a degenerate state, where the calibration cannot be uniquely recovered. For systems that do not possess dynamics, determining the degenerate parameters is equivalent to identifying the columns of the measurement jacobian the cause it to be rank deficient. We identify two degenerate configurations that arise when the joint angles for the first and last joint are added to the parameters to be estimated. As a final contribution, in order to accelerate research in Visual SLAM using DCCs, we release our calibration code at https:\\/\\/github.com\\/TRAILab\\/AC-DCC'\\n\\n'This paper demonstrated a method that is capable of calibrating a Dynamic Camera Cluster (DCC) consisting of any number of cameras while achieving sufficient measurement excitation guaranteeing a more accurate calibration compared to previous methods. Our proposed approach was tested in simulation consisting of a 3-DOF gimbal and multiple static cameras. In addition, we showed that incorporating the first and last joint angles into the estimation results in a degeneracy from which a unique calibration cannot be recovered. We provide a sensitivity analysis and make our code available to the research community in order to further development in DCC SLAM. Future work will consist of performing the calibration on real hardware as shown in Fig. 1 and incorporating the DCC in an Active Visual SLAM application and compared against Passive SLAM approaches.'\\n\\n'https:\\/\\/github.com\\/TRAILab\\/AC-DCC'\",\"588\":null,\"589\":null,\"590\":\"'The factor graph for configuration estimation. The spatial constraints are encoded as factors between the nodes.'\",\"591\":null,\"592\":null,\"593\":\"\\\"Overview of the SNN SLAM architecture: (1) Two ring attractor networks encode the robot's heading direction (HD) based on the neuronal path integration and memory of previously detected visual cues (blinking LEDs); (2) Shift neurons drive the active neuron in the path integration HD network to move with a speed determined by their firing rate; (3) At a loop closure event, the error estimation network (2D array of neurons) estimates the magnitude and sign of the difference between the location of the active neuron in the memory-driven HD network and the path integration HD network; (4) Small errors drive Shift neurons and trigger plasticity in the synapses connecting them to the motor neurons. Large errors induce map updates. See main text for details.\\\"\\n\\n'From top to bottom: (1) Activity of three LED neurons detecting LEDs blinking at different frequencies. (2) Synaptic weight values from LED to memory HD neurons. Colors denote the identity of the associated LED. Stars indicate the true positions of LEDs. The true angular distances between the LEDs with respect to the robot and converted to neuron units are annotated in blue while the learned angular distances are shown in red. Blue rectangles denote \\\"large error\\\" neuron\\\\'s spikes. (3) Activity of the path integration HD neurons. (4) Robot\\\\'s angular velocity inferred from the neural path integration (blue) and obtained from wheel encoders (red). Note how mismatch is reduced after all errors are corrected.'\\n\\n'Fig. 8 shows the outcome of the experiment. The top panel shows the activity of the three LED neurons, the second panel shows the synaptic weights that are formed during detection. Small blue squares correspond to spikes emitted from neurons in the \\\"large error\\\" population, which induce forgetting of the previously learned LEDs. The third panel shows the neural activity in the path integration HD population, and the bottom panel shows angular velocity obtained from wheel encoders and from neural path integration.'\",\"594\":null,\"595\":\"'https:\\/\\/youtu.be\\/VZaw5b43NQc'\\n\\n'The controller is first verified by simulations and validated by field experiments, but only field experiments are presented. Open sourced MPC [20] from the Apollo autonomous driving group of Baidu is applied as comparison. Test scenarios consist of different controllers and driving speeds (0~10m\\/s, and 0~15m\\/s), with each scenario evaluated live times. Since v is in the denominator of the models, a small positive speed v\\u03f5 is set as the threshold before engaging the controller and observer. The test platform is a 2017 Lincoln MKZ with a drive-by-wire system and a NovATel GPS\\/IMU with dual antenna. Localization precision is ~0.04 m and ~0.5 deg\\/h.'\",\"596\":null,\"597\":\"'The actuator demonstrated above is a multi-input device that measures spring deflection to infer the output torque at the end-effector. The deflection is measured through the difference in encoder measurements between the motormounted encoder and the spring-side shaft.'\\n\\n'The objective of ADRC is to provide accurate torque outputs based strictly on measurements of the deflection of the spring. With the estimate of the backlash angle, a reasonable estimate of the true deflection angle can be extracted from the encoder readings on either side of the spring. Backlash can then be compensated for and the updated reference can be used to compute error in the controller. Active disturbance rejection control (ADRC) will be used for this purpose, as ADRC is an error-based control method that can be used to compensate for backlash via a transient profile generator, i.e., a time-optimal solution reference trajectory designed for non-ideal systems. The output of the controller can be distributed to multiple inputs, which is the case for the actuator described by (4). Convergence of nonlinear ADRC for multi-input systems is demonstrated in [23].'\\n\\n'If backlash is not considered, which is the case for PID and classical ADRC, raw encoder inputs about the deflection are taken to be the true deflection angle of the spring causing deviations in expected output torque (\\u2206\\u03b8s = \\u03b8m \\u2212\\u03b8s \\u2212\\u03b8bk). With backlash compensation, an updated estimation of the true deflection can be inferred through the addition of the estimated backlash angle\\\\n(\\u0394\\\\n\\u03b8\\\\ns\\\\n=(\\\\n\\u03b8\\\\nm\\\\n\\u2212\\\\n\\u03b8\\\\ns\\\\n\\u2212\\\\n\\u03b8\\\\nbk\\\\n)+\\\\n\\u03b8\\\\n^\\\\nbk\\\\n)\\\\n.'\",\"598\":null,\"599\":null,\"600\":\"\\\"Many manipulation tasks, such as placement or within-hand manipulation, require the object's pose relative to a robot hand. The task is difficult when the hand significantly occludes the object. It is especially hard for adaptive hands, for which it is not easy to detect the finger's configuration. In addition, RGB-only approaches face issues with texture-less objects or when the hand and the object look similar. This paper presents a depth-based framework, which aims for robust pose estimation and short response times. The approach detects the adaptive hand's state via efficient parallel search given the highest overlap between the hand's model and the point cloud. The hand's point cloud is pruned and robust global registration is performed to generate object pose hypotheses, which are clustered. False hypotheses are pruned via physical reasoning. The remaining poses' quality is evaluated given agreement with observed data. Extensive evaluation on synthetic and real data demonstrates the accuracy and computational efficiency of the framework when applied on challenging, highly-occluded scenarios for different object types. An ablation study identifies how the framework's components help in performance. This work also provides a dataset for in-hand 6D object pose estimation. Code and dataset are available at: https:\\/\\/github.com\\/wenbowen123\\/icra20-hand-object-pose.\\\"\",\"601\":\"'Accurate 6D object pose estimation is fundamental to robotic manipulation and grasping. Previous methods follow a local optimization approach which minimizes the distance between closest point pairs to handle the rotation ambiguity of symmetric objects. In this work, we propose a novel discrete- continuous formulation for rotation regression to resolve this local-optimum problem. We uniformly sample rotation anchors in SO(3), and predict a constrained deviation from each anchor to the target, as well as uncertainty scores for selecting the best prediction. Additionally, the object location is detected by aggregating point-wise vectors pointing to the 3D center. Experiments on two benchmarks: LINEMOD and YCB-Video, show that the proposed method outperforms state-of-the-art approaches. Our code is available at https:\\/\\/github.com\\/mentian\\/object-posenet.'\",\"602\":null,\"603\":\"'Variational Grasp Sampling: The grasp sampler is a con-ditional Variational Autoencoder [41] and is a deterministic function that predicts the grasp g given a point cloud Xo and a latent variable z.\\\\nP(z)=N(0,I)\\\\nis a known probability density function of the latent space. The likelihood of the grasps can be written as such:'\",\"604\":null,\"605\":null,\"606\":null,\"607\":\"'After the last frame has been integrated into the voxel grid we stream all remaining voxels from the hash table to the octree. Then, we stream back large blocks from the octree to the hash table and extract the mesh on GPU. The surface is implicitly encoded in F(x) as its zero iso-surface. We use marching cubes ([27]) for this step. The individual meshes are stitched together for the final result as shown in Fig. 1.'\",\"608\":\"'https:\\/\\/github.com\\/Shubodh\\/ICRA2020'\\n\\n'https:\\/\\/github.com\\/Shubodh\\/ICRA2020'\",\"609\":null,\"610\":\"'Algorithm 1 The pseudocode of DiffG&C'\\n\\n'A Parrot Bebop 1 is used as the flying platform. The original autopilot is fully replaced by an open-source autopilot called Paparazzi UAV.'\",\"611\":null,\"612\":\"'https:\\/\\/youtu.be\\/GhKnd3DB1bE.'\",\"613\":\"'https:\\/\\/bit.ly\\/2k8syvh'\\n\\n'http:\\/\\/bit.ly\\/34xh7z4'\\n\\n'http:\\/\\/bit.ly\\/34xh7z4'\\n\\n'http:\\/\\/bit.ly\\/34xh7z4'\\n\\n'http:\\/\\/bit.ly\\/34xh7z4'\\n\\n'http:\\/\\/bit.ly\\/34xh7z4'\\n\\n'http:\\/\\/bit.ly\\/34xh7z4'\\n\\n'https:\\/\\/neptune.ai\\/'\",\"614\":\"'Driving Style Encoder: Situational Reward Adaptation for General-Purpose Planning in Automated Driving'\\n\\n'Automated driving in urban environments requires intelligent decision making that scales over a variety of traffic situations. A scalable approach needs to be able to address both structured and un-structured traffic as experienced in urban environments. In model-based planning, a semantic description of the environment is encoded in the form of static and kinematic features. The planning system has to translate this semantic description of the environment into safe and human-acceptable actions. The underlying planning algorithm often relies on manually-tuned linear reward functions that encode the relevance of the predefined features. Tuning such reward functions is a tedious task and is usually performed by motion planning experts. As a result, the reward function is often considered to be a static external signal that does not depend on the driving situation [1]. Manual tuning of the reward function becomes infeasible if a planning system is applied at scale and has to adopt a variety of driving styles. In this paper, we propose a deep learning approach in which a neural network dynamically predicts reward functions based on constantly changing static and kinematic features of the environment.'\\n\\n'This figure illustrates our planner for automated driving that samples policies for our path integral (PI) IRL formulation. The visualized state space is color-coded based on the state-action values. The z-axis corresponds to the velocity, whereas the ground plane depicts the spatial feature maps such as distance transformed lane centers and road boundaries. Three color-coded policies are visualized, namely the optimal policy (black), the odometry of a human demonstration (green), and the projection of the demonstration into the state space (red).'\",\"615\":null,\"616\":null,\"617\":null,\"618\":\"'The sensor measurements obtained from 3 cameras, an inertial measurement unit (IMU) and a pair of wheel encoders are preprocessed at the front-end of the proposed framework (see Figure 1).'\\n\\n'Two rotary wheel encoders and an MPU-9250 IMU provide odometry information to the EKF.'\",\"619\":\"'We also conducted experiments in a real indoor environment. The robot was equipped with a Hokuyo 2D LiDAR (UTM-30LX) and wheel encoders. The training dataset for E2E localization was created using the simulation. Notably, the real-environment experiments only demonstrated the qualitative results because the ground truth was not available.'\",\"620\":null,\"621\":null,\"622\":\"'The contributions of this paper are as follows. (1) We introduce a data augmentation technique and associated training regime called Accept Synthetic Objects as Real which allows for the generation of training data suitable for training manipulation in the clutter from demonstrations collected in scenarios without clutter. (2) We introduce a network architecture ASOR-Implicit Attention that trains a visual representation that implicitly encodes an attention feature on the target object in the primary latent encoding and its associated motor component. (3) We introduce a network architecture ASOR-Explicit Attention which trains a visual representation that explicitly focuses on the target object in the form of spatial attention, as well as its associated motor component. Both approaches are trained with demonstrations generated by ASOR. (4) We evaluate the two approaches on a series of manipulation experiments and compare them with previous approaches trained on the same data. The results show that ASOR-IA and ASOR-EA succeed in a significant fraction of trials in cluttered environments where previous approaches never succeed. In addition, we find both ASOR-IA and ASOR-EA outperform previous approaches even in uncluttered environments, ASOR-EA performs better than ASOR-IA, and ASOR-EA performs better even in clutter compared to the previous approaches in an uncluttered environment.'\\n\\n'Our objective is to teach a robot arm to manipulate objects of different types under conditions of clutter. We will perform this by training a visuomotor policy that takes as input a video stream of the scene and generates commands to the robot. To be able to make a one-to-one comparison to previous approaches, we will reuse one of the existing datasets for which both training data and code for previous approaches is publicly available [1]. This scenario includes training data for picking up 8 different object types and pushing to place 5 different object types. The objects are distinguished by shape and color, and have different degrees of rigidity including a rigid plastic bowl, a foam dumbbell, a piece of bubble wrap and a cotton towel.'\\n\\n'Encoder(E)'\\n\\n'The encoder E receives M as input, [\\u00b5z \\u03c3z] = E(M) where \\u00b5z,\\\\n\\u03c3\\\\nz\\\\n\\u2208\\\\nR\\\\nd\\\\nz\\\\n, and dz is the length of the primary latent encoding z. E is a multi-layer convolutional neural network with a 2dz dimensional vector output split into \\u00b5z and \\u03c3z. We assume that\\\\nz\\u223cN(\\\\n\\u03bc\\\\nz\\\\n,\\\\n\\u03c3\\\\nz\\\\n)\\\\n. The generator receives z and reconstructs the input frame O and the masked frame M. Note that in this case information regarding the target object and the text encoding is not passed to the encoder. The encoder should rely solely on the masked frame to extract the required information for G to reconstruct the original frame and the masked frame. In addition to the\\\\nL\\\\nA\\\\n, the reconstruction loss between different types of input images will also force the attention to focus on parts of the image related to the task.'\",\"623\":null,\"624\":\"'We present an open-source framework that provides a low barrier to entry for real-time simulation, visualization, and interactive manipulation of user-specifiable soft-bo...'\\n\\n'We present an open-source framework that provides a low barrier to entry for real-time simulation, visualization, and interactive manipulation of user-specifiable soft-bodies, environments, and robots (using a human-readable front-end interface). The simulated soft-bodies can be interacted by a variety of input interface devices including commercially available haptic devices, game controllers, and the Master Tele-Manipulators (MTMs) of the da Vinci Research Kit (dVRK) with real-time haptic feedback. We propose this framework for carrying out multi-user training, user-studies, and improving the control strategies for manipulation problems. In this paper, we present the associated challenges to the development of such a framework and our proposed solutions. We also demonstrate the performance of this framework with examples of soft-body manipulation and interaction with various input devices.'\",\"625\":\"'Stereo microscope (M205 FA, Leica) optical images present the basic components and final assembly of the SSND-type robot. The basic components are assembled with cyanoacrylate adhesives. The assembled robot is color-coded for image processing.'\",\"626\":null,\"627\":null,\"628\":\"'To validate SL1M qualitatively, the generated contact sequences are given as input to an open-source whole-body motion generator4 presented in [24], [25]. For lack of space we cannot describe this generator in detail here.'\\n\\n'github.com\\/loco-3d\\/sl1m'\",\"629\":\"'We are grateful to Junhyeok Ahn of the Human Centered Robotics Lab for his C++ neural network code. This work is supported by a NASA Space Technology Research Fellow-ship (NSTRF) grant #NNX15AQ42H.'\",\"630\":null,\"631\":null,\"632\":null,\"633\":\"'https:\\/\\/youtu.be\\/TDVluOeCQUA.'\",\"634\":\"'Programming languages, libraries, and development tools have transformed the application development processes for mobile computing and machine learning. This paper introduces CyPhyHouse-a toolchain that aims to provide similar programming, debugging, and deployment benefits for distributed mobile robotic applications. Users can develop hardware-agnostic, distributed applications using the high-level, event driven Koord programming language, without requiring expertise in controller design or distributed network protocols. The modular, platform-independent middleware of CyPhyHouse implements these functionalities using standard algorithms for path planning (RRT), control (MPC), mutual exclusion, etc. A high-fidelity, scalable, multi-threaded simulator for Koord applications is developed to simulate the same application code for dozens of heterogeneous agents. The same compiled code can also be deployed on heterogeneous mobile platforms. The effectiveness of CyPhyHouse in improving the design cycles is explicitly illustrated in a robotic testbed through development, simulation, and deployment of a distributed task allocation application on in-house ground and aerial vehicles.'\\n\\n'Nevertheless, it requires significant effort and time (of the order of weeks) to develop, simulate, and debug a new application for a single mobile robot\\u2014not including the effort to build the robot hardware. The required effort grows quickly for distributed and heterogeneous systems, as none of the existing robotics libraries provide either (a) support for distributed coordination, or (b) easy portability of code across different platforms.'\\n\\n'A high-fidelity, scalable, and flexible simulator for distributed heterogeneous systems The simulator executes instances of the application code generated by the Koord compiler\\u2014one for each robot in the scenario. Within the simulator, individual robots communicate with each other over a wired or a wireless network and with their own simulated sensors and actuators through ROS topics. For example, a simulation with 16 drones can spawn over 1.4K ROS topics and 1.6K threads, yet, our simulator is engineered to execute and visualize such scenarios in Gazebo running on standard workstations and laptops. In Section V, we present detailed performance analysis of the simulator.'\\n\\n'Different planners can work with the same code. Left shows the xy plots of concurrently available paths during a round of the Task application using an RRT planner for two quadcopters. Middle shows the same configuration, where paths computed are not viable to be traversed concurrently. The green markers are current quadcopter positions, The black path is a fixed path, and the red points are unassigned task locations. Right shows the same scenarios under which paths cannot be traversed concurrently, except that a different RRT-based planner (with path smoothing) is used.'\\n\\n'Car: Similarly, the car platform uses off-the-shelf hardware based on the open-source MIT RACECAR project [32]. The computing unit consists of an NVIDIA TX2 board. In the car platform, instead of using Ardupilot to handle the waypoint following, we wrote a custom ROS node uses the current position and desired waypoints to compute the input speed and steering angle using a Model Predictive Controller (MPC). The car has an electronic speed controller that handles low-level hardware control.'\",\"635\":null,\"636\":null,\"637\":\"'https:\\/\\/youtu.be\\/nfr1Fdketrc'\\n\\n'We compare our method to using kinodynamic methods without any high-level plans, e.g. KPIECE and RRT. We also compare our method to hierarchical methods which generate high-level plans autonomously. For the latter, we implemented a non-prehensile variation of the NAMO planner as well as an approach which uses a straight-line motion heuristic to generate candidate objects for the high-level plan. We performed experiments in simulation and on a real robot, which shows that the human-in-the-loop approach produces more successful plans and faster planning times. This gain, of course, comes at the expense of a human operator\\u2019s time. We show that this time is minimal and to evaluate this further, we experiment with a single human operator providing high-level plans in-parallel to multiple robots and present an analysis. We discuss whether such an approach may be feasible in a warehouse automation setting. To support reproducibility, we provide the source code of all our algorithms and experiments in an open repository1.'\\n\\n'We introduced a new human-in-the-loop framework for physics-based non-prehensile manipulation in clutter (GRTCHITL). We showed through simulation and real-world experiments that GRTC-HITL is more successful and faster in finding solutions than the three baselines we compared with. We also presented experiments where a single human-operator guides multiple robots in parallel, to make best use of the operator\\u2019s time. We made the source code of our framework and of the baselines publicly available.'\",\"638\":null,\"639\":\"'Overview: the perception module segments the object and computes its pose. An EKF estimates the full object state including latent properties like the COM c. The object shape is encoded by a silhouette, coordinates and normals in a top-down view. It is input to the affordance prediction module, that approximates the possible object motions at each contact point on the silhouette. The planning module selects contact point candidates using the predicted affordances and optimizes the pushing motion there.'\",\"640\":null,\"641\":null,\"642\":null,\"643\":\"'To complement the strengths between different depth estimation methods, we develop a strength prediction network for both the stereo model and structured-light camera to predict their pixel-wise strength relative to others. The strength prediction network is an encoder-decoder architecture inspired by U-Net [20], with color and depth image as inputs. We concatenate the outputs of strength prediction networks and apply soft-max to produce the weighting maps W1 and W2. The fused depth is the pixel-wise weighted summation of depth images.\\\\nD\\\\nf\\\\n=\\\\n\\u03a3\\\\nN\\\\ni=1\\\\nW\\\\ni\\\\n\\u2217\\\\nD\\\\ni\\\\nwhere Df is the fused depth image and N = 2 in our setting, which can be further extended to a multi-camera system. The depth fusion architecture is shown in the middle of Fig. 2. An L1 loss is applied to Df in the training stage.'\",\"644\":\"'A sensor pod is developed to have multiple onboard sensors, including two Ximea color camera (12 megapixel, global shutter), and a Velodyne Puck LiDAR (VLP-16) mounted on a continuously rotating motor. The scanned points from the VLP-16 are transformed into a fixed base frame using the motor angle measured by an encoder. Additionally, we assume the stereo camera and the LiDAR-motor system are both pre-calibrated. The LiDAR scanned points are transformed from the rotating LiDAR frame to a fixed motor frame, which is then referred to as the LiDAR frame. The extrinsic transform from the left camera to the fixed LiDAR frame is jointly optimized as Te in our pipeline.'\\n\\n'Reconstruction results on collected datasets. (a) - (d) are reconstructed sparse models of small structures, while (e) shows and compares the Smith Hall models using different methods. The error is encoded in the colors of points. Part of the openMVG model has a different scale and therefore is not used for evaluation. In (f) the histograms of errors are visualized and the mean and median values are annotated.'\\n\\n'SfM has been an active research area over the last two decades. One of the most widely used open-source tools Bundler, also known as PhotoTourism [1], shows the ability to process a vast amount of online photos to build large-scale 3D models. Since then, significant advancements have been achieved in works such as Theia [2], VisualSFM [3], OpenMVG [4] [5] and more recent COLMAP [6]. A comprehensive comparison of available SfM approaches is given in [7] which concludes that COLMAP achieved the state-of-the-art accuracy, robustness and completeness. Although many approaches exist, the majority are targeted on the general-purpose reconstruction, which does not deal with narrow FOV and close distance issues. Our work is focused on more detailed textures using narrow FOV cameras for inspection purposes. Additionally, a calibrated stereo pair is used together with a LiDAR sensor to provide metric information for the reconstructed model.'\",\"645\":null,\"646\":null,\"647\":null,\"648\":null,\"649\":\"'In many tasks such as finishing operations, achieving accurate force tracking is essential. However, uncertainties in the robot dynamics and the environment limit the force tracking accuracy. Learning a compensation model for these uncertainties to reduce the force error is an effective approach to overcome this limitation. However, this approach requires an adaptive and robust framework for motion and force generation. In this paper, we use the time-invariant Dynamical System (DS) framework for force adaptation in contact tasks. We propose to improve force tracking accuracy through online adaptation of a state-dependent force correction model encoded with Radial Basis Functions (RBFs). We evaluate our method with a KUKA LWR IV+ robotic arm. We show its efficiency to reduce the force error to a negligible amount with different target forces and robot velocities. Furthermore, we study the effect of the hyper-parameters and provide a guideline for their selection. We showcase a collaborative cleaning task with a human by integrating our method to previous works to achieve force, motion, and task adaptation at the same time. Thereby, we highlight the benefits of using adaptive force control in real-world environments where we need reactive and adaptive behaviours in response to interactions with the environment.'\\n\\n'In this work, we used dynamical systems for force adaptation in contact tasks. our method uses online adaptation of a state-dependent force correction model encoded with gaussian RBF kernels. It is particularly suitable for repetitive tasks as shown in section III-A where the robot should repeat the same motion while generating a desired contact force. For non-repetitive tasks which do not require to learn a model (no structural errors), our method could be simply modified to online adaptation of an offset. Our results showed that the force tracking accuracy is significantly improved for different desired motion-force profiles even in the face of real-time disturbances. For the correction to be effective, the hyper-parameters should be set properly; in particular the number of gaussians and kernel width. In our experiments, we distributed the RBFs locally on a small area around the attractor instead of covering the whole surface. The alternative requires a large number of gaussians which increases the computational cost and convergence time. The latter can be reduced using higher adaptation rate which, however, is prone to fluctuations and instabilities. Moreover, the surface used in the experiments remains fairly smooth. With more complex surfaces, a uniform distribution would not be able capture all the surface non-linearities, and increasing globally the number of RBFs is not a proper solution regarding computational cost. One could consider Gaussian Process Regression (GPR) [32] as an alternative to our method which is similar but more generic. Indeed, GPR can model complex non-linear functions and has been applied to learning surfaces [33]. Furthermore, our model only depends on the robot\\u2019s end-effector position. Such models perform satisfactorily as long as the robot re-visits the same position with similar velocities. Mathematically speaking, non-stationary behaviors with relatively lower dynamics than the adaptation can be compensated. To improve the performance further, one can include other variables such as velocity in the model. However, one has to deal with the curse of dimensionality or find efficient ways to distribute the RBFs in the input space. Overall, our experimental results suggest that many robotic applications can benefit from force adaptation with dynamical systems. As demonstrated in the collaborative cleaning task in section III-B, the time-invariant DS framework provides reactive and adaptive robotic behaviour. This enables robots to perform tasks in uncertain environment where the robot is required to physically interact with humans, objects, and surfaces.'\",\"650\":\"'https:\\/\\/github.com\\/xdspacelab\\/sscdnet'\\n\\n'This paper presents a novel semantic scene change detection scheme with only weak supervision. A straightforward approach for this task is to train a semantic change detection network directly from a large-scale dataset in an end-to-end manner. However, a specific dataset for this task, which is usually labor-intensive and time-consuming, becomes indispensable. To avoid this problem, we propose to train this kind of network from existing datasets by dividing this task into change detection and semantic extraction. On the other hand, the difference in camera viewpoints, for example, images of the same scene captured from a vehicle-mounted camera at different time points, usually brings a challenge to the change detection task. To address this challenge, we propose a new siamese network structure with the introduction of correlation layer. In addition, we create a publicly available dataset for semantic change detection to evaluate the proposed method. The experimental results verified both the robustness to viewpoint difference in change detection task and the effectiveness for semantic change detection of the proposed networks. Our code and dataset are available at https:\\/\\/github.com\\/xdspacelab\\/sscdnet.'\\n\\n'https:\\/\\/github.com\\/xdspacelab\\/sscdnet'\",\"651\":\"'We propose a novel fast and robust 3D point clouds segmentation framework via coupled feature selection, named 3DCFS, that jointly performs semantic and instance segmentation. Inspired by the human scene perception process, we design a novel coupled feature selection module, named CFSM, that adaptively selects and fuses the reciprocal semantic and instance features from two tasks in a coupled manner. To further boost the performance of the instance segmentation task in our 3DCFS, we investigate a loss function that helps the model learn to balance the magnitudes of the output embedding dimensions during training, which makes calculating the Euclidean distance more reliable and enhances the generalizability of the model. Extensive experiments demonstrate that our 3DCFS outperforms state-of-the-art methods on benchmark datasets in terms of accuracy, speed and computational cost. Codes are available at: https:\\/\\/github.com\\/Biotan\\/3DCFS.'\",\"652\":\"'Specifically, during training, the task decoder uses the sum of observations from all normal agents weighted by their corresponding matching scores and further computes the final prediction akin to Eq. 4 during inference:'\\n\\n'Compression (centralized):, the compression model applies two additional convolutional layers and performs uniform compression of all observations at rate 25%, with concatenation used for combining them. Note that we can certainly replace our image encoder with more sophisticated compression encoders to further improve the compression rate [5].'\",\"653\":\"'Illustration of the fully convolutional architecture of the networks we use for view-based and map-based labelling. Depth (view-based) or height (map-based) and RGB encodings are fused at every encoding layer. Both encoders consist of 8 convolutional modules. Deconvolutional modules consist of one upsampling and two convolutional layers.'\",\"654\":null,\"655\":\"'Our proposed text parser provides knowledge representation that is not observed in the image. All text descriptions are firstly tokenised and encoded using pretrained Glove [26] and then parsed through bidirectional LSTM [27] serving as an encoder. The hidden state of the encoder is concatenated with the one-hot vector representing detected visual attributes and parsed through a linear layer to obtain a list of text attributes. The training is performed in a fully supervised way with the use of multilabel classification loss (Binary Cross Entropy).'\",\"656\":null,\"657\":null,\"658\":\"'In this paper an automated data labeling (ADL) neural network is proposed to streamline dataset collecting for real-time predicting the continuous motion of hand and wrist, these gestures are only decoded from a surface electromyography (sEMG) array of eight channels. Unlike collecting both the bio-signals and hand motion signals as samples and labels in supervised learning, this algorithm only collects unlabeled sEMG into an unsupervised neural network, in which the hand motion labels are auto-generated. The coefficient of determination (R 2 ) for three DOFs, i.e. wrist flex\\/extension, wrist pro\\/supination, hand open\\/close, was 0.86, 0.89 and 0.87 respectively. The comparison between real motion labels and auto-generated labels shows that the latter has earlier response than former. The results of Fitts\\u2019 law test indicate that ADL has capability of controlling multi-DOFs simultaneously even though the training set only contains sEMG data from single DOF gesture. Moreover, no more hand motion measurement needed which greatly helps upper limb amputee imagine the gesture of residual limb to control a dexterous prosthesis.'\\n\\n'In this paper, a myoelectric regression model based on automated data labeling neural network was designed to decode low-dimensional hand motion from high-dimensional sEMG signals. Compared with the traditional supervised learning model, the proposed ADL only rely on unlabeled sEMG data to learn the gesture regression model. ADL has a hierarchical network with three layers, the first layer is spatial-temporal feature (STF) layer, which extracts temporally and spatially related high-dimensional principal component features from redundant sEMG signals. The second layer is muscle synergy feature(MSF) layer that uses AEN to learn low-dimensional muscle synergy features. The third layer is mapping layer of muscle synergy feature to actual motion intention. This layer is actually a regression neural network layer, the motion label used by this layer is extracted from MSF layer. In summary, this method breaks through the traditional pattern recognition mindset that classifying discrete hand gestures. Comparing with traditional PR methods, the proposed ADL networks are superior to predict the continuous and simultaneous motion of hand and wrist, as well as the force level during the static gesture.'\\n\\n'This paper proposes an ADL algorithm for real-time predicting the simultaneous hand gestures of three DOFs, which are only decoded from a sEMG array of eight channels. We utilize the unlabeled sEMG to auto-generate the potential hand motion so as to simplify dataset labeling. The comparison between real motion label and auto-generated label shows that the latter has earlier response than former. The results of Fitts\\u2019 law test indicate that ADL has capability of controlling multi-DOFs simultaneously even though the training set only contains sEMG data from single DOF gesture. Moreover, No more hand motion measurement needed which greatly helps upper limb amputee learn the gesture of residual limb to control a dexterous prosthesis.'\",\"659\":null,\"660\":\"'It can be supposed that human\\u2019s social perception of robots will most likely differ from their social perception of other humans, and we claim that understanding how humans will augment their social density with robots (compared to other humans or pets) will require people to have a real cultural experience of large scale robot integration. But it can already be shown that the fact that such navigation dimensions may need to be variable is coherent with our review of a small selection of works on the widely studied domain of proxemics, which relates to dimensions such as movement, touch, distances and gaze between humans. These works show that beyond the purely \\\"technical\\\" reasons for movement and navigation, there is also an aspect of communication and interaction encoded in the variations of these dimensions, even when they are subtle. Furthermore, these works show that the communicative value attributed to such variations depends heavily on culture, situation and the relative roles of the interacting humans. In contrast, current Social Navigation (SN) methods treat such dimensions as constraints, user preferences, or fixed parameters, rarely considering their impact on Human-Robot Interaction (HRI) and the acceptance and evaluation of the robot.'\",\"661\":null,\"662\":\"'Our special consideration in parameterization is to make our network less sensitive to object shapes, which are mostly encoded in data along column axis in\\\\no\\\\nt\\\\nm\\\\n. So we design a large kernel size on the column direction followed by a max-pooling layer. We use DDPG [23] for training. Our actor and critic network structure are illustrated in fig. 4.'\",\"663\":null,\"664\":null,\"665\":null,\"666\":null,\"667\":null,\"668\":\"'A render of the U-Joint connecting bodies of the proposed serpentine robot. Timing belts with tension idlers are used for transmission because of the simple mechanical design, high load rating, and low backlash. Magnetic encoders are placed on the axis of rotation for absolute position feedback and redundant sensing.'\\n\\n'Two AS5048B I2C 14-bit magnetic encoders are placed on the U-Joint, as seen in Fig. 3. These provide absolute sensing of the rotary axis with 0.022\\u00b0 resolution. The optical encoders on the motors provide redundant sensing to detect slipping and sensor failure.'\\n\\n'Three Maxon Motor ESCON 50\\/5 brushless motor drivers are attached to the cape. The motors have Hall effect sensors and optical encoders, which are directly connected to their respective motor drivers. Therefore, the motor drivers have the option of closed loop torque and velocity control. Currently, all motors are set in velocity control mode and the current readings are sent via an analog signal to the BeagleBone Black. The motor drivers have multiple built-in safeties, including temperature, voltage, and current protections, to ensure safe operation of the motors.'\\n\\n'The ancillary components are located on separate breakouts. The magnetic encoders (AS5048B) and IMU with temperature sensor (BNO055) both communicate with the BeagleBone Black via I2C. The network switch (IP175G) connects the incoming and outgoing Ethernet to the segment and the BeagleBone Black. To improve signal integrity and for galvanic isolation when operating on batteries in future iterations, the network switch uses a transformer for decoupling.'\",\"669\":\"'Once we obtain all the field data, we label each single measurement in two aspects. First, this paper transform all the B-scan measurement into color image encoded with \\u2019Hot\\u2019 {xmin, ymin, xmax, ymax}. Besides the bounding box, we also assign a dielectric value for each measurement as the ground truth. The system is used to predict dielectric is used to estimate the depth of the subsurface objects.'\",\"670\":\"'Segment the stems using an encoder-decoder CNN to separate the stems from the background and leaves.'\\n\\n'A dataset with more than 1200 labelled images of rose stems used to train the encoder-decoder CNN to segment rose bushes2.'\\n\\n'The image from the left camera is used as input to an encoder-decoder CNN with residual connections to segment the stems from the background, similar to [21]. The network outputs a binary image where it assigns a value of 1 to all the pixels that form part of a stem and 0 to the background. This network outperforms most of the state-of-the-art segmentations for the branch segmentation task [22]. The binary output is used to mask the disparity image to obtain only the disparities of the stems. This masked disparity is then converted into a pointcloud. Figure 3c shows the pointcloud of the bush after performing the segmentation and post-processing.'\",\"671\":null,\"672\":\"'In this paper, binary coding is used to encode parametric joints corresponding to line 1 in algorithm 1. For n \\u2212 m parameters, the corresponding code Bin is set in (4).'\",\"673\":null,\"674\":\"'Inverse kinematics is a fundamental challenge for articulated robots: fast and accurate algorithms are needed for translating task-related workspace constraints and goals into feasible joint configurations. In general, inverse kinematics for serial kinematic chains is a difficult nonlinear problem, for which closed form solutions cannot easily be obtained. Therefore, computationally efficient numerical methods that can be adapted to a general class of manipulators are of great importance. In this paper, we use convex optimization techniques to solve the inverse kinematics problem with joint limit constraints for highly redundant serial kinematic chains with spherical joints in two and three dimensions. This is accomplished through a novel formulation of inverse kinematics as a nearest point problem, and with a fast sum of squares solver that exploits the sparsity of kinematic constraints for serial manipulators. Our method has the advantages of post-hoc certification of global optimality and a runtime that scales polynomially with the number of degrees of freedom. Additionally, we prove that our convex relaxation leads to a globally optimal solution when certain conditions are met, and demonstrate empirically that these conditions are common and represent many practical instances. Finally, we provide an open source implementation of our algorithm.'\\n\\n'an open source implementation and experimental analysis of our algorithm in MATLAB.1'\",\"675\":null,\"676\":null,\"677\":\"'To extract the structural features\\u2013i.e., in our case, a family of salient planes in the scene\\u2013we use an encoder-decoder network with skip connections and multiple levels of scales for plane estimation [13]. This network predicts plane segmentation maps and plane parameters. As in [30], an input image is passed through the encoder to produce a set of high-level feature maps and then the decoder upsamples the feature maps via a series of deconvolution layers to infer the plane segmentation maps with m+1 channels including the non-planar class where m is a hyper-parameter specifying the number of planes. The multiple scales allow the network to abstract feature maps and the skip connections help preserve high-level information. Another branch of the network infers plane parameters pn = {an,bn,cn} where n is the plane index. A 3D point X belongs to plane\\\\np\\\\nT\\\\nn\\\\nX=1\\\\n. This branch shares the same encoder for the plane segmentation maps and has two fully connected layers whose output is 1 \\u00d7 3m of the m planes.'\\n\\n'With the estimated plane segmentation maps and its parameters, we predict 6-DoF poses from the pose network. The pose network has a similar structure as the encoder of the plane network, but takes the image and the plane segmentation map as inputs. We reduce m+1 channels of the plane segmentation map to one without loss of information by using the softmax and argmax operations. We then pass a concatenation of the image and that one-channel plane segmentation map through the convolution layers.'\\n\\n'The encoded feature is passed to the two separate, fully-connected pose-prediction layers to generate the position of the camera t and its orientation r encoded as the Euler angles [21]. We minimize a pose loss function LRT as below:'\",\"678\":null,\"679\":\"'Such predictive models are often parameterized to en-code variations in decision-making between different people. However, since modeling human behavior and how people make decisions a priori is challenging, predictors can maintain a belief distribution over these model parameters [16], [17]. This stochastic nature of the predictor naturally incorporates the model parameter uncertainty into future human state uncertainty [4], [18], [19]. Importantly, once deployed, the robot can observe human behavior and update the distribution over the model parameters to better align its predictive model with the observations.'\",\"680\":null,\"681\":null,\"682\":\"'https:\\/\\/youtu.be\\/JuUn4DIa0-w'\",\"683\":\"'Algorithm 1: Pseudo code for swarm size estimation'\",\"684\":\"'We conducted experiments testing both algorithms in Kilosim, an open-source Kilobot simulator we developed that is able to run at over 700\\u00d7 real speed for 100 robots [17], allowing us to thoroughly investigate the parameter space. A demonstration video is available on YouTube [18]. All experiments were conducted with 100 robots in a 2.4 m \\u00d7 2.4 m arena. To investigate the performance in settings of varying difficulty, we tested five different fill ratios f: 0.52, 0.55, 0.6, 0.7, 0.8. Fill patterns (as seen in Fig. 1) were generated for each trial by pseudo-randomly filling a 10 10 grid of squares with black or white to match the fill ratio. Trial duration was capped at 50,000 s (\\u2248 14 hours) each.'\",\"685\":\"'Video recordings from all 10 experimental trials and additional resources (models, the Nadzoru tool, the used source code) can be found in the online supplementary material [24].'\\n\\n\\\"We extended the open source software tool Nadzoru [18], [19] to support public events in the modelling of free behaviour models and specifications. Using the graphical interface, the user can flag an event as public. If the event is controllable, they also need to choose a corresponding public uncontrollable event. No further action is needed by the user. The implementation of the extended framework automatically generates the controller's source code.\\\"\",\"686\":\"'Robot simulators provide an easy way for evaluation of new concepts and algorithms in a simulated physical environment reducing development time and cost. Therefore it is convenient to have a tool that quickly creates a 3D landscape from an arbitrary 2D image or 2D laser range finder data. This paper presents a new tool that automatically constructs such landscapes for Gazebo simulator. The tool converts a grayscale image into a 3D Collada format model, which could be directly imported into Gazebo. We run three different simultaneous localization and mapping (SLAM) algorithms within three varying complexity environments that were constructed with our tool. A real-time factor (RTF) was used as an efficiency benchmark. Successfully completed SLAM missions with acceptable RTF levels demonstrated the efficiency of the tool. The source code is available for free academic use.'\\n\\n'(a) An occupancy map that was built from 2D Hokuyo LRF sensory data at Laboratory of Intelligent Robotic Systems. The map encodes occupancy data where white pixels represent free cells, black pixels are occupied cells and gray pixels are not yet explored; (b) a grayscale image, which represents uneven mountain landscape'\\n\\n'The main contribution of this paper is LIRS World Construction Tool (LIRS-WCT) - a new efficient tool for creating a realistic 3D virtual environment of Gazebo world from an arbitrary 2D image or 2D LRF data. A practical usability of worlds, which are created by LIRS-WCT, was evaluated by running the three selected SLAM algorithms with Husky robot and different complexity maps. Moreover, the new tool performance was compared to the only open source Gazebo world construction tool [12], [13] and it demonstrated a significantly better performance in terms of RTF.'\\n\\n'To the best of our knowledge at the moment the only open source project that allows converting grayscale images into Gazebo-compliant 3D map is a tool by Lavrenov and Zakiev [12], [13]. It provides an automatic image processing before importing it into Gazebo simulator as a heightmap and allows for noise filtering, original image cropping and resizing, and automatic converting of images to grayscale. The tool receives an image as an input (Fig. 1 demonstrates input examples) and generates a SDF-world file, which describes objects and environment in XML format [14]. Figure 2 shows the world, which is stored as a SDF-element heightmap, with Turtlebot robot [15] and a pair of Hector quadrotor UAVs [16].'\",\"687\":\"'https:\\/\\/github.com\\/jocacace\\/arva_sim'\\n\\n'https:\\/\\/github.com\\/jocacace\\/arva_sim\\/wiki\\/ARVA-Gazebo-ROS-plugins'\\n\\n'https:\\/\\/github.com\\/jocacace\\/arva_sim'\\n\\n'https:\\/\\/github.com\\/jocacace\\/arva_sim'\\n\\n'https:\\/\\/github.com\\/jocacace\\/arva_sim'\\n\\n'https:\\/\\/github.com\\/jocacace\\/arva_sim\\/wiki\\/ARVA-Gazebo-ROS-plugins'\",\"688\":\"'https:\\/\\/chirikjianlab.github.io\\/chairimagination\\/'\\n\\n'https:\\/\\/chirikjianlab.github.io\\/chairimagination\\/'\",\"689\":\"'The simulator communicates with a controller using an efficient cross-application messaging system. This decoupling enables control code to switch between interacting with the simulator and real robot with minimal change. The robot is controlled using either joint trajectories, velocities, accelerations, or forces through a configurable PID controller. Specific sensors such as force, depth images, RGB images, object segmentation, object poses, 2D bounding boxes, and 3D cuboids can be added\\/subscribed. At runtime, the scene can be modified such as changing the poses of object(s), camera(s) and\\/or robot(s). Physical objects in the simulator are represented using URDF which facilitates the introduction of new robots or objects. The simulator is designed for fast iterations and easy experimentation, by making the physics engines switchable at runtime, simplifying URDF import to dragging and dropping the file, and other facilities to emphasize ease of use. In addition, the simulator includes domain randomization tools such as randomizing textures, lights, colors, and object physical properties, to facilitate sim-to-real transfer.'\",\"690\":null,\"691\":\"'https:\\/\\/sites.google.com\\/view\\/ke-to'\\n\\n'We evaluate the proposed framework in three tool manipulation tasks: hammering, pushing and reaching. Each of the task requires a different strategy of grasping and manipulation for achieving the task goal. Our framework outperforms baselines using end-to-end neural network policy and hard-coded keypoints in terms of the achieved task success rate. We qualitatively demonstrate the learned keypoint representations by visualizing the predicted keypoints, as well as inversely generating the optimal tool objects given the keypoints. Video results can be found at sites.google.com\\/view\\/ke-to'\",\"692\":null,\"693\":\"'To calculate the distance of the object to the environment, we used the Flexible Collision Library (FCL). In the FCL scene, we represented only the peg as well as the objects that the robot touches: a simple table surface and a box. All code was written in Python and is planned to be released as a ROS package.'\",\"694\":null,\"695\":null,\"696\":\"'This work was supported by JSPS Grants-in-Aid for Scientific Research (A) 17H06291. We thank Dr. Raluca Scona for opening source the codes of StaticFusion. The discussion with her is very helpful.'\",\"697\":\"'This section describes how the networks for each inference technique were trained. Full details of hyper-parameters can be found in the code associated with this work.'\\n\\n'The experiments in this paper use the CARLA simulator, a state-of-the-art, open-source simulator for autonomous driving research [8]. However, we stress that any simulator can be used within this framework, assuming it can simulate car trajectories, and generate images that can be used by the controller. All training data, which consists of (image, steering angle) pairs, was acquired within the CARLA simulator, either through manual driving or use of the built-in autopilot. During experiments, we also make use of the car\\u2019s trajectory data, which is provided in the form of a list of GPS coordinates from the simulator. Images are converted to grayscale and scaled to a size of 64 \\u00d7 48 pixels, and steering angles (recorded between -1 and 1) are binned into intervals of tenths. The data recorded consists of three scenarios: a right turn on a roundabout and a straight segment of a road with and without an obstacle (stationary vehicle). It is possible to vary the weather within the simulator, however the weather condition in all of the training data is \\\"clear noon\\\".'\",\"698\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34621556'\",\"699\":\"'The overview of the RL-assisted fault-tolerant control. The vehicle model is identified and the control algorithm is obtained through source code or reverse engineering. The information is used in a dynamic simulation where randomized attacks are injected. Using reinforcement learning, a fault-tolerant policy is optimized to maintain vehicle control under various attacks.'\",\"700\":\"'Our network consists of a ResNet-50 encoder with an output stride size of 8 and no global pooling layer. We then pass the output of the encoder through three upsample blocks consisting of a bilinear resize, concatenation with the input image, and then two convolutional layers to match the output resolution to the input. The probability distribution that the network outputs is discretised over 64 channels.'\",\"701\":null,\"702\":\"'http:\\/\\/mrsl.grasp.upenn.edu\\/yashm\\/ICRA2020.mov'\",\"703\":null,\"704\":\"'https:\\/\\/github.com\\/cvas-ug\\/simple-reactive-nn'\\n\\n'https:\\/\\/youtu.be\\/z7kUW9yyka0'\\n\\n'https:\\/\\/github.com\\/cvas-ug\\/simple-reactive-nn'\",\"705\":null,\"706\":null,\"707\":null,\"708\":null,\"709\":null,\"710\":\"'The picture depicts the learning process. Each information collected during the teaching is processed and encoded by DMPs. Finally, object related information (DMPs weights, point cloud, etc\\u2026) are stored in a data base.'\",\"711\":null,\"712\":null,\"713\":\"'Our experimental platform is the open-source iCub humanoid robot with 53 DoF. We conducted experiments on iCub with 7 DoF on each arm and 9 DoF on each hand. In our framework, the robot learns the skills as geometric constraints. Solutions in the nullspace of these constraints are desired poses for the robot to perform this learnt skill.'\",\"714\":\"https:\\/\\/github.com\\/epfl-lasa\",\"715\":null,\"716\":\"'To encode global task specifications we use LTL, allowing users to design global goals for swarm execution. In addition to this, global goals enable scalability (i.e. goals are independent of the swarm size) and reduce cognitive load on the designer as they do not have to assign each agent a specification. This type of interaction modality is easily adapted from temporal logic formula, in addition to providing formal guarantees for global specification satisfaction.'\\n\\n'The algorithm developed in this paper, provided in pseudocode format in Algorithm 1, can be described in four steps: (1) given the initial state of the agent product automaton, find the cost of transitioning to the next state using cross-entropy and the cost function assigned to the agent, (2) if this state is contained in the decomposition set, check all other agent cost functions and, (3) if an agent has a lower total cost, switch to this agent for the remainder of the algorithm or until a new switch is determined, (4) this process in continued until the end state is found and corresponding trajectories are returned to all agents for execution.'\",\"717\":\"'T encodes the fact that only one robot is required to execute each of the tasks. According to'\\n\\n'results in the former allocation, which satisfies the global specification encoded in \\u03c0'\\n\\n'This paper introduces an adaptive task allocation and execution framework for heterogeneous teams of robots which perform a set of given tasks. This was achieved by updating a parameter which encodes the effectiveness of the robots at accomplishing the tasks, thereby allowing the task allocation algorithm to reassign tasks to robots based on their current capabilities towards perfroming the different tasks. Experimental results showcase the efficacy of the approach in various multi-robot experiments conducted on the Robotarium.'\",\"718\":null,\"719\":null,\"720\":null,\"721\":\"'We used an open source code for PILCO [31]. On each primitive, PILCO learned the policy to reach the desired position. In the fit, align, and insert primitives, the timestep T was 15, 10, and 10, respectively. We used the reward function given in Eq. (3) to reduce the position error. The control frequency was 20 Hz. Two initial data collection trials in which the robot provides uniform random actions, were conducted, and the number of learning trials was 20. We evaluated the errors between the desired and current positions on each trial in the two methods. We expected that our method would show better performance since the area of exploration was smaller than that in the other method.'\",\"722\":\"'https:\\/\\/github.com\\/ja3067\\/Titan'\\n\\n'We see great potential for Titan as a platform to expand upon. Our lab has already explored using Titan as a base engine to model robotic components, and to perform topology optimization. Since objects can be simulated and have their components dynamically altered in response to analytics computed in real time on the CPU, Titan opens up many possibilities from simulating adaptive materials to programmable matter. Future additions to Titan could add new CUDA kernels for more accurate time integration methods. Collision detection between meshes could make Titan more useful for multi-agent systems. We are excited that the nature of Titan as an open-source project will allow researchers to modify and improve its components as necessary. We provide a Python API to make the Titan library accessible to the robotics community.'\\n\\n'https:\\/\\/github.com\\/ja3067\\/Titan'\\n\\n'Code'\\n\\n'Code is available at https:\\/\\/github.com\\/ja3067\\/Titan.'\",\"723\":\"'Due to uncertainty in the manufacturing process, open-sourced hands differ in size, weight, friction and inertia [5]. For example, 3D-printed units of the same hand model often differ in their mechanical properties due to variations in fabrication. Consequently, precise analytical models for such hands are often unavailable, as they are hard to derive. Thus, and as in previous work [4], data-based modeling enables useful predictions and can be used for motion planning and closed-loop control. Nevertheless, the accuracy of a learned model is limited, even with increased data. For instance, neural networks with a fixed number of units and layers have a limited capacity to fit functions. Their accuracy begins to plateau after utilizing a certain amount of data for training.'\",\"724\":\"'Humans, in comparison to robots, are remarkably adept at reaching for objects in cluttered environments. The best existing robot planners are based on random sampling of configuration space- which becomes excessively high-dimensional with large number of objects. Consequently, most planners often fail to efficiently find object manipulation plans in such environments. We addressed this problem by identifying high-level manipulation plans in humans, and transferring these skills to robot planners. We used virtual reality to capture human participants reaching for a target object on a tabletop cluttered with obstacles. From this, we devised a qualitative representation of the task space to abstract the decision making, irrespective of the number of obstacles. Based on this representation, human demonstrations were segmented and used to train decision classifiers. Using these classifiers, our planner produced a list of waypoints in task space. These waypoints provided a high-level plan, which could be transferred to an arbitrary robot model and used to initialise a local trajectory optimiser. We evaluated this approach through testing on unseen human VR data, a physics-based robot simulation, and a real robot (dataset and code are publicly available 1 ). We found that the human-like planner outperformed a state-of-the-art standard trajectory optimisation algorithm, and was able to generate effective strategies for rapid planning- irrespective of the number of obstacles in the environment.'\",\"725\":\"'In this section, we give an overview of the proposed rearrangement planning algorithm (the pseudocode is shown in Alg. 1) followed by detailed descriptions. We also provide example executions of the monotone and non-monotone problems.'\",\"726\":null,\"727\":null,\"728\":null,\"729\":null,\"730\":null,\"731\":null,\"732\":\"'https:\\/\\/github.mit.edu\\/mCube\\/residual-pybullet-public'\\n\\n'We first present the simulation framework with an illustrative example. We then discuss the step-by-step implementation of the approach. We conclude the paper with results on an empirical data-set and a discussion of related work. The code and data for this paper is hosted at https:\\/\\/github.mit.edu\\/mCube\\/residual-pybullet-public.'\\n\\n'https:\\/\\/github.mit.edu\\/mCube\\/residual-pybullet-public'\",\"733\":null,\"734\":null,\"735\":null,\"736\":null,\"737\":null,\"738\":\"'Real-time human activity recognition plays an essential role in real-world human-centered robotics applications, such as assisted living and human-robot collaboration. Although previous methods based on skeletal data to encode human poses showed promising results on real-time activity recognition, they lacked the capability to consider the context provided by objects within the scene and in use by the humans, which can provide a further discriminant between human activity categories. In this paper, we propose a novel approach to real-time human activity recognition, through simultaneously learning from observations of both human poses and objects involved in the human activity. We formulate human activity recognition as a joint optimization problem under a unified mathematical framework, which uses a regression-like loss function to integrate human pose and object cues and defines structured sparsity-inducing norms to identify discriminative body joints and object attributes. To evaluate our method, we perform extensive experiments on two benchmark datasets and a physical robot in a home assistance setting. Experimental results have shown that our method outperforms previous methods and obtains real-time performance for human activity recognition with a processing speed of 10 4 Hz.'\",\"739\":\"'Demonstration of Hospital Receptionist Robot with Extended Hybrid Code Network to Select Responses and Gestures'\\n\\n\\\"Task-oriented dialogue system has a vital role in Human-Robot Interaction (HRI). However, it has been developed based on conventional pipeline approach which has several drawbacks; expensive, time-consuming, and so on. Based on this approach, developers manually define a robot's behaviour such as gestures and facial expressions on the corresponding dialogue states. Recently, end-to-end learning of Recurrent Neural Networks (RNNs) is an attractive solution for the dialogue system. In this paper, we proposed a social robot system using end-to-end dialogue system in the context of hospital receptionist. We utilized Hybrid Code Network (HCN) as an end-to-end dialogue system and extended to select both response and gesture using RNN based gesture selector. We evaluate its performance with human users and compare the results with one of the conventional methods. Empirical result shows that the proposed method has benefits in terms of dialogue efficiency, which indicates how efficient users were in performing the given tasks with the help of the robot. Moreover, we achieved the same performance regarding the robot's gesture with the proposed method compared to manually defined gestures.\\\"\\n\\n\\\"We address the research question on how we can apply end-to-end dialogue system to the robot. A different approach may be necessary to build a robot system because end-to-end based dialogue system which calculates the dialogue state with a trained hidden state cannot manually define a robot's behaviour such as a gesture, expression and so on. We aim to fill this gap by demonstrating how we build a receptionist robot using the end-to-end dialogue system. There are two aspects of HRI with the end-to-end approach; 1) how end-to-end dialogue system is applied to the robot and 2) how we make the robot express its behaviour. For the sake of this, we propose trainable components to generate not only the response but robot to execute its gestures. Note that we only focus on generating robot\\u2019s gesture as a first attempt to HRI with end-to-end approach. We utilized Hybrid Code Network (HCN) and extended to produce a response with selected gesture. We applied Recurrent Neural Network (RNN) to select the robot\\u2019s gesture depending on the system response generated from HCN.\\\"\",\"740\":null,\"741\":null,\"742\":null,\"743\":\"'https:\\/\\/vimeo.com\\/348728254'\",\"744\":null,\"745\":\"'We provide both real and simulated data (including simulation code) of the same setup1.'\\n\\n'We evaluate a few widely-used dynamics-learning methods on this dataset and release the code of all implementations and evaluations1. The main insights we gained can be summarized as follows: The iid test error of a model (i.e. its accuracy under the controller from which the training data was generated) can be a poor indicator of its transfer error (i.e. the accuracy under a different controller). This effect appears to be more pronounced for very flexible models, e.g. neural networks (NNs) and Gaussian processes (GPs), than for more constrained models, e.g. linear. This suggests that it may be important to evaluate dynamics-learning methods in terms of their transfer performance, rather than only their iid error.'\\n\\n'For experimental evaluation, we selected a diverse set of algorithms, each representing a different family of methods. All the algorithms are implemented in the open-source code repository introduced in Section I.'\",\"746\":\"'For our point-mass dataset, state code of agents is the concatenated position and velocity components along both dimensions (si \\u2208 \\u211d4). We predict the acceleration vector of length 2 for each agent. Velocity vector for next state is not predicted by the network directly, rather we compute it from acceleration and current velocity. Finally, next position is computed from the current position and predicted velocity. Number of neurons in both layers of function h is 64. The first layer of function f consists of 64 neurons while second layer has 8 neurons. Therefore, the output of function f is length 8 vector which is reshaped in to a matrix of size 4 \\u00d7 2 for the following dot product layer. Iij\\u2019s are matrices of size 4 \\u00d7 2. First layer of functions gi\\u2019s are of size 4 and are shared among all agents. Outputs from the first layers of gi\\u2019s are reshaped in to matrices of size 2\\u00d72 for the following dot product layers (one for each agent). We also add agent-wise bias in these dot product layers. Table I shows the total number of parameters and FLOP count of the used network for N agents.'\\n\\n'Same implementation is used for predator-swarm interaction dynamics and the Kuramoto model except the changes required for state code dimension. For Kuramoto model, phase of the oscillating agents are used as the state code (si \\u2208 \\u211d).'\\n\\n'MLP We use a baseline MLP that takes the concatenated state codes from all agents as input and predict the same for next timestep. This configuration does not share any weights among agents and therefore, is not scalable with number of agents. For four-agent system, we use three hidden layers, each of size 64, followed by two layers of size N \\u00d7dim(si), where dim(si) denotes the dimension of vector si. Size of the network is chosen to have similar parameter count with MagNet.'\\n\\n'LSTM We use a baseline LSTM that uses state codes from previous four timesteps to predict the next state. Similar to baseline MLP, the LSTM model does not share any weights among agents and therefore, is not scalable with number of agents. For four-agent system, we use a two-layer LSTM (each layer is of size 64). The LSTM core is preceded by a linear layer of size 64 and is followed by a output linear layer of size N \\u00d7 dim(si).'\",\"747\":\"'Moreover, though this experiment featured a batch of nearly the same 3D-printed shoe insoles, our ultimate goal would be to extend this robotic system to a various types of 3D-printed parts. For example, in many cases, a batch of 3D-printed parts may contain a number of different parts. The cleaning routines, hence, would have to be altered to cater to the different geometries of each parts. To achieve that, the state machine can be encoded with specific cleaning instructions for each type of part. Much in the same way human operators change cleaning routines for different parts, our robotic system may recognize different parts, look up a prior defined handling instructions, perform the cleanliness evaluation and execute accordingly.'\",\"748\":null,\"749\":null,\"750\":null,\"751\":\"'http:\\/\\/arxiv.org\\/abs\\/2001.07856'\\n\\n'http:\\/\\/arxiv.org\\/abs\\/2001.07856'\\n\\n'http:\\/\\/arxiv.org\\/abs\\/2001.07856'\\n\\n'https:\\/\\/youtu.be\\/avtlx1X3lo'\\n\\n'A LORD Microstrain 3DM-GX4-25 Inertial Measurement Unit (IMU) provides Kalman-filtered Euler angles and rates of the unicycle body with respect to gravity. Additionally, each servo motor is equipped with a 4096 PPR encoder (i.e., 0.088\\u25e6 resolution) detecting motor shaft angular position and velocity. Full-state feedback control can thus be achieved.'\",\"752\":\"'https:\\/\\/youtu.be\\/K80sV5Xf7v4'\",\"753\":null,\"754\":null,\"755\":null,\"756\":null,\"757\":\"'https:\\/\\/www.mucar3.de\\/icra2020-apriltags'\\n\\n'https:\\/\\/github.com\\/AprilRobotics\\/apriltag'\\n\\n'https:\\/\\/github.com\\/Humhu\\/apriltags'\\n\\n'https:\\/\\/sourceforge.net\\/projects\\/aruco'\\n\\n'https:\\/\\/github.com\\/UniBwTAS\\/apriltags_tas'\\n\\n'With a calibrated camera, AprilTags are often used as artificial landmarks in robotic tasks. Since the tags contain an internal identification code, they can be used for robot localization [3], [4] and navigation - provided that each tag\\u2019s position is known. Otherwise, AprilTags allow the localization and identification of static [5] and dynamic [6], [7] objects holding markers.'\\n\\n'In general, it is not guaranteed that a correctly detected and decoded tag was completely visible in the image. Especially partial occlusions of the tag borders are critical. Whereas in such cases the tag outline is often detected accurately enough for correct decoding, the measured image position is significantly displaced compared to the true position. Figure 8 shows examples for this phenomenon.'\\n\\n'https:\\/\\/github.com\\/AprilRobotics\\/apriltag'\\n\\n'https:\\/\\/github.com\\/Humhu\\/apriltags'\\n\\n'https:\\/\\/github.com\\/UniBwTAS\\/apriltags_tas'\",\"758\":null,\"759\":\"'Localization\\/SLAM algorithms. We ran experiments with three different open source localization and SLAM systems: AMCL [12], Google Cartographer [13] and RTAB-Map [14]. Further, we demonstrate that our method can be used to differentiate the performance between several different algorithms. For all the algorithms we used existing ROS implementations or wrappers.'\",\"760\":null,\"761\":\"'A defining characteristic of intelligent systems is the ability to make action decisions based on the anticipated outcomes. Video prediction systems have been demonstrated as a solution for predicting how the future will unfold visually, and thus, many models have been proposed that are capable of predicting future frames based on a history of observed frames (and sometimes robot actions). However, a comprehensive method for determining the fitness of different video prediction models at guiding the selection of actions is yet to be developed.Current metrics assess video prediction models based on human perception of frame quality. In contrast, we argue that if these systems are to be used to guide action, necessarily, the actions the robot performs should be encoded in the predicted frames. In this paper, we are proposing a new metric to compare different video prediction models based on this argument. More specifically, we propose an action inference system and quantitatively rank different models based on how well we can infer the robot actions from the predicted frames. Our extensive experiments show that models with high perceptual scores can perform poorly in the proposed action inference tests and thus, may not be suitable options to be used in robot planning systems.'\",\"762\":\"'https:\\/\\/github.com\\/Zheyu-Zhuang\\/ur5_joint_calib_toolkit'\\n\\n'https:\\/\\/github.com\\/Zheyu-Zhuang\\/ur5_joint_calib_toolkit'\\n\\n'We jointly calibrate the camera intrinsics, camera pose and tabletop height with respect to the manipulator base frame via an automatic procedure. A calibration board is attached to the end-effector. The joint angles and the images of the calibration board are captured for calibrating the camera pose. We also place the board on the tabletop to calibrate the tabletop height. The camera poses and tabletop height are optimised with collected images. Code of the calibration package is available at: https:\\/\\/github.com\\/Zheyu-Zhuang\\/ur5_joint_calib_toolkit'\",\"763\":\"\\\"Our approach to object finding in clutter, using interactive perception, is built upon an RL-based control algorithm and a color detector. Since object detection in a single image is already a very well studied problem and to simplify our approach, we assume that the target object is of a specific color. In addition, we encode the scene state using a discretized Truncated Signed Distance Field (TSDF) volumetric representation [12]. The agent's next action is determined by an RL algorithm based on the current encoded state and the knowledge obtained from past experiences. Since specifying which actions are good and which are bad is very hard and non-intuitive, supervised methods are not adapted to this task. With such a framework, we show that we can learn a policy that can effectively search an object in a cluttered scene. The main contributions of this paper are:\\\"\\n\\n'To find the objects in clutter, we encode the sensor measurements from the RGB and the depth camera into a vector that is then given to the RL agent. The vector is generated from the volumetric TSDF map and the object detections around the gripper. This information is summarized by discretizing the space into bins centered in the end-effector frame and projected onto the xy -plane. The final state vector is then composed of these 68 values, 2 normalization factors for the TSDF and detection part and the current tilt angle value. The agent computes the next action which is executed by the robot and a feedback, a reward, is provided to the agent. Once the agent finds the object, claims that there is no object in the scene, or reaches the maximum number of time steps, the execution is terminated.'\\n\\n\\\"Depending on the resolution of the voxels, such a representation can contain a very large amount of information. Since it would be infeasible to use this information directly in an RL algorithm, we encode it into a smaller sized vector that can be efficiently used for learning. To do this, we first transform the TSDF enriched with the detection channel to the end-effector frame projected onto the xy-plane. Thus, the grids are centered on the end-effector's position in the xy-plane and are rotated around the z axis such that their x and y axis are aligned with the projected end-effector's x and y axis. We also crop the two volumetric representations in a square centered on the robot to be able to scale to large environments. Thus, the 3D representation contains only the local TSDF and detection values around the robot, whereas the scene and the TSDF can be much bigger. The two 3D grids are then split into two layers, above and below the fingers of the gripper. Each layer is summed along the z coordinate to get 4 flattened maps. These maps are then discretized into a 3 \\u00d7 3 grid, where the middle cell is further separated into a 3 \\u00d7 3 grid. Finally, the 2 layers coming from the TSDF and the 2 other layers containing the detections are normalized separately, respectively by the sum of the TSDF map values and the sum of the detection map values. As a result, we obtain 4 maps encoded with 17 values, which are then concatenated into a 68 dimensional vector, as depicted in Figure 2. The final state vector is composed of 71 values: the previously defined 68 dimensional vector, the 2 normalization factors (to keep track of how well the scene has been explored), and a scalar indicating the current tilt angle of the gripper. Even though the 3D maps are defined in a frame relative to the robot, because we use a projected frame, the tilt of the robot cannot be observed, therefore it is added to the state.\\\"\\n\\n'In future work, we plan to integrate a more sophisticated object detection pipeline such that we do not rely on the fact that the target object has a specific color. Furthermore, we plan to extend our volumetric representation to also encode dynamic objects and avoid the need to \\\"forget\\\" the volume where objects moved.'\",\"764\":\"'We represent the visual inputs as two RGB images (i.e., the query and the workspace images) captured from a monocular camera respectively. To the encoder, we use Resnet18 [22] as the feature extractor for both Iq and Iw so that the images are projected to the same feature space. In order to reduce the computational cost for both encoder and attention operation, we set the output strides of the encoder to 8. The feature maps of Iq and Iw are denoted as\\\\n\\u03d5\\\\ne\\\\n(\\\\nI\\\\nq\\\\n)\\u2208\\\\nR\\\\nH\\\\nq\\\\n\\u00d7\\\\nW\\\\nq\\\\n\\u00d7N\\\\nand\\\\n\\u03d5\\\\ne\\\\n(\\\\nI\\\\nw\\\\n)\\u2208\\\\nR\\\\nH\\\\nw\\\\n\\u00d7\\\\nW\\\\nw\\\\n\\u00d7N\\\\nand then feed to the attention module. For the convenience of description, we denote (\\u2022)flatten as the flattened features of the feature map. For example, the flattened features of \\u03d5e(Iq) and \\u03d5e(Iw) are designated\\\\n(\\\\n\\u03d5\\\\ne\\\\n(\\\\nI\\\\nq\\\\n))\\\\nflatten\\\\n\\u2208\\\\nR\\\\nN\\\\nq\\\\n\\u00d7N\\\\nand\\\\n(\\\\n\\u03d5\\\\ne\\\\n(\\\\nI\\\\nw\\\\n))\\\\nflatten\\\\n\\u2208\\\\nR\\\\nN\\\\nw\\\\n\\u00d7N\\\\nrespectively, where Nq = Hq \\u00d7 Wq and Nw = Hw \\u00d7 Ww.'\",\"765\":\"'Performing 3D object detection on streaming data is however complex. First of all, acquiring accurate 3D information in real world is hard. On one hand, camera data can provide rich appearance features but lack of depth information. On the other hand, though point cloud can precisely provide the position of an object, it is very sparse and thus difficult to represent its appearance. Secondly, how to encode the temporal information in consecutive frames is not obvious. For example, generating 3D scene flow for temporal feature representation will need to determine the correspondence of thousands of points between frames, which is not straightforward and also challenging. Last but not least, due to the sheer numbers of frames in streaming data, frame-byframe detection will introduce tremendous computational costs, which is impracticable for real-life applications.'\",\"766\":\"'To predict the instance segmentation map, only the left feature maps are used. The instance segmentation network consists of a simple decoder; the feature map is processed by three repeating bilinear up-sampling and 3 \\u00d7 3 convolutional layers resulting in a w \\u00d7 h instance segmentation mask. For each instance, the predicted segmentation mask is applied to the estimated local disparity map. To deal with overlapping instance masks, each local disparity is converted to a global disparity, resized to the original box size, and placed in farthest to closest depth order in the scene.'\",\"767\":null,\"768\":\"'Pose-guided Auto-Encoder and Feature-Based Refinement for 6-DoF Object Pose Regression'\\n\\n'Accurately estimating the 6-DoF object pose from a single RGB image is a challenging task in computer vision. Though pose regression approaches have achieved great progress, the performance is still limited. In this work, we propose Pose-guided Auto-Encoder (PAE), which can distill better pose-related features from the image by utilizing a suitable pose representation, 3D Location Field (3DLF), to guide the encoding process. The features from PAE show strong robustness to pose-irrelevant factors. Compared with traditional auto-encoder, PAE can not only improve the pose estimation performance but also handle the ambiguity viewpoints problem. Further, we propose Feature-based Pose Refiner (FPR), which refines the pose from the extracted features without rendering. Combining PAE with FPR, our approach achieved state-of-the-art performance on the widely used LINEMOD dataset. Our approach not only outperforms the direct regression-based approaches with a large margin but also thrillingly surpasses current state-of-the-art indirect PnP-based approach.'\\n\\n'Current auto-encoders learn to reconstruct the object appearance, thus the features with symmetric orientation but similar appearance tend to be close in embedding space and hard to be distinguished. We utilize pose information to guide the training process to extract more suitable pose-related features.'\\n\\n'In direct approaches, the key is to extract features that can perfectly represent the pose. Existing auto-encoders [16], [19] are trained to distill features to recover the object appearance. However, the widely existed geometric symmetry in objects make the appearance in image can be fairly similar in distinct views. These features locate close in latent space and hard to be distinguished, which yields the viewpoint ambiguity problem [18]. We proposed Pose-guided Auto-Encoder (PAE) to introduce pose information to supervise the encoding process to solve this challenging problem.'\\n\\n'We propose Pose-guided Auto-Encoder (PAE), which is able to extract more suitable features for pose estimation by utilizing the pose to guide the encoding.'\\n\\n'A. Autoencoder-based Pose Estimation'\\n\\n'The auto-encoder-based pose estimation approach includes an encoder\\\\nE\\\\nto embed the input\\\\nI\\u2208\\\\nR\\\\nN\\\\ninto the low-dimensional latent space\\\\nF\\u2208\\\\nR\\\\nM\\\\n, and a decoder\\\\nD\\\\nto reconstruct\\\\nI\\\\nfrom\\\\nF\\\\n(Eq. 2). Training with loss in Eq. 3, the encoder\\\\nE\\\\nis able to extract the feature that can represent the input. After training, a feature-rotation codebook can be built (Eq. 1). Given a test image\\\\nI\\\\ntest\\\\n, we first detect and encode the target to feature\\\\nF\\\\ntest\\\\n. And the rotation can be obtained by matching\\\\nF\\\\ntest\\\\nwith\\\\nC\\\\nto find the most similar feature\\\\nF\\\\nmatch\\\\n(Eq. 4). [16] proposed Augmented Auto-Encoder (AAE) by introducing augmentations (translation, scale, lighting, occlusion, etc.) to facilitate\\\\nE\\\\nto distill the \\u2019augmentation-invariant\\u2019 features\\\\nF\\\\n^\\\\n(Eq. 5).\\\\nF\\\\n^\\\\nshows robustness to rotation-irrelevant augmentations. However, the training is still guided by the object appearance reconstruction loss.'\\n\\n'In auto-encoder-based pose estimation approaches [16], [19],\\\\nE\\\\nand\\\\nD\\\\nare trained to reconstruct the input\\\\nI\\\\nto extract features representing the object orientation. They assume the object appearance can accurately describe the rotation. But it can be affected by considerable factors and the object in different poses can show quite similar appearances.'\\n\\n'We propose Pose-guided Auto-Encoders (PAE) and Feature-based Pose Refiner (FPR) to achieve highly accurate pose regression.'\\n\\n'C. Pose-guided Auto-Encoder (PAE)'\\n\\n'The crux of this problem is the features are clustered by the object appearance regardless of the rotation. We introduce pose to guide the encoding to distill features that can better reflect the pose information. The auto-encoder should learn to predict an rotation-related quantity\\\\nP\\\\n. Concretely, each rotation should own a unique\\\\nP\\\\n. And more importantly, the difference in\\\\nP\\\\nshould reflect the similar difference in rotation. In this way, the\\\\nE\\\\nis impelled to distinguish those similar object appearances with different rotations.'\\n\\n'Building codebook When only synthetic data are available, we build the codebook on rendered objects with orientations uniformly sampled from the spherical surface. When real training data are available, the codebook is built on realonly training data. After building the codebook, we employ the cosine distance to evaluate the feature similarity during test.'\\n\\n'Building codebook'\\n\\n'The initially estimated pose is usually not enough accurate in existing approaches, thus pose refinement is needed. In terms of auto-encoder-based approaches, a benefit from pose refiner is the size of codebook\\\\nC\\\\ncan be significantly reduced. Traditionally, a large codebook can provide a more precise result but consuming more time and space, so we need to make a trade-off. However, with a pose refiner\\\\nR\\\\n, we can use a sparse codebook to obtain a low-precision estimate and further refine it with\\\\nR\\\\n.'\\n\\n'In this setting, the PAE-S is trained in class-specific manner and the codebook is built on synthetic data.'\\n\\n'In this setting, we trained one PAE-R model and one FPR refiner for all objects. We only use real data (around 200 samples per object) to build an extremely codebook.'\\n\\n'In this paper, we propose Pose-guided Auto-Encoder (PAE), which leverages pose to guide the encoding process to achieve more accurate pose estimation. Then, we propose Feature-based Pose Refinement (FPR), which can achieve fast and accurate pose refinement. Our FPR is the first learning- and RGB-based pose refiner without the need of rendering. By combing PAE with FPR, we achieve the stateof-the-art performance on LINEMOD dataset, even outperforming the current state-of-the-art PnP-based approaches. Our approach shows that direct regression-based strategy can also achieve highly accurate pose estimation.'\",\"769\":\"'Algorithm 1: Voting decisions for joint primitive detection: plane PPFs are disjoint from all other types. Sphere PPFs are a subset of cylinder PPFs, which are a subset of cone PPFs. This is reflected in the code structure. The conditions NP, PC, AS, and VT are defined in Table II.'\",\"770\":\"'Front-end network with ResNet-18 and back-end net-work with original ASPP and the decoder from DeeplabV3+ [4].'\\n\\n'Front-end network with ResNet-18. The back-end net-work contains one F-ASPP (without feature space resolution) and the decoder from DeeplabV3+ [4].'\\n\\n'Front-end network with ResNet-18. The back-end net-work contains CF-ASPP (without feature space resolution) and no decoder from DeeplabV3+ [4].'\",\"771\":\"\\\"To enable intelligent automated driving systems, a promising strategy is to understand how human drives and interacts with road users in complicated driving situations. In this paper, we propose a 3D-aware egocentric spatial-temporal interaction framework for automated driving applications. Graph convolution networks (GCN) is devised for interaction modeling. We introduce three novel concepts into GCN. First, we decompose egocentric interactions into ego-thing and ego- stuff interaction, modeled by two GCNs. In both GCNs, ego nodes are introduced to encode the interaction between thing objects (e.g., car and pedestrian), and interaction between stuff objects (e.g., lane marking and traffic light). Second, objects' 3D locations are explicitly incorporated into GCN to better model egocentric interactions. Third, to implement ego-stuff interaction in GCN, we propose a MaskAlign operation to extract features for irregular objects.We validate the proposed framework on tactical driver behavior recognition. Extensive experiments are conducted using Honda Research Institute Driving Dataset, the largest dataset with diverse tactical driver behavior annotations. Our framework demonstrates substantial performance boost over baselines on the two experimental settings by 3.9% and 6.0%, respectively. Furthermore, we visualize the learned affinity matrices, which encode ego-thing and ego-stuff interactions, to showcase the proposed framework can capture interactions effectively.\\\"\",\"772\":\"'https:\\/\\/www.turbosquid.com\\/3d-models\\/c-3po-star-wars-3d-obj\\/903731'\\n\\n'https:\\/\\/github.com\\/gd-goblin\\/NTU_DB_Data_Loader'\\n\\n'https:\\/\\/youtu.be\\/C37Fip1X0Y0'\\n\\n'Based on a unified policy for six motion classes and an encoder-decoder network, we show that our model can sufficiently perform human-robot motion retargeting using the MC method in the non-Markovian environment.'\\n\\n'In order to learn the robot motion encoder-decoder, we need to sample a set of reference robot motion trajectories for each class. We generated a small set of reference motion trajectories \\u03c4i for all classes using V-REP [49] and Choregraphe [50], where\\\\n\\u03c4\\\\ni\\\\n={\\\\nx\\\\nrj\\\\nt\\\\n,\\\\nx\\\\nrj\\\\nt+1\\\\n,\\u22ef,\\\\nx\\\\nrj\\\\nt+T\\\\n}\\\\nfor \\u2200i, and i \\u2208 H = {cheer up,\\u22ef}. The reference motion generation took a few minutes per class on average. Based on the reference motion, augmented training dataset were generated by adding uniform noise to \\u03c4i iteratively as\\\\nx\\\\nr\\\\nj\\\\n\\u2032\\\\nt\\\\n=\\\\nx\\\\nrj\\\\nt\\\\n+\\u03b5\\\\nwhere \\u03f5 = [\\u22120.05,0.05]. We augmented our reference motion trajectories up to 20k frames per class and combined them to learn a unified robot motion encoder-decoder from a total of 120k augmented datasets. The robot motion network consists of three FC layers with Tanh including 256, 128, 64 for the encoder, 7 for latent representation, and identical but in reverse order for the decoder. MSE loss is used for both the skeleton and robot motion networks with learning rate=1.0 \\u00d7 10\\u22124, weight decay=1.0 \\u00d7 10\\u22126 and batch size=128.'\\n\\n'https:\\/\\/github.com\\/gd-goblin\\/NTU_DB_Data_Loader'\",\"773\":\"'Surgical scene understanding and multi-tasking learning are crucial for image-guided robotic surgery. Training a real-time robotic system for the detection and segmentation of high-resolution images provides a challenging problem with the limited computational resource. The perception drawn can be applied in effective real-time feedback, surgical skill assessment, and human-robot collaborative surgeries to enhance surgical outcomes. For this purpose, we develop a novel end-to-end trainable real-time Multi-Task Learning (MTL) model with weight-shared encoder and task-aware detection and segmentation decoders. Optimization of multiple tasks at the same convergence point is vital and presents a complex problem. Thus, we propose an asynchronous task-aware optimization (ATO) technique to calculate task-oriented gradients and train the decoders independently. Moreover, MTL models are always computationally expensive, which hinder real-time applications. To address this challenge, we introduce a global attention dynamic pruning (GADP) by removing less significant and sparse parameters. We further design a skip squeeze and excitation (SE) module, which suppresses weak features, excites significant features and performs dynamic spatial and channel-wise feature re-calibration. Validating on the robotic instrument segmentation dataset of MICCAI endoscopic vision challenge, our model significantly outperforms state-of-the-art segmentation and detection models, including best-performed models in the challenge.'\\n\\n'Network architecture of the proposed AP-MTL model for real-time detection and segmentation. The architecture consists of VGG16 encoder, segmentation decoder like [6], and detection branch like SSD [7].'\\n\\n'We propose a real-time MTL model with a light weightshared encoder and task-aware spatial decoders for detection and segmentation.'\\n\\n'We develop an innovative design of segmentation decoder with fusing bypass connection in scSE to boost up the model performance.'\\n\\n'In this work, we construct a real-time multi-task learning model with light-weight encoder of VGG16 [25] and Skip-scSE decoder, SSD decoder [7] for segmentation and detection tasks respectively. To optimize our MTL model at the same convergence point, we introduce asynchronous task-aware optimization technique (ATO). We also designed a novel skip spatial-channel squeeze and excitation module (Skip-scSE), by integrating skip connection to enhance segmentation prediction. Moreover, to boost real-time computation, reduce singularity, and redundancy in the model, we propose global attention dynamic pruning (GADP) method.'\\n\\n'Our AP-MTL model forms of a shared-encoder, a segmentation decoder, and a detection block (illustrated in Fig. 1). The encoder and detection block adopt from SSD [7] where VGG16 [25] exploit in the encoder section. The segmentation decoder is designed with proposed skip-scSE by following our previous work [6], as shown in Fig. 3. Multi-scale feature maps are extracted from different stages in the encoder network. Feature network\\u2019s lower level score maps are concatenated with high-level score maps to increase parameter learning, and then it gets convoluted and excited. Further excited score maps are upsampled with a deconvolution layer. The final semantic score map will be generated after the last upsampling, which is used to output the prediction results.'\\n\\n'Our proposed Skip-scSE Decoder for segmentation of surgical iinstruments.'\\n\\n'MTL methods are generally heavy in computation and not applicable in real-time, especially for high-resolution images. Every network contains a sub-network, and most networks\\u2019 parameter learning is redundant and insignificant. Besides, redundancy and singularity in the learning parameters are the huge hindrances for task-specific decoders in MTL model. Thus, pruning can solve this over-parameterization and singularity problem. In prior works, network are pruned using Taylor expansion ranking [30] or attention statistics [31]. Nonetheless, pruning filters based on local gradient flow or activation is highly redundant, as it is a function of all previous filters. Therefore, we propose a dynamic pruning method based on global understanding of parameters and channel-wise attention [26], [32]. To avoid initial layer bottleneck pruning, we follow proportional pruning in each layer.'\\n\\n'Overview of our Global Attention Dynamic Pruning (GADP) method. GADP attaches in-between encoder blocks, calculate the rank of the each kernel, and remove low ranked kernels gradually by retraining the model. Attention-based pruning, rank the network parameters accurately which assists to remove redundant and weak parameters.'\\n\\n'In this paper, we present attention pruned multi-task learning model (AP-MTL) for real-time instrument localization and tracking in endoscopic surgery. We introduce attention based dynamic pruning technique to eliminate sparsity and singularity in MTL model\\u2019s shared weight encoder. We present a novel method to optimize the task-aware MTL model to obtain the same optimal convergence point for multi-tasks. To enhance segmentation, we design a novel decoder using skip-scSE to reduce sparsity and redundancy in the decoder. Also, we introduced a novel post-processing method which exploits the benefit of multi-tasking for increasing segmentation accuracy. Further, our model outperforms most of the state-of-the-art architecture by a significant margin. In future work, we extend our research to segment defected tissues along with instruments and learning to exploit the surgical scene representation, compositionality, and reasoning.'\",\"774\":\"'Color-coded ribbon illustration of surgical gesture from a complete video. We present (1) the ground truth (2) recognition results from the policy network (3) rectified predictions by tree search. Each color stands for a gesture class. The method based on tree search could recognize the missing gesture of the policy network.'\",\"775\":\"'Based on the author\\u2019s knowledge, all codes were optimized. Further optimization may exist and may influence the recorded memory usage and training time. The applications of the proposed ACNN are not limited to medical image segmentation, but also could be expanded to other pixel-level tasks, which needs further explorations.'\\n\\n'A new full resolution DCNN - ACNN is proposed for medical image segmentation with the use of cascaded atrous II-blocks, residual learning and IN. A new atrous rate setting is proposed to achieve the largest and fully-covered receptive field with a minimum number of atrous convolutional layers. With much less trainable parameters than that used in the Deeplabv3+ and U-Net, improved accuracy is achieved by even ACNN-II and further improved accuracy can be achieved by deeper ACNN. The derived atrous rate setting contributes to other researches as well. Codes are available at Xiao-Yun Zhou\\u2019s github.'\",\"776\":null,\"777\":\"'We currently approach robot sensors from the perspective of consumers, purchasing whatever seems necessary from a catalogue, then writing program code to make robots useful. This perspective puts practical constraints up front: it is influenced by technologies that are currently available, it limits options to what can be fabricated cheaply and sold profitably. Worse, it relies on roboticists to reason (often only heuristically) about the information needed for a robot to achieve its goals. If there is some notion of task structure, reasoning about it is seldom formalized, and may be tied to assumptions often taken for granted (e.g., for fixed price, greater sensor precision is better). This paper approaches the question of sensors from a more fundamental perspective\\u2014 asking how we might represent and explore conceivable sensors. It is, therefore, of a more theoretical nature.'\",\"778\":\"'https:\\/\\/github.com\\/iitkcpslab\\/TStar'\\n\\n'https:\\/\\/youtu.be\\/gKR4cRLVaM4'\\n\\n'https:\\/\\/github.com\\/iitkcpslab\\/TStar'\",\"779\":\"'https:\\/\\/saras-project.eu\\/'\\n\\n'http:\\/\\/www.ros.org\\/'\",\"780\":null,\"781\":null,\"782\":\"'Foreground object detection under camouflage using multiple camera-based codebooks'\\n\\n'Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of capturing aerial (bird-view) images. The availability of aerial visual data and the recent advances in object detection algorithms led the computer vision community to focus on object detection tasks on aerial images. As a result of this, several aerial datasets have been introduced, including visual data with object annotations. UAVs are used solely as flying-cameras in these datasets, discarding different data types regarding the flight (e.g., time, location, internal sensors). In this work, we propose a multi-purpose aerial dataset (AU-AIR) that has multi-modal sensor data (i.e., visual, time, location, altitude, IMU, velocity) collected in real-world outdoor environments. The AU-AIR dataset includes meta-data for extracted frames (i.e., bounding box annotations for traffic-related object category) from recorded RGB videos. Moreover, we emphasize the differences between natural and aerial images in the context of object detection task. For this end, we train and test mobile object detectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR dataset, which are applicable for real-time object detection using on-board computers with UAVs. Since our dataset has diversity in recorded data types, it contributes to filling the gap between computer vision and robotics. The dataset is available at https:\\/\\/bozcani.github.io\\/auairdataset.'\",\"783\":\"'https:\\/\\/youtu.be\\/mkotvIK8Dmo'\",\"784\":\"'https:\\/\\/grvc.us.es\\/davis-dataset-for-intrusion-monitoring\\/'\\n\\n'The contribution of this paper is three-fold. First, a fully asynchronous scheme for intruder monitoring using UAS is presented. It can be tuned to run in real time adapting to the hardware computational constraints. Second, two asynchronous methods for feature tracking and event clustering are presented. Third, code and datasets will be released in order to contribute to the development of event-based vision community (https:\\/\\/grvc.us.es\\/davis-dataset-for-intrusion-monitoring\\/).'\",\"785\":null,\"786\":\"'A FMU rotating rig was custom-designed and developed to physically simulate the rapid rotating motion of the craft. The rig consists of the FMU mounted on a high-torque T-Motor U10 brushless dc (BLDC) motor. The motor shaft connects to a US Digital E6 Optical Encoder with 10,000 cycles per revolution (CPR) to provide the ground truth angular rate of the rotation. A first Teensy 3.5 microcontroller (MC) interfaces and processes the encoder data and outputs the measured angular frequency via serial to a laptop. The laptop records and saves the measurements. A second Teensy 3.5 MC commands the rotation speed of the rig by sending a pulse-width modulated (PWM) signal to the electronic speed controller (ESC), which directly controls the power input to the BLDC motor. In this experiment, the PWM commanded was 1170 \\u2212 1310\\u00b5s at 10\\u00b5s interval.'\\n\\n'tracks the ground truth encoder frequency \\u03c9truth with a low rms error of 0.0045Hz.'\",\"787\":\"'During the past years, the use of MAVs in applications such as environmental monitoring, aerial filming, surveillance and search and rescue has dramatically increased. However, most commonly used control algorithms lack the necessary robustness needed for the critical applications stated above and they often struggle when aggressive maneuvers are required. To some extent, these problems can be eliminated when the Micro Aerial Vehicle (MAV) model is taken into account in the control design. Model based control approaches such as Model Predictive Controller (MPC) have become more popular in robotics thanks to increasing computational capabilities and improved algorithmic efficiency. The design and implementation of such algorithms on real robots has become significantly easier due to the open source availability of optimisation and control toolboxes such as [1], [2], [3]. At the same time, robust performance under mechanical failures (such as motor failures), can only be achieved when the failure can be correctly identified and appropriately handled by the control design. In our paper we address the problem of aggressive, precise and fault tolerant MAV navigation. Our contributions are as follows:'\",\"788\":null,\"789\":null,\"790\":null,\"791\":\"'https:\\/\\/github.com\\/sportdeath\\/range_mi'\\n\\n'The software is available open-source at https:\\/\\/github.com\\/sportdeath\\/range_mi.'\\n\\n'https:\\/\\/github.com\\/sportdeath\\/range_mi'\",\"792\":\"'A high fidelity visual and physical underwater simulator was developed for generating photo-realistic scenes of shallow underwater environments to obtain both underwater imagery (from an AUV) and aerial imagery (from a UAV) to evaluate the proposed framework. The simulator is based on the open source UAV and car simulator AirSim [15]. Unlike widely used simulation frameworks such as Gazebo [16] and UWSim [17], which feature a realistic physics simulations but simplified graphical outputs, this simulator is based on Unreal Engine 4 [18] and provides sufficiently realistic camera feeds with many of the visual imperfections that would be expected in a real world environment (e.g. colour attenuation, light reflections, moving objects such as fish, and a limited visibility range). In particular, the simulator integrates a model of the open sourced BlueROV AUV platform1 using 6 thrusters that allows movement in 5 degrees of freedom and additional sensors such as an altimeter and a forward-looking stereo vision camera. Figure 1 shows a screenshot from within the simulator in a generated cluttered reef environment. Additionally, the simulator is integrated with the Robotic Operating System (ROS) [19] to allow future deployment of the algorithms to a real target platform.'\",\"793\":null,\"794\":\"'Illustration of our approach. Based on a sequence of measurement grid maps a) the network predicts occupancy and velocity, i.e. a dynamic occupancy grid map b). Our network is a combination of feedforward (red) and recurrent (blue) network layers. The Encoder-LSTM, outlined in c), uses information of a sequence of measurement grid maps to update the internal states and estimates occupancy and velocity. The recurrent skip architecture d) ensures dense predictions at the output. The two upscaling paths use the same skip connections, indicated by the stars.'\",\"795\":\"'http:\\/\\/robotics.jacobs-university.de\\/projects\\/Valentin3D-DE'\",\"796\":\"'https:\\/\\/bitbucket.org\\/fafz\\/bayesian-icp'\\n\\n'The main contribution of this paper is the introduction of Bayesian ICP, an efficient and scalable ICP variant that provides a high-quality approximation over the posterior transformation. This distribution encodes both the expected pose transformation, as provided by other ICP methods, as well as uncertainty information. A C++ implementation of Bayesian ICP is available at https:\\/\\/bitbucket.org\\/fafz\\/bayesian-icp.'\",\"797\":\"'https:\\/\\/youtu.be\\/K348uuCB8gY'\",\"798\":null,\"799\":\"'The proposed BNN, which is named a Bayesian U-net, has a 2D encoder-decoder structure and predicts depth from a single RGB image as an end-to-end model. The architecture of the proposed network is shown in Fig. 2. Depth regression problem resembles semantic segmentation problem because both problems require dense prediction per pixel. Therefore, the network is composed of fully convolutional layers proposed by [25] - which is mainly used in semantic segmentation. A fully convolutional network is not limited by the size of the input, and it also efficiently extracts the location information while using fewer parameters. To maintain deep structure of neural network and to transmit the detail information of the low level features to the decoder, the skip layers are connected from the encoder to the decoder like U-net [26]. Also, summation-based skip connection is used as one of the methods to overcome vanishing gradient problem which is one of U-net limitation [27]. To adjust the size of features, strides of convolutional layers is set to 2 at both downsampling and upsampling path, and center is cropped to match the size of features at skip connection. Dropout places on only half of encoder and decoder to prevent the strong regularization to the model parameters as suggested by [28]. Since MC sampling by dropout produces different T outputs for one input, we obtain the final predictions of depth and uncertainty using mean and variance of T model\\u2019s outputs.'\",\"800\":\"'http:\\/\\/covis.cs.nctu.edu.tw\\/IF-Net\\/'\",\"801\":null,\"802\":\"'The source code of the solver included in GC-RANSAC is available at our webpage.5'\",\"803\":null,\"804\":\"'https:\\/\\/haram-kim.github.io\\/LARR-RGB-D-datasets\\/'\\n\\n'https:\\/\\/haram-kim.github.io\\/LARR-RGB-D-datasets\\/'\",\"805\":null,\"806\":null,\"807\":\"\\\"A methodology for implementing arbitrary foot shapes in the passive walking dynamics of biped robots is developed. The dynamic model of a walking robot is defined in a way that allows shape-dependent foot kinetics to contribute to the robot's dynamics, for all convex foot shapes regardless of the exact foot geometry: for the developed method, only the set of points describing the foot profile curve is needed. The method is mathematically derived and then showcased with an application. The open-source pose estimation system OpenPose is used to determine the foot profile that enables the rigid-foot passive robot to reproduce the ankle trajectory of the actively powered, multi-DOF human foot complex. The passive gait of the biped robot walking on the specified foot shape is simulated and analyzed, and a stable walking cycle is found and evaluated. The proposed model enables the study of the effects of foot shape on the walking dynamics of biped robots, eliminating the necessity of solely using simple, and analytically defined geometric shapes as the walking robots' feet. The method can be used for foot shape optimization towards achieving any desired walking pattern in walking robots.\\\"\",\"808\":null,\"809\":null,\"810\":null,\"811\":null,\"812\":null,\"813\":\"'https:\\/\\/github.com\\/MISTLab\\/CAPRICORN'\\n\\n'https:\\/\\/github.com\\/MISTLab\\/CAPRICORN'\",\"814\":null,\"815\":null,\"816\":null,\"817\":\"'To carry out an experimental validation of our method, we have built a prototype tractor-trailer system using a commercial radio-controlled model (Carson Unimog U300, a 1:12 replica of the Mercedes Unimog U300 truck) which has been modified and instrumented so as to allow the implementation of the proposed controller. In particular, we replaced the original electronics with an Arduino Uno microcontroller board; moreover, we added an H bridge for driving the two DC motors, encoders on wheels and a Bluetooth module for communication.'\",\"818\":null,\"819\":null,\"820\":null,\"821\":null,\"822\":\"'We propose to learn this shape prior as a probabilistic generative model, a denoising version of Variational Autoencoders (VAE) [34], from a set of object point clouds. The model takes a PointNet encoder [35] and a decoder with coarse intermediate reconstruction [36]. The shape prior is captured as an isotropic Gaussian\\\\np(z),z\\u2208\\\\nR\\\\nd\\\\nz\\\\n,\\\\nd\\\\nz\\\\n<<|\\\\nP\\\\nv\\\\n|\\\\n, in the latent space. Similar to [37], the full shape is recovered by searching z given the partial observation Pv:'\",\"823\":null,\"824\":\"'https:\\/\\/youtu.be\\/f59FoS-hV7c.'\\n\\n'https:\\/\\/mcube.mit.edu\\/research\\/tactile_dexterity.html'\\n\\n'https:\\/\\/youtu.be\\/f59FoS-hV7c'\",\"825\":null,\"826\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34422444'\\n\\n'To compare the shape reconstruction results from the FBG and the ROGUE fiber triplets, a stereo camera setup with 1024 \\u00d7 768 resolution was used to track colorized markers (red) attached to the center-line of the CDM. The stereo camera pair was calibrated using the stereo camera calibration toolbox in MATLAB with an overall mean error of 0.12 pixels. For each stereo image pair, the 2D pixel locations of the marker centers were found in each image by applying a color segmentation with experimentally-determined thresholds. The intrinsic and extrinsic parameters from the calibration procedure were then used in custom-written Python code to find the corresponding markers within the two color-segmented images and obtain the 3D location of the markers by triangulation [17]. An erosion morphological operation, followed up with a dilation were applied to the segmented images to remove potential noise in the color segmentation algorithm. Of note, the 0.12 pixel error during the calibration process results in 0.02 mm mean 3D position accuracy error when triangulating and measuring the distance between two markers with known pre-determined spacial locations on a custom-designed validation jig.'\",\"827\":\"'The main LP to be solved offline encodes the state space constraints in polytopic containment forms, including the final polytopic containment constraint YN \\u2286 XG, as well as trying to maximize the volumes of the polytopes. During online execution, if the current state x is in some polytope Yi, then p(x) can be found by solving an LP through Equation (3) and ui(x) can be calculated by Equation (2). By following the control law ui(x), the system is guaranteed to land inside the next polytope Yi+1 and hence eventually it will reach YN \\u2286 XG.'\",\"828\":null,\"829\":\"'The source code is available online1. To the best of our knowledge, LINS is the first tightly-coupled LIO that solves the 6 DOF ego-motion via iterated Kalman filtering.'\",\"830\":null,\"831\":null,\"832\":null,\"833\":null,\"834\":null,\"835\":null,\"836\":null,\"837\":null,\"838\":null,\"839\":\"'The remainder of this work will focus on two issues: (1) how to generally encode different types of geometric association constraints and build more complex geometric skills from them; and (2) how to optimize such constraints given one human demonstration video. Experimental results are reported in Sec. IV.'\",\"840\":\"'Our first design is based on directly estimating scene\\u2019s depth from single image. We employ an encoder-decoder based convolutional neural network architecture proposed by Alhashim et al. [17]. Authors inspire their encoder design from the DenseNet-169 [21], whereas the decoder is composed of 3 up-sampling blocks, each consisting of an up-sampling layer which is then concatenated with respective layer from encoder, similar to the U-net [22] and is followed by 2 convolution layers.'\",\"841\":null,\"842\":null,\"843\":null,\"844\":null,\"845\":\"'Toxicodendron diversilobum'\",\"846\":null,\"847\":\"\\\"One of the rollers and its corresponding motor and bearings is mounted on two sliding bearing platforms attached to linear guides housed in the sides of the device. These platforms are tensioned with constant force linear springs to ensure solid contact between the rollers and the inner robot material. The distance between the rollers passively adjusts as the carriage moves over the layers and the valves as it drives along the inner robot material. Two small screws are used as a mechanical stop to keep a minimum roller distance. A 3-D printed electromagnet housing for a 12 V electromagnet (uxcell) is attached to the back of the motor housing. The wires from the motorized rollers and the electromagnet run all the way through the soft robot body from the carriage to the base. The electromagnet constrains the carriage's design. The device measures approximately 70 mm by 50 mm by 90 mm and weighs 202 g. Much of its length is due to the electromagnet holder, and the electromagnet responsible for about half of its weight. The carriage operated at a speed of approximately 1 cm\\/s for the demonstrations. While the carriage can move much faster, the limiting factor in speed is aligning the electromagnet with the valve positions. The addition of carriage position sensing via encoders could address this.\\\"\",\"848\":null,\"849\":null,\"850\":null,\"851\":null,\"852\":\"'https:\\/\\/biomechatronics.stanford.edu\\/bump-em'\\n\\n'In this paper, we present a modular perturbation system, where each module has an independently controlled motor that can apply forces in a single direction with a rope. It is capable of perturbing a standing or walking subject at the hip with various force profiles, magnitudes of up to 200 N and a rise time as little as 44 ms. We also demonstrate how modules can be used cooperatively to emulate perturbations in any direction in the transverse plane, and can be paired with optional encoders to render force-fields. By providing this research tool to the rehabilitation robotics and biomechanics communities, we hope to enable researchers to improve the collective understanding of how falls occur, how to prevent them, and how humans move in novel environments.'\\n\\n'An optional magnetic encoder can also be used to collect data on the angular displacement of the drive shaft. This enables implementation of state-based control to render virtual environments such as force-fields. The encoder magnet (RMA37A3, Renishaw, United Kingdom) is clamped directly to the shaft, and the encoder (RM22, Renishaw, United Kingdom) is mounted on a 3D printed mount that is attached to the mounting frame. Both the force sensor and motor encoder signals are filtered with a 2nd order Butterworth filter with a cutoff frequency of 60 Hz.'\\n\\n'We designed our modules to be easy for others to replicate. The open-loop configuration is constructed entirely from 3D- printed or catalog components. The reel drum, reel drum covers, encoder mount and mounting frame are made from 3D-printed Polylactic Acid (PLA). We provide the CAD models in the accompanying repository. The brushless motor, motor driver, shunt regulator, power supply, signal amplifier, strain gauge, encoder and other mechanical components are catalog parts. We provide a bill of materials for all these components. For the closed-loop module, the aluminum dogbone can be ordered from Proto Labs (MN, USA). We provide the CAD models in the accompanying repository. The components in the strain gauge wheatstone bridge circuitry are catalog parts, but require some soldering to be assembled, and we provide schematics and photos for how to do so. A complete bill of materials, all design files, and assembly instructions for the system are freely available at https:\\/\\/biomechatronics.stanford.edu\\/bump-em, where we also intend to post future versions of the system.'\\n\\n'We designed the system to be inexpensive. A system with a single module, excluding the optional encoder, costs\\\\n1,718fortheopen\\u2212loopand\\\\n2,389 for the closed-loop configuration. Some items, such as the subject harness, are baseline costs that do not scale with number of modules. Table I lists the cost of a single-module system, which includes baseline costs, and the cost of additional modules for both open-loop and closed-loop configurations.'\\n\\n'We used the optional encoder on the drive shaft to provide estimates of subject displacement from a set point. We applied motor commands to simulate a virtual spring force with spring constants of 250, 500, 750, and 1000 Nm-1 (Fig. 4D). Linear fits to the data (with intercepts forced through the origin) have slopes of 39, 192, 345, and 460 Nm-1 with R2 of 0.16, 0.77, 0.90, and 0.91, respectively, for the open-loop configuration and 174,376, 609, and 660 N m-1 with R2 of 0.90, 0.96, 0.99, and 0.96, respectively, for the closed-loop configuration.'\\n\\n'In this paper, we provide the design and characterization of a modular, open-source perturbation system that can be made entirely out of affordable catalog and 3D-printed components. The modularity of this system makes it useful in a wide range of experiments. The simple design using easily-obtained components makes the system financially and technically accessible and therefore easy to replicate. The system is capable of perturbing subjects with forces of up to 200 N in any direction at the hips, fast enough to display 90% of the desired force before a subject can detect and respond to it, and capable of rendering force-field environments with the use of optional encoders. We provide two versions of the system: an open-loop configuration which is less expensive, simpler to assemble, and suitable for applications requiring less-precise force control, and a closed-loop configuration which is approximately 30-40% more expensive depending on the number of modules in the system, requires assembly of additional sensors, and provides better force tracking, especially during high-movement activities. Both system configurations can serve as useful research tools that will enable researchers from all backgrounds to study how falls occur, how to prevent them, and how humans move in novel environments.'\\n\\n'Providing the research community with a financially and technically accessible research tool would enable more widespread studies on falls. There are many existing perturbation systems that have high functionality, but are specialized and would be difficult to replicate. An accessible, open-source design should be low-cost, made entirely of 3D- printed and catalog components, and come with instructions for component purchase, assembly and control. These features would allow the system to be built and used by any researcher regardless of their level of technical expertise and would avoid future researchers having to design and build their own system for a specific experiment.'\\n\\n'We do not include the controller in the listed costs due to the wide range of options. The system only requires 0-5 V analog inputs and outputs from the controller. Hence, one can use anything from open-source microcontrollers such as Arduino (Arduino, MA, USA) to larger real-time systems such as Speedgoat (Speedgoat, MA, USA).'\",\"853\":\"'The proposed robotic exoskeleton glove contains three soft pneumatic actuators that execute the abduction and adduction motions of the fingers. These actuators are made out of urethane rubber (Smooth-On Vytaflex 40), and they are inspired by the open-source designs found in the Soft Robotics Toolkit [26]. The correct estimation of the limits of the soft actuator in terms of force and motion capabilities is highly important in order to design the best actuator that fits the selected grasping requirements. For this reason, a finite element method (FEM) model was developed to estimate the performance of the actuator according to the available air pressure of the system. Similar structures, like soft reinforced actuators for finger flexion, have been previously modeled [27], [28]. The proposed abduction \\/ adduction actuator can be modeled using the Mooney-Rivlin model for hyperelastic structures [29]. The strain energy density W of the model can be written as:'\",\"854\":\"\\\"The overall intention detection solution is expected to be functional, easy to deploy, low-cost, operational in realtime. The Myo-Armband (Thalmic Labs, ON, Canada) meets all specifications. The armband was positioned around the arm to capture biceps and triceps muscle activities, rather than on the foreman as it was originally designed. Indeed, because the exoskeleton is tied to the palm of the user's hand, the muscle of the forearm will not be relieved by the assistance of the exoskeleton, unlike the biceps and triceps. The Armband is composed of eight pairs of dry electrodes. We optimized the longitudinal placement to maximize the EMG signal empirically. The raw output of the armband is a zero-mean signal coded over 8 bit, it has no unit and is comprised between -126 and +127.\\\"\",\"855\":null,\"856\":null,\"857\":null,\"858\":null,\"859\":null,\"860\":null,\"861\":null,\"862\":\"'https:\\/\\/sites.google.com\\/view\\/dex-pilot'\\n\\n'https:\\/\\/sites.google.com\\/view\\/dex-pilot'\\n\\n'https:\\/\\/sites.google.com\\/view\\/dex-pilot'\",\"863\":\"'https:\\/\\/youtu.be\\/-XOGMFwi8_s'\",\"864\":null,\"865\":\"'To test the effectiveness of our novel algorithm we implemented it and then tested it with two different examples. The code was written in C++ and compiled in Microsoft Visual Studio Professional 2017 (Version 15.9) on Windows 10 Pro 64 bit. The computer that we used to run the test cases contained an Intel\\u00ae Core\\u2122 i7-7700K CPU @ 4.20 GHz (4 cores, 8 threads), a NVIDIA GeForce GTX 970 (4 GB) GPU, and 32 GB RAM.'\",\"866\":\"'Currently, IPS CDC uses PQP_FIT. To demonstrate the effectiveness of our algorithm (i.e., INERTIA_FIT), we implemented it on top of IPS CDC. The code was written in C++ and compiled in Microsoft Visual Studio Professional 2017 (Version 15.9.12) on Windows 10 Pro 64 bit. The computer that we used contained an Intel\\u00ae Core\\u2122 i7-7700K CPU @ 4.20 GHz (4 cores, 8 threads), a NVIDIA GeForce GTX 970 (4 GB) GPU, and 32 GB RAM. For comparison, we have also implemented RAPID2_FIT on top of IPS CDC.'\",\"867\":null,\"868\":\"'To estimate the probabilities of lane selection, we use each final encoder LSTM state which can be expected to encode how the vehicle follows the lane. Each LSTM state passes through \\u03d52( ) and is converted to a one-dimensional vector in order to remove the order dependency of the lanes. These one-dimensional vectors are concatenated and then fed through the softmax function to produce the lane selection probabilities {p0,\\u2026, pM}.'\\n\\n'Each decoder module consists of an LSTM, embedding functions, and a Tailored-KF module. We also assign a weight-shared decoder module to each lane. Each hidden state of LSTM2 in Fig. 3 is updated from the hidden state of the LSTM1 corresponding to the lane. The outputs of the decoder at each timestep and for each lane are the predicted state\\\\nx\\\\nm\\\\nt+1\\\\nand the covariance matrix\\\\nP\\\\nm\\\\nt+1\\\\n, produced through the Tailored-KF module.\\\\nLF\\\\nm\\\\nt+1\\\\nis calculated by using\\\\nx\\\\nm\\\\nt+1\\\\n.\\\\nLF\\\\nm\\\\nt+1\\\\nand\\\\nx\\\\nm\\\\nt+1\\\\nare concatenated and taken as the input to the decoder, recurrently.'\\n\\n'Vanilla LSTM: LSTM encoder-decoder model that directly predicts the position from past trajectories only.'\",\"869\":null,\"870\":null,\"871\":null,\"872\":null,\"873\":null,\"874\":null,\"875\":null,\"876\":null,\"877\":\"'Process Diagram. (Top row) Images are first clustered and then consolidated by an expert to yield a small set of ecologically relevant class labels. As the bathymetric patches are significantly larger than the footprint of an image, the distribution of habitat labels within each patch is calculated. (Bottom row) Bathymetric patches are fed through an autoencoder that yields a latent space used to train a Bayesian neural network. The network is trained to classify unseen patches using the training targets provided by the image-based habitat labels.'\\n\\n'Figure 1 illustrates the method proposed in this study. Habitat labels derived from imagery are used to train a Bayesian neural network using the latent space of an autoencoder that can reconstruct the bathymetric patches. The steps in this process are detailed below.'\\n\\n'The encoder utilises two convolutional layers each with 1024 and 512 filters respectively, and a kernel size of 3. Following this, two fully connected layers are used with 512 units in each. The latent space was 32 dimensions when aiming to reconstruct a 21x21 raster patch. The decoder has a mirrored network structure, with an additional single filter convolutional layer to output the reconstructed patch.'\\n\\n'This paper demonstrates the application of Bayesian neural networks for building probabilistic habitat models. The focus has been on building these models with minimal human supervision. Convolutional autoencoders are used for feature extraction, as they can be applied to various remotely-sensed data sources and extract informative features. This research shows that using epistemic uncertainty to direct further samples results in greater model improvement with fewer samples, therefore making AUV surveys more efficient.'\",\"878\":\"'Multispectral recognition has attracted increasing attention from the research community due to its potential competence for many applications from day to night. However, due to the domain shift between RGB and thermal image, it has still many challenges to apply and to use RGB domain-based tasks. To reduce the domain gap, we propose multispectral domain invariant framework, which leverages the unpaired image translation method to generate a semantic and strongly discriminative invariant image by enforcing novel constraints in the objective function. We demonstrate the efficacy of the proposed method on mainly multispectral place recognition task and achieve significant improvement compared to previous works. Furthermore, we test on multispectral semantic segmentation and unsupervised domain adaptations to prove the scalability and generality of the proposed method. We will open our source code and dataset.'\\n\\n'To reduce the domain shift, we focus on extracting multispectral domain invariant image, including common information between RGB and thermal domain. To convert multispectral images to domain invariant images, we can reuse a lot of RGB images with annotations and we can apply newly captured image from alternative sensors to the RGB image-based algorithm in various applications. To do that, instead of using labeled pairs, we exploit advantages of a recent unpaired image-to-image translation. This method is conveniently unsupervised and practical in the real world scenario, rather than requiring a set of fully aligned multispectral images capturing the same place. Based on this method, we propose a novel and compatible framework to generate the domain invariant image corresponding to common semantic contents from multispectral domains. Motivated by the physical difference of both domains, we instead use the intermediate feature from the proposed domain invariant encoder as the domain invariant image rather than directly translating RGB images to thermal images. Moreover, we optimize the model to generate diverse and discriminative invariant image as the feature itself for place recognition task by the proposed constraints in the objective function.'\\n\\n'Similar to place recognition experiments, we first train the domain invariant encoder by using a training sample of the dataset. Then we extract the invariant feature image, and then use them to train the semantic segmentation model. All configurations are evaluated with the class-wise pixel accuracy and the mean Intersection-over-Union (mloU) metric. For the validity, we follow the procedure of [23] and test on two types of segmentation models as MFNet [23] and a modified version of SegNet [40] respectively. We report our results in Table II, alongside results of the source only and target only models.'\",\"879\":\"'Because of the high complexity of possible real world interactions, effect probabilities must not be hard coded, but have to be learned over time as visualized in Fig. 1. Moreover, once a robot learned for example that placing a glass on the edge of a table can lead to this glass falling down with a certain probability, it should be able to generalize this knowledge to other glasses. Existing work tackling the issue of learning probabilistic action representations from experience [3], [4], [5] is mostly concerned with extracting meaningful action effect representations from the data. We, on the other hand, assume that the possible effects of an action are accessible and aim at generalizing knowledge about their probabilities.'\\n\\n'The symbolic planning process underlying the hybrid reasoning approach depends on a symbolic description of the world. Usually the symbolic world state is hard-coded for a scenario and updated only based on the described effects of actions executed by the robot. Keeping the world state up-to-date is especially challenging when effects of actions cannot be predicted exactly. This is the case when dealing with probabilistic action effects or in case of possible failures. [20] present a framework for synchronizing a physical simulation of the world with the real world state and extracting semantic representations from it. We use this framework for retrieving the semantic world state from simulations we run.'\",\"880\":\"'https:\\/\\/aair-lab.github.io\\/stamp.html.'\\n\\n'https:\\/\\/aair-lab.github.io\\/stamp.html.'\\n\\n'We implemented the presented framework using an open- source implementation from MDP-Lib github repository [22] of LAO* [23] as the SSP solver, the OpenRAVE [24] robot simulation system along with its collision checkers, CBiRRT implementation from PrPy suite [25] for motion planning. Since there are no common benchmarks for evaluating stochastic task and motion planning problems, we evaluated our algorithm on 7 diverse and challenging test problems over 4 domains and evaluated 5 of those problems with physical robot systems. In practice, fixing the horizon h a priori can render some problems unsolvable. Instead, we implemented a variant that dynamically increases the horizon until the goal is reached with probability greater than 0. We evaluated our approach on a variety of problems where combined task and motion planning is necessary. The source code and the videos for our experiments experiment can be found at https:\\/\\/aair-lab.github.io\\/stamp.html.'\\n\\n'https:\\/\\/aair-lab.github.io\\/stamp.html.'\",\"881\":null,\"882\":\"'https:\\/\\/robotics.iiit.ac.in\\/Archives\\/motion-singularity-region.pdf'\",\"883\":\"'https:\\/\\/youtu.be\\/PwDf6h0Om3c'\",\"884\":null,\"885\":null,\"886\":null,\"887\":\"'Specifically, the invariants in this experiment are PYTHON functions, human coded from portions of the system documentation. These functions are evaluated over the output log from a test input to determine if the rules were violated. An example of a subset of the invariants in FETCH is in Figure 2. This subset of invariants represents restrictions on the allowable values for properties of the TORSO_LIFT_JOINT.'\\n\\n'While we use explicit invariants taken from system documentation as ground truth, these invariants are far from perfect. In fact, they were even sometimes violated by normal system behavior. For example, one of the designer-provided invariants for BMB specified a minimum transmit frequency of 5Hz, while the code (and associated comments) set the target transmit frequency at 1Hz. In such cases, we chose to modify our invariants because labeling the nominal behavior of the system as faulty would make further analysis difficult.'\\n\\n'Code listing for a sample FETCH invariant.'\",\"888\":\"'http:\\/\\/www.nationalarchives.gov.uk\\/doc\\/open-government-licence\\/version\\/3'\\n\\n'Structural code coverage combined with requirements coverage has been the primary approach for measuring test completeness as assurance evidence to support safety arguments, for the certification of safety-critical software. Testing techniques for machine learning components, primarily DNNs, are comparatively new and have only been actively developed in the past few years, e.g., [3], [4]. Unlike model-based software systems, DNNs are usually considered as black boxes, and therefore it is difficult to understand their behaviour by means of inspection. In [5], we developed a tool, DeepConcolic, to work with a number of extensions to the MC\\/DC coverage metric, targeting DNNs. The MC\\/DC coverage metric [6] is recommended in a number of certification documents and standards, such as RTCA\\u2019s DO-178C and ISO26262, for the development of safety critical software. Its extension to DNN testing has been shown to be successful in testing DNNs by utilising white-box testing methods, i.e., by exercising the known structure with the parameters of a DNN to gather assurance evidence. It is, however, unclear whether such a testing tool can still be effective when working with learning- enabled systems containing both learning and model-based components. Primarily, we want to understand the following two research questions:'\",\"889\":null,\"890\":\"https:\\/\\/github.com\\/kevinzakka\\/form2fit\",\"891\":\"'Robotic manipulation of deformable 1D objects such as ropes, cables, and hoses is challenging due to the lack of high-fidelity analytic models and large configuration spaces. Furthermore, learning end-to-end manipulation policies directly from images and physical interaction requires significant time on a robot and can fail to generalize across tasks. We address these challenges using interpretable deep visual representations for rope, extending recent work on dense object descriptors for robot manipulation. This facilitates the design of interpretable and transferable geometric policies built on top of the learned representations, decoupling visual reasoning and control. We present an approach that learns point-pair correspondences between initial and goal rope configurations, which implicitly encodes geometric structure, entirely in simulation from synthetic depth images. We demonstrate that the learned representation \\u2014 dense depth object descriptors (DDODs) \\u2014 can be used to manipulate a real rope into a variety of different arrangements either by learning from demonstrations or using interpretable geometric policies. In 50 trials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66% knot-tying success rate from previously unseen configurations. See https:\\/\\/tinyurl.com\\/rope-learning for supplementary material and videos.'\\n\\n'This research was performed at the AUTOLAB at UC Berkeley with partial support from Toyota Research Institute, the Berkeley AI Research (BAIR) Lab and by equipment grants from PhotoNeo. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors. Ashwin Balakrishna is supported by an NSF GRFP. We thank our colleagues who provided helpful feedback, code, and suggestions, especially David Tseng, Aditya Ganapathi, Michael Danielczuk, Jeffrey Ichnowski, and Daniel Seita.'\",\"892\":\"'https:\\/\\/www.turbosquid.com\\/'\\n\\n'Even though little is known in speeding up ED graph based SLAM systems, numerous publications are dedicated to the graph based optimization in rigid scenarios. A typical graph optimization problem models robot poses (or landmark positions) as nodes in the graph, while the edges encode the relative measurement between connected nodes [20]. Graph sparsification is the most widely applied technique to marginalize subsets of nodes [21] [22] [23]. The key process in this topic is to sparsify edges and marginalize nodes based on indicators like divergence [24]. We will show in this paper that marginalization methods are of great value to enhance efficiency in ED based SLAM. In this paper, we will mitigate the O(n2) computational bottleneck encountered in the expanding environment, for ED graph based SLAM as reported in [10]. We analyze the spatial relationship between ED nodes and observed features and reveal the inherent sparsity pattern of the Hessian matrix. With this discovery, we classify ED nodes into points relevant (PR) and points irrelevant (PI) nodes and propose a decoupled optimization strategy.'\",\"893\":\"'The encoder consists of the convolutional layers of VGG-19 [12] pretrained on ImageNet. The decoder (upsampling) component is composed of four 2D transpose convolutional layers (stride = 2, padding = 1, output padding = 1), and each layer is followed by a normal 3\\u00d73 convolutional layer and ReLU activation layer. We also experimented with a ResNet-based encoder, viz., our reimplementation of [13], with the same batch normalization, upsampling layers, and so forth as described in the paper.'\\n\\n'The DREAM framework. A deep encoder-decoder neural network takes as input an RGB image of the robot from an externally-mounted camera, and it outputs n belief maps (one per keypoint). The 2D peak of each belief map is then extracted and used by PnP, along with the forward kinematics and camera intrinsics, to estimate the camera-to-robot pose,\\\\nR\\\\nC\\\\nT\\\\n.'\\n\\n'For comparison, we trained three versions of our DREAM network. As described earlier, the architecture uses either a VGG- or ResNet-based encoder, and the decoder outputs either full (F), half (H), or quarter (Q) resolution. Each neural network was trained for 50 epochs using Adam [26] with 1.5e-4 learning rate and 0.9 momentum. Training for each robot used approximately 100k synthetic DR images. The best-performing weights were selected according to a synthetic validation set.'\\n\\n'We present an approach for estimating the pose of an external camera with respect to a robot using a single RGB image of the robot. The image is processed by a deep neural network to detect 2D projections of keypoints (such as joints) associated with the robot. The network is trained entirely on simulated data using domain randomization to bridge the reality gap. Perspective-n-point (PnP) is then used to recover the camera extrinsics, assuming that the camera intrinsics and joint configuration of the robot manipulator are known. Unlike classic hand-eye calibration systems, our method does not require an off-line calibration step. Rather, it is capable of computing the camera extrinsics from a single frame, thus opening the possibility of on-line calibration. We show experimental results for three different robots and camera sensors, demonstrating that our approach is able to achieve accuracy with a single frame that is comparable to that of classic off-line hand-eye calibration using multiple frames. With additional frames from a static pose, accuracy improves even further. Code, datasets, and pretrained models for three widely-used robot manipulators are made available.'\",\"894\":\"'https:\\/\\/github.com\\/ShreyasSkandanS\\/pst900_thermal_rgb'\\n\\n'The RGB stream is a ResNet-18 architecture with an encoder-decoder skip-connection scheme similar to UNet. Our network heavily based on the implementation of Usuyama et al. [34] and is shown in Fig 5. The network is first trained on annotated RGB images only. We use a weighted negative log-likelihood loss during training and select the model with the highest mean Intersection-over-Union (mIoU). The weights for the loss function are calculated using the weighting scheme proposed by Paszke et al. [35]. On datasets such as ours, a weighting scheme is necessary given the dramatic imbalance between background and foreground classes as seen in Table I. Once the network is trained, we remove the final softmax operator which results in what is intuitively a per-pixel confidence volume for the different classes in our dataset. We use this volume, along with the thermal modality, as input to our next fusion stream.'\\n\\n'The Fusion stream takes as it\\u2019s input the confidence volume along with the input thermal imagery and color image. The information is concatenated and passed to an ERFNet-based encoder-decoder architecture [26]. Our architecture differs in that it has a larger set of initial feature layers to account for the larger input. Additionally, we use fewer layers at the end of the encoder. We then freeze the RGB stream and train this entire architecture as a whole using the same loss function as before. Once again, we select our best model to be the one with the lowest mean IoU value.'\\n\\n'https:\\/\\/github.com\\/ShreyasSkandanS\\/pst900_thermal_rgb'\",\"895\":\"'For efficiency, we directly encode the point cloud into a 2D high-resolution bird\\u2019s-eye view representation. The segmentation and localization is then based on this regular representation. Compared with more complicated 3D voxels, our approach is much faster and memory efficient, since no 3D convolutions will be involved.'\\n\\n'We propose a robust baseline method for instance segmentation which are specially designed for large-scale outdoor LiDAR point clouds. Our method includes a novel dense feature encoding technique, allowing the localization and segmentation of small, far-away objects, a simple but effective solution for single-shot instance prediction and effective strategies for handling severe class imbalances. Since there is no public dataset for the study of LiDAR instance segmentation, we also build a new publicly available LiDAR point cloud dataset to include both precise 3D bounding box and point-wise labels for instance segmentation, while still being about 3~20 times as large as other existing LiDAR datasets. The dataset will be published at https:\\/\\/github.com\\/feihuzhang\\/LiDARSeg.'\",\"896\":\"'This paper presents a novel approach to the generation of object candidates by a robot that is endowed with a pan-tilt camera. The importance of this problem stems from the fact that these candidates serve as a basis for the robot to categorize and\\/or recognize the objects in its surroundings. The novelty of our proposed approach is that it simultaneously enables the robot to look around and to take advantage of the temporal coherence of the incoming video data. The robot\\u2019s camera movements are governed by a family of controllers whose constructions depend on the set of object candidates that have been hitherto generated, but not directly looked at. In parallel, the robot discovers the object candidates from the incoming video by determining the spatio-temporally coherent segments. The advantage of the proposed approach is that while the robot can explore its surroundings by simply moving its camera prior to more sophisticated exploration behavior (involving possibly bodily translation), the generated object candidates turn out to be consolidated across the incoming visual stream as demonstrated in the experimental results. The proposed approach can easily be used on a resource constrained system. Furthermore, in case of robots with restricted motion capabilities like aerial robots flying indoors, it is easier to move the pan-tilt camera than the robot itself. Our ongoing work is focused on using the generated object candidates in object categorization. We are also working on code optimization for speeding up real-time performance. We plan to extend this work by considering dynamic environments and its integration with exploration involving locomotion.'\",\"897\":null,\"898\":\"'Obstacle detection by semantic segmentation shows a great promise for autonomous navigation in unmanned surface vehicles (USV). However, existing methods suffer from poor estimation of the water edge in presence of visual ambiguities, poor detection of small obstacles and high false-positive rate on water reflections and wakes. We propose a new deep encoder-decoder architecture, a water-obstacle separation and refinement network (WaSR), to address these issues. Detection and water edge accuracy are improved by a novel decoder that gradually fuses inertial information from inertial measurement unit (IMU) with the visual features from the encoder. In addition, a novel loss function is designed to increase the separation between water and obstacle features early on in the network. Subsequently, the capacity of the remaining layers in the decoder is better utilised, leading to a significant reduction in false positives and increased true positives. Experimental results show that WaSR outperforms the current state-of-the-art by a large margin, yielding a 14% increase in F-measure over the second-best method.'\\n\\n'Architecture of the proposed WaSR network. Encoder generates rich deep features, which are gradually fused in the decoder with a horizon mask computed from an IMU readout to boost detection and water edge estimation. A water-obstacle separation loss \\u2112WS computed at the end of encoder drives learning of discriminative features, further reducing false positives and increasing true positives.'\\n\\n'One of primary tasks of the decoder is fusion of visual and inertial information. We introduce the inertial information by constructing an IMU feature channel that encodes location of horizon at a pixel level. In particular, camera-IMU projection [10] is used to estimate the horizon line and a binary mask with all pixels below the horizon set to one is constructed (Figure 1). This IMU mask serves a prior probability of water location and for improving the estimated location of the water edge in the output segmentation.'\\n\\n'The IMU mask is treated as an externally generated feature channel, which is fused with the encoder features at multiple levels of the decoder. However, the values in the IMU channel and the encoder features are at different scales. To avoid having to manually adjust the fusion weights, we apply an approach called Attention Refinement Modules (ARM) proposed by [13] to learn an optimal fusion strategy.'\\n\\n'The decoder starts with the ARM1 block (Figure 2), which differs from ARM [13] in the way the input is pre-processed. The IMU mask is resized and concatenated with the encoder output features. The remaining steps follow [13]: global aver-age pooling followed by depth reduction and normalization is used to learn channel weights, which are subsequently used to re-weight the concatenated feature channels. The resulting features are further fused with res3 output features and the IMU mask using another ARM block called ARM2 (Figure 2). ARM2 first applies an ARM1 block to fuse the IMU mask and the features from lower part of the decoder. This is followed by a set of 1 \\u00d7 1 convolutions to double the number of feature channels, which are per-channel summed with the res3 features from the encoder.'\\n\\n'Attention refinement modules ARM1, ARM2 and feature fusion module FFM adjust the scale of heterogeneous input feature channels and gradually fuse inertial and visual information in the WaSR decoder.'\\n\\n'A novel obstacle detection deep neural network, WaSR, for USV navigation was presented. WaSR improves the water-edge segmentation and overall obstacle detection by fusing visual information with inertial sensory data from an on-board IMU. A deep encoder extracts rich visual features from the input image, while a non-symmetric and shallow decoder fuses the visual features with inertial data. Additional ro-bustness is achieved by introducing a novel water-obstacle separation loss at the end of the encoder, which enforces learning a feature space in which separation between water and obstacle appearances is increased.'\",\"899\":\"'Anchor boxes act as potential object localization candidates allow single-stage detectors to achieve real-time performance, at the cost of localization accuracy when compared to state-of-the-art two-stage detectors. Therefore, correct selection of the scale and aspect ratio associated with an anchor box is crucial for detector performance. In this work, we propose a novel architecture called DANet for improving the localization performance of single-stage object detectors, while maintaining real-time inference. The proposed network achieves this by predicting (1) the combination of aspect ratio and scale per feature map based on object density and (2) localization confidence per anchor box. We evaluate the proposed network using the benchmark dataset. On the MS COCO dataset, DANet achieves 30.9% AP at 51.8 fps using ResNet-18 and 45.3% AP at 7.4 fps using ResNeXt-101. The code and models will be available at https:\\/\\/github.com\\/PS06\\/AnchorNet.'\",\"900\":\"'Network architecture for predicting keypoint locations, scores and descriptors. The height of each block indicates the spatial dimensions of the feature map, which vary by a factor of 2 between blocks through max-pooling or bilinear-interpolation. A dense pixel-wise descriptor map (top right) is created by resizing the output of each encoder block to the size of the input before concatenation into a single feature map. For keypoint locations, spatial softmax is performed on a per cell basis with cell size chosen such that 400 keypoints are predicted. A pointwise convolution with no activation and single channel output precedes the sigmoid and spatial softmax operations. The number of output channels are detailed for each block.'\\n\\n'The Descriptors aim to uniquely identify real-world locations under keypoints so that we can relate points by comparing descriptor similarity. Dense descriptors are created by resizing the output of each encoder block (shown in yellow) to the input resolution before concatenation into a single 248 channel feature map.'\\n\\n'Qualitative loop closure detections. For a given radar input, shown as on the map (left) and top row (right), our location specific embeddings enable us to detect loop closures from different traversals of the route, shown as on the map and the corresponding colour-coded scans in the third row. As can be seen from the temporally closest camera images, place recognition can be extremely challenging in vision due to limited field-of-view, lens-glare and other environmental conditions. Using radar data, we are not faced with the same challenges.'\",\"901\":null,\"902\":\"'A. Voxel Feature Encoder'\\n\\n'To estimate the motion of dynamic objects in the scene we need to aggregate temporal context. Voxel Feature Encoder outputs tensor representations of consequent point clouds, but each of them is in its local coordinate system. If one naively aggregates such representations using RNN or 3D convolutions, the model would be forced to learn ego-motion of the observer implicitly. We believe that such setting lowers the quality of the resulting motion estimation. However, as in a self-driving setting localization of the ego-vehicle is a crucial part of the pipeline, ego-motion is often known precisely. Given transforms from the local coordinate system to the world coordinates from localization module it is quite natural to compute transforms Ti\\u22121,i between two consecutive local coordinate systems. Namely,\\\\nT\\\\ni\\u22121,i\\\\n=\\\\nT\\\\n\\u22121\\\\ni\\\\nT\\\\ni\\u22121\\\\nwhere Ti, Ti\\u22121 are transforms from local to world coordinates.'\\n\\n'To estimate the influence of the ego-motion compensation layer and measure the error which lies inside the discretization error between transforms, we train the AMDNet-3Dconv-PCT with 3D convolution time aggregation module without ego-motion compensation layer. Instead of this, we transform all the point clouds into one coordinate system before passing them to Voxel Feature Encoder. Table I shows that AMDNet-3Dconv-PCT models achieve slightly better results due to the absence of the discretization error in transforms. However, computational demands increase dramatically leading to model\\u2019s disability to operate in realtime.'\\n\\n'To emphasize the importance of odometry correction we train AMDNet-RNN-no-odo without ego-motion compensation layer. We also do not transform point clouds before passing them to Voxel Feature Encoder. In order to give high-quality predictions, this model should implicitly take ego-motion into account. Experiments show a drop in the velocity estimation quality. The reason can be the ambiguity between the ego-motion and other objects\\u2019 motion. At the same time, this model still surpasses the quality of the baselines.'\\n\\n'Motion estimation of common road participants. Though biker is not presented in the training data, their speed is also estimated precisely. To visualize the output, we project the bird\\u2019s-eye view grid into the image and illustrate the speed of moving cells with arrows. The lengths of the arrows are proportional to the speed; the direction of the speed is shown with the arrows\\u2019 and color-coded in HSV color space.'\",\"903\":\"'Fast, non-linear trajectories have been shown to be more accurately visually measured, and hence predicted, when sampled spatially (that is when the target position changes) rather than temporally, i.e. at a fixed-rate as in traditional frame-based cameras. Event-cameras, with their asynchronous, low latency information stream, allow for spatial sampling with very high temporal resolution, improving the quality of the data and the accuracy of post-processing operations. This paper investigates the use of Long Short-Term Memory (LSTM) networks with event-cameras spatial sampling for trajectory prediction. We show the benefit of using an Encoder-Decoder architecture over parameterised models for regression on event-based human-to-robot handover trajectories. In particular, we exploit the temporal information associated to the events stream to predict not only the incoming spatial trajectory points, but also when these will occur in time. After having studied the proper LSTM input\\/output sequence length, the network performance are compared to other regression models. Then, prediction behavior and computational time are analysed for the proposed method. We carry out the experiment using an iCub robot equipped with event-cameras, addressing the problem from the robot perspective.'\\n\\n'Full pipeline: the output of event-cameras is fed into an event-based tracker. Our contribution consists of the LSTM encoder-decoder architecture that handles asynchronous event-based data and predicts the future trajectory of the target.'\\n\\n'LSTM Encoder-Decoder: it consists of an encoder, that receives an input sequence of length win, followed by a decoder that outputs a sequence of length wout'\\n\\n'd) LSTM Encoder-Decoder'\",\"904\":\"'Network Architecture: The paint network processes a given image and produces the spray paint parameters at each instant of time. The paint network\\u2019s architecture is based on the idea of a variational auto-encoder [22], since they are proven to be extremely efficient in understanding generative tasks. An encoder network captures salient information in the input data x by modeling the distribution of the latent code z. From this latent code distribution, a decoder network tries to identify controllable parameters determining the spray paint output on the canvas. Our network is inspired by the DRAW network proposed in [9], where both the encoder and decoder are RNNs making decisions based on the history. Additionally, the encoder is aware of the decoder\\u2019s response from previous iteration and decoder modifies the canvas incrementally by only making decisions regarding \\\"what to paint\\\" and \\\"where to paint\\\" at each time step. This process is named PAINT mechanism. Similar to the DRAW network, we employ attention mechanism to determine what part of the input data has to be shown to the encoder RNN, named as READ mechanism. Please refer to [9] for further details. The proposed network, shown in Fig. 5, differs from the original DRAW network as mentioned below.'\",\"905\":null,\"906\":\"'The following is the pseudo code of the proposed LHA*. From given initial vertex, vi, and the goal vertex, vg, we will utilize the learning heuristic function, hl(\\u2022, \\u2022), to guide the search efficiently and the admissible heuristic function, h0(\\u2022, \\u2022), to generate the path with the bounded suboptimality. Function g(v) denotes the current best cost from vi to v during searching. In the above Algorithm 1, OPEN.pop()returns the element, v, in the set OPEN having the smallest value of f (v) and removes it from the set OPEN, the procedure minCost(S, vg ) finds the vertex in S which has the smallest sum of the current best cost from the initial vertex and the estimated cost to the goal vertex by the admissible heuristic, g(v) + h0(v,vg), to return this value. This algorithm works similar to A* while expansions of vertices are guided by a learned heuristic, hl(\\u2022, \\u2022), which does not need to be admissible. However, the proposed LHA* algorithm guarantees the bounded suboptimality by adapting an admissible heuristic function for termination criteria at line 13.'\\n\\n'Also, the ratio of the computation time to solve the path planning problem can be considered. However, computation time depends on various factors like code implementation, compile options, power management of CPU and existence\\/performance of GPU in case of LHA*, while having a strong correlation with the number of expansions. Therefore, the number of expansions can be more adequate object measure than computation time, so the above two measures are considered to demonstrate the performance.'\",\"907\":null,\"908\":\"'https:\\/\\/groups.csail.mit.edu\\/rrg\\/lsm'\\n\\n'We generated 1500 navigation tasks in known maps, categorized by indoor or outdoor start and goal locations, omitting outdoor only tasks due to the lack of semantic information outdoors in our environments. We simulated a robot completing each navigation task using a modified open source implementation of PRM [23] and a Euclidean objective function without a priori map information.'\",\"909\":\"'In this paper, we propose a deep neural network that predicts the feasibility of a mixed-integer program from visual input for robot manipulation planning. Integrating learning into task and motion planning is challenging, since it is unclear how the scene and goals can be encoded as input to the learning algorithm in a way that enables to generalize over a variety of tasks in environments with changing numbers of objects and goals. To achieve this, we propose to encode the scene and the target object directly in the image space.Our experiments show that our proposed network generalizes to scenes with multiple objects, although during training only two objects are present at the same time. By using the learned network as a heuristic to guide the search over the discrete variables of the mixed-integer program, the number of optimization problems that have to be solved to find a feasible solution or to detect infeasibility can greatly be reduced.'\\n\\n'One of the major challenges in integrating learning\\/experience into TAMP is the question of how the problem setting, e.g. the objects in the scene and goals, can be encoded as input to the learning algorithm [1], [5], [6]. Often, machine learning methods are integrated into robotic problems for a single task only. In this case, a fixed size feature representation might be sufficient. However, a remarkable property of TAMP approaches is that they generalize over a large variety of tasks in different environments with changing numbers of objects and goals, which makes fixed sized features not directly suitable.'\\n\\n'More specifically, we propose a deep neural network that predicts the feasibility of mixed-integer programs, which are one way to realize TAMP, where the discrete action (integer) variable represents the abstract decision and the resulting nonlinear trajectory optimization problem the geometric part. The input to our neural network is a multi-channel image and the discrete action. The image consists of a depth image of the scene and object centric masks in the image space, which enable to encode the objects that are involved in the action. In the experiments, we consider the problem of grasping boxshaped objects in a scene with two robot arms (Fig. 1). The discrete actions encode how and with which robot a box should be grasped. Infeasibility of an action can occur due to the fact that a box is kinematically out of reach for the chosen grasping or that other objects obstruct this action.'\\n\\n'The specific network architecture used in the experiments is as follows. The CNN part are two convolutional layers with a filter size of 5 \\u00d7 5, followed by max-pooling of size 2 and stride 2. The first convolution layer has 5 filters, the second one 10. The output of the CNN is flattened and passed through a fully connected layer to produce a feature size of 500. The action encoder consists of two fully connected layers with a final feature size of 500. Both features are concatenated and passed through one additional fully connected layer with size 100, before the final linear layer with one sigmoid output. All hidden layer use ReLUs.'\",\"910\":\"'At each depth measurement integration into the map, the voxels inside the camera frustum are updated. Each of the updated voxels is tested for being a frontier voxel. Voxels are considered frontiers if their occupancy probability is lower than 0.5, while one or more of their 6 face neighbour voxels has an occupancy probability of exactly 0.5. More intuitively, frontier voxels are free voxels located next to completely unobserved voxels. The Morton code list is updated as new frontier voxels emerge and previously unobserved regions are observed. The latter leads to removal of frontier voxels. By only considering the last updated voxels for the map frontiers update, a map-wide operation is avoided. This frontier update is performed continuously at each sensor measurement integration.'\",\"911\":null,\"912\":null,\"913\":\"'With the intention of implementing the MPC in real-time in an embedded computer, we have encoded the two optimal problems with the ACADO framework [20], employing Multiple-Shooting SQP. The autogenerated, compiled controller code of the low-level controller would require 0.645 ms per call on an Intel Core i7-6500\\/2.50 GHz processor, while the translational dynamics controller required 3.91 ms. Both controllers had ample time to run in real-time.'\\n\\n'Regarding the proposed Flight Envelope determination algorithm, it was encoded in Python, interfacing with the C++ NLOPT library and the UAV model library for trim point sampling and the Parma Polyhedra Library (PPL) for certain polytope operations. A slice at the 3D space of an algorithm run can be seen in Figure 3.'\\n\\n'In addition, a two-layer Model Predictive Controller (MPC) has been designed able to incorporate the Flight Envelope in the form of convex linear constraints. The overall control scheme has been encoded in fast real-time software and tested in simulation.'\",\"914\":\"'This model uses an encoder-decoder structure. It is based on LSTM networks for encoding and forecasting. We propose to add two multi-head self-attention layers to this architecture to account for interactions. The first attention layer is added after encoding to incorporate current time interactions. The second attention layer is added after forecasting time unrolling. This allows the forecast position sequences to remain coherent with each other.'\\n\\n'B. Encoder'\\n\\n'The encoder acts as a current state estimation for each vehicle using the past observation sequences. This state is an intermediary vector of the neural network and is difficult to interpret. However, since it should encode the current state with at least the information of position, kinematic state, and interaction features, it should have a sufficient dimension, we chose 120. The input (x,y) position sequences are fed to a one dimensional convolutional layer with a kernel of size 3 sliding over the time dimension that creates sequences of 120 features for each vehicle. This first layer increases the number of features in the vector used for the following computations. A convolution allows this first layer to compute derivatives, smoothed values and other features extracted from successive positions. Then each feature sequence is encoded with a Long Short-Term Memory (LSTM) [2] into a vector of 120 features for each vehicle.'\\n\\n'D. Predictor and decoder'\\n\\n'Feature sequences are decoded with two linear layers shared for each time step and ReLU activations. Finally, a last linear layer produces the mixture of Gaussian coefficients. The output is described in section III. Let oi be the ith coordinate of the output tensor before the activation function. To constraint it, the following activation function is applied on each coordinate at every time steps:'\\n\\n'This model is defined with a few specific hyperparameters that should be tuned: number of encoded features, number of embedding and decoding layers and their activation functions, number of heads in each self-attention layer, number of mixture component in the output distribution and the error covariance clipping value. Other choices have been made and should be questioned such as the data normalization, the use of shortcut connections with or without layer normalization, the use of LSTM layers, the use of two attention layers and some implicit choices that may have been overlooked. Optimizing the hyperparameters with a thorough process could bring some improvements and help understand the model but is not a part of the present study. In this work, only the general concept was prioritized and the hyperparameters were chosen from experience.'\\n\\n'This model was implemented using the Pytorch library. The NGSIM datasets US-101 and I-80 and its pre-processing were taken from the published code accompanying the article [5]. This also defines the dataset splitting into training, validation, and test sets. Thus, a fair comparison with these results is made. The dataset contains the tracks of all vehicle position on a road segment observed from a camera. The preprocessing produces data that simulates observations from a given vehicle. Each vehicle is alternatively chosen as the observing vehicle. Its neighbors in adjacent lanes and within a 60m road segment are recorded to produce a local road scene centered on the observing vehicle. This road scene is tracked to produce 8 seconds sequences with all positions being recorded at a 5Hz frequency. The 3 first seconds are used as past observations and the 5 next seconds are used as forecast supervision. A typical training time for our model is 20 to 30 hours.'\\n\\n'The simplest extension is to add observations on each vehicle such as velocity, orientation, size or blinkers. Another extension is to match various object classes such as cars and trucks with specific encoders, predictors, and decoders to allow inter-class interactions. These adaptations can easily be made because our forecasting algorithm is model-free.'\\n\\n'encode'\\n\\n'extEncoder'\",\"915\":null,\"916\":null,\"917\":null,\"918\":null,\"919\":null,\"920\":null,\"921\":null,\"922\":\"'https:\\/\\/youtu.be\\/SjE1Ptu0bTo'\\n\\n\\\"To train and test the network, a total of 4656 images were recorded using our PointGrey Grasshopper3 camera. The images were cropped around the table tennis ball to have a fixed size of 60 \\u00d7 60 pixels. 46.7% were labeled as having no visible brand logo. The ball's pose was labeled with the help of a 3D scene containing a ball with realistic logo texture. The 3D scene was modeled with the open-source 3D computer graphics software Blender [13]. Each real ball image was placed transparently over the scene. Next, the 3D ball model can be optimized to fit the actual image and the pose can be read out by the Blender Python API.\\\"\",\"923\":null,\"924\":null,\"925\":null,\"926\":\"'It is difficult for both cameras and depth sensors to obtain reliable information in hazy scenes. Therefore, image dehazing is still one of the most challenging problems to solve in computer vision and robotics. With the development of convolutional neural networks (CNNs), lots of dehazing and depth estimation algorithms using CNNs have emerged. However, very few of those try to solve these two problems at the same time. Focusing on the fact that traditional haze modeling contains depth information in its formula, we propose a CNN-based simultaneous dehazing and depth estimation network. Our network aims to estimate both a dehazed image and a fully scaled depth map from a single hazy RGB input with end-to-end training. The network contains a single dense encoder and four separate decoders; each of them shares the encoded image representation while performing individual tasks. We suggest a novel depth-transmission consistency loss in the training scheme to fully utilize the correlation between the depth information and transmission map. To demonstrate the robustness and effectiveness of our algorithm, we performed various ablation studies and compared our results to those of state-of-the-art algorithms in dehazing and single image depth estimation, both qualitatively and quantitatively. Furthermore, we show the generality of our network by applying it to some real-world examples.'\\n\\n'This paper proposes an end-to-end convolutional neural network designed for simultaneous dehazing and depth estimation, focusing on the fact that a transmission map has a correlation with depth information. The network consists of a shared encoder and four decoders, where each decoder is assigned for an individual task. Some of the tasks are to recover the true radiance of the scene, while others are to make the training procedure more efficient. The technical details of the network and training scheme are presented more fully in Sec. III.'\\n\\n'The encoder of the network follows the structure of the Densely Connected Convolutional Network (DenseNet) [17], with four layers of dense blocks. Between every two dense blocks, there exists a transition block. A transition block is a combination of a 1\\u00d71 convolution layer and a 2\\u00d72 average pooling layer. From the first dense block to the third transition block, we initialized the network using the pre-trained weight of DenseNet201 [17]. This weight was trained on the ImageNet [18], which is a dataset for image classification. Although image classification and dehazing or depth estimation are different tasks, the rich image representation learned from image classification helps to train the proposed network more effectively. Since we have to preserved some spatial resolution of the encoded image feature, we modified the last dense and transition block from the original DenseNet201 [17]. The network architecture of the encoder is described in detail in Table I.'\\n\\n'There are four decoders in our simultaneous dehazing and depth estimation network. These decoders share the same image representation extracted from the encoder. Each decoder outputs predicted directly-regressed true scene radiance Jdirect, atmospheric light \\u03b1pred, transmission map tpred(x), and depth map of the scene Dpred(x). The overall architecture of each decoder is similar to the encoder, but it has transition blocks with upsampling layers instead of average pooling layers. Transition blocks help the network to effectively reorder and expand the spatial size of the encoded feature. Also, additional residual blocks suggested in [19] and refinement blocks suggested in [1] were added. The residual blocks have two consecutive dense blocks with two 3 \\u00d7 3 convolutional layers, which gives the ability to recover more high-frequency information. The refinement blocks takes a dense pyramid-like structure with four different spatial scales of average pooling and upsampling. This helps the network to preserve both the global and local information of the image, where global information refers to high-level scene description, and local information means semantic features and spatial location in the image. The detailed architecture of each refinement block is shown in Table II.'\\n\\n'TABLE I: Detailed architecture of the encoder.'\\n\\n'The outputs of all four refinement blocks are then concatenated and fed into the final dense block to predict the information for which each decoder was designed. The final dehazed result that we used is the dehazed image reconstructed using estimated \\u03b1 and t. The output of Decoder J is only used to help the whole network\\u2019s training. This idea was first proposed in [20], and was proven to be effective.'\\n\\n'Atmospheric light, a transmission map, and a depth map are individual outputs from the decoders of our network. Atmospheric light and the transmission map are trained with L2 loss as in Eq. (6). The training loss for depth map estimation is L2 loss between the predicted depth map Dpred and the ground truth depth Dgt.'\\n\\n'First, we divide the log-scaled transmission map predicted from the decoder t, tpred, with the predicted depth map Dpred, and name it consistency term, noting as C. This term should have uniform value over all pixels because C equals to \\u03b2 times depth-normalizing constant, following Eq. (3). Therefore, a standard deviation of C should be 0 for each transmission map\\/depth map pair.'\\n\\n'In the training procedure, the whole network was trained in an end-to-end manner, and every decoder was trained from scratch. We used the ADAM optimizer with an initial learning rate of 1e\\u22124 and a weight decay of 1e\\u22128. The learning rate was reduced to 70% for every 25 epochs. We used a batch size of 4 and trained for 100 epochs. Our network was implemented by using PyTorch on a machine equipped with four NVidia 1080 Ti GPUs.'\\n\\n'In this work, we addressed the difficulty of CNN-based dehazing as well as the depth estimation from hazy scenes. By fully utilizing the principal of the haze model, we propose a CNN-based simultaneous dehazing and depth estimation network. Our network was trained with multi-tasking loss, helping the decoders be guided to each other. Moreover, with the depth-transmission consistency loss, we maximized the correlation between the decoders and were able to output the best result. We showed that our algorithm performs in a promising manner, in both dehazing and depth estimation, achieving a performance comparable to or better than other state-of-the-art algorithms.'\",\"927\":null,\"928\":null,\"929\":null,\"930\":null,\"931\":\"'Figure 4 depicts the system architecture for the proposed application. Once the rolling cart is securely coupled, DRC-Hubo can read force\\/torque data through FT sensors, and the joint encoder will monitor the actuators\\u2019 real positions. The green section of Fig. 4 which is called PODO (software that DRC-Hubo uses to communicate with the programs, sensors, simulator, and user) is responsible for sending motor commands to the controllers. In addition, the robot state publisher is responsible for the exact joint position during operation. The diagram also demonstrates how the input from cart interacts with the humanoid hardware. The system has been encapsulated by using ROS (Robot Operating System) nodes to generate a stable walking trajectory.'\",\"932\":null,\"933\":null,\"934\":null,\"935\":null,\"936\":null,\"937\":null,\"938\":\"'Executing multiple tasks concurrently is important in many robotic applications. Moreover, the prioritization of tasks is essential in applications where safety-critical tasks need to precede application-related objectives, in order to protect both the robot from its surroundings and vice versa. Furthermore, the possibility of switching the priority of tasks during their execution gives the robotic system the flexibility of changing its objectives over time. In this paper, we present an optimization-based task execution and prioritization framework that lends itself to the case of time-varying priorities as well as variable number of tasks. We introduce the concept of extended set-based tasks, encode them using control barrier functions, and execute them by means of a constrained-optimization problem, which can be efficiently solved in an online fashion. Finally, we show the application of the proposed approach to the case of a redundant robotic manipulator.'\\n\\n'Consider the Jacobian-based task encoded by the following differential kinematic equation:'\\n\\n'The 3 non-safety-critical tasks are encoded by the CBFs'\\n\\n'These examples demonstrate how our framework can encode the prioritization of different task stacks, especially when multiple tasks might be safety-critical. As the objective of this paper is to illustrate how such a formulation can also allow for dynamically evolving task prioritizations, the next section considers a time-varying prioritization matrix K(t), and demonstrates how this lends itself to the synthesis of continuous controllers for task switching and task insertion\\/removal.'\\n\\n',\\u2026,M encode M tasks through the continuously differentiable cost functions Cm, and\\\\nK:\\\\nR\\\\n\\u22650\\\\n\\u2192\\\\nR\\\\nN\\\\np\\\\n\\u00d7M\\\\nis a mapping that, for each time instant t, provides a Np'\",\"939\":\"'http:\\/\\/bit.ly\\/2lLMl4n'\\n\\n'However, in human-robot interaction, we often have different scenarios where the relative importance between the precision of the robot and its compliance might vary. As it is very difficult to perform highly in both respects, a tradeoff between precision and compliance needs to be attained. The precision requirements are usually provided by a motion characterization that encodes a time-varying precision at every part of the trajectory. In the case of Probabilistic Movement Primitives (ProMPs) [9], such precision can be extracted from the time variance through several motion demonstrations to the robot. Nevertheless, ProMPs suffer from the high-dimensionality of their parametrical representation. For this reason, we use dimensionality reduction techniques as in [10]. Moreover, [9] presents a stochastic controller that tracks the desired precision at every timestep. However, such controller suffers from a high computational cost if the data used for generating the ProMP is not rich enough to fully characterize its covariance. On the other hand, some authors [11] use characteristics of the motion to regulate the stiffness of the robot, but they do not take into account the requirements introduced by the user.'\\n\\n'Throughout this work we will study the control problem in the operational space, using ProMPs to encode the trajectories learned from demonstration and an Expectation-Maximization (EM) algorithm [14] to reduce the dimension of the problem. In this section, we introduce these concepts which are going to be used along the paper.'\\n\\n'ProMPs are a stochastic approach to learn and encode a set of similar motion trajectories that present time-dependent variances over time [9]. Given a number of basis functions per DoF, Nf, ProMPs use time-dependent Gaussian kernels \\u03a6t to encode the state of a trajectory, \\u03a6t being the vector of normalized kernel basis functions (e.g., uniformly distributed Gaussian basis function over time). Thus, the position and\\/or velocity state vector yt can be represented as'\\n\\n'ProMPs also provides a model-based stochastic controller that reproduces the encoded trajectory distribution [9].'\\n\\n'Finally, the reduced control law is decoded to the Cartesian space and a gravity compensation term g(q) is added:'\",\"940\":\"'NIR spectra can be classified perfectly in an ideal environment where noises and errors can be neglected. However, in practical, noises are not avoidable and may lead to a low classification accuracy. In this work, we trained a complex structure of autoencoders (shown in Fig. 3) with the AE dateset to reduce noises and improve performance of our algorithm.'\\n\\n'Structure of autoencoder for noise reduction'\\n\\n'Near-infrared spectra of six types of materials including ceramic, stainless steel, wood, cardboard, plastic and glass were collected in this work. All these materials covered a major majority of daily objects. A total of 54 different daily used objects were selected. This material\\/object selection pattern would expand the universality of our datasets. Two NIR datasets were established to train the material\\/feature detection algorithm. MLP dataset was collected to train a MLP network for material recognition while AE dataset is used to train our autoencoder to reduce noise in practical spectra.'\",\"941\":\"'https:\\/\\/youtu.be\\/o7L2nE8dwQM'\\n\\n'This work was supported by the National Natural Science Foundation of China under Grant 61876054, the China Scholarship Council, the EPSRC CDT in Robotics and Autonomous Systems (EP\\/L016834\\/1), EPSRC Future AI and Robotics for Space (EP\\/R026092\\/1), and Offshore Robotics for Certification of Assets (EP\\/R026173\\/1). Thanks for Jiacheng Gu\\u2019s codes on detecting objects\\u2019 pose.'\",\"942\":null,\"943\":\"'The real-time segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, it is still a challenging task to implement deep learning models to do real-time segmentation for surgical instruments due to their high computational costs and slow inference speed. In this paper, we propose an attention-guided lightweight network (LWANet), which can segment surgical instruments in real-time. LWANet adopts encoder-decoder architecture, where the encoder is the lightweight network MobileNetV2, and the decoder consists of depthwise separable convolution, attention fusion block, and transposed convolution. Depthwise separable convolution is used as the basic unit to construct the decoder, which can reduce the model size and computational costs. Attention fusion block captures global contexts and encodes semantic dependencies between channels to emphasize target regions, contributing to locating the surgical instrument. Transposed convolution is performed to upsample feature maps for acquiring refined edges. LWANet can segment surgical instruments in real-time while takes little computational costs. Based on 960x544 inputs, its inference speed can reach 39 fps with only 3.39 GFLOPs. Also, it has a small model size and the number of parameters is only 2.06 M. The proposed network is evaluated on two datasets. It achieves state-of-the- art performance 94.10% mean IOU on Cata7 and obtains a new record on EndoVis 2017 with a 4.10% increase on mean IOU.'\\n\\n'To address these issues, an attention-guided lightweight network (LWANet) is proposed to segment surgical instruments in real-time. It adopts encoder-decoder architecture to get high-resolution masks, which can provide more detailed location information for robot control. A lightweight network, MobileNetV2 [8], is adopted as the encoder. It owns fast inference speed and has powerful feature extraction capabilities. Besides, we design a lightweight attention decoder to recover the location details. Depthwise separable convolution [9] is used as a basic unit to construct the decoder. It factorizes a standard convolution into two parts to reduce the computational costs and model size. To better recover location details, transposed convolution is used to perform upsampling in the decoder.'\\n\\n'Attention fusion block is designed to fuse high-level and low-level features. It introduces global average pooling to capture global contexts and encodes semantic dependencies between channels. Since different channels correspond to the various semantic response, this block can distinguish target regions and background by semantic dependencies between channels. By emphasizing the specific channels, it can focus on target regions and accurately locate surgical instruments, contributing to solving the specular reflection and shadow issues as well as improving the segmentation accuracy. Furthermore, attention fusion block only takes little computational costs, contributing to improving inference speed.'\\n\\n'Due to the limitation of computing resources, the application of deep learning models in robots is very difficult. To address this issue, we propose the attention-guided lightweight network (LWANet) to segment robotic instruments in realtime. It adopts encoder-decoder architecture to acquire high- resolution masks and provide detailed location information. The architecture of LWANet is shown in Fig. 2. To reduce computational costs, a lightweight network, MobileNetV2, is used as an encoder to extract semantic features. It is based on the inverted residual block, which is fast and memory efficient. The last two layers of mobilenetv2 are dropped, including the average pooling layer and the fully connected layer. They are not suitable for semantic segmentation task. The output scale of the MobilenetV2 is 1\\/32 of the original image. Upsampling is bound to increase the computational cost of the network. Therefore, a lightweight attention decoder is designed to recover position details. It only takes little computational costs, contributing to realtime segmentation for surgical instruments. The output scale of LWANet is 1\\/4 of the original image. The lightweight attention decoder will be introduced in detail next.'\\n\\n'The architecture of Attention-guided Lightweight Network and its components. (a) Attention-guided Lightweight Network: it adopts the encoder- decoder architecture. (b) Attention Fusion Block (c) Depthwise Separable Convolution'\\n\\n'B. Lightweight Attention Decoder'\\n\\n'The lightweight attention decoder consists of depthwise separable convolution [9], attention fusion block, and transposed convolution. The depthwise separable convolution is used as the basic unit of the decoder, contributing to reducing computational costs. Attention fusion block captures global contexts and encodes semantic dependencies between channels to focus on target regions. Besides, transposed convolution is adopted to perform upsampling.'\\n\\n'Depthwise separable convolution is adopted as the basic unit of the decoder, replacing the standard convolution. Depthwise separable convolution factorizes a standard convolution into a depthwise convolution and a pointwise convolution, breaking the interaction between the size of the kernel and the channels of output [9]. In this way, it can reduce the computational cost. Its architecture is shown in Fig. 2(c). We consider a case that a convolution takes a d1 \\u00d7 m \\u00d7 n feature map as input and produces a d2 \\u00d7 m \\u00d7 n feature map, where d1 and d2 is the number of feature map channels. When the kernel size is k \\u00d7 k, the computational cost of standard convolution is k \\u00d7 k \\u00d7 d1 \\u00d7 d2 \\u00d7 m \\u00d7 n. The computational cost of depthwise separable convolution is k \\u00d7 k \\u00d7 d1 \\u00d7 m \\u00d7 n + d1 \\u00d7 d2 \\u00d7 m \\u00d7 n [9].'\\n\\n'Global average pooling is essential to capture global contexts and encode semantic dependencies [20], [21]. It squeezes global contexts into an attentive vector to encode semantic dependencies between channels. Then, the attentive vector is transformed by convolutions to further capture semantic dependencies. The generation of the attentive vector is shown in Eq.(2). The output x is generated by Eq.(4).'\\n\\n'The decoder recovers the position details and obtains high-resolution feature maps by upsampling. However, upsampling often results in blurred edges and reduces image quality. To address this issue, transposed convolution is introduced to perform upsampling. It can learn the weights to suit various objects, helping preserve edge information. In this way, we can acquire refined edges and improve segmentation accuracy.'\\n\\n'Surgical videos or images are difficult to obtain. Also, the annotation for the surgical instrument takes a lot of time and costs. Thus, a transfer learning strategy is adopted to overcome this difficulty. We use samples from other tasks to improve the segmentation accuracy for surgical instruments. In our network, the encoder MobileNetV2 [9] is pre-trained on the ImageNet. Images in the ImageNet are all from life scenes. By pre-training, the network can learn low-level features such as boundary, color, and texture of objects. These features can also be applied in surgical scenes. In this way, the encoder has a better ability to extract low-level features. Then the network is trained on surgical instrument datasets to capture high-level semantic features of instruments. This strategy improves network performance and accelerates network convergence.'\",\"944\":null,\"945\":null,\"946\":\"'Deep convolutional neural networks (DCNNs) are known to need a large amount of training data to avoid over-fitting, and manual annotation is a time-consuming and error-prone task. In multi-task learning, even more complicated annotation is required. For instance, in SSD-6D and this work, the semantic annotation, the bounding boxes, and the precise pose of the shafts are necessary. To address this issue, we partially relied on the automatic annotation of CG images using an open-source rendering software (Blender, Blender Foundation, Netherlands), using a rendering pipeline based on [19].'\",\"947\":\"'https:\\/\\/sites.google.com\\/site\\/cathetersegmentation\\/'\\n\\n'Accurate real-time catheter segmentation is an important pre-requisite for robot-assisted endovascular intervention. Most of the existing learning-based methods for catheter segmentation and tracking are only trained on smallscale datasets or synthetic data due to the difficulties of ground-truth annotation. Furthermore, the temporal continuity in intraoperative imaging sequences is not fully utilised. In this paper, we present FW-Net, an end-to-end and real-time deep learning framework for endovascular intervention. The proposed FW-Net has three modules: a segmentation network with encoder-decoder architecture, a flow network to extract optical flow information, and a novel flow-guided warping function to learn the frame-to-frame temporal continuity. We show that by effectively learning temporal continuity, the network can successfully segment and track the catheters in real-time sequences using only raw ground-truth for training. Detailed validation results confirm that our FW-Net outperforms stateof-the-art techniques while achieving real-time performance.'\\n\\n'While deep learning-based approaches can learn meaningful features from input data, applying deep learning to catheter segmentation problem is not straightforward due to the lack of real X-ray data, and the tediousness when manually labeling ground-truth. In this work, we propose to learn from raw ground-truth data and encode the temporal consistency between neighborhood X-ray frames. This will help the network rely more on the temporal information to segment the catheter in X-ray sequences.'\\n\\n'An overview of our FW-Net architecture. The network consists of three modules: a segmentation network with encoder-decoder architecture and skip connections, a flow network to extract optical flow information from two neighborhood frames, and a flow-guided warping function to learn the frame-to-frame temporal continuity.'\\n\\n'Our specific segmentation task is to compute a binary mask separating the foreground (i.e., catheter and guidewire) from the background for every X-ray frame of the video. Inspired by the effectiveness of deep neural networks in image segmentation, we build our segmentation branch based on encoder-decoder architecture [17] [43]. To improve the real-time performance of the network, we use big convolution kernels with large strides to extract features from the input X-ray frame. Since the convolution operation is comparably cheap with a small number of channels as in X-ray images, using big kernels does not significantly increase the computational costs. Furthermore, we combine large strides with skip connections as in U-Net architecture [17] to maintain low-level features during the decoding process.'\\n\\n'Specifically, the input of the segmentation network is the RGB X-ray image of size (256 \\u00d7 256) pixels. The encoder network has 5 ResNet blocks [44] to extract the depth features from input images. Each ResNet block consists of a convolutional layer, ReLU, skip links and pooling operations. The output map after each ResNet block in the encoder network has the size of 128,64,32,16, and 8 respectively. Each decoder block is associated with an encoder. In each decoder block, the encoder feature map is upsampled using the deconvolutional operation. Finally, a 2 classes soft-max layer is used at the end of the decoder network to classify the background and foreground for all pixels in the current X-ray frame.'\\n\\n'Unlike the traditional image segmentation problem, where the temporal information is not available, in video segmentation, temporal consistency across frames is the key to success. Our observation is that the consecutive X-ray frames are highly similar. This similarity is even stronger in the deep feature maps since they encode high level semantic concepts from these frames [48]. We exploit the similarity by warping the deep features from segmentation network with the flow motion from flow network.'\\n\\n'We also observe a significant improvement of our FW-Net over Adaptive U-Net and Siamese U-Net, which are the deep learning-based methods exploit the temporal information. It shows that our proposed flow-guided warping method can encode the temporal information more successfully than Adaptive U-Net (which only trains the video frame sequentially) or Siamese U-Net (which relies heavily on data augmentation). We also found that all networks exploit temporal information achieve better results than the original U-Net. However, since all the network are trained using the raw ground-truth, other deep networks except our FW-Net cannot outperform the classical TCF method.'\",\"948\":\"'2D DDU-Net architecture. This encoder-decoder network applies the distributed dense connection. The semantic context including the original data is transferred to a deeper layer and reused by each stage at the encoder path.'\\n\\n'This network inherits the U-Net architecture with five stages at each side and four concatenations between encoder and decoder modules. At the encoder side of the network, we use the skip connections to transmit features between each stage in a distributed dense form. Throughout the encoder path, the last layer of the current stage is down-sampled by max pooling, and become the first layer of its subsequent stage. Meanwhile, down-sampling is also operated by average pooling on the first layer of the current stage so that it can be concatenated to the first layer of the subsequent stage.'\",\"949\":null,\"950\":null,\"951\":null,\"952\":\"'Algorithm 1 gives the pseudocode for the proposed tracking method. We implemented this method using TensorFlow [26], and all independent threads and for-loops except for the outermost loop (\\u21132) can be executed efficiently in parallel on GPUs. Our prototype implementation can simultaneously track 10 poses at about 30 fps on a single NVIDIA RTX2080 GPU. The rest of this section explains the variables and procedures of Algorithm. 1.'\",\"953\":\"'https:\\/\\/github.com\\/aljosaosep\\/4DGVT'\\n\\n'https:\\/\\/github.com\\/aljosaosep\\/4DGVT'\",\"954\":null,\"955\":null,\"956\":\"'The STRF took the tracking trajectories for all previous frames as input, and output the final counting number. It consisted of two steps: 1) spatial encoding; and 2) temporal response filtering. The spatial encoding stage processed each video frame independently, and each detected pig candidate in the frame was assigned a code number based on their spatial locations. The temporal response filtering stage examined each candidate\\u2019s trajectory across time and obtained a count number, counti \\u2208{0,1,\\u22121}, for this single candidate. The final counting result was the sum of all count number for all candidates:\\\\n\\u2211\\\\nN\\\\ni=0\\\\ncoun\\\\nt\\\\ni\\\\n.'\\n\\n'In the temporal response filtering step, lists of spatial codes in temporal order were generated for each trajectory. One trajectory had one list of spatial codes, and each element of the list corresponded to a time point. Fig. 6c illustrated one example of one single pig trajectory from time point t\\u22126 to time point t, where the blue color represented code 1 and the red color represented code 0. As it was shown, the generated temporal code was [0, 0, 0, 0, 1, 1, 1] from t \\u2212 6 to t. The final count for this trajectory counti was obtained as the sum of the first order difference of the temporal codes. In this case, the count would be 1, which indicated that this pig was scanned once (from deactivated zone into activated zone) and the total count should be added by 1. Similarly, Fig. 6d showed a pig trajectory with code [1, 0, 0, 0, 1, 1, 0, 0] and sum of the the first order difference inferred that the count was \\u22121. This meant that this pig, which has been counted before, moved from scanned zone to to-be scanned zone. Thus, the total count should minus 1. This design enabled the algorithm to avoid false positives counting caused by pig movements into\\/out camera view. Fig. 6e-g showed examples when the pig trajectory count was 0. Fig. 6e-f represented pig trajectories that never went across the scanning line. Fig. 6g represented cases where the trajectory started and ended in the same activity zone. These examples demonstrated that SFRT would not be influenced by the tracking failures (e.g. broke one trajectory into several cased by occlusion) that happened only in one single zone. In this study, a lowpass filter with window size of 5 was applied before the first order differential calculation. This low-pass filtering step was designed to avoid the trajectory jitter near the activity scanning line. The final counting result for the whole video also added the number of detected candidates in deactivated zone of the beginning frame and the number of detected candidates in activated zone of the ending frame.'\",\"957\":\"'We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https:\\/\\/sites.google.com\\/view\\/6packtracking.'\",\"958\":null,\"959\":null,\"960\":null,\"961\":null,\"962\":null,\"963\":null,\"964\":null,\"965\":null,\"966\":null,\"967\":null,\"968\":\"\\\"Autonomous robots have the potential to serve as versatile caregivers that improve quality of life for millions of people worldwide. Yet, conducting research in this area presents numerous challenges, including the risks of physical interaction between people and robots. Physics simulations have been used to optimize and train robots for physical assistance, but have typically focused on a single task. In this paper, we present Assistive Gym, an open source physics simulation framework for assistive robots that models multiple tasks. It includes six simulated environments in which a robotic manipulator can attempt to assist a person with activities of daily living (ADLs): itch scratching, drinking, feeding, body manipulation, dressing, and bathing. Assistive Gym models a person's physical capabilities and preferences for assistance, which are used to provide a reward function. We present baseline policies trained using reinforcement learning for four different commercial robots in the six environments. We demonstrate that modeling human motion results in better assistance and we compare the performance of different robots. Overall, we show that Assistive Gym is a promising tool for assistive robotics research.\\\"\\n\\n'Assistive Gym is a simulation framework with high level interfaces for building and customizing simulation environments for robots that physically interact with and assist people. Assistive Gym environments are built in the open source PyBullet physics engine [11]. PyBullet presents several benefits for simulating physical human-robot interaction, including real time simulation on both CPUs and GPUs, soft bodies and cloth simulation, and the ability to programmatically create robots and human models of varying shapes, sizes, weights, and joint limits. Assistive Gym integrates directly into the OpenAI Gym interface, allowing for the use of existing control policy learning algorithms, such as deep reinforcement learning.'\\n\\n'We presented Assistive Gym, an open source physics simulation framework for assistive robotics. Assistive Gym focuses on physical interaction between robots and humans. It models human physical capabilities and preferences for receiving assistance. We have provided baseline policies for four robots and six assistive tasks. We also demonstrated the use of Assistive Gym for benchmarking, for developing environments for assistive tasks, and for comparing robots. Overall, we have shown that Assistive Gym is a promising open source framework for the development of autonomous robots that can provide versatile physical assistance.'\",\"969\":null,\"970\":null,\"971\":null,\"972\":null,\"973\":null,\"974\":\"'Haptic cues are easy to decode for operators'\",\"975\":null,\"976\":null,\"977\":null,\"978\":null,\"979\":null,\"980\":\"'https:\\/\\/github.com\\/marinaKollmitz\\/learn-collisions'\\n\\n'Our approach presents the first work for collision prediction in the indoor mobile robotics context that allows the robot to collect and integrate new training examples after an initial training phase since the robot can collect collision examples with the onboard bumper sensor in a self-supervised fashion. Thus, the robot can adapt to new environments by learning better models to represent them. Our simulated dataset and the code for our work are available at https:\\/\\/github.com\\/marinaKollmitz\\/learn-collisions.'\\n\\n'https:\\/\\/github.com\\/marinaKollmitz\\/learn-collisions'\",\"981\":\"'https:\\/\\/youtu.be\\/vBPKiqtCYRU'\\n\\n'This approach allows the encoding of observed sequences of agent positions and robot actions, which can be used by the decoder stage to generate likely responses of all agents to a robot\\u2019s action. The generative model is used within the MCTS to simulate state transitions for sampled actions during a tree search of the robot\\u2019s action space.'\\n\\n'Path planning in dynamic environments can be formulated as a sequential decision making problem. By including all relevant agent dynamics in the current state, it can be framed as a Markov Decision Process (MDPs). This can be achieved by using a trajectory prediction model to encode the observed sequence in a hidden state of an RNN, which we detail further in Section III.'\\n\\n'Fig. 2 illustrates the overall architecture of our approach, outlining the use of the predictive model to first encode the observed trajectories for t \\u2264 Tobs \\u2212 1. The state of the root node of the MCTS Sroot is formed from the final encoded state ht\\u22121 and the current observation Xt. During the creation of the search tree, the predictive model is again used at each expanded node to predict the next state S\\u2032, given the state-action pair (S, a) in the MCTS simulation step. Our integrated predictive planner is summarised in Alg. 1.'\\n\\n'System overview illustrating the use of a learnt model of social response within a tree search based planner. After training (blue), the Encoder\\u2019s final hidden state for a given observed sequence is used alongside the latest observation Xt as the root state of the planner (red). The Decoder can then be used in a single step to simulate state transitions of the MDP for a given action A and node state S.'\\n\\n'Model Structure: The Encoder and Decoder have the same structure, comprised of a non linear embedding layer that takes in the input at each timestep, followed by two LSTM [25] layers. The inputs of the Encoder are made up of Xt and Rt+\\u0394t for all t \\u2264 Tobs \\u2212 1. The current observation at t = Tobs is used as the first input to the Decoder.'\\n\\n'The Decoder takes the same size inputs as the Encoder, however, at all timesteps after the first Decoder input we feed the Decoder zeros in place of the agent positions. This is done for both training and inference. This zero-feed approach has been shown to improve performance at inference time, when there are no known ground truth agent positions [26]. This is in comparison to other approaches that use a sample from the output of the prior step as input to the next step. The non-linear embedding layer uses Rectified Linear unit (ReLu) activations and the same weights for both encoding and decoding steps. The outputs of the Decoder are passed through a linear layer that maps to a bivariate Gaussian output for each agent\\u2019s position at each predicted timestep.'\\n\\n'Training: We use variable length encoding sequences between 8 and 20 timesteps. We decode for a fixed length of 8 timesteps, and compare the output of each Decoder step, \\u0176t, to the ground truth positions of each agent Yt. Training of the generative RNN is done so as to minimise the loss shown in Eq. 2, which is the negative log-likelihood of Y given \\u0176, across all prediction timesteps.'\\n\\n'This also allows parallelisation of the simulation stage, as all simulations now run for the same number of iterations and use the same Decoder model. We alter the selection stage to find the K best nodes to expand in the tree. This is implemented by updating the number of traversals ni across each node in between selections, before we simulate. This results in a temporarily decreased value of the node as determined by the UCT method in Eq. 1, and so a decreased likelihood of selecting a node from the same branch.'\\n\\n'Implementation: Each dataset has been split into 5 non-overlapping sets, of which 1 has been left for testing. We have used a 20% validation split during training. The network is implemented in tensorflow with ADAM optimiser for 100 epochs on a single Titan-X GPU, taking approximately 1 hour to train. Inference time per decoder step is less than 0.1ms.'\",\"982\":null,\"983\":null,\"984\":null,\"985\":\"'Numerical simulation results based on the model: a) Left and right bristles\\u2019 speed vs. frequency, and b) average and differential speed between the two sides, with color coded predicted direction of motion. In this sample calculation, the bristle angle is 45\\u00b0, \\u00b5s =0.4, and \\u00b5k =0.3.'\",\"986\":\"'(a) A perspective view of HAMR-Jr with components labeled. Also indicated are the axes of motion for lift and swing DOFs. (b) Schematic of HAMR-Jr\\u2019s transmission (rear right leg) with subsystems color coded. The length and width of the actuators are also denoted. Inset on the bottom left is a schematic of the flexure joints highlighting the relevant dimensions.'\",\"987\":null,\"988\":\"'We are interested in controlling robots from high- level specifications, such that the specifications capture what the robot should be doing and our algorithms automatically create and execute the implementation, or provide feedback as to why the task cannot be guaranteed. This approach takes much of the burden away from the user to manually encode an implementation and allows behaviors to be deployed quickly and intuitively. An underlaying assumption in this approach is that robots have a collection of actions, or skills, they can execute, for example moving a block if the robot has an arm or moving between rooms if the robot has a mobile base. These skills are automatically composed to achieve the high-level task.'\\n\\n'Approach: We encode the specification and current skills in a game structure. The preconditions of skills are encoded in\\\\n\\u03c6\\\\nt\\\\ns\\\\n, restricting when the system is able to perform a skill. The postconditions of skills are encoded in\\\\n\\u03c6\\\\nt\\\\ne\\\\n, where given a skill, the environment can only make certain symbols True. In Example 1, the task is encoded in\\\\n\\u03c6\\\\ng\\\\ns\\\\nas \\u25a1\\u25car0, the preconditions of skill a0 are encoded in\\\\n\\u03c6\\\\nt\\\\ns\\\\nas \\u25a1 (\\u00ac \\u25ef r0 \\u2192 \\u00ac \\u25ef a0), and the postconditions of skill a0 are encoded in\\\\n\\u03c6\\\\nt\\\\ne\\\\nas \\u25a1 (a0 \\u2192 \\u25ef(r1 \\u02c5 r2)).'\\n\\n'We presented an algorithm for suggesting new or modified robot skills that repair an unrealizable task, and demonstrated it on a physical Baxter robot and a simulated KUKA IIWA arm. Our skill suggestions are generated by symbolically repairing a specification encoded in the GR(1) fragment of LTL.'\",\"989\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/33123408'\\n\\n'https:\\/\\/youtu.be\\/pn6afwf5INc'\",\"990\":null,\"991\":\"'http:\\/\\/www.ibex-lib.org\\/'\\n\\n'We have computed the prototype equilibrium configurations for several active joint configurations and loadings using the numerical solver IBEXSOLVE, which is distributed in the open source library IBEX (http:\\/\\/www.ibex-lib.org\\/). It implements a typical branch-and-prune algorithm [30], [31] relying on numerical constraint programming, i.e., contractors like HC4 and interval Newton operators [32], which is able to compute all solutions of Eq. (1) with certification.'\",\"992\":null,\"993\":\"'https:\\/\\/www.fit-pc.com\\/web\\/'\\n\\n'This paper presents the development and results of a large 3 d.o.f cable-driven parallel robot (CDPR) that has been extensively used between June and August 2019 for an artistic exhibition. The purpose of the exhibition was to 3D print a wall of glass powder, which will slowly collapse after the deposit of each layer. Positioning control on the assigned trajectory was an issue because of the CDPR geometry imposed by the specific configuration of the exhibition place. We describe how this problem was solved using a combination of cable length estimation based on the winch rotation measured by encoder, together with 3 on-board lidars that were used to provide a measure of the robot position. To the best of our knowledge this is the first time that such method was used for controlling a large CDPR. This CDPR has run for 174 hours since 6\\/18\\/2019, averaging a run time of 4h15mn per day. The 3D printing of the wall started on 7\\/18\\/2019 and stops on 8\\/31\\/2019. During this period the robot was used for 32 days with an average of 2h18mn run-time per day. The robot has traveled on a total distance of 4757 meters, of which 3893 meters on the assigned trajectory. During the period 76 layers have been deposited, representing a mass of 1.5 tons of glass powder.'\",\"994\":null,\"995\":null,\"996\":null,\"997\":null,\"998\":\"'Much progress has been made on allowing robots to solve challenging problems purely from sensor data, e.g. RGB images and joint encoder readings [1], [2], [3], [4], [5], [6]. These approaches can be very reactive, and they operate on sensor data rather than needing models of the environment. However, task specification remains a problem: even recent work in one-shot learning from demonstration focuses on tasks in visually simple environments, and requires a video demonstration of the task to be executed [3], [7], [4].'\\n\\n'We then train a decoder\\\\nf\\\\ndec\\\\n(\\\\ns\\\\n^\\\\nt\\\\n)\\\\nto predict the current logical state lt. The network also produces two auxiliary outputs: depth image Idepth,t, and semantic segmentation image Iseg,t. For our experiments, we chose a high-level time window of size N = 3: at each time step t, the network intakes three of the most recent RGB image data Irgb,(t\\u22122):t and robot arm joint angles q(t\\u22122):t. The High-Level V-TCN is shown in Fig. 2.'\\n\\n'Each of the temporal RGB images is encoded using convolutional layers. We use CoordConv for the first two convolutional layers to add additional information about spatial relationships [29]. We use a single fully connected layer to increase the dimensionality of arm joint angles to match the encoded RGB image features. The encoded values of arm joint angles and RGB images are then concatenated and further encoded with more convolutional layers. As spatial information is critical, no pooling is used in this network. The encoded data from each time slice is then convoluted in the temporal dimension to capture temporal information. The temporally encoded features are further encoded with dense layers before features are sampled to extract latent features similarly to Variational Autoencoder.'\\n\\n'The latent features are used to compute boolean-valued predicates using dense layers. The latent features are also decoded using deconvolutional layers into current depth images and semantic segmentation images to improve training.'\",\"999\":\"'Depth by Poking model architecture. Disjoint feature pyramidal networks are used to encode input images. Feature maps are then processed by a depth estimation head predicting\\\\nZ\\\\n^\\\\nand an uncertainty estimation head predicting\\\\nV\\\\n^\\\\n. Dashed boxes denote loss functions. Gray arrows denote supervision signal. Elementwise merging of the feature pyramids makes the architecture easy to modify for RGB Only models.'\\n\\n'In our experiments we represent DbP with a FCN having an encoder-decoder architecture with disjoint encoders for the I and D inputs shown in g. 4. The encoders are implemented as feature pyramidal networks (FPN) [24], each with a ResNet-101 backbone [25] pretrained on the MS COCO object detection dataset [26]. Since the depth encoder has one channel, its input layer can\\u2019t be loaded from COCO and is instead randomly initialized. The encoders\\u2019 output feature maps are merged by elementwise addition. The decoder is a simpler architecture that uses convolution transpose layers to upsample feature maps followed by dimension-preserving convolution layers with 3 \\u00d7 3 filters. The output layer has linear activations and 1 \\u00d7 1 convolutions.'\\n\\n'2) Deep Autoencoders'\\n\\n'For the adversarial dataset, all baselines failed to predict reasonable estimates at grasp points. Inaccurate values reported by the RealSense camera caused these errors, even in the autoencoder baseline which relied on noisy depth maps for training data. For example, we observed that depth predictions of the RealSense camera to a mirror\\u2019s surface could be off by up to 5 meters. These large errors can be seen in the top right of g. 7. DbP avoids these errors by predicting end effector positions, which are not sensitive to surface specular properties. Nevertheless, the adversarial items still proved more difficult for DbP than the consumer goods dataset, even though it consisted of fewer items. Here the RGB Only models performed more similarly to the RGB-D models, indicating that DbP can use visual cues, rather than simply denoising the depth input.'\\n\\n'Depth by Poking improves on standard structured light based depth estimation by training a model on physical pressure feedback measurements from an end effector. We\\u2019ve shown this technique generalizes to complete depth maps, despite being trained on single-pixel labels. DbP can also model uncertainty with minor changes to its architecture and loss function. Here we experimented with two types of uncertainty representations and demonstrate that both were helpful for pruning hard picks. This approach outperforms structured light sensors and deep autoencoder baselines for both non-reflective objects, as well as reflective, shiny, or transparent objects on which other methods fail.'\",\"1000\":null,\"1001\":null,\"1002\":\"'http:\\/\\/mprg.jp\\/research\\/arc_dataset_2017_e'\\n\\n'The typical approach for semantic segmentation has the structure of an encoder-decoder network. For example, SegNet [8] performs the segmentation with an encoder-decoder structure, where the encoder repeats convolution and max pooling and the decoder repeats upsampling and convolution and expands the feature maps to the input size.'\\n\\n'As the structure of DSSD is similar to the encoder-decoder structure of SegNet, our concept is to have multi-task DSSD (MT-DSSD) improve accuracy by learning object detection and semantic segmentation at the same time. Furthermore, we add a grasping point detector for the suction cup to this model. The network structure of our MT-DSSD is shown in Fig. 2. We use the VGG[26]-based DSSD as a base and add a layer that performs semantic segmentation and object grasping detection. As a result, the three tasks (object detection, semantic segmentation, and grasping detection) are performed simultaneously using the common feature maps.'\",\"1003\":null,\"1004\":\"'https:\\/\\/sketchfab.com\\/'\\n\\n'We further developed a texture modification method driven by the fact that objects are often slightly modified from their original appearance, e.g. by placing a bar code sticker or product label on them. We model these effects by randomly projecting an image randomly selected from an Internet search using the keyword \\\"product label\\\" onto the object surface. We call this extension Sticker Projection (SP).'\",\"1005\":\"'This paper presents an approach to address data scarcity problems in underwater image datasets for visual detection of marine debris. The proposed approach relies on a two-stage variational autoencoder (VAE) and a binary classifier to evaluate the generated imagery for quality and realism. From the images generated by the two-stage VAE, the binary classifier selects \\\"good quality\\\" images and augments the given dataset with them. Lastly, a multi-class classifier is used to evaluate the impact of the augmentation process by measuring the accuracy of an object detector trained on combinations of real and generated trash images. Our results show that the classifier trained with the augmented data outperforms the one trained only with the real data. This approach will not only be valid for the underwater trash classification problem presented in this paper, but it will also be useful for any data-dependent task for which collecting more images is challenging or infeasible.'\\n\\n'In this paper, we focus on improving underwater object classification tasks to enable high accuracy trash detection by proposing a novel approach to realistically extend available datasets both in volume and variety. Specifically, we adopt deep generative models, in particular, variational autoencoders (VAEs) to synthesize realistic images of underwater trash. Generative methods estimate a probabilistic mapping from random noise, sampled from a Gaussian distribution, to input data (e.g., images). After training, these methods are used to generate realistic representations of the input data by feeding random noise to the mapping. In this way, we can increase the number of images while adding variety by using a generative model to create synthetic images to expand the original dataset. Generative models have gained popularity since the appearance of Generative Adversarial Network (GAN) [12] due to its relatively straightforward and effective approach in tackling the generation problem. Other generative models such as VAE [13], Bayesian networks [14], and Gaussian mixture models [15] have also been shown to work well in image generation problems. We have decided to use VAE-based models to generate synthetic images, for reasons which are discussed in Section II. In this paper, we make the following contributions:'\\n\\n'B. Variational Autoencoder'\\n\\n'The VAE consists of two neural networks which are an encoder q\\u03c6(z|x) and a decoder p\\u03b8(x|z). The encoder outputs the parameters of the normal distribution, \\u00b5 and \\u03c3. From N(\\u00b5,\\u03c3), the \\u03ba-dimensional latent variable z is sampled. Once the latent variable z is sampled and provided as an input to the decoder, it generates the reconstructed original input\\\\nx\\\\n^\\\\n. The cost function for the VAE is shown in Eq. 1 where \\u00b5gt is a ground-truth probability measure and\\\\n\\u222b\\\\nX\\\\n\\u03bc\\\\ngt\\\\n(dx)=1\\\\n. The function is minimized by stochastic gradient descent and the VAE jointly learns the latent variable and inference models during the training.'\",\"1006\":\"'Our proposed spatiotemporal representation learning architecture trained using the GAN framework. The generator is a LSTM-LSTM encoder with a 3D convolution decoder. The discriminator uses a LSTM-LSTM encoder. After the representation is learned, the latent space will be used in a Reinforcement Learning framework to learn task- specific policies.'\\n\\n'The core component of our representation learning model is the Layered Spatiotemporal Memory Long Short-Memory (LSTM-LSTM) network in the encoder that encodes the image sequences into a lower dimensional space. The decoder takes the lower dimensional latent space and applies 3D convolutions (3D-Conv) to upsample and generate the predicted outputs. The architecture for the LSTM-LSTM network is presented in Figure 2. During training, the image sequences are divided into two parts: the observed past and the future sequence. The future frames are used to calculate the loss. Our goal is not just to predict the future, but rather to use future prediction as a surrogate to evaluate how \\\"good\\\" that latent space is. If the system is able to generate reasonable future predictions from this latent space, then it should have the relevant spatiotemporal information. Compared to standard autoencoders which takes one image at a time, it is the ability to extract information from input sequences that distinguishes the proposed model from existing ones.'\\n\\n'Given the power of the LSTM-LSTM encoder, we do not need to have two separate discriminators: spatial and temporal. Instead, we use a discriminator\\\\nD\\\\nthat mirrors the architecture of the one used in encoding the latent space in the generator\\\\nG\\\\n(although with different weights). Following the Wasserstein GAN [38] (WGAN) formulation where the objective is to obtain'\\n\\n'In practice, we pretrain the generator network by autoencoder training using L1 + L2 loss to provide the\\\\nG\\\\nwith good initial weights, before switching to GAN training. For the discriminator, since our task is to generate realistic future videos, we compare the ground truth sequence of images with the generated sequence of images directly. For example, if we are using the first 10 frames to predict the next 10 frames in a video, x would be the entire 20 frames while\\\\nx\\\\n\\u02dc\\\\nwould be the 10 input frames plus the 10 predicted frames.'\\n\\n'We formulate the policy learning as a model-free RL problem where the high-dimensional image stream is fed into the LSTM-LSTM encoder to obtain an encoded latent space which is used as the state space for the learner. Specifically, we chose Proximal Policy Optimization (PPO) [49], a policy gradient algorithm that adds a soft constraint on the objective function making it simpler to implement. Our policy network is a 3-layer ConvNet followed by a fully-connected layer that takes the latent space representation as input and produces the translation and rotation velocity for the robot. Policy learning is not the focus of this paper.'\",\"1007\":\"'Physics Prediction: Propagation networks encode the states of the objects and the relations between them separately. This encoding is carried out by two encoders, one for the relations denoted by\\\\nf\\\\nenc\\\\nR\\\\nand one for the objects denoted by\\\\nf\\\\nO\\\\nenc\\\\n, defined as follows:'\\n\\n'To predict the next state of the system, these encoders are used in the subsequent propagation steps within two different propagator functions,\\\\nf\\\\nl\\\\nR\\\\nfor relations and\\\\nf\\\\nl\\\\nO\\\\nfor objects, at the propagation step l, as follows:'\\n\\n'Belief Regulation: The success of physics prediction step highly depends on how accurate the environment is encoded in the graph structure. Here we refer to the term belief as the estimated world state and given previous states and motor commands, the role of the belief regulation module is to constantly update this crucial part. In propagation network [4], the authors provide a method for estimating unknown parameters using gradient updates. In theory, it is possible to adapt this framework, however, since the relation types need to be represented as one-hot vectors, employing a continuous representation may lead to unreliable predictions. As the main theoretical contribution of this work, we propose a temporal propagation network architecture that augments a propagation network with a recurrent neural network (RNN) unit to regulate beliefs regarding object and relation information over time. More formally, it takes a sequence of a set of state variables during the action execution as input and employing a secondary special-purpose propagation network, it encodes these structured observations, which are then fed into an RNN cell to update the current world state, as follows:'\\n\\n'Our physic prediction module takes object position, velocity and radius as object features, and joint relation type between objects as relation features. More specifically, object encoder is a MLP with 3 hidden layers of 150 neurons, and it takes object radius and velocity as inputs. The relation encoder is a MLP with 1 hidden layer of 100 neurons. It takes radius, joint relation type, and position and velocity differences between the objects as input. While our relation propagator is an MLP with 2 hidden layers of 150 neurons, our object propagator is an MLP with one hidden layer of 100 neurons. During training, at each epoch, we validated our physics prediction module on the validation set containing instances from the sparse configuration and selected the model that has the lowest mean squared error (MSE) over 200 time-step trajectory roll-outs.'\\n\\n'For quantitative analysis, we compared our method with PropNets with alternative (hard-coded) relation assignments: As a strong baseline, PropNetgt uses ground-truth relations. PropNetf assumes all pairs of contacting objects have fixed relations between them. PropNetn assumes no joints between objects. Furthermore, to analyze the influence of temporal data in predicting relations within our model, we also report results with 1-step BRDPN that predicts object relations using only the observation from the previous step.'\",\"1008\":\"'Self-driving vehicles (SDVs) hold great potential for improving traffic safety and are poised to positively affect the quality of life of millions of people. To unlock this potential one of the critical aspects of the autonomous technology is understanding and predicting future movement of vehicles surrounding the SDV. This work presents a deep-learning- based method for kinematically feasible motion prediction of such traffic actors. Previous work did not explicitly encode vehicle kinematics and instead relied on the models to learn the constraints directly from the data, potentially resulting in kinematically infeasible, suboptimal trajectory predictions. To address this issue we propose a method that seamlessly combines ideas from the AI with physically grounded vehicle motion models. In this way we employ best of the both worlds, coupling powerful learning models with strong feasibility guarantees for their outputs. The proposed approach is general, being applicable to any type of learning method. Extensive experiments using deep convnets on real-world data strongly indicate its benefits, outperforming the existing state-of-the-art.'\\n\\n'Our proposed vehicle kinematic layer is agnostic to the model architecture and the learning method, and it can be used to replace the trajectory decoding module (e.g., LSTM decoder) or policy model of the above related work to improve accuracy and guarantee kinematic feasibility.'\",\"1009\":null,\"1010\":\"'This paper describes a collection of novel encoder-decoder deep neural networks that exploit historical data to model data fields from a limited number of continuous observations. We leverage a network architecture used for image inpainting, the task of filling in holes in an image, to infer the data field from a limited number of samples. The network is trained on historic data and example sampling paths. To facilitate the selection of future samples, our method trains two additional networks to serve as arbiters of information value. One network is trained to err on the side of overestimation, while the other is trained to underestimate. These two networks can then be used as a measure of variance to investigate future samples for potential information gain.'\\n\\n'The proposed network architecture to handle time varying data builds upon our two-dimensional design. The data in the three-dimensional case is time series data. To handle this, we propose a recurrent network utilizing convolutional LSTM layers with modified partial convolution to handle sparse data. The network has a similar architecture to the 2D case, with an encoder-decoder network architecture that iteratively fills in the missing data through partial convolution. The recurrent nature of the network provides a memory of previously sampled data, allowing the network to learn how the data field will change through time and use previously sampled data in its current estimate. Each encoder layer has a corresponding decoder layer, which receives the encoded information along with all high level encoder information. These two sources of information are concatenated together in the filter dimension and used as inputs into the current masked convolutional LSTM layer. This allows the network to utilize high level information from the encoder without losing lower level information from the less encoded pieces of data. The same process occurs for the LSTM memory as well. Figure 2 provides a detailed illustration of the network architecture.'\",\"1011\":\"'https:\\/\\/youtu.be\\/_eEQnEUJrYY'\\n\\n'The architecture of the prediction model is an encode-process-decode graph network. The input graph Gin is encoded into a latent space using a graph independent block, i. e. , nodes, edges and global attributes are transformed individually. We execute a full graph network block ten times as the core processing step of our model. The latent representation is then transformed into the output graph Gout using a graph independent block as a decoder.'\\n\\n'We chose an encode-process-decode structure for the graph network (see Figure 3). First, the input graph Gin is encoded where nodes, edges, and globals are expanded into a latent representation. The process step consists of a full graph network block which processes the latent representation 10 times. The process block uses Multi-layer perceptrons (MLPs) with batch normalization as update functions:'\",\"1012\":\"'Our P23 process combines the P2 and P3. Pseudo code of P23 is shown in Algorithm 1. P23 includes recursive call of P23 to make loop until reaching goal. Subroutines of P23 are described as follows:'\",\"1013\":null,\"1014\":\"'https:\\/\\/github.com\\/personalrobotics'\\n\\n'https:\\/\\/github.com\\/mayoyamasaki'\\n\\n'https:\\/\\/github.com\\/andreaazzini'\\n\\n'https:\\/\\/aair-lab.github.io\\/ll.html'\\n\\n'Naive approaches for using critical regions (either learned or hand-coded) in existing sampling-based motion planners tend to fail: merely increasing the probability of sampling from such regions does not improve the performance of RRT planners because the parts of the tree(s) closest to the critical regions tend to be those that are crashing into the walls adjacent to them. Even when allowing a PRM planner to select configurations from critical regions as vertices in its roadmap, its simple local planner is unable to connect the vertices in critical regions to those uniformly sampled unless a considerable amount of time is used in the roadmap building process (see Figure 2). The LL planners presented in this paper leverage the positives of these planners while having the necessary modifications to properly utilize critical regions.'\\n\\n'The coupling of learning and MP has been extensively investigated in the past. As discussed in the introduction, naive approaches for using learning to bias sampling in stochastic motion planners don\\u2019t perform well. Recent work by Ichter et al. uses a conditional variational autoencoder to bias sample points for MP conditioned on encoded environment variables [11]. This encoding is generalizable to higher dimensions. However, it requires structuring the data to encompass the state of the robot, the environment, the obstacles (encoded as an occupancy grid), and the start and goal configurations. Moreover, during inference, the network model requires this expensive data structuring again, which can take around 50 seconds. In contrast, we focus on image-based learning where data can be easily generated for training using a top-view camera. Moreover, inferences can also be made using a top-view image of the environment in less than 5 seconds. Havoutis et al. use topology to learn sub-manifold approximations that are defined by a set of possible trajectories in the C-space [12]. This requires either motion plans that are generated through a motion capture device, or hand-crafted partial plans. Pan et al. use instance-based learning where prior collision results are stored as an approximate representation of the collision space and the free C-space [13]. This is used to make cheaper probabilistic queries. Although their method shows significant improvement in some environments, their work is limited in finding solutions through narrow passages between obstacles where the optimal solution may lie. In our work, the network learns the positions of regions that are critical for a given class of MP problems, but have a low probability of getting sampled under a uniform distribution, such as narrow regions. The closest related work to this paper presented a preliminary version of our methods in a non-archival venue [14]. The current paper rigorously develops and presents our full approach.'\\n\\n'We propose a general structure for a convolutional encoder-decoder neural network which learns to detect critical regions.'\\n\\n'In the decoder network, corresponding deconvolutional layers to the encoder network are used. The upsampled output is used for pixel-wise classification using a softmax cross-entropy loss function. Each layer in the network is activated using ReLu nonlinearity.'\\n\\n'We presented a new approach to learning for MP and used it to create a new suite of sampling-based motion planners, Learn and Link. We also constructed a fully convolutional encoder-decoder neural network to learn critical regions for MP problems that generalizes across different domains. Our model is used by our LL planners to remedy the limitations of uniform sampling without compromising guarantees of correctness.'\\n\\n'https:\\/\\/github.com\\/personalrobotics'\\n\\n'https:\\/\\/github.com\\/mayoyamasaki'\\n\\n'https:\\/\\/github.com\\/andreaazzini'\\n\\n'https:\\/\\/aair-lab.github.io\\/ll.html'\",\"1015\":null,\"1016\":null,\"1017\":null,\"1018\":null,\"1019\":\"'Figure 2B shows the output layer actions for the discrete algorithm. We use 5 nodes to encode the possible angular velocities for the discrete DDQN ([\\u201390, \\u201345, 0, 45, 90] deg\\/s) with a linear activation function. For the continuous DDPG and PPO a single node with a hyperbolic tangent activation function is used. This value multiplied by an hyper-parameter, is used end-to-end as angular velocity in a range (\\u201390, 90) deg\\/s. Section V present the experimental results in terms of: (i) travel distance; (ii) successful attempts; (iii) training times. It is important to notice that given the low update rate of the laser sensor, both movebase and the continuous models can fail in some cases where the discrete model is able to succeed.'\",\"1020\":null,\"1021\":\"'In this section, we describe our robust policy optimization algorithm, for the pseudocode see Algorithm 1. At iteration k, we select the controller that maximizes the EHI criterion. Then, we run two experiments to estimate its performance and robustness. For the performance, we introduce a state and action dependent reward and we define the return as the average reward obtained over an episode. The performance index is defined as the expectation of the return, which we approximate with a Monte Carlo estimate over multiple episodes. To estimate the robustness, we use the experiments from Sec. III-B. We update the data set with the experiments results. Finally, we update the estimate of the Pareto front that is used to compute the EHI as the set of dominating points of the data set. Other options to compute such estimate from the posterior of the GP exist. However, they are computationally more expensive and they resulted in a similar performance in our experiments. In the end, the algorithm returns an estimate of the Pareto set and front. The choice of a controller from the Pareto set depends on the performance-robustness trade-off required by the test applications and, therefore, the choice is left to the practitioner.'\",\"1022\":null,\"1023\":null,\"1024\":null,\"1025\":\"'We can see that the number of primary PIs starts at around 200 and then increases in several steps to about 900, while in between, the number also decreases sometimes, indicating expert contraction. From scan 800 on, the number of experts does not change and only primary PIs are inserted. This is the phase when the robot travels back to the starting point and gaps are filled in in unobserved locations. Local experts usually encode single geometric features such as a wall, a corner, or clutter. From error results, we can see that increased noise leads to larger errors and larger numbers of PIs. This is because the algorithm mistakes the noise for surface details and spends more PIs to encode the structure. In this case, the features encoded by single experts are smaller. Finally, we also observe that Basement-real is 300s long, and our algorithm runs for 142.74s which means that it is potentially suitable for real-time application.'\",\"1026\":null,\"1027\":\"'As in most architectures, the earliest stage performs feature extraction from the input images. The shared encoder, depicted in blue in Figure 2, is made of two initial 3 \\u00d7 3 convolutions extracting c features and bringing the resolution to half, then followed by four blocks each one containing a 2 \\u00d7 2 max-pooling operation and two 3 \\u00d7 3 layers. The four respectively extract 2c, 4c, 8c, 16c features while progressively halving the resolution, i.e.\\\\n1\\\\n4\\\\n,\\\\n1\\\\n8\\\\n,\\\\n1\\\\n16\\\\nand\\\\n1\\\\n32\\\\nrespectively. Batch normalization and ReLU operations follow all convolutional layers. Features extracted by this module are processed by two subnetworks, in charge respectively of semantic segmentation and disparity estimation. This forces RTS2Net to learn a general and enriched representation meaningful for both tasks. This design allows us for a dramatic reduction of the computational cost compared to much more complex encoders such as VGG [56], yet enabling accurate results. In particular, previous works [12] proved that a tiny amount of features, i.e. c=1, already enables for decent disparity estimation while significantly increasing the framerate. However, it is insufficient to learn a representation good enough for semantic segmentation too.'\\n\\n'We adapted this approach to the fully residual strategy followed both in the disparity network and in the semantic decoder. To achieve this, we perform a cascade of residual concatenations between semantic class probabilities and disparity volumes. The refinement module, in purple in Figure 2, performs three steps: 1) in order to limit computational time and balance the contributions in the hybrid volume, we compress the semantic embedding so to have dimensionality similar to the disparity cost volume, 2) we concatenate compressed semantic features with disparity volumes (reorganized so to have disparity dimension as channels) to form the hybrid volumes, in the second and third stage we also concatenate the upsampled previously computed refined disparity, 3) the hybrid volume is then processed through three 2D convolutional layers, producing disparity residuals summed up to the original, reorganized volumes on which the soft-argmin operator is applied.'\",\"1028\":\"'Deep neural networks have significantly enhanced the performance of various computer vision tasks, including single image depth estimation and image segmentation. However, most existing approaches handle them in supervised manners and require a large number of ground truth labels that consume extensive human efforts and are not always available in real scenarios. In this paper, we propose a novel framework to estimate disparity maps and segment images simultaneously by jointly training an encoder-decoder-based interactive convolutional neural network (CNN) for single image depth estimation and a multiple class CNN for image segmentation. Learning the neural network for one task can be beneficial from simultaneously learning from another one under a multi-task learning framework. We show that our proposed model can learn per-pixel depth regression and segmentation from just a single image input. Extensive experiments on available public datasets, including KITTI, Cityscapes urban, and PASCAL-VOC demonstrate the effectiveness of our model compared with other state-of-the-art methods for both tasks.'\\n\\n'Overview of the proposed learning framework. The proposed architecture consists of two tasks (single image depth estimation and segmentation) and five loss constraints from both spectral and spatial perspectives across the tasks. The shared encoder is connected to the respective decoder of each task to produce a pixel-wise depth map and segmentation.'\",\"1029\":\"'https:\\/\\/github.com\\/pczhao\\/TA_GaitDesign.git.'\\n\\n'https:\\/\\/github.com\\/pczhao\\/TA_GaitDesign.git.'\",\"1030\":null,\"1031\":null,\"1032\":null,\"1033\":\"'code-replacement'\\n\\n'In this paper, we have shown that transformation techniques can be realized on plans designed for robots acting in the real world by using code replacements. For utilizing the system on the robot, a plan is first projected, then automatically transformed and, finally, executed right away in the real world to produce more efficient behavior.'\\n\\n'One limitation of our approach is that the transformations cannot change the original tree structure of the plan, i.e. add or remove nodes. Similarly, it is difficult to alter an already transformed node, as that requires examining the code replacement of the node, which cannot be done robustly without considering all other code replacements in the tree.'\",\"1034\":\"'Representation of downsampled version of our reachability database. (Left) Reachable poses represented by colored arrows, the color encodes the manipulability of the corresponding manipulator configuration. (Right) The cross sectional view of the voxelized 3d workspace of a Fetch robot, the color indicates the number of grasping poses contained in the 3d voxel. For further pose query, the entire 92 million poses are distributed to 2 million 6d voxels, instead of 3d voxels.'\",\"1035\":\"'Fig. 5 illustrates our component-based architecture. Each component is a block that shows the component name and its interfaces. As shown in yellow, the main component comprises four manipulation components for the tasks and one navigation component. The manipulation components are separated from perception, and the navigation in dark gray is supported by a few other components. Common code components are shown in red with only interface files.'\\n\\n'The Unified Modeling Language (UML) diagram of our architecture. Yellow, blue, cyan, gray and red blocks indicate the main, manipulation, perception, navigation and common code components respectively.'\\n\\n'After the competition, we continued to improve the code base, then evaluated the performance. We conducted 20 nonstop full runs in our test course and collected data from metrics such as task completion time, perception time, motion planning time, manipulation time, and task failure rate. We also recorded recoverable and non-recoverable failures. Fig. 6 shows images from a successful run.'\",\"1036\":null,\"1037\":null,\"1038\":null,\"1039\":null,\"1040\":\"'Generative networks such as Autoencoders (AE) and Variational Autoencoders (VAE) [9], [16] that leverage reconstruction loss.'\\n\\n'Since these methods are based on reconstruction of the input image, to generate the VBP image for the modified generative network, we use the same prediction module that was trained for our task-aware to maintain the same generated network saliency maps. We train the autoencoders with standard mean squared error loss and the variational autoencoders with the summed loss of reconstruction loss with the Kullback-Leibler divergence loss. Novelty is detected by establishing a cut-off threshold in reconstruction loss based on the training loss distribution and determine if the reconstruction for a new image is greater than the cut-off. In our experiments we use a cut-off of \\u03b1 = 0.01 as in [9].'\",\"1041\":\"'As cars can only locate on road surface and rotate along the axis that is perpendicular to the road, we encode this domain knowledge with two priors for the pose estimation. Besides, since cars cannot have randomly diverse shapes, we further regularize the estimated shape to be close to our mean shape. Our prior term is therefore defined as:'\",\"1042\":null,\"1043\":\"'PWC-Encoder'\\n\\n'PWC-Decoder.'\\n\\n'In our network, after cropping and resizing the vehicle region from the last layer of the PWC-Net encoder using ROIAlign [26], the feature vector fi is aggregated by two convolution layers. By considering the distance estimation as a regression problem, di is obtained through several fully connected layers, which takes deep feature clue fi and geometric clues fy\\/(bi \\u2212 ti), fx\\/(ri \\u2212 li) as input. Accordingly, we formulate the distance regression function Fdist as follows:'\\n\\n'Instead of inputting the bounding box directly, we utilize its homogeneous coordinates as another geometric clue combined with intrinsic parameters of the camera. In order to get optical flow clue, ROIAlign is applied in every flow pyramid layer from PWC-Net decoder to obtain multi-scale flow patches of the vehicle. Then these flow patches are concatenated and reconstructed into a flow clue vector mi. Altogether, all extracted clues are listed bellow:'\\n\\n'In order to analyse the efficiency of our distance regression model, we trained an independent network only based on the PWC-Net encoder without vehicle-centric operation. Deep feature clue and two geometric supplementary clue are feed into 4 fully connect layers to estimate the distance to vehicles. The training data consists of 3800 frames from KITTI raw dataset with at least one moving vehicle in each frame and 764 frames constitutes the testing data. From Table. V, our method outperforms the 3Dbbox and Unsfm in every metric. Note that Unsfm gets the worst results, because it is trained using camera ego-motion and the image warping consistency which is not satisfied for a dynamic scene with moving vehicles. As the first place method on KITTI depth prediction benchmark, DORN shows remarkable performances, yet our method achieves competitive results and gets less outliers than it.'\",\"1044\":\"'Fig. 1 shows the architecture that enables the short-term DFT prediction of our model. This architecture encodes the GelSight image into a texture representation vector and combines it with the encoded action representation from the user\\u2019s force and speed to predict the desired DFT using an acceleration predictor module. This structure was chosen empirically through a hyper-parameter search.'\\n\\n'The architecture is trained in two stages. First, we train the image encoder augmented with three fully connected layers for texture classification using a cross-entropy loss. Afterwards, we freeze these pre-trained weights and use the output of the image encoder as input to the texture encoder. We then train the full architecture for predicting the DFT magnitude of the accelerations. The choice of freezing over fine-tuning was motivated by its better performance on the validation set. As loss we choose the Euclidean distance between the ground truth and predicted magnitude up to 1000 Hz (100 DFT bins).'\\n\\n'Finally, as a qualitative metric, we gain insight regarding the texture encoder in our generative model by visualizing the high-dimensional texture representation vector in our model (the output of texture encoder in Fig. 1) in a 2D space using a dimension reduction technique called t-Distributed Stochastic Neighbor Embedding (t-SNE) [32].'\\n\\n'Looking at the t-SNE visualization of the representation vector learned by the material encoder provides insight into the source of such improvement. This method visualizes high-dimensional vectors (here of the size 256) in a lower-dimensional space (here 2D). Fig. 4 shows that our texture encoder has learned to place the materials that feel similar closer to each other. For example, our encoder has created approximate clusters for all carpets as well as artificial grass (which feels like carpet), meshes, similar floortiles, stones, and similar sandpapers. Our hypothesis is that a unified model enables our network to share data between similar materials and cover a wider range of force and speed. Thereby, the unified model achieves an improved generalization performance.'\",\"1045\":\"'The underlying technical requirement for implementing a BLDC in a wearable device is the position estimation problem. The existing solution from Texas Instruments injects current at the motor startup to align the electrical angle and mechanical angle. However, this is not applicable in a wearable device since the motor already contains the added inertia of the human interface when the current is injected. We customized the code to read our optical encoder\\u2019s index pin, such that the arbitrary mechanical angle of the encoder at the startup is refreshed to an accurate joint angle by a wearer moving their arm and hitting the index pin. Then the electric angle aligns to the mechanical angle to be used in the Field Oriented Control (FOC). Precise angle estimation is crucial for FOC to generate an accurate torque output.'\\n\\n'For the second test session, two different weights pull on the exoskeleton via a weight on a rope passing over a pulley. The rope was connected 13 cm from the elbow joint on the exoskeleton. Thus, a constant bias torque is transferred to the arm through the exoskeleton. The order of the test was: neutral (NLF & NLE), 500g of flexional preloading, 1000g of flexional preloading (FLF & FLE), 500g of extensional preloading, then 1000g of extensional preloading (ELF & ELE). The 500g weights correspond to a preload torque of 0.638Nm, while the 1000g weights correspond to a preload torque of 1.275Nm. Extensional preloading means the weight is pulling the arm away from the body such that the individual must create a torque using their biceps, and for the flexional preloading the individual must create a torque with their triceps. The state machine is coded such that when the experiment yields a converged value, it notifies the experimenter by changing the GUI to its IDLE color. Then the experimenter attaches a new weight and proceeds with the test.'\",\"1046\":\"'The conceptual design of the 6 DoF manipulator is based on the four-bar-linkage mechanism. An active ring-type joint is designed for the gimbal mechanism, where a human wrist could be located, to realize the 3 DoF rotating motion of the human wrist. Six actuators are used to control the manipulator. The linear motion and linear force display are controlled with the first three joints. The other three actuators are designed to be at the end-point to control the rotational motion and rotational force display. The joint position of the device is measured by the encoder and the force\\/torque is measured by the F\\/T sensor implemented at the end-effector of the manipulator. By this configuration, the 6 DoF force is displayed.'\",\"1047\":null,\"1048\":null,\"1049\":null,\"1050\":\"'Soft robotics promises developments in the research areas of safety, bio-mimicry, manipulation, human-robot interaction, and alternative locomotion techniques. The research presented here is directed towards developing an improved, low-cost, and open-source method for soft robotic control using electrorheological fluids in compact, 3D-printed electroactive hydraulic valves. We construct high-pressure electrorheological valves and deformable actuators using only commercially available materials and accessible fabrication methods. The printed valves were characterized with industrial-grade electrorheological fluid (RheOil 3.0), but the design is generalizable to other electrorheological fluids. Valve performance was shown to be an improvement over comparable work with demonstrated higher yield pressures at lower voltages (up to 230 kPa), larger flow rates (up to 15 ml\\/min) and lower response times (1 to 3 seconds, depending on design). The resulting valve and actuator systems enable future novel applications of electrorheological fluid-based control and hydraulics in soft robotics and other disciplines.'\",\"1051\":\"'Comparisons of group results on the rise time in three different experimental conditions. Green: negative, blue: positive, orange: variable. The same color codes are used in all subsequent figures. Bars and error bars denote the means and 95% confidence intervals. Asterisks denote statistically difference for pairwise comparison: *: p < 0.05, **: p < 0.01, ***: p < 0.001.'\",\"1052\":\"'The emergence of the variational Bayes auto-encoder (VAE) framework [13] paved the way for optimizing inference and learning probabilistic distributions in latent variables, by re-parameterizing the variational lower bound. This framework has been extended for studying on-line interaction, as for example anticipating human behavior [14].'\\n\\n'The software platform: The open-source implementation of the models is provided by the neural robotics library [20]. From previous experiences [21], C++ was chosen as the base programming language. The programs run in the Robot Operative System (ROS) Kinetic Kane over Ubuntu 16.04 LTS. The Network block (Fig. 2) ran at 4 Hz in the host computer (Alienware Aurora R7, 12 Intel\\u00ae Core\\u2122 i7- 8700K CPU at 3.70GHz, and 31.1 GiB RAM memory). In interaction mode, BPTT was computed within a sliding window (20 time steps) during 28 epochs. The models learned 12 degrees of freedom (6 for each arm), constant desired references were given to the torso and the head joints.'\",\"1053\":null,\"1054\":null,\"1055\":null,\"1056\":\"'The primary link lengths of the RAVEN-S design were chosen to reach at least 75 mm past the primary workspace for rodent dissection. From this requirement, worst-case joint loads were calculated from the requirements and used to drive gearbox and motor selection. Since the first two axes have equivalent maximum moment arms, they were designed with the same motor and gearbox. Furthermore, advantages of repeated equipment outweighed the small weight and cost savings of a different motor gearbox pair for the third joint. The gearbox chosen is the Harmonic Drive CSG-14-100-2UH-LW, a single stage gearbox with a reduction of 100:1. The nearly negligible backlash results in a theoretical worst-case accuracy of less than 0.1 mm. The motor paired to this gearbox is the 70 Watt brushless Maxon EC-45 with integrated encoder. Once the motors, gearboxes, and motor controllers were chosen, the link lengths and structure were then fine-tuned to minimize mass, volume and moment arms. Each link is encased in protective covers that will be smooth and easy to clean with disposable wipes after missions.'\",\"1057\":\"'https:\\/\\/vosizneias.com\\/'\",\"1058\":null,\"1059\":\"'The goal of this paper is to generate simulations with real-world collision scenarios for training and testing autonomous vehicles. We use numerous dashcam crash videos uploaded on the internet to extract valuable collision data and recreate the crash scenarios in a simulator. We tackle the problem of extracting 3D vehicle trajectories from videos recorded by an unknown and uncalibrated monocular camera source using a modular approach. A working architecture and demonstration videos along with the open-source implementation are provided with the paper.'\",\"1060\":null,\"1061\":null,\"1062\":null,\"1063\":\"'The next domain uses a genetic algorithm to solve job-shop problems (JSP). A JSP has a set of jobs composed of a sequence of tasks that must be scheduled on a set of machines. The genetic algorithm is a standard open-source Python implementation based on swap mutation and generalized order crossover used to solve JSPs approximately [38]. Solution (schedule) quality is approximated using the time required to complete the longest job as the lower bound \\u2113jsp.'\\n\\n'The final domain uses simulated annealing to solve quadratic assignment problems (QAP). A QAP has a set of facilities that must be assigned to a set of locations where a distance is given for each pair of locations and a flow is given for each pair of facilities. The simulating annealing algorithm is a standard open-source Fortran implementation used to solve QAPs approximately [39]. Solution (assignment) quality is approximated using the Gilmore-Lawler bound, the optimal cost of a linearized QAP [40], as the lower bound \\u2113qap.'\\n\\n'We now evaluate our meta-level control approach on a mobile robot domain. On an iClebo Kobuki in simulation, we use a path planning algorithm that computes path plans that minimize the probability of collision gradually from an open-source robotics C++ framework called epic [41]. Solution (path plan) quality is defined as safety in terms of the probability of collision. The mobile robot must therefore trade computation time with safety. Fig. 1 depicts a simple demonstration of the mobile robot domain in simulation.'\",\"1064\":\"'http:\\/\\/imitrob.ciirc.cvut.cz\\/schedulComplex.html'\",\"1065\":null,\"1066\":\"'https:\\/\\/github.com\\/raide-project\\/ctf_public'\\n\\n'https:\\/\\/github.com\\/raide-project\\/ctf_public'\\n\\n'CtF Code: https:\\/\\/github.com\\/raide-project\\/ctf_public'\",\"1067\":null,\"1068\":null,\"1069\":\"'We consider the problem of planning views for a robot to acquire images of an object for visual inspection and reconstruction. In contrast to offline methods which require a 3D model of the object as input or online methods which rely on only local measurements, our method uses a neural network which encodes shape information for a large number of objects. We build on recent deep learning methods capable of generating a complete 3D reconstruction of an object from a single image. Specifically, in this work, we extend a recent method which uses Higher Order Functions (HOF) to represent the shape of the object. We present a new generalization of this method to incorporate multiple images as input and establish a connection between visibility and reconstruction quality. This relationship forms the foundation of our view planning method where we compute viewpoints to visually cover the output of the multiview HOF network with as few images as possible. Experiments indicate that our method provides a good compromise between online and offline methods: Similar to online methods, our method does not require the true object model as input. In terms of number of views, it is much more efficient. In most cases, its performance is comparable to the optimal offline case even on object classes the network has not been trained on.'\\n\\n'In this paper, we present a new approach which combines the strengths of online and offline approaches. Our approach builds on recent progress on single-view reconstruction methods in which prior information about shapes of objects is encoded by a neural network capable of generating a 3D model of the object from a single image [1]\\u2013[4].'\\n\\n'We show that a similar approach can be used to encode prior information which can then be used for planning views effectively. These images afterwards can be used to generate fine scale reconstructions using traditional approaches or other visual inspection tasks such as quality inspection.'\",\"1070\":\"'https:\\/\\/sites.google.com\\/view\\/srrn\\/home'\\n\\n'In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRRN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world. The code and training environment for this project is made publicly available at https:\\/\\/sites.google.com\\/view\\/srrn\\/home.'\\n\\n'Successful trajectory achieved by sRRN in a real world cluttered indoor environment running as the local component of the ROS navigation stack. We colour code the trajectory to indicate which control effect was executed in the environment. Purple regions indicate execution of prior only in events of high policy uncertainty. The red regions indicate execution of the combined systems for improved navigation performance.'\",\"1071\":null,\"1072\":\"'https:\\/\\/sites.google.com\\/view\\/reconstruction-grasp\\/'\\n\\n'We implicitly encode geometry by introducing the point cloud embedding from PointSDF into a grasp success prediction network [8]. We enable explicit geometric reasoning by constraining the optimization of our grasp success prediction network to be collision free. We achieve this by extending previous approaches to learning-based grasp optimization [8], [9], [11] to include the full robot arm configuration, instead of only a 6DOF wrist pose, and add SDF collision constraints between the reconstructed object and all links of the robot. By formulating the optimization in the robot joint space, we ensure not only kinematic feasibility of all synthesized grasps but also Euclidean updates of the gradients in the optimization by propagating learned gradients through the kinematic Jacobian. We examine the efficacy of our approach through real robot grasping experiments on a KUKA LBR4 robot with an Allegro multi-fingered hand.'\\n\\n'We present a new architecture for predicting a 3D reconstruction of an object from a single view point cloud. Motivated for its use in grasp planning, we desire that our reconstruction approach seamlessly handles seen and unseen objects alike from arbitrary viewpoints and accurately encodes geometric concepts, while efficiently performing inference in terms of both time and space. As such, we propose learning to directly predict the signed distance function, which implicitly represents the object surface as the zero level set of the function. The signed distance function defines the shortest distance between a query point in 3D space and the surface of the object, where distances are negative for points inside the object and positive when outside. We call our architecture PointSDF. Unlike previous iterations of similar design [21], we enable single-pass evaluation using a simple encoder-decoder structure.'\\n\\n'In order to make our grasp success prediction network geometrically aware, we utilize the same point cloud encoder architecture used in PointSDF to embed the point cloud of the target object. Along with the grasp configuration, the embedding is passed through several fully-connected layers, leading to a single prediction passed through a sigmoid activation to get our grasp success probability estimate. Our network design is shown in the bottom half of Figure 2.'\"},\"Stars\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":2,\"4\":-1,\"5\":-1,\"6\":31,\"7\":-1,\"8\":115,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":4,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":26,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":18,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":139,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":14,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":3,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":14,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":1,\"250\":-1,\"251\":2,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":4,\"297\":0,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":0,\"312\":-1,\"313\":3,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":0,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":13,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":6,\"361\":7,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":0,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":80,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":5,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":88,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":127,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":5,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":4,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":15,\"561\":34,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":38,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":41,\"601\":49,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":7,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":14,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":20,\"651\":7,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":2,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":3,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":17,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":356,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":0,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":0,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":2,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":6,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":3,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":76,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":12,\"895\":56,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":-1,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":2,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":3,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1,\"1022\":-1,\"1023\":-1,\"1024\":-1,\"1025\":-1,\"1026\":-1,\"1027\":-1,\"1028\":-1,\"1029\":6,\"1030\":-1,\"1031\":-1,\"1032\":-1,\"1033\":-1,\"1034\":-1,\"1035\":-1,\"1036\":-1,\"1037\":-1,\"1038\":-1,\"1039\":-1,\"1040\":-1,\"1041\":-1,\"1042\":-1,\"1043\":-1,\"1044\":-1,\"1045\":-1,\"1046\":-1,\"1047\":-1,\"1048\":-1,\"1049\":-1,\"1050\":-1,\"1051\":-1,\"1052\":-1,\"1053\":-1,\"1054\":-1,\"1055\":-1,\"1056\":-1,\"1057\":-1,\"1058\":-1,\"1059\":-1,\"1060\":-1,\"1061\":-1,\"1062\":-1,\"1063\":-1,\"1064\":-1,\"1065\":-1,\"1066\":11,\"1067\":-1,\"1068\":-1,\"1069\":-1,\"1070\":-1,\"1071\":-1,\"1072\":-1},\"Forks\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":11,\"4\":-1,\"5\":-1,\"6\":6,\"7\":-1,\"8\":33,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":8,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":63,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":7,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":31,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":41,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":3,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":8,\"250\":-1,\"251\":0,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":9,\"297\":1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":1,\"312\":-1,\"313\":13,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":2,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":19,\"361\":37,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":7,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":24,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":298,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":42,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":2,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":2,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":33,\"561\":131,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":9,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":5,\"601\":12,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":21,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":7,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":76,\"651\":2,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":4,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":7,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":79,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":888,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":4,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":5,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":2,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":11,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":4,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":23,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":51,\"895\":4,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":-1,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":9,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":6,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1,\"1022\":-1,\"1023\":-1,\"1024\":-1,\"1025\":-1,\"1026\":-1,\"1027\":-1,\"1028\":-1,\"1029\":4,\"1030\":-1,\"1031\":-1,\"1032\":-1,\"1033\":-1,\"1034\":-1,\"1035\":-1,\"1036\":-1,\"1037\":-1,\"1038\":-1,\"1039\":-1,\"1040\":-1,\"1041\":-1,\"1042\":-1,\"1043\":-1,\"1044\":-1,\"1045\":-1,\"1046\":-1,\"1047\":-1,\"1048\":-1,\"1049\":-1,\"1050\":-1,\"1051\":-1,\"1052\":-1,\"1053\":-1,\"1054\":-1,\"1055\":-1,\"1056\":-1,\"1057\":-1,\"1058\":-1,\"1059\":-1,\"1060\":-1,\"1061\":-1,\"1062\":-1,\"1063\":-1,\"1064\":-1,\"1065\":-1,\"1066\":6,\"1067\":-1,\"1068\":-1,\"1069\":-1,\"1070\":-1,\"1071\":-1,\"1072\":-1},\"Citations\":{\"0\":7,\"1\":17,\"2\":2,\"3\":1,\"4\":0,\"5\":56,\"6\":47,\"7\":7,\"8\":28,\"9\":19,\"10\":8,\"11\":24,\"12\":2,\"13\":6,\"14\":1,\"15\":0,\"16\":0,\"17\":38,\"18\":5,\"19\":19,\"20\":7,\"21\":7,\"22\":4,\"23\":4,\"24\":1,\"25\":1,\"26\":55,\"27\":2,\"28\":3,\"29\":0,\"30\":9,\"31\":11,\"32\":1,\"33\":9,\"34\":4,\"35\":7,\"36\":4,\"37\":3,\"38\":1,\"39\":11,\"40\":15,\"41\":32,\"42\":2,\"43\":11,\"44\":0,\"45\":23,\"46\":15,\"47\":2,\"48\":2,\"49\":28,\"50\":9,\"51\":6,\"52\":0,\"53\":0,\"54\":7,\"55\":2,\"56\":5,\"57\":5,\"58\":3,\"59\":2,\"60\":4,\"61\":15,\"62\":4,\"63\":13,\"64\":10,\"65\":37,\"66\":29,\"67\":9,\"68\":0,\"69\":6,\"70\":2,\"71\":35,\"72\":7,\"73\":10,\"74\":2,\"75\":2,\"76\":4,\"77\":6,\"78\":7,\"79\":28,\"80\":13,\"81\":5,\"82\":6,\"83\":2,\"84\":1,\"85\":2,\"86\":4,\"87\":4,\"88\":4,\"89\":1,\"90\":0,\"91\":3,\"92\":1,\"93\":2,\"94\":3,\"95\":2,\"96\":0,\"97\":4,\"98\":8,\"99\":20,\"100\":1,\"101\":0,\"102\":6,\"103\":0,\"104\":2,\"105\":0,\"106\":15,\"107\":0,\"108\":0,\"109\":5,\"110\":9,\"111\":1,\"112\":1,\"113\":4,\"114\":3,\"115\":0,\"116\":16,\"117\":25,\"118\":7,\"119\":9,\"120\":0,\"121\":14,\"122\":11,\"123\":10,\"124\":15,\"125\":5,\"126\":0,\"127\":1,\"128\":2,\"129\":3,\"130\":31,\"131\":11,\"132\":16,\"133\":1,\"134\":11,\"135\":9,\"136\":0,\"137\":5,\"138\":10,\"139\":6,\"140\":0,\"141\":12,\"142\":5,\"143\":5,\"144\":1,\"145\":2,\"146\":1,\"147\":5,\"148\":3,\"149\":8,\"150\":3,\"151\":16,\"152\":11,\"153\":21,\"154\":4,\"155\":4,\"156\":5,\"157\":0,\"158\":7,\"159\":3,\"160\":48,\"161\":0,\"162\":1,\"163\":5,\"164\":0,\"165\":10,\"166\":12,\"167\":0,\"168\":9,\"169\":2,\"170\":3,\"171\":1,\"172\":21,\"173\":0,\"174\":18,\"175\":3,\"176\":1,\"177\":9,\"178\":23,\"179\":1,\"180\":2,\"181\":2,\"182\":5,\"183\":7,\"184\":3,\"185\":130,\"186\":5,\"187\":3,\"188\":5,\"189\":2,\"190\":3,\"191\":6,\"192\":4,\"193\":4,\"194\":14,\"195\":12,\"196\":5,\"197\":0,\"198\":4,\"199\":1,\"200\":4,\"201\":4,\"202\":4,\"203\":10,\"204\":3,\"205\":3,\"206\":2,\"207\":3,\"208\":14,\"209\":23,\"210\":7,\"211\":3,\"212\":11,\"213\":4,\"214\":6,\"215\":38,\"216\":5,\"217\":1,\"218\":14,\"219\":23,\"220\":5,\"221\":3,\"222\":25,\"223\":14,\"224\":13,\"225\":14,\"226\":18,\"227\":1,\"228\":21,\"229\":8,\"230\":3,\"231\":4,\"232\":12,\"233\":3,\"234\":3,\"235\":2,\"236\":47,\"237\":18,\"238\":2,\"239\":11,\"240\":9,\"241\":30,\"242\":1,\"243\":6,\"244\":12,\"245\":2,\"246\":11,\"247\":30,\"248\":7,\"249\":11,\"250\":1,\"251\":27,\"252\":2,\"253\":1,\"254\":0,\"255\":5,\"256\":21,\"257\":2,\"258\":30,\"259\":0,\"260\":0,\"261\":2,\"262\":30,\"263\":15,\"264\":2,\"265\":0,\"266\":3,\"267\":6,\"268\":11,\"269\":1,\"270\":109,\"271\":5,\"272\":15,\"273\":5,\"274\":9,\"275\":0,\"276\":25,\"277\":22,\"278\":0,\"279\":12,\"280\":2,\"281\":2,\"282\":2,\"283\":21,\"284\":6,\"285\":6,\"286\":5,\"287\":31,\"288\":44,\"289\":6,\"290\":6,\"291\":12,\"292\":3,\"293\":2,\"294\":3,\"295\":11,\"296\":9,\"297\":1,\"298\":0,\"299\":1,\"300\":10,\"301\":1,\"302\":1,\"303\":2,\"304\":1,\"305\":1,\"306\":1,\"307\":0,\"308\":7,\"309\":1,\"310\":3,\"311\":4,\"312\":3,\"313\":12,\"314\":1,\"315\":4,\"316\":13,\"317\":3,\"318\":77,\"319\":8,\"320\":43,\"321\":52,\"322\":4,\"323\":7,\"324\":7,\"325\":32,\"326\":12,\"327\":4,\"328\":12,\"329\":1,\"330\":9,\"331\":1,\"332\":33,\"333\":5,\"334\":2,\"335\":1,\"336\":15,\"337\":9,\"338\":10,\"339\":0,\"340\":1,\"341\":2,\"342\":41,\"343\":12,\"344\":2,\"345\":12,\"346\":11,\"347\":3,\"348\":6,\"349\":3,\"350\":0,\"351\":1,\"352\":0,\"353\":1,\"354\":8,\"355\":13,\"356\":1,\"357\":10,\"358\":11,\"359\":4,\"360\":8,\"361\":31,\"362\":73,\"363\":24,\"364\":11,\"365\":69,\"366\":2,\"367\":0,\"368\":3,\"369\":11,\"370\":7,\"371\":5,\"372\":3,\"373\":11,\"374\":0,\"375\":12,\"376\":0,\"377\":0,\"378\":2,\"379\":3,\"380\":5,\"381\":8,\"382\":4,\"383\":1,\"384\":0,\"385\":7,\"386\":2,\"387\":2,\"388\":4,\"389\":11,\"390\":6,\"391\":6,\"392\":0,\"393\":2,\"394\":0,\"395\":21,\"396\":3,\"397\":4,\"398\":6,\"399\":60,\"400\":4,\"401\":4,\"402\":6,\"403\":8,\"404\":48,\"405\":17,\"406\":6,\"407\":19,\"408\":20,\"409\":58,\"410\":3,\"411\":3,\"412\":2,\"413\":6,\"414\":2,\"415\":13,\"416\":2,\"417\":7,\"418\":5,\"419\":3,\"420\":1,\"421\":2,\"422\":21,\"423\":5,\"424\":2,\"425\":18,\"426\":14,\"427\":16,\"428\":4,\"429\":2,\"430\":13,\"431\":36,\"432\":12,\"433\":18,\"434\":18,\"435\":16,\"436\":2,\"437\":2,\"438\":4,\"439\":44,\"440\":11,\"441\":3,\"442\":4,\"443\":2,\"444\":0,\"445\":2,\"446\":1,\"447\":0,\"448\":13,\"449\":7,\"450\":0,\"451\":3,\"452\":2,\"453\":45,\"454\":8,\"455\":0,\"456\":6,\"457\":2,\"458\":2,\"459\":3,\"460\":1,\"461\":13,\"462\":1,\"463\":34,\"464\":15,\"465\":5,\"466\":2,\"467\":106,\"468\":4,\"469\":5,\"470\":1,\"471\":3,\"472\":5,\"473\":0,\"474\":1,\"475\":1,\"476\":2,\"477\":8,\"478\":3,\"479\":28,\"480\":3,\"481\":0,\"482\":15,\"483\":3,\"484\":9,\"485\":1,\"486\":1,\"487\":3,\"488\":10,\"489\":7,\"490\":6,\"491\":2,\"492\":10,\"493\":8,\"494\":1,\"495\":15,\"496\":5,\"497\":3,\"498\":0,\"499\":3,\"500\":13,\"501\":1,\"502\":1,\"503\":4,\"504\":2,\"505\":2,\"506\":3,\"507\":0,\"508\":3,\"509\":2,\"510\":4,\"511\":0,\"512\":2,\"513\":1,\"514\":1,\"515\":6,\"516\":3,\"517\":6,\"518\":13,\"519\":1,\"520\":3,\"521\":0,\"522\":8,\"523\":1,\"524\":5,\"525\":4,\"526\":7,\"527\":7,\"528\":7,\"529\":11,\"530\":7,\"531\":0,\"532\":31,\"533\":9,\"534\":3,\"535\":10,\"536\":21,\"537\":2,\"538\":8,\"539\":4,\"540\":1,\"541\":15,\"542\":1,\"543\":4,\"544\":2,\"545\":1,\"546\":2,\"547\":7,\"548\":5,\"549\":8,\"550\":2,\"551\":6,\"552\":0,\"553\":1,\"554\":6,\"555\":2,\"556\":2,\"557\":13,\"558\":9,\"559\":14,\"560\":40,\"561\":11,\"562\":1,\"563\":9,\"564\":21,\"565\":20,\"566\":21,\"567\":2,\"568\":1,\"569\":0,\"570\":2,\"571\":4,\"572\":2,\"573\":1,\"574\":3,\"575\":13,\"576\":3,\"577\":3,\"578\":8,\"579\":4,\"580\":3,\"581\":0,\"582\":1,\"583\":42,\"584\":1,\"585\":1,\"586\":6,\"587\":1,\"588\":2,\"589\":0,\"590\":1,\"591\":3,\"592\":2,\"593\":7,\"594\":3,\"595\":2,\"596\":5,\"597\":1,\"598\":4,\"599\":12,\"600\":17,\"601\":20,\"602\":12,\"603\":86,\"604\":11,\"605\":66,\"606\":0,\"607\":12,\"608\":1,\"609\":1,\"610\":9,\"611\":16,\"612\":5,\"613\":41,\"614\":4,\"615\":11,\"616\":145,\"617\":2,\"618\":5,\"619\":6,\"620\":11,\"621\":14,\"622\":3,\"623\":2,\"624\":5,\"625\":4,\"626\":1,\"627\":1,\"628\":14,\"629\":4,\"630\":1,\"631\":6,\"632\":3,\"633\":3,\"634\":6,\"635\":4,\"636\":2,\"637\":16,\"638\":1,\"639\":10,\"640\":12,\"641\":4,\"642\":20,\"643\":1,\"644\":4,\"645\":2,\"646\":3,\"647\":8,\"648\":1,\"649\":5,\"650\":20,\"651\":9,\"652\":19,\"653\":2,\"654\":6,\"655\":8,\"656\":4,\"657\":2,\"658\":0,\"659\":7,\"660\":3,\"661\":0,\"662\":16,\"663\":0,\"664\":5,\"665\":3,\"666\":5,\"667\":0,\"668\":4,\"669\":8,\"670\":6,\"671\":0,\"672\":2,\"673\":2,\"674\":5,\"675\":1,\"676\":0,\"677\":1,\"678\":2,\"679\":16,\"680\":4,\"681\":27,\"682\":1,\"683\":0,\"684\":13,\"685\":0,\"686\":18,\"687\":1,\"688\":4,\"689\":9,\"690\":20,\"691\":45,\"692\":39,\"693\":2,\"694\":19,\"695\":1,\"696\":28,\"697\":38,\"698\":7,\"699\":16,\"700\":3,\"701\":1,\"702\":4,\"703\":0,\"704\":6,\"705\":1,\"706\":6,\"707\":4,\"708\":36,\"709\":8,\"710\":4,\"711\":3,\"712\":2,\"713\":3,\"714\":2,\"715\":1,\"716\":10,\"717\":15,\"718\":7,\"719\":10,\"720\":12,\"721\":7,\"722\":11,\"723\":3,\"724\":5,\"725\":14,\"726\":1,\"727\":10,\"728\":1,\"729\":0,\"730\":1,\"731\":3,\"732\":5,\"733\":0,\"734\":5,\"735\":4,\"736\":8,\"737\":3,\"738\":1,\"739\":3,\"740\":6,\"741\":1,\"742\":2,\"743\":2,\"744\":6,\"745\":3,\"746\":0,\"747\":4,\"748\":0,\"749\":0,\"750\":0,\"751\":4,\"752\":6,\"753\":6,\"754\":3,\"755\":0,\"756\":0,\"757\":8,\"758\":0,\"759\":0,\"760\":0,\"761\":5,\"762\":2,\"763\":34,\"764\":4,\"765\":0,\"766\":37,\"767\":1,\"768\":3,\"769\":3,\"770\":6,\"771\":29,\"772\":6,\"773\":7,\"774\":21,\"775\":17,\"776\":8,\"777\":9,\"778\":6,\"779\":1,\"780\":1,\"781\":6,\"782\":39,\"783\":5,\"784\":15,\"785\":1,\"786\":1,\"787\":8,\"788\":1,\"789\":7,\"790\":2,\"791\":4,\"792\":0,\"793\":13,\"794\":8,\"795\":2,\"796\":3,\"797\":2,\"798\":2,\"799\":1,\"800\":1,\"801\":10,\"802\":1,\"803\":13,\"804\":2,\"805\":1,\"806\":0,\"807\":2,\"808\":3,\"809\":1,\"810\":3,\"811\":17,\"812\":8,\"813\":2,\"814\":2,\"815\":45,\"816\":0,\"817\":0,\"818\":3,\"819\":0,\"820\":0,\"821\":0,\"822\":1,\"823\":20,\"824\":32,\"825\":19,\"826\":6,\"827\":8,\"828\":8,\"829\":48,\"830\":1,\"831\":4,\"832\":4,\"833\":1,\"834\":2,\"835\":1,\"836\":8,\"837\":3,\"838\":4,\"839\":6,\"840\":7,\"841\":1,\"842\":5,\"843\":3,\"844\":2,\"845\":8,\"846\":4,\"847\":11,\"848\":11,\"849\":4,\"850\":5,\"851\":0,\"852\":2,\"853\":12,\"854\":5,\"855\":8,\"856\":0,\"857\":13,\"858\":3,\"859\":2,\"860\":9,\"861\":5,\"862\":53,\"863\":0,\"864\":1,\"865\":1,\"866\":1,\"867\":1,\"868\":1,\"869\":0,\"870\":8,\"871\":1,\"872\":0,\"873\":1,\"874\":0,\"875\":2,\"876\":0,\"877\":7,\"878\":1,\"879\":8,\"880\":11,\"881\":3,\"882\":11,\"883\":11,\"884\":7,\"885\":3,\"886\":2,\"887\":5,\"888\":9,\"889\":11,\"890\":58,\"891\":41,\"892\":1,\"893\":28,\"894\":29,\"895\":26,\"896\":0,\"897\":45,\"898\":9,\"899\":3,\"900\":39,\"901\":104,\"902\":5,\"903\":0,\"904\":0,\"905\":29,\"906\":5,\"907\":7,\"908\":9,\"909\":21,\"910\":29,\"911\":16,\"912\":6,\"913\":2,\"914\":50,\"915\":6,\"916\":3,\"917\":11,\"918\":3,\"919\":7,\"920\":5,\"921\":1,\"922\":3,\"923\":63,\"924\":3,\"925\":15,\"926\":5,\"927\":14,\"928\":1,\"929\":8,\"930\":5,\"931\":1,\"932\":1,\"933\":14,\"934\":0,\"935\":7,\"936\":1,\"937\":4,\"938\":4,\"939\":2,\"940\":6,\"941\":4,\"942\":3,\"943\":16,\"944\":5,\"945\":0,\"946\":6,\"947\":11,\"948\":2,\"949\":0,\"950\":1,\"951\":0,\"952\":4,\"953\":15,\"954\":5,\"955\":6,\"956\":5,\"957\":63,\"958\":9,\"959\":11,\"960\":3,\"961\":4,\"962\":2,\"963\":2,\"964\":1,\"965\":1,\"966\":3,\"967\":5,\"968\":45,\"969\":2,\"970\":2,\"971\":14,\"972\":7,\"973\":2,\"974\":4,\"975\":1,\"976\":11,\"977\":0,\"978\":1,\"979\":9,\"980\":1,\"981\":11,\"982\":10,\"983\":5,\"984\":2,\"985\":3,\"986\":10,\"987\":3,\"988\":4,\"989\":0,\"990\":49,\"991\":1,\"992\":3,\"993\":3,\"994\":1,\"995\":0,\"996\":1,\"997\":3,\"998\":22,\"999\":1,\"1000\":5,\"1001\":1,\"1002\":7,\"1003\":12,\"1004\":18,\"1005\":5,\"1006\":1,\"1007\":5,\"1008\":16,\"1009\":4,\"1010\":2,\"1011\":7,\"1012\":0,\"1013\":29,\"1014\":14,\"1015\":1,\"1016\":26,\"1017\":4,\"1018\":1,\"1019\":22,\"1020\":11,\"1021\":17,\"1022\":1,\"1023\":0,\"1024\":7,\"1025\":6,\"1026\":10,\"1027\":26,\"1028\":5,\"1029\":5,\"1030\":4,\"1031\":8,\"1032\":1,\"1033\":2,\"1034\":5,\"1035\":8,\"1036\":3,\"1037\":9,\"1038\":0,\"1039\":6,\"1040\":1,\"1041\":10,\"1042\":16,\"1043\":5,\"1044\":2,\"1045\":2,\"1046\":0,\"1047\":2,\"1048\":5,\"1049\":7,\"1050\":1,\"1051\":3,\"1052\":7,\"1053\":4,\"1054\":6,\"1055\":1,\"1056\":0,\"1057\":1,\"1058\":38,\"1059\":3,\"1060\":0,\"1061\":0,\"1062\":3,\"1063\":9,\"1064\":4,\"1065\":2,\"1066\":1,\"1067\":5,\"1068\":14,\"1069\":4,\"1070\":11,\"1071\":4,\"1072\":31}}"