"{\"Conference\":{\"0\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"1\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"2\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"3\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"4\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"5\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"6\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"7\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"8\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"9\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"10\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"11\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"12\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"13\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"14\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"15\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"16\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"17\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"18\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"19\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"20\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"21\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"22\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"23\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"24\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"25\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"26\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"27\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"28\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"29\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"30\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"31\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"32\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"33\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"34\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"35\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"36\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"37\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"38\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"39\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"40\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"41\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"42\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"43\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"44\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"45\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"46\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"47\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"48\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"49\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"50\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"51\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"52\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"53\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"54\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"55\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"56\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"57\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"58\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"59\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"60\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"61\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"62\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"63\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"64\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"65\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"66\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"67\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"68\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"69\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"70\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"71\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"72\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"73\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"74\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"75\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"76\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"77\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"78\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"79\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"80\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"81\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"82\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"83\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"84\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"85\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"86\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"87\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"88\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"89\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"90\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"91\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"92\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"93\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"94\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"95\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"96\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"97\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"98\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"99\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"100\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"101\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"102\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"103\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"104\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"105\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"106\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"107\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"108\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"109\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"110\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"111\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"112\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"113\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"114\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"115\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"116\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"117\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"118\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"119\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"120\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"121\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"122\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"123\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"124\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"125\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"126\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"127\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"128\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"129\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"130\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"131\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"132\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"133\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"134\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"135\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"136\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"137\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"138\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"139\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"140\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"141\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"142\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"143\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"144\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"145\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"146\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"147\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"148\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"149\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"150\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"151\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"152\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"153\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"154\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"155\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"156\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"157\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"158\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"159\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"160\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"161\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"162\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"163\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"164\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"165\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"166\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"167\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"168\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"169\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"170\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"171\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"172\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"173\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"174\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"175\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"176\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"177\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"178\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"179\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"180\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"181\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"182\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"183\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"184\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"185\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"186\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"187\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"188\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"189\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"190\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"191\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"192\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"193\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"194\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"195\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"196\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"197\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"198\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"199\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"200\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"201\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"202\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"203\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"204\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"205\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"206\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"207\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"208\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"209\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"210\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"211\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"212\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"213\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"214\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"215\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"216\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"217\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"218\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"219\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"220\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"221\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"222\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"223\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"224\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"225\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"226\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"227\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"228\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"229\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"230\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"231\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"232\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"233\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"234\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"235\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"236\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"237\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"238\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"239\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"240\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"241\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"242\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"243\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"244\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"245\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"246\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"247\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"248\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"249\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"250\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"251\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"252\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"253\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"254\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"255\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"256\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"257\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"258\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"259\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"260\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"261\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"262\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"263\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"264\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"265\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"266\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"267\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"268\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"269\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"270\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"271\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"272\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"273\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"274\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"275\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"276\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"277\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"278\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"279\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"280\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"281\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"282\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"283\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"284\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"285\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"286\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"287\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"288\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"289\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"290\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"291\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"292\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"293\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"294\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"295\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"296\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"297\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"298\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"299\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"300\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"301\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"302\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"303\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"304\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"305\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"306\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"307\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"308\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"309\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"310\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"311\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"312\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"313\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"314\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"315\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"316\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"317\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"318\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"319\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"320\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"321\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"322\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"323\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"324\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"325\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"326\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"327\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"328\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"329\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"330\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"331\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"332\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"333\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"334\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"335\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"336\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"337\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"338\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"339\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"340\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"341\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"342\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"343\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"344\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"345\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"346\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"347\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"348\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"349\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"350\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"351\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"352\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"353\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"354\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"355\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"356\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"357\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"358\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"359\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"360\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"361\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"362\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"363\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"364\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"365\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"366\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"367\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"368\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"369\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"370\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"371\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"372\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"373\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"374\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"375\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"376\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"377\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"378\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"379\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"380\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"381\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"382\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"383\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"384\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"385\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"386\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"387\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"388\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"389\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"390\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"391\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"392\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"393\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"394\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"395\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"396\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"397\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"398\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"399\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"400\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"401\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"402\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"403\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"404\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"405\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"406\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"407\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"408\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"409\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"410\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"411\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"412\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"413\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"414\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"415\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"416\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"417\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"418\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"419\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"420\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"421\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"422\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"423\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"424\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"425\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"426\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"427\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"428\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"429\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"430\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"431\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"432\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"433\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"434\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"435\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"436\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"437\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"438\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"439\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"440\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"441\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"442\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"443\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"444\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"445\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"446\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"447\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"448\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"449\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"450\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"451\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"452\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"453\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"454\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"455\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"456\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"457\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"458\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"459\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"460\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"461\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"462\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"463\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"464\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"465\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"466\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"467\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"468\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"469\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"470\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"471\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"472\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"473\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"474\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"475\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"476\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"477\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"478\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"479\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"480\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"481\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"482\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"483\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"484\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"485\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"486\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"487\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"488\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"489\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"490\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"491\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"492\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"493\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"494\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"495\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"496\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"497\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"498\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"499\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"500\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"501\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"502\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"503\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"504\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"505\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"506\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"507\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"508\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"509\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"510\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"511\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"512\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"513\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"514\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"515\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"516\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"517\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"518\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"519\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"520\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"521\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"522\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"523\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"524\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"525\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"526\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"527\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"528\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"529\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"530\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"531\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"532\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"533\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"534\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"535\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"536\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"537\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"538\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"539\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"540\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"541\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"542\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"543\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"544\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"545\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"546\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"547\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"548\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"549\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"550\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"551\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"552\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"553\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"554\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"555\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"556\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"557\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"558\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"559\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"560\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"561\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"562\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"563\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"564\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"565\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"566\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"567\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"568\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"569\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"570\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"571\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"572\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"573\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"574\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"575\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"576\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"577\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"578\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"579\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"580\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"581\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"582\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"583\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"584\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"585\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"586\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"587\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"588\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"589\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"590\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"591\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"592\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"593\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"594\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"595\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"596\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"597\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"598\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"599\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"600\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"601\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"602\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"603\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"604\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"605\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"606\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"607\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"608\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"609\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"610\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"611\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"612\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"613\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"614\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"615\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"616\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"617\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"618\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"619\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"620\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"621\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"622\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"623\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"624\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"625\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"626\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"627\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"628\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"629\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"630\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"631\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"632\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"633\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"634\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"635\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"636\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"637\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"638\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"639\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"640\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"641\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"642\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"643\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"644\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"645\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"646\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"647\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"648\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"649\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"650\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"651\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"652\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"653\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"654\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"655\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"656\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"657\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"658\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"659\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"660\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"661\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"662\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"663\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"664\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"665\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"666\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"667\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"668\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"669\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"670\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"671\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"672\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"673\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"674\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"675\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"676\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"677\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"678\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"679\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"680\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"681\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"682\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"683\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"684\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"685\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"686\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"687\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"688\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"689\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"690\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"691\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"692\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"693\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"694\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"695\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"696\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"697\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"698\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"699\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"700\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"701\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"702\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"703\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"704\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"705\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"706\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"707\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"708\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"709\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"710\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"711\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"712\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"713\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"714\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"715\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"716\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"717\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"718\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"719\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"720\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"721\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"722\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"723\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"724\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"725\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"726\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"727\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"728\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"729\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"730\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"731\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"732\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"733\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"734\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"735\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"736\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"737\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"738\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"739\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"740\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"741\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"742\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"743\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"744\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"745\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"746\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"747\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"748\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"749\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"750\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"751\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"752\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"753\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"754\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"755\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"756\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"757\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"758\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"759\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"760\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"761\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"762\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"763\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"764\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"765\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"766\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"767\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"768\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"769\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"770\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"771\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"772\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"773\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"774\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"775\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"776\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"777\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"778\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"779\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"780\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"781\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"782\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"783\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"784\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"785\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"786\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"787\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"788\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"789\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"790\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"791\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"792\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"793\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"794\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"795\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"796\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"797\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"798\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"799\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"800\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"801\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"802\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"803\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"804\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"805\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"806\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"807\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"808\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"809\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"810\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"811\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"812\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"813\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\"},\"Year\":{\"0\":\"Date of Conference: 21-25 May 2018\",\"1\":\"Date of Conference: 21-25 May 2018\",\"2\":\"Date of Conference: 21-25 May 2018\",\"3\":\"Date of Conference: 21-25 May 2018\",\"4\":\"Date of Conference: 21-25 May 2018\",\"5\":\"Date of Conference: 21-25 May 2018\",\"6\":\"Date of Conference: 21-25 May 2018\",\"7\":\"Date of Conference: 21-25 May 2018\",\"8\":\"Date of Conference: 21-25 May 2018\",\"9\":\"Date of Conference: 21-25 May 2018\",\"10\":\"Date of Conference: 21-25 May 2018\",\"11\":\"Date of Conference: 21-25 May 2018\",\"12\":\"Date of Conference: 21-25 May 2018\",\"13\":\"Date of Conference: 21-25 May 2018\",\"14\":\"Date of Conference: 21-25 May 2018\",\"15\":\"Date of Conference: 21-25 May 2018\",\"16\":\"Date of Conference: 21-25 May 2018\",\"17\":\"Date of Conference: 21-25 May 2018\",\"18\":\"Date of Conference: 21-25 May 2018\",\"19\":\"Date of Conference: 21-25 May 2018\",\"20\":\"Date of Conference: 21-25 May 2018\",\"21\":\"Date of Conference: 21-25 May 2018\",\"22\":\"Date of Conference: 21-25 May 2018\",\"23\":\"Date of Conference: 21-25 May 2018\",\"24\":\"Date of Conference: 21-25 May 2018\",\"25\":\"Date of Conference: 21-25 May 2018\",\"26\":\"Date of Conference: 21-25 May 2018\",\"27\":\"Date of Conference: 21-25 May 2018\",\"28\":\"Date of Conference: 21-25 May 2018\",\"29\":\"Date of Conference: 21-25 May 2018\",\"30\":\"Date of Conference: 21-25 May 2018\",\"31\":\"Date of Conference: 21-25 May 2018\",\"32\":\"Date of Conference: 21-25 May 2018\",\"33\":\"Date of Conference: 21-25 May 2018\",\"34\":\"Date of Conference: 21-25 May 2018\",\"35\":\"Date of Conference: 21-25 May 2018\",\"36\":\"Date of Conference: 21-25 May 2018\",\"37\":\"Date of Conference: 21-25 May 2018\",\"38\":\"Date of Conference: 21-25 May 2018\",\"39\":\"Date of Conference: 21-25 May 2018\",\"40\":\"Date of Conference: 21-25 May 2018\",\"41\":\"Date of Conference: 21-25 May 2018\",\"42\":\"Date of Conference: 21-25 May 2018\",\"43\":\"Date of Conference: 21-25 May 2018\",\"44\":\"Date of Conference: 21-25 May 2018\",\"45\":\"Date of Conference: 21-25 May 2018\",\"46\":\"Date of Conference: 21-25 May 2018\",\"47\":\"Date of Conference: 21-25 May 2018\",\"48\":\"Date of Conference: 21-25 May 2018\",\"49\":\"Date of Conference: 21-25 May 2018\",\"50\":\"Date of Conference: 21-25 May 2018\",\"51\":\"Date of Conference: 21-25 May 2018\",\"52\":\"Date of Conference: 21-25 May 2018\",\"53\":\"Date of Conference: 21-25 May 2018\",\"54\":\"Date of Conference: 21-25 May 2018\",\"55\":\"Date of Conference: 21-25 May 2018\",\"56\":\"Date of Conference: 21-25 May 2018\",\"57\":\"Date of Conference: 21-25 May 2018\",\"58\":\"Date of Conference: 21-25 May 2018\",\"59\":\"Date of Conference: 21-25 May 2018\",\"60\":\"Date of Conference: 21-25 May 2018\",\"61\":\"Date of Conference: 21-25 May 2018\",\"62\":\"Date of Conference: 21-25 May 2018\",\"63\":\"Date of Conference: 21-25 May 2018\",\"64\":\"Date of Conference: 21-25 May 2018\",\"65\":\"Date of Conference: 21-25 May 2018\",\"66\":\"Date of Conference: 21-25 May 2018\",\"67\":\"Date of Conference: 21-25 May 2018\",\"68\":\"Date of Conference: 21-25 May 2018\",\"69\":\"Date of Conference: 21-25 May 2018\",\"70\":\"Date of Conference: 21-25 May 2018\",\"71\":\"Date of Conference: 21-25 May 2018\",\"72\":\"Date of Conference: 21-25 May 2018\",\"73\":\"Date of Conference: 21-25 May 2018\",\"74\":\"Date of Conference: 21-25 May 2018\",\"75\":\"Date of Conference: 21-25 May 2018\",\"76\":\"Date of Conference: 21-25 May 2018\",\"77\":\"Date of Conference: 21-25 May 2018\",\"78\":\"Date of Conference: 21-25 May 2018\",\"79\":\"Date of Conference: 21-25 May 2018\",\"80\":\"Date of Conference: 21-25 May 2018\",\"81\":\"Date of Conference: 21-25 May 2018\",\"82\":\"Date of Conference: 21-25 May 2018\",\"83\":\"Date of Conference: 21-25 May 2018\",\"84\":\"Date of Conference: 21-25 May 2018\",\"85\":\"Date of Conference: 21-25 May 2018\",\"86\":\"Date of Conference: 21-25 May 2018\",\"87\":\"Date of Conference: 21-25 May 2018\",\"88\":\"Date of Conference: 21-25 May 2018\",\"89\":\"Date of Conference: 21-25 May 2018\",\"90\":\"Date of Conference: 21-25 May 2018\",\"91\":\"Date of Conference: 21-25 May 2018\",\"92\":\"Date of Conference: 21-25 May 2018\",\"93\":\"Date of Conference: 21-25 May 2018\",\"94\":\"Date of Conference: 21-25 May 2018\",\"95\":\"Date of Conference: 21-25 May 2018\",\"96\":\"Date of Conference: 21-25 May 2018\",\"97\":\"Date of Conference: 21-25 May 2018\",\"98\":\"Date of Conference: 21-25 May 2018\",\"99\":\"Date of Conference: 21-25 May 2018\",\"100\":\"Date of Conference: 21-25 May 2018\",\"101\":\"Date of Conference: 21-25 May 2018\",\"102\":\"Date of Conference: 21-25 May 2018\",\"103\":\"Date of Conference: 21-25 May 2018\",\"104\":\"Date of Conference: 21-25 May 2018\",\"105\":\"Date of Conference: 21-25 May 2018\",\"106\":\"Date of Conference: 21-25 May 2018\",\"107\":\"Date of Conference: 21-25 May 2018\",\"108\":\"Date of Conference: 21-25 May 2018\",\"109\":\"Date of Conference: 21-25 May 2018\",\"110\":\"Date of Conference: 21-25 May 2018\",\"111\":\"Date of Conference: 21-25 May 2018\",\"112\":\"Date of Conference: 21-25 May 2018\",\"113\":\"Date of Conference: 21-25 May 2018\",\"114\":\"Date of Conference: 21-25 May 2018\",\"115\":\"Date of Conference: 21-25 May 2018\",\"116\":\"Date of Conference: 21-25 May 2018\",\"117\":\"Date of Conference: 21-25 May 2018\",\"118\":\"Date of Conference: 21-25 May 2018\",\"119\":\"Date of Conference: 21-25 May 2018\",\"120\":\"Date of Conference: 21-25 May 2018\",\"121\":\"Date of Conference: 21-25 May 2018\",\"122\":\"Date of Conference: 21-25 May 2018\",\"123\":\"Date of Conference: 21-25 May 2018\",\"124\":\"Date of Conference: 21-25 May 2018\",\"125\":\"Date of Conference: 21-25 May 2018\",\"126\":\"Date of Conference: 21-25 May 2018\",\"127\":\"Date of Conference: 21-25 May 2018\",\"128\":\"Date of Conference: 21-25 May 2018\",\"129\":\"Date of Conference: 21-25 May 2018\",\"130\":\"Date of Conference: 21-25 May 2018\",\"131\":\"Date of Conference: 21-25 May 2018\",\"132\":\"Date of Conference: 21-25 May 2018\",\"133\":\"Date of Conference: 21-25 May 2018\",\"134\":\"Date of Conference: 21-25 May 2018\",\"135\":\"Date of Conference: 21-25 May 2018\",\"136\":\"Date of Conference: 21-25 May 2018\",\"137\":\"Date of Conference: 21-25 May 2018\",\"138\":\"Date of Conference: 21-25 May 2018\",\"139\":\"Date of Conference: 21-25 May 2018\",\"140\":\"Date of Conference: 21-25 May 2018\",\"141\":\"Date of Conference: 21-25 May 2018\",\"142\":\"Date of Conference: 21-25 May 2018\",\"143\":\"Date of Conference: 21-25 May 2018\",\"144\":\"Date of Conference: 21-25 May 2018\",\"145\":\"Date of Conference: 21-25 May 2018\",\"146\":\"Date of Conference: 21-25 May 2018\",\"147\":\"Date of Conference: 21-25 May 2018\",\"148\":\"Date of Conference: 21-25 May 2018\",\"149\":\"Date of Conference: 21-25 May 2018\",\"150\":\"Date of Conference: 21-25 May 2018\",\"151\":\"Date of Conference: 21-25 May 2018\",\"152\":\"Date of Conference: 21-25 May 2018\",\"153\":\"Date of Conference: 21-25 May 2018\",\"154\":\"Date of Conference: 21-25 May 2018\",\"155\":\"Date of Conference: 21-25 May 2018\",\"156\":\"Date of Conference: 21-25 May 2018\",\"157\":\"Date of Conference: 21-25 May 2018\",\"158\":\"Date of Conference: 21-25 May 2018\",\"159\":\"Date of Conference: 21-25 May 2018\",\"160\":\"Date of Conference: 21-25 May 2018\",\"161\":\"Date of Conference: 21-25 May 2018\",\"162\":\"Date of Conference: 21-25 May 2018\",\"163\":\"Date of Conference: 21-25 May 2018\",\"164\":\"Date of Conference: 21-25 May 2018\",\"165\":\"Date of Conference: 21-25 May 2018\",\"166\":\"Date of Conference: 21-25 May 2018\",\"167\":\"Date of Conference: 21-25 May 2018\",\"168\":\"Date of Conference: 21-25 May 2018\",\"169\":\"Date of Conference: 21-25 May 2018\",\"170\":\"Date of Conference: 21-25 May 2018\",\"171\":\"Date of Conference: 21-25 May 2018\",\"172\":\"Date of Conference: 21-25 May 2018\",\"173\":\"Date of Conference: 21-25 May 2018\",\"174\":\"Date of Conference: 21-25 May 2018\",\"175\":\"Date of Conference: 21-25 May 2018\",\"176\":\"Date of Conference: 21-25 May 2018\",\"177\":\"Date of Conference: 21-25 May 2018\",\"178\":\"Date of Conference: 21-25 May 2018\",\"179\":\"Date of Conference: 21-25 May 2018\",\"180\":\"Date of Conference: 21-25 May 2018\",\"181\":\"Date of Conference: 21-25 May 2018\",\"182\":\"Date of Conference: 21-25 May 2018\",\"183\":\"Date of Conference: 21-25 May 2018\",\"184\":\"Date of Conference: 21-25 May 2018\",\"185\":\"Date of Conference: 21-25 May 2018\",\"186\":\"Date of Conference: 21-25 May 2018\",\"187\":\"Date of Conference: 21-25 May 2018\",\"188\":\"Date of Conference: 21-25 May 2018\",\"189\":\"Date of Conference: 21-25 May 2018\",\"190\":\"Date of Conference: 21-25 May 2018\",\"191\":\"Date of Conference: 21-25 May 2018\",\"192\":\"Date of Conference: 21-25 May 2018\",\"193\":\"Date of Conference: 21-25 May 2018\",\"194\":\"Date of Conference: 21-25 May 2018\",\"195\":\"Date of Conference: 21-25 May 2018\",\"196\":\"Date of Conference: 21-25 May 2018\",\"197\":\"Date of Conference: 21-25 May 2018\",\"198\":\"Date of Conference: 21-25 May 2018\",\"199\":\"Date of Conference: 21-25 May 2018\",\"200\":\"Date of Conference: 21-25 May 2018\",\"201\":\"Date of Conference: 21-25 May 2018\",\"202\":\"Date of Conference: 21-25 May 2018\",\"203\":\"Date of Conference: 21-25 May 2018\",\"204\":\"Date of Conference: 21-25 May 2018\",\"205\":\"Date of Conference: 21-25 May 2018\",\"206\":\"Date of Conference: 21-25 May 2018\",\"207\":\"Date of Conference: 21-25 May 2018\",\"208\":\"Date of Conference: 21-25 May 2018\",\"209\":\"Date of Conference: 21-25 May 2018\",\"210\":\"Date of Conference: 21-25 May 2018\",\"211\":\"Date of Conference: 21-25 May 2018\",\"212\":\"Date of Conference: 21-25 May 2018\",\"213\":\"Date of Conference: 21-25 May 2018\",\"214\":\"Date of Conference: 21-25 May 2018\",\"215\":\"Date of Conference: 21-25 May 2018\",\"216\":\"Date of Conference: 21-25 May 2018\",\"217\":\"Date of Conference: 21-25 May 2018\",\"218\":\"Date of Conference: 21-25 May 2018\",\"219\":\"Date of Conference: 21-25 May 2018\",\"220\":\"Date of Conference: 21-25 May 2018\",\"221\":\"Date of Conference: 21-25 May 2018\",\"222\":\"Date of Conference: 21-25 May 2018\",\"223\":\"Date of Conference: 21-25 May 2018\",\"224\":\"Date of Conference: 21-25 May 2018\",\"225\":\"Date of Conference: 21-25 May 2018\",\"226\":\"Date of Conference: 21-25 May 2018\",\"227\":\"Date of Conference: 21-25 May 2018\",\"228\":\"Date of Conference: 21-25 May 2018\",\"229\":\"Date of Conference: 21-25 May 2018\",\"230\":\"Date of Conference: 21-25 May 2018\",\"231\":\"Date of Conference: 21-25 May 2018\",\"232\":\"Date of Conference: 21-25 May 2018\",\"233\":\"Date of Conference: 21-25 May 2018\",\"234\":\"Date of Conference: 21-25 May 2018\",\"235\":\"Date of Conference: 21-25 May 2018\",\"236\":\"Date of Conference: 21-25 May 2018\",\"237\":\"Date of Conference: 21-25 May 2018\",\"238\":\"Date of Conference: 21-25 May 2018\",\"239\":\"Date of Conference: 21-25 May 2018\",\"240\":\"Date of Conference: 21-25 May 2018\",\"241\":\"Date of Conference: 21-25 May 2018\",\"242\":\"Date of Conference: 21-25 May 2018\",\"243\":\"Date of Conference: 21-25 May 2018\",\"244\":\"Date of Conference: 21-25 May 2018\",\"245\":\"Date of Conference: 21-25 May 2018\",\"246\":\"Date of Conference: 21-25 May 2018\",\"247\":\"Date of Conference: 21-25 May 2018\",\"248\":\"Date of Conference: 21-25 May 2018\",\"249\":\"Date of Conference: 21-25 May 2018\",\"250\":\"Date of Conference: 21-25 May 2018\",\"251\":\"Date of Conference: 21-25 May 2018\",\"252\":\"Date of Conference: 21-25 May 2018\",\"253\":\"Date of Conference: 21-25 May 2018\",\"254\":\"Date of Conference: 21-25 May 2018\",\"255\":\"Date of Conference: 21-25 May 2018\",\"256\":\"Date of Conference: 21-25 May 2018\",\"257\":\"Date of Conference: 21-25 May 2018\",\"258\":\"Date of Conference: 21-25 May 2018\",\"259\":\"Date of Conference: 21-25 May 2018\",\"260\":\"Date of Conference: 21-25 May 2018\",\"261\":\"Date of Conference: 21-25 May 2018\",\"262\":\"Date of Conference: 21-25 May 2018\",\"263\":\"Date of Conference: 21-25 May 2018\",\"264\":\"Date of Conference: 21-25 May 2018\",\"265\":\"Date of Conference: 21-25 May 2018\",\"266\":\"Date of Conference: 21-25 May 2018\",\"267\":\"Date of Conference: 21-25 May 2018\",\"268\":\"Date of Conference: 21-25 May 2018\",\"269\":\"Date of Conference: 21-25 May 2018\",\"270\":\"Date of Conference: 21-25 May 2018\",\"271\":\"Date of Conference: 21-25 May 2018\",\"272\":\"Date of Conference: 21-25 May 2018\",\"273\":\"Date of Conference: 21-25 May 2018\",\"274\":\"Date of Conference: 21-25 May 2018\",\"275\":\"Date of Conference: 21-25 May 2018\",\"276\":\"Date of Conference: 21-25 May 2018\",\"277\":\"Date of Conference: 21-25 May 2018\",\"278\":\"Date of Conference: 21-25 May 2018\",\"279\":\"Date of Conference: 21-25 May 2018\",\"280\":\"Date of Conference: 21-25 May 2018\",\"281\":\"Date of Conference: 21-25 May 2018\",\"282\":\"Date of Conference: 21-25 May 2018\",\"283\":\"Date of Conference: 21-25 May 2018\",\"284\":\"Date of Conference: 21-25 May 2018\",\"285\":\"Date of Conference: 21-25 May 2018\",\"286\":\"Date of Conference: 21-25 May 2018\",\"287\":\"Date of Conference: 21-25 May 2018\",\"288\":\"Date of Conference: 21-25 May 2018\",\"289\":\"Date of Conference: 21-25 May 2018\",\"290\":\"Date of Conference: 21-25 May 2018\",\"291\":\"Date of Conference: 21-25 May 2018\",\"292\":\"Date of Conference: 21-25 May 2018\",\"293\":\"Date of Conference: 21-25 May 2018\",\"294\":\"Date of Conference: 21-25 May 2018\",\"295\":\"Date of Conference: 21-25 May 2018\",\"296\":\"Date of Conference: 21-25 May 2018\",\"297\":\"Date of Conference: 21-25 May 2018\",\"298\":\"Date of Conference: 21-25 May 2018\",\"299\":\"Date of Conference: 21-25 May 2018\",\"300\":\"Date of Conference: 21-25 May 2018\",\"301\":\"Date of Conference: 21-25 May 2018\",\"302\":\"Date of Conference: 21-25 May 2018\",\"303\":\"Date of Conference: 21-25 May 2018\",\"304\":\"Date of Conference: 21-25 May 2018\",\"305\":\"Date of Conference: 21-25 May 2018\",\"306\":\"Date of Conference: 21-25 May 2018\",\"307\":\"Date of Conference: 21-25 May 2018\",\"308\":\"Date of Conference: 21-25 May 2018\",\"309\":\"Date of Conference: 21-25 May 2018\",\"310\":\"Date of Conference: 21-25 May 2018\",\"311\":\"Date of Conference: 21-25 May 2018\",\"312\":\"Date of Conference: 21-25 May 2018\",\"313\":\"Date of Conference: 21-25 May 2018\",\"314\":\"Date of Conference: 21-25 May 2018\",\"315\":\"Date of Conference: 21-25 May 2018\",\"316\":\"Date of Conference: 21-25 May 2018\",\"317\":\"Date of Conference: 21-25 May 2018\",\"318\":\"Date of Conference: 21-25 May 2018\",\"319\":\"Date of Conference: 21-25 May 2018\",\"320\":\"Date of Conference: 21-25 May 2018\",\"321\":\"Date of Conference: 21-25 May 2018\",\"322\":\"Date of Conference: 21-25 May 2018\",\"323\":\"Date of Conference: 21-25 May 2018\",\"324\":\"Date of Conference: 21-25 May 2018\",\"325\":\"Date of Conference: 21-25 May 2018\",\"326\":\"Date of Conference: 21-25 May 2018\",\"327\":\"Date of Conference: 21-25 May 2018\",\"328\":\"Date of Conference: 21-25 May 2018\",\"329\":\"Date of Conference: 21-25 May 2018\",\"330\":\"Date of Conference: 21-25 May 2018\",\"331\":\"Date of Conference: 21-25 May 2018\",\"332\":\"Date of Conference: 21-25 May 2018\",\"333\":\"Date of Conference: 21-25 May 2018\",\"334\":\"Date of Conference: 21-25 May 2018\",\"335\":\"Date of Conference: 21-25 May 2018\",\"336\":\"Date of Conference: 21-25 May 2018\",\"337\":\"Date of Conference: 21-25 May 2018\",\"338\":\"Date of Conference: 21-25 May 2018\",\"339\":\"Date of Conference: 21-25 May 2018\",\"340\":\"Date of Conference: 21-25 May 2018\",\"341\":\"Date of Conference: 21-25 May 2018\",\"342\":\"Date of Conference: 21-25 May 2018\",\"343\":\"Date of Conference: 21-25 May 2018\",\"344\":\"Date of Conference: 21-25 May 2018\",\"345\":\"Date of Conference: 21-25 May 2018\",\"346\":\"Date of Conference: 21-25 May 2018\",\"347\":\"Date of Conference: 21-25 May 2018\",\"348\":\"Date of Conference: 21-25 May 2018\",\"349\":\"Date of Conference: 21-25 May 2018\",\"350\":\"Date of Conference: 21-25 May 2018\",\"351\":\"Date of Conference: 21-25 May 2018\",\"352\":\"Date of Conference: 21-25 May 2018\",\"353\":\"Date of Conference: 21-25 May 2018\",\"354\":\"Date of Conference: 21-25 May 2018\",\"355\":\"Date of Conference: 21-25 May 2018\",\"356\":\"Date of Conference: 21-25 May 2018\",\"357\":\"Date of Conference: 21-25 May 2018\",\"358\":\"Date of Conference: 21-25 May 2018\",\"359\":\"Date of Conference: 21-25 May 2018\",\"360\":\"Date of Conference: 21-25 May 2018\",\"361\":\"Date of Conference: 21-25 May 2018\",\"362\":\"Date of Conference: 21-25 May 2018\",\"363\":\"Date of Conference: 21-25 May 2018\",\"364\":\"Date of Conference: 21-25 May 2018\",\"365\":\"Date of Conference: 21-25 May 2018\",\"366\":\"Date of Conference: 21-25 May 2018\",\"367\":\"Date of Conference: 21-25 May 2018\",\"368\":\"Date of Conference: 21-25 May 2018\",\"369\":\"Date of Conference: 21-25 May 2018\",\"370\":\"Date of Conference: 21-25 May 2018\",\"371\":\"Date of Conference: 21-25 May 2018\",\"372\":\"Date of Conference: 21-25 May 2018\",\"373\":\"Date of Conference: 21-25 May 2018\",\"374\":\"Date of Conference: 21-25 May 2018\",\"375\":\"Date of Conference: 21-25 May 2018\",\"376\":\"Date of Conference: 21-25 May 2018\",\"377\":\"Date of Conference: 21-25 May 2018\",\"378\":\"Date of Conference: 21-25 May 2018\",\"379\":\"Date of Conference: 21-25 May 2018\",\"380\":\"Date of Conference: 21-25 May 2018\",\"381\":\"Date of Conference: 21-25 May 2018\",\"382\":\"Date of Conference: 21-25 May 2018\",\"383\":\"Date of Conference: 21-25 May 2018\",\"384\":\"Date of Conference: 21-25 May 2018\",\"385\":\"Date of Conference: 21-25 May 2018\",\"386\":\"Date of Conference: 21-25 May 2018\",\"387\":\"Date of Conference: 21-25 May 2018\",\"388\":\"Date of Conference: 21-25 May 2018\",\"389\":\"Date of Conference: 21-25 May 2018\",\"390\":\"Date of Conference: 21-25 May 2018\",\"391\":\"Date of Conference: 21-25 May 2018\",\"392\":\"Date of Conference: 21-25 May 2018\",\"393\":\"Date of Conference: 21-25 May 2018\",\"394\":\"Date of Conference: 21-25 May 2018\",\"395\":\"Date of Conference: 21-25 May 2018\",\"396\":\"Date of Conference: 21-25 May 2018\",\"397\":\"Date of Conference: 21-25 May 2018\",\"398\":\"Date of Conference: 21-25 May 2018\",\"399\":\"Date of Conference: 21-25 May 2018\",\"400\":\"Date of Conference: 21-25 May 2018\",\"401\":\"Date of Conference: 21-25 May 2018\",\"402\":\"Date of Conference: 21-25 May 2018\",\"403\":\"Date of Conference: 21-25 May 2018\",\"404\":\"Date of Conference: 21-25 May 2018\",\"405\":\"Date of Conference: 21-25 May 2018\",\"406\":\"Date of Conference: 21-25 May 2018\",\"407\":\"Date of Conference: 21-25 May 2018\",\"408\":\"Date of Conference: 21-25 May 2018\",\"409\":\"Date of Conference: 21-25 May 2018\",\"410\":\"Date of Conference: 21-25 May 2018\",\"411\":\"Date of Conference: 21-25 May 2018\",\"412\":\"Date of Conference: 21-25 May 2018\",\"413\":\"Date of Conference: 21-25 May 2018\",\"414\":\"Date of Conference: 21-25 May 2018\",\"415\":\"Date of Conference: 21-25 May 2018\",\"416\":\"Date of Conference: 21-25 May 2018\",\"417\":\"Date of Conference: 21-25 May 2018\",\"418\":\"Date of Conference: 21-25 May 2018\",\"419\":\"Date of Conference: 21-25 May 2018\",\"420\":\"Date of Conference: 21-25 May 2018\",\"421\":\"Date of Conference: 21-25 May 2018\",\"422\":\"Date of Conference: 21-25 May 2018\",\"423\":\"Date of Conference: 21-25 May 2018\",\"424\":\"Date of Conference: 21-25 May 2018\",\"425\":\"Date of Conference: 21-25 May 2018\",\"426\":\"Date of Conference: 21-25 May 2018\",\"427\":\"Date of Conference: 21-25 May 2018\",\"428\":\"Date of Conference: 21-25 May 2018\",\"429\":\"Date of Conference: 21-25 May 2018\",\"430\":\"Date of Conference: 21-25 May 2018\",\"431\":\"Date of Conference: 21-25 May 2018\",\"432\":\"Date of Conference: 21-25 May 2018\",\"433\":\"Date of Conference: 21-25 May 2018\",\"434\":\"Date of Conference: 21-25 May 2018\",\"435\":\"Date of Conference: 21-25 May 2018\",\"436\":\"Date of Conference: 21-25 May 2018\",\"437\":\"Date of Conference: 21-25 May 2018\",\"438\":\"Date of Conference: 21-25 May 2018\",\"439\":\"Date of Conference: 21-25 May 2018\",\"440\":\"Date of Conference: 21-25 May 2018\",\"441\":\"Date of Conference: 21-25 May 2018\",\"442\":\"Date of Conference: 21-25 May 2018\",\"443\":\"Date of Conference: 21-25 May 2018\",\"444\":\"Date of Conference: 21-25 May 2018\",\"445\":\"Date of Conference: 21-25 May 2018\",\"446\":\"Date of Conference: 21-25 May 2018\",\"447\":\"Date of Conference: 21-25 May 2018\",\"448\":\"Date of Conference: 21-25 May 2018\",\"449\":\"Date of Conference: 21-25 May 2018\",\"450\":\"Date of Conference: 21-25 May 2018\",\"451\":\"Date of Conference: 21-25 May 2018\",\"452\":\"Date of Conference: 21-25 May 2018\",\"453\":\"Date of Conference: 21-25 May 2018\",\"454\":\"Date of Conference: 21-25 May 2018\",\"455\":\"Date of Conference: 21-25 May 2018\",\"456\":\"Date of Conference: 21-25 May 2018\",\"457\":\"Date of Conference: 21-25 May 2018\",\"458\":\"Date of Conference: 21-25 May 2018\",\"459\":\"Date of Conference: 21-25 May 2018\",\"460\":\"Date of Conference: 21-25 May 2018\",\"461\":\"Date of Conference: 21-25 May 2018\",\"462\":\"Date of Conference: 21-25 May 2018\",\"463\":\"Date of Conference: 21-25 May 2018\",\"464\":\"Date of Conference: 21-25 May 2018\",\"465\":\"Date of Conference: 21-25 May 2018\",\"466\":\"Date of Conference: 21-25 May 2018\",\"467\":\"Date of Conference: 21-25 May 2018\",\"468\":\"Date of Conference: 21-25 May 2018\",\"469\":\"Date of Conference: 21-25 May 2018\",\"470\":\"Date of Conference: 21-25 May 2018\",\"471\":\"Date of Conference: 21-25 May 2018\",\"472\":\"Date of Conference: 21-25 May 2018\",\"473\":\"Date of Conference: 21-25 May 2018\",\"474\":\"Date of Conference: 21-25 May 2018\",\"475\":\"Date of Conference: 21-25 May 2018\",\"476\":\"Date of Conference: 21-25 May 2018\",\"477\":\"Date of Conference: 21-25 May 2018\",\"478\":\"Date of Conference: 21-25 May 2018\",\"479\":\"Date of Conference: 21-25 May 2018\",\"480\":\"Date of Conference: 21-25 May 2018\",\"481\":\"Date of Conference: 21-25 May 2018\",\"482\":\"Date of Conference: 21-25 May 2018\",\"483\":\"Date of Conference: 21-25 May 2018\",\"484\":\"Date of Conference: 21-25 May 2018\",\"485\":\"Date of Conference: 21-25 May 2018\",\"486\":\"Date of Conference: 21-25 May 2018\",\"487\":\"Date of Conference: 21-25 May 2018\",\"488\":\"Date of Conference: 21-25 May 2018\",\"489\":\"Date of Conference: 21-25 May 2018\",\"490\":\"Date of Conference: 21-25 May 2018\",\"491\":\"Date of Conference: 21-25 May 2018\",\"492\":\"Date of Conference: 21-25 May 2018\",\"493\":\"Date of Conference: 21-25 May 2018\",\"494\":\"Date of Conference: 21-25 May 2018\",\"495\":\"Date of Conference: 21-25 May 2018\",\"496\":\"Date of Conference: 21-25 May 2018\",\"497\":\"Date of Conference: 21-25 May 2018\",\"498\":\"Date of Conference: 21-25 May 2018\",\"499\":\"Date of Conference: 21-25 May 2018\",\"500\":\"Date of Conference: 21-25 May 2018\",\"501\":\"Date of Conference: 21-25 May 2018\",\"502\":\"Date of Conference: 21-25 May 2018\",\"503\":\"Date of Conference: 21-25 May 2018\",\"504\":\"Date of Conference: 21-25 May 2018\",\"505\":\"Date of Conference: 21-25 May 2018\",\"506\":\"Date of Conference: 21-25 May 2018\",\"507\":\"Date of Conference: 21-25 May 2018\",\"508\":\"Date of Conference: 21-25 May 2018\",\"509\":\"Date of Conference: 21-25 May 2018\",\"510\":\"Date of Conference: 21-25 May 2018\",\"511\":\"Date of Conference: 21-25 May 2018\",\"512\":\"Date of Conference: 21-25 May 2018\",\"513\":\"Date of Conference: 21-25 May 2018\",\"514\":\"Date of Conference: 21-25 May 2018\",\"515\":\"Date of Conference: 21-25 May 2018\",\"516\":\"Date of Conference: 21-25 May 2018\",\"517\":\"Date of Conference: 21-25 May 2018\",\"518\":\"Date of Conference: 21-25 May 2018\",\"519\":\"Date of Conference: 21-25 May 2018\",\"520\":\"Date of Conference: 21-25 May 2018\",\"521\":\"Date of Conference: 21-25 May 2018\",\"522\":\"Date of Conference: 21-25 May 2018\",\"523\":\"Date of Conference: 21-25 May 2018\",\"524\":\"Date of Conference: 21-25 May 2018\",\"525\":\"Date of Conference: 21-25 May 2018\",\"526\":\"Date of Conference: 21-25 May 2018\",\"527\":\"Date of Conference: 21-25 May 2018\",\"528\":\"Date of Conference: 21-25 May 2018\",\"529\":\"Date of Conference: 21-25 May 2018\",\"530\":\"Date of Conference: 21-25 May 2018\",\"531\":\"Date of Conference: 21-25 May 2018\",\"532\":\"Date of Conference: 21-25 May 2018\",\"533\":\"Date of Conference: 21-25 May 2018\",\"534\":\"Date of Conference: 21-25 May 2018\",\"535\":\"Date of Conference: 21-25 May 2018\",\"536\":\"Date of Conference: 21-25 May 2018\",\"537\":\"Date of Conference: 21-25 May 2018\",\"538\":\"Date of Conference: 21-25 May 2018\",\"539\":\"Date of Conference: 21-25 May 2018\",\"540\":\"Date of Conference: 21-25 May 2018\",\"541\":\"Date of Conference: 21-25 May 2018\",\"542\":\"Date of Conference: 21-25 May 2018\",\"543\":\"Date of Conference: 21-25 May 2018\",\"544\":\"Date of Conference: 21-25 May 2018\",\"545\":\"Date of Conference: 21-25 May 2018\",\"546\":\"Date of Conference: 21-25 May 2018\",\"547\":\"Date of Conference: 21-25 May 2018\",\"548\":\"Date of Conference: 21-25 May 2018\",\"549\":\"Date of Conference: 21-25 May 2018\",\"550\":\"Date of Conference: 21-25 May 2018\",\"551\":\"Date of Conference: 21-25 May 2018\",\"552\":\"Date of Conference: 21-25 May 2018\",\"553\":\"Date of Conference: 21-25 May 2018\",\"554\":\"Date of Conference: 21-25 May 2018\",\"555\":\"Date of Conference: 21-25 May 2018\",\"556\":\"Date of Conference: 21-25 May 2018\",\"557\":\"Date of Conference: 21-25 May 2018\",\"558\":\"Date of Conference: 21-25 May 2018\",\"559\":\"Date of Conference: 21-25 May 2018\",\"560\":\"Date of Conference: 21-25 May 2018\",\"561\":\"Date of Conference: 21-25 May 2018\",\"562\":\"Date of Conference: 21-25 May 2018\",\"563\":\"Date of Conference: 21-25 May 2018\",\"564\":\"Date of Conference: 21-25 May 2018\",\"565\":\"Date of Conference: 21-25 May 2018\",\"566\":\"Date of Conference: 21-25 May 2018\",\"567\":\"Date of Conference: 21-25 May 2018\",\"568\":\"Date of Conference: 21-25 May 2018\",\"569\":\"Date of Conference: 21-25 May 2018\",\"570\":\"Date of Conference: 21-25 May 2018\",\"571\":\"Date of Conference: 21-25 May 2018\",\"572\":\"Date of Conference: 21-25 May 2018\",\"573\":\"Date of Conference: 21-25 May 2018\",\"574\":\"Date of Conference: 21-25 May 2018\",\"575\":\"Date of Conference: 21-25 May 2018\",\"576\":\"Date of Conference: 21-25 May 2018\",\"577\":\"Date of Conference: 21-25 May 2018\",\"578\":\"Date of Conference: 21-25 May 2018\",\"579\":\"Date of Conference: 21-25 May 2018\",\"580\":\"Date of Conference: 21-25 May 2018\",\"581\":\"Date of Conference: 21-25 May 2018\",\"582\":\"Date of Conference: 21-25 May 2018\",\"583\":\"Date of Conference: 21-25 May 2018\",\"584\":\"Date of Conference: 21-25 May 2018\",\"585\":\"Date of Conference: 21-25 May 2018\",\"586\":\"Date of Conference: 21-25 May 2018\",\"587\":\"Date of Conference: 21-25 May 2018\",\"588\":\"Date of Conference: 21-25 May 2018\",\"589\":\"Date of Conference: 21-25 May 2018\",\"590\":\"Date of Conference: 21-25 May 2018\",\"591\":\"Date of Conference: 21-25 May 2018\",\"592\":\"Date of Conference: 21-25 May 2018\",\"593\":\"Date of Conference: 21-25 May 2018\",\"594\":\"Date of Conference: 21-25 May 2018\",\"595\":\"Date of Conference: 21-25 May 2018\",\"596\":\"Date of Conference: 21-25 May 2018\",\"597\":\"Date of Conference: 21-25 May 2018\",\"598\":\"Date of Conference: 21-25 May 2018\",\"599\":\"Date of Conference: 21-25 May 2018\",\"600\":\"Date of Conference: 21-25 May 2018\",\"601\":\"Date of Conference: 21-25 May 2018\",\"602\":\"Date of Conference: 21-25 May 2018\",\"603\":\"Date of Conference: 21-25 May 2018\",\"604\":\"Date of Conference: 21-25 May 2018\",\"605\":\"Date of Conference: 21-25 May 2018\",\"606\":\"Date of Conference: 21-25 May 2018\",\"607\":\"Date of Conference: 21-25 May 2018\",\"608\":\"Date of Conference: 21-25 May 2018\",\"609\":\"Date of Conference: 21-25 May 2018\",\"610\":\"Date of Conference: 21-25 May 2018\",\"611\":\"Date of Conference: 21-25 May 2018\",\"612\":\"Date of Conference: 21-25 May 2018\",\"613\":\"Date of Conference: 21-25 May 2018\",\"614\":\"Date of Conference: 21-25 May 2018\",\"615\":\"Date of Conference: 21-25 May 2018\",\"616\":\"Date of Conference: 21-25 May 2018\",\"617\":\"Date of Conference: 21-25 May 2018\",\"618\":\"Date of Conference: 21-25 May 2018\",\"619\":\"Date of Conference: 21-25 May 2018\",\"620\":\"Date of Conference: 21-25 May 2018\",\"621\":\"Date of Conference: 21-25 May 2018\",\"622\":\"Date of Conference: 21-25 May 2018\",\"623\":\"Date of Conference: 21-25 May 2018\",\"624\":\"Date of Conference: 21-25 May 2018\",\"625\":\"Date of Conference: 21-25 May 2018\",\"626\":\"Date of Conference: 21-25 May 2018\",\"627\":\"Date of Conference: 21-25 May 2018\",\"628\":\"Date of Conference: 21-25 May 2018\",\"629\":\"Date of Conference: 21-25 May 2018\",\"630\":\"Date of Conference: 21-25 May 2018\",\"631\":\"Date of Conference: 21-25 May 2018\",\"632\":\"Date of Conference: 21-25 May 2018\",\"633\":\"Date of Conference: 21-25 May 2018\",\"634\":\"Date of Conference: 21-25 May 2018\",\"635\":\"Date of Conference: 21-25 May 2018\",\"636\":\"Date of Conference: 21-25 May 2018\",\"637\":\"Date of Conference: 21-25 May 2018\",\"638\":\"Date of Conference: 21-25 May 2018\",\"639\":\"Date of Conference: 21-25 May 2018\",\"640\":\"Date of Conference: 21-25 May 2018\",\"641\":\"Date of Conference: 21-25 May 2018\",\"642\":\"Date of Conference: 21-25 May 2018\",\"643\":\"Date of Conference: 21-25 May 2018\",\"644\":\"Date of Conference: 21-25 May 2018\",\"645\":\"Date of Conference: 21-25 May 2018\",\"646\":\"Date of Conference: 21-25 May 2018\",\"647\":\"Date of Conference: 21-25 May 2018\",\"648\":\"Date of Conference: 21-25 May 2018\",\"649\":\"Date of Conference: 21-25 May 2018\",\"650\":\"Date of Conference: 21-25 May 2018\",\"651\":\"Date of Conference: 21-25 May 2018\",\"652\":\"Date of Conference: 21-25 May 2018\",\"653\":\"Date of Conference: 21-25 May 2018\",\"654\":\"Date of Conference: 21-25 May 2018\",\"655\":\"Date of Conference: 21-25 May 2018\",\"656\":\"Date of Conference: 21-25 May 2018\",\"657\":\"Date of Conference: 21-25 May 2018\",\"658\":\"Date of Conference: 21-25 May 2018\",\"659\":\"Date of Conference: 21-25 May 2018\",\"660\":\"Date of Conference: 21-25 May 2018\",\"661\":\"Date of Conference: 21-25 May 2018\",\"662\":\"Date of Conference: 21-25 May 2018\",\"663\":\"Date of Conference: 21-25 May 2018\",\"664\":\"Date of Conference: 21-25 May 2018\",\"665\":\"Date of Conference: 21-25 May 2018\",\"666\":\"Date of Conference: 21-25 May 2018\",\"667\":\"Date of Conference: 21-25 May 2018\",\"668\":\"Date of Conference: 21-25 May 2018\",\"669\":\"Date of Conference: 21-25 May 2018\",\"670\":\"Date of Conference: 21-25 May 2018\",\"671\":\"Date of Conference: 21-25 May 2018\",\"672\":\"Date of Conference: 21-25 May 2018\",\"673\":\"Date of Conference: 21-25 May 2018\",\"674\":\"Date of Conference: 21-25 May 2018\",\"675\":\"Date of Conference: 21-25 May 2018\",\"676\":\"Date of Conference: 21-25 May 2018\",\"677\":\"Date of Conference: 21-25 May 2018\",\"678\":\"Date of Conference: 21-25 May 2018\",\"679\":\"Date of Conference: 21-25 May 2018\",\"680\":\"Date of Conference: 21-25 May 2018\",\"681\":\"Date of Conference: 21-25 May 2018\",\"682\":\"Date of Conference: 21-25 May 2018\",\"683\":\"Date of Conference: 21-25 May 2018\",\"684\":\"Date of Conference: 21-25 May 2018\",\"685\":\"Date of Conference: 21-25 May 2018\",\"686\":\"Date of Conference: 21-25 May 2018\",\"687\":\"Date of Conference: 21-25 May 2018\",\"688\":\"Date of Conference: 21-25 May 2018\",\"689\":\"Date of Conference: 21-25 May 2018\",\"690\":\"Date of Conference: 21-25 May 2018\",\"691\":\"Date of Conference: 21-25 May 2018\",\"692\":\"Date of Conference: 21-25 May 2018\",\"693\":\"Date of Conference: 21-25 May 2018\",\"694\":\"Date of Conference: 21-25 May 2018\",\"695\":\"Date of Conference: 21-25 May 2018\",\"696\":\"Date of Conference: 21-25 May 2018\",\"697\":\"Date of Conference: 21-25 May 2018\",\"698\":\"Date of Conference: 21-25 May 2018\",\"699\":\"Date of Conference: 21-25 May 2018\",\"700\":\"Date of Conference: 21-25 May 2018\",\"701\":\"Date of Conference: 21-25 May 2018\",\"702\":\"Date of Conference: 21-25 May 2018\",\"703\":\"Date of Conference: 21-25 May 2018\",\"704\":\"Date of Conference: 21-25 May 2018\",\"705\":\"Date of Conference: 21-25 May 2018\",\"706\":\"Date of Conference: 21-25 May 2018\",\"707\":\"Date of Conference: 21-25 May 2018\",\"708\":\"Date of Conference: 21-25 May 2018\",\"709\":\"Date of Conference: 21-25 May 2018\",\"710\":\"Date of Conference: 21-25 May 2018\",\"711\":\"Date of Conference: 21-25 May 2018\",\"712\":\"Date of Conference: 21-25 May 2018\",\"713\":\"Date of Conference: 21-25 May 2018\",\"714\":\"Date of Conference: 21-25 May 2018\",\"715\":\"Date of Conference: 21-25 May 2018\",\"716\":\"Date of Conference: 21-25 May 2018\",\"717\":\"Date of Conference: 21-25 May 2018\",\"718\":\"Date of Conference: 21-25 May 2018\",\"719\":\"Date of Conference: 21-25 May 2018\",\"720\":\"Date of Conference: 21-25 May 2018\",\"721\":\"Date of Conference: 21-25 May 2018\",\"722\":\"Date of Conference: 21-25 May 2018\",\"723\":\"Date of Conference: 21-25 May 2018\",\"724\":\"Date of Conference: 21-25 May 2018\",\"725\":\"Date of Conference: 21-25 May 2018\",\"726\":\"Date of Conference: 21-25 May 2018\",\"727\":\"Date of Conference: 21-25 May 2018\",\"728\":\"Date of Conference: 21-25 May 2018\",\"729\":\"Date of Conference: 21-25 May 2018\",\"730\":\"Date of Conference: 21-25 May 2018\",\"731\":\"Date of Conference: 21-25 May 2018\",\"732\":\"Date of Conference: 21-25 May 2018\",\"733\":\"Date of Conference: 21-25 May 2018\",\"734\":\"Date of Conference: 21-25 May 2018\",\"735\":\"Date of Conference: 21-25 May 2018\",\"736\":\"Date of Conference: 21-25 May 2018\",\"737\":\"Date of Conference: 21-25 May 2018\",\"738\":\"Date of Conference: 21-25 May 2018\",\"739\":\"Date of Conference: 21-25 May 2018\",\"740\":\"Date of Conference: 21-25 May 2018\",\"741\":\"Date of Conference: 21-25 May 2018\",\"742\":\"Date of Conference: 21-25 May 2018\",\"743\":\"Date of Conference: 21-25 May 2018\",\"744\":\"Date of Conference: 21-25 May 2018\",\"745\":\"Date of Conference: 21-25 May 2018\",\"746\":\"Date of Conference: 21-25 May 2018\",\"747\":\"Date of Conference: 21-25 May 2018\",\"748\":\"Date of Conference: 21-25 May 2018\",\"749\":\"Date of Conference: 21-25 May 2018\",\"750\":\"Date of Conference: 21-25 May 2018\",\"751\":\"Date of Conference: 21-25 May 2018\",\"752\":\"Date of Conference: 21-25 May 2018\",\"753\":\"Date of Conference: 21-25 May 2018\",\"754\":\"Date of Conference: 21-25 May 2018\",\"755\":\"Date of Conference: 21-25 May 2018\",\"756\":\"Date of Conference: 21-25 May 2018\",\"757\":\"Date of Conference: 21-25 May 2018\",\"758\":\"Date of Conference: 21-25 May 2018\",\"759\":\"Date of Conference: 21-25 May 2018\",\"760\":\"Date of Conference: 21-25 May 2018\",\"761\":\"Date of Conference: 21-25 May 2018\",\"762\":\"Date of Conference: 21-25 May 2018\",\"763\":\"Date of Conference: 21-25 May 2018\",\"764\":\"Date of Conference: 21-25 May 2018\",\"765\":\"Date of Conference: 21-25 May 2018\",\"766\":\"Date of Conference: 21-25 May 2018\",\"767\":\"Date of Conference: 21-25 May 2018\",\"768\":\"Date of Conference: 21-25 May 2018\",\"769\":\"Date of Conference: 21-25 May 2018\",\"770\":\"Date of Conference: 21-25 May 2018\",\"771\":\"Date of Conference: 21-25 May 2018\",\"772\":\"Date of Conference: 21-25 May 2018\",\"773\":\"Date of Conference: 21-25 May 2018\",\"774\":\"Date of Conference: 21-25 May 2018\",\"775\":\"Date of Conference: 21-25 May 2018\",\"776\":\"Date of Conference: 21-25 May 2018\",\"777\":\"Date of Conference: 21-25 May 2018\",\"778\":\"Date of Conference: 21-25 May 2018\",\"779\":\"Date of Conference: 21-25 May 2018\",\"780\":\"Date of Conference: 21-25 May 2018\",\"781\":\"Date of Conference: 21-25 May 2018\",\"782\":\"Date of Conference: 21-25 May 2018\",\"783\":\"Date of Conference: 21-25 May 2018\",\"784\":\"Date of Conference: 21-25 May 2018\",\"785\":\"Date of Conference: 21-25 May 2018\",\"786\":\"Date of Conference: 21-25 May 2018\",\"787\":\"Date of Conference: 21-25 May 2018\",\"788\":\"Date of Conference: 21-25 May 2018\",\"789\":\"Date of Conference: 21-25 May 2018\",\"790\":\"Date of Conference: 21-25 May 2018\",\"791\":\"Date of Conference: 21-25 May 2018\",\"792\":\"Date of Conference: 21-25 May 2018\",\"793\":\"Date of Conference: 21-25 May 2018\",\"794\":\"Date of Conference: 21-25 May 2018\",\"795\":\"Date of Conference: 21-25 May 2018\",\"796\":\"Date of Conference: 21-25 May 2018\",\"797\":\"Date of Conference: 21-25 May 2018\",\"798\":\"Date of Conference: 21-25 May 2018\",\"799\":\"Date of Conference: 21-25 May 2018\",\"800\":\"Date of Conference: 21-25 May 2018\",\"801\":\"Date of Conference: 21-25 May 2018\",\"802\":\"Date of Conference: 21-25 May 2018\",\"803\":\"Date of Conference: 21-25 May 2018\",\"804\":\"Date of Conference: 21-25 May 2018\",\"805\":\"Date of Conference: 21-25 May 2018\",\"806\":\"Date of Conference: 21-25 May 2018\",\"807\":\"Date of Conference: 21-25 May 2018\",\"808\":\"Date of Conference: 21-25 May 2018\",\"809\":\"Date of Conference: 21-25 May 2018\",\"810\":\"Date of Conference: 21-25 May 2018\",\"811\":\"Date of Conference: 21-25 May 2018\",\"812\":\"Date of Conference: 21-25 May 2018\",\"813\":\"Date of Conference: 21-25 May 2018\"},\"Paper Title\":{\"0\":\"Automatic Optimized 3D Path Planner for Steerable Catheters with Heuristic Search and Uncertainty Tolerance\",\"1\":\"Design and kinematics characterization of a laser-profiled continuum manipulator for the guidance of bronchoscopic instruments\",\"2\":\"Design, Modeling and Control of a 2-DoF Robotic Guidewire\",\"3\":\"A Self-propelled Catheter Capable of Generating Travelling Waves with Steering Function by Mono-Line Drive\",\"4\":\"Towards a Modular Suturing Catheter for Minimally Invasive Vascular Surgery\",\"5\":\"An Observer-Based Fusion Method Using Multicore Optical Shape Sensors and Ultrasound Images for Magnetically-Actuated Catheters\",\"6\":\"Reflection-Aware Sound Source Localization\",\"7\":\"Deep Neural Networks for Multiple Speaker Detection and Localization\",\"8\":\"DroneEARS: Robust Acoustic Source Localization with Aerial Drones\",\"9\":\"Ultra-Wideband Radar for Robust Inspection Drone in Underground Coal Mines\",\"10\":\"Inertial Machine Monitoring System for Automated Failure Detection\",\"11\":\"iMag: Accurate and Rapidly Deployable Inertial Magneto-Inductive Localisation\",\"12\":\"Parallel Pick and Place Using Two Independent Untethered Mobile Magnetic Microgrippers\",\"13\":\"Development and Experimental Validation of a Combined FBG Force and OCT Distance Sensing Needle for Robot-Assisted Retinal Vein Cannulation\",\"14\":\"Requirements Based Design and End-to-End Dynamic Modeling of a Robotic Tool for Vitreoretinal Surgery\",\"15\":\"ESD CYCLOPS: A New Robotic Surgical System for GI Surgery\",\"16\":\"Differentiation of C2C12 Myoblasts and Characterization of Electro-Responsive Beating Behavior of Myotubes Using Circularly Distributed Multiple Electrodes for Bio-Syncretic Robot\",\"17\":\"String Untying Planning Based on Knot Theory and Proposal of Algorithms to Generate the Motion of a Manipulator\",\"18\":\"Compliant Low Profile Multi-Axis Force Sensors\",\"19\":\"A Novel Approach to Under-Actuated Control of Fluidic Systems\",\"20\":\"Introducing PneuAct: Parametrically-Designed MRI-Compatible Pneumatic Stepper Actuator\",\"21\":\"Efficient Planning for Near-Optimal Compliant Manipulation Leveraging Environmental Contact\",\"22\":\"Constrained Sampling-Based Planning for Grasping and Manipulation\",\"23\":\"Geometric In-Hand Regrasp Planning: Alternating Optimization of Finger Gaits and In-Grasp Manipulation\",\"24\":\"Manipulating Highly Deformable Materials Using a Visual Feedback Dictionary\",\"25\":\"Reactive Planar Manipulation with Convex Hybrid MPC\",\"26\":\"Stable Prehensile Pushing: In-Hand Manipulation with Alternating Sticking Contacts\",\"27\":\"Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement Learning\",\"28\":\"Decentralized Adaptive Control for Collaborative Manipulation\",\"29\":\"Trajectory Generation for Minimum Closed-Loop State Sensitivity\",\"30\":\"Modeling and Control of Multi-Arm and Multi-Leg Robots: Compensating for Object Dynamics During Grasping\",\"31\":\"Multi-Priority Cartesian Impedance Control Based on Quadratic Programming Optimization\",\"32\":\"Responsive and Reactive Dual-Arm Robot Coordination\",\"33\":\"Contact Point Localization for Articulated Manipulators with Proprioceptive Sensors and Machine Learning\",\"34\":\"L\\n1\\nRobustness of Computed Torque Method for Robot Manipulators\",\"35\":\"Online Safe Trajectory Generation for Quadrotors Using Fast Marching Method and Bernstein Basis Polynomial\",\"36\":\"Coverage Path Planning Under the Energy Constraint\",\"37\":\"The Dubins Traveling Salesman Problem with Neighborhoods in the Three-Dimensional Space\",\"38\":\"The Dubins Car and Other Arm-Like Mobile Robots\",\"39\":\"Planning, Fast and Slow: A Framework for Adaptive Real-Time Safe Trajectory Planning\",\"40\":\"Reactive Bipedal Walking Method for Torque Controlled Robot\",\"41\":\"Disturbance Observer Based Linear Feedback Controller for Compliant Motion of Humanoid Robot\",\"42\":\"Unsupervised Contact Learning for Humanoid Estimation and Control\",\"43\":\"Robust Control of Dynamic Walking Robots Using Transverse H\\u221e\",\"44\":\"Investigation of a Bipedal Platform for Rapid Acceleration and Braking Manoeuvres\",\"45\":\"Comparison Study of Nonlinear Optimization of Step Durations and Foot Placement for Dynamic Walking\",\"46\":\"Torque-Based Dynamic Walking - A Long Way from Simulation to Experiment\",\"47\":\"Online Falling-Over Control of Humanoids Exploiting Energy Shaping and Distribution Methods\",\"48\":\"Low-Cost Electromechanical Actuator Arrays for Tactile Display Applications\",\"49\":\"High Stiffness in Teleoperated Comanipulation: Necessity or Luxury?\",\"50\":\"Scaling Inertial Forces to Alter Weight Perception in Virtual Reality\",\"51\":\"Effects of Latency and Refresh Rate on Force Perception via Sensory Substitution by Force-Controlled Skin Deformation Feedback\",\"52\":\"Know Rob 2.0 \\u2014 A 2nd Generation Knowledge Processing Framework for Cognition-Enabled Robotic Agents\",\"53\":\"Intuitive Constraint-Based Robot Programming for Robotic Assembly Tasks\",\"54\":\"MaestROB: A Robotics Framework for Integrated Orchestration of Low-Level Control and High-Level Reasoning\",\"55\":\"Ctrl-MORE: A Framework to Integrate Controllers of Multi-DoF Robot for Developers and Users\",\"56\":\"Cross-Layer Retrofitting of UAVs Against Cyber-Physical Attacks\",\"57\":\"A Prototype-Based Skill Model for Specifying Robotic Assembly Tasks\",\"58\":\"Facilitating Model-Based Control Through Software-Hardware Co-Design\",\"59\":\"Multilayered Kinodynamics Simulation for Detailed Whole-Body Motion Generation and Analysis\",\"60\":\"Inference of User Qualities in Shared Control\",\"61\":\"Sample and Feedback Efficient Hierarchical Reinforcement Learning from Human Preferences\",\"62\":\"Investigation of Communicative Flight Paths for Small Unmanned Aerial Systems\",\"63\":\"Inverse Reinforcement Learning via Function Approximation for Clinical Motion Analysis\",\"64\":\"Learning User Preferences in Robot Motion Planning Through Interaction\",\"65\":\"Deep-LK for Efficient Adaptive Object Tracking\",\"66\":\"End-to-end Learning of Multi-sensor 3D Tracking by Detection\",\"67\":\"Visual Articulated Tracking in the Presence of Occlusions\",\"68\":\"Planar Object Tracking in the Wild: A Benchmark\",\"69\":\"Constrained Confidence Matching for Planar Object Tracking\",\"70\":\"Deep Forward and Inverse Perceptual Models for Tracking and Prediction\",\"71\":\"ModQuad: The Flying Modular Structure that Self-Assembles in Midair\",\"72\":\"Autonomous Battery Exchange of UAVs with a Mobile Ground Base\",\"73\":\"A Whole Body Attitude Stabilizer for Hybrid Wheeled-Legged Quadruped Robots\",\"74\":\"Learning Motion Predictors for Smart Wheelchair Using Autoregressive Sparse Gaussian Process\",\"75\":\"Local Behavior-Based Navigation in Rough Off-Road Scenarios Based on Vehicle Kinematics\",\"76\":\"Controlling a Non-Holonomic Mobile Manipulator in a Constrained Floor Space\",\"77\":\"A Parametric MPC Approach to Balancing the Cost of Abstraction for Differential-Drive Mobile Robots\",\"78\":\"Dynamic Simulation of Planetary Rovers with Terrain Property Mapping\",\"79\":\"Development of an Fast-Omnidirectional Treadmill (F-ODT) for Immersive Locomotion Interface\",\"80\":\"Design Considerations and Redundancy Resolution for Variable Geometry Continuum Robots\",\"81\":\"Extending a Dynamic Friction Model with Nonlinear Viscous and Thermal Dependency for a Motor and Harmonic Drive Gear\",\"82\":\"Eddy Current Damper Design for Vibration Suppression in Robotic Milling Process\",\"83\":\"Learning-Based Image Enhancement for Visual Odometry in Challenging HDR Environments\",\"84\":\"Spherical Visual Gyroscope for Autonomous Robots Using the Mixture of Photometric Potentials\",\"85\":\"Learning Place-and-Time-Dependent Binary Descriptors for Long-Term Visual Localization\",\"86\":\"Correlation Flow: Robust Optical Flow Using Kernel Cross-Correlators\",\"87\":\"Cubic Range Error Model for Stereo Vision with Illuminators\",\"88\":\"Fusion of Stereo and Still Monocular Depth Estimates in a Self-Supervised Learning Context\",\"89\":\"Exposure Control Using Bayesian Optimization Based on Entropy Weighted Image Gradient\",\"90\":\"Compliant Manipulation of Free-Floating Objects\",\"91\":\"Validation of the Robot Rendezvous and Grasping Manoeuvre Using Microgravity Simulators\",\"92\":\"Collision-Based Contact Mode Estimation for Dynamic Rigid Body Capture\",\"93\":\"Workspace Fixation for Free-Floating Space Robot Operations\",\"94\":\"Robust Visual Localization for Hopping Rovers on Small Bodies\",\"95\":\"Wheel Design Methodology for a Lunar Exploration Rover in Order to Improve Trafficability Considering Operation Environment\",\"96\":\"Whole-Body Impedance Control for a Planetary Rover with Robotic Arm: Theory, Control Design, and Experimental Validation\",\"97\":\"Kinematic Design Optimization of a Parallel Surgical Robot to Maximize Anatomical Visibility via Motion Planning\",\"98\":\"Workspace, Transmissibility and Dynamics of a New 3T3R Parallel Pick-and-place Robot with High Rotational Capability\",\"99\":\"Dynamic Control of Cable Driven Parallel Robots with Unknown Cable Stiffness: a Joint Space Approach\",\"100\":\"New Kinematic Structures for Two-Loop Generalized Parallel Mechanism Designs\",\"101\":\"Available Wrench Set for Planar Mobile Cable-Driven Parallel Robots\",\"102\":\"Closed-form Solution for the Direct Kinematics Problem of the Planar 3-RPR Parallel Mechanism\",\"103\":\"Yet Another Approach to the Gough-Stewart Platform Forward Kinematics\",\"104\":\"Bounding Drift in Cooperative Localisation Through the Sharing of Local Loop Closures\",\"105\":\"Monocular Visual Odometry Scale Recovery Using Geometrical Constraint\",\"106\":\"Detection and Resolution of Motion Conflict in Visual Inertial Odometry\",\"107\":\"Predicting Alignment Risk to Prevent Localization Failure\",\"108\":\"Adversarial Training for Adverse Conditions: Robust Metric Localisation Using Appearance Transfer\",\"109\":\"Algorithm for Optimal Chance Constrained Knapsack Problem with Applications to Multi-Robot Teaming\",\"110\":\"Planning-Aware Communication for Decentralised Multi-Robot Coordination\",\"111\":\"Multi-Robot Realization Based on Goal Adjacency Constraints\",\"112\":\"Cooperative Object Transport in 3D with Multiple Quadrotors Using No Peer Communication\",\"113\":\"An Energy-Based Approach for the Multi-Rate Control of a Manipulator on an Actuated Base\",\"114\":\"Optimal Intermittent Deployment and Sensor Selection for Environmental Sensing with Multi-Robot Teams\",\"115\":\"Cooperative Object Transportation by Multiple Ground and Aerial Vehicles: Modeling and Planning\",\"116\":\"Integrating Planning and Execution for a Team of Heterogeneous Robots with Time and Communication Constraints\",\"117\":\"Data-Driven Approach to Simulating Realistic Human Joint Constraints\",\"118\":\"Generative Adversarial Nets in Robotic Chinese Calligraphy\",\"119\":\"Socially Compliant Navigation Through Raw Depth Inputs with Generative Adversarial Imitation Learning\",\"120\":\"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation\",\"121\":\"Incremental Task Modification via Corrective Demonstrations\",\"122\":\"Time-Contrastive Networks: Self-Supervised Learning from Video\",\"123\":\"Learning Sensor Feedback Models from Demonstrations via Phase-Modulated Neural Networks\",\"124\":\"Robot Navigation from Human Demonstration: Learning Control Behaviors\",\"125\":\"Relocalization, Global Optimization and Map Merging for Monocular Visual-Inertial SLAM\",\"126\":\"Elastic LiDAR Fusion: Dense Map-Centric Continuous-Time SLAM\",\"127\":\"Design, modeling and control of t3-multirotor: a tilting thruster type multirotor\",\"128\":\"Design and Analysis of a Fixed-Wing Unmanned Aerial-Aquatic Vehicle\",\"129\":\"An Empirical Evaluation of Ground Effect for Small-Scale Rotorcraft\",\"130\":\"Design, Modeling and Control of a Solar-Powered Quadcopter\",\"131\":\"The UNAV, a Wind-Powered UAV for Ocean Monitoring: Performance, Control and Validation\",\"132\":\"Distributed Multi-Robot Cooperation for Information Gathering Under Communication Constraints\",\"133\":\"Automated Pick-Up of Suturing Needles for Robotic Surgical Assistance\",\"134\":\"A Hybrid Actuated Robotic Prototype for Minimally Invasive Surgery\",\"135\":\"Design and Test of an In-Vivo Robotic Camera Integrated with Optimized Illumination System for Single-port Laparoscopic Surgery\",\"136\":\"A Novel Magnetic Anchored and Steered Camera Robot for Single Port Access Surgery\",\"137\":\"Vehicle Detection, Tracking and Behavior Analysis in Urban Driving Environments Using Road Context\",\"138\":\"GOMSF: Graph-Optimization Based Multi-Sensor Fusion for robust UAV Pose estimation\",\"139\":\"Attitude, Linear Velocity and Depth Estimation of a Camera Observing a Planar Target Using Continuous Homography and Inertial Data\",\"140\":\"Deep Inference for Covariance Estimation: Learning Gaussian Noise Models for State Estimation\",\"141\":\"A Study on Optimal Placement of Accelerometers for Pose Estimation of a Robot Arm\",\"142\":\"Encoder-Camera-Ground Penetrating Radar Tri-Sensor Mapping for Surface and Subsurface Transportation Infrastructure Inspection\",\"143\":\"Angle Estimation for Robotic Arms on Floating Base Using Low-Cost IMUS\",\"144\":\"IntuBot: Design and Prototyping of a Robotic Intubation Device\",\"145\":\"Locomotion Envelopes for Adaptive Control of Powered Ankle Prostheses\",\"146\":\"Modeling and Characterization of a Potential Bladder Based Orthotic Device to Mitigate Shoe Slip\",\"147\":\"Preliminary Results of a Handheld Nerve Electrode Insertion Device\",\"148\":\"Programmable Medicine: Autonomous, Ingestible, Deployable Hydrogel Patch and Plug for Stomach Ulcer Therapy\",\"149\":\"Open-Loop Drug Delivery Strategy to the Cochlea Using a Permanent Magnetic Actuator\",\"150\":\"Augmented Joint Stiffness and Actuation Using Architectures of Soft Pneumatic Actuators\",\"151\":\"APAM: Antagonistic Pneumatic Artificial Muscle\",\"152\":\"Passive and Active Particle Damping in Soft Robotic Actuators\",\"153\":\"A Fluid-Filled Tubular Dielectric Elastomer Variable Stiffness Structure Inspired by the Hydrostatic Skeleton Principle\",\"154\":\"Stiffness Variability in Jamming of Compliant Granules and a Case Study Application in Climbing Vertical Shafts\",\"155\":\"A Geometric and Unified Approach for Modeling Soft-Rigid Multi-Body Systems with Lumped and Distributed Degrees of Freedom\",\"156\":\"Morphological Adaptation in an Energy Efficient Vibration-Based Robot\",\"157\":\"Bio-Inspired Octopus Robot Based on Novel Soft Fluidic Actuator\",\"158\":\"Completion Time Analysis for Automated Manufacturing Systems with Parallel Processing Modules\",\"159\":\"Reliably Arranging Objects in Uncertain Domains\",\"160\":\"RoboTSP \\u2013 A Fast Solution to the Robotic Task Sequencing Problem\",\"161\":\"An Automated Reactive Approach to Single Robot Exogeneous Planar Assembly\",\"162\":\"Robotic Cleaning Through Dirt Rearrangement Planning with Learned Transition Models\",\"163\":\"Fast Planning for 3D Any-Pose-Reorienting Using Pivoting\",\"164\":\"An Intelligent Control Scheme to Facilitate Abrupt Stopping on Self-Adjustable Treadmills\",\"165\":\"Modeling and Control of Brachiating Robots Traversing Flexible Cables\",\"166\":\"Performance Indicator for Benchmarking Force-Controlled Robots\",\"167\":\"A Failure-Tolerant Approach to Synchronous Formation Control of Mobile Robots Under Communication Delays\",\"168\":\"Perceived Stiffness Estimation for Robot Force Control\",\"169\":\"Grasp a Moving Target from the Air: System & Control of an Aerial Manipulator\",\"170\":\"Task Space Motion Planning Decomposition\",\"171\":\"Planning Hybrid Driving-Stepping Locomotion on Multiple Levels of Abstraction\",\"172\":\"Design and Evaluation of Skating Motions for a Dexterous Quadruped\",\"173\":\"Gradient-Informed Path Smoothing for Wheeled Mobile Robots\",\"174\":\"Indoor Coverage Path Planning: Survey, Implementation, Analysis\",\"175\":\"Robot Navigation in Complex Workspaces Using Harmonic Maps\",\"176\":\"Backprop-MPDM: Faster Risk-Aware Policy Evaluation Through Efficient Gradient Optimization\",\"177\":\"Spherical Foot Placement Estimator for Humanoid Balance Control and Recovery\",\"178\":\"Simultaneous Optimization of ZMP and Footsteps Based on the Analytical Solution of Divergent Component of Motion\",\"179\":\"Bayesian Optimization Using Domain Knowledge on the ATRIAS Biped\",\"180\":\"Balance Control Using Both ZMP and COM Height Variations: A Convex Boundedness Approach\",\"181\":\"An MPC Walking Framework with External Contact Forces\",\"182\":\"Inclusion of Angular Momentum During Planning for Capture Point Based Walking\",\"183\":\"Subject-Independent Data Pooling in Classification of Gait Intent Using Mechanomyography on a Transtibial Amputee\",\"184\":\"Embroidered Electrodes for Control of Affordable Myoelectric Prostheses\",\"185\":\"Continuous Wrist Joint Control Using Muscle Deformation Measured on Forearm Skin\",\"186\":\"Empirical Quantification and Modeling of Muscle Deformation: Toward Ultrasound-Driven Assistive Device Control\",\"187\":\"Grasp-training Robot to Activate Neural Control Loop for Reflex and Experimental Verification\",\"188\":\"Multi-View 3D Entangled Forest for Semantic Segmentation and Mapping\",\"189\":\"Mark Yourself: Road Marking Segmentation via Weakly-Supervised Annotations from Multimodal Data\",\"190\":\"Semantic Labeling of Indoor Environments from 3D RGB Maps\",\"191\":\"Practical Motion Segmentation for Urban Street View Scenes\",\"192\":\"SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud\",\"193\":\"Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments\",\"194\":\"Semantic Mapping with Omnidirectional Vision\",\"195\":\"Semantic Segmentation from Limited Training Data\",\"196\":\"Planning Ergonomic Sequences of Actions in Human-Robot Interaction\",\"197\":\"Proposal of Collaboration Safety in a Coexistence Environment of Human and Robots\",\"198\":\"A Robust Method to Predict Temporal Aspects of Actions by Observation\",\"199\":\"Augmented Reality for Feedback in a Shared Control Spraying Task\",\"200\":\"Interactive Robot Knowledge Patching Using Augmented Reality\",\"201\":\"Using Constrained Optimization for Real-Time Synchronization of Verbal and Nonverbal Robot Behavior\",\"202\":\"Learning Task-Based Instructional Policy for Excavator-Like Robots\",\"203\":\"Playdough to Roombots: Towards a Novel Tangible User Interface for Self-reconfigurable Modular Robots\",\"204\":\"A Hierarchical Model for Action Recognition Based on Body Parts\",\"205\":\"3D Human Pose Estimation in RGBD Images for Robotic Task Learning\",\"206\":\"Safe and Efficient Human-Robot Collaboration Part I: Estimation of Human Arm Motions\",\"207\":\"Robust Real-Time 3D Person Detection for Indoor and Outdoor Applications\",\"208\":\"Pedestrian Feature Generation in Fish-Eye Images via Adversary\",\"209\":\"Eye on You: Fusing Gesture Data from Depth Camera and Inertial Sensors for Person Identification\",\"210\":\"Human Motion Capture Using a Drone\",\"211\":\"Navigating Occluded Intersections with Autonomous Vehicles Using Deep Reinforcement Learning\",\"212\":\"Autonomous Vehicle Navigation in Rural Environments Without Detailed Prior Maps\",\"213\":\"Design of an Autonomous Racecar: Perception, State Estimation and System Integration\",\"214\":\"Dynamic Occupancy Grid Prediction for Urban Autonomous Driving: A Deep Learning Approach with Fully Automatic Labeling\",\"215\":\"Realtime Vehicle and Pedestrian Tracking for Didi Udacity Self-Driving Car Challenge\",\"216\":\"End-to-End Race Driving with Deep Reinforcement Learning\",\"217\":\"Scalable Decision Making with Sensor Occlusions for Autonomous Driving\",\"218\":\"Situation Assessment for Planning Lane Changes: Combining Recurrent Models and Prediction\",\"219\":\"Design and Analysis of a Novel Underwater Glider - RoBuoy\",\"220\":\"Modeling Speed-, Load-, and Position-Dependent Friction Effects in Strain Wave Gears\",\"221\":\"Real-Time Identification of Robot Payload Using a Multirate Quaternion-Based Kalman Filter and Recursive Total Least-Squares\",\"222\":\"Optimal Active Sensing with Process and Measurement Noise\",\"223\":\"Encoderless Gimbal Calibration of Dynamic Multi-Camera Clusters\",\"224\":\"Stickman: Towards a Human Scale Acrobatic Robot\",\"225\":\"3D Lidar-IMU Calibration Based on Upsampled Preintegrated Measurements for Motion Distortion Correction\",\"226\":\"High-Precision Depth Estimation with the 3D LiDAR and Stereo Fusion\",\"227\":\"Sampled-Point Network for Classification of Deformed Building Element Point Clouds\",\"228\":\"Recognizing Objects in-the-Wild: Where do we Stand?\",\"229\":\"A Controlled-Delay Event Camera Framework for On-Line Robotics\",\"230\":\"Gemsketch: Interactive Image-Guided Geometry Extraction from Point Clouds\",\"231\":\"Where can i do this? Geometric Affordances from a Single Example with the Interaction Tensor\",\"232\":\"A Low-Cost Navigation Strategy for Yield Estimation in Vineyards\",\"233\":\"Object Detection for Cattle Gait Tracking\",\"234\":\"Routing Algorithms for Robot Assisted Precision Irrigation\",\"235\":\"Real-Time Semantic Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs\",\"236\":\"Robustly Adjusting Indoor Drip Irrigation Emitters with the Toyota HSR Robot\",\"237\":\"Preliminary Study of Twisted String Actuation Through a Conduit Toward Soft and Wearable Actuation\",\"238\":\"Stiffness Decomposition and Design Optimization of Under-Actuated Tendon-Driven Robotic Systems\",\"239\":\"Design and Development of Effective Transmission Mechanisms on a Tendon Driven Hand Orthosis for Stroke Patients\",\"240\":\"Discovering a Library of Rhythmic Gaits for Spherical Tensegrity Locomotion\",\"241\":\"Line-Based Global Localization of a Spherical Camera in Manhattan Worlds\",\"242\":\"Optimizing Placement and Number of RF Beacons to Achieve Better Indoor Localization\",\"243\":\"Robust Target-Relative Localization with Ultra-Wideband Ranging and Communication\",\"244\":\"Visual Odometry Using a Homography Formulation with Decoupled Rotation and Translation Estimation Using Minimal Solutions\",\"245\":\"Local Nearest Neighbor Integrity Risk Evaluation for Robot Navigation\",\"246\":\"Aided Inertial Navigation with Geometric Features: Observability Analysis\",\"247\":\"Omnidirectional CNN for Visual Place Recognition and Navigation\",\"248\":\"Addressing Challenging Place Recognition Tasks Using Generative Adversarial Networks\",\"249\":\"Re-Deployment Algorithms for Multiple Service Robots to Optimize Task Response\",\"250\":\"Multi-Agent Time-Based Decision-Making for the Search and Action Problem\",\"251\":\"Multi-robot Dubins Coverage with Autonomous Surface Vehicles\",\"252\":\"How Many Robots are Enough: A Multi-Objective Genetic Algorithm for the Single-Objective Time-Limited Complete Coverage Problem\",\"253\":\"Joint Multi-Policy Behavior Estimation and Receding-Horizon Trajectory Planning for Automated Urban Driving\",\"254\":\"Robust Environmental Mapping by Mobile Sensor Networks\",\"255\":\"Best Response Model Predictive Control for Agile Interactions Between Autonomous Ground Vehicles\",\"256\":\"A Deep Incremental Boltzmann Machine for Modeling Context in Robots\",\"257\":\"Accelerating Model Learning with Inter-Robot Knowledge Transfer\",\"258\":\"Online Learning of a Memory for Learning Rates\",\"259\":\"Learning Coupled Forward-Inverse Models with Combined Prediction Errors\",\"260\":\"DEFO-NET: Learning Body Deformation Using Generative Adversarial Networks\",\"261\":\"Bodily Aware Soft Robots: Integration of Proprioceptive and Exteroceptive Sensors\",\"262\":\"Deep Learning a Quadrotor Dynamic Model for Multi-Step Prediction\",\"263\":\"Safe Learning of Quadrotor Dynamics Using Barrier Certificates\",\"264\":\"Data-Efficient Decentralized Visual SLAM\",\"265\":\"A Linear Least Square Initialization Method for 3D Pose Graph Optimization Problem\",\"266\":\"IMLS-SLAM: Scan-to-Model Matching Based on 3D Data\",\"267\":\"ApriISAM: Real-Time Smoothing and Mapping\",\"268\":\"Fast Nonlinear Approximation of Pose Graph Node Marginalization\",\"269\":\"A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots\",\"270\":\"Direct Sparse Visual-Inertial Odometry Using Dynamic Marginalization\",\"271\":\"A Monocular SLAM System Leveraging Structural Regularity in Manhattan World\",\"272\":\"Visual Saliency-Aware Receding Horizon Autonomous Exploration with Application to Aerial Robotics\",\"273\":\"Perception-aware Receding Horizon Navigation for MAVs\",\"274\":\"Viewpoint-Tolerant Place Recognition Combining 2D and 3D Information for UAV Navigation\",\"275\":\"Flexible Stereo: Constrained, Non-Rigid, Wide-Baseline Stereo Vision for Fixed-Wing Aerial Platforms\",\"276\":\"Visual-Inertial Navigation Algorithm Development Using Photorealistic Camera Simulation in the Loop\",\"277\":\"Development and Implementation of High Power Hexapole Magnetic Tweezer System for Micromanipulations\",\"278\":\"Robotic Immobilization of Motile Sperm\",\"279\":\"Automated Non-Invasive Measurement of Sperm Motility and Morphology Parameters\",\"280\":\"Construction of Hepatic Lobule-Like Vascular Network by Using Magnetic Fields\",\"281\":\"A Framework for Sensorless Tissue Motion Tracking in Robotic Endomicroscopy Scanning\",\"282\":\"SAT-C: An Efficient Control Strategy for Assembly of Heterogeneous Stress-Engineered MEMS Microrobots\",\"283\":\"Robotic Intracellular Manipulation: 3D Navigation and Measurement Inside a Single Cell\",\"284\":\"ViTac: Feature Sharing Between Vision and Tactile Sensing for Cloth Texture Recognition\",\"285\":\"Calibration and Analysis of Tactile Sensors as Slip Detectors\",\"286\":\"Voronoi Features for Tactile Sensing: Direct Inference of Pressure, Shear, and Contact Locations\",\"287\":\"Learning Manipulation Graphs from Demonstrations Using Multimodal Sensory Signals\",\"288\":\"ExoSense: Measuring Manipulation in a Wearable Manner\",\"289\":\"Robotizing Double-Bar Ankle-Foot Orthosis\",\"290\":\"Design and Benchtop Validation of a Powered Knee-Ankle Prosthesis with High-Torque, Low-Impedance Actuators\",\"291\":\"Variable Transmission Series Elastic Actuator for Robotic Prosthesis\",\"292\":\"Towards Restoring Locomotion for Paraplegics: Realizing Dynamically Stable Walking on Exoskeletons\",\"293\":\"Autonomous Multi-Joint Soft Exosuit for Assistance with Walking Overground\",\"294\":\"A Lightweight and Efficient Portable Soft Exosuit for Paretic Ankle Assistance in Walking After Stroke\",\"295\":\"Comparing Assistive Admittance Control Algorithms for a Trunk Supporting Exoskeleton\",\"296\":\"Dynamic Actuator Selection and Robust State-Feedback Control of Networked Soft Actuators\",\"297\":\"Safety and Guaranteed Stability Through Embedded Energy-Aware Actuators\",\"298\":\"High-Level MLN-Based Approach for Spatial Context Disambiguation\",\"299\":\"Pairwise Consistent Measurement Set Maximization for Robust Multi-Robot Map Merging\",\"300\":\"Task-Specific Sensor Planning for Robotic Assembly Tasks\",\"301\":\"Map-Aware Particle Filter for Localization\",\"302\":\"Active Motion-Based Communication for Robots with Monocular Vision\",\"303\":\"A Novel Recurrent Neural Network for Improving Redundant Manipulator Motion Planning Completeness\",\"304\":\"Robust Collision Avoidance via Sliding Control\",\"305\":\"Optimizing Simulations with Noise-Tolerant Structured Exploration\",\"306\":\"Using a Memory of Motion to Efficiently Warm-Start a Nonlinear Predictive Controller\",\"307\":\"Goal Directed Dynamics\",\"308\":\"Regression-Based Linear Quadratic Regulator\",\"309\":\"Time-Optimal Path Tracking via Reachability Analysis\",\"310\":\"Acceleration of Gradient-Based Path Integral Method for Efficient Optimal and Inverse Optimal Control\",\"311\":\"Charging Station Placement for Indoor Robotic Applications\",\"312\":\"Path Tracking of a Two-Wheel Steering Mobile Robot: An Accurate and Robust Multi-Model Off-Road Steering Strategy\",\"313\":\"Annotating Traversable Gaps in Walkable Environments\",\"314\":\"Topological Nearest-Neighbor Filtering for Sampling-Based Planners\",\"315\":\"Integration of Local Geometry and Metric Information in Sampling-Based Motion Planning\",\"316\":\"Realization of a Real-Time Optimal Control Strategy to Stabilize a Falling Humanoid Robot with Hand Contact\",\"317\":\"Markerless Visual Servoing on Unknown Objects for Humanoid Robot Platforms\",\"318\":\"Generating Assistive Humanoid Motions for Co-Manipulation Tasks with a Multi-Robot Quadratic Program Controller\",\"319\":\"Affordance-Based Multi-Contact Whole-Body Pose Sequence Planning for Humanoid Robots in Unknown Environments\",\"320\":\"Model-Based External Force\\/Moment Estimation for Humanoid Robots with no Torque Measurement\",\"321\":\"Nonintuitive Optima for Dynamic Locomotion: The Acrollbot\",\"322\":\"Simultaneous Planning and Estimation Based on Physics Reasoning in Robot Manipulation\",\"323\":\"Learning Modes of Within-Hand Manipulation\",\"324\":\"Cost Functions to Specify Full-Body Motion and Multi-Goal Manipulation Tasks\",\"325\":\"Robot Composite Learning and the Nunchaku Flipping Challenge\",\"326\":\"Iterative Learning Scheme for Dexterous In-Hand Manipulation with Stochastic Uncertainty\",\"327\":\"Dexterous Manipulation by Two Fingers with Coupled Joints\",\"328\":\"Extrinsic Dexterity Through Active Slip Control Using Deep Predictive Models\",\"329\":\"A General Pipeline for 3D Detection of Vehicles\",\"330\":\"Meshed Up: Learnt Error Correction in 3D Reconstructions\",\"331\":\"Fast Disparity Estimation Using Dense Networks\",\"332\":\"SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes\",\"333\":\"Bayesian Viewpoint-Dependent Robust Classification Under Model and Localization Uncertainty\",\"334\":\"Signature of Topologically Persistent Points for 3D Point Cloud Description\",\"335\":\"Label Fusion: A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes\",\"336\":\"Dropout Sampling for Robust Object Detection in Open-Set Conditions\",\"337\":\"Early Turn-Taking Prediction with Spiking Neural Networks for Human Robot Collaboration\",\"338\":\"Learning Human Ergonomic Preferences for Handovers\",\"339\":\"Joining High-Level Symbolic Planning with Low-Level Motion Primitives in Adaptive HRI: Application to Dressing Assistance\",\"340\":\"A Passivity-Based Strategy for Coaching in Human-Robot Interaction\",\"341\":\"A Tensegrity-Inspired Compliant 3-DOF Compliant Joint\",\"342\":\"Training Deep Neural Networks for Visual Servoing\",\"343\":\"A Delay Compensation Approach for Pan-Tilt-Unit-based Stereoscopic 360 Degree Telepresence Systems Using Head Motion Prediction\",\"344\":\"Improving 6D Pose Estimation of Objects in Clutter Via Physics-Aware Monte Carlo Tree Search\",\"345\":\"SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Control\",\"346\":\"Fast Object Learning and Dual-arm Coordination for Cluttered Stowing, Picking, and Packing\",\"347\":\"Optical Sensing and Control Methods for Soft Pneumatically Actuated Robotic Manipulators\",\"348\":\"Mono-Stixels: Monocular Depth Reconstruction of Dynamic Street Scenes\",\"349\":\"The DriveU Traffic Light Dataset: Introduction and Comparison with Existing Datasets\",\"350\":\"Modeling Driver Behavior from Demonstrations in Dynamic Environments Using Spatiotemporal Lattices\",\"351\":\"Multimodal Probabilistic Model-Based Planning for Human-Robot Interaction\",\"352\":\"AA-ICP: Iterative Closest Point with Anderson Acceleration\",\"353\":\"Robust and Fast 3D Scan Alignment Using Mutual Information\",\"354\":\"Drive Video Analysis for the Detection of Traffic Near-Miss Incidents\",\"355\":\"A Modular Dielectric Elastomer Actuator to Drive Miniature Autonomous Underwater Vehicles\",\"356\":\"Proprioceptive-Inertial Autonomous Locomotion for Articulated Robots\",\"357\":\"Autonomous Bio-Inspired Small-Object Detection and Avoidance\",\"358\":\"PISRob: A Pneumatic Soft Robot for Locomoting Like an Inchworm\",\"359\":\"Continuous Growth in Plant-Inspired Robots Through 3D Additive Manufacturing\",\"360\":\"Investigation of Scaling Effect of Copper Microwire Based on in-Situ Nanorobotic Twisting Inside SEM\",\"361\":\"Optimisation of Trap Design for Vibratory Bowl Feeders\",\"362\":\"Teach-and-Replay of Mobile Robot with Particle Filter on Episode\",\"363\":\"Vision-Based Robotic Grasping and Manipulation of USB Wires\",\"364\":\"Visual Grasping for a Lightweight Aerial Manipulator Based on NSGA-II and Kinematic Compensation\",\"365\":\"Track, Then Decide: Category-Agnostic Vision-Based Multi-Object Tracking\",\"366\":\"Vision-Based Global Localization Using Ceiling Space Density\",\"367\":\"Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking\",\"368\":\"Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation\",\"369\":\"Learning Robotic Assembly from CAD\",\"370\":\"Accurate and Adaptive in Situ Fabrication of an Undulated Wall Using an on-Board Visual Sensing System\",\"371\":\"Robot Assisted Carpentry for Mass Customization\",\"372\":\"A General and Flexible Search Framework for Disassembly Planning\",\"373\":\"1-Actuator 3-DoF Manipulation Using a Virtual Turntable Based on Differential Friction Surface\",\"374\":\"A Fish-Like Magnetically Propelled Microswimmer Fabricated by 3D Laser Lithography\",\"375\":\"Liftoff of a 190 mg Laser-Powered Aerial Vehicle: The Lightest Wireless Robot to Fly\",\"376\":\"Soft Miniaturized Linear Actuators Wirelessly Powered by Rotating Permanent Magnets\",\"377\":\"Eight-Degrees-of-Freedom Remote Actuation of Small Magnetic Mechanisms\",\"378\":\"Feature-Based SLAM for Imaging Sonar with Under-Constrained Landmarks\",\"379\":\"SLAMBench2: Multi-Objective Head-to-Head Benchmarking for Visual SLAM\",\"380\":\"Don't Look Back: Robustifying Place Categorization for Viewpoint- and Condition-Invariant Place Recognition\",\"381\":\"Delight: An Efficient Descriptor for Global Localisation Using LiDAR Intensities\",\"382\":\"Online Probabilistic Change Detection in Feature-Based Maps\",\"383\":\"The Dynamic Bearing Observability Matrix Nonlinear Observability and Estimation for Multi-Agent Systems\",\"384\":\"CDDT: Fast Approximate 2D Ray Casting for Accelerated Localization\",\"385\":\"Towards Globally Consistent Visual-Inertial Collaborative SLAM\",\"386\":\"Modelling Resource Contention in Multi-Robot Task Allocation Problems with Uncertain Timing\",\"387\":\"Constrained-Action POMDPs for Multi-Agent Intelligent Knowledge Distribution\",\"388\":\"Incorporating Potential Contingency Tasks in Multi-Robot Mission Planning\",\"389\":\"Distributed Simultaneous Action and Target Assignment for Multi-Robot Multi-Target Tracking\",\"390\":\"How to Make Fat Autonomous Robots See all Others Fast?\",\"391\":\"A Synchronization Scheme for Position Control of Multiple Rope-Climbing Robots\",\"392\":\"Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching\",\"393\":\"Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-to-End Learning from Demonstration\",\"394\":\"Learning 6-DOF Grasping Interaction via Deep Geometry-Aware 3D Representations\",\"395\":\"Interactively Picking Real-World Objects with Unconstrained Spoken Language Instructions\",\"396\":\"Translating Videos to Commands for Robotic Manipulation with Deep Recurrent Neural Networks\",\"397\":\"Distributed Learning for the Decentralized Control of Articulated Mobile Robots\",\"398\":\"Neural Task Programming: Learning to Generalize Across Hierarchical Tasks\",\"399\":\"Sim-to-Real Transfer of Robotic Control with Dynamics Randomization\",\"400\":\"Topomap: Topological Mapping and Navigation Based on Visual SLAM Maps\",\"401\":\"PIRVS: An Advanced Visual-Inertial SLAM System with Flexible Sensor Fusion and Hardware Co-Design\",\"402\":\"ProSLAM: Graph SLAM from a Programmer's Perspective\",\"403\":\"Talk Resource-Efficiently to Me: Optimal Communication Planning for Distributed Loop Closure Detection\",\"404\":\"StaticFusion: Background Reconstruction for Dense RGB-D SLAM in Dynamic Environments\",\"405\":\"Vision Based Collaborative Path Planning for Micro Aerial Vehicles\",\"406\":\"Semi-Dense Visual-Inertial Odometry and Mapping for Quadrotors with SWAP Constraints\",\"407\":\"Approximation Algorithms for Tours of Orientation-Varying View Cones\",\"408\":\"Geometric Calibration of an OCT Imaging System\",\"409\":\"Marker-Based Registration for Large Deformations - Application to Open Liver Surgery\",\"410\":\"Real-Time Image-Guided Cooperative Robotic Assist Device for Deep Anterior Lamellar Keratoplasty\",\"411\":\"Hall Effect Sensing Workspace Estimation with Non-Permanent Magnetic Needle for Eye Anesthesia Training System via Robotic Experiments\",\"412\":\"Precision Needle Tip Localization Using Optical Coherence Tomography Images for Subretinal Injection\",\"413\":\"Proprioceptive Inference for Dual-Arm Grasping of Bulky Objects Using RoboSimian\",\"414\":\"Compact and High Performance Torque-Controlled Actuators and its Implementation to Disaster Response Robot\",\"415\":\"High Dynamic Range Sensing by a Multistage Six-Axis Force Sensor with Stopper Mechanism\",\"416\":\"Principal Components of Touch\",\"417\":\"Design and Force-Tracking Impedance Control of a 2-DOF Wall-Cleaning Manipulator Using Disturbance Observer and Sliding Mode Control\",\"418\":\"Artistic Pen Drawing on an Arbitrary Surface Using an Impedance-Controlled Robot\",\"419\":\"Detection and Control of Contact Force Transients in Robotic Manipulation Without a Force Sensor\",\"420\":\"Unsupervised Learning of Hierarchical Models for Hand-Object Interactions\",\"421\":\"Decoupled Motion Control of Wearable Robot for Rejecting Human Induced Disturbances\",\"422\":\"Estimation of Object Orientation Using Conductive Ink and Fabric Based Multilayered Tactile Sensor\",\"423\":\"Magnified Force Sensory Substitution for Telemanipulation via Force-Controlled Skin Deformation\",\"424\":\"Obstacle-Aided Navigation of a Soft Growing Robot\",\"425\":\"Color-Based Sensing of Bending Deformation on Soft Robots\",\"426\":\"Modelling and Control of a Novel Soft Crawling Robot Based on a Dielectric Elastomer Actuator\",\"427\":\"Geometry-based Direct Simulation for Multi-Material Soft Robots\",\"428\":\"Incorporate Oblique Muscle Contractions to Strengthen Soft Robots\",\"429\":\"Efficient FEM-Based Simulation of Soft Robots Modeled as Kinematic Chains\",\"430\":\"Evaluating the Quality of Non-Prehensile Balancing Grasps\",\"431\":\"Transferring Grasping Skills to Novel Instances by Latent Space Non-Rigid Registration\",\"432\":\"Grasping Objects Big and Small: Human Heuristics Relating Grasp-Type and Object Size\",\"433\":\"Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping\",\"434\":\"Coordination of Intrinsic and Extrinsic Degrees of Freedom in Soft Robotic Grasping\",\"435\":\"Reinforcement Learning for 4-Finger-Gripper Manipulation\",\"436\":\"Popcorn-Driven Robotic Actuators\",\"437\":\"A Hybrid Dynamic-Regenerative Damping Scheme for Energy Regeneration in Variable Impedance Actuators\",\"438\":\"Screw-Powered Propulsion in Granular Media: An Experimental and Computational Study\",\"439\":\"Axially and Radially Expandable Modular Helical Soft Actuator for Robotic Implantables\",\"440\":\"Displacement Amplifier Mechanism for Piezoelectric Actuators Design Using SIMP Topology Optimization Approach\",\"441\":\"Clothoid-Based Global Path Planning for Autonomous Vehicles in Urban Scenarios\",\"442\":\"Surface-Based Exploration for Autonomous 3D Modeling\",\"443\":\"Departure and Conflict Management in Multi-Robot Path Coordination\",\"444\":\"A Single-Planner Approach to Multi-Modal Humanoid Mobility\",\"445\":\"Information Based Mobile Sensor Planning for Source Term Estimation of a Non-Continuous Atmospheric Release\",\"446\":\"Collision-Free Motion Planning for Human-Robot Collaborative Safety Under Cartesian Constraint\",\"447\":\"Sampling-Based Motion Planning with \\u03bc-Calculus Specifications Without Steering\",\"448\":\"Generating Vibration Free Rest-to-Rest Trajectories for Configuration Dependent Dynamic Systems via 3-Segmented Input Shaping\",\"449\":\"A Model-Based Hierarchical Controller for Legged Systems Subject to External Disturbances\",\"450\":\"Fore-Aft Leg Specialization Controller for a Dynamic Quadruped\",\"451\":\"Contact Model Fusion for Event-Based Locomotion in Unstructured Terrains\",\"452\":\"Single-Image Footstep Prediction for Versatile Legged Locomotion\",\"453\":\"Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors\",\"454\":\"Learning to Parse Natural Language to Grounded Reward Functions with Weak Supervision\",\"455\":\"Deep Haptic Model Predictive Control for Robot-Assisted Dressing\",\"456\":\"EmoRL: Continuous Acoustic Emotion Classification Using Deep Reinforcement Learning\",\"457\":\"Temporal Spatial Inverse Semantics for Robots Communicating with Humans\",\"458\":\"Brain-Computer Interface Meets ROS: A Robotic Approach to Mentally Drive Telepresence Robots\",\"459\":\"FaNeuRobot: A Framework for Robot and Prosthetics Control Using the NeuCube Spiking Neural Network Architecture and Finite Automata Theory\",\"460\":\"Human in the Loop of Robot Learning: EEG-Based Reward Signal for Target Identification and Reaching Task\",\"461\":\"Incremental Adversarial Domain Adaptation for Continually Changing Environments\",\"462\":\"DeepVP: Deep Learning for Vanishing Point Detection on 1 Million Street View Images\",\"463\":\"Deep Lidar CNN to Understand the Dynamics of Moving Vehicles\",\"464\":\"Optimization Beyond the Convolution: Generalizing Spatial Relations with End-to-End Metric Learning\",\"465\":\"Constructing Category-Specific Models for Monocular Object-SLAM\",\"466\":\"DPDB-Net: Exploiting Dense Connections for Convolutional Encoders\",\"467\":\"The Hands-Free Push-Cart: Autonomous Following in Front by Predicting User Trajectory Around Obstacles\",\"468\":\"Socially Constrained Tracking in Crowded Environments Using Shoulder Pose Estimates\",\"469\":\"Anticipating Many Futures: Online Human Motion Prediction and Generation for Human-Robot Interaction\",\"470\":\"Joint Long-Term Prediction of Human Motion Using a Planning-Based Social Force Approach\",\"471\":\"Negotiating with a Robot: Analysis of Regulatory Focus Behavior\",\"472\":\"Multi3: Multi-Sensory Perception System for Multi-Modal Child Interaction with Multiple Robots\",\"473\":\"Multi-Robot Coordination in Dynamic Environments Shared with Humans\",\"474\":\"Social Attention: Modeling Attention in Human Crowds\",\"475\":\"Fully Convolutional Neural Networks for Road Detection with Multiple Cues Integration\",\"476\":\"A Visual-Inertial Approach to Human Gait Estimation\",\"477\":\"A Deep Learning Based Behavioral Approach to Indoor Autonomous Navigation\",\"478\":\"Footstep Planning in Rough Terrain for Bipedal Robots Using Curved Contact Patches\",\"479\":\"Robust and Precise Vehicle Localization Based on Multi-Sensor Fusion in Diverse City Scenes\",\"480\":\"Safe Distributed Lane Change Maneuvers for Multiple Autonomous Vehicles Using Buffered Input Cells\",\"481\":\"Deep Predictive Models for Collision Risk Assessment in Autonomous Driving\",\"482\":\"End-to-End Driving Via Conditional Imitation Learning\",\"483\":\"VisualBackProp: Efficient Visualization of CNNs for Autonomous Driving\",\"484\":\"Predicting Ego-Vehicle Paths from Environmental Observations with a Deep Neural Network\",\"485\":\"Learning Steering Bounds for Parallel Autonomous Systems\",\"486\":\"End to End Learning of Spiking Neural Network Based on R-STDP for a Lane Keeping Vehicle\",\"487\":\"A Dual-Modal Vision-Based Tactile Sensor for Robotic Hand Grasping\",\"488\":\"Adapting the Goals\\/Questions\\/Metrics (GQM) Method for Applications in Robot Design\",\"489\":\"The Exchange of Knowledge Using Cloud Robotics\",\"490\":\"Dry Stacking for Automated Construction with Irregular Objects\",\"491\":\"High-Speed Well-Focused Image-Capturing System for Moving Micro-Objects Based on Histograms of the Luminance\",\"492\":\"Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image\",\"493\":\"Real-Time Object Tracking in Sparse Point Clouds Based on 3D Interpolation\",\"494\":\"Robust Generalized Point Cloud Registration Using Hybrid Mixture Model\",\"495\":\"Multi-Stage Suture Detection for Robot Assisted Anastomosis Based on Deep Learning\",\"496\":\"Active Clothing Material Perception Using Tactile Sensing and Deep Learning\",\"497\":\"Optimal Path Planning in Time-Varying Flows with Forecasting Uncertainties\",\"498\":\"Topological Hotspot Identification for Informative Path Planning with a Marine Robot\",\"499\":\"Heterogeneous Multi-Robot System for Exploration and Strategic Water Sampling\",\"500\":\"Extended Kalman Filter-Based 3D Active-Alignment Control for LED Communication\",\"501\":\"Robust Model-Aided Inertial Localization for Autonomous Underwater Vehicles\",\"502\":\"Preliminary Evaluation of Cooperative Navigation of Underwater Vehicles without a DVL Utilizing a Dynamic Process Model\",\"503\":\"Self-Calibration of Mobile Manipulator Kinematic and Sensor Extrinsic Parameters Through Contact-Based Interaction\",\"504\":\"Geometry Based Self Kinematic Calibration Method for Industrial Robots\",\"505\":\"Inertial Parameters Identification of a Humanoid Robot Hanged to a Fix Force Sensor\",\"506\":\"Online System Identification and Calibration of Dynamic Models for Autonomous Ground Vehicles\",\"507\":\"On Geometric Models and Their Accuracy for Extrinsic Sensor Calibration\",\"508\":\"Dynamic Modeling and Identification of an Heterogeneously Actuated Underwater Manipulator Arm\",\"509\":\"A General Framework for Flexible Multi-Cue Photometric Point Cloud Registration\",\"510\":\"Just-in-Time Reconstruction: Inpainting Sparse Maps Using Single View Depth Predictors as Priors\",\"511\":\"Fast Global Labelling for Depth-Map Improvement Via Architectural Priors\",\"512\":\"A Method to Segment Maps from Different Modalities Using Free Space Layout MAORIS: Map of Ripples Segmentation\",\"513\":\"Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping\",\"514\":\"Efficient Mobile Robot Exploration with Gaussian Markov Random Fields in 3D Environments\",\"515\":\"A Scalable Multi-Robot Task Allocation Algorithm\",\"516\":\"Distributed Intermittent Communication Control of Mobile Robot Networks Under Time-Critical Dynamic Tasks\",\"517\":\"Landmark-based Exploration with Swarm of Resource Constrained Robots\",\"518\":\"Coverage Control for Wire-Traversing Robots\",\"519\":\"Control of Multiple Passive-Follower Type Robots Based on Feasible Braking Control Region Analysis\",\"520\":\"Voronoi-Based Coverage Control of Pan\\/Tilt\\/Zoom Camera Networks\",\"521\":\"Shaping in Practice: Training Wheels to Learn Fast Hopping Directly in Hardware\",\"522\":\"Speeding Up Incremental Learning Using Data Efficient Guided Exploration\",\"523\":\"Eager and Memory-Based Non-Parametric Stochastic Search Methods for Learning Control\",\"524\":\"Data-driven Construction of Symbolic Process Models for Reinforcement Learning\",\"525\":\"PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-Based Planning\",\"526\":\"Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics\",\"527\":\"Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation\",\"528\":\"Direct Line Guidance Odometry\",\"529\":\"Direct Visual SLAM Using Sparse Depth for Camera-LiDAR System\",\"530\":\"Bayesian Scale Estimation for Monocular SLAM Based on Generic Object Detection for Correcting Scale Drift\",\"531\":\"Efficient Active SLAM Based on Submap Joining, Graph Topology and Convex Optimization\",\"532\":\"2D SLAM Correction Prediction in Large Scale Urban Environments\",\"533\":\"Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition\",\"534\":\"Online Initialization and Automatic Camera-IMU Extrinsic Calibration for Monocular Visual-Inertial SLAM\",\"535\":\"Sonar Visual Inertial SLAM of Underwater Structures\",\"536\":\"Differential Flatness Transformations for Aggressive Quadrotor Flight\",\"537\":\"Autonomous Control of the Interacting-BoomCopter UAV for Remote Sensor Mounting\",\"538\":\"Model Predictive Control of a Multi-Rotor with a Suspended Load for Avoiding Obstacles\",\"539\":\"Asymmetric Collaborative Bar Stabilization Tethered to Two Heterogeneous Aerial Vehicles\",\"540\":\"Innovative Bio-Impedance Sensor Towards Puncture Detection in Eye Surgery for Retinal Vein Occlusion Treatment\",\"541\":\"Distal End Force Sensing with Optical Fiber Bragg Gratings for Tendon-Sheath Mechanisms in Flexible Endoscopic Robots\",\"542\":\"Trajectory-Optimized Sensing for Active Search of Tissue Abnormalities in Robotic Surgery\",\"543\":\"Active Constraints Using Vector Field Inequalities for Surgical Robots\",\"544\":\"A Method for Online Optimization of Lower Limb Assistive Devices with High Dimensional Parameter Spaces\",\"545\":\"Endo-VMFuseNet: A Deep Visual-Magnetic Sensor Fusion Approach for Endoscopic Capsule Robots\",\"546\":\"EndoSensorFusion: Particle Filtering-Based Multi-Sensory Data Fusion with Switching State-Space Model for Endoscopic Capsule Robots\",\"547\":\"Force Control of Series Elastic Actuators-Driven Parallel Robot\",\"548\":\"Analyzing and Improving Cartesian Stiffness Control Stability of Series Elastic Tendon-Driven Robotic Hands\",\"549\":\"A Projected Inverse Dynamics Approach for Multi-Arm Cartesian Impedance Control\",\"550\":\"Whole-Body Sensory Concept for Compliant Mobile Robots\",\"551\":\"Robust, Compliant Assembly via Optimal Belief Space Planning\",\"552\":\"Cooperative Manipulation and Identification of a 2-DOF Articulated Object by a Dual-Arm Robot\",\"553\":\"A Soft Pneumatic Fabric-Polymer Actuator for Wearable Biomedical Devices: Proof of Concept for Lymphedema Treatment\",\"554\":\"Force Control of Textile-Based Soft Wearable Robots for Mechanotherapy\",\"555\":\"HapWRAP: Soft Growing Wearable Haptic Device\",\"556\":\"Autonomous and Portable Soft Exosuit for Hip Extension Assistance with Online Walking and Running Detection Algorithm\",\"557\":\"Design and Analysis of a Wearable Robotic Forearm\",\"558\":\"Real-Time Learning of Efficient Lift Generation on a Dynamically Scaled Flapping Wing Using Policy Search\",\"559\":\"Exploration and Inspection with Vine-Inspired Continuum Robots\",\"560\":\"The Role of Massive Morphing Wings for Maneuvering a Bio-Inspired Bat-Like Robot\",\"561\":\"Stability and Predictability in Dynamically Complex Physical Interactions\",\"562\":\"First Autonomous Multi-Room Exploration with an Insect-Inspired Flapping Wing Vehicle\",\"563\":\"Evaluating Robust Trajectory Control of a Miniature Rolling and Spinning Robot in Outdoor Conditions\",\"564\":\"Bio-Inspired Tensegrity Flexural Joints\",\"565\":\"Grasp Quality Evaluation with Whole Arm Kinematic Noise Propagation\",\"566\":\"Realtime Planning for High-DOF Deformable Bodies Using Two-Stage Learning\",\"567\":\"Grasping Flat Objects by Exploiting Non-Convexity of the Object and Support Surface\",\"568\":\"Caging Loops in Shape Embedding Space: Theory and Computation\",\"569\":\"Dex-Net 3.0: Computing Robust Vacuum Suction Grasp Targets in Point Clouds Using a New Analytic Model and Deep Learning\",\"570\":\"Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation\",\"571\":\"Accelerated Testing and Evaluation of Autonomous Vehicles via Imitation Learning\",\"572\":\"Feature-Based Transfer Learning for Robotic Push Manipulation\",\"573\":\"Inducing Probabilistic Context-Free Grammars for the Sequencing of Movement Primitives\",\"574\":\"Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations\",\"575\":\"Generalized Task-Parameterized Skill Learning\",\"576\":\"Teaching Human Teachers to Teach Robot Learners\",\"577\":\"Sensor-Based Reactive Symbolic Planning in Partially Known Environments\",\"578\":\"Near-optimal Irrevocable Sample Selection for Periodic Data Streams with Applications to Marine Robotics\",\"579\":\"Autonomous Feature Tracing and Adaptive Sampling in Real-World Underwater Environments\",\"580\":\"Navigating Congested Environments with Risk Level Sets\",\"581\":\"Algorithms for Routing of Unmanned Aerial Vehicles with Mobile Recharging Stations\",\"582\":\"Topological Multi-Robot Belief Space Planning in Unknown Environments\",\"583\":\"Efficient Stabilization of Zero-Slope Walking for Bipedal Robots Following Their Passive Fixed-Point Trajectories\",\"584\":\"Straight-Leg Walking Through Underconstrained Whole-Body Control\",\"585\":\"Agile and Adaptive Hopping Height Control for a Pneumatic Robot\",\"586\":\"Robust Rough-Terrain Locomotion with a Quadrupedal Robot\",\"587\":\"Central Pattern Generator With Inertial Feedback for Stable Locomotion and Climbing in Unstructured Terrain\",\"588\":\"On Time Optimization of Centroidal Momentum Dynamics\",\"589\":\"Toward Intuitive Teleoperation in Surgery: Human-Centric Evaluation of Teleoperation Algorithms for Robotic Needle Steering\",\"590\":\"Human-guided Optical Manipulation of Multiple Microscopic Objects\",\"591\":\"Enhanced Tele-interaction in Unknown Environments Using Semi-Autonomous Motion and Impedance Regulation Principles\",\"592\":\"Intuitive Hand Teleoperation by Novice Operators Using a Continuous Teleoperation Subspace\",\"593\":\"Avoiding Human-Robot Collisions Using Haptic Communication\",\"594\":\"High Speed Whole Body Dynamic Motion Experiment with Real Time Master-Slave Humanoid Robot System\",\"595\":\"Deep Trail-Following Robotic Guide Dog in Pedestrian Environments for People who are Blind and Visually Impaired - Learning from Virtual and Real Worlds\",\"596\":\"MergeNet: A Deep Net Architecture for Small Obstacle Discovery\",\"597\":\"Deep Encoder-Decoder Networks for Mapping Raw Images to Dynamic Movement Primitives\",\"598\":\"What is (Missing or Wrong) in the Scene? A Hybrid Deep Boltzmann Machine for Contextualized Scene Modeling\",\"599\":\"Scene Recognition and Object Detection in a Unified Convolutional Neural Network on a Mobile Manipulator\",\"600\":\"AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection\",\"601\":\"ContextualNet: Exploiting Contextual Information Using LSTMs to Improve Image-Based Localization\",\"602\":\"Learning Human-Aware Path Planning with Fully Convolutional Networks\",\"603\":\"Pedestrian Prediction by Planning Using Deep Neural Networks\",\"604\":\"Anticipation in Human-Robot Cooperation: A Recurrent Neural Network Approach for Multiple Action Sequences Prediction\",\"605\":\"Text2Action: Generative Adversarial Synthesis from Language to Action\",\"606\":\"A Data-driven Model for Interaction-Aware Pedestrian Motion Prediction in Object Cluttered Environments\",\"607\":\"Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data\",\"608\":\"Functional Object-Oriented Network: Construction & Expansion\",\"609\":\"3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data\",\"610\":\"Conditional Compatibility Branch and Bound for Feature Cloud Matching\",\"611\":\"Assigning Visual Words to Places for Loop Closure Detection\",\"612\":\"Dijkstra Model for Stereo-Vision Based Road Detection: A Non-Parametric Method\",\"613\":\"Asynchronous Multi-Sensor Fusion for 3D Mapping and Localization\",\"614\":\"Localization Under Topological Uncertainty for Lane Identification of Autonomous Vehicles\",\"615\":\"Stabilizing Traffic with Autonomous Vehicles\",\"616\":\"Data-Driven Model Predictive Control of Autonomous Mobility-on-Demand Systems\",\"617\":\"VALUE: Large Scale Voting-Based Automatic Labelling for Urban Environments\",\"618\":\"Automated Process for Incorporating Drivable Path into Real-Time Semantic Segmentation\",\"619\":\"Precise Ego-Motion Estimation with Millimeter-Wave Radar Under Diverse and Challenging Conditions\",\"620\":\"SeDAR - Semantic Detection and Ranging: Humans can Localise without LiDAR, can Robots?\",\"621\":\"Design of a Novel 3-DoF Leg with Series and Parallel Compliant Actuation for Energy Efficient Articulated Robots\",\"622\":\"Design of a Serial-Parallel Hybrid Leg for a Humanoid Robot\",\"623\":\"Self-Engaging Spined Gripper with Dynamic Penetration and Release for Steep Jumps\",\"624\":\"Design, Modeling, and Analysis of Inductive Resonant Coupling Wireless Power Transfer for Micro Aerial Vehicles (MAVs)\",\"625\":\"Simplified Quasi-Steady Aeromechanic Model for Flapping-Wing Robots with Passively Rotating Hinges\",\"626\":\"Surface Edge Explorer (see): Planning Next Best Views Directly from 3D Observations\",\"627\":\"Active Image-Based Modeling with a Toy Drone\",\"628\":\"Fusing Object Context to Detect Functional Area for Cognitive Robots\",\"629\":\"When Regression Meets Manifold Learning for Object Recognition and Pose Estimation\",\"630\":\"Ultra-Fast Multi-Scale Shape Estimation of Light Transport Matrix for Complex Light Reflection Objects\",\"631\":\"A Deep Learning-Based Stalk Grasping Pipeline\",\"632\":\"Configuration of Perception Systems via Planning Over Factor Graphs\",\"633\":\"Intelligent Shipwreck Search Using Autonomous Underwater Vehicles\",\"634\":\"A Robust Model Predictive Control Approach for Autonomous Underwater Vehicles Operating in a Constrained Workspace\",\"635\":\"Design, Modeling, and Nonlinear Model Predictive Tracking Control of a Novel Autonomous Surface Vehicle\",\"636\":\"Reinforcement Learning of Depth Stabilization with a Micro Diving Agent\",\"637\":\"Real-Time Underwater 3D Reconstruction Using Global Context and Active Labeling\",\"638\":\"Dynamic Reconfiguration of Mission Parameters in Underwater Human-Robot Collaboration\",\"639\":\"Gaussian Process Adaptive Sampling Using the Cross-Entropy Method for Environmental Sensing and Monitoring\",\"640\":\"OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World\",\"641\":\"Composable Deep Reinforcement Learning for Robotic Manipulation\",\"642\":\"Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning\",\"643\":\"Tensegrity Robot Locomotion Under Limited Sensory Inputs via Deep Reinforcement Learning\",\"644\":\"Applying Asynchronous Deep Classification Networks and Gaming Reinforcement Learning-Based Motion Planners to Mobile Robots\",\"645\":\"Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning\",\"646\":\"Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods\",\"647\":\"Overcoming Exploration in Reinforcement Learning with Demonstrations\",\"648\":\"Fast Image-Based Geometric Change Detection Given a 3D Model\",\"649\":\"Multimodal 2D Image to 3D Model Registration via a Mutual Alignment of Sparse and Dense Visual Features\",\"650\":\"An Efficient Volumetric Mesh Representation for Real-Time Scene Reconstruction Using Spatial Hashing\",\"651\":\"Mapping with Dynamic-Object Probabilities Calculated from Single 3D Range Scans\",\"652\":\"A Survey of Voxel Interpolation Methods and an Evaluation of Their Impact on Volumetric Map-Based Visual Odometry\",\"653\":\"Complex Urban LiDAR Data Set\",\"654\":\"Live Structural Modeling Using RGB-D SLAM\",\"655\":\"Adaptive Sampling and Online Learning in Multi-Robot Sensor Coverage with Mixture of Gaussian Processes\",\"656\":\"Coordinated Dense Aerial Traffic with Self-Driving Drones\",\"657\":\"Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous Multi-Agent Systems\",\"658\":\"Data Ferrying with Swarming UAS in Tactical Defence Networks\",\"659\":\"Distance-Based Multi-Robot Coordination on Pocket Drones\",\"660\":\"Human-in-the-Loop Mixed-Initiative Control Under Temporal Tasks\",\"661\":\"Cooperative Adaptive Control for Cloud-Based Robotics\",\"662\":\"Optimized Environment Exploration for Autonomous Underwater Vehicles\",\"663\":\"Pilot Surveys for Adaptive Informative Sampling\",\"664\":\"Gaze-Assisted Adaptive Motion Scaling Optimization Using Graded and Preference Based Bayesian Approaches\",\"665\":\"Learning to Race Through Coordinate Descent Bayesian Optimisation\",\"666\":\"Towards Emergence of Tool Use in Robots: Automatic Tool Recognition and Use Without Prior Tool Learning\",\"667\":\"Put-in-Box Task Generated from Multiple Discrete Tasks by aHumanoid Robot Using Deep Learning\",\"668\":\"CASSL: Curriculum Accelerated Self-Supervised Learning\",\"669\":\"Learning to Control Redundant Musculoskeletal Systems with Neural Networks and SQP: Exploiting Muscle Properties\",\"670\":\"Q-CP: Learning Action Values for Cooperative Planning\",\"671\":\"Long-Term Visual Localization Using Semantically Segmented Images\",\"672\":\"Robust Localization of Mobile Robots Considering Reliability of LiDAR Measurements\",\"673\":\"Sparse Gaussian Processes on Matrix Lie Groups: A Unified Framework for Optimizing Continuous-Time Trajectories\",\"674\":\"Complexity Analysis and Efficient Measurement Selection Primitives for High-Rate Graph SLAM\",\"675\":\"Dense Planar-Inertial SLAM with Structural Constraints\",\"676\":\"Radiation Source Localization in GPS-Denied Environments Using Aerial Robots\",\"677\":\"LineDrone Technology: Landing an Unmanned Aerial Vehicle on a Power Line\",\"678\":\"Direction Controlled Descent of Samara Autorotating Wings (SAW) with N-Wings\",\"679\":\"Pseudo-bearing Measurements for Improved Localization of Radio Sources with Multirotor UAVs\",\"680\":\"Onboard State Dependent LQR for Agile Quadrotors\",\"681\":\"Autonomous Fixed-Wing Aerobatics: From Theory to Flight\",\"682\":\"Adaptive Attitude Control for a Tail-Sitter UAV with Single Thrust-Vectored Propeller\",\"683\":\"Online Aerodynamic Model Identification on Small Fixed-Wing UAVs with Uncertain Flight Data\",\"684\":\"Towards X-Ray Medical Imaging with Robots in the Open: Safety Without Compromising Performances\",\"685\":\"Safety-Enhanced Human-Robot Interaction Control of Redundant Robot for Teleoperated Minimally Invasive Surgery\",\"686\":\"Three-Dimensional Surgical Needle Localization and Tracking Using Stereo Endoscopic Image Streams\",\"687\":\"Robotic Assistance-as-Needed for Enhanced Visuomotor Learning in Surgical Robotics Training: An Experimental Study\",\"688\":\"Semi-Autonomous Laparoscopic Robotic Electro-Surgery with a Novel 3D Endoscope\",\"689\":\"An Ultrasonic Bone Cutting Tool for the da Vinci Research Kit\",\"690\":\"Fast and Reliable Autonomous Surgical Debridement with Cable-Driven Robots Using a Two-Phase Calibration Procedure\",\"691\":\"Distributed Real Time Control of Multiple UAVs in Adversarial Environment: Algorithm and Flight Testing Results\",\"692\":\"Collaborative 6DoF Relative Pose Estimation for Two UAVs with Overlapping Fields of View\",\"693\":\"BFM: a Scalable and Resource-Aware Method for Adaptive Mission Planning of UAVs\",\"694\":\"Simultaneous Optimization of Assignments and Goal Formations for Multiple Robots\",\"695\":\"Machine Learning for Placement-Insensitive Inertial Motion Capture\",\"696\":\"Optimizing Stiffness of a Novel Parallel-Actuated Robotic Shoulder Exoskeleton for a Desired Task or Workspace\",\"697\":\"Adaptive Oscillator-Based Control for Active Lower-Limb Exoskeleton and its Metabolic Impact\",\"698\":\"Human-Exoskeleton System Dynamics Identification Using Affordable Sensors\",\"699\":\"A Locomotion Recognition System Using Depth Images\",\"700\":\"The Effect of Bending Compliance on Adhesion Pressure of Hybrid Electrostatic\\/Gecko-Like Adhesives\",\"701\":\"Design of Frictional 2D-Anisotropy Surface for Wriggle Locomotion of Printable Soft-Bodied Robots\",\"702\":\"Inchworm Locomotion Mechanism Inspired Self-Deformable Capsule-Like Robot: Design, Modeling, and Experimental Validation\",\"703\":\"Realtime On-Board Attitude Estimation of High-Frequency Flapping Wing MAVs Under Large Instantaneous Oscillation\",\"704\":\"FireAnt: A Modular Robot with Full-Body Continuous Docks\",\"705\":\"Perception-Informed Autonomous Environment Augmentation with Modular Robots\",\"706\":\"Design and Online Calibration of a Highly Compact Microgripper\",\"707\":\"Grasping of Unknown Objects Using Deep Convolutional Neural Networks Based on Depth Images\",\"708\":\"Grasp Planning for Load Sharing in Collaborative Manipulation\",\"709\":\"Human-Inspired Object Manipulation Control with the Anatomically Correct Testbed Hand\",\"710\":\"Improving Superquadric Modeling and Grasping with Prior on Object Shapes\",\"711\":\"Active Reward Learning from Critiques\",\"712\":\"Uncertainty-Aware Learning from Demonstration Using Mixture Density Networks with Sampling-Free Variance Modeling\",\"713\":\"Human-Driven Feature Selection for a Robotic Agent Learning Classification Tasks from Demonstration\",\"714\":\"Object-Centric Approach to Prediction and Labeling of Manipulation Tasks\",\"715\":\"Deep Auxiliary Learning for Visual Localization and Odometry\",\"716\":\"An Experimental Investigation of Extra Measurements for Solving the Direct Kinematics of Cable-Driven Parallel Robots\",\"717\":\"Kinematic Optimization of a Novel Partially Decoupled Three Degree of Freedom Hybrid Wrist Mechanism\",\"718\":\"Reconfiguration Analysis and Motion Planning of a Novel Reconfigurable Mobile Manipulator Torso\",\"719\":\"Efficient Event-Driven Forward Kinematics of Open Kinematic Chains with O(Log n) Complexity\",\"720\":\"Reactive Magnetic-Field-Inspired Navigation for Non-Holonomic Mobile Robots in Unknown Environments\",\"721\":\"Aerial Grasping Based on Shape Adaptive Transformation by HALO: Horizontal Plane Transformable Aerial Robot with Closed-Loop Multilinks Structure\",\"722\":\"Towards a Flying Assistant Paradigm: the OTHex\",\"723\":\"Emulating a Fully Actuated Aerial Vehicle Using Two Actuators\",\"724\":\"LASDRA: Large-Size Aerial Skeleton System with Distributed Rotor Actuation\",\"725\":\"A Flying Gripper Based on Cuboid Modular Robots\",\"726\":\"ACT: An Autonomous Drone Cinematography System for Action Scenes\",\"727\":\"Approximate Branch and Bound for Fast, Risk-Bound Stochastic Path Planning\",\"728\":\"Rapidly-Exploring Random Vines (RRV) for Motion Planning in Configuration Spaces with Narrow Passages\",\"729\":\"Generalizing Informed Sampling for Asymptotically-Optimal Sampling-Based Kinodynamic Planning via Markov Chain Monte Carlo\",\"730\":\"Dancing PRM: Simultaneous Planning of Sampling and Optimization with Configuration Free Space Approximation\",\"731\":\"Randomized Kinodynamic Planning for Constrained Systems\",\"732\":\"Learning Sampling Distributions for Robot Motion Planning\",\"733\":\"Deep Object-Centric Representations for Generalizable Robot Learning\",\"734\":\"Real-time 3D Glint Detection in Remote Eye Tracking Based on Bayesian Inference\",\"735\":\"Generative One-Shot Learning (GOL): A Semi-Parametric Approach to One-Shot Learning in Autonomous Vision\",\"736\":\"Adaptive Deep Learning Through Visual Domain Localization\",\"737\":\"GeneSIS-Rt: Generating Synthetic Images for Training Secondary Real-World Tasks\",\"738\":\"Enhancing Underwater Imagery Using Generative Adversarial Networks\",\"739\":\"Faster R-CNN with Classifier Fusion for Small Fruit Detection\",\"740\":\"Robot Button Pressing in Human Environments\",\"741\":\"Enhancing Overall Object Placement by Understanding Uncertain Spatial and Qualitative Distance Information in User Commands\",\"742\":\"Robust Human Following by Deep Bayesian Trajectory Prediction for Home Service Robots\",\"743\":\"Ruling the Control Authority of a Service Robot Based on Information Precision\",\"744\":\"A Nonparametric Motion Flow Model for Human Robot Cooperation\",\"745\":\"Learning by Demonstration and Adaptation of Finishing Operations Using Virtual Mechanism Approach\",\"746\":\"Hybrid Probabilistic Trajectory Optimization Using Null-Space Exploration\",\"747\":\"Feature-constrained Active Visual SLAM for Mobile Robot Navigation\",\"748\":\"Level-Headed: Evaluating Gimbal-Stabilised Visual Teach and Repeat for Improved Localisation Performance\",\"749\":\"Low-Drift Visual Odometry in Structured Environments by Decoupling Rotational and Translational Motion\",\"750\":\"Visual Homing via Guided Locality Preserving Matching\",\"751\":\"LOGOS: Local Geometric Support for High-Outlier Spatial Verification\",\"752\":\"Selection and Compression of Local Binary Features for Remote Visual SLAM\",\"753\":\"UnDeepVO: Monocular Visual Odometry Through Unsupervised Deep Learning\",\"754\":\"Counterexamples for Robotic Planning Explained in Structured Language\",\"755\":\"Multi-Vehicle Motion Planning for Social Optimal Mobility-on-Demand\",\"756\":\"Verifying Controllers Against Adversarial Examples with Bayesian Optimization\",\"757\":\"On the Relationship Between Bisimulation and Combinatorial Filter Reduction\",\"758\":\"Learning-Based Model Predictive Control Under Signal Temporal Logic Specifications\",\"759\":\"Auctioning over Probabilistic Options for Temporal Logic-Based Multi-Robot Cooperation Under Uncertainty\",\"760\":\"Path Clustering with Homology Area\",\"761\":\"GraspMan - A Novel Robotic Platform with Grasping, Manipulation, and Multimodal Locomotion Capability\",\"762\":\"Design and Evaluation of a Novel Cable-Driven Gripper with Perception Capabilities for Strawberry Picking Robots\",\"763\":\"Underactuated Hand Design Using Mechanically Realizable Manifolds\",\"764\":\"Robotic Handling of Liquids with Spilling Avoidance: A Constraint-Based Control Approach\",\"765\":\"A Robust Robot Design for Item Picking\",\"766\":\"Physics-Based Selection of Informative Actions for Interactive Perception\",\"767\":\"Pick and Place Without Geometric Object Models\",\"768\":\"Automatic Material Properties Estimation for the Physics-Based Robotic Garment Folding\",\"769\":\"Slipping Control Algorithms for Object Manipulation with Sensorized Parallel Grippers\",\"770\":\"Semantic Robot Programming for Goal-Directed Manipulation in Cluttered Scenes\",\"771\":\"Off-Road Lidar Simulation with Data-Driven Terrain Primitives\",\"772\":\"Historical Data is Useful for Navigation Planning: Data Driven Route Generation for Autonomous Ship\",\"773\":\"A Preliminary Study of Ice-Relative Underwater Vehicle Navigation Beneath Moving Sea Ice\",\"774\":\"A Soft Robot for Random Exploration of Terrestrial Environments\",\"775\":\"Micro Underwater Vehicle Hydrobatics: A Submerged Furuta Pendulum\",\"776\":\"Satellite-Based Tele-Operation of an Underwater Vehicle-Manipulator System. Preliminary Experimental Results\",\"777\":\"Robust Dense Mapping for Large-Scale Dynamic Environments\",\"778\":\"Self-triggered Adaptive Planning and Scheduling of UAV Operations\",\"779\":\"Cross-Domain Transfer in Reinforcement Learning Using Target Apprentice\",\"780\":\"Intent-Aware Multi-Agent Reinforcement Learning\",\"781\":\"Improving Model-Based Balance Controllers Using Reinforcement Learning and Adaptive Sampling\",\"782\":\"Deep Reinforcement Learning Supervised Autonomous Exploration in Office Environments\",\"783\":\"Bayesian Optimization with Automatic Prior Selection for Data-Efficient Direct Policy Search\",\"784\":\"Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning\",\"785\":\"Adapting Parameterized Motions Using Iterative Learning and Online Collision Detection\",\"786\":\"Trajectory Replanning for Quadrotors Using Kinodynamic Search and Elastic Optimization\",\"787\":\"On Bisection Continuous Collision Checking Method: Spherical Joints and Minimum Distance to Obstacles\",\"788\":\"MPC-based Collision Avoidance Strategy for Existing Marine Vessel Guidance Systems\",\"789\":\"Avoidance of High-Speed Obstacles Based on Velocity Obstacles\",\"790\":\"NanoMap: Fast, Uncertainty-Aware Proximity Queries with Lazy Search Over Local 3D Data\",\"791\":\"A Sensorless Collision Detection Approach Based on Virtual Contact Points\",\"792\":\"Probabilistic Graph Security for Networked Multi-Robot Systems\",\"793\":\"Controlling the Interaction of a Multi-Robot System with External Entities\",\"794\":\"Network Topology Inference in Swarm Robotics\",\"795\":\"Using Hardware Specialization and Hierarchy to Simplify Robotic Swarms\",\"796\":\"From Swarms to Stars: Task Coverage in Robot Swarms with Connectivity Constraints\",\"797\":\"Using Information Invariants to Compare Swarm Algorithms and General Multi-Robot Algorithms\",\"798\":\"Learning Robust Policies for Object Manipulation with Robot Swarms\",\"799\":\"Modeling and Identification of a Single Link Flexible Arm with a Passive Gravity Compensation Mechanism\",\"800\":\"A Nonlinear Control Strategy for Extensible Continuum Robots\",\"801\":\"Continuously Controllable Series Clutches for Efficient Robot Actuation\",\"802\":\"Stiffness Modulator: A Novel Actuator for Human Augmentation\",\"803\":\"Cartman: The Low-Cost Cartesian Manipulator that Won the Amazon Robotics Challenge\",\"804\":\"Slip Detection with Combined Tactile and Visual Information\",\"805\":\"Realtime State Estimation with Tactile and Visual Sensing. Application to Planar Manipulation\",\"806\":\"Touch-Based Grasp Primitives for Soft Hands: Applications to Human-to-Robot Handover Tasks and Beyond\",\"807\":\"Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning\",\"808\":\"U sing a UAV for Destructive Surveys of Mosquito Population\",\"809\":\"Optical Fiber-Based Sensor for Assessing Electric Current in Unmanned Aerial Vehicles with ROS Interface\",\"810\":\"A Lightweight, Compliant, Contact-Resistance-Based Airflow Sensor for Quadcopter Ground Effect Sensing\",\"811\":\"Experiments in Fast, Autonomous, GPS-Denied Quadrotor Flight\",\"812\":\"A Self-contained Teleoperated Quadrotor: On-Board State-Estimation and Indoor Obstacle Avoidance\",\"813\":\"Safe Teleoperation of Dynamic UAVs Through Control Barrier Functions\"},\"First and Last Author Affiliations\":{\"0\":null,\"1\":null,\"2\":null,\"3\":null,\"4\":null,\"5\":null,\"6\":null,\"7\":null,\"8\":null,\"9\":null,\"10\":null,\"11\":null,\"12\":null,\"13\":null,\"14\":null,\"15\":null,\"16\":null,\"17\":null,\"18\":null,\"19\":null,\"20\":null,\"21\":null,\"22\":null,\"23\":null,\"24\":null,\"25\":null,\"26\":null,\"27\":null,\"28\":null,\"29\":null,\"30\":null,\"31\":null,\"32\":null,\"33\":null,\"34\":null,\"35\":null,\"36\":null,\"37\":null,\"38\":null,\"39\":null,\"40\":null,\"41\":null,\"42\":null,\"43\":null,\"44\":null,\"45\":null,\"46\":null,\"47\":null,\"48\":null,\"49\":null,\"50\":null,\"51\":null,\"52\":null,\"53\":null,\"54\":null,\"55\":null,\"56\":null,\"57\":null,\"58\":null,\"59\":null,\"60\":null,\"61\":null,\"62\":null,\"63\":null,\"64\":null,\"65\":null,\"66\":null,\"67\":null,\"68\":null,\"69\":null,\"70\":null,\"71\":null,\"72\":null,\"73\":null,\"74\":null,\"75\":null,\"76\":null,\"77\":null,\"78\":null,\"79\":null,\"80\":null,\"81\":null,\"82\":null,\"83\":null,\"84\":null,\"85\":null,\"86\":null,\"87\":null,\"88\":null,\"89\":null,\"90\":null,\"91\":null,\"92\":null,\"93\":null,\"94\":null,\"95\":null,\"96\":null,\"97\":null,\"98\":null,\"99\":null,\"100\":null,\"101\":null,\"102\":null,\"103\":null,\"104\":null,\"105\":null,\"106\":null,\"107\":null,\"108\":null,\"109\":null,\"110\":null,\"111\":null,\"112\":null,\"113\":null,\"114\":null,\"115\":null,\"116\":null,\"117\":null,\"118\":null,\"119\":null,\"120\":null,\"121\":null,\"122\":null,\"123\":null,\"124\":null,\"125\":null,\"126\":null,\"127\":null,\"128\":null,\"129\":null,\"130\":null,\"131\":null,\"132\":null,\"133\":null,\"134\":null,\"135\":null,\"136\":null,\"137\":null,\"138\":null,\"139\":null,\"140\":null,\"141\":null,\"142\":null,\"143\":null,\"144\":null,\"145\":null,\"146\":null,\"147\":null,\"148\":null,\"149\":null,\"150\":null,\"151\":null,\"152\":null,\"153\":null,\"154\":null,\"155\":null,\"156\":null,\"157\":null,\"158\":null,\"159\":null,\"160\":null,\"161\":null,\"162\":null,\"163\":null,\"164\":null,\"165\":null,\"166\":null,\"167\":null,\"168\":null,\"169\":null,\"170\":null,\"171\":null,\"172\":null,\"173\":null,\"174\":null,\"175\":null,\"176\":null,\"177\":null,\"178\":null,\"179\":null,\"180\":null,\"181\":null,\"182\":null,\"183\":null,\"184\":null,\"185\":null,\"186\":null,\"187\":null,\"188\":null,\"189\":null,\"190\":null,\"191\":null,\"192\":null,\"193\":null,\"194\":null,\"195\":null,\"196\":null,\"197\":null,\"198\":null,\"199\":null,\"200\":null,\"201\":null,\"202\":null,\"203\":null,\"204\":null,\"205\":null,\"206\":null,\"207\":null,\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":null,\"213\":null,\"214\":null,\"215\":null,\"216\":null,\"217\":null,\"218\":null,\"219\":null,\"220\":null,\"221\":null,\"222\":null,\"223\":null,\"224\":null,\"225\":null,\"226\":null,\"227\":null,\"228\":null,\"229\":null,\"230\":null,\"231\":null,\"232\":null,\"233\":null,\"234\":null,\"235\":null,\"236\":null,\"237\":null,\"238\":null,\"239\":null,\"240\":null,\"241\":null,\"242\":null,\"243\":null,\"244\":null,\"245\":null,\"246\":null,\"247\":null,\"248\":null,\"249\":null,\"250\":null,\"251\":null,\"252\":null,\"253\":null,\"254\":null,\"255\":null,\"256\":null,\"257\":null,\"258\":null,\"259\":null,\"260\":null,\"261\":null,\"262\":null,\"263\":null,\"264\":null,\"265\":null,\"266\":null,\"267\":null,\"268\":null,\"269\":null,\"270\":null,\"271\":null,\"272\":null,\"273\":null,\"274\":null,\"275\":null,\"276\":null,\"277\":null,\"278\":null,\"279\":null,\"280\":null,\"281\":null,\"282\":null,\"283\":null,\"284\":null,\"285\":null,\"286\":null,\"287\":null,\"288\":null,\"289\":null,\"290\":null,\"291\":null,\"292\":null,\"293\":null,\"294\":null,\"295\":null,\"296\":null,\"297\":null,\"298\":null,\"299\":null,\"300\":null,\"301\":null,\"302\":null,\"303\":null,\"304\":null,\"305\":null,\"306\":null,\"307\":null,\"308\":null,\"309\":null,\"310\":null,\"311\":null,\"312\":null,\"313\":null,\"314\":null,\"315\":null,\"316\":null,\"317\":null,\"318\":null,\"319\":null,\"320\":null,\"321\":null,\"322\":null,\"323\":null,\"324\":null,\"325\":null,\"326\":null,\"327\":null,\"328\":null,\"329\":null,\"330\":null,\"331\":null,\"332\":null,\"333\":null,\"334\":null,\"335\":null,\"336\":null,\"337\":null,\"338\":null,\"339\":null,\"340\":null,\"341\":null,\"342\":null,\"343\":null,\"344\":null,\"345\":null,\"346\":null,\"347\":null,\"348\":null,\"349\":null,\"350\":null,\"351\":null,\"352\":null,\"353\":null,\"354\":null,\"355\":null,\"356\":null,\"357\":null,\"358\":null,\"359\":null,\"360\":null,\"361\":null,\"362\":null,\"363\":null,\"364\":null,\"365\":null,\"366\":null,\"367\":null,\"368\":null,\"369\":null,\"370\":null,\"371\":null,\"372\":null,\"373\":null,\"374\":null,\"375\":null,\"376\":null,\"377\":null,\"378\":null,\"379\":null,\"380\":null,\"381\":null,\"382\":null,\"383\":null,\"384\":null,\"385\":null,\"386\":null,\"387\":null,\"388\":null,\"389\":null,\"390\":null,\"391\":null,\"392\":null,\"393\":null,\"394\":null,\"395\":null,\"396\":null,\"397\":null,\"398\":null,\"399\":null,\"400\":null,\"401\":null,\"402\":null,\"403\":null,\"404\":null,\"405\":null,\"406\":null,\"407\":null,\"408\":null,\"409\":null,\"410\":null,\"411\":null,\"412\":null,\"413\":null,\"414\":null,\"415\":null,\"416\":null,\"417\":null,\"418\":null,\"419\":null,\"420\":null,\"421\":null,\"422\":null,\"423\":null,\"424\":null,\"425\":null,\"426\":null,\"427\":null,\"428\":null,\"429\":null,\"430\":null,\"431\":null,\"432\":null,\"433\":null,\"434\":null,\"435\":null,\"436\":null,\"437\":null,\"438\":null,\"439\":null,\"440\":null,\"441\":null,\"442\":null,\"443\":null,\"444\":null,\"445\":null,\"446\":null,\"447\":null,\"448\":null,\"449\":null,\"450\":null,\"451\":null,\"452\":null,\"453\":null,\"454\":null,\"455\":null,\"456\":null,\"457\":null,\"458\":null,\"459\":null,\"460\":null,\"461\":null,\"462\":null,\"463\":null,\"464\":null,\"465\":null,\"466\":null,\"467\":null,\"468\":null,\"469\":null,\"470\":null,\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":null,\"476\":null,\"477\":null,\"478\":null,\"479\":null,\"480\":null,\"481\":null,\"482\":null,\"483\":null,\"484\":null,\"485\":null,\"486\":null,\"487\":null,\"488\":null,\"489\":null,\"490\":null,\"491\":null,\"492\":null,\"493\":null,\"494\":null,\"495\":null,\"496\":null,\"497\":null,\"498\":null,\"499\":null,\"500\":null,\"501\":null,\"502\":null,\"503\":null,\"504\":null,\"505\":null,\"506\":null,\"507\":null,\"508\":null,\"509\":null,\"510\":null,\"511\":null,\"512\":null,\"513\":null,\"514\":null,\"515\":null,\"516\":null,\"517\":null,\"518\":null,\"519\":null,\"520\":null,\"521\":null,\"522\":null,\"523\":null,\"524\":null,\"525\":null,\"526\":null,\"527\":null,\"528\":null,\"529\":null,\"530\":null,\"531\":null,\"532\":null,\"533\":null,\"534\":null,\"535\":null,\"536\":null,\"537\":null,\"538\":null,\"539\":null,\"540\":null,\"541\":null,\"542\":null,\"543\":null,\"544\":null,\"545\":null,\"546\":null,\"547\":null,\"548\":null,\"549\":null,\"550\":null,\"551\":null,\"552\":null,\"553\":null,\"554\":null,\"555\":null,\"556\":null,\"557\":null,\"558\":null,\"559\":null,\"560\":null,\"561\":null,\"562\":null,\"563\":null,\"564\":null,\"565\":null,\"566\":null,\"567\":null,\"568\":null,\"569\":null,\"570\":null,\"571\":null,\"572\":null,\"573\":null,\"574\":null,\"575\":null,\"576\":null,\"577\":null,\"578\":null,\"579\":null,\"580\":null,\"581\":null,\"582\":null,\"583\":null,\"584\":null,\"585\":null,\"586\":null,\"587\":null,\"588\":null,\"589\":null,\"590\":null,\"591\":null,\"592\":null,\"593\":null,\"594\":null,\"595\":null,\"596\":null,\"597\":null,\"598\":null,\"599\":null,\"600\":null,\"601\":null,\"602\":null,\"603\":null,\"604\":null,\"605\":null,\"606\":null,\"607\":null,\"608\":null,\"609\":null,\"610\":null,\"611\":null,\"612\":null,\"613\":null,\"614\":null,\"615\":null,\"616\":null,\"617\":null,\"618\":null,\"619\":null,\"620\":null,\"621\":null,\"622\":null,\"623\":null,\"624\":null,\"625\":null,\"626\":null,\"627\":null,\"628\":null,\"629\":null,\"630\":null,\"631\":null,\"632\":null,\"633\":null,\"634\":null,\"635\":null,\"636\":null,\"637\":null,\"638\":null,\"639\":null,\"640\":null,\"641\":null,\"642\":null,\"643\":null,\"644\":null,\"645\":null,\"646\":null,\"647\":null,\"648\":null,\"649\":null,\"650\":null,\"651\":null,\"652\":null,\"653\":null,\"654\":null,\"655\":null,\"656\":null,\"657\":null,\"658\":null,\"659\":null,\"660\":null,\"661\":null,\"662\":null,\"663\":null,\"664\":null,\"665\":null,\"666\":null,\"667\":null,\"668\":null,\"669\":null,\"670\":null,\"671\":null,\"672\":null,\"673\":null,\"674\":null,\"675\":null,\"676\":null,\"677\":null,\"678\":null,\"679\":null,\"680\":null,\"681\":null,\"682\":null,\"683\":null,\"684\":null,\"685\":null,\"686\":null,\"687\":null,\"688\":null,\"689\":null,\"690\":null,\"691\":null,\"692\":null,\"693\":null,\"694\":null,\"695\":null,\"696\":null,\"697\":null,\"698\":null,\"699\":null,\"700\":null,\"701\":null,\"702\":null,\"703\":null,\"704\":null,\"705\":null,\"706\":null,\"707\":null,\"708\":null,\"709\":null,\"710\":null,\"711\":null,\"712\":null,\"713\":null,\"714\":null,\"715\":null,\"716\":null,\"717\":null,\"718\":null,\"719\":null,\"720\":null,\"721\":null,\"722\":null,\"723\":null,\"724\":null,\"725\":null,\"726\":null,\"727\":null,\"728\":null,\"729\":null,\"730\":null,\"731\":null,\"732\":null,\"733\":null,\"734\":null,\"735\":null,\"736\":null,\"737\":null,\"738\":null,\"739\":null,\"740\":null,\"741\":null,\"742\":null,\"743\":null,\"744\":null,\"745\":null,\"746\":null,\"747\":null,\"748\":null,\"749\":null,\"750\":null,\"751\":null,\"752\":null,\"753\":null,\"754\":null,\"755\":null,\"756\":null,\"757\":null,\"758\":null,\"759\":null,\"760\":null,\"761\":null,\"762\":null,\"763\":null,\"764\":null,\"765\":null,\"766\":null,\"767\":null,\"768\":null,\"769\":null,\"770\":null,\"771\":null,\"772\":null,\"773\":null,\"774\":null,\"775\":null,\"776\":null,\"777\":null,\"778\":null,\"779\":null,\"780\":null,\"781\":null,\"782\":null,\"783\":null,\"784\":null,\"785\":null,\"786\":null,\"787\":null,\"788\":null,\"789\":null,\"790\":null,\"791\":null,\"792\":null,\"793\":null,\"794\":null,\"795\":null,\"796\":null,\"797\":null,\"798\":null,\"799\":null,\"800\":null,\"801\":null,\"802\":null,\"803\":null,\"804\":null,\"805\":null,\"806\":null,\"807\":null,\"808\":null,\"809\":null,\"810\":null,\"811\":null,\"812\":null,\"813\":null},\"Keywords or Approach\":{\"0\":\"neurosurgery\\ncatheters\\nthree-dimensional displays\\nkinematics\\nneedles\\nplanning\\nuncertainty\\ndrugs\\nmedical robotics\\npath planning\\nsurgery\\nrrt-connect\\nsample-based algorithms\\nobstacle occupancy\\ninsertion procedure\\ncatheter modeling\\nasymptotically-optimal solution\\nbit* algorithm\\nsample-based heuristic search\\ndrug delivery\\nmultisegment steerable probe\\nprogrammable bevel-tip needle\\neden2020\\nneurosurgeon\\nminimally invasive neurosurgery\\nautomatic planner\\nsteerable catheters\",\"1\":\"manipulators\\nwires\\ninstruments\\nelectron tubes\\nsurgery\\nkinematics\\ndexterous manipulators\\ndiseases\\nlung\\nmanipulator kinematics\\nmedical robotics\\npatient treatment\\nkinematics characterization\\nlaser-profiled continuum manipulator\\nbronchoscopic instruments\\nbronchoscopic intervention\\nminimally invasive method\\nlung diseases\\nendobronchial instruments\\nperipheral airways\\nprecision laser profiling\\ncommercial bronchoscopes\\ndistal airways\\nkinematic models\\nmanipulator configuration\\nactuation wires\\nmanipulator joints\\ninstrument guidance robot\\nwire-driven dexterous manipulator\",\"2\":\"tendons\\nelectron tubes\\narteries\\ncatheters\\nrobot kinematics\\nkinematics\\nactuators\\nblood vessels\\ndiseases\\nmedical robotics\\nsurgery\\ntwo-degree-of-freedom robotic guidewire\\njoints laser micromachining\\nnitinol tube\\ntendon force\\nshape sensing mechanism\\nperipheral arterial disease\\nsize 0.78 mm\",\"3\":\"electron tubes\\npropulsion\\nmuscles\\nactuators\\ncatheters\\nprototypes\\nmuscle\\npneumatic actuators\\ncatheter capable\\nbranch pipe\\nmckibben chambers\\ninner tube equips\\ntravelling waves\\nmckibben artificial muscles\\nbronchi\\nmoving steering\\nmono-line drive\\nsteering function\",\"4\":\"catheters\\nneedles\\narteries\\nsurgery\\nprototypes\\nyarn\\nshafts\\nblood vessels\\ncardiovascular system\\ndiseases\\nphantoms\\nstents\\nelectromagnetic connector\\nphantom vessel\\nsilicone material\\nsingle-sided suturing catheter\\nopen surgery\\nabdominal aortic aneurysm treatment\\nendovascular aneurysm repair\\nminimally invasive vascular surgery\\nmodular suturing catheter\\nsuturing module\\nabdominal aorta\\nstent graft migration\\ngeneral anesthesia\",\"5\":\"catheters\\nfiber gratings\\noptical sensors\\nshape\\nmulticore processing\\nbiomedical ultrasonics\\nendoscopes\\nfeedforward neural nets\\nimage fusion\\nkalman filters\\nmedical image processing\\nmedical robotics\\nobservers\\nstate estimation\\nsurgery\\nmulticore optical shape sensors\\nultrasound images\\nmagnetically-actuated catheters\\nminimally invasive surgery\\nflexible medical instruments\\nmagnetically actuated catheters\\nsteering precision\\nconventional catheters\\nactuation method\\naccurate tip position\\nprecise control\\nrobust sensor fusion algorithm\\ntemplate-based tracker\\nconvolutional neural network based tracker\\nobserver-based fusion\\neuclidean error\\nluenberger observer\\nkalman filter\",\"6\":\"acoustics\\nray tracing\\nmicrophone arrays\\nthree-dimensional displays\\nrobots\\nindoor environments\\nacoustic signal processing\\narray signal processing\\nmobile robots\\nmonte carlo methods\\ndirect acoustic paths\\ndirect sound signal\\n3d sound localization\\nsingle frame\\nreflected acoustic paths\\n3d sound source position\\nnonline-of-sight sound source\\nmobile sound source\\nlocalization accuracy\\nmobile source\\nintermittent sound signals\\ncube-shaped microphone array\\nmonte carlo localization\\ninverse acoustic ray tracing\\nindirect sound signals\\nstationary source\\ncontinuous sound signals\\nreflection-aware method\\nreflection-aware sound source localization\\ndirect acoustic rays\\ntime 3.0 d\\nsize 0.8 m\\nsize 7.0 m\\nsize 3.0 m\",\"7\":\"encoding\\ndelays\\nrobots\\nartificial neural networks\\nmicrophones\\nestimation\\nacoustic generators\\nhuman-robot interaction\\nmicrophone arrays\\nneural nets\\nspeaker recognition\\ndeep neural networks\\nmultiple speaker detection\\nsimultaneous detection\\nmultiple sound sources\\nneural network-based sound source localization methods\\nsingle sound source\\nlikelihood-based encoding\\nnetwork output\\nsound mixtures\\nspatial spectrum-based approaches\",\"8\":\"acoustics\\nrobot sensing systems\\nsensor arrays\\nsignal to noise ratio\\narray signal processing\\npropellers\\nacoustic signal processing\\nautonomous aerial vehicles\\ndroneears\\nrobust acoustic source localization\\naerial drones\\nmicroaerial vehicles\\nhigh value mobile sensing assets\\nexternal sensing scene\\nacoustic clues\\nmav auditory system\\nrobust acoustic localization system\\nmav propeller units\\nbinaural sensing system\\ngeo-locating sound sources\\nsparse sensor array design\\nplatform constraints\\nsevere ego-noise\\nreceived signal-to-noise ratio\\nsource localization accuracy\\nphysical space-of-interest\\nmobility-aided beamforming\",\"9\":\"coal mining\\nultra wideband radar\\nrobustness\\ndrones\\ncoal\\nsensors\\ndust\\ninspection\\nmining\\nmining industry\\npersonnel\\nsafety\\nwalls\\nultra-wideband radar\\nrobust inspection drone\\nunderground coal mine\\nhuman workers\\nautonomous inspection drone\\ncoal dust\\nrobust sensing solution\\nsafety risk\",\"10\":\"feature extraction\\nrobot sensing systems\\nvibrations\\nmonitoring\\naccelerometers\\ndatabases\\nthree-dimensional displays\\ncondition monitoring\\nfailure analysis\\nneural nets\\nproduction engineering computing\\nproductivity\\nsensors\\nsupport vector machines\\ncombine industrial equipment failure\\ninertial machine monitoring system\\nmanufacturing productivity\\n3d printer\\nneural networks\\nsmart manufacturing technologies\\nautomated failure detection\\ninternet-of-things sensors\",\"11\":\"transmitters\\nrobustness\\nsimultaneous localization and mapping\\nmagnetic resonance imaging\\ndistortion\\ntrajectory\\nglobal positioning system\\nsensor placement\\nslam (robots)\\nwireless sensor networks\\ninertial magneto-inductive localisation\\nshort-term construction work\\nimag\\nrobust simultaneous localisation\\ninertial measurement units\\nmagneto-inductive device\\ninertial measurements\\nlocalisation\\nslam\",\"12\":\"grippers\\nmagnetic separation\\nbarium\\nmagnetic hysteresis\\nmagnetoelasticity\\nmicromagnetics\\ntask analysis\\nend effectors\\nfreight handling\\nindustrial robots\\nmicromanipulators\\nmicrorobots\\nmobile robots\\nposition control\\nindependent untethered mobile magnetic microgrippers\\nparallel targeted cargo delivery\\ntwo-microgripper pair\\nlocal magnetic interactions\\nglobal magnetic field\\nparallel pick and place\\n3d microgrippers configuration\\nmagnetic microgripper\\nmulti-agent control at microscales\\nsoft robotics\\ntargeted cargo delivery\",\"13\":\"retina\\nrobot sensing systems\\nforce\\nneedles\\ninstruments\\nveins\\nbiomedical optical imaging\\nblood vessels\\nbragg gratings\\ncalibration\\ncoagulation\\ndistance measurement\\neye\\nfibre optic sensors\\nforce sensors\\nmanipulators\\nmedical disorders\\nmedical robotics\\noptical tomography\\nsurgery\\nvision defects\\nreal-time distance estimation algorithm\\ncalibration method\\nmanufacturing process\\nfiber bragg grating\\ndepth estimation\\nanticoagulant\\nrobot-assisted procedure\\nretinal vascular disorder\\nfbg force\\noptical coherence tomography a-scan technology\\ndistance sensing cannulation needle\\ninstrument-tissue interaction forces\\nretinal vein occlusion\\nrobot-assisted retinal vein cannulation\\noct distance sensing needle\",\"14\":\"surgery\\nmanipulator dynamics\\ntools\\nmathematical model\\ndynamics\\nmanipulator kinematics\\nmedical robotics\\nmotion control\\nend-to-end dynamic modeling\\nrobotic tool\\nvitreoretinal surgery\\nsub-optimal motor selection\\nmicroprecise surgery\\nsurgical tool\\n3-link surgical manipulator\\nanti-backlash lead screw assembly\\nmulti-degree of freedom robotic system\\ndynamics analysis\\nrigorous kinematics analysis\\neuler-lagrange equations of motion\",\"15\":\"instruments\\nendoscopes\\nelectrostatic discharges\\nsurgery\\ntendons\\nrobots\\ncancer\\nbiomedical optical imaging\\nlaser applications in medicine\\nmedical robotics\\ntumours\\nendoscopic submucosal dissection\\nrobotic surgical system\\ntherapeutic endoscopy technique\\ngi surgery\\ngi surgeon\\nbimanual surgical robotic attachment\\nesd cyclops system\\nsurgical systems\\ngastrointestinal cancers\",\"16\":\"electrodes\\nelectrical stimulation\\nmuscles\\nrobots\\nelectric fields\\nbiological materials\\nforce\\nbiomechanics\\ncellular biophysics\\nelectromechanical actuators\\nmedical robotics\\nmicrorobots\\nmuscle\\nphysiological models\\ntissue engineering\\nultrasonic therapy\\nmyotubes\\nbio-syncretic robot\\ncircularly distributed multiple electrodes\\nc2c12 myoblasts\\nelectro-responsive beating behavior\\nbiomedical field\\nc2c12 differentiation\\nmuscle tissue engineering\",\"17\":\"shape\\nplanning\\nmanipulators\\nrobot motion\\nthree-dimensional displays\\ntask analysis\\npath planning\\ndeformable object manipulation\\nstring untying planning method\\nmotion generation\\noptimal string shape operation\\nknot theory\",\"18\":\"robot sensing systems\\nresistance\\nfabrication\\ngeometry\\nforce sensors\\ncontacts\\ncarbon\\ncarbon fibre reinforced composites\\ncompliant mechanisms\\ndeformation\\nelastomers\\nstrain sensors\\nmultiaxis force sensors\\nsoft force sensors\\ncompliant force sensors\\nmicroscale meanders\\nelastomers layers\\nsensor contact mechanics\\ndifferential measurement\\nlaser-machined carbon fiber composite micro-structures\\nmechanical compliance\\nsoft material robotics\\nwearable robots\",\"19\":\"robots\\npneumatic systems\\nvalves\\npistons\\ndamping\\ntask analysis\\ninspection\\nactuators\\nflow control\\nfluidics\\nunder-actuated control\\nfluidic actuation\\nfluidic systems\",\"20\":\"pistons\\nsynchronous motors\\nelectron tubes\\npneumatic systems\\nforce\\nactuators\\nrobots\\nbiomedical mri\\nelectromagnetic actuators\\nmedical robotics\\nmotion control\\npneumatic actuators\\nstepping motors\\nmr compatible robotics\\nproof-of-concept-prototypes\\nrotational pneumatic stepper motors\\nparametrically-designed mri-compatible pneumatic stepper actuator\\ngeneral purpose nonelectromagnetic actuation\\npneuact\",\"21\":\"aerospace electronics\\nuncertainty\\nkinematics\\nmathematical model\\nmanifolds\\ntask analysis\\ncomplexity theory\\nlarge-scale systems\\nmanipulators\\noptimisation\\npath planning\\nrobot kinematics\\nassembly tasks\\naction uncertainty\\noptimal manipulation\\nleverage environmental contact\\ncomplex kinematics\\naction space\\ncontact manifold\\nproblem complexity\\nnear-optimal compliant manipulation\\nenvironmental contact\\ndiscretization\",\"22\":\"planning\\ngrasping\\ntask analysis\\nmanipulators\\njacobian matrices\\nrobustness\\nmanifolds\\nmanipulator dynamics\\nmotion control\\npath planning\\nposition control\\nredundant manipulators\\nsampling methods\\nsensors\\ntrees (mathematics)\\nconstrained sampling-based planning\\nsampling-based motion\\ntransport tasks\\nredundant robotic manipulator\\nplanning margin\\ngrasp configuration\\napproach direction\\nsensor uncertainty\\nexecution errors\\nsoft constraints\\ncomputational efficiency\\nstudied approaches\\ntarget position\\ngrasp tasks\\noptimal grasp pose\\nrapidly-exploring random tree algorithm\\nrrt algorithm\",\"23\":\"planning\\nrobots\\noptimization\\ntask analysis\\nthumb\\ncollision avoidance\\ndexterous manipulators\\ngait analysis\\nmanipulator kinematics\\nmesh generation\\nmobile robots\\noptimisation\\npath planning\\nfinger gaiting\\nin-grasp manipulation\\ncontact location\\nfinger gaits\\nobject reposing actions\\ngeometric in-hand regrasp planning\\nrobots fingers\\nobjects geometry\\nhands kinematic structure\\nkinematic feasibility\\ncollision free\",\"24\":\"visualization\\ndictionaries\\nfeature extraction\\ntask analysis\\nvisual servoing\\ndeformable models\\nmanipulators\\nmobile robots\\nrobot vision\\ncomplex physical properties\\nautonomous robotic manipulation systems\\nvisual feedback dictionary-based method\\ndeformable objects\\nrgb sensor stream\\ndeformable model features\\nhistogram features\\nhigh-level representations\\ndeformable material\\nmanipulation data\\nrobotic end-effectors\\ncomplex manipulation tasks\\nhuman-robot manipulation tasks\\nmaterial characteristics\\ndeformable materials\",\"25\":\"task analysis\\nforce\\nschedules\\nfriction\\npredictive control\\nmanipulators\\noptimization\\nclosed loop systems\\nlearning (artificial intelligence)\\noptimal control\\noptimisation\\nmodel predictive control formulation\\noptimal sequence\\nrobot motions\\ndesired object motion\\nmultiple contact modes\\nfrictional interactions\\ncombinatorial complexity\\noptimal mode sequences offline\\noptimal control inputs\\nconvex hybrid mpc program\\nplanar manipulation experimental setup\\nconvex hybrid mpc formulation\\nclosed-loop performance\\nreactive planar manipulation\\nreactive controller\\nplanar manipulation tasks\\noptimization program\\nmachine learning\",\"26\":\"planning\\ndynamics\\ngrippers\\nfriction\\nrobots\\ngeometry\\nforce\\ndexterous manipulators\\nmanipulator dynamics\\nmechanical contact\\nmotion control\\npath planning\\nfrictional coefficients\\npushing strategy\\nin-hand manipulation planning\\ndynamics formulation\\nobject grasping\\nalternating sticking contacts\\nprehensile pushing stability\",\"27\":\"planning\\ntask analysis\\nrobots\\ntools\\ncameras\\nvisualization\\ntraining\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\npath planning\\npotential field-based heuristic exploration strategy\\ndeep q-network\\nnonprehensile rearrangement strategy\\nphysical environment\\nphysical world\\nskillful interaction\\ntabletop surface\\nrearranging objects\\ndeep reinforcement learning\\nnonprehensile manipulation\\nquicker learning\\ntraining process\",\"28\":\"payloads\\nrobots\\ncollaboration\\nangular velocity\\nvelocity measurement\\ntask analysis\\nstability analysis\\nadaptive control\\ndecentralised control\\nlyapunov methods\\nmanipulators\\nmulti-robot systems\\nstability\\ncenter-of-mass measurements\\nlocal measurements\\ncollaborative manipulation\\ndecentralized adaptive controller\\ncommon payload\\nagent positions\\npayload properties\\nlyapunov-style analysis\\nconvergence\",\"29\":\"trajectory\\nsensitivity\\ntask analysis\\noptimization\\nuncertainty\\nrobots\\nrobustness\\nautonomous aerial vehicles\\nclosed loop systems\\nmobile robots\\nmonte carlo methods\\noptimisation\\ncontroller dynamics\\nreference trajectory\\nsystem trajectories\\ntrajectory optimization problems\\nclosed-loop sensitivity\\ntrajectory generation\\nminimum closed-loop state sensitivity\\ndynamical system fulfil\\nnominal parameters\\nclosed-loop trajectory\\nsystem\\/controller pair\\ncontrol inputs\\nsystem states\\nrobotic systems\\nmonte carlo simulations\\nunicycle\\nquadrotor uav\",\"30\":\"grasping\\ndynamics\\ntask analysis\\njacobian matrices\\nmanipulator dynamics\\nactuators\\ndexterous manipulators\\nhumanoid robots\\nlegged locomotion\\nmanipulator kinematics\\nmobile robots\\nmotion control\\nobject dynamics compensation\\nanymal\\nfree-floating robot link\\nkuka lwr iv+ representing fingers\\ncontact wrenches control\\nfloating-base multileg robots control\\nvirtual dof\\nenormous robot hand\\nunderactuated robots\\ncontact consistent motion generation\\nprojected inverse dynamics control approach\\nunderactuated system\\nmultiarm robot\\nmodeling approach\\ngrasping scenarios\\nvirtual manipulator\\nmass 9.0 kg\",\"31\":\"task analysis\\nimpedance\\nrobots\\nforce\\ntorque\\noptimization\\nacceleration\\ncollision avoidance\\nhumanoid robots\\nhuman-robot interaction\\nmotion control\\nquadratic programming\\nrobot dynamics\\nservice robots\\ntorque control\\nprioritized cartesian impedance control\\ninverse dynamics\\nmatrix pseudoinversion\\ninverse kinematics computation\\nqp optimization\\nqp implementation\\nclassical cartesian impedance controller\\nhumanoid upper-body torque controlled robot\\nquadratic programming optimization\\ninequality constraints\\nmultipriority cartesian impedance control\\nalgebraic implementation\\njoint torque limits\\nvirtual model control\",\"32\":\"robot kinematics\\ntrajectory\\ncollision avoidance\\ntask analysis\\nservice robots\\nmanipulators\\ncontrol engineering computing\\nindustrial robots\\nmobile robots\\nmotion control\\nmulti-robot systems\\npath planning\\ndual-arm robot coordination\\ntemporal coordination\\nspatial coordination\\nshared workspace\\nindustrial service-oriented robotics\\nuser experience\\nexecution performance\\nindependently planned motions\\ndual-arm manipulator\\nmotion commands\\nrobot motion\\nabb yumi robot\",\"33\":\"force\\ntorque\\nthree-dimensional displays\\nrobot sensing systems\\nmanipulator dynamics\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nmultilayer perceptrons\\noptimisation\\nposition control\\ncontact point localization\\narticulated manipulators\\nproprioceptive sensors\\nmachine learning\\njoint positions\\none-dimensional joint torques\\nrobot arm\\nrfs\\ncontact link\\ncontact points\\nkinova jaco 2 manipulator\\noptimization based approach\\nml approach\\nserial manipulator\\nrandom forests\\nmlp\",\"34\":\"torque\\nmanipulator dynamics\\ncomputational modeling\\nuncertainty\\nrobustness\\ncontinuous time systems\\nlinear systems\\nmanipulators\\nrobust control\\nstability\\ntime-varying systems\\ntorque control\\nuncertain systems\\nrobot manipulator\\nl1 robustness\\nl1 robust stability condition\\nperformance measure\\ninduced norm bounded model uncertainty\\ncontinuous-time linear time-invariant nominal plant\\nmultiplicative model uncertainty\\nexogenous disturbance\\nmodelling errors\\ncomputed torque controller\\nmodel uncertainties\\ncomputed torque method\",\"35\":\"resource management\\nplanning\\ntrajectory optimization\\nsafety\\nautonomous robots\\nautonomous aerial vehicles\\nclutter\\ncomputational geometry\\nconvex programming\\nhelicopters\\nindoor environment\\nmobile robots\\npath planning\\npolynomials\\nsearch problems\\nstate estimation\\ntrajectory control\\nonline safe trajectory generation\\nbernstein basis polynomial\\nonboard state estimation\\nvelocity field\\neuclidean signed distance field\\ntime allocation\\nflight corridor\\npiecewise b\\u00e9zier curves\\noutdoor environments\\nconvex programs\\nautonomous navigation\\nmarching-based path searching method\\nlight-weight quadrotor platform\\nonline quadrotor motion planning\\ncluttered indoor environments\\nesdf\\nopen-source package\",\"36\":\"robots\\ncharging stations\\napproximation algorithms\\npath planning\\nbatteries\\npartitioning algorithms\\nenergy consumption\\napproximation theory\\ngeometry\\nmobile robots\\nenergy constraint\\ncoverage path planning problem\\nbattery limitations\\nworking environment\\ngeometric version\\npolygonal grid\\nsingle charging station\\nconstant-factor approximation algorithm\\ncontour-connected environments\\naerial robot\\nmobile robot systems\",\"37\":\"three-dimensional displays\\ntrajectory\\natmospheric modeling\\nairplanes\\nsolid modeling\\ntwo dimensional displays\\noptimization\",\"38\":\"kinematics\\nmobile robots\\ntrajectory\\nautomobiles\\nmanipulators\\nmathematical model\\nforce control\\njacobian matrices\\noptimal control\\npath planning\\nrobot kinematics\\ntorque control\\ndubins car\\nlagrange multipliers\\nexternal force\\nequal torques\\narm jacobian yields\\noptimal paths\\narm-like mobile robots\\nrobots arm kinematics\\ngeometric interpretations\\nrotation center locations\",\"39\":\"planning\\ntrajectory\\nsafety\\nreal-time systems\\nrobustness\\nnavigation\\ncomputational modeling\\nautonomous aerial vehicles\\ncollision avoidance\\nmobile robots\\nmulti-robot systems\\npath planning\\ntrajectory control\\nmotion planning\\nrobotics community\\nfastrack\\nsensor measurements\\nmeta-planning notion\\ncrazyflie 2.0 quadrotor\\nadaptive realtime safe trajectory planning\\nsafety guarantee\\nonline planner\\noffline computation\\nmotion plans\\nmodular safety guarantee\",\"40\":\"legged locomotion\\nfoot\\nacceleration\\ntrajectory tracking\\nhumanoid robots\\nrobot kinematics\\ncollision avoidance\\nmotion control\\nrobot dynamics\\nstability\\ntorque control\\nwalking algorithm\\nwhole-body control algorithm\\noperational space control framework\\ncompliant task behavior\\nrobust walking\\nunexpected obstacle\\nreactive bipedal walking method\\ntorque controlled robot\\nunexpected situations\\nreactive biped robot walking method\\ntime plan\\ntrajectory tracking control\\nreactive behavior\\nunexpected contact\\n12-dof torque controlled biped robot\",\"41\":\"elasticity\\nlegged locomotion\\nhumanoid robots\\ngravity\\nvibrations\\ntorque\\nactuators\\ncontrol system synthesis\\nfeedback\\nmotion control\\nobservers\\nposition control\\nrobot dynamics\\nstability\\nlinear feedback controller\\nhumanoid robot\\nindustrial robots\\nposition-controlled humanoid robots\\ndisturbance observer based estimator\\nflexible joint model\\njoint elasticity\\ndyros-jet robot\\ncompliant motion\",\"42\":\"friction\\nsensors\\nforce\\nfoot\\nstate estimation\\ncomputational modeling\\nend effectors\\nhumanoid robots\\nlegged locomotion\\npattern clustering\\nprobability\\nrobot kinematics\\nunsupervised learning\\nhumanoid estimation\\ncontact state estimation\\nfuzzy clustering\\nsix-dimensional humanoid contacts\\nproprioceptive sensors - endeffector contact wrench sensors\\ninertial measurement units\\nclustering-based contact probability estimator\\nkinematics-based base state estimator\\nsensor noise\\nunsupervised contact learning\\nimus\\ndofs\",\"43\":\"legged locomotion\\nrobot kinematics\\nrobustness\\nsensitivity\\nrobot sensing systems\\ntrajectory\\ncontrol system synthesis\\ngait analysis\\nlinear quadratic control\\nmotion control\\nrobot dynamics\\nrobust control\\nh\\u221e control\\nlqr controllers\\ncompass gait walker\\ngait sensitivity norm\\ndisturbance rejection\\ntransverse coordinates\\nrobust controllers\\ndynamic walkers\\nunstructured environments\\ndynamic walking robots\",\"44\":\"legged locomotion\\ncouplings\\nmathematical model\\nactuators\\nacceleration\\nforce\\nbraking\\noptimisation\\ntime optimal control\\nrapid acceleration manoeuvres\\noptimal control\\ntime optimal sprint\\nrealistic linkage morphology\\npre-specified actuator\\nnominal leg length\\noptimisation problem\\nbrute force approach\\nunique motion trajectories\\ntime optimal behaviour\\nsteady state motion\\nbraking manoeuvres\\nphysical bipedal robotic platform\\nbipedal platform\",\"45\":\"optimization\\nfoot\\nlegged locomotion\\nrobustness\\nlips\\ndynamics\\ncomputational modeling\\nhumanoid robots\\nnonlinear control systems\\noptimisation\\npendulums\\nrobot dynamics\\ndynamic walking\\nnonlinear optimization problem\\ncontinuous dynamics\\ndiscrete dynamics\\nremaining step duration\\nfoot location\\nmotion model captures\\nmass dynamics\\nlow-dimensionality\\nholistic approach\\nthree-dimensional parametric space\\ncomputational efficiency\\nsequential approach\\ncustomized optimization\\ncurrent step duration\\noptimal solutions\\nbipedal locomotion\",\"46\":\"legged locomotion\\ntask analysis\\nfoot\\ntrajectory\\nforce\\ntorque\\ngait analysis\\nmotion control\\nrobot dynamics\\ntorque control\\ntorque-based dynamic walking - a long way\\ntorque-controlled robots\\ntrajectory generation\\ndcm controller\\nwhole-body controller\\nwbc\\nfull-body walking behavior\\nsophisticated walking gaits\\noriginal control framework\\ndivergent component\",\"47\":\"force\\nnumerical models\\nenergy conversion\\nrobot kinematics\\nposition control\\ndynamics\\nend effectors\\nhumanoid robots\\nnonlinear control systems\\nenergy distribution polygons\\nhumanoid robot\\nlateral falls\\nsagittal falls\\nedp concepts\\ntotal energy\\nimpact forces\\nonline falling-over control\\nfall control technique\\nenergy concepts\\norientation control\\nenergy shaping\\nnonlinear control\\nes concepts\\nhumanoids\",\"48\":\"actuators\\nforce\\nball bearings\\ninductors\\nmagnetic levitation\\nprototypes\\nmagnetic flux\\nelectromechanical actuators\\nhandicapped aids\\nhaptic interfaces\\nlow-cost electromechanical actuator array\\ntactile display applications\\ndynamic tactile displays\\ncompact tactile display\\nlow-displacement bistable actuators\\nhaptic devices\\nvisually impaired\\nblind\\nlow-force bistable actuators\\n6-dot braille cell\\nhaptics\\ntactile display\\nactuator array\\nlow cost\\nbraille\\nassistive device\",\"49\":\"task analysis\\nforce\\nhaptic interfaces\\nvisualization\\nimpedance\\nmeasurement\\nrobots\\ndecision making\\nelastic constants\\nhuman-robot interaction\\nmanipulators\\ntelerobotics\\nteleoperated comanipulation\\nhigh stiffness controllers\\nhuman dyads\\ncomanipulative tasks\\nlow-level interactions\\nhigh-level interactions\\ndecision-making\\nteleoperated control\\nphysical human-human interaction\",\"50\":\"skin\\nhaptic interfaces\\nstrain\\nforce\\nvirtual environments\\ntraining\\nforce feedback\\nrendering (computer graphics)\\nvirtual reality\\nforce output\\nhaptic device\\nwearable devices\\nuser-grounded feedback modality\\nforce-output saturation\\nvirtual environment\\nvirtual object\\nscaled inertial forces\\nvirtual weight perception\\nhaptic feedback\\nwearable skin deformation feedback devices\\nvirtual blocks\\ninertial scaling factors\\nkinesthetic force feedback devices\\nhaptic rendering algorithm\\ninertial force scaling\\npoint of subjective equality\\nmass 200.0 g\\nmass 171.0 g\\nmass 151.0 g\",\"51\":\"skin\\nstrain\\nforce\\nforce feedback\\ndamping\\ndelay effects\\ncontrol engineering computing\\nhaptic interfaces\\nmedical robotics\\nrobot kinematics\\ntelerobotics\\nsensory substitution\\nforce-controlled skin deformation feedback device\\n3-degree-of-freedom kinesthetic force feedback device\\nreference object\\nhuman force perception\\nlatency\\nrefresh rate\",\"52\":\"robots\\ncognition\\ndata structures\\nontologies\\ntask analysis\\nknowledge based systems\\nphysics\\nknowledge representation\\nmanipulator kinematics\\nmanipulators\\nservice robots\\ncomplex manipulation tasks\\nchemical experiments\\nfirst-order time interval logic knowledge base\\nlogical expressions\\nrobotics algorithms\\nmotion planning\\nopen-ended manipulation skills\\ncommonsense knowledge\\n2nd generation knowledge processing framework\\ncognition-enabled robotic agents\\ngeneration knowledge representation\\nadvanced knowledge\\ninverse kinematic problem solving\\nknowrob2.0\",\"53\":\"task analysis\\nrobot sensing systems\\nrobot programming\\nguidelines\\nrobotic assembly\\nassembling\\nmanipulators\\nmobile robots\\nintuitive constraint-based robot programming\\nrobotic assembly tasks\\nrecent intuitive robot programming approaches\\nencapsulating robot capabilities\\ngeneral guidelines\\nassembly process descriptions\\ngerman engineers vdi\\nparticular addressing assembly applications\\nconstraint-based approach itasc\\nelementary processes\\nexemplarily assembly tasks\\ninstantaneous task specification\",\"54\":\"task analysis\\nservice robots\\nnatural languages\\nrobot kinematics\\nmiddleware\\nrobot sensing systems\\nhumanoid robots\\nhuman-robot interaction\\nindustrial robots\\ninference mechanisms\\nintelligent robots\\nmulti-robot systems\\nontologies (artificial intelligence)\\nrobot programming\\nmaestrob\\nintegrated orchestration\\nlow-level control\\nhigh-level reasoning\\nmaestrobe\\ncomplex tasks\\nsimple high-level instructions\\nhierarchical structure\\nontology\\nactuation control\\nsymbolic planner\\nwatson apis\\ncognitive capabilities\\nsemantic understanding\\nopen source robot middleware\\ncomplex scenario\\ncommunication robot\\nindustrial robot\\ncommon industrial task\\nassembly task\\nhumanoid robot\\nsoftbank robotics\\nnatural language conversation\\nhuman demonstration\\ncollaborative robot arm\\nuniversal robots\\nrobotic framework\",\"55\":\"robots\\ndocumentation\\nsoftware\\ntask analysis\\ncomputer architecture\\nhardware\\ntools\\ncontrol system synthesis\\nfeedback\\nmobile robots\\nmulti-robot systems\\npath planning\\nrobust control\\nsoftware architecture\\ntrajectory control\\nmultidof robot\\nrobotic applications\\nmodular framework\\ncontrol developers\\nfeedback controllers\\ncoupling\\nsoftware modules\\nctrl-more\\nmanipulation\\nlocomotion\\nvision\\nstabilizers\\ntrajectory planners\\nrobustness\",\"56\":\"software\\nhardware\\nsecurity\\nsensor fusion\\nvehicle dynamics\\ncontrol systems\\nautonomous aerial vehicles\\nhelicopters\\nsecurity of data\\nbluebox\\ncyber-physical attacks\\ncyber-physical platform\\nunmanned aerial vehicles\\nsecurity threats\\nsophisticated attacks\\ngeneric security framework\\ncross-layer retrofitting\\nuav\\noff-the-shelf quadcopter\\ncontroller\\nmotors\\noperating system\\nvehicle control system\",\"57\":\"task analysis\\nrobot kinematics\\nkinematics\\ndsl\\nlibraries\\nrobot sensing systems\\nrobot programming\\nrobotic assembly\\nspecification languages\\nindustrial assembly applications\\ntask function approach\\ntask frame formalism\\nmodel robot task description\\nrobot tasks\\nmodel-based manipulation skills\\nrobotic assembly tasks\\nprototype-based skill model\\nprototype-based inheritance\\nreuse\\ndomain-specific languages\\nmodel coordination mechanisms\",\"58\":\"legged locomotion\\nrobot sensing systems\\nactuators\\nhardware\\ntorque\\ncontrol engineering computing\\nforce control\\nhardware-software codesign\\nmobile robots\\nmotion control\\nrobot dynamics\\nmodel-based control\\ndesign process\\nlegged machines\\ndynamic behaviors\\nhigh performance robots\\ndesign requirements\\ncontrol algorithm\\nphysical robot\\nactuation\\nnatural dynamic behavior\\nphysical machine\\nlegged robots\\nhigh bandwidth force control\\nsoftware-hardware codesign processes\\nrobotic control\\nrobotic platforms\\nhardware design choices\\nmodel-based balance controller\",\"59\":\"dynamics\\ncomputational modeling\\nforce\\nlegged locomotion\\nkinematics\\ndata models\\nanalytical models\\nbiomedical measurement\\nfracture\\ngait analysis\\ninjuries\\nmonte carlo methods\\nwhole-body motions\\nwhole-body motion generation\\nvertical contact force\\nhuman motion mechanisms\\nwhole-body human model\\nmlkd sim\\nmultilayered kinodynamics simulation\",\"60\":\"robots\\ntelepresence\\ncollision avoidance\\ntask analysis\\nforce\\nhuman-robot interaction\\nsystem performance\\ngroupware\\ntelerobotics\\nshared control\\ntelepresence robot\\nlocus of control\\nteleoperation controllers\\ncollaborative performance\\nrobotic systems\",\"61\":\"robots\\ngrasping\\ntask analysis\\ntrajectory\\nlearning (artificial intelligence)\\ncontext modeling\\ncustomer relationship management\\nhuman-robot interaction\\nhuman feedback\\nreward function\\nbi-perspective reward learning\\nsimulated robot grasping task\\ngeneral hierarchical reinforcement learning framework\\nrobot perspective\\nfeedback efficiency\\nphysical robot\\ninformative reward function\\nhuman preferences\",\"62\":\"biology\\nhumanoid robots\\nstakeholders\\nbatteries\\naerospace electronics\\nservice robots\\naircraft control\\nautonomous aerial vehicles\\nhuman-robot interaction\\nhuman-humanoid robot interactions\\navian flight paths\\nsuas manufacturers\\nsuas flight paths\\nsmall unmanned aerial system flight paths\\nhuman-human interactions\",\"63\":\"learning (artificial intelligence)\\nmathematical model\\ntrajectory\\nfunction approximation\\nspinal cord injury\\nmarkov processes\\nestimation\\nimage motion analysis\\ninjuries\\nmedical image processing\\nneurophysiology\\nbellman optimality equation\\nreward learning\\ninverse reinforcement learning\\nlearned reward function\\ncomputationally expensive reinforcement learning problems\\nfunction approximation method\\nclinical motion analysis\",\"64\":\"task analysis\\nservice robots\\nroads\\nrobot motion\\nplanning\\nshortest path problem\\nhuman-robot interaction\\nmobile robots\\noptimal control\\npath planning\\nuser preferences\\nrobot motion planning\\ncomplex task specifications\\ntemporal constraints\\ncomplex robot tasks\\nuser constraint\\nuser-optimal path\\nspatial constraints\",\"65\":\"object tracking\\nmathematical model\\nrobustness\\nfeature extraction\\nlinear regression\\nvideos\\nstrain\\nobject detection\\nregression analysis\\ngoturn\\nsiamese regression networks\\nic-lk framework\\ndeep-lk\\nadaptive object tracking\\nregression-based object tracking\\ngeneric object tracking using regression networks framework\\ninverse compositional lucas & kanade algorithm\",\"66\":\"trajectory\\nthree-dimensional displays\\nlaser radar\\ntracking\\ncameras\\nneural networks\\nradar tracking\\nimage matching\\nlearning (artificial intelligence)\\nlinear programming\\nobject detection\\nobject tracking\\ntarget tracking\\n3d trajectories\\nmultisensor 3d tracking\\nend-to-end learning\\nconvolutional networks\\nlinear program\\nlidar data\",\"67\":\"manipulators\\ndata models\\nvisualization\\nrobot sensing systems\\ntraining\\nradio frequency\\nimage colour analysis\\niterative methods\\nobject tracking\\nrobot vision\\nrgb-d camera\\nmanipulated object\\nper-pixel data-to-model associations\\ntracked object\\niterative closest point\\nvisual articulated tracking\\nrobotic manipulator\",\"68\":\"object tracking\\nbenchmark testing\\ncameras\\ntarget tracking\\nrobots\\nimage motion analysis\\nimage sequences\\nobject detection\\nrobot vision\\nvideo signal processing\\nvision-based robotic applications\\nevaluating state-of-the-art algorithms\\ncarefully designed planar object tracking benchmark\\nplanar objects\",\"69\":\"robustness\\nlighting\\nperturbation methods\\nobject tracking\\nkalman filters\\nvisualization\\nimage matching\\nimage motion analysis\\nmotion estimation\\nobject detection\\ntracking\\ntemplate tracking algorithms\\nocclusion detector\\nstate-of-the-art planar object trackers\\nrobust kalman filter\\nplanar object tracking\\nheavy motion blur\",\"70\":\"predictive models\\ninverse problems\\nrobot sensing systems\\nkinematics\\ntraining\\ndeconvolution\\nfeedforward neural nets\\nimage sensors\\nkalman filters\\nlearning (artificial intelligence)\\nmobile robots\\nnonlinear filters\\nstate estimation\\nhigh-dimensional images\\ndeconvolutional methods\\nrobotic system\\nrobot trajectories\\nconvolutional neural network model\\nphoto-realistic images\\nimage generation\\nvideo frames\\nperceptual model\\nrobotics\\ninverse models\\ninverse perceptual models\",\"71\":\"robot kinematics\\nrotors\\nbuildings\\npayloads\\nshape\\ntask analysis\\nattitude control\\nautonomous aerial vehicles\\nmobile robots\\nmulti-robot systems\\nrobot dynamics\\nself-assembly\\nmidair\\nmodular robotic structure\\nself-assemble\\nagile flying modules\\nquadrotor platform\\nmodquad swarm\\nmodular flying structures\\ndecentralized modular attitude controller\\ndocking method\\nflying modular structure\\nflying structure assembling\\ncooperative flying method\",\"72\":\"batteries\\ntask analysis\\nactuators\\nrobot kinematics\\nplanning\\nmanipulators\\nautonomous aerial vehicles\\ncontrol engineering computing\\nmobile robots\\nmulti-robot systems\\nplanetary rovers\\nsmall scale uav\\nautonomous battery exchange operation\\nautonomous operations\\nlanded uav\\nground rover\\nautonomous outdoor experiments\\ncollaborative software framework\\nrobotic systems\\npersistence\\nbattery exchange mechanism\\nservice station\\nrobotic arm\\nmobile ground base\",\"73\":\"wheels\\nlegged locomotion\\ntask analysis\\nstability analysis\\nkinematics\\nattitude control\\nmotion control\\nquadratic programming\\nrobot kinematics\\nstability\\nmotion capabilities\\ncentauro robot\\nbody attitude stabilizer\\nattitude balancing strategy\\nquadrupedal robot\\ninverse kinematics solution scheme\\nstable reaction response\\nsmooth reaction response\\nrobot hybrid wheeled-legged mobility system\\nquadratic programming optimization\",\"74\":\"cameras\\ngaussian processes\\nrobot sensing systems\\nwheelchairs\\nmobile robots\\nhardware\\nadaptive control\\nautoregressive processes\\ndistance measurement\\nhandicapped aids\\nimage capture\\ninteractive devices\\nlearning (artificial intelligence)\\nmotion control\\npath planning\\nrobot vision\\nmotion predictors\\nsmart wheelchair\\nrobotics\\nanalog joystick inputs\\nblack-box transformations\\nintuitive motion control\\nadaptable motion control\\nhuman operators\\ncommercial pwc platform\\nphysical modification\\nelectronic modification\\nindustry standard auxiliary input port\\nvisual odometry\\njoystick signals\\nautoregressive sparse gaussian process model\\nshort-term path prediction experiments\\npowered wheelchair platform\\nrgb-d camera\\narduino interface board\\nmotion data capture\\nstandard axle mounted odometers\",\"75\":\"wheels\\nnavigation\\ntrajectory\\nrobot sensing systems\\naxles\\nthree-dimensional displays\\nmobile robots\\noff-road vehicles\\npath planning\\nrobot dynamics\\nrobot kinematics\\ntrajectory control\\nelevation grid map\\nrobot orientation\\nbehavior-based control paradigm\\nrough terrains\\ntraversability\\noccupancy maps\\non-road local navigation approaches\\ntrajectory candidates\\nbehavior-based local navigation approach\\nvehicle kinematics\\nrough off-road scenarios\\nrobotics\\noff-road navigation\\nbehavior-based control\\ntentacles\",\"76\":\"manipulators\\ntask analysis\\ntrajectory\\nhardware\\nkinematics\\nrobot kinematics\\ncollision avoidance\\nend effectors\\nmobile robots\\nmotion control\\nredundant manipulators\\nconstrained floor space\\nrobotic manipulators\\nmobile platforms\\nwarehouse shelf stacking\\nassistive robots\\ncritical time\\ncontinuous operation\\nexperienced operator\\nend-effector workspace\\nfloor obstacles\\nstraightforward control method\\ntime-dependent constraints\\ntime constraints\\nmobile base trajectory\\nsensor-assisted obstacle avoidance\\nfreedom mobility\\nsafe obstacle-free time-independent path\\n5-dof redundant planar mobile manipulator\\n9-dof redundant mobile manipulator\\nmobile platform motion\\nallowed obstacle-free path\\ntask completion\\nnonholonomic mobile manipulator\",\"77\":\"mobile robots\\nwheels\\nrobot kinematics\\npredictive control\\nmeasurement\\nparametric statistics\\nmotion control\\nparametric mpc approach\\ndifferential-drive mobile robots\\nthree-state unicycle model\\ntwo-state single-integrator model\\nmaneuverability costs\\ncontrol signal\\nparametric model predictive control method\",\"78\":\"soil\\nmathematical model\\nwheels\\nstress\\ncomputational modeling\\nshearing\\nforce\\ncartography\\ncontrol engineering computing\\ngeometry\\nmars\\nmobile robots\\nplanetary rovers\\ndigital elevation map\\nterrain physical properties\\nterrain pressure-sinkage property\\ncontact model\\nterramechanics model\\nshearing property\\nfriction angle\\nmars exploration\\ncomplex terrains\\nterrain property mapping\\ndynamic simulation\\nthree-wheel-rover\",\"79\":\"belts\\nmotion segmentation\\npower transmission\\ntorque\\nsynchronous motors\\nangular velocity\\nacceleration\\ncomputer based training\\ngears\\nmedical computing\\npatient rehabilitation\\npulleys\\nvirtual reality\\npower transmission performance\\nweight structure\\npower transmission efficiency\\ntransversal treadmills\\nomnidirectional treadmills\\nfast-omnidirectional treadmill\\ngeared omni-pulley\\npower transmission mechanism\\npower transmission inefficiency\\nomnidirectional walking\\nhuman locomotion\\nvirtual environment\\nnatural navigation\\nimmersive navigation\\nimmersive locomotion interface\\nvirtual reality environments\\nlocomotion interface platform\\nf-odt system\\nindependent y-axis motion\",\"80\":\"couplings\\nkinematics\\nelbow\\nrobot sensing systems\\nredundancy\\nfasteners\\njacobian matrices\\nmanipulator kinematics\\nmobile robots\\nredundant manipulators\\ndesign alternative\\nkinematic redundancy\\nkinematic parameter adaptation\\ncontinuum robot design\\nadmissible design parameter values\\njoint forces\\ngradient descent redundancy resolution problem\\nvariable geometry continuum\\njoint limits\\nmultibackbone continuum robots\\ncontinuum robot segment\\nsituational awareness\\ntask execution performance\\ncontinuum robots\\nvariable geometry robots\\nangulated scissor mechanism\",\"81\":\"friction\\nmathematical model\\ntorque\\ngears\\nrobots\\nharmonic analysis\\nsprings\\nbrushless dc motors\\ncompensation\\nelectric drives\\nhumanoid robots\\nmanipulator dynamics\\ndynamic friction model\\nthermal dependency\\nrobotic actuation\\nfriction behavior\\nactuator components\\nfriction compensation\\noutput torque estimation\\ndynamic simulations\\nbrush-less dc motor\\nharmonic drive gear\\nhumanoid david\\ndlr floating spring joint\\nfriction models\\nnonlinear viscous dependency\\ntemperature 24 degc to 50 degc\",\"82\":\"robots\\nmilling\\ndamping\\ntools\\nforce\\ncopper\\nvibrations\\ndesign engineering\\neddy currents\\nfinite element analysis\\nfrequency response\\nindustrial robots\\nmachine tool spindles\\nmachining chatter\\nmagnetic flux\\nmagnetic forces\\nmechanical stability\\nvibration control\\nrobotic milling process\\nvibration attenuation method\\nvibration suppression process\\ntool tip frequency response function\\nmilling spindle tool\\nchatter stability\\nmagnetic force\\nmagnetic flux density\\nfinite element method\\neddy current damper design\\nrobotic milling\\neddy current damper\\nvibration suppression\\nchatter\",\"83\":\"robustness\\ncameras\\nbrightness\\nlighting\\ntraining\\ndecoding\\nestimation\\nconvolution\\ndistance measurement\\nfeedforward neural nets\\nimage enhancement\\nimage representation\\nimage sequences\\nlearning (artificial intelligence)\\nobject tracking\\nrobot vision\\nslam (robots)\\nvisual odometry\\nhigh dynamic range environments\\ninterest points\\nbold assumptions\\nbrightness constancy\\ndeep learning perspective\\ndeep neural network\\nlong short term memory\\ndeep networks\\nvo framework\\nconvolutional neural network\\nillumination conditions\\nhdr environments\",\"84\":\"cameras\\nvisualization\\ngyroscopes\\nrobot vision systems\\nthree-dimensional displays\\nconvergence\\naerospace components\\nattitude control\\nautonomous aerial vehicles\\nend effectors\\nimage sensors\\nimage sequences\\nleast squares approximations\\nmobile robots\\nnonlinear programming\\nrobot vision\\nrobust control\\nspherical visual gyroscope\\nautonomous robots\\ndirect omnidirectional visual gyroscope\\nmobile robotic platforms\\ncamera-robot\\npixel intensities\\nextended convergence domain\\nspherical image sequences\\nrobot arm\\n3d orientation\\nmixture of photometric potentials\\nimage-similarity measure\\nnonlinear least-squares optimization scheme\\ntwin-fisheye camera\\nend-effector\\nfixed-wing uav\\nrobust attitude estimates\",\"85\":\"lighting\\nvisualization\\nlatches\\nnavigation\\nrobustness\\nmeasurement\\nevolutionary computation\\ncomputer vision\\nnatural scenes\\npose estimation\\npose change\\nplace-and-time-dependent binary descriptor\\ngrief evolution algorithm\\ncorrespondence generation\\nsingle-experience visual teach and repeat system\\nlocalization failures\\nnatural scene changes\\nvision-based navigation\\nlong-term visual localization\\nextreme illumination changes\\nbinary descriptors\\nsingle descriptor scheme\\nadaptive descriptor\\ndescriptor generation\\nlong-term seasonal variations\\nshort-term illumination changes\",\"86\":\"kernel\\noptical sensors\\noptical imaging\\ncorrelation\\ncorrelators\\nrobustness\\nestimation\\naircraft control\\naircraft navigation\\nautonomous aerial vehicles\\ncameras\\nhelicopters\\nmobile robots\\nposition control\\ncorrelation flow\\nreliable velocity estimation\\nrobust trajectory estimation\\nrobust optical flow\\nkernel cross-correlators\\nposition estimation\\nautonomous robot navigation\\nautonomous navigation\\nkernel cross-correlator based algorithm\\nmonocular camera\\nros framework\\nyaw rate\",\"87\":\"cameras\\nrobot sensing systems\\nuncertainty\\nlighting\\ngeometry\\nimage sensors\\nrobot vision\\nstereo image processing\\ntelecommunication scheduling\\ncubic range error model\\nstereo vision\\nlow-cost depth sensors\\nstereo camera setup\\nrobotics\\naugmented reality\\nmap generation\\nsensor scheduling policy\\nmultisensor setup\\nrange error models\\nuncertainty estimates\\nrange measurements\\nintegrated illuminators\\noff-the-shelf structured light stereo system\",\"88\":\"estimation\\nstereo vision\\nrobot sensing systems\\ncameras\\ntraining\\nfeedforward neural nets\\nimage colour analysis\\nintelligent robots\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nslam (robots)\\nmonocular depth estimates\\nautonomous robots\\nself-supervised learning setup\\nstereo vision depth\\nconvolutional neural network\\nfusion method\\ncnn estimates\\nautonomous navigation\\ndepth estimation\\nself-supervised learning\\nmonocular depth estimation\\nconvolutional neural networks\",\"89\":\"entropy\\nmeasurement\\ncameras\\noptimization\\nbayes methods\\nrobot vision systems\\ngradient methods\\noptimisation\\nrobot vision\\nimage degradation\\nexposure control scheme\\nimage frame grab\\nlight conditions\\nvision-based approaches\\noptimal exposure value\\nimage information measure\\ndynamic lighting conditions\\ncamera exposure\\nvision-based robotic applications\\nentropy weighted image gradient\\nbayesian optimization\",\"90\":\"manipulators\\nforce\\nforce control\\nimpedance\\ndamping\\naerospace electronics\\nforce sensors\\nminimisation\\nmotion control\\nposition control\\ninteraction forces\\ndirect force control\\nimplicit force control algorithms\\ncompliant manipulation\\nfree-floating objects\\ncompliant motions\\nreaction forces\\nconstant force\\nmanipulator inertia\\nkuka lwr4+ manipulator arm\",\"91\":\"manipulators\\nsatellites\\norbits\\nspace vehicles\\nservice robots\\ncontrol systems\\naerospace robotics\\nartificial satellites\\ncontrol system synthesis\\nend effectors\\nindustrial robots\\nmulti-robot systems\\npath planning\\nspace debris\\nrobot rendezvous\\ngrasping manoeuvre\\nunmanned chaser satellite\\nperforming rendezvous\\ngrasping manoeuvres\\nmanoeuvres high disturbances\\nmanipulator arm end-effector\\nrobotic subsystem\\nchaser rendezvous\\nplanar air-bearing microgravity simulators\\nspecified test-bed system\",\"92\":\"estimation\\nrobot sensing systems\\ncomputational modeling\\ncollision avoidance\\npredictive models\\nbayes methods\\naerospace robotics\\nmanipulator dynamics\\nmobile robots\\nmotion estimation\\nmulti-robot systems\\nparticle filtering (numerical methods)\\nrobot vision\\nspace debris\\nvehicle dynamics\\nbrach collision model\\ncollision-based contact mode estimation\\nparticle filter\\npre-capture phase\\nmotion estimation error\\nreasonable computation resources\\ncollision-triggered filter\\nmoving rigid body\\nforce-torque sensor\\ndynamic rigid body capture\",\"93\":\"manipulators\\nsymmetric matrices\\nfuels\\nrobot kinematics\\nsatellites\\naerospace robotics\\nend effectors\\nmobile robots\\ncom\\ndegree-of-freedom\\nfree-floating space robot operations\\nworkspace fixation\\n6dof moving base\\ncenter-of-mass regulation\\nend-effector\",\"94\":\"cameras\\nspace vehicles\\nvisualization\\nsimultaneous localization and mapping\\noptimization\\nlighting\\nsolar system\\nplanetary rovers\\npose estimation\\nrobot vision\\nslam (robots)\\nvisual slam algorithms\\norb-slam2\\norbiting primary spacecraft\\nonboard visual simultaneous localization and mapping\\nvisual slam implementation\\nwide field of view camera\\noff-nadir camera pointing angles\\nnarrow fov camera\\norbiting spacecraft\\nvisual appearance\\nhigh-contrast shadows\\nhopping rover\\nillumination angles\\nsolar system bodies\\ncollaborative visual localization method\\nrobust visual localization\\ntime 1.0 hour to 12.0 hour\",\"95\":\"wheels\\nmoon\\nmathematical model\\nstress\\ndesign methodology\\npower demand\\nazimuth\\nmobile robots\\nplanetary rovers\\nwheel design methodology\\nlunar exploration rover\\noperation environment\\nsuccessful mission\\nrover wheel\\npower consumption\\nconceptual design stage\\npower acquisition\\nterrain characteristics\\nlunar simulant\\nwheel-terrain interaction model\\noptimal wheel dimension\\nmaximal trafficability\\nsingle wheel test bed\\ntractive performance maximization\",\"96\":\"wheels\\nmanipulators\\nimpedance\\nmobile robots\\nrobot kinematics\\nnull space\\naerospace robotics\\ncontrol system synthesis\\ncontrollability\\nmechanical variables control\\nmotion control\\noptimisation\\nplanetary rovers\\nposition control\\nrescue robots\\nplanetary rover\\nrobotic arm\\ncontrol framework\\nversatile manipulation\\nplanetary exploration\\ncontrol design\\nexperimental validation\\ncontact interaction\\nterrestrial applications\\nterrestrial whole-body controllers\\nwheel force distribution\\nwhole-body impedance control\\nmaneuverability\\nwhole-body cartesian impedance controller\\nglobal optimization\\noveractuation redundancy\\nmobile base\\nkinematic redundancy handling\\nserial kinematic subchain\\ndlr lightweight rover unit\\nrough terrain\\nterrestrial search-and-rescue scenario\",\"97\":\"electron tubes\\nkinematics\\ncollision avoidance\\nrobot kinematics\\ncameras\\nrobot vision systems\\nbiological tissues\\nmanipulator kinematics\\nmedical robotics\\nmotion control\\nneedles\\noptimisation\\npath planning\\nsimulated annealing\\nsurgery\\ntissue surface\\ncrisp robot\\nparallel structure connection points\\nglobal stochastic optimization algorithm\\nkinematic design optimization\\nparallel surgical robot\\ncontinuum reconfigurable incisionless surgical parallel robot\\nneedle-diameter medical robot\\nminimally invasive procedures\\nmotion planning\\nanatomical visibility\\nadaptive simulated annealing\\nasa\",\"98\":\"robot kinematics\\nmanipulators\\nactuators\\nkinematics\\nservice robots\\ndynamics\\nend effectors\\nmanipulator dynamics\\nmanipulator kinematics\\n3-axis translations\\nrotations\\nrobot end-effector\\norientational workspace\\ngearbox\\nworkspace volume\\ncommercial delta-type robots\\nhigh-speed parallel robot\\n6-axis delta-type robot\\nstructural complexity\\nrotational capability\\n3t3r parallel pick-and-place robot\\ncommercial actuation combination\",\"99\":\"observers\\nkinematics\\nposition measurement\\nvelocity measurement\\njacobian matrices\\nparallel robots\\ncables (mechanical)\\nclosed loop systems\\ncontrol nonlinearities\\ncontrol system synthesis\\nend effectors\\nfeedback\\nlinearisation techniques\\nmanipulator dynamics\\nmanipulator kinematics\\nmotion control\\nposition control\\nvelocity control\\nclosed-loop controllers\\nredundant dynamics\\nend effector\\ndynamic control\\njoint space approach\\ndynamic controller\\njoint variables\\ncable driven parallel robots\\ncable stiffness\\nobserver linearization\\nthree-tendon planar platform\\nbackstepping technique\\nquasivelocity method\",\"100\":\"kinematics\\ncouplings\\nfasteners\\nmanipulators\\nforce\\nmechanical engineering\\nperiodic structures\\ndesign engineering\\nmanipulator kinematics\\nmotion control\\ncoupling sub-chain\\ntwo-loop mechanism\\ntwo-loop generalized parallel mechanism designs\\nmoving platform\\nserial kinematic chains\\nkinematic structures\\nscrew theory\\nrotational degrees of freedom\\ntranslational degrees of freedom\",\"101\":\"task analysis\\nparallel robots\\npower cables\\ncollision avoidance\\nprototypes\\nwheels\\ncables (mechanical)\\nend effectors\\nmanipulator kinematics\\nmobile robots\\nrobot dynamics\\ngeometric architecture\\nparallel manipulators\\nconvex hull methods\\nhyperplane shifting methods\\npoint-mass end-effector\\nstatic equilibrium\\nmobile cable-driven parallel robots\\navailable wrench set\",\"102\":\"manipulators\\nkinematics\\nactuators\\nsensors\\nclosed-form solutions\\nlegged locomotion\\nconferences\\nmanipulator dynamics\\nmanipulator kinematics\\npose estimation\\nrobot vision\\nclosed-form solution\\ndirect kinematics problem\\nplanar 3-rpr parallel mechanism\\nactive prismatic joints\\nmanipulator platform\\npose estimations\\nreference drives\\nworkspace limitations\",\"103\":\"legged locomotion\\ntopology\\nkinematics\\ndistance measurement\\nconferences\\nautomation\\naustralia\\nmanipulator kinematics\\nmatrix algebra\\ngough-stewart platform forward kinematics\\nleg endpoints coalesce\\nvariable elimination methods\\nvirtual legs\",\"104\":\"simultaneous localization and mapping\\nbandwidth\\njacobian matrices\\nvisualization\\nmessage systems\\ngraph theory\\nmobile robots\\nrobot vision\\nslam (robots)\\ndirect intervehicle observations\\nfuses single vehicle slam\\ncooperative localisation\\ndata association\\nmap data\\nlocal subgraphs\\nshared states\\nlocalisation accuracy\\nbounding drift\\nlocal loop closures\\nrobotic scenarios\\ndata consistency\\nbandwidth limitations\\nsingle vehicle visual slam framework\\ninformation matrix\",\"105\":\"roads\\ncameras\\nvisual odometry\\nthree-dimensional displays\\nrobot vision systems\\nrobustness\\nfeature extraction\\ngeometry\\nimage colour analysis\\nimage matching\\nimage segmentation\\nimage sequences\\nmesh generation\\nobject detection\\nstereo image processing\\nroad detection\\nroad geometrical model calculation\\nroad region detection\\nroad geometrical model estimation\\nvisual odometry scale recovery method\\ngeometrical constraint\\nmonocular visual odometry scale recovery methods\\ncolor information\\ndelaunay triangulation method\",\"106\":\"visualization\\ncameras\\nestimation\\nrobustness\\nsimultaneous localization and mapping\\nhidden markov models\\ndistance measurement\\ninertial navigation\\nmotion estimation\\nmotion conflict detection\\nmotion conflict resolution\\nvisual-inertial odometry\\nmotion conflict aware visual inertial odometry\",\"107\":\"three-dimensional displays\\ncloud computing\\nrobot sensing systems\\nmeasurement\\niterative closest point algorithm\\noctrees\\nfeature extraction\\nimage registration\\nslam (robots)\\ncluttered man-made environments\\ngeometric constraints\\nspatial overlap\\nfailed alignment\\npoint cloud content\\nlaser-based localization failure\\ngeometric features\\npoint cloud registration\\nalignment risk\",\"108\":\"generators\\ndetectors\\nmeasurement\\nfeature extraction\\ncomputer architecture\\ntraining\\npipelines\\nimage filtering\\nimage recognition\\nadversarial training\\nadverse conditions\\nrobust metric localisation\\nappearance transfer\\nvisual place recognition\\ninvertable generator\\nimage transforming filter\\nfeature-matching\\ndense descriptor maps\\noutput synthetic images\\ninput rgb image\\ngenerated images\\nmultiple traversals\\nreliable localisation\",\"109\":\"robots\\noptimization\\nrandom variables\\ntask analysis\\nbatteries\\nlinear programming\\napproximation algorithms\\nautonomous aerial vehicles\\nconcave programming\\nknapsack problems\\nmobile robots\\nmulti-robot systems\\nstochastic programming\\nchance-constrained 0-1 knapsack problem\\nvariance-mean plane\\ndeterministic knapsack problems\\nmultirobot team selection problem\\noptimal chance constrained knapsack problem\\n2d discrete optimization problem\\nrisk-averse knapsack problem\",\"110\":\"planning\\nrobot kinematics\\nprediction algorithms\\nprobability distribution\\naustralia\\ncognition\\ncontrol engineering computing\\nmobile robots\\nmonte carlo methods\\nmulti-robot systems\\npath planning\\nplanning (artificial intelligence)\\nstatistical distributions\\ntree searching\\ndecentralised multirobot coordination\\ncoordinated multirobot missions\\npolynomial-time belief-space planning algorithm\\ninformative communication planning\\nplanning-aware communication\\nmultirobot information gathering\\nrobot simulation\\ndecentralised monte carlo tree search\",\"111\":\"conferences\\nautomation\\naustralia\\nclosed loop systems\\nmatrix algebra\\nmobile robots\\nmulti-robot systems\\nnavigation\\npath planning\\nposition control\\nvelocity control\\npairwise distances\\nvelocity-controlled robot\\nmultirobot realization\\ngoal adjacency constraints\\nrobot positions\\nexact goal positions\\nrelative distances\\npairwise adjacency constraints\\nmultirobots\\nadjacency matrix\\nadjacency threshold\\nrobot pairs\\ncoordinated navigation\\nclosed-loop dynamics\",\"112\":\"robot kinematics\\ntrajectory\\ntorque\\nthree-dimensional displays\\npayloads\\nunmanned aerial vehicles\\naerospace robotics\\nasymptotic stability\\ncompensation\\ncontrollability\\ndistributed control\\nhelicopters\\nmobile robots\\ntrajectory optimisation (aerospace)\\nexponential stability\\ndistributed compensation scheme\\nlocal optimization problem\\nentire assembly\\ndistributed wrench controller\\nrigidly attached quadrotor aerial robots\\nmultiple quadrotors\\nsubsequent trajectory optimization\\noutput wrench space\\ncontrol wrench\\ngroup control authority\",\"113\":\"manipulators\\nsatellites\\njacobian matrices\\nstability analysis\\ndelays\\ntime-domain analysis\\naerospace robotics\\ndamping\\nobservers\\nstability\\nvibration control\\nenergy-based approach\\nmultirate control\\nrobotic system\\nactuated floating base\\nspace applications\\nstability issues\\ntime domain passivity approach\\nbase-manipulator multibody simulation\\npassivity-based stabilizing controller\\nenergy observer design\",\"114\":\"robot sensing systems\\nrobot kinematics\\nmarkov processes\\ncomputational modeling\\ndelays\\ndecision theory\\nmulti-robot systems\\npartial environmental information\\noptimal policy\\noptimal intermittent deployment\\nmultirobot team\\nenvironmental sensing problem\\nteam composition\\nenvironmental process\\nheterogeneous robots\\nheterogeneous robot teams\\nsensor types\\nsensor selection policy\",\"115\":\"vehicle dynamics\\nmanipulators\\nload modeling\\nunmanned aerial vehicles\\ndynamics\\ncables (mechanical)\\ndynamic programming\\nmobile robots\\nmulti-robot systems\\noptimal control\\npath planning\\nplanning problems\\naerial robots\\ntransportation task\\nground robots\\nnonrigid inextensible cables\\nheterogeneous multirobot system\\nmultiple aerial vehicles\\ngeneral constrained optimal planning problem\\nmultiple ground vehicles\\ncooperative object transportation\\nmodeling problems\",\"116\":\"maintenance engineering\\nplanning\\ncomputer architecture\\nrobot kinematics\\nsurveillance\\ndelays\\nautonomous aerial vehicles\\ndistributed control\\nmobile robots\\nmulti-robot systems\\nintermittent communications\\nhigh-level decision skills\\ndistributed decision architecture\\nhybrid planner\\ndistributed execution algorithm\\nsurveillance missions\\nground robots\\nheterogeneous robots\\nfield multirobot missions\\nautonomous aerial robot\\nunavoidable disturbances\\nintegrating planning and execution\\ncommunication constrains\\ntime constraints\\ndecentralized repairs\",\"117\":\"joints\\nmathematical model\\nphysics\\nrobots\\nneural networks\\nelbow\\ncomputational modeling\\nbackpropagation\\nhuman-robot interaction\\nneural nets\\noptimisation\\nphysical human-robot interaction\\nphysics simulation\\nimplicit equation\\nhuman data\\nphysics engine\\ndata-driven approach\\nhuman joint limits\\njoint motion\\nrealistic human joint limits\\nhuman joint configurations\\nrealistic human joint constraints\\noptimization problem\\nfully connected neural network\",\"118\":\"writing\\ntrajectory\\ntraining\\ngallium nitride\\nmanipulators\\nprobability distribution\\ncharacter sets\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nrobot programming\\nrobotic chinese calligraphy\\nrobotic writing\\nchinese character strokes\\nfont generation methods\\ngenerative adversarial nets-based calligraphic robotic framework\\ninteractive modules\\nstroke generation module\\nstroke discriminative module\\nstroke generative module\\ncalligraphic robot\\nhuman-level stroke\\nrobotic autonomous creation ability\\nreinforcement learning\",\"119\":\"force\\nnavigation\\ncloning\\nsensors\\nmobile robots\\nlearning (artificial intelligence)\\nvisualization\\npath planning\\nsocially compliant navigation\\nraw depth inputs\\nsocially compliant manner\\ngenerative adversarial imitation learning strategy\\nraw sensory input\\ngail-based approach\\nbehavior cloning policy\\nsocial force model\",\"120\":\"task analysis\\nrobots\\ncontext modeling\\nlearning (artificial intelligence)\\nvisualization\\ntools\\ncloning\\nlearning by example\\nrobot programming\\nvideo signal processing\\ncontext translation\\nobservation-action tuples\\nsupervised learning algorithm\\nimitation-from-observation\\ndeep reinforcement learning\\nvideo prediction\\nraw video\\nrobotic skills learning\\nimitation learning\",\"121\":\"hidden markov models\\ntask analysis\\nrobots\\ncomputational modeling\\nadaptation models\\nprobabilistic logic\\nbayes methods\\nautoregressive processes\\nfinite state machines\\nintelligent robots\\nmanipulators\\nincremental task modification via corrective demonstrations\\nstate transition auto-regressive hidden markov model\\nprobabilistic properties\\nsimulated block sorting domain\\nreal-world pouring task\\nitmcd model selection\\napproximate bayesian model selection\\nfsa\\nfinite state automaton representation\",\"122\":\"robots\\ntask analysis\\nvisualization\\nlearning (artificial intelligence)\\ntraining\\nliquids\\nlighting\\nimage representation\\npose estimation\\nrobot programming\\nrobot vision\\nvideo signal processing\\ntime-contrastive networks\\nrobotic behaviors\\nrobotic imitation settings\\nhuman poses\\nviewpoint-invariant representation\\nend-effectors\\nreinforcement learning algorithm\\nself-supervised learning\\nrobotic systems\",\"123\":\"robot sensing systems\\nadaptation models\\ntask analysis\\nmathematical model\\nneural networks\\nquaternions\\nadaptive control\\nfeedback\\nhumanoid robots\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nradial basis function networks\\ntactile sensors\\nphase-modulated neural networks\\nfeedback model maps\\nmotion plan adaptation\\nradial basis function network structure\\ntactile sensor traces\\ndata-driven framework\\nanthropomorphic robot\",\"124\":\"navigation\\nrobots\\ntrajectory\\nentropy\\ntraining\\ncollision avoidance\\ntask analysis\\nlearning (artificial intelligence)\\nmobile robots\\noptimal control\\nrobot navigation\\nhuman collaborators\\ndynamic environments\\ndisaster recovery\\nunmanned ground vehicle\\nugv\\nfast field adaptation\\nminimal human supervision\\nvisual perception\\ninverse optimal control\\nminimal human supervisory examples\\nnavigation behavior\\nreal-world environment\\nminimal human demonstration\",\"125\":\"cameras\\noptimization\\nvisualization\\nfeature extraction\\nmicrosoft windows\\nsimultaneous localization and mapping\\nreal-time systems\\ndistance measurement\\ngraph theory\\ninertial navigation\\nmobile robots\\noptimisation\\npath planning\\npose estimation\\nrobot vision\\nslam (robots)\\nrelocalization\\nglobal optimization\\nmonocular visual-inertial slam\\nvisual-inertial system\\nlow-cost inertial measurement unit\\nstate estimation\\nvisual-inertial odometry\\nabsolute pose estimation\\nvisual-inertial slam system\\nglobal pose graph optimization\\nmap merging ability\\nmap reuse\\npose graph optimization\",\"126\":\"laser radar\\ntrajectory optimization\\nsimultaneous localization and mapping\\nstrain\\ninterpolation\\nimage reconstruction\\nmobile robots\\noptical radar\\noptimisation\\nprobability\\nradar imaging\\nrobot vision\\nsensor fusion\\nslam (robots)\\ndense map-centric continuous-time slam\\nct-slam\\ncomputational complexity\\nsurfel fusion\\nglobal batch trajectory optimization\\nprobabilistic surface element fusion\\nmap deformation\\nglobal trajectory optimization\\ncontinuous-time slam\\nglobal batch optimization\\nmultimodal sensor fusion\\ncontinuous-time trajectory representation\\nelastic lidar fusion\",\"127\":\"servomotors\\nmathematical model\\nforce\\ntorque\\ndynamics\\nattitude control\\nanalytical models\\naerospace components\\nhelicopters\\nlarge-scale systems\\nservomechanisms\\ntilting thruster type multirotor\\nmechanically separated thrusters\\nfuselage posture\\nrelative attitude control\\nt3-multirotor\\ntranslational acceleration\\nservo-linkage mechanism\\ndynamically complex system\\nautonomous level flight\",\"128\":\"propellers\\nvehicle dynamics\\nprototypes\\nunmanned aerial vehicles\\ndesign tools\\naerospace components\\nautonomous aerial vehicles\\nautonomous underwater vehicles\\nclosed loop systems\\nmobile robots\\noptimisation\\nfixed-wing unmanned aerial-aquatic vehicle\\nfixed-wing vehicle\\naerobatic post-stall maneuvers\\nwater-to-air transition execution\\ndirect hybrid trajectory optimization\\nclosed-loop control\",\"129\":\"rotors\\npropellers\\nmathematical model\\natmospheric modeling\\nblades\\nhelicopters\\naircraft control\\nautonomous aerial vehicles\\nhelicopter ground effect\\nhover performance\\nmultirotor uav\\nrotor performance\\nsingle-rotor configuration\\nfixed propellers\\npropeller configuration\\nuav flight controller\\nflight stability\\nground effect\\nhelicopter models\\ncheeseman-bennett model\\nsmall-scale rotorcraft\",\"130\":\"solar panels\\nprototypes\\nbatteries\\nphotovoltaic cells\\npayloads\\npropellers\\naerodynamics\\naerospace control\\ncontrol system synthesis\\nenergy harvesting\\nfeedback\\nhelicopters\\nsolar powered vehicles\\nvirtualisation\\nlong-endurance missions\\naerodynamic\\nfeedback control system\\nvirtual simulation\\nsolar energy harvesting capabilities\\nsolar-powered quadcopter\\nsolar energy\\nquadcopter\\nsystem design\\nvehicle control\",\"131\":\"drag\\nforce\\nsea surface\\naerodynamics\\nwind\\natmospheric modeling\\nactuators\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\nautonomous underwater vehicles\\nhydrodynamics\\nvehicle dynamics\\nunav\\nunmanned nautical air-water vehicle\\nalbatrosses\\nsailboats\\nwind power\\nocean monitoring\\nwind-powered uav\\nmultiinput longitudinal flight controller\\ntrim analysis\\nsailboat\\nairborne wings\\ngravity-cancelling force\\nhigh lift-to-drag ratio\\nalbatross\\nspan-wise axes\\nvertical surface-piercing hydrofoil keel\\nvertical wing-sail\\nglider-type airframe\",\"132\":\"clustering algorithms\\nrobot kinematics\\nrobot sensing systems\\nheuristic algorithms\\nlinear programming\\ncooperative systems\\ndecision making\\ndistributed control\\ngaussian processes\\nmulti-robot systems\\npath planning\\npath clustering\\nmulti-robot exploration\\ninter-robot communication constraints\\ninformation-theoretic utility function\\nmax-sum algorithm\\ndistributed decision-making algorithm\\nmultirobot information gathering\\ninter-robot restrictions\\ndistributed multirobot cooperation\",\"133\":\"needles\\ngrasping\\nrobots\\ntools\\ninstruments\\nsurgery\\ntask analysis\\ncancer\\nendoscopes\\nmedical image processing\\nmedical robotics\\nrobot vision\\nsurgical tool\\nralp\\nurethrovesical anastomosis\\nrobotic surgical assistance\\nrobot-assisted laparoscopic prostatectomy\\nprostate cancer\\nnerve sparing removal prostate tissue\\nbladder neck\\ndexterity demanding tasks\\nsuturing instruments\\nrobotic instruments\\nvision-guided needle grasping method\\nsuturing needle\\ngrasping process\\nneedle detection algorithm\",\"134\":\"probes\\nmanipulators\\nactuators\\nwires\\nkinematics\\nend effectors\\ngraphical user interfaces\\nhaptic interfaces\\nmedical computing\\nmedical robotics\\nmotion control\\nshape memory effects\\nsurgery\\ntelerobotics\\nhybrid actuated robotic prototype\\nprototype robotic platform\\nminimally invasive surgical procedures\\nextra-operative motion\\npivoting motion\\n4 dof shape memory alloy actuated probe\\nintra-operative dexterity\\nend-effector\\nrobot operating system framework\\ninteraction forces\\ngraphical user interface\\nservo-actuated manipulator\\noperation mode switching\\nstereo imaging\\nteleoperation\\nhaptic device\\nminimally invasive surgery\\nrobot assisted surgery\\nshape memory alloy actuation\\nvisual servoing\\nmedical imaging\",\"135\":\"cameras\\nrobot vision systems\\nlighting\\nlenses\\nlaparoscopes\\nbiomedical optical imaging\\nimage sensors\\nlight emitting diodes\\nmedical robotics\\noptical design techniques\\nsurgery\\nin-vivo robotic laparoscopic camera design\\noptical efficiency\\nled\\nminiature optical lenses\\nilluminance\\nillumination uniformity\\nfreeform optical lens design method\\nsingle-port laparoscopic surgery\\noptimized illumination system\\ndistance 100.0 mm\",\"136\":\"cameras\\nrobot vision systems\\nmagnetic separation\\nsoft magnetic materials\\nsurgery\\ninstruments\\nfinite element analysis\\nmedical robotics\\nrobot vision\\nsingle port access surgery\\nminimally invasive surgery\\nplanar pan\\/tilt workspace\\nlower robot footprint\\nvertical space\\ninternal permanent magnets\\nipms\\ncylindrical capsule\\ncamera module\\ncamera view orientation\\nplanar workspace\\nanchoring\\nintra-abdominal surface\\nsteering\\ntilting panning\\ncamera robot prototype\\nminimal footprint\\nmagnetic anchored steered camera robot\\nsize 6.0 mm\\nsize 4.0 cm\\nsize 7.0 mm\\nmass 3.6 g\",\"137\":\"roads\\nlaser radar\\nsensor fusion\\nrobot sensing systems\\nvehicle detection\\nestimation\\nautonomous vehicles\\ndriver information systems\\nlearning (artificial intelligence)\\nobject detection\\nobject tracking\\noptical radar\\nreal-time systems\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\n2d lidar\\ndeep learning\\nvehicle tracking\\nlidar sensor fusion\\nglobal map coordinate system\\ntrack management\\ndata association\\nhigh precision range estimation\\nmonocular camera\\nrobust fusion system\\ntracking system\\nreal-time vehicle detection\\nroad context\\nurban driving environments\\nbehavior analysis\",\"138\":\"robot sensing systems\\nrobot kinematics\\noptimization\\npose estimation\\nglobal positioning system\\ntime measurement\\nautonomous aerial vehicles\\ndistance measurement\\ngraph theory\\nkalman filters\\nmobile robots\\nnonlinear filters\\noptimisation\\nrobot vision\\nsensor fusion\\nslam (robots)\\ngomsf\\nproprioceptive measurements\\nexteroceptive measurements\\nnavigation algorithms\\nagile mobile robots\\nunmanned aerial vehicles\\nuav pose estimation\\ngraph optimization based multisensor fusion\\n6 degree-of-freedom visual-inertial odometry poses\\nextended kalman filter\",\"139\":\"observers\\nobservability\\nrobot sensing systems\\naccelerometers\\nmagnetometers\\ncameras\\nimage sequences\\ninertial navigation\\nkalman filters\\nmotion estimation\\nstability\\nstate estimation\\nmotion excitation conditions\\ndeterministic observer\\naccelerometer measurements\\ngyrometer\\noptical flow\\nlinear velocity\\ninertial data\\ncontinuous homography\\nplanar target\\nattitude\\ntestbed imu-camera system\\nestimation errors\\nobservability analysis\",\"140\":\"robot sensing systems\\nmeasurement uncertainty\\nmeasurement errors\\ncovariance matrices\\npredictive models\\nestimation\\nneural networks\\ncovariance analysis\\ngaussian noise\\nimage representation\\nlearning (artificial intelligence)\\nneural nets\\nregression analysis\\nstate estimation\\nraw sensor data\\nraw sensor measurement\\nground-truth measurement error\\nmeasurement model\\nprediction performance\\ncovariance prediction\\nmeasurement covariance estimation\\ndeep neural network\\ngaussian noise models\\ndeep inference for covariance estimation\\npredictive sensor modeling\\nhand-coded features\",\"141\":\"robot sensing systems\\naccelerometers\\nmanipulators\\npose estimation\\nrobot kinematics\\nend effectors\\nlegged locomotion\\nmobile robots\\nmonte carlo methods\\nposition control\\nprosthetics\\nsensors\\noptimal placement\\ninertial sensor placement\\nnoise characteristics\\njoint angle encoders\\nend-effector positioning\\ndual arm\\nprosthetic limbs\\ninertial measurement units\\nartificial skin patches\\nnoise properties\\nsignal-to-noise ratio\\nmicromachined sensors\\ntwo-link robot arm\\nexpected estimation error metric values\\naccelerometer configurations\\noptimal number\\narm pose estimation error\\nsnr\\nmonte-carlo simulations\\nrobot arm pose estimation\\nimu\",\"142\":\"ground penetrating radar\\ncameras\\ninspection\\nsynchronization\\nrobot sensing systems\\nthree-dimensional displays\\nautomatic optical inspection\\ngraph theory\\nimage fusion\\nimage reconstruction\\noptimisation\\npose estimation\\nstructural engineering computing\\ntransportation\\ngpr\\nwheel encoder\\nsensing suite\\ndata collection scheme\\nals\\ntypes data streams\\ncamera images\\ndata fusion\\nsensory data\\nsensor fusion approach\\nencoder-camera-ground penetrating radar tri-sensor mapping\\nsubsurface transportation infrastructure inspection\\nalgorithmic development\\nmultiple sensors\\nmultimodal mapping\",\"143\":\"accelerometers\\nestimation\\nforce\\nmanipulators\\nhydraulic systems\\ngyroscopes\\ninertial navigation\\nkalman filters\\nmobile robots\\nnonlinear filters\\nsensor fusion\\nwheels\\ncommercial mobile working machine\\nextended kalman filter\\ncomplementary filter\\nsensors data fusion\\nlow-cost imus\\nsix degrees-of-freedom wheeled base platform\\n3-dof hydraulic anthropomorphic arm\\nfloating base hydraulic arm\\nvibrational disturbances\\nmachines diesel engine\\ndeformation\\nekf\\ncf\\nroot mean square error\\nrms error\\nfloating base robotic platforms\\nlink angles\\nlow-cost inertial measurement units\\nrobotic arms\\nangle estimation\",\"144\":\"gears\\nprototypes\\nservomotors\\nelectron tubes\\nmouth\\ntraining\\ntesting\\nbiomedical equipment\\ncomputerised tomography\\nmedical image processing\\nmedical robotics\\nmobile robots\\nrobot vision\\nservomechanisms\\nsteering systems\\nrobotic intubation device\\nendotracheal intubation\\nrobotic prototype\\nhardware system\\nstepper motor\\nservo motors\\nstylet tip\\nvocal cords\\npre-clinical testing\\nintubot\\nvision-based navigation algorithm\\nct scan images\",\"145\":\"legged locomotion\\nprosthetics\\nimpedance\\nmanifolds\\nkernel\\ngaussian processes\\nrobustness\\nadaptive control\\ngait analysis\\nmedical robotics\\nregression analysis\\ngaussian process regression\\nhuman subjects walking\\nlocomotion variables\\nnonlinear manifolds\\nanthropomorphic control\\nimpedance control\\npowered ankle foot prosthesis\\nlocomotion envelope\\ngait-cycle duration\\ntemporal evolution\\nvelocity range\",\"146\":\"bladder\\natmospheric modeling\\nconferences\\nautomation\\naustralia\\nfriction\\naccident prevention\\nfootwear\\northotics\\nphysiological models\\nwearable robots\\nfriction force\\nvalves\\nslip event\\nimpact force\\nbladder walls\\ncontact forces\\nslip-mitigating potential\\northotic device\\nintelligent orthotic shoe\\nlongitudinal slip\\nshoe slip\\nbladder based orthotic device\\nrubberized shoe sole\\nrobotic shoes\\nslip-fall accidents\",\"147\":\"electrodes\\nsprings\\nforce\\nthumb\\npolyimides\\nsilicon\\nbiomedical electrodes\\nmicroelectrodes\\nneuromuscular stimulation\\nsurgery\\nhandheld nerve electrode insertion device\\nperipheral nervous system\\nnerve holder\\nelectrode inserter\\nintrafascicular planar electrodes\",\"148\":\"plugs\\nstomach\\nnavigation\\nshape\\nfabrication\\nrobot sensing systems\\nbiological organs\\nbiomechanics\\nbiomedical materials\\ndiseases\\ndrug delivery systems\\ndrugs\\nhydrogels\\nremotely navigatable hydrogel patch\\ndeployable ingestible hydrogel patch\\nstomach ulcer therapy\\nremotely navigatable hydrogel plug\\ndeployable ingestible hydrogel plug\\nmagnetic field\\ndeployable origami design\\nfolded configuration\\ndehydration\\nhydration\\nagarose hydrogel\\ngastric ulcer treatment\\ncatastrophic situation\\ninflammatory process\\nstomach wall\\nprogrammable medicine\",\"149\":\"actuators\\nrobots\\near\\nmagnetic separation\\ndrug delivery\\ndrugs\\npermanent magnets\\nbiomems\\ndiseases\\ndrug delivery systems\\nelectromagnetic actuators\\nmagnetic actuators\\nmanipulators\\nmedical robotics\\nmicrorobots\\ndisease locations\\nmagnetics fields\\nreliable solution\\ninnovative solution\\nhuman body\\nuse ofrobotic devices\\npermanent magnetic actuator\\nopen-loop drug delivery strategy\\nreliable drug administration\\nprecise drug administration\\nhuman phantom cochlea\\nnavigation proposed strategy\\nhuman cochlea\\nmagnetic microparticle\\nfreedom robotic manipulator\\nrobotic drug delivery strategy\\nopen-loop control way\\nmagnetic actuator axis\\npushing pulling forces\\nend-effector\\nmicronanorobots\",\"150\":\"conferences\\nautomation\\naustralia\\nelastomers\\npneumatic actuators\\nrobots\\nhelical actuators\\nparallel actuators\\naugmented joint stiffness\\nsoft pneumatic actuators\\nsoft robotic actuators\\nwearable soft robotic sleeve\\nfiber reinforced elastomeric enclosures\\nhelical actuator architectures\\nlinear actuators\\nlinear-helical actuator configuration\\nfree\",\"151\":\"actuators\\nshape\\nelectron tubes\\nforce\\nmuscles\\nrobots\\ngeometry\\nelectroactive polymer actuators\\nmedical robotics\\nmotion control\\nmuscle\\npneumatic actuators\\nsupports\\ninternal chambers\\ntetrahedron apex\\ncompliant truss robot\\nindependent control\\nexternal chambers\\npneumatic actuator\\nantagonistic pneumatic artificial muscle\\napam\",\"152\":\"actuators\\ndamping\\nvibrations\\nrobots\\nfriction\\nshock absorbers\\nforce\\ndesign engineering\\nelasticity\\nindustrial robots\\npneumatic actuators\\nvibration control\\npassive particle damping\\nvibration damping method\\nuniversity of hong kong\\nelastic bodies\\nenergy source\\nsoft actuator design\\nsoft robotic actuators\\nactive particle damping\",\"153\":\"electron tubes\\nstrain\\noils\\nmuscles\\nhydraulic systems\\nforce\\nsoft robotics\\ndielectric materials\\nelastic constants\\nelastomers\\nelongation\\npre-stretch\\ntensile stiffness\\nfluid-filled tubular dielectric elastomer variable stiffness structure\\ninsulating oil\\nfiber-constrained dielectric elastomer tube\\nhydrostatic skeleton principle\",\"154\":\"jamming\\nbiomembranes\\nend effectors\\nshape\\nshafts\\nlegged locomotion\\nbending\\nbiomechanics\\nelastic constants\\nelasticity\\nforce measurement\\nfriction\\ngranular materials\\npressure sensors\\ncubic shaped granules\\ngradual stiffness change\\njamming membranes\\nbending stiffness\\nclimbing task\\nshaft walls\\ncompressive stiffness variation\\nmultimodal properties\\ngranular material\\nstiffness variability\\nquasisolid state\\npacking density\\ncompliant granules jamming\\ngranular media jamming\\nbioinspired robotic platform\\nstraight vertical shafts climbing\\nfriction force measurement\\npressure sensor\\nforce dissipators\\nsoft robotics\\nvariable stiffness joints\\nvacuum jamming\\nuniversal gripper\\nclimbing\",\"155\":\"kinematics\\nfasteners\\nstrain\\nheuristic algorithms\\nalgebra\\nsoft robotics\\nelasticity\\ngeometry\\ninverse problems\\nnewton method\\nrobot dynamics\\ndiscrete cosserat approach\\nsoft-body dynamics\\ngeometric theory\\nrecursive newton-euler algorithm\\nforward dynamic problems\\nsoft robots\\nsoft-rigid multibody systems\\nlinear complexity\",\"156\":\"legged locomotion\\ndc motors\\nsprings\\nresonant frequency\\nfoot\\nshape\\nbeams (structures)\\nelasticity\\nenergy conservation\\nvibrations\\nadaptation ability\\nrobot locomotion\\nmorphological adaptation\\nenergy efficient vibration-based robot\\nmorphological computation\\nsoft materials\\nelastic materials\\nlow-cost robot\\nelastic curved beam\\nrobots rich dynamics\\nrobots shape\\nrotating frequency\",\"157\":\"actuators\\nmanipulators\\nforce\\nrobot sensing systems\\nsoft robotics\\nfabrication\\nbiomechanics\\nbiomimetics\\nlegged locomotion\\nmobile robots\\nmotion control\\npneumatic actuators\\nrobot dynamics\\nbio-inspired octopus robot\\nnovel soft fluidic actuator\\nmodern roboticists\\nnovel robotic structures\\nsoft robots\\ncomplex motion patterns\\noctopus tentacles\\nbio-mimetic approach\\nsoft material\",\"158\":\"tools\\nrobots\\ntask analysis\\nswitches\\noptimal scheduling\\njob shop scheduling\\ntime factors\\ncluster tools\\netching\\nhoists\\nindustrial robots\\nlithography\\nlot sizing\\nmanufacturing systems\\nmaterials testing\\nparallel processing\\nscheduling\\nsemiconductor device manufacture\\nsemiconductor industry\\nstorage\\nsemiconductor manufacturing processes\\nlot finish processing\\nstockers\\noverhead hoist transports\\nwafer lots\\ntransport robot\\ndual-armed cluster tool\\nparallel processing modules\\nautomated manufacturing systems\\ncompletion time analysis\",\"159\":\"planning\\nrobot sensing systems\\ntask analysis\\nreliability\\ncomputational modeling\\nuncertainty\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nmulti-robot systems\\npath planning\\ncontrol uncertainty\\nconformant planning approach\\nrobot manipulation\\nmultiple planar objects\\nspecified arrangement\\nexternal sensing\\nbelief-state planning problem\\ninitial belief state\\nforward belief-state planning\\ndeterministic belief-state transition model\\noff-line physics simulations\\non-line physics-based manipulation approach\\nphysical robot experiments\\nuncertain domains\",\"160\":\"task analysis\\nmeasurement\\ncollision avoidance\\nservice robots\\nplanning\\nspace exploration\\nindustrial robots\\ntravelling salesman problems\\nrobotic task sequencing problem\\nindustrial robotics applications\\nspray-painting\\nrobot travel time\\nexecution time\\nrobot configurations\\ntask space\\nconfiguration space\\nrtsp literature\\ngeneralized traveling salesman problem\",\"161\":\"conferences\\nautomation\\naustralia\\ncollision avoidance\\nmotion control\\nrobotic assembly\\nunactuated disk-shaped parts\\nautomated reactive approach\\nsingle robot exogeneous planar assembly\\nassembly task\",\"162\":\"tools\\nplanning\\nsurface cleaning\\nmanipulators\\ntask analysis\\ncleaning\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nsampling methods\\nservice robots\\npr2 robots\\nfetch robots\\nprimitive dirt-oriented tool actions\\nheuristic search\\ncleaning tool\\narbitrary amounts\\nmanipulator\\nlearned transition models\\ndirt rearrangement planning\\nrobotic cleaning\",\"163\":\"grippers\\nplanning\\ngravity\\nrobots\\nthree-dimensional displays\\nfriction\\ndexterous manipulators\\ngeometry\\nmesh generation\\nsolid modelling\\nfast planning\\ntwo-finger pinch gripper\\n3d mesh model\\ngripper motions\\narbitrary object\\ngrasping positions\\ngripper poses\\nmotion primitives\\ncompliant rolling\\nplanning problem\\nobject shapes\\nnontrivial geometry\\ngripper workspace\\n3d any-pose-reorienting\\nmesh models approximation\\npivoting\",\"164\":\"observers\\nacceleration\\nforce\\nconvergence\\nfeedforward systems\\nclosed loop systems\\nacceleration control\\ncontrol system synthesis\\nfeedback\\ngait analysis\\nintelligent control\\npatient rehabilitation\\nposition control\\nuncertain systems\\nvelocity control\\naf\\noscillation-free response\\nsupervisory control scheme\\nrise controller\\npositive-output controller\\nstopping stage\\nposition error\\nsupervisory system\\nintelligent control scheme\\nself-adjustable treadmills\\nreference position\\noscillatory response\\nclosed-loop system\\nhigh-gain observer\\nuser velocity\\nacceleration estimation\\nintentional velocity estimation\\nabrupt stopping\\nself-adjustable treadmill\\nobserver\",\"165\":\"junctions\\nmathematical model\\ngrippers\\nlegged locomotion\\ntrajectory\\nrobot kinematics\\ncables (mechanical)\\nmanipulator dynamics\\nmotion control\\noptimal control\\ntorque control\\ntrajectory control\\nvibration control\\nbrachiating robots\\ncoupling soft junctions\\nmultiple-shooting\\nparametric trajectory approaches\\ncatenary cable\\nenergy-efficient continuous brachiation\\noptimal torque profiles\\ncontrol torque profiles\\noptimized trajectories\\nrobot locomotion\\ncable vibration\\nenergy-minimizing optimal control strategy\\ntwo-link robot\\nmultibody system\\nflexible cable\\ntwo-link underactuated brachiating robot\\nlocomotion control\\ndynamic modeling\",\"166\":\"force\\nrobot sensing systems\\nservice robots\\ntask analysis\\ntools\\nbenchmark testing\\nforce control\\nforce feedback\\nmobile robots\\nposition control\\nrobotic assembly\\nperformance indicator\\nimpedance control\\nrobot-based sensitive assembly\\nrobotics\\nforce feedback information\\ndirect force control\\nforce-controlled robot benchmarking\",\"167\":\"topology\\nswitches\\nrobot kinematics\\nnetwork topology\\nsynchronization\\ndelays\\nmobile robots\\nmotion control\\nmulti-robot systems\\nswitching systems (control)\\nfailure-tolerant approach\\ncommunication delays\\nrobot malfunction\\nrobot formation control\\nsystem malfunction\\nsynchronous formation control problem\\nnetwork connectivity\\nmotion synchronism\\nrobot replacements\\nsynchronous formation control method\\nrecursive switched topology control strategy\\nneighboring robots\",\"168\":\"robots\\nforce\\nestimation\\ndynamics\\nforce control\\neffective mass\\nadaptation models\\nadaptive control\\nmanipulators\\nposition control\\nenvironment stiffness\\nperceived stiffness estimation\\npositive force feedback loop\\ncontact dynamics\\nforce measurements\\nforce signals\\nlow pass filters\\nenvironment dynamics\\nforce control performance\\nforce control perspective\\nrobot effective mass\\nforce based stiffness estimation strategy\\ncontrol gains\\ncontrol optimization parameter\\nrobot dynamics\",\"169\":\"manipulator dynamics\\ngrasping\\ncontrol systems\\nkinematics\\nvehicle dynamics\\nautonomous aerial vehicles\\nforce control\\nmobile robots\\nmotion control\\nrobot vision\\nrotors\\nvisual servoing\\ncom offset motion\\ncenter of mass\\naerial grasping experiments\\naerial vehicle\\naerial manipulator control system\\nindependent control structure\\nhex-rotor\\nfixed-base manipulator\",\"170\":\"planning\\ntask analysis\\nmanipulators\\nmanifolds\\nkinematics\\nrobot kinematics\\nmobile robots\\npath planning\\nautonomous robotics\\nt-space parameters\\nmaze navigation\\nmotion planning\\ntask space motion planning decomposition\\ntsmpd\\nmanipulator\",\"171\":\"planning\\nrobot sensing systems\\nlegged locomotion\\nsemantics\\nmotion segmentation\\npath planning\\nnavigating\\nrescue environments\\nlocomotion methods\\nnavigation planning method\\nhybrid driving-stepping locomotion planning\\nmomaro robot\\nrobot state dimensionality\",\"172\":\"wheels\\nmobile robots\\nplanning\\nforce\\ntrajectory\\nacceleration\\ncontrol system synthesis\\nlegged locomotion\\nmotion control\\nrobot dynamics\\ndexterous quadruped\\ncontact force distribution\\nwheel slip\\nthree-wheeled skating maneuvers\\nomnidirectional motion primitives\\ndexterous limbs\\nquadruped robot\\nskating motions\\nlocomotion\\ncontinuous ground contact\",\"173\":\"trajectory\\nsmoothing methods\\nshape\\nmobile robots\\ninterpolation\\nstrain\\ngradient methods\\nmotion control\\noptimal control\\noptimisation\\npath planning\\nrobot dynamics\\nrobot kinematics\\ntrajectory control\\nwheels\\nwheeled mobile robots\\ncrowded environments\\noptimal sampling-based motion planners\\nnonoptimal motion planners\\npost-smoothing step\\ngradient-informed post-smoothing algorithm\\ngradient-informed path smoothing\\nsmooth trajectories planning\\ngrips\\nkinodynamic constraints\",\"174\":\"path planning\\ncleaning\\nrobot sensing systems\\nplanning\\ntraveling salesman problems\\nmobile robots\\nservice robots\\ntrajectory control\\nindoor coverage path planning\\nrobot trajectories\\nmobile cleaning robots\\nlawn mowing robots\\nharvesting machines\",\"175\":\"harmonic analysis\\nnavigation\\nrobot kinematics\\nconvergence\\ntuning\\ntrajectory\\ngeometry\\nmobile robots\\nmotion control\\npath planning\\nrobot vision\\nslam (robots)\\ncomplex workspaces\\nharmonic maps\\nartificial potential fields\\nautonomous robot navigation control schemes\\nlocal minima\\napf based control scheme\\ngoal configuration\\nmultiply connected compact 2d workspaces\",\"176\":\"robots\\ncomputational modeling\\ntrajectory\\nnavigation\\ndecision making\\ncost function\\ngradient methods\\nlearning (artificial intelligence)\\nmarkov processes\\noptimisation\\nstochastic processes\\nmultipolicy decision-making\\ngradient optimization\\nrisk-aware policy evaluation\\nbackprop-mpdm policy\\nrobot platform\\neasily-differentiable heuristic function\\nrandom sampling\\nstochastic gradient optimization algorithms\\ndecision making process\\nrisk-aware formulations\",\"177\":\"three-dimensional displays\\nfoot\\nintegrated circuit modeling\\nmathematical model\\nlegged locomotion\\nsolid modeling\\niterative closest point algorithm\\nfeedback\\nhumanoid robots\\nmotion control\\noptimal control\\nrobot dynamics\\nstepping\\nsfpe-based feedback\\nsfpe point\\ncontrol reference\\nbalance criteria\\nrecovery step location prediction\\nmomentum objectives\\nspherical foot placement estimator\\nbipedal gait\\nunknown disturbances\\nmomentum-based leaning\\nsmooth dynamics\",\"178\":\"planning\\nlips\\ntrajectory\\nfoot\\nreal-time systems\\noptimization\\nlaplace equations\\nlegged locomotion\\nlinear systems\\nmotion control\\nnonlinear control systems\\npath planning\\npendulums\\nquadratic programming\\ndcm\\nzmp\\nfoot placements\\nbipedal research\\nquadratic programming problem\\ndivergent component of motion\\nreal-time planning\\nlinear inverted pendulum\\nzero moment point\\npush recovery experiment\\ndisturbance compensation\",\"179\":\"hardware\\nlegged locomotion\\noptimization\\nkernel\\ntransforms\\nmeasurement\\nbayes methods\\ncontrol engineering computing\\ngait analysis\\nlearning (artificial intelligence)\\nmotion control\\noptimisation\\nbayesian optimization\\n1-dimensional space\\nfeature transformation\\ndifferent walking controllers\\natrias bipedal robot\\nnonhumanoid robot morphologies\\nhuman-inspired neuromuscular controller\\nhuman walking\\n16-dimensional locomotion controller\\nbipedal locomotion\\nblack-box data-efficient optimization scheme\\ndata-efficient learning techniques\\nsimulation transfer\\nexpert-designed heuristics\\nrobotics controllers\\ndomain knowledge\",\"180\":\"trajectory\\nmathematical model\\nthree-dimensional displays\\ntwo dimensional displays\\nrobots\\nradio frequency\\ndifferential equations\\nconvex programming\\nhumanoid robots\\nlegged locomotion\\nmechanical stability\\nmechanical variables control\\nconvex boundedness approach\\nbiped robots\\ncom height variations\\nnonconvex direct transcriptions\\ncentroidal dynamics\\nboundedness condition\\nbalance control\\nzmp\\nsagittal plane\\ncom trajectories\",\"181\":\"optimization\\nforce\\nlegged locomotion\\ntrajectory\\nfriction\\ndynamics\\ncompensation\\ninteger programming\\nlinear systems\\npredictive control\\nquadratic programming\\nhand contact\\nmpc walking framework\\nexternal contact forces\\ntwo-step optimization problem\\nzero moment point\\nzmp tracking error\\nfriction cone\\nwalking control scheme\\nlinear model predictive control scheme\\nmultiple contact locations\\ncenter of mass trajectory\\nmixed integer quadratic program\\nfrequency 100.0 hz to 300.0 hz\",\"182\":\"legged locomotion\\ntrajectory\\niterative closest point algorithm\\nplanning\\ndynamics\\nlips\\nangular momentum\\ngait analysis\\nhumanoid robots\\nmotion control\\ntrajectory control\\nsmooth trajectories\\ncontinuous trajectories\\nleg swing\\nnonnegligible angular momentum rate\\nreference trajectory generator\\nbipedal walking\\ncentroidal angular momentum\\nplanning stage\\nclosed-form trajectory solutions\\ncentroidal moment pivot\\ninstantaneous capture point\\ncmp\\nicp\\ncenter of mass\",\"183\":\"electromyography\\ntraining\\nlegged locomotion\\nmuscles\\nsensors\\nprosthetics\\nsupport vector machines\\nartificial limbs\\ngait analysis\\nlearning (artificial intelligence)\\nmedical signal processing\\nneurophysiology\\nsignal classification\\nsupport vector machine classifier\\nmovement delay classification\\ncontroller response\\ntraining pool\\nsubject specific classifiers\\nprosthetic legs\\nsubject-specific amputee data sets\\nsupervised training\\nreal-time monitoring\\nemg\\nneuromuscular interfaces\\npatient activities\\ninertial measurement units\\nprosthetic measurement units\\ngait mode\\nactive lower limb prosthetics\\ntranstibial amputee\\nmechanomyography\\ngait intent\\nsubject-independent data pooling\\nprosthetic control\\nsubject-specific training\\nmmg gait classifier\\nuser-specific data\\npooled training data\",\"184\":\"electrodes\\nelectromyography\\ntextiles\\nprosthetics\\nmuscles\\ngesture recognition\\nwrist\\nbiomechanics\\nbiomedical electrodes\\nmyoelectric control\\nlow-cost sensory systems\\naccurate gesture recognition\\ngesture recognition method\\nsurface electromyography experiments\\nlow-cost prostheses\\nhand gesture recognition\\ntextile electrodes\\nelectrode reuse\\ndisposable gel electrodes\\nsurface-electromyographic measurements\\npre-programmed control strategies\\nlow-cost prosthesis actuation\\ndeveloping countries\\nlow-cost manufacturing\\naffordable myoelectric prostheses\",\"185\":\"wrist\\nmuscles\\nstrain\\nskin\\nprosthetic hand\\nmathematical model\\nestimation\\nartificial limbs\\nbiomechanics\\ndeformation\\nelectromyography\\nmedical control systems\\nmedical signal processing\\nmuscle\\nprosthetics\\nviscoelasticity\\nwrist movement tasks\\nvoigt model\\nmeasured estimated angles\\nmuscle viscoelastic model\\nstable wrist joint angle control\\naccurate wrist joint angle control\\ndeformation signal\\ncontinuous prosthesis wrist joint control method\\nintended wrist joint angle\\nskin surface change\\nmuscle bulge\\nmeasured deformation\\ntransra-dial amputees\\nintended angle motion\\nupper-limb amputees\\nintended joint angles\\nforearm skin\\nmuscle deformation measured\\ncontinuous wrist joint control\",\"186\":\"muscles\\nstrain\\nultrasonic imaging\\nelbow\\nassistive devices\\ndeformable models\\nultrasonic variables measurement\\nbiomechanics\\nbiomedical ultrasonics\\nelectromyography\\nmedical image processing\\nmedical robotics\\nmedical signal processing\\nmuscle\\nassistive device sensor locations\\nreal-time ultrasound scanning\\nmuscle cross-sectional area\\nforce-associated deformation signals\\nmotion capture\\nultrasound scanner\\nhigh-dof assistive devices\\nb-mode ultrasound\\nexoskeletons\\nbiosignal-driven prostheses\\nsurface electromyography\\ntoward ultrasound-driven assistive device control\\nempirical quantification\\nmuscle deformation models\",\"187\":\"robots\\ngrasping\\nshafts\\nbars\\nforce\\nelectromyography\\nrubber\\nhandicapped aids\\nmedical robotics\\nneurocontrollers\\nneurophysiology\\npatient rehabilitation\\ngrasp rehabilitation\\nrobot design\\ngrasping movements\\nmechanical motions\\nreflex response\\nmotion intention\\nrehabilitation robot\\nactivate neural control loop\\ngrasp-training robot\\nparalyzed hand\\ngrasp reflex\\nelastic bar\",\"188\":\"semantics\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nforestry\\nlabeling\\ncontext modeling\\nstandards\\nimage fusion\\nimage reconstruction\\nimage segmentation\\nlearning (artificial intelligence)\\nmultiview frame fusion technique\\nperceived 3d structure\\nsemantic labelling task\\nhuman interaction\\nverbal references\\nlocation related services\\nmultiview 3d entangled forest\\nsemantic maps\\noffline reconstruction\\nsingle frames\\nonline multiview semantic segmentation\\nbatch approach\",\"189\":\"roads\\nlaser radar\\ncameras\\nimage segmentation\\nreal-time systems\\nlabeling\\nsemantics\\nfeature extraction\\nobject detection\\nobject recognition\\ntraffic engineering computing\\nunsupervised learning\\nvideo signal processing\\nvisual databases\\nroad marking segmentation\\nweakly-supervised annotations\\nmultimodal data\\nweakly-supervised learning system\\ncomplex urban environments\\nmonocular camera\\nexpensive manual labelling\\nannotated images\\ndeep semantic segmentation network\\nroad markings\\ntraffic situations\\nweather conditions\\nsensor modalities\\nlighting\\nqualitative performance\\nreal-time road marking detection\\nlabelling effort\\noxford robotcar dataset\\ncamvid dataset\",\"190\":\"semantics\\nlabeling\\nthree-dimensional displays\\nrobots\\ntraining\\ntask analysis\\nsolid modeling\\ndata mining\\nfeature extraction\\ngeometry\\nimage classification\\nimage colour analysis\\nimage reconstruction\\nimage sensors\\nindoor navigation\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nobject recognition\\nrobot vision\\nslam (robots)\\nstereo image processing\\nsemantic labels assignment\\nrooms reconstruction\\ndeep-learning techniques\\nvirtual rgb views\\ngeometric analysis\\nscene classification\\nroom types\\n3d rgb maps\\nindoor environments\\nsemantic labeling\",\"191\":\"motion segmentation\\ntrajectory\\ncomputer vision\\ncameras\\ntransmission line matrix methods\\nsemantics\\ncomputational modeling\\nimage motion analysis\\nimage segmentation\\nimage sequences\\ntraffic engineering computing\\nvideo signal processing\\ngeneric motion segmentation algorithms\\nkitti dataset\\nvideo sequence annotation\\nrestricted camera movement\\napplication-specific factors\\nurban environment\\nimage-based motion segmentation\\nurban street view scenes\\npractical motion segmentation\\nrealistic motion segmentation benchmark dataset\",\"192\":\"three-dimensional displays\\nlaser radar\\ncomputational modeling\\npipelines\\nautonomous vehicles\\nsemantics\\nclustering algorithms\\ncomputer games\\nfeedforward neural nets\\nimage classification\\nimage segmentation\\nlearning (artificial intelligence)\\nobject detection\\noptical radar\\npattern clustering\\ngrand theft auto\\nvideo game\\nautonomous driving\\nroad-objects\\nsemantic segmentation\\n3d lidar point cloud\\nreal-time road-object segmentation\\nrecurrent crf\\nrealistic training data\\nlidar simulator\\nextra training data\\n3d bounding boxes\\npoint-wise segmentation labels\\ncnn model\\ninstance-level labels\\npoint-wise label map\\ntransformed lidar point cloud\\nconvolutional neural networks\\nsqueezeseg\\nend-to-end pipeline\\npoint-wise classification problem\\nlidar point clouds\\ntime 8.2 ms to 9.2 ms\\ntime 3.0 d\",\"193\":\"three-dimensional displays\\ncameras\\nrobustness\\nvisual odometry\\nmotion estimation\\nentropy\\ntraining data\\ndistance measurement\\nfeature extraction\\nimage classification\\nimage sensors\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\npose estimation\\nrobot vision\\nslam (robots)\\nstereo image processing\\nself-supervised distractor learning\\nrobust monocular visual odometry\\nself-supervised approach\\ndistractors\\ncamera images\\ncluttered urban environments\\nper-pixel ephemerality mask\\ndepth map\\ndeep convolutional network\\nmonocular visual odometry pipeline\\nsparse features\\ndense photometric matching\\nmetric-scale vo\\nsingle camera\\nrobust vo methods\\nodometry drift\\negomotion estimation\\nmoving vehicles\\nurban traffic\\nvehicle motion\\nephemerality\\noffline multisession mapping approaches\",\"194\":\"semantics\\nsensors\\ncameras\\nrobots\\nbuildings\\nmirrors\\nvisualization\\ndistortion\\nimage classification\\nimage fusion\\nimage segmentation\\nimage sensors\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nomnidirectional vision\\nomnidirectional images\\nrobust segmentation\\noccupancy grid maps\\ninverse sensor model\\nnonlinear distortions\\nomnidirectional camera mirror\\nplace category classifier\\nrange-based occupancy grid\\ndense semantic map\\nbird eye view\\nvisual semantic mapping framework\\nrobot local free space\",\"195\":\"task analysis\\nimage segmentation\\ntraining\\nsemantics\\nrobots\\nmeasurement\\nthree-dimensional displays\\nconvolution\\nimage classification\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\nobject recognition\\nrecurrent neural nets\\nrobot vision\\nlimited training data\\nrobotic perception\\ncluttered scenes\\nshiny surfaces\\ntransparent surfaces\\nrobust perception pipeline\\ndata acquisition\\ndeep metric learning approach\\nsemantic-agnostic boundary detection\\npixel-wise voting\\nfully-supervised semantic segmentation approach\\narc 2017 dataset\\namazon robotics challenge 2017\\ndataset collection\\ndeep convolutional neural networks\",\"196\":\"ergonomics\\ntask analysis\\nrobot kinematics\\nplanning\\ncost function\\nhuman-robot interaction\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\noptimization formulation\\nergonomic situations\\nhuman-robot collaboration\\nmotion planning problem\\nmultiagent case\\nhuman robot\",\"197\":\"collaboration\\nservice robots\\nproduction\\nhazards\\noptimization\\nbig data\\nfactory automation\\nhuman-robot interaction\\nindustrial robots\\ninternet of things\\nmanufacturing systems\\nproduction engineering computing\\nsafety\\nsolution-oriented industrial society\\nhigh technological capabilities\\nnext-generation manufacturing systems\\nflexible productivity\\nhuman-robot collaboration\\nrobot revolution\\ncollaboration safety\\nsafety level\\niot technology\\nprogressing digitalization\\n4th industrial revolution\\nmanufacturing sites\\nrobot safety\",\"198\":\"task analysis\\nfrequency modulation\\nmotion segmentation\\npredictive models\\nprocess control\\nhuman-robot interaction\\nmanipulators\\nrobust method\\ntemporal properties\\nground truth data\\ntemporal aspec prediction\\ntime-constrained tasks\\nwiping\\ntable\\nvegetable chopping\\nfloor cleaning\",\"199\":\"task analysis\\nrobots\\nspraying\\naugmented reality\\ncalibration\\npaints\\nheadphones\\ncontrol engineering computing\\nfeedback\\nindustrial robots\\nmobile robots\\nhandheld spraying robot\\ntask awareness\\nmicrosoft hololens system\\nmotion capture system\\naugmented reality spraying task\\ncalibration routine\\naugmented reality interfaces\\nlogical approach\\ntarget regions\\nshared control methods\\nshared control spraying task\\ntime 4.0 s\",\"200\":\"task analysis\\ndecision making\\nknowledge representation\\nvisualization\\nrobot sensing systems\\ngrammar\\naugmented reality\\ndata visualisation\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nrobot programming\\ntemporal and-or graph\\nrobot program\\ninteractive robot teaching\\nar interface\\ncomprehensive visualizations\\ndecision making process\\nmicrosoft hololens\\ninteractive robot knowledge patching\",\"201\":\"optimization\\ntrajectory\\nangular velocity\\nsynchronization\\ndynamics\\nrobot motion\\ncomputer animation\\nhumanoid robots\\nimage motion analysis\\nmobile robots\\nmotion control\\noptimisation\\nrobot vision\\nconstrained optimization\\nreal-time synchronization\\nmotion re-targeting techniques\\nvirtual character animation research\\njoint angular velocities\\nre-targeted motion sequences\\nhumanoid robot\\njoint motion trajectories\\nverbal behavior synchronization\\nnonverbal behavior synchronization\\nverbal robot behavior\\nnonverbal robot behavior\",\"202\":\"task analysis\\nrobots\\ntrajectory\\nhidden markov models\\nload modeling\\nactuators\\nloading\\nexcavators\\nlearning by example\\nmobile robots\\nlearning from demonstration\\ndemonstration trajectories automatic segmentation\\nhydraulic actuated scaled excavator robot\\ncomplex truck loading task\\nnongeneric policy model\\nmapping continuous state action trajectories\\nexpert demonstration\\ntask-based instructional policy\",\"203\":\"shape\\nthree-dimensional displays\\nuser interfaces\\nsolid modeling\\nrobots\\nplanning\\nbuildings\\ncontrol engineering computing\\nfurniture\\nmobile robots\\nrobot dynamics\\nself-reconfigurable modular robots\\nshape-shift\\nself-reconfigurable furniture\\nintuitive user interface\\ntangible user interface\\nroombots shape\\n3d shape scanning\\nsrmr system\\ndeformable material\\nshape formation\",\"204\":\"skeleton\\nfeature extraction\\nthree-dimensional displays\\ntask analysis\\ntrajectory\\ncouplings\\nbiological system modeling\\ngesture recognition\\nimage motion analysis\\nimage representation\\nobject detection\\nvectors\\nhierarchical model\\nbody parts\\nhuman action recognition\\nhuman actions\\nhuman skeleton\\ndiscriminative body-parts selection\\nfisher vectors\\nhierarchical representations\\nhierarchical rrv descriptors\\nrotation and relative velocity descriptors\",\"205\":\"three-dimensional displays\\ntwo dimensional displays\\ncolor\\npose estimation\\nrobot sensing systems\\ntask analysis\\nimage colour analysis\\nimage sensors\\nlearning (artificial intelligence)\\nservice robots\\nmonocular 3d pose estimation\\nservice robot\\ncolor images\\nrobust human keypoint detectors\\nrobotic task learning\\nrgbd images\\n3d human pose estimation\\nhuman teacher\\npr2 robot\",\"206\":\"collision avoidance\\nmanipulators\\nkinematics\\ncollaboration\\nsafety\\ntask analysis\\nhuman-robot interaction\\nmanipulator dynamics\\nmotion control\\npath planning\\nhuman arm motions\\nfenceless robot cells\\nsafety requirements\\nrobot motions\\ncontrol-oriented dynamic model\\nhuman-robot collaboration\\ncycle times\\nadmissible path velocity\",\"207\":\"three-dimensional displays\\nrobot sensing systems\\nfeature extraction\\nvisualization\\npipelines\\nreal-time systems\\ngraphics processing units\\nhuman computer interaction\\nmobile robots\\nobject detection\\nrobot vision\\nstereo image processing\\nmobile robotics\\n3d sensor types\\nindoor applications\\noutdoor applications\\noutdoor scenarios\\nindoor scenarios\\nsmaller robotic systems\\nsingle cpu thread\\nmultiple cpu cores\\nhuman interaction\\nrobotic applications\\nrobust person detection\\nrobust real-time 3d person detection\",\"208\":\"detectors\\ntraining\\nfeature extraction\\nmathematical model\\ncameras\\nproposals\\nconvolution\\ndriver information systems\\nimage classification\\nobject detection\\npedestrians\\nfstn\\npedestrian detection\\npedestrian feature generation\\nadvanced driver assistance systems\\nadas\\nfish eye spatial transformer network\\nfish eye images\\nfeature maps\\nrobust\\ndeformation\\neth\\nkitti pedestrian datasets\\nadversarial network\\nfish eye pedestrian detectors\",\"209\":\"skeleton\\ncameras\\nsensors\\ncorrelation\\niris recognition\\nfeature extraction\\nmotion segmentation\\nbiometrics (access control)\\ngesture recognition\\nimage fusion\\neoy\\nasynchronous timing\\ncoordinate calibration\\nenvironmental constraints\\niot applications\\nperson identification\\ngesture data\\nfusion algorithms\\ninertial measurement unit\\nwearable sensors\\n3d depth camera\\ndata fusion approach\\neye on you\\nremote pid\\nreliable pid\\nrecognition rate\",\"210\":\"cameras\\ndrones\\ntwo dimensional displays\\nthree-dimensional displays\\nimage reconstruction\\ntracking\\njoints\\ncalibration\\nimage motion analysis\\nimage sensors\\nmobile robots\\nrobot vision\\nmotion capture systems\\ncalibrated cameras\\nindoor environments\\non-board rgb camera\\nautonomously flying drone\\n3d human mocap\\ndrone-based system\\nhuman motion capture\\nconsumer drone\\nmotion reconstruction\\nreconstruction algorithm\",\"211\":\"autonomous vehicles\\nautomobiles\\nmachine learning\\nsafety\\nnavigation\\nlearning (artificial intelligence)\\nlearning systems\\nmobile robots\\npath planning\\nroad vehicles\\nunsignaled intersections\\ndeep rl\\nintersection handling problem\\ndeep reinforcement learning system\\noccluded intersections\\nactive sensing behaviors\",\"212\":\"roads\\nnavigation\\nautonomous vehicles\\ntrajectory\\nrobot sensing systems\\nreliability\\ncollision avoidance\\nleast squares approximations\\nmobile robots\\nrecursive filters\\nrobot vision\\nsensors\\nprior maps\\npositive societal impact\\nautonomous vehicle navigating\\nrecursive filtering approach\\nnavigate road networks\\nleast-squares residual approach\\nvehicle frame\\nsensor-based perception system\\nglobal navigation\\nsparse topological maps\\nmapless driving framework\\nautonomous navigation\\nautonomous driving technology\\ntransmit detailed maps\\nurban areas\\nrural environments\\nautonomous vehicle navigation\",\"213\":\"automobiles\\nstate estimation\\nrobot sensing systems\\ncurrent measurement\\nlaser radar\\nwheels\\nglobal positioning system\\nmobile robots\\nroad vehicles\\nmodular redundant sub-systems\\nlateral accelerations\\nlongitudinal accelerations\\nfl\\u00fcela driverless\\nonboard sensing\\nformula student driverless competition\\nsystem integration\\nautonomous racecar\",\"214\":\"vehicle dynamics\\nmachine learning\\nsensor fusion\\nroads\\ntime series analysis\\nlaser radar\\nbayes methods\\nconvolution\\nfeedforward neural nets\\nfiltering theory\\nintelligent transportation systems\\nmobile robots\\nmonte carlo methods\\nroad traffic\\ntime series\\ntraffic engineering computing\\nunsupervised learning\\ncomplex interactions\\ndynamic occupancy grid prediction\\nurban autonomous driving\\ndeep learning approach\\nlong-term situation prediction\\nintelligent vehicles\\ncomplex downtown scenarios\\nmultiple road users\\nmotor vehicles\\nbayesian filtering technique\\nenvironment representation\\ndeep convolutional neural network\\nspatially distributed velocity estimates\\nraw data sequence\\ninput time series\\nmultiple sensors\\nconvolutional neural networks\\nroad user interaction\\npixel-wise balancing\\nstatic cells\\ndynamic cells\\nunsupervised learning character\\npedestrians\\nbikes\\ndistributed velocity estimation\\nmonte-carlo simulation\",\"215\":\"radar tracking\\nlaser radar\\ncameras\\nthree-dimensional displays\\nsensor fusion\\nimage fusion\\nobject tracking\\noptical radar\\npedestrians\\nroad vehicles\\ntraffic engineering computing\\nroad scene evaluation subsystem\\nvehicle tracking\\nimage processing\\nlidar processing\\npedestrian tracking\\ndidi-udacity self-driving challenge datasets\\nfrequency 25.0 hz\",\"216\":\"automobiles\\ntraining\\nbrakes\\ngames\\nroads\\nphysics\\ncomputer architecture\\ncameras\\ncomputer games\\nimage colour analysis\\nlearning (artificial intelligence)\\ntraffic engineering computing\\ndeep reinforcement learning\\nmediated perception\\nobject recognition\\nscene understanding\\nlearning strategies\\nrgb image\\nforward facing camera\\ncar control\\nroad structures\\nend-to-end race driving\\nreinforcement learning algorithm\\nlegal speed limits\\nasynchronous actor critic framework\\nrally game\\ntemperature 3.0 c\",\"217\":\"automobiles\\nuncertainty\\nroads\\nacceleration\\nautonomous vehicles\\napproximation algorithms\\nrobot sensing systems\\ncollision avoidance\\ndecision making\\nmarkov processes\\nmobile robots\\noptimisation\\npath planning\\nroad safety\\nroad vehicles\\nroad users\\nscalable decision making\\nsensor occlusions\\npomdp solution techniques\\noptimal avoidance strategy\\ndecomposition method\\ncomputational cost\\npartially observable markov decision process\\nrobust navigation\\nautonomous driving\",\"218\":\"automobiles\\nplanning\\nmachine learning\\npredictive models\\nprediction algorithms\\nautonomous automobiles\\ndriver information systems\\nintelligent transportation systems\\nlearning (artificial intelligence)\\nmobile robots\\nprediction theory\\nrecurrent neural nets\\nroad safety\\nroad traffic\\nroad traffic control\\nfully autonomous cars\\ncomplex scenes\\ndynamic scenes\\ncar following scenarios\\nsituation assessment algorithm\\nlane changing\\nrecurrent models\\ndriver-assistance systems\\nbidirectional recurrent neural network\\nintelligent driver model\\nlane changes planning\\nmaneuvers planning\\ndriving situations classification\\ndeep learning architecture\\nlong short-term memory units\",\"219\":\"actuators\\nbuoyancy\\nbellows\\nsurges\\npistons\\nunmanned underwater vehicles\\nprototypes\\nautonomous underwater vehicles\\ndesign engineering\\nmathematical analysis\\nmechatronics\\nmobile robots\\noceanographic equipment\\nvariable buoyancy method\\nunderwater robots\\nmechatronic system\\nunderwater gliders robuoy\\nmetallic bellows\\nintegrated mathematical model\\nwings\\nopen loop performance\\npower efficient\\nactuated metallic bellows\\nparts fouling\\noptimized dimensions\",\"220\":\"friction\\nload modeling\\ntorque\\ngravity\\nstrain\\ngears\\nrobots\\nforce control\\nindustrial robots\\nposition control\\nvelocity control\\nstrain wave gears\\nrobotic joint\\nposition-dependent friction effects\\nload-dependent friction effects\\nsensorless force control\\nspeed-dependent friction effects\",\"221\":\"kalman filters\\nquaternions\\nrobot kinematics\\nservice robots\\nrobot sensing systems\\nestimation\\nend effectors\\nindustrial manipulators\\nleast squares approximations\\nrecursive estimation\\nleast-squares process\\nmultirate quaternion-based kalman filter\\nrecursive total least-squares\\ninertial parameters\\nrigid load\\ninertial sensors\\nrobot payload real-time identification\\nend-effector\\nindustrial manipulator\",\"222\":\"robot sensing systems\\nestimation\\ntrajectory\\neigenvalues and eigenfunctions\\nobservability\\ncovariance matrices\\ngradient methods\\nkalman filters\\nmobile robots\\nnoise measurement\\nnonlinear filters\\noptimisation\\npath planning\\nriccati equations\\ntrajectory control\\nestimation algorithm\\nog\\nnonnegligible process noise\\nlargest eigenvalue\\noptimal active sensing\\nmeasurement noise\\nnonlinear differentially flat system\\nonline gradient descent method\\noptimal trajectories\\nmaximum estimation uncertainty\\nkalman filter\\nonboard sensors\\nobservability gramian\\ninversely proportional\\nposteriori covariance matrix\\ncontinuous riccati equation\\nunicycle robot\",\"223\":\"cameras\\ncalibration\\nrobot vision systems\\nestimation\\nreluctance motors\\nkinematics\\nvehicle dynamics\\nangular measurement\\nencoderless gimbal calibration\\ndynamic multicamera clusters\\ndynamic camera clusters\\nmulticamera systems\\njoint angle measurements\\ntime-varying transformation\\nstatic camera\\nmotor encoders\\ntransformation chain\\nencoderless gimbal mechanism\\nonline estimation\",\"224\":\"robot sensing systems\\nangular velocity\\nmathematical model\\naerodynamics\\nhumanoid robots\\nlaser ranging\\nmobile robots\\nmotion control\\npendulums\\ngravity-driven pendulum launch\\ntwo degree of freedom robot\\nautonomous robot\\nmobile robot\\nacrobatic techniques\\nacrobatic capability\\nlaser range-finder\\nsomersaulting stunts\\ngymnastic arts\\nhuman scale acrobatic robot\\nstickman\",\"225\":\"laser radar\\ncalibration\\ndistortion\\nrobot sensing systems\\nthree-dimensional displays\\ndistortion measurement\\nmotion measurement\\ncameras\\nimage motion analysis\\noptical radar\\noptimisation\\nprobability\\nsampling methods\\nstereo image processing\\ninterpolated inertial measurements\\nlidar scan\\nlidar point-to-plane distances\\nupsampled preintegrated measurements\\nmotion distortion correction\\nprobabilistic framework\\nlidar-imu sensing system\\nmotion distortion\\non-manifold optimisation\\n3d lidar-imu calibration\",\"226\":\"three-dimensional displays\\nlaser radar\\nestimation\\ncomputer architecture\\nsensors\\nreliability\\nimage color analysis\\nfeedforward neural nets\\noptical radar\\nradar imaging\\nstereo image processing\\nlidar\\ncompact convolution module\\ndense stereo depth information\\nsparse 3d lidar\\ndeep convolutional neural network architecture\\nstereo fusion\\nhigh-precision depth estimation\\noff-the-shelf stereo algorithm\",\"227\":\"three-dimensional displays\\nstrain\\nobject recognition\\ndeformable models\\nmachine learning\\nfeature extraction\\nbuildings\\ndisasters\\nimage classification\\nlearning (artificial intelligence)\\nrobot vision\\npost-disaster urban areas\\nsearch-and-rescue robots\\ndeformed building element point clouds\\npoint network\\nsynthetically-deformed object datasets\\npoint sorting\\npoint coordinates\\nclassification network\\ndeformed building elements\\n3d class recognition\\npoint cloud input\\ndisaster relief operations\\npotentially-deformed objects\\nunstructured environments\\npoint cloud data\\n3d point cloud\\nphysical site information\",\"228\":\"task analysis\\nclutter\\nvisualization\\nmobile robots\\ncameras\\nrobot vision systems\\nimage classification\\nimage colour analysis\\nimage representation\\nlearning (artificial intelligence)\\nneural nets\\nobject recognition\\nrobot vision\\nmultiview object dataset\\nrgb-d camera\\ndeep convolutional networks\\nweb images\\nrobotic system\\nautonomous agents\\ngood visual perceptual systems\\nrobotic vision research communities\\nhuman-populated environments\\nreal-life robotic data\\nobject classification\\ndeep representations\\nobject recognition algorithms\\nreal-life application\\nmobile robot\",\"229\":\"cameras\\nrobot vision systems\\ndelays\\ncorner detection\\nvisualization\\nhumanoid robots\\nimage sensors\\nmobile robots\\nrobot vision\\ncontrolled-delay event camera framework\\ndynamic robotics\\nlow latency response\\nhigh dynamic range\\ninherent compression\\nvisual signal\\nreal-time performance\\noff-line datasets\\ncamera resolution\\nlatency-free operation\\nevent-driven framework\\nicub robot\\nalgorithm processing rate\\nactual event-rate\\nalgorithm performance\",\"230\":\"three-dimensional displays\\nshape\\ngeometry\\nsolid modeling\\ntools\\ncameras\\ndata mining\\nfeature extraction\\ninteractive systems\\nsolid modelling\\nmultiple-view point clouds\\ncuboids\\ninteractive system\\ninteractive image-guided geometry extraction\\ngemsketch\",\"231\":\"tensile stress\\nrobots\\nthree-dimensional displays\\nsolid modeling\\ntask analysis\\nshape\\ntools\\ncomputational geometry\\nfeature extraction\\nimage capture\\nimage colour analysis\\nimage representation\\ntensors\\ngeometric affordance\\ninteraction tensor\\ntensor field representation\\nbisector surface representation\\nsurface points\\ndirectional vectors\\ncognitive robots\\nautonomous robots\\nrgb-d sensors\",\"232\":\"navigation\\npipelines\\nyield estimation\\ncameras\\nrobot vision systems\\nlasers\\nagriculture\\nimage colour analysis\\nimage resolution\\nobject detection\\ngrape varieties\\nvineyard management\\nlow-cost navigation strategy\\nnavigation algorithm\\ngrape pictures\\nlow-cost autonomous system\\nrgb camera\\nrgb image processing\",\"233\":\"cows\\nfeature extraction\\nkinematics\\nlaser radar\\nrobot sensing systems\\nthree-dimensional displays\\naustralia\\nconvolution\\ndairy products\\ndairying\\nfeedforward neural nets\\ngait analysis\\nimage sampling\\nobject detection\\nveterinary medicine\\nkinematic gait features\\ncattle gait tracking\\nhealth issue\\nlocomotion score\\nwidespread commercial adoption\\nsensor configuration\\nflight sensors\\nrotary milking dairy\\nlameness detection systems\\ncattle kinematics\\ncnn\\ncartesian space\",\"234\":\"irrigation\\nrouting\\napproximation algorithms\\nrobot sensing systems\\nnavigation\\ncomputational complexity\\ngraph theory\\ngreedy algorithms\\nmobile robots\\norienteering problem\\nnp-hard\\nrouting algorithms\\nrobot assisted precision irrigation\\ntemporal budget\\npossible motions\\nspatially distributed sites\\nbattery charge\\noptimization problem\\nirrigation adjustments\\nrobots navigate\\ncommercial vineyard\",\"235\":\"agriculture\\nvegetation mapping\\nsemantics\\nreal-time systems\\nsoil\\nindexes\\ntask analysis\\nagricultural machinery\\nagrochemicals\\nconvolution\\ncrops\\nfeedforward neural nets\\nimage colour analysis\\nimage segmentation\\nindustrial robots\\nrobot vision\\nprecision agriculture robots leveraging background knowledge\\nprecision farming robots\\ncnn-based semantic segmentation\\ncrop fields\\nsugar beet plants\\nrgb data\\nvegetation indexes\\nreal-time semantic segmentation\\ntrigger weeding actions\\nagricultural robot operator\",\"236\":\"cameras\\nirrigation\\nrobot vision systems\\nmanipulators\\ngrippers\\ncomputer vision\\nmobile robots\\nbuild-in hand camera\\nmodular emitter localization device\\nlightweight emitter localization device\\ntoyota hsr mobile manipulator robot\\ncommercial buildings\\nindoor plants\\ntoyota hsr robot\\nindoor drip irrigation emitters\\nemitter axis\\ngripper axis\",\"237\":\"friction\\nforce\\nmathematical model\\ncable shielding\\ndc motors\\npower cables\\nrobots\\nactuators\\ncables (mechanical)\\nlubrication\\nmobile robots\\nwearable robots\\ntwisted string actuation\\nconduit\\nwearable actuation\\ntsas\\nmodern engineering\\nrobotic applications\\nconventional cable sliding transmission\\nlubricated twisting\",\"238\":\"robots\\ntendons\\npulleys\\nrouting\\nsprings\\ngrasping\\nactuators\\ncompliant mechanisms\\ndesign engineering\\nelasticity\\nmanipulator dynamics\\nmotion control\\nstiffness decomposition\\ndesign optimization\\nsystematic design framework\\nunder-actuated tendon-driven robotic systems\\nfree motion\\ncontact task\\nconfiguration space\\nuatd robotic systems\\nactuation\\nactive tendons\\nun-actuated space\\npassive compliance\\nuatd robotic finger\\ncontact wrench\",\"239\":\"tendons\\nforce\\nexoskeletons\\nrobots\\nelectron tubes\\ntask analysis\\nthumb\\nbiomechanics\\nfatigue\\northotics\\npatient rehabilitation\\nthree-dimensional printing\\n3d-printed artificial finger\\nmoment arms\\nspasticity\\nwearable tendon-driven hand orthosis\\ntransmission efficiency\\nlow-profile design\\neffective transmission mechanisms\\nstroke patients\\njoint angle characteristics\\nfinger joints\\neffective force transmission\",\"240\":\"robots\\noptimization\\naerospace electronics\\nbayes methods\\nshape\\nangular velocity\\nlibraries\\ngait analysis\\ngaussian processes\\nlegged locomotion\\nmonte carlo methods\\nmotion control\\npattern classification\\nregression analysis\\nsignificant control challenges\\nhigh-dimensionality\\nnonlinear nature\\neffective parameterization\\nrhythmic gaits\\nperiodic control signals\\nrhythmic control signals\\ngait parameters\\nparameter space\\nbayesian optimization\\nparameter sample\\ngait discovery process\\nspherical tensegrity locomotion\\ntensegrity robots\\nrigid elements\\nsoft elements\\nlocomotion capabilities\\ncentral pattern generators\\ngaussian process regression model\",\"241\":\"cameras\\nthree-dimensional displays\\nimage edge detection\\nrobustness\\nrobot vision systems\\nsolid modeling\\nestimation\\nfeature extraction\\ngradient methods\\nhough transforms\\nslam (robots)\\n3d line map\\ncomplicated six degrees of freedom search\\n2d line information\\nline-based global localization\\nspherical hough representation\\n6 dof localization process\\nmanhattan world assumption\\n3d-2d line correspondences\\nspherical-gradient filtering\\nspherical image\\nindoor environment\\ncamera position\\nglobal environmental information\\nspherical camera\\nindoor localization\\nindoor spaces\",\"242\":\"sensors\\noptimization\\nwireless sensor networks\\nlattices\\ngenetic algorithms\\nradio frequency\\nrf signals\\nindoor radio\\nmobility management (mobile radio)\\noptimisation\\nrf beacons\\nrf signal propagation\\nindoor infrastructure\\ncost function\\nradio frequency beacons\\nindoor localization\\nbeacon deployment\\nbluetooth low energy (ble)\\nk-coverage\\nwireless sensor networks (wsn)\",\"243\":\"distance measurement\\nsensors\\nantenna measurements\\nrobustness\\niron\\nrobots\\nkalman filters\\naircraft communication\\naircraft navigation\\naltimeters\\nautonomous aerial vehicles\\nhelicopters\\nnonlinear filters\\nposition control\\ntarget tracking\\nultra wideband communication\\nquadcopter\\nautonomous flight\\nextended kalman filter\\nuwb ranging measurements\\nonboard sensors\\noptical flow\\nuwb based communication capability\\nrobust target-relative localization\\nultra-wideband ranging communication\\nultra-wideband ranging sensors\",\"244\":\"estimation\\nvisual odometry\\nmathematical model\\ncameras\\nhistograms\\ngravity\\nmotion estimation\\ndistance measurement\\nestimation theory\\nimage matching\\nmatrix algebra\\npose estimation\\nrobot vision\\ndecoupled rotation\\noptimal inlier set\\nhistogram voting\\nransac step\\nkitti data set\\nroad driving scenarios\\nhomography formulation\\nexhaustive search\\nmotion hypothesis\\ntranslation estimation\\ndominant ground plane\",\"245\":\"feature extraction\\ntechnological innovation\\ncovariance matrices\\nrobots\\nupper bound\\nsafety\\nnoise measurement\\ncomputational complexity\\nmobile robots\\nnearest neighbour methods\\nrisk analysis\\nsensor fusion\\nassociation faults\\nnearest neighbor data association algorithm\\nnearest neighbor integrity risk evaluation\\nrobot localization\\nintegrity risk prediction\\nrobot navigation\\ndata association algorithms\",\"246\":\"observability\\njacobian matrices\\nsensors\\nposition measurement\\ngravity\\nrotation measurement\\ncurrent measurement\\ncameras\\ninertial navigation\\nmonte carlo methods\\noptical radar\\nradar imaging\\nradionavigation\\nsonar imaging\\nstereo image processing\\nmonte carlo simulations\\nvins\\nplane features\\nbearing measurements\\npoint features\\nbearing sensor\\nvision-aided ins\\ngeneric exteroceptive range\\ninertial navigation systems\\nobservability analysis\",\"247\":\"visualization\\nnavigation\\nrobots\\nmeasurement\\ncameras\\nfeature extraction\\ntask analysis\\nfeedforward neural nets\\nimage matching\\nimage retrieval\\nlearning (artificial intelligence)\\nmobile robots\\nobject recognition\\npose estimation\\nrobot vision\\nvisual place recognition\\nplace exemplars\\nrecognition method\\nomnidirectional cameras\\nvisual input\\nmatched place exemplar\\nclosest place exemplar\\nrelative distance\\nretrieved closest place\\nomnidirectional view\\npowerful o-cnn\\nomnidirectional cnn\\nvirtual world datasets\\nreal-world datasets\\nomnidirectional convolutional neural network\\ncamera pose variation\",\"248\":\"task analysis\\ngallium nitride\\nlighting\\ngenerators\\nimage recognition\\nfeature extraction\\nvisualization\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nslam (robots)\\nvisual perception\\nperception task\\nplace recognition tasks\\nsimultaneous localization and mapping\\nslam\\ncoupled generative adversarial networks\\ndomain translation task\",\"249\":\"robots\\ntask analysis\\ntime factors\\napproximation algorithms\\nprobability distribution\\nmeasurement\\nvehicle dynamics\\napproximation theory\\ncomputational complexity\\ngreedy algorithms\\nmobile robots\\nmulti-robot systems\\noptimisation\\nservice robots\\nn task arrivals\\nservice tasks\\nredeployment cost\\none-stage greedy algorithm\\nconstant-factor approximation algorithm\\nservice cost\\nmultiple service robots\\nautonomous robots\\nre-deployment algorithms\\ntask response optimization\\nnp-hard\",\"250\":\"task analysis\\nsearch problems\\ndecision making\\nplanning\\ntime factors\\nrobots\\nprobabilistic logic\\ncomputational complexity\\ncontrol engineering computing\\nmulti-agent systems\\nmulti-robot systems\\nprobability\\nrescue robots\\nsearch-and-rescue\\ntask allocation\\nprobabilistic reasoning\\ngazebo-based environment\\nmultiagent time-based decision-making\\nmohamed bin zayed international robotics challenge\\nnear-optimal decisions\\nagent action\\nallocated budget\\ntime constraints\\ndecentralized multiagent decision-making framework\\ntask selection\\nmissions present several challenges\\nrobotic applications\\naction problem\",\"251\":\"robot sensing systems\\nclustering algorithms\\ntask analysis\\nlakes\\nmulti-robot systems\\nkinematics\\ncomputational complexity\\nmobile robots\\npath planning\\nremotely operated vehicles\\ntravelling salesman problems\\nmultirobot dubins coverage\\naerial monitoring\\nsingle robot approaches\\nmultirobot approaches\\ndubins vehicle kinematics\\nenvironmental monitoring\\nmultirobot team\\ndubins vehicles\\nnp-complete problems\\nsalesman problem-k-tsp-formulation\\nautonomous surface vehicles\\nlarge scale coverage operations\\nmarine exploration\",\"252\":\"robots\\nvegetation\\ntask analysis\\ngenetic algorithms\\nresource management\\noptimization\\napproximation algorithms\\ncomputational complexity\\nmulti-robot systems\\ntrees (mathematics)\\nmultiobjective genetic algorithm\\nmultirobot complete coverage problem\\ntask-allocation\\nnumber-fixed problem\\nmultiobjective ga\\nmofint\\nsingle-objective time-limited complete coverage problem\",\"253\":\"trajectory\\nplanning\\nestimation\\nspace vehicles\\nuncertainty\\nroads\\ncomputational modeling\\ncollision avoidance\\nmarkov processes\\nmobile robots\\nmulti-robot systems\\npath planning\\nroad vehicles\\nmultipolicy decision-making\\ntraffic participants\\nplanned trajectory\\nego-vehicle\\nsafe trajectories\\nmultiple motion policies\\nreceding-horizon planner\\nsimulated multivehicle intersection scenarios\\njoint multipolicy behavior\\nautomated urban driving\\nurban environments\\nautonomous vehicle\\nmultiple motion hypothesis\\njoint behavior estimation\\nobservable markov decision processes\\nreceding-horizon control\\nreceding-horizon trajectory planning\",\"254\":\"robot sensing systems\\nrobustness\\nmutual information\\ncomputational modeling\\nmobile robots\\ntask analysis\\nbayes methods\\ncomputational geometry\\nenvironmental factors\\nfires\\nmobile radio\\nwireless sensor networks\\nenvironmental mapping\\nground truth distribution\\nvoronoi diagram\\nad-hoc communication\\nhuman safety\\nsatisfactory convergence\\nautonomous agents\\nmapping tasks\\nterrain elevation\\nphysical quantities\\nforest fires\\nhazardous chemical leakages\\nspatial map\\nmobile sensor networks\\ndecentralized manner\\ndisjoint regions\\nhardware failures\\nshort-range sensors\\nenvironmental parameters\\nrobust spatial mapping\\nbayesian approach\",\"255\":\"games\\nstochastic processes\\npredictive control\\nnash equilibrium\\noptimization\\nvehicle dynamics\\nprediction algorithms\\ngame theory\\ninformation theory\\nmobile robots\\nnonlinear control systems\\nremotely operated vehicles\\nstochastic systems\\nbest response model predictive control\\nautorally platforms\\nnonlinear stochastic systems\\ninformation theoretic model predictive control algorithm\\niterated best response\\ngame theoretic notion\\nautonomous control\\nautonomous ground vehicles\",\"256\":\"neurons\\ncontext modeling\\nhidden markov models\\nrobots\\ncomputational modeling\\nadaptation models\\ndata models\\nboltzmann machines\\nlearning (artificial intelligence)\\npattern classification\\ncontextual model\\ncontext layer\\nscene classification benchmark\\nnonincremental models\\ndeep incremental boltzmann machine\\ncontext modeling efforts\\nfixed structure\\nincremental deep model\",\"257\":\"adaptation models\\nmanipulator dynamics\\ndata models\\nacceleration\\ntask analysis\\nlearning (artificial intelligence)\\nmulti-robot systems\\nrobot programming\\ntraining transfer models\\nonline learning\\ninverse dynamics model\\nmodel learning\\ninter-robot knowledge transfer\\nmultirobot setting\\ntrajectory tracking tasks\\nrobot inverse dynamics model\\ntabula rasa learning\\nrobot learning\\ninterbotix phantomx pincher arm\\nkuka youbot arm\",\"258\":\"task analysis\\noptimization\\nprediction algorithms\\nrobots\\ntransforms\\ncomputational modeling\\npredictive models\\ngradient methods\\nlearning (artificial intelligence)\\noptimisation\\npattern classification\\nlearning rates\\nlearning process\\nmemory model\\noptimal learning rate landscape\\ntask specific optimization\\nmeta-learner\\ninternal memory\\noptimization tasks\\nmeta-learning algorithm speeds\\nlearning control tasks\\nonline learning settings\\ngradient behaviors\\ngradient-based optimizer\\nmnist classification\",\"259\":\"inverse problems\\ncomputational modeling\\ndata models\\npredictive models\\nadaptation models\\nrobots\\ncontext modeling\\nlearning (artificial intelligence)\\nmultiple solutions\\ninverse space\\nforward models\\npaired forward-inverse models\\nmultiple modules\\nlocal minima\\ntraining multiple models-that\\nmonolithic complex network\\nefficient alternative\\nmultiple simple models\\ncomplex models\\nunstructured environments\\ncombined prediction errors\\ncoupled forward-inverse models\",\"260\":\"strain\\ngallium nitride\\nforce\\ndeformable models\\nrobots\\ntraining\\ngenerators\\nfinite element analysis\\nimage colour analysis\\nimage reconstruction\\nmobile robots\\nrobot vision\\nrgb-d image\\nfinite element methods\\nsingle depth view\\nphysical finite element model simulator\\nautonomous robots\\ngenerative adversarial networks\\nbody deformation\\ndefo-net\",\"261\":\"convolution\\nsoft robotics\\nrobot sensing systems\\nvisualization\\nrecurrent neural networks\\ncameras\\ndexterous manipulators\\nmobile robots\\nobject tracking\\npath planning\\nrecurrent neural nets\\nsensors\\nbodily aware soft robots\\nexteroceptive sensors\\nproprioceptive sensors\\nbend sensors\\nvisual sensor\\nnonlinearity\\noctopus-inspired arm\\ncamera record\\narm capturing\\ninternal sensory signals\\nstacked convolutional autoencoder\\ncae\\nrecurrent neural network\\nrnn\\nmotion\",\"262\":\"mathematical model\\npredictive models\\nvehicle dynamics\\naerodynamics\\nrecurrent neural networks\\ntraining\\naircraft control\\nautonomous aerial vehicles\\ncontrol engineering computing\\nhelicopters\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npredictive control\\nrecurrent neural nets\\nrobot dynamics\\nrobot kinematics\\ntrajectory control\\nquadrotor dynamic model\\nmotion prediction\\ndynamic systems\\nlong horizons\\ndeep learning\\ndeep recurrent neural networks\\nquadrotor motion model\\ninitial system state\\nmotor speeds\\nprediction horizon\\nrecurrent neural network state initialization\\nquadrotor vehicle flights\\nindoor flight arena\\nhybrid network architecture\\nsystem identification methods\\nrobust state predictions\\ntime 2.0 s\\nfrequency 100.0 hz\",\"263\":\"safety\\ncontrol systems\\ncomputational modeling\\ngaussian processes\\nadaptation models\\nlyapunov methods\\nsystem dynamics\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nhelicopters\\nlearning systems\\nmobile robots\\nnonlinear control systems\\nprobability\\nuncertain systems\\ncomplex dynamical systems\\naccurate nonlinear models\\ndata-driven approach\\nlearning process\\nbarrier certificates\\nsafe learning\\nlearning controller\\nquadrotor dynamics\",\"264\":\"simultaneous localization and mapping\\nvisualization\\noptimization\\npose estimation\\ntrajectory\\nbandwidth\\ncameras\\ndata mining\\ngraph theory\\nimage sensors\\nmulti-robot systems\\noptimisation\\nrobot vision\\nslam (robots)\\ndecentralized visual slam system\\ndecentralized slam components\\ndata-efficient decentralized visual slam\\npose-graph optimization method\\ndata association scales\\nrobot count\\ndata transfers\\nrobots\\nmap data\\nvisual slam systems exchange\\nversatile cameras\\nlightweight cameras\\ncheap cameras\\nmultirobot applications\\nmapping\\nsupplementary material data\",\"265\":\"robots\\nintegrated circuits\\ncost function\\niterative methods\\nthree-dimensional displays\\nestimation\\napproximation theory\\ncomputer vision\\ngraph theory\\nimage reconstruction\\nleast squares approximations\\noptimisation\\npose estimation\\nslam (robots)\\nimportant optimization problem\\nmachine vision applications\\n3d slam\\ngraph corresponds\\npgo problem\\nrelative noisy observation\\npgo solvers\\nstate-of-the-art initialization methods\\nlow noise problems\\nmeasurement noise\\nhigh noise problems\\npgo optimization problem\\niterative least-squares method\\nlinear least square initialization method\\npose graph optimization\\nleast-square problem\\nleast square\\ninitialization method\",\"266\":\"three-dimensional displays\\nlaser radar\\nsimultaneous localization and mapping\\ntwo dimensional displays\\niterative closest point algorithm\\nobservability\\ncollision avoidance\\nmobile robots\\noptical radar\\nremotely operated vehicles\\nroad traffic control\\nrobot vision\\nslam (robots)\\nstereo image processing\\nrobotics community\\nstereo cameras\\ndepth sensors\\nvelodyne lidar\\nautonomous driving\\nlow-drift slam algorithm\\n3d lidar data\\nscan-to-model matching framework\\nspecific sampling strategy\\nlidar scans\\nvelodyne hdl32\\nvelodyne hdl64\\nglobal drift\\nimls-slam\\n3d data\\nlocalized lidar sweeps\\nimls surface representation\\nimplicit moving least squares\\nsize 4.0 km\\nsize 16.0 m\\ntime 10.0 year\",\"267\":\"simultaneous localization and mapping\\nheuristic algorithms\\nsmoothing methods\\nsparse matrices\\nclustering algorithms\\nreal-time systems\\nerror analysis\\nmatrix decomposition\\nmobile robots\\nslam (robots)\\nfixed computational budget\\ndynamic variable reordering algorithm\\napriisam\\nreal-time smoothing\\nonline robots\\nincremental slam algorithms\\nbatch algorithms\\nabsolute error\\nincremental cholesky factorizations\\nmarginalization order\\nisam\\nre-linearize\",\"268\":\"topology\\njacobian matrices\\ncovariance matrices\\ngaussian distribution\\nsimultaneous localization and mapping\\napproximation methods\\napproximation theory\\ngraph theory\\nmobile robots\\noptimisation\\npath planning\\npose estimation\\nslam (robots)\\npose graph node marginalization\\nlongterm localization\\nlongterm mapping\\nlongterm navigation\\npose graph structure\\nabsolute-to relative-pose spaces\\npose-composition approach scaled version\\napproximate subgraph\\nfast nonlinear approximation method\",\"269\":\"state estimation\\nvisualization\\noptimization\\npipelines\\nhardware\\nrobot sensing systems\\naerospace robotics\\ncameras\\nglobal positioning system\\nimage capture\\nmobile robots\\npose estimation\\nrobot vision\\nmonocular visual-inertial odometry algorithms\\nstate estimation algorithms\\ncomputational constraints\\ninertial measurement units\\nvio algorithms\\nsingle-board computer systems\\nflying robots\\nimus\\nmotion capture\\nglobal positioning systems\\nmsckf\\nokvis\\nrovio\\nvins-mono\\nsvo-msf\\nsvo-gtsam\\nhardware configurations\\neuroc datasets\\nsix degree of freedom\\n6 dof\",\"270\":\"cameras\\noptimization\\nvisualization\\ngravity\\nmeasurement\\ngeometry\\nthree-dimensional displays\\ndistance measurement\\nimage motion analysis\\nimage sequences\\noptimisation\\npose estimation\\nimu information\\nintensity gradients\\nphotometric error\\nkey-point based systems\\nphotometric imu measurement errors\\nsparse scene geometry\\ncamera poses\\ndirect sparse visual-inertial odometry\\nvi-dso\\ndynamic marginalization\\nvisual-inertial system\\nimu data\\ngravity direction\",\"271\":\"three-dimensional displays\\ncameras\\noptimization\\nparallel processing\\nsimultaneous localization and mapping\\nrobustness\\nestimation\\nfeature extraction\\nmobile robots\\noptimisation\\npose estimation\\nrobot vision\\nslam (robots)\\nrotation optimization strategy\\nparallelism\\nglobal binding method\\nabsolute rotation\\nrelative rotation\\ntranslation optimization strategy leveraging coplanarity\\ncoplanar features\\nrelative translation\\noptimal absolute translation\\n3d line optimization strategy\\nstructural line segments\\nstructural features\\nstructural feature-based optimization module\\n3d map\\nstructural regularity\\noptimization strategies\\nmonocular slam systems\\nmanhattan world\\ncamera poses\",\"272\":\"visualization\\nrobot sensing systems\\nplanning\\ncomputational modeling\\npath planning\\nmobile robots\\noptimisation\\nrobot vision\\ntrees (mathematics)\\nvisual saliency-aware receding horizon autonomous exploration\\nreobserving salient regions\\nenvironment exploration rate\\nrobot endurance\\nrandom tree\\ntwo-step optimization paradigm\\nsalient objects\\npath planner\\nvisual attention\\naerial robotics\",\"273\":\"trajectory\\nstate estimation\\nplanning\\nnavigation\\ncameras\\nmeasurement\\ntask analysis\\naircraft control\\ncollision avoidance\\nmobile robots\\npath planning\\nrobot vision\\nperception-aware receding horizon navigation\\nmicroaerial vehicle\\nstate estimation uncertainty\\nperception-aware receding horizon approach\\nmonocular state estimation\\ncandidate trajectories\\nperception quality\\ncollision probability\\nreceding horizon navigation framework\\nimproved state estimation accuracy\\ngoal-reaching task\\npurely-reactive navigation system\\nmav\",\"274\":\"simultaneous localization and mapping\\nthree-dimensional displays\\nvisualization\\nvocabulary\\nimage recognition\\nnavigation\\nautonomous aerial vehicles\\ndistance measurement\\ngeometry\\nmobile robots\\npath planning\\nrobot vision\\nstereo image processing\\n3d information\\nuav navigation\\nunmanned aerial vehicles\\nvision-based odometry\\nloop-closure detection\\nplace recognition framework\\nlocal 3d geometry\\nviewpoint-tolerant place recognition\\n2d information\\nhand-held datasets\\nperceptual aliasing\\nbinary features\",\"275\":\"cameras\\nvisualization\\nmathematical model\\nquaternions\\nunmanned aerial vehicles\\nreal-time systems\\naccelerometers\\naerospace components\\nangular velocity measurement\\nautonomous aerial vehicles\\ncollision avoidance\\nkalman filters\\nnonlinear filters\\npose estimation\\nrobot vision\\nstereo image processing\\nlanding maneuvers\\nwing model\\nprobability density function\\nmeasured deviations\\nnominal relative baseline transformation\\nrelative pose measurements\\nrelative perspective n-point problem\\ninertial measurement units\\nhighly accurate baseline transformations\\nflexible stereo\\nwide-baseline stereo vision\\nfixed-wing aerial platforms\\ncomputationally efficient method\\nvisual-inertial sensor rigs\\nfixed-wing unmanned aerial vehicle\\nestimated relative poses\\nhighly accurate depth maps\\nobstacle avoidance\\nlow-altitude flights\\nextended kalman filter\",\"276\":\"cameras\\nsensors\\nvisualization\\nreal-time systems\\nunmanned aerial vehicles\\nnavigation\\nsolid modeling\\naerospace computing\\nautonomous aerial vehicles\\ncomputer vision\\ncontrol engineering computing\\nimage sensors\\ninertial navigation\\nrendering (computer graphics)\\nvirtual reality\\nvision-based perception\\nvisual-inertial navigation algorithm\\nhigh-rate cameras\\nimage simulation system\\nphotorealistic camera simulation\\nagile maneuvering\\non-board inertial measurement unit\\nrapidly prototype computing\\non-board camera images\\nnvidia jetson tegra x1 system-on-chip compute module\\ninertial sensors\\nmicrouav platform\\nvision-in-the-loop control algorithms\\nagile microunmanned aerial vehicles\",\"277\":\"magnetic flux\\nmagnetic resonance imaging\\nmagnetic fields\\nsaturation magnetization\\nmagnetic hysteresis\\nforce\\npower supplies\\ncoils\\ncollision avoidance\\nelectromagnetic actuators\\nmagnetic devices\\nmicromanipulators\\nmicrorobots\\nmobile robots\\nnon-newtonian fluids\\nradiation pressure\\nthree-dimensional printing\\nelectromagnetic coil\\n3d printed magnetic yokes\\ndouble layer structure\\npower source\\nmicroscale robotic swimmer manipulations\\nhigh power hexapole magnetic tweezer system\\ntapering-tipped magnetic poles\\ncartesian coordinate system\\n3d micromanipulations\\nnewtonian fluid\\nnonnewtonian fluid\\nobstacle avoidance\",\"278\":\"robots\\nhead\\nvisualization\\ntarget tracking\\nservosystems\\nglass\\ncell motility\\ncellular biophysics\\nmanipulators\\nmedical robotics\\nmicroorganisms\\nposition control\\nservomechanisms\\nvisual servoing\\nrobotic sperm immobilization\\nvisual servo control\\nsperm velocity\\nsperm orientation\\nsperm tail positions\\nrobotic system\\nproximal sperms\\nsperm head\\nmotile cells\\nmotile sperm\\nrobotic immobilization\",\"279\":\"morphology\\nswitches\\nhead\\ntarget tracking\\nmicroscopy\\nrobots\\nbiological techniques\\nbiomedical measurement\\ncell motility\\nfiltering theory\\nimage reconstruction\\nimage segmentation\\nmedical image processing\\nprobability\\nmotile cells\\nautomation techniques\\nnoninvasive measurement\\nadapted joint probabilistic data association filter\\nmultisperm tracking\\ninherent inhomogeneous image intensity\\nquadratic cost function method\\ndic image reconstruction\\nsperm motility measurement\\ndifferential interference contrast imaging method\\nsperm morphology measurement\\nimage intensity\\nsperm subcellular structures\\nsingle sperm motility\\nsperm morphology parameters\\nillumination effect\",\"280\":\"steel\\noptical fiber networks\\nveins\\nmagnetic flux\\nthree-dimensional displays\\nelectromagnetics\\ntoroidal magnetic fields\\nbiomagnetism\\nbiomedical materials\\nblood vessels\\ncellular biophysics\\nferrites\\ngels\\nliver\\nmolecular biophysics\\nproteins\\ntissue engineering\\ncell viability\\nmultilayered structure\\ndifferent magnetic poles\\nmagnetic tweezer\\nmagnetizer\\nalginate gel fibers\\nmagnetic fibers\\nkinds veins\\nportal vein\\ncentral vein\\n3d cellular structure\\ntransporting required nutrients\\nmagnetic fields\\nvascular network\\nhepatic lobule-like\\ntemperature 22.0 degc\\ntime 3.0 d\",\"281\":\"probes\\ntools\\nforce\\nrobot sensing systems\\nstrain\\nbiological tissues\\nbiomedical optical imaging\\nendoscopes\\nimage resolution\\nimage segmentation\\nlaser applications in medicine\\nmedical image processing\\nmedical robotics\\noptical microscopy\\nprobe-based confocal laser endomicroscopy\\nrobotic endomicroscopy scanning\\nimage-quality metric\\nsensorless tissue motion tracking\\nex vivo porcine tissue validate\\nautonomous endomicroscopy scanning\\npcle robotic tool\\nnovel sensorless framework\\nsensorless approaches\\nendomicroscopy probe\\nrobotic manipulation\\ntissue deformation\\nprobe-tissue contact force\",\"282\":\"robots\\nhysteresis\\nmicroassembly\\nmicromechanical devices\\nvoltage control\\nbandwidth\\nactuators\\ncontrollability\\nmicrorobots\\nmobile robots\\nmotion control\\nmulti-robot systems\\nposition control\\ncontrol primitives\\ncontrol pulses\\nmicrorobotic systems\\ncontrol policy\\nplanar assembly\\nefficient control strategy\\nheterogeneous stress-engineered mems microrobots\\nefficient control framework\\ncontrollable microrobots\\ntheoretical control strategy\\nmultiple-shapes microassembly\\narbitrary initial configuration\\npower delivery waveform\\nnonholonomic unicycles\\nmultiple macroscale robots\\ndirect drive wheels\",\"283\":\"magnetic resonance imaging\\nmagnetic levitation\\nforce\\nmagnetic noise\\nmagnetic shielding\\nmagnetic devices\\nmagnetic separation\\nbiomagnetism\\nbiomechanics\\nbiomems\\nbrownian motion\\ncancer\\ncellular biophysics\\nforce control\\nmedical robotics\\nmicromanipulators\\noptical microscopy\\npatient treatment\\nposition control\\npredictive control\\nmagnetic bead\\ncell nucleus minor axes\\ncell nucleus major axes\\nstiffness polarity\\nsub-micrometer object\\ntissue level\\nuntethered technique\\n3d navigation\\nrobotic intracellular manipulation\\nforce-displacement data\\nbrownian motion-imposed constraint\\nhigh-resolution confocal microscopy\\nslow visual feedback\\ngeneralized predictive controller\\nsingle human bladder cancer cell\\npiconewton force control\\nsub-micrometer position control\\nmagnetic micromanipulation task\\nsize 0.7 mum\\nfrequency 1.0 hz\\ndistance 0.43 mum\",\"284\":\"visualization\\ntactile sensors\\ncameras\\ntask analysis\\nsurface topography\\ncovariance analysis\\nfeature extraction\\nimage recognition\\nimage texture\\nneural nets\\ntouch (physiological)\\ntactile data\\ncloth textures\\ngood recognition performance\\nperception performance\\ntactile sensing\\nshared representation space\\nfeature sharing\\ncloth texture recognition\\nmultimodal sensing ability\\ntactile images\\ndeep maximum covariance analysis\\nlearned features\\ndmca framework\\nunimodal data\\njoint latent space\\ngelsight sensor\\ndeep neural networks\\nsensing modalities\",\"285\":\"force\\ntactile sensors\\ndetectors\\nrobustness\\nspectral analysis\\ncalibration\\nforce sensors\\nneural nets\\nslip\\nlong short-term memory neural networks\\nhigh-quality slip detection\\ntactile technologies\\nelectro-mechanical resistance\\nsensing mechanics\\ntactile sensing\\nrobust slip detectors\\nsensor behavior\\nsensory responses\\nsensory data points\\nsystematic data collection process\\nrobust slip detection\\ntactile-based slip detection\\nsecondary force modulation protocols\\nhuman hand\\nmechanical transients\\ntactile afferents\\nslip detection\\nneural networks\\ndeep learning\",\"286\":\"pins\\ntactile sensors\\noptical sensors\\ndata visualization\\nbiomedical optical imaging\\nstrain\\ncalibration\\ncomputational geometry\\ncomputerised instrumentation\\ninference mechanisms\\npressure measurement\\npressure sensors\\ntransducers\\nvoronoi tessellation\\nvisualisation mode\\noptical tactile sensor\\nshear magnitude\\ntactile contact\\nobject grasping\\nmanipulation\\nperception\\ncontact location inference\\npressure location inference\\nshear location inference\\ntransducing method\\nlocal shear measurement\\ntactip\\ncomplex systems\\nrobot hands\",\"287\":\"robot sensing systems\\ntask analysis\\nfasteners\\ntrajectory\\nmotion segmentation\\nvibrations\\ndexterous manipulators\\ngraph theory\\nlearning (artificial intelligence)\\nmobile robots\\ntactile sensors\\nerroneous contact states\\nmanipulation demonstrations\\nmotor primitive\\nmanipulation task\\ninsertion tasks\\nlearned manipulation graphs\\nrobust manipulation executions\\nsensory goals\\nmultimodal sensory signals\\ncomplex contact manipulation tasks\\ncontact state\\ncontact state information\\nbarrett arm\\nbiotacs\\ncontact changes\",\"288\":\"kinematics\\nexoskeletons\\nrobot sensing systems\\nforce\\ncalibration\\ndesign engineering\\ndexterous manipulators\\nend effectors\\nforce control\\nforce measurement\\nforce sensors\\ngrippers\\nmanipulator dynamics\\nmedical robotics\\nposition control\\ntactile sensors\\ntorque\\nend-effector posture measurements\\nhuman internal grasp force variations\\nrobotic hand\\npassive hand exoskeleton\\nwearable manner\\nforces measurement\\nexosense\\nrobotic manipulators control\\nfingertip wearable force\\ntorque sensing system\",\"289\":\"robots\\nforce\\nactuators\\nmechanical cables\\nexoskeletons\\ntorque\\nsprings\\ncables (mechanical)\\ngait analysis\\niterative learning control\\nmedical robotics\\nmuscle\\northotics\\npatient rehabilitation\\npneumatic actuators\\nposition control\\npulleys\\nshafts\\nsprings (mechanical)\\ndouble-bar ankle-foot orthosis\\npost-stroke gait rehabilitation\\ndouble-bar afo\\nrehabilitation facilities\\npneumatic actuator\\nbowden cable force-transmission system\\nmodular joint system\\nmodular exoskeletal joint\\nmej\\nhollow shaft\\nafo's pivot\\nbowden cables\\ncontraction forces\\nactuation scheme\\nnested-cylinder pneumatic artificial muscle system\\npam\\nideal actuation system\\nexoskeletal robots\\nncpam houses\\ncable-tensioning spring\\ncable tension\\ncable stopper\\nankle-joint trajectory tracking performances\\nintegrated system\",\"290\":\"actuators\\ngears\\nlegged locomotion\\nknee\\ntorque\\nbrushless dc motors\\nartificial limbs\\ngait analysis\\nmotion control\\nopen loop systems\\ntorque control\\npowered knee-ankle prosthesis\\nlow-impedance actuators\\nhigh torque density actuators\\nlow-reduction transmissions\\nlow-speed motor\\nlow mechanical impedance\\nhigh backdrivability\\nrobotic prosthetic legs\\nnegligible unmodeled actuator dynamics\\npower regeneration\\nfree-swinging knee tests\\nopen-loop impedance control tests\\nintrinsic impedance\\njoint impedance\\ntorque feedback\\npowered knee-and-ankle transfemoral prosthetic leg\\nactuation styles\\nfree-swinging knee motion\\nsize 3.0 nm\",\"291\":\"knee\\nprosthetics\\nlegged locomotion\\nsprings\\nbatteries\\nforce\\nactuators\\nartificial limbs\\nbiomechanics\\nelasticity\\nmedical robotics\\nmotion control\\nrobot dynamics\\ntorque control\\nrobotic prosthetic knee\\npassive mode test\\npassive prosthesis\\nrotary motion\\nlinear motion\\nslider crank mechanism\\nknee angle\\nvariable transmission mechanism\\nsuknee\\nrobotic prosthesis\\nvariable transmission series elastic actuator\",\"292\":\"exoskeletons\\nlegged locomotion\\nfoot\\ncompanies\\noptimization\\nkinematics\\ngait analysis\\nmedical robotics\\noptimisation\\nrobot dynamics\\nstable walking gaits\\ndirect collocation optimization formulation\\nparaplegics\\ncrutch-less dynamic walking\\nlower-body exoskeleton\\nfrench start-up company wandercraft\\npartial hybrid zero dynamics framework\\nphzd\",\"293\":\"legged locomotion\\nhip\\nbelts\\nforce\\nactuators\\nthigh\\nbiomechanics\\nforce control\\ngait analysis\\nmedical robotics\\nmotion control\\nautonomous multijoint soft exosuit\\nhuman locomotion\\nassistive torques\\ngait assistance\\noverground walking\\nsoft exosuit assists\\nankle plantarflexion\\nhip flexion\\nhip extension\\nmobile actuation system\\nhigh assistive forces\\nforce profiles\\nwalking cycle\\ncontrol adaptation method\\nforce consistency\\npeak force\\ntarget force\\ncountry-course walking\\nrms error\\nhuman energy economy\",\"294\":\"legged locomotion\\nprototypes\\nfoot\\nactuators\\nforce\\nmechanical cables\\ngait analysis\\nmedical robotics\\npatient rehabilitation\\nhemiparetic gait\\nparetic ankle assistance\\noverground walking\\nheterogeneous gait patterns\\nmechanical assistance\\nclinical gait training\\noptimized soft exosuit\\npoststroke patients\\nsoft exosuits\\nsoft wearable robots\\nparetic ankle dorsiflexion\\nforward propulsion symmetry\\nimpaired paretic ankle plantarflexion\\nparetic ankle function\\nwalking deficits\",\"295\":\"force\\nadmittance\\nrobots\\ntrajectory\\nforce measurement\\nexoskeletons\\ntask analysis\\ndiseases\\nfeedforward\\nhandicapped aids\\nhuman-robot interaction\\nmedical robotics\\nmuscle\\nassistive admittance control algorithms\\ntrunk supporting exoskeleton\\nduchenne muscular dystrophy\\nhealth care\\nactive exoskeletons\\ndaily living\\ntrunk supporting robot\\nconstant parameters\\nvariable parameters\\ncontrol laws\\nvariable admittance controllers\\nstandard admittance\\nfeed-forward force\\nfitts-like experiment\",\"296\":\"actuators\\nmuscles\\nforce\\ncoils\\nrobust control\\nmagnetic cores\\nsprings\\ncontrol system synthesis\\nelectroactive polymer actuators\\npneumatic actuators\\nrobots\\nstate feedback\\nelectromagnetic soft actuator\\nlogistic constraints\\ndynamic actuator selection\\nnetworked soft actuators\\nsoft robotic systems\\ndynamic environments\\nrobust state-feedback control\\ncontrol input bounds\\nminimal actuator selection problem\\nphysical network\\nartificial muscle fiber\\nsoft actuator matrix\\nnetworked esas\\nsoft-body actuators\\nactuator selection algorithms\\nrealtime control\\nlightweight power sources\\nexternal stimuli\",\"297\":\"actuators\\nrobots\\nsafety\\nforce\\nsprings\\nshock absorbers\\nhuman-robot interaction\\nstability\\nembedded energy-aware actuators\\ncontrol algorithm\\ndiscrete-time computer\\ncommunication delays\\nmodel-free passivity\\nsafety layer\\ncomplex robotic systems\\nphysical human-robot interaction\\nsafety mechanism\",\"298\":\"robot sensing systems\\ncontext modeling\\nsemantics\\nprobabilistic logic\\nobject recognition\\ncontrol engineering computing\\ninference mechanisms\\nlearning (artificial intelligence)\\nmarkov processes\\nmobile robots\\nprobability\\nrobot dynamics\\nsensor fusion\\nspatial context disambiguation\\nprobabilistic mln-based model\\nincomplete knowledge\\nhigh-level task planning\\nsemantic spatial relations\\nrobot dynamic\\nhigh-level mln\\nmln probabilistic reasoning\",\"299\":\"simultaneous localization and mapping\\nrobot kinematics\\nphase change materials\\ntrajectory\\nrobustness\\nmerging\\nexpectation-maximisation algorithm\\ngraph theory\\nmobile robots\\nmulti-robot systems\\noptimisation\\nrobot vision\\nslam (robots)\\npcm\\nrobust multirobot map\\nrobust selection\\nrobust slam methods\\nmultirobot case\\npairwise consistency set maximization\\npairwise consistent measurement set maximization\\nodometry backbone\",\"300\":\"robot sensing systems\\ntask analysis\\nuncertainty\\nplanning\\nrobotic assembly\\nestimation\\nfeedback\\nmulti-robot systems\\nopen loop systems\\npath planning\\nsensors\\ntask-specific sensor planning\\nrobotic assembly tasks\\nsensory feedback\\ntask planning\\nopen-loop simulation\\ntask-specific uncertainty approximants\\nmultirobot planner\\nmultirobot tasks\",\"301\":\"trajectory\\nroads\\natmospheric measurements\\nparticle measurements\\nsensors\\nparticle filters\\ntwo dimensional displays\\ndistance measurement\\nglobal positioning system\\noptical radar\\nparticle filtering (numerical methods)\\nslam (robots)\\nmap-aware particle filter\\n2d lidar localization\\ngps localization\\nmap information\\nlocalization sensors\\nparticle filter framework\\nmap-matching\\nprior occupancy grid\\nvehicle localization\",\"302\":\"trajectory\\nreceivers\\ncameras\\nrobot vision systems\\nbayes methods\\nestimation\\ndecoding\\nestimation theory\\nimage classification\\nkalman filters\\nmonte carlo methods\\nrobot vision\\nonline bayesian estimation algorithm\\nmonocular camera\\nreceiver robot\\nsending robot\\nactive motion-based communication\\naccurate trajectory classification\\ntrajectory class distribution\\nactive vision-based control policy\\nmessage decoding\\ntrajectory identification\\nmonocular vision model\\nreceiving robot\",\"303\":\"planning\\nmanipulators\\ntask analysis\\nrecurrent neural networks\\ncollision avoidance\\nrobustness\\naerospace electronics\\nneurocontrollers\\nrecurrent neural nets\\nredundant manipulators\\nmanipulator control optimization\\nobstacle avoidance\\nrnn control\\nredundant manipulator motion planning\\nmotion planning\\nkinematic control\\nredundant manipulator\\nrobot\",\"304\":\"trajectory\\nelectron tubes\\nuncertainty\\nrobustness\\noptimization\\nadaptation models\\nadaptive control\\ncollision avoidance\\nmobile robots\\nnavigation\\nnonlinear control systems\\nparameter estimation\\nrobust control\\ntrajectory control\\nuncertain systems\\nvariable structure systems\\nrobust collision avoidance\\nplanning algorithms\\nrobots\\nunknown environments\\ncluttered environments\\nmodel uncertainty\\nexternal disturbances\\nnonlinear control theory\\ncasc\\nsafe trajectory\\ncomposite adaptive sliding controller\\nquadrotor navigation\",\"305\":\"perturbation methods\\noptimization\\nstandards\\nsmoothing methods\\nrobots\\njacobian matrices\\ntask analysis\\ndigital simulation\\nfast fourier transforms\\nfinite difference methods\\ngradient methods\\nhadamard transforms\\nlegged locomotion\\nnewton method\\noptimisation\\nrendering (computer graphics)\\nwalsh functions\\ntrajectory optimizers\\nnoisy dynamics\\nquasinewton optimizer\\nturning policies\\nnoise-tolerant structured exploration\\nblackbox optimization\\nparameter perturbation directions\\nstructured orthogonal matrices\\nstructured finite differences\\ncontinuous control tasks\\nagile walking learning\\nfast walsh-hadamard fourier transform\\nfwht fft\\ndrop-in noise-tolerant replacement\\nquadruped locomotion\\ndeep reinforcement learning\\nmujoco simulator\\n3d renderers\",\"306\":\"computational modeling\\napproximation algorithms\\noptimal control\\nplanning\\nrobots\\ntrajectory optimization\\nautonomous aerial vehicles\\niterative methods\\nlearning (artificial intelligence)\\nnonlinear control systems\\noptimisation\\npath planning\\npredictive control\\nsampling methods\\ntrajectory optimisation (aerospace)\\ndirect optimal control\\ncontrol policy\\noptimal state-control trajectories\\nnonlinear predictive controller\\nnonlinear optimization problem\\nmodel-based methodology\\ncontrol cycle\\nkinodynamic probabilistic roadmap\\nnonlinear solver\\nunmanned aerial vehicle\\nuav\\ncomplex dynamical systems\\nsampling-based planning\\npolicy learning\",\"307\":\"acceleration\\nrobots\\nforce\\ncost function\\ndynamics\\ncontrol engineering computing\\nend effectors\\nhumanoid robots\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmotion control\\nquadratic programming\\nrobot dynamics\\ntelerobotics\\nforward dynamics\\ngreedy optimization\\nfeature-based control\\ncontrol policies\\ntrajectory optimization\\ngoal directed dynamics\\ngeneral control framework\\nlow-level optimizer\\ndynamical system\\nhigh level command\\nend-effector poses\\nsoft-constraint physics model\\nquadratic programming framework\\nteleoperation\\nmujoco simulator\",\"308\":\"cost function\\napproximation algorithms\\nheuristic algorithms\\nregulators\\noptimal control\\nrobots\\naerospace electronics\\ncontrol system synthesis\\nfeedback\\nlinear quadratic control\\nmobile robots\\noptimisation\\nregression analysis\\nsearch problems\\nnonlinear dynamics\\nnonquadratic cost functions\\nfree-derivative algorithm\\nlocal quadratic regressions\\nrobot motion policy\\nlocally-optimal control feedback policies\\nregression-based linear quadratic regulator\\nr-lqr\\nsearch space\\noptimization\",\"309\":\"trajectory\\ntorque\\nrobustness\\nperturbation methods\\nacceleration\\nmanipulator dynamics\\ncontrol system analysis\\nmobile robots\\npath planning\\nperturbation techniques\\nreachability analysis\\ntime optimal control\\ntorque control\\ntrajectory control\\ngeometric path\\ncontrol strategy\\ntracking controller\\npath parameterization problems\\ntime-optimal path tracking problem\\ntracking error regulation\\nreference trajectory\\ntracking performance degradation\\npath controller parameterization\\njoint torques\\ntorque bounds\\nperturbations\",\"310\":\"iterative methods\\nacceleration\\noptimal control\\nconvergence\\nmirrors\\ntrajectory\\nvehicle dynamics\\nconvergence of numerical methods\\ngradient methods\\nlearning (artificial intelligence)\\noptimisation\\npredictive control\\ngradient-based path integral method\\ninverse optimal control\\naccelerated path integral method\\ngradient descent\\niterative path integral method\\noptimization methods\\nmomentum-based acceleration\\nmomentum-based methods\\nnesterov accelerated gradient\\nsimulated control systems\\nmodel predictive control\\npath integral networks\\naccelerated pi-net\\nreinforcement learning\",\"311\":\"charging stations\\ntrajectory\\nbatteries\\nplanning\\nmobile robots\\ntask analysis\\ncomputability\\npath planning\\nindoor robotic application\\nbattery\\nsatisfiability modulo theory\\ncharging station placement problem\\nautonomous mobile robot\",\"312\":\"mobile robots\\nkinematics\\nmathematical model\\ntrajectory\\nobservers\\ndynamics\\ncontrol nonlinearities\\nrobot kinematics\\nrobust control\\nsteering systems\\nwheels\\nrobust multi-model off-road steering strategy\\npath tracking algorithms\\nbackstepping control strategy\\ntwo-wheel steering mobile robot\\ncontrol law\\ndynamic models\\nkinematic models\\nobserver\",\"313\":\"navigation\\nthree-dimensional displays\\nmaintenance engineering\\nimage edge detection\\ngeometry\\nsurface morphology\\npath planning\\ncomputational geometry\\nmesh generation\\nmobile robots\\nsolid modelling\\nvirtual reality\\nnavigation mesh generation\\nwalkable areas\\n3d virtual environment\\nwalkable environment\\nannotating traversable gaps\",\"314\":\"filtering theory\\nmobile robots\\nnearest neighbour methods\\npath planning\\nsampling methods\\ntrees (mathematics)\\ntopological nearest-neighbor filtering\\ncomputational techniques\\nlocality-sensitive hashing\\nworkspace connectivity\\nnearest-neighbor time\\nnearest-neighbor algorithm\\ncandidate neighbor configurations\\ntopologically relevant set\\nsampling-based motion planning algorithms\\nnearest-neighbor finding\\nsampling-based planners\",\"315\":\"measurement\\nheuristic algorithms\\ntrajectory\\ngeometry\\nrobots\\nsystem dynamics\\nplanning\\nlinear quadratic control\\nlinear systems\\nlinearisation techniques\\nmatrix algebra\\nmobile robots\\npath planning\\nrobot dynamics\\nrobot kinematics\\nsampling methods\\nsampling-based motion planning algorithms\\nsteering procedure\\nconfiguration space geometry\\nsample configurations\\nlocal system dynamics\\nconvex subsets\\nfree space\\nlocal behavior\\nlqr cost-to-go function\\nsystem linearization\\nlinear-gaussian system\\nsecond-order linear system\\ngram matrix\\nmahalanobis distance\\nkinematic unicycle\\nlocal geometry\\nmetric information\",\"316\":\"real-time systems\\nlegged locomotion\\ncomputational modeling\\nhumanoid robots\\nrobot sensing systems\\nkinetic energy\\ncollision avoidance\\nfriction\\nmobile robots\\nmotion control\\noptimal control\\nrobot dynamics\\nstability\\nfalling humanoid robot\\nenvironmental obstacles\\nobstacle geometry\\ncontact point\\nplanar dynamic model\\noptimal control approach\\nthree-link robot model\\nhand contact optimization\\ndarwin-mini robot\\nrealtime optimal control strategy\\nrealtime falling robot stabilization system\",\"317\":\"visual servoing\\ngrasping\\nsolid modeling\\nthree-dimensional displays\\nmathematical model\\nhumanoid robots\\nbayes methods\\nend effectors\\nleast squares approximations\\nmonte carlo methods\\nrecursive filters\\nstereo image processing\\nvisual perception\\nleast squares minimization problem\\nstereo vision\\nimage-based visual servo control\\nnonlinear constrained optimization problem\\nsequential monte carlo filtering\\nrecursive bayesian filtering technique\\nmarkerless visual servoing\\nicub humanoid robot platform\",\"318\":\"task analysis\\ndynamics\\nrobot sensing systems\\nmathematical model\\noptimization\\nhumanoid robots\\nmobile robots\\nmotion control\\nmulti-robot systems\\noptimal control\\nquadratic programming\\nhuman-humanoid collaborative tasks\\nmultirobot quadratic program controller\\nhuman dynamics reconstruction\\noptimal robot controls\\ninteraction motions\\ninteraction forces\\nhumanoid controller\\nco-manipulation tasks\\nrobot platform simulation\\noptimization problem\",\"319\":\"planning\\nhumanoid robots\\ntask analysis\\nmotion segmentation\\nrobot sensing systems\\ncomplexity theory\\nend effectors\\nmanipulators\\nmobile robots\\nmotion control\\nstability\\nend-effectors\\nmulticontact contact pose sequence planning\\nhumanoid robot armar-4\\nloco-manipulation tasks\\ncontacts\\nloco-manipulation affordances\\nvision-based detection\\nwhole-body multicontact tasks\\nmotion planning\\nmulticontact pose sequences\\ngoal-directed planning\\nend-effector contact opportunities\\nautonomous detection\\nwhole-body loco-manipulation actions\\nautonomous planning\\nhumanoid robotics\\nwhole-body pose sequence planning\\naffordance-based multicontact\",\"320\":\"dynamics\\nrobot sensing systems\\nkinematics\\nforce\\nmathematical model\\nbiomechanics\\nforce measurement\\nforce sensors\\nhumanoid robots\\nhumanoid robot\\ntorque measurement\\nexternal forces\\ndirect force measurements\\nregular force sensors\\nmodel-based estimator\\nfloating-base kinematics\\nfiltered measurement\\ncontact force\\nadditional estimation external force\\nmodel-based external force-moment estimation\",\"321\":\"wheels\\noptimization\\ndamping\\ntrajectory\\ncontrollability\\nmobile robots\\nlegged locomotion\\nmotion control\\noptimal control\\nrobot dynamics\\nvelocity control\\nnonintuitive optima\\ndynamic locomotion\\nacrollbot\\nlocally-optimal locomotion\\ntwo-link planar robot\\nunactuated wheel\\npassive wheel\\nground reaction forces\\nnet accelerations\\ndecelerations\\nbipedal robot locomotion\\ntoy system\\nlocomotion speed\\nactuation\\nforward velocity\\noptimization techniques\\ndirect collocation optimization framework\\nnonintuitive dynamic robot models\\nphysical robot parameterizations\\nlocomotion efficiency\\ndata-driven optimization\\ndynamically-stable locomotion\\nlegged rolling locomotion solutions\",\"322\":\"planning\\ncognition\\nestimation\\nforce\\nrobot sensing systems\\nmanipulators\\nmulti-robot systems\\npath planning\\nphysics reasoning\\nrobot manipulation\\nmanipulation planning\\nhuman-robot cooperation\\nmulti-robot cooperation\",\"323\":\"actuators\\nvisualization\\ntask analysis\\nrobot sensing systems\\ngrippers\\nfeature extraction\\ncontrol engineering computing\\nimage classification\\nlearning (artificial intelligence)\\nmanipulators\\nmotion control\\nrobot vision\\ntactile sensors\\nlearning modes\\nprehensile fingertip-based within-hand manipulation\\nactuator states\\nvisual data\\nsupervised learning techniques\\nclassification performance\\nextra trees\\ngradient boosting\\nvisual features\\nclassification rate\\nactuator loads\\nwithin-hand manipulation movements\\nhand\\/object system\",\"324\":\"kinematics\\ntask analysis\\ncost function\\nrobot kinematics\\nend effectors\\nquaternions\\ncontrol engineering computing\\nevolutionary computation\\ngradient methods\\nhumanoid robots\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nmotion control\\noperating systems (computers)\\nparticle swarm optimisation\\npublic domain software\\nrobot programming\\ntrees (mathematics)\\nfull-body motion generation\\nopen-source software package\\ninverse kinematics\\narbitrary kinematic trees\\nevolutionary optimization\\nparticle swarm optimization\\ncost functions\\nmultigoal manipulation tasks\\nserial kinematic chains\\ndual-arm manipulation\\nmultifinger hands\\nmemetic algorithm\\nfull-body motion specification\\nros\\nmoveit!\",\"325\":\"petri nets\\nrobot learning\\nmobile robots\\ncompounds\\ndynamics\\nreal-time systems\\nhumanoid robots\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nrobot dynamics\\nheavily case-specific engineering\\nubiquitous manner\\nhuman demonstration\\nlfd\\ndynamic skills\\ncomposite learning scheme\\nhuman definition\\nadvanced motor skills\\ndynamic time-critical maneuver\\ncomplex contact control\\npartly soft partly rigid objects\\nnunchaku flipping challenge\\nphysical success\\nrobot composite learning\\nhyper robot motor capabilities\",\"326\":\"uncertainty\\nstochastic processes\\nrobots\\ntorque\\ncost function\\nnoise measurement\\nrobustness\\ndexterous manipulators\\ngradient methods\\niterative learning control\\nstochastic systems\\nuncertain systems\\ndexterous manipulation tasks\\nmodel-based approaches\\nstochastic uncertainty\\niterative learning scheme\\nadaptive learning rate methods\\ndexterous in-hand manipulation\\ngradient descent-based iterative learning control\",\"327\":\"acceleration\\nkinematics\\ntask analysis\\nthumb\\nservice robots\\ndexterous manipulators\\nhumanoid robots\\nmanipulator kinematics\\nmechanical contact\\nfinger\\nlinks\\ncoupled joints\\nlagrangian mechanics\\nindependent joint angles\\ncontact kinematics\\nslip modes\\nangular accelerations\\njoint accelerations\\njoint torques\\nproportional-derivative law\\ndexterous manipulation\\nstick modes\",\"328\":\"grippers\\ntraining\\nrobot sensing systems\\npredictive models\\nacceleration\\ndexterous manipulators\\nend effectors\\nlearning (artificial intelligence)\\nslip\\ntactile sensors\\nextrinsic dexterity\\nactive slip control\\nmachine learning methodology\\nrobot dexterity\\nrecent insights\\ndeep learning\\ntactile sensor information\\nmanipulated object\\nrobot end-effector\\ndeep predictive models\",\"329\":\"three-dimensional displays\\ntwo dimensional displays\\nautomobiles\\npipelines\\nsolid modeling\\nproposals\\nlaser radar\\nconvolution\\nfeature extraction\\nfeedforward neural nets\\nmobile robots\\nrobot vision\\ntraffic engineering computing\\n2d detection network\\ngeneralised car models\\ntwo-stage convolutional neural network\\n3d detection algorithms\\ngeneral pipeline\\nautonomous driving\\nflexible pipeline\\n3d point cloud\\nmodel fitting algorithm\\n3d box detection\\n2d vehicle detection\",\"330\":\"image reconstruction\\nfeature extraction\\ncameras\\nthree-dimensional displays\\nlasers\\nimage color analysis\\ntraining\\nerror correction\\ninverse problems\\nlearning (artificial intelligence)\\nstereo image processing\\n3d reconstruction accuracy\\nsensors\\nlaser reconstruction\\ndeep network architecture\\nstereo image reconstruction\\ntwo dimensional inverse-depth image extraction\\n2d inverse-depth image extraction\\nrmse\\ndepth reconstructions\\ndepth estimate errors\\nmachine learning technique\\nlearnt error correction\",\"331\":\"estimation\\ntwo dimensional displays\\nsemantics\\nimage resolution\\nthree-dimensional displays\\ncameras\\ncomputer vision\\nconvolution\\nfeedforward neural nets\\nimage colour analysis\\nstereo image processing\\ncnn-based methods\\ndensemapnet\\ndeep convolutional neural networks\\nrepetitive regions\\ntextureless regions\\nstereo vision\\ncolor stereo images\\ndense networks\\ndisparity map\\nautoencoder method\",\"332\":\"image segmentation\\nsemantics\\nsilicon\\nobject segmentation\\nrobots\\ntraining\\nthree-dimensional displays\\ngeometry\\nimage colour analysis\\nimage representation\\nlearning (artificial intelligence)\\nobject detection\\njoint geometric\\nindoor scenes\\nunseen objects\\nnonobject surfaces\\nscene semantics\\nscene surfaces\\nunified energy function\\nhierarchical segmentation trees\\nrgb-d image\\ndeep learning-based methods\\nscenecut\\nconvolutional oriented boundary network\",\"333\":\"uncertainty\\nmeasurement uncertainty\\nrobots\\ncorrelation\\nbayes methods\\nrobustness\\ntraining data\\nimage classification\\npattern classification\\nbayesian viewpoint-dependent robust classification\\nlocalization uncertainty\\nrobust visual classification\\nblack-box bayesian classifier\\nlocalization error\\nspatial correlation\",\"334\":\"three-dimensional displays\\nshape\\nrobot sensing systems\\ngenerators\\nhistograms\\ntopology\\nface\\ncomputer graphics\\nimage resolution\\nsolid modelling\\nobject classification\\nobject detection\\n3d point cloud processing tasks\\nrgb-d dataset\\nspatial resolutions\\ntopological invariant encoding\\nglobal descriptor\\ntopologically persistent point signature\\nhomology groups\\ncompetitive 3d point cloud descriptor\\ntopological space\\n3d point cloud data\\nstpp\\ntime 3.0 d\",\"335\":\"pipelines\\nthree-dimensional displays\\nrobot sensing systems\\nimage segmentation\\ncameras\\nimage reconstruction\\nimage colour analysis\\nimage fusion\\nimage representation\\nlearning (artificial intelligence)\\nneural net architecture\\nobject detection\\nobject tracking\\npose estimation\\nrobot vision\\nvideo cameras\\nvideo signal processing\\nreconstruction techniques\\nground truth label generation\\nlabeled object instances\\nobject pose\\nvideo scene collection\\nannotation pipeline\\ndnn architecture\\nrgbd image\\nobject meshes\\nhuman assisted icp-fitting\\n3d dense reconstruction\\nrgbd camera\\npixelwise labels\\nspecific robotic manipulation task\\ntraining data\\ndnn pipelines\\nobject segmentation\\ndeep neural network architectures\\ncluttered scenes\\nreal rgbd data\\nlabel fusion\",\"336\":\"object detection\\nuncertainty\\ntraining\\nbayes methods\\nentropy\\ntask analysis\\nrobots\\napproximation theory\\nimage classification\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\nregression analysis\\nrobot vision\\nsampling methods\\ndropout sampling network\\ndropout variational inference\\napproximation technique\\nbayesian deep learning\\nmobile robot\\nversatile campus environment\\nrobotic vision\\nregression tasks\\nopen-set conditions\\nrobust object detection\",\"337\":\"neurons\\nrobot kinematics\\ntask analysis\\ntraining\\nteamwork\\ncognition\\ncontrol engineering computing\\ngroupware\\nhuman computer interaction\\nhuman-robot interaction\\nmedical computing\\nmedical robotics\\nneural nets\\nsurgery\\nteam working\\nuser interfaces\\nearly prediction capability\\nturn-taking actions\\nearly turn-taking prediction\\nspiking neural networks\\nhuman robot collaboration\\nhuman teamwork\\ncognitive turn-taking model\\nturn-taking prediction algorithms\\ncttm\\nrobotic scrub nurse\\nhuman turn-taking intentions\\nmultimodal human communication cues\",\"338\":\"ergonomics\\nhandover\\ncost function\\ntraining\\nmanipulators\\nhuman-robot interaction\\nhuman ergonomic preferences\\nhandovers\\nrobots\\nergonomic human grasping configurations\\nergonomic cost function\\nonline estimation problem\\nin-person user study\",\"339\":\"task analysis\\nplanning\\nrobot sensing systems\\nfootwear\\nfoot\\ncomputational modeling\\ncontrol engineering computing\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nplanning (artificial intelligence)\\nrobot programming\\nservice robots\\ndaily living assistance\\nrobot motion encoding\\nprogramming\\nshoe-dressing scenario\\nrobot verbosity\\nrobot speed\\nsafe living assistance\\ndressing assistance\\nadaptive hri\\nlow-level motion primitives\\nhigh-level symbolic planning\\nuser preferences\",\"340\":\"trajectory\\nadmittance\\nservice robots\\nrobot sensing systems\\nhidden markov models\\ntask analysis\\ncontrol engineering computing\\nend effectors\\nhuman-robot interaction\\nmobile robots\\nmotion control\\nrobot programming\\nend-effector\\ninitial trajectory\\npath tracking\\nadmittance parameters\\npassivity-based strategy\\ncoaching\\nprogramming techniques\\npassivity-based framework\\ndynamical movement primitives\\nadmittance control\\nhuman operator grabs\",\"341\":\"robot kinematics\\nrobot sensing systems\\ngeometry\\nforce\\ntopology\\nactuators\\ndesign engineering\\nmechatronics\\nmotion control\\noptimisation\\nposition control\\nrobot dynamics\\nvelocity control\\ntensegrity-inspired compliant three degree-of-freedom robotic joint\\ncontinuously soft materials\\nembedded sensing\\nposition information\\nvelocity information\\ngeometry selection\\noptimization\\ntheoretical configuration space\\nmechatronic design solutions\\nhardware prototype\\nlow order dynamic systems\\nsoft robotic systems\\nrobotic limb\\nomnidirectional compliance\\ntensegrity-inspired compliant 3-dof compliant joint\",\"342\":\"training\\nrobots\\nfeature extraction\\ntask analysis\\ncameras\\nvoltage control\\nrobustness\\nfeedforward neural nets\\nlearning (artificial intelligence)\\npose estimation\\nposition control\\nrobot vision\\nservomechanisms\\nvisual servoing\\n6 dof robot\\ndeep neural network-based method\\nconvolutional neural network\\nrobust handling\\nscene-agnostic network\\ndeep neural network training\\nreal-time 6 dof positioning\\npose-based visual servoing control law\\nocclusions\\nlighting variations\",\"343\":\"delays\\nhead\\ncameras\\nstereo image processing\\nthree-dimensional displays\\nvisualization\\nresists\\nhelmet mounted displays\\nmotion estimation\\ntelecontrol\\nvideo signal processing\\nvirtual reality\\nhead-mounted displays\\nimmersive experience\\nunbearable motion sickness\\nteleoperation session\\ndelay compensation approach\\nhead motion prediction\\nhead motion estimation\\nquality of experience\\ncommunication delays\\nmean compensation rates\\npan-tilt-unit-based stereoscopic 360 degree telepresence systems\\nteleoperation applications\\nquality-reducing effect\\nhead movement predictors\\nhead motion datasets\\n3d 360\\u00b0 video\\ntime 100.0 ms to 1000.0 ms\\ntime 3.0 d\",\"344\":\"search problems\\nthree-dimensional displays\\nsolid modeling\\npose estimation\\ncomputational modeling\\ntraining\\nimage segmentation\\nclutter\\nimage registration\\nmonte carlo methods\\nobject detection\\noptimisation\\nrendering (computer graphics)\\ntree searching\\nglobal point cloud registration techniques\\nglobal optimization process\\ncluttered scenes physically-consistent object poses\\n6d pose estimation\\nphysics-aware monte carlo tree search\\ncartesian product\\nmcts\\nrendering\\nupper confidence bound technique\\nucb technique\",\"345\":\"three-dimensional displays\\npredictive models\\ntransforms\\ncomputational modeling\\ndata models\\naerospace electronics\\ntraining\\ncameras\\nclosed loop systems\\ncontrol engineering computing\\ngradient methods\\nimage colour analysis\\nimage segmentation\\nindustrial robots\\nlearning (artificial intelligence)\\nminimisation\\nneural nets\\npose estimation\\nrobot dynamics\\nrobot vision\\nse3-pose-nets\\ndeep visuomotor control\\nse3-nets\\nencoder-decoder structure\\npose embedding\\npoint-wise data associations\\nclosed-loop control\\nscene dynamics\\nstructred deep dynamics models\\npose error minimization\\ngradient-based methods\\nbaxter robot\",\"346\":\"task analysis\\ntraining\\nrobot kinematics\\npipelines\\nrobot sensing systems\\nsemantics\\ngrippers\\nhumanoid robots\\nhuman-robot interaction\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\nobject detection\\npath planning\\nrobot vision\\nservice robots\\nrobotic picking\\ncluttered bins\\n2017 amazon robotics challenge\\narc\\nstorage system\\ndeep object perception pipeline\\ncustom turntable capture system\\ntransfer learning\\nrobot arms\\nnimbro picking\\nstow-and-pick task\",\"347\":\"robot sensing systems\\noptical fiber sensors\\noptical fibers\\npneumatic systems\\nelastomers\\nmanipulators\\nmean square error methods\\noptical sensors\\npneumatic actuators\\nregression analysis\\nstrain data\\nchamber pressures\\ngravitational tip loading conditions\\nsoft continuum robot\\npressure data\\nbase orientation\\ncombined optical sensor\\ncontrol methods\\nsoft pneumatically actuated robotic\\nsoft pneumatic manipulator motion\\noptically-diffuse elastomer sensors\\nstrain mode\\noptical sensors measure local strains\\naxial center\\noptical sensing method\\nsoft pneumatically actuated robotic manipulators\\nregression analyses\\nend-effector\",\"348\":\"semantics\\ncameras\\noptical imaging\\nvehicle dynamics\\nestimation\\ndynamics\\nmotion segmentation\\nimage motion analysis\\nimage reconstruction\\nimage representation\\nimage segmentation\\nimage sequences\\nstereo image processing\\nmonocular depth reconstruction\\ndynamic street scenes\\nsemantic information\\npixel-wise semantic segmentation\\ncamera motion\\noptical flow estimation\\ndepth reconstruction\\nstereo depth measurements\\nmonostixel model\",\"349\":\"cameras\\nurban areas\\nbenchmark testing\\nlenses\\ntraining\\nvisualization\\ndetectors\\ncomputer vision\\nimage recognition\\ntraffic engineering computing\\ndriveu traffic light dataset\\ntraffic light recognition\\nautonomous driving\\nuniversity of ulm traffic light dataset\\ndaimler ag\",\"350\":\"trajectory\\nlattices\\ntask analysis\\nspatiotemporal phenomena\\nvehicle dynamics\\nlearning (artificial intelligence)\\ncost function\\ncollision avoidance\\ndriver information systems\\nmobile robots\\nroad vehicles\\nspatiotemporal lattices\\npath planners\\nintelligent vehicles\\nmodel parameters\\ndemonstrated driving data\\ninverse reinforcement\\nirl methods\\nforward control problem\\ntraditional path-planning techniques\\nconformal spatiotemporal state lattice\\ndynamic obstacles\\nmodel assessment\\nirl framework\\nhighly dynamic environments\\nhighway tactical driving task\\ninstrumented vehicle\\ndriver behavior modeling\",\"351\":\"robots\\nvehicles\\npredictive models\\nhistory\\ncognition\\nprobabilistic logic\\nweaving\\ndecision making\\nhuman-robot interaction\\nintelligent transportation systems\\nlearning (artificial intelligence)\\nprobability\\nhighway on-ramp-off-ramps\\nhuman-robot interaction policies\\nmultimodal probabilistic model-based planning\\ntraffic weaving scenario\\nhuman-in-the-loop simulation\\ncandidate future robot actions\\ninteraction history\\naction distributions\\ndirect learning\\ncandidate robot action sequences\\nhuman responses\\nmassively parallel sampling\\nreal-time robot policy construction\\nhuman-human exemplars\\nfuture human actions\\nmultimodal probability distributions\\ninherent multimodal uncertainty\\nexperienced drivers\\nentering exiting cars\",\"352\":\"iterative closest point algorithm\\nacceleration\\nhistory\\nconvergence\\nthree-dimensional displays\\nrobots\\ntwo dimensional displays\\niterative methods\\npcl\\npoint cloud library\\nfixed point problem\\nicp implementations\\nstandard picard iteration\\niterative procedure\\nregistration\\nscan-matching\\nanderson acceleration\\niterative closest point\\naa-icp\",\"353\":\"three-dimensional displays\\nmutual information\\nrobustness\\nhistograms\\niterative closest point algorithm\\noptimization\\nfeature extraction\\ngraphics processing units\\nimage registration\\nsolid modelling\\nstereo image processing\\npoint clouds\\n3d voxel grid\\nparallel implementation\\n3d scan alignment\\n6 dof rigid body transformation\\n6-degree-of-freedom rigid body transformation\\nmi\",\"354\":\"databases\\nvehicles\\nautonomous automobiles\\nsemantics\\nadvanced driver assistance systems\\npublic transportation\\ntraining\\nautomobiles\\ndriver information systems\\nlearning (artificial intelligence)\\nobject detection\\nroad safety\\nroad traffic\\nvideo signal processing\\ntraffic near-miss incidents\\nself-driving cars\\nadvanced driver assistance system equipped vehicles\\ndangerous traffic\\nnormal drivers\\nnovel traffic database\\nmounting driving recorders\\nautomated systems\\ndatabase instances\\nlarge-scale traffic near-miss incident database\\nmonocular driving recorder\\nnidb traffic\\nprimary database-related improvements\\nnear-miss scenes\\nnear-miss detection\\ndrive video analysis\\nnear-miss incident\\nmotion representation\\nperformance level\",\"355\":\"aquatic robots\\npower supplies\\npropulsion\\ndielectric elastomer actuators\\nautonomous underwater vehicles\\nbiomimetics\\ncontrol system synthesis\\nelastomers\\nelectroactive polymer actuators\\nforce control\\nmobile robots\\nmotion control\\nremotely operated vehicles\\nvelocity control\\nthrust force\\nactuation layers\\nfin-like dielectric elastomer actuator\\ndea design\\nfish fins undulatory motions\\ntunable deas\\nsoft actuators\\nautonomous planar swimming\\nactuator designs\\nswimming speed\\nvertical swimming\\nunderwater operation\\nautonomous mobility\\nauv\\nminiature autonomous underwater vehicle\\nmodular dielectric elastomer actuator\",\"356\":\"shape\\nrobot sensing systems\\nsnake robots\\nlegged locomotion\\nforce feedback\\nrobot motion\\ncollision avoidance\\nfeedback\\nforce control\\nmotion control\\nparallel controller\\nbi-stable dynamical system\\nsnake robot\\nunevenly-spaced obstacles\\nproprioceptive controller\\nhexaprint robot\\nproprioceptive-inertial autonomous locomotion\\narticulated robots\\nproprioception\\nvestibular feedback\\ngait\\nforce sensing\",\"357\":\"optical sensors\\noptical imaging\\nnavigation\\nbiomedical optical imaging\\ninsects\\nneurons\\ncollision avoidance\\nfourier analysis\\nhelicopters\\nimage sequences\\nmobile robots\\nobject detection\\nrobot vision\\nsmall-field motion-sensitive interneurons\\ninsect visuomotor system\\nsmall-field object detection\\nartificial potential function-based low-order steering control law\\nsmall-field clutter\\nbio-inspired approach\\nautonomous robots\\nautonomous vehicles\\nbio-inspired navigation technique\\nfourier residual analysis\\ninstantaneous optic flow\",\"358\":\"legged locomotion\\nsoft robotics\\npneumatic systems\\nstrain\\nfabrication\\nglass\\nactuators\\nbending\\nmobile robots\\npneumatic actuators\\ninchworm-like locomotion\\npisrob\\npneumatic soft robot\\npneumatic actuation\\npneumatic system\\nsoft climbing robots\\nsoft parts\\nsystem development\",\"359\":\"magnetic heads\\nforce\\nrobot sensing systems\\nsoil\\nwires\\nfingers\\nagriculture\\nbending\\nbio-inspired materials\\nposition control\\nrapid prototyping (industrial)\\nrobots\\nthree-dimensional printing\\nthree-term control\\nplant-inspired robots\\n3d additive manufacturing\\nplant growth\\n3d printer-like mechanism\\ntubular body\\nmaterial deposition process\\nturning behavior\\nfilament height\\nposition pid control algorithm\\nhomogeneous structures\\nrobust structures\\ncontinuous growth\\nplotting velocity\",\"360\":\"copper\\nscanning electron microscopy\\nmanipulators\\nmechanical factors\\ncalibration\\ndeformation\\nfracture\\nnanomechanics\\nnanostructured materials\\nplastic deformation\\ntwinning\\ndeformation intertwine\\ndeformation twin\\nassembly method\\npositioning method\\nscanning electron microscope\\ncopper microwire in situ twisting test\\nmicromaterial\\nscaling effects\\nnanomaterial\\ncopper microwire sample fracture morphology\\ncopper microwire specimen\\ndegree-of-freedoms nanorobotic manipulator\\nnanorobotics manipulation system\\ncopper microwire mechanical properties\\nscaling effect\\nmicroelectron mechanical systems\\nsem\\nin-situ nanorobotic twisting\\ncu\",\"361\":\"optimization\\nbayes methods\\ntask analysis\\nvibrations\\nshape\\nmanuals\\nrobustness\\nassembling\\ndesign engineering\\noptimisation\\nproduction engineering computing\\nprototypes\\nregression analysis\\ntrap design\\nvibratory bowl feeders\\nindustrial part feeding\\nvbf design\\noptimal parameter\\npassive devices\\ndynamic simulation\\nmodified upper confidence bound\\nbayesian optimisation\\nkernel density estimation\",\"362\":\"robot sensing systems\\nrobot kinematics\\neducation\\nhidden markov models\\nmobile robots\\ntask analysis\\nlearning (artificial intelligence)\\nparticle filtering (numerical methods)\\nrobust control\\nmobile robot\\nreinforcement learning method\\ntask teaching\\nmicromouse type robot\\nteach-and-replay\\npfoe\\nparticle filter on episode\",\"363\":\"wires\\nuniversal serial bus\\nrobots\\ngrasping\\ngrippers\\nstrain\\nimage color analysis\\nclosed loop systems\\nindustrial manipulators\\nlyapunov methods\\nperipheral interfaces\\nrobot vision\\nstability\\nusb cables\\nvision-based controller\\nwire alignment\\nusb color code\\nvision-based robotic grasping\\nusb wires\\ntwo-level structure\\ndynamic stability\\nclosed-loop system\",\"364\":\"manipulator dynamics\\ntrajectory\\ngrasping\\nkinematics\\nacceleration\\nvisualization\\nautonomous aerial vehicles\\ncalibration\\ncollision avoidance\\ncompensation\\ngenetic algorithms\\nmanipulator kinematics\\nvisual grasping\\nlightweight aerial manipulator\\ncomplex kinematics\\/dynamics\\nmotion constraints\\nx8 coaxial octocopter\\n4-dof manipulator\\ngrasping control problem\\nnsga-ii method\\ntrajectory planning\\nkinematic compensation-based visual trajectory tracking\\ntrajectory generation\\ndynamic parameter calibration\",\"365\":\"proposals\\nthree-dimensional displays\\ntracking\\nlaser radar\\nsemantics\\ndetectors\\ntwo dimensional displays\\ncomputer vision\\nimage colour analysis\\nimage recognition\\nimage segmentation\\nobject detection\\nobject tracking\\nobject category\\ntracking-by-detection methods\\nsegmentation mask-based tracker\\npixel-precise masks\\ncategory-agnostic vision-based multiobject tracking\\ngeneric object proposals\\nclass-agnostic multiobject tracking\",\"366\":\"cameras\\nkernel\\nthree-dimensional displays\\nrobot vision systems\\nsimultaneous localization and mapping\\nmobile robots\\nrobot vision\\nservice robots\\nhome environments\\nfree space density\\navailable blueprint information\\nceiling vision\\nrobust localization information\\nrobotic vacuum\\nsuperior localization results\\nvision-based global localization\\nceiling space density\\nhomes\\nself-localize\\nman-made constructions\\ndocumented blueprint\\nrobot localization\\nsmart home applications\\nmovable objects\\ncomplicated task\\nhorizontal range-finders\\neffective global localization approach\",\"367\":\"three-dimensional displays\\nshape\\ntarget tracking\\nroads\\ntrajectory\\ncameras\\nimage motion analysis\\nobject detection\\nobject tracking\\noptimisation\\npose estimation\\ndata association method\\ntracking-by-detection framework\\nobject detectors\\nobject motions\\nonline multiobject tracking\\nobject shape\\nmonocular camera\\nobject pose\\nobject motion\",\"368\":\"grasping\\nrobots\\nadaptation models\\ndata models\\nfeature extraction\\nimage segmentation\\nneural networks\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nrobot vision\\nmultitask domain adaptation\\ndeep learning\\nsuccessful grasping probability\\ntransfer learning framework\\ndomain-adversarial loss\\ncandidate motor command\\nspecified target object\\ninstance segmentation mask\\nmonocular rgb images\\nneural network\\ncluttered scenes\\ninstance grasping\\nrobotic manipulation\",\"369\":\"task analysis\\nplanning\\nrobots\\ntrajectory\\ntracking\\nrobotic assembly\\ndynamics\\ncad\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nproduction engineering computing\\nsuboptimal control\\nautonomous robotic assembly\\nindustrial assembly tasks\\ncontact-rich manipulation skills\\nmotion planning approaches\\nrobot controllers\\nreinforcement learning\\nrobot skills\\ncontact-rich dynamics\\ncontrol policy\\nrobot executions\\nlocally suboptimal solutions\\nrl performance\\ncad design files\\ngeometric motion plan\\ncad data\\nassembly controller\\nmanufacturing trends\",\"370\":\"buildings\\nrobot sensing systems\\nwires\\nfabrication\\nsteel\\nbuildings (structures)\\ncad\\ngeometry\\nmobile robots\\nreinforced concrete\\nrobot vision\\nstructural engineering computing\\nwalls\\nin situ fabrication\\nvisual sensing system\\ncurved steel reinforced concrete wall\\nsteel wire mesh\\nbuilding construction\\ndigital building process\\ncad model\\nmaterial deformations\\nmobile robot\\nvision-based sensing\\nload-bearing\\nbuilding plan\",\"371\":\"fabrication\\nrobots\\nsolid modeling\\ntools\\nstandards\\nface\\nconnectors\\ndesign engineering\\nfurniture\\nmass production\\nmobile robots\\nproduct customisation\\nproduction engineering computing\\nmass customization\\nlaymen editable templates\\ncnc fabrication\\ntemplate based system\\nrobotic fabrication system\\nstandard carpentry tools\\nend-to-end design\\ntemplate design\\nlaymen users\\nrobotics system\\ndesign tools\\nrobot assisted carpentered items\",\"372\":\"planning\\ntrajectory\\nmeasurement\\ndata structures\\nsearch problems\\nlearning systems\\ncontainers\\nassembly planning\\ndesign for disassembly\\niterative methods\\niterative motion planning\\ncollision information\\nsubassembly identification\\npreemptive scheme\\nexhaustive scheme\\nsearch strategies\\nhierarchical approach\\ndisassembly sequence planning\\nparallelism\\npart separation techniques\",\"373\":\"friction\\nactuators\\norbits\\nvibrations\\nend effectors\\nfrequency control\\ndexterous manipulators\\nmanipulator dynamics\\nmotion control\\nplates (structures)\\nposition control\\nflat plate\\nmanipulator\\nactive-passive hybrid joint mechanism\\nsurface friction property\\n3- dof manipulation strategy\\nvirtual turntable\\n1-actuator 3-dof manipulation\\ndifferential friction surface\\nnonprehensile manipulation\\nsingle actuator\\nmanipulation strategy\",\"374\":\"magnetic fields\\nmagnetic domains\\nmagnetic heads\\nmagnetic flux\\nmagnetic resonance imaging\\npropulsion\\nbiomems\\nbiomimetics\\ncell motility\\nhydrodynamics\\nlaser materials processing\\nmagnetic actuators\\nmedical robotics\\nmicrofabrication\\nmicroorganisms\\nmicrorobots\\nnickel\\npermanent magnets\\nfish-like magnetically propelled microswimmer fabrication\\nglass substrate\\ndetoxification tools\\nbiosensing tools\\nfabricated microswimmers\\nmagnetic control system\\nexternal magnetic field\\noscillating uniform magnetic field\\nmagnetic actuation\\ncaudal fin\\n3d laser lithography\\nsize 50.0 nm\\nni\",\"375\":\"actuators\\ninsects\\nmicrocontrollers\\nhigh-voltage techniques\\ncapacitors\\nrobot sensing systems\\naerospace robotics\\nautonomous aerial vehicles\\navionics\\nelectronics packaging\\nfeedback\\nmicrorobots\\nmobile robots\\npower convertors\\nrobot dynamics\\nwire tethers\\nhigh-voltage power electronics\\nseverely constrained weight budgets\\nwireless liftoff\\nfast-turnaround laser based circuit fabrication technique\\nonboard electronics\\nhigh voltage bias\\ndrive signals\\ninsect scale aerial robots\\naerial vehicle\\npower electronics package\\nwireless robot\\nlaser-powered aerial vehicle\\nmicrocontroller\\nfeedback control\\nmass 190.0 mg\\nmass 104.0 mg\\nwavelength 976.0 nm\",\"376\":\"actuators\\nmagnetic fields\\nmagnetic resonance imaging\\npermanent magnets\\nmagnetic moments\\ntorque\\nmagnetic flux\\nmagnetic actuators\\nmicroactuators\\ntorque control\\nsoft miniaturized linear actuators\\nwirelessly powered microactuators\\nmagnet assembly\\nmagnetic field generator\\nexternally applied magnetic torque\\nsoft miniaturized actuator\\nmagnetic torques\\nuntethered miniaturized devices\\nwireless actuation\\nrotating permanent magnets\",\"377\":\"magnetic resonance imaging\\nmagnetic devices\\nmagnetic moments\\ntorque\\nmathematical model\\nforce\\nwireless communication\\nmagnetic fields\\nmedical robotics\\nmicrofluidics\\nmicrorobots\\nmotion control\\nmicroscale devices\\nmicrorobotic devices\\nindependent motions\\nmagnetic elements\\nindependent actuation\\nhomogeneous magnetic field input\\nmagnetic field signals\\nfield generation source\\nmagnetic microrobots\\ncomplex mechanism motions\\nmultiagent mechanism motions\\nstationary devices\\nmobile devices\\nmicrofactories\\nmicrofluidic tools\\nmedical procedures\\nremote applications\\nmillimeter-scale robotic devices\\nmagnetic mechanisms\\ndegrees-of-freedom remote actuation\\nsize 500.0 mum\\nsize 0.6 mm\",\"378\":\"feature extraction\\nimaging\\nsonar measurements\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nimage reconstruction\\nimage sensors\\nreliability\\nslam (robots)\\nsonar imaging\\npoint landmark identification\\nfeature-point extraction\\ngeneral-purpose method\\nplanar scene assumption\\nunderwater feature-based slam\\nunder-constrained landmarks\",\"379\":\"simultaneous localization and mapping\\nmeasurement\\ntrajectory\\nbenchmark testing\\nuser interfaces\\nc++ languages\\naugmented reality\\nautonomous aerial vehicles\\nmobile computing\\nmobile robots\\nnavigation\\nrobot vision\\nslam (robots)\\nvisual slam\\naugmented reality systems\\nnonfunctional requirements\\nmobile phone-based ar application\\ntight energy budget\\nuav navigation system\\nslambench2\\nbenchmarking framework\\nopen source\\nclose source\\nperformance metrics\\norb-slam2\\npublicly-available software framework\\nslam applications\\nslam systems\\nslam algorithms\\nmultiobjective head-to-head benchmarking\\nfunctional requirements\",\"380\":\"semantics\\nvisualization\\nrobustness\\ndatabases\\nimage recognition\\ncameras\\nneural networks\\nfeature extraction\\nimage matching\\nimage representation\\nimage sequences\\nmobile robots\\nneural nets\\nobject recognition\\npath planning\\nrobot vision\\nslam (robots)\\nsemantics-aware higher-order layers\\ndeep neural networks\\npure appearance-based techniques\\nplace categorization\\nplace-centric characteristics\\ncondition-invariant place recognition\\nrear view mirror\\nsemantic visual understanding\\nvisual places\",\"381\":\"laser radar\\nhistograms\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nmobile robots\\nnavigation\\noptical information processing\\noptical radar\\npath planning\\nrobot vision\\nintensity information\\ndelight\\ndistributed histograms\\nchi-squared tests\\ntwo-stage solution\\ngeometry-based verification\\nrange information\\ngps-denied areas\\nrobot position\\nkidnapped robot problems\\nmobile robotics\\nplace recognition\\nglobal localisation\\nintensity-based prior estimation\\nlidar intensities\",\"382\":\"feature extraction\\nsimultaneous localization and mapping\\nrobustness\\nheuristic algorithms\\nprobabilistic logic\\nvisualization\\ncompressed sensing\\nimage representation\\nobject detection\\nprobability\\nsensor fusion\\nterrain mapping\\nonline data association decisions\\nonline probabilistic change detection\\nsparse feature-based maps\\ncompact representation\\nstatic map features\\nfeature repeatability\\nprobabilistically principled approach\\nsparse mapping model\",\"383\":\"observability\\nrobot sensing systems\\ngeometry\\nmanifolds\\ncameras\\nestimation\\ngroup theory\\nkalman filters\\nlie groups\\nmatrix algebra\\nmulti-agent systems\\nmulti-robot systems\\nnonlinear filters\\nmultiagent formations\\ndynamic agents\\nalgebraic properties\\nfirst-order derivatives\\nnonlinear observability theory\\nhigher order derivatives\\nlocalization problem\\ndynamic bearing observability matrix nonlinear observability\\nmultiagent systems\\nrigidity matrix\",\"384\":\"casting\\ntable lookup\\nrobot sensing systems\\ntransforms\\napproximation algorithms\\nmemory management\\nacceleration\\ndata structures\\nmobile robots\\nparticle filtering (numerical methods)\\nray tracing\\nconstant time ray casting performance\\nparticle filter algorithm\\nresource-constrained mobile robots\\nlocalization approach\\napproximate 2d ray casting\\nmobile robot\\ncompressed directional distance transform\\ntwo dimensional occupancy grid maps\\nautonomous robots\\nthree dimensional lookup table\\nfrequency 40.0 hz\",\"385\":\"simultaneous localization and mapping\\ncollaboration\\nunmanned aerial vehicles\\noptimization\\nmeasurement\\ntrajectory\\nautonomous aerial vehicles\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nglobally consistent tracking\\nautonomous robot navigation\\nmonocular-inertial odometry\\nvision-based perception\\nmetric scale estimation\\nbenchmarking datasets\\nuavs\\nmonocular-inertial sensor suite\\nvisual-inertial collaborative slam\\ndrift correction\",\"386\":\"robots\\ntask analysis\\nuncertainty\\nprobability distribution\\nrandom variables\\nresource management\\napproximation theory\\nmonte carlo methods\\nmulti-robot systems\\nnormal distribution\\noptimisation\\nprobability\\nindependent normally distributed random events\\nconditional probability distributions\\nmultirobot task allocation problems\\ndeterministic method\\noptimisation method\\ntravel times\\ntask durations\\napproximation methods\\nresource contention modelling\",\"387\":\"markov processes\\ncollaboration\\nmonte carlo methods\\nbandwidth\\nproposals\\nentropy\\npower capacitors\\ndecision theory\\nintelligent control\\nmulti-agent systems\\nmulti-robot systems\\noptimal control\\noptimisation\\nmultiagent intelligent knowledge distribution\\ninfinite-horizon policy\\nminimal constraint guarantees\\nconstraint analysis\\ninformation content\\nmarkov chain monte carlo analysis\\nprobabilistic constraint satisfaction\\npartially observable markov decision processes\\naction-based constraints\\nmultiagent coordination\\nmultiagent systems\\ncommunication requirements\\nconstrained-action pomdps\",\"388\":\"task analysis\\nrobot kinematics\\nuncertainty\\nschedules\\nmarine vehicles\\nresource management\\nmobile robots\\nmulti-robot systems\\npath planning\\nprobability\\npotential contingency task\\nmultirobot mission planning\\nexpected mission completion time\",\"389\":\"robot sensing systems\\ntarget tracking\\napproximation algorithms\\npartitioning algorithms\\ncomputational complexity\\ndistributed control\\nmulti-robot systems\\no(hlog1\\/\\u03b5) communication rounds\\ndistributed simultaneous action\\nmultirobot multitarget tracking\\nmultirobot assignment problems\",\"390\":\"robot kinematics\\nfats\\ncogeneration\\ncollision avoidance\\nruntime\\ncomputational modeling\\ncomputational complexity\\ndeterministic algorithms\\ndistributed algorithms\\nmobile robots\\nmulti-robot systems\\nscheduling\\nfat autonomous robots\\ncoordination problems\\nautonomous mobile robots\\ndistributed robotics community\\nconvex hull\\nnontransparent fat robots\\ndeterministic distributed algorithm\\nsemisynchronous scheduler\",\"391\":\"robot kinematics\\nsynchronization\\ntask analysis\\nactuators\\nmathematical model\\nposition control\\nasymptotic stability\\nmanipulators\\nmobile robots\\nmulti-robot systems\\nrobot dynamics\\nsingularly perturbed systems\\nsingle rope-climbing robot\\nmultiple rope-climbing robots\\nclimbing robots\\nmotion control\",\"392\":\"grasping\\nrobots\\nclutter\\ngrippers\\nrobustness\\nproposals\\ntask analysis\\nimage classification\\nimage matching\\nobject recognition\\nrobot vision\\nrobotic pick-and-place\\nimage classification framework\\n2017 amazon robotics challenge\\nmit-princeton team system\\ncategory-agnostic affordance prediction algorithm\\ncross-domain image matching\",\"393\":\"task analysis\\nrobots\\nfeature extraction\\nneural networks\\nimage reconstruction\\ntraining\\nvisualization\\nlearning (artificial intelligence)\\nmanipulators\\nrecurrent neural nets\\nrobot vision\\nnonprehensile manipulation\\nrecurrent neural network\\nraw images\\nvae-gan-based reconstruction\\nautoregressive multimodal action prediction\\ncomplex manipulation tasks\\ntowel\\nweight\\nreconstruction-based regularization\\nvision-based multitask manipulation\\nend-to-end learning\\nmultitask learning\\nlow-cost robotic arm\\nrobot arm trajectories\\ncomplex picking and placing tasks\",\"394\":\"grasping\\nthree-dimensional displays\\nshape\\ngeometry\\nsolid modeling\\ntwo dimensional displays\\nrobots\\nconvolution\\ndexterous manipulators\\ngrippers\\nimage reconstruction\\nimage representation\\nintelligent robots\\nlearning (artificial intelligence)\\noptimisation\\nrecurrent neural nets\\nvirtual reality\\noutcome prediction model\\n3d shape modeling\\ndggn\\nanalysis-by-synthesis optimization\\n6-dof grasping net\\nsensory annotations\\ndata augmentation strategy\\ncnn\\n3d occupancy grid\\nmental geometry-aware representation\\ndeep geometry-aware grasping network\\n3d geometry prediction\\ngrasping interaction learning\\nparallel jaw gripper\\nrgbd input\\ninternal geometry-aware representation\",\"395\":\"task analysis\\nobject recognition\\nnatural languages\\nobject detection\\nfeature extraction\\nservice robots\\nindustrial robots\\ninteractive systems\\nlearning (artificial intelligence)\\nmanipulators\\nnatural language interfaces\\nnatural language processing\\nrobot vision\\nnatural language processing technologies\\nunconstrained spoken instructions\\ninstruction ambiguity\\nphysical industrial robot arm\\nnatural instructions\\nobject picking task\\nreal-world objects\\nunconstrained spoken language instructions\\nspoken natural language\\nhuman instructions\\ncomprehensive system\",\"396\":\"videos\\nrobots\\nfeature extraction\\ntask analysis\\nvisualization\\nrecurrent neural networks\\nlogic gates\\ncontrol engineering computing\\nconvolution\\nfeedforward neural nets\\nhumanoid robots\\nmobile robots\\nrecurrent neural nets\\nvideo signal processing\\nvideo translation\\ncnn\\nrnn\\nfull-size humanoid robot walk-man\\nmanipulation tasks\\ntranslation module\\nvisual features\\nencoder-decoder architecture\\nrnn layers\\ndeep convolutional neural networks\\ninput video frames\\ndeep features\\ncommand\\ndeep recurrent neural networks\\nrobotic manipulation\",\"397\":\"shape\\ndecentralized control\\naerospace electronics\\nrobot kinematics\\nadmittance\\nhardware\\ndecentralised control\\ndistributed control\\nlearning systems\\nmobile robots\\nmulti-agent systems\\nhighly cluttered evaluation environments\\ndecentralized control architectures\\ncentral pattern generators\\nspatially distributed portions\\narticulated bodies\\nsystem-level objectives\\nreinforcement learning\\nindependent agents\\nparallel environments\\nmeta-level agent\\nhomogeneous decentralized control\\narticulated locomotion\\ndistributed learning\\nasynchronous advantage actor-critic algorithm\\na3c\\ndecentralized control policies\\nindependently controlled portion\\nautonomous decentralized compliant control framework\\ncompliant control baseline\\narticulated mobile robots\",\"398\":\"task analysis\\nprogramming\\nrobots\\nsorting\\nsemantics\\ntopology\\ndata models\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nneural task programming\\nunseen tasks\\nsequential tasks\\nrobot manipulation tasks\\nbottom-level programs\\nhierarchical neural program\\nfiner sub-task specifications\\ntask specification\\nneural program induction\\nfew-shot learning\\nntp\\nnovel robot learning framework\\nhierarchical tasks\",\"399\":\"robots\\ntraining\\nadaptation models\\ntask analysis\\ntrajectory\\ndata models\\nrobustness\\ncalibration\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nrobot dynamics\\nsim-to-real transfer\\nrobotic control\\ndynamics randomization\\ntraining agents\\ntraining process\\nrobotic arm\\ncalibration error\",\"400\":\"navigation\\nvisualization\\nsimultaneous localization and mapping\\npath planning\\nthree-dimensional displays\\nplanning\\nmobile robots\\nrobot vision\\nslam (robots)\\nthree-dimensional topological map\\nnoisy sparse point cloud\\nconvex free-space clusters\\nglobal planning\\nmobile robotic platform\\ntopomap\\nvisual slam\\nvisual robot navigation\\nnavigation task\\nsparse feature-based map\\npath planning algorithms\\nvisual simultaneous localization and mapping system\",\"401\":\"simultaneous localization and mapping\\nhardware\\ncameras\\nfeature extraction\\nsynchronization\\nvisualization\\nimage fusion\\nrobot vision\\nslam (robots)\\nstereo image processing\\nsynchronisation\\nembedded simultaneous localization and mapping algorithm\\nmulti-core processor\\npublic visual-inertial datasets\\nperceptin robotics vision system\\nhardware co-design\\nadvanced visual-inertial slam system\\nstate-of-the-art visual-inertial algorithms\\nadditional sensor modalities\\ninertial measurements\\nvisual measurements\\nflexible sensor fusion approach\\npirvs software features\\nprecise hardware synchronization\\nglobal-shutter stereo camera\\npirvs hardware\\nvisual-inertial computing hardware\",\"402\":\"simultaneous localization and mapping\\nvisualization\\nthree-dimensional displays\\ncameras\\ndata structures\\nbenchmark testing\\nc++ language\\ngraph theory\\npublic domain software\\nrobot vision\\nslam (robots)\\nstereo image processing\\ngraph slam\\nc++ programming language\\nstandard libraries\\nlightweight open-source stereo visual slam system\\nprogrammer\\nproslam\\nalgorithmic aspects\\nmathematical aspects\\nhighly modular system\",\"403\":\"robot sensing systems\\ndistributed databases\\nplanning\\ntrajectory\\nvisualization\\nmetadata\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nslam (robots)\\ncooperative simultaneous localization and mapping\\ninter-robot loop closures\\ngeneral resource-efficiency communication planning\\nsensory data sharing\\ndistributed loop closure detection\\noptimal communication planning\\ncslam\",\"404\":\"cameras\\nrobustness\\nimage segmentation\\nmotion segmentation\\ndynamics\\nthree-dimensional displays\\nimage reconstruction\\nimage colour analysis\\nimage filtering\\nimage motion analysis\\nimage sensors\\nimage sequences\\nmotion estimation\\nobject detection\\nobject tracking\\npose estimation\\nprobability\\nrobot vision\\nslam (robots)\\nframe-to-model alignment\\n3d model estimation\\noutlier filtering techniques\\nmoving object detection\\ncamera pose tracking\\nprobabilistic static-dynamic segmentation\\nbackground structure reconstruction\\ndynamic scenes\\nstatic environments\\ndynamic sequences\\nstatic sequences\\ncamera motion estimation\\nweighted dense rgb-d fusion\\ncurrent rgb-d image pair\\nimplicit robust penalisers\\nbackground structure\\nrobust dense rgb-d slam\\nvisual slam\\ndynamic environments\",\"405\":\"planning\\nuncertainty\\ncameras\\nthree-dimensional displays\\ncollaboration\\noptimization\\npath planning\\ncollision avoidance\\ncovariance matrices\\nmobile robots\\noptimisation\\nrobot vision\\ntrees (mathematics)\\nmicroaerial vehicles\\ncollaborative path-planning framework\\nlocalization uncertainty\\ntwo-step planning framework\\nvisual-fidelity aerial vehicle simulator\",\"406\":\"cameras\\nthree-dimensional displays\\nvisual odometry\\noptimization\\nnavigation\\nrobot vision systems\\nhelicopters\\ninertial navigation\\nmobile robots\\npath planning\\nrobot vision\\nstate estimation\\nstereo image processing\\nsemidense visual-inertial odometry\\nquadrotors\\nswap constraints\\nautonomous navigation capabilities\\ndense 3d maps\\nindoor environments\\nvisual inertial state estimation\\nmicroaerial vehicles\\nsize, weight, and power constraints\\nstereo camera\",\"407\":\"approximation algorithms\\nthree-dimensional displays\\ntask analysis\\ntraveling salesman problems\\nlakes\\nanimals\\ncameras\\napproximation theory\\ncomputational complexity\\ntravelling salesman problems\\napex angle\\n3d traveling salesman problem\\nshorter tours\\ntilted cone-tspn problem\\nplanar surface\\napex points\\ninverted cone views\\nshortest tour\\norientation-varying view cones\",\"408\":\"optical distortion\\ndistortion\\noptical imaging\\nmirrors\\nadaptive optics\\ntwo dimensional displays\\nthree-dimensional displays\\nbiological organs\\nbiomedical optical imaging\\ncalibration\\nmedical image processing\\noptical tomography\\noptical coherence tomography\\noct geometric calibration method\\noct imaging system\\nspectral domain oct system\\n3d images\\ncalibration model\\nspectral distortions\\noptical path\\noct images formation\\noptical biopsies\\noct medical imaging system\",\"409\":\"surgery\\nstrain\\nliver\\ncameras\\ndeformable models\\nbiological system modeling\\naugmented reality\\ncomputerised tomography\\nimage registration\\nmedical image processing\\ntumours\\naugmented reality system\\nmarker-based method\\nliver resection surgery\\nrealtime tracking algorithm\\nnonrigid initial registration method\\npreoperative model\\nopen surgery\\nopen liver surgery\\nmarker-based registration\",\"410\":\"needles\\nsurgery\\nrobot sensing systems\\nvisualization\\ntools\\ncornea\\nbiomechanics\\nbiomedical equipment\\nbiomedical optical imaging\\neye\\nmanipulators\\nmedical robotics\\noptical tomography\\nrobot arm\\ngraft rejection risk\\nchronic immunosuppression comorbidities\\ncorneal transplantation\\npromising technique\\ndeep anterior lamellar keratoplasty\\ntime image-guided cooperative robotic\\nperforation-free needle depth\\ndalk needle insertions\\nreal-time oct segmentation\\nposterior corneal boundary virtual fixture\\noptical coherence tomography imaging\\nrobot-assisted solution\\ninadequate needle depth\\ncooperative control\",\"411\":\"needles\\nrobot sensing systems\\nmagnetic flux\\nanesthesia\\norbits\\neye\\nhall effect devices\\nkalman filters\\nmanipulators\\nmedical robotics\\nposition control\\nsensor arrays\\nsensors\\nsurgery\\nhall effect sensing workspace\\nnonpermanent magnetic needle\\neye anesthesia training system\\nrobotic experiments\\neye surgery\\nneedle tip tracking system\\nophthalmic anesthesia training\\nanesthesia needle\\nmagnetized needle tip\\nhall-effect sensor array\\norbital structure model\\nhall-effect sensors\\nophthalmic anesthesia pathway\\nneedle tip position\\ncommercial robotic manipulator\\ndeveloped system\",\"412\":\"needles\\nretina\\nsurgery\\nmicroscopy\\nrobots\\nprobes\\nbiological tissues\\nbiomedical equipment\\nbiomedical optical imaging\\neye\\nmedical image processing\\nmedical robotics\\noptical tomography\\nrobot-assisted subretinal injection\\nneedle tip localization\\nmicrosurgery\\nmicroscope-integrated intraoperative optical coherence tomography\\ninsertion depth\\nsubretinal visual feedback\\noptical coherence tomography images\",\"413\":\"manipulators\\nprobabilistic logic\\nrotation measurement\\ngrasping\\nshape\\ntask analysis\\nbayes methods\\ndexterous manipulators\\nhumanoid robots\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\nposition control\\nrobot kinematics\\nsensors\\ntorque measurement\\nnasa jet propulsion laboratorys\\nrobosimian\\njpl\\nsupporting manipulator\\ncumbersome objects\\ndata-driven bayesian models\\ninferred object properties\\ndual-arm lifting\\nbulky object\\ndual-arm grasping\\nproprioceptive inference\",\"414\":\"torque\\nactuators\\nrobot sensing systems\\ntorque measurement\\nstrain measurement\\ndisasters\\nelectromagnetic interference\\ngears\\nlegged locomotion\\nrescue robots\\nstrain gauges\\ntorque control\\nhigh performance torque-controlled actuators\\ndisaster response robot\\nscattered debris\\naxial compactness\\ntorque sensors\\nanalog digital converter board\\ndifferential control\\njoint torque\\ntorque ripple\\ntorque-controlled legged robot\\ndisaster environments\\nharmonic drive gear\",\"415\":\"robot sensing systems\\nforce sensors\\nforce\\nstrain\\nforce measurement\\nmathematical model\\nseals (stoppers)\\ntorque measurement\\nhigh dynamic range six-axis force-torque sensor\\nhdr six-axis force-torque sensor\\nstopper mechanism\\nmultistage six-axis force sensor\\nhigh dynamic range sensing\\nhdr measurement\\noverload protection mechanism\\nlow-rigidity flexure element\\nhigh-rigidity flexure element\",\"416\":\"principal component analysis\\ntactile sensors\\ndata visualization\\nsensor arrays\\npins\\nbiology computing\\ndata visualisation\\ntouch (physiological)\\nvibrissal arrays\\npca\\ntouch\\ncomplex robotic manipulation\\ntactile sensor arrays\\nvisualisation approach\\nk-nn\\neuclidean distance\",\"417\":\"force\\nmanipulator dynamics\\nbrushes\\nimpedance\\ndynamics\\nshape\\ncontrol system synthesis\\nforce control\\nmotion control\\nobservers\\nposition control\\nvariable structure systems\\ndisturbance observer\\nsliding mode control\\nsmc\\nposition-based force\\nftic\\nconstant contact force\\nforce tracking capability\\nforce-tracking impedance control\\n2-degree-of-freedom\\n2-dof wall-cleaning manipulator\",\"418\":\"surface impedance\\nrobot sensing systems\\nservice robots\\nsurface reconstruction\\nrendering (computer graphics)\\nart\\ncomputational geometry\\ncurve fitting\\nmanipulators\\nposition control\\nsplines (mathematics)\\nvectors\\nsurface-reconstruction\\nvector-graphics engine\\nb\\u00e9zier spline curves\\nartistic pen drawing\\nimpedance control\\nseven-degree-of-freedom manipulator\\npen strokes\\npen art\\nsemiautonomous robotic pen-drawing system\\nimpedance-controlled robot\\narbitrary surface\",\"419\":\"robot sensing systems\\ntransient analysis\\ntraining\\nswitches\\ntask analysis\\ndata models\\nforce sensors\\nindustrial robots\\nmanipulators\\nrecurrent neural nets\\ntorque\\nrecurrent neural network\\nrnn\\nindustrial robot\\nforce transient detection\\nrobot joint torques\\nforce sensor\\nrobotic manipulation\\ncontact force transients\",\"420\":\"force\\ngrammar\\nrobot sensing systems\\ntask analysis\\nmotion segmentation\\nunsupervised learning\\nimage segmentation\\npose estimation\\nsupport vector machines\\ntactile sensors\\nforce vectors\\nunsupervised manner\\nevent labeling sequences\\nmanipulation event segmentation\\nhierarchical models\\nhand-object interactions\\ncontact forces\\nunsupervised learning approach\\nmanipulation event parsing\\nlow-cost easy-to-replicate tactile glove\\ntemporal grammar model\",\"421\":\"5g mobile communication\\nconferences\\nautomation\\naustralia\\nartificial limbs\\ndexterous manipulators\\nmedical robotics\\nmobile robots\\nmotion control\\nposition control\\nwearable robots\\nhuman arm\\nhuman induced disturbances\\ndata-driven latent space impedance control method\\nlatent space impedance controller\\nwearable robotic fingers\\ndecoupled motion control\\nwearable extra limbs\\nhuman movement\\nself-standing robots\\nsingle-handed object manipulation\",\"422\":\"ink\\nfabrics\\ntactile sensors\\npiezoresistance\\nsilver\\naccelerometers\\ndexterous manipulators\\nforce sensors\\ngrippers\\nmanipulator kinematics\\nfabric based multilayered tactile sensor\\nhard materials\\nsoft materials\\nrobotic hand gripper\\nkinesthetic sensation\\ngrasped object orientation\\ntactile sensing\\nrigid inertial measurement units\\nobject orientation estimation\\nconductive silver ink\\nconductive fabric\\nfabric based sensors\\nhard surfaces\\nsoft surfaces\\nobject manipulation tasks\\nfabric tactile sensor\\nmultilayered sensor\\ntilt sensing\",\"423\":\"force\\nforce feedback\\nmanipulators\\nrobot sensing systems\\nskin\\nstrain\\nforce sensors\\nforce control\\nhaptic interfaces\\nmedical robotics\\nsurgery\\ntactile sensors\\ntelerobotics\\nmagnified force sensory substitution\\nteleoperation systems\\nkinesthetic force feedback systems\\nforce-controlled tactile skin deformation\\ntangential force\\nnormal force\\nsensory substitution device\\nskin deformation force feedback\\nmaximum stable kinesthetic force feedback\\nda vinci research kit teleoperation system\\nforce magnification\\ninteraction force\\nmagnified force feedback\\nforce feedback maximized performance\\nforce-controlled skin deformation feedback\\nmagnified kinesthetic force feedback\\nhaptics and haptic interfaces\\nsurgical robotics\\nlaparoscopy\\ntelerobotics and teleoperation\",\"424\":\"collision avoidance\\ncomputational modeling\\nkinematics\\npneumatic systems\\nsoft robotics\\nnavigation\\nmobile robots\\npath planning\\nobstacle-aided navigation\\nsoft growing robot\\nobstacle avoidance\\nrobot path planning\\nsoft robots\\nintentional obstacle collisions\\nsoft robot navigation\\nrobot-obstacle interaction\\ntip-extending soft robot\\nobstacle interaction model\\naccount obstacle collisions\",\"425\":\"color\\nstrain\\nactuators\\nrobot sensing systems\\nsignal generators\\nsoft robotics\\nimage colour analysis\\npneumatic actuators\\nrobots\\nthree-dimensional printing\\nsignal generator\\ncolor sensors\\nsoft pneumatic actuators\\nsoft actuators\\nmulticolor 3d printing\\nsoft robots\\nbending deformation\\ncolor-based sensing\",\"426\":\"force\\nactuators\\nsoft robotics\\nfriction\\nsteady-state\\nelectrodes\\nelastomers\\nelectric actuators\\nelectroactive polymer actuators\\nfeedback\\nfeedforward\\nmobile robots\\nmotion control\\nviscoelasticity\\ndielectric elastomer actuator\\nsoft crawling robot\\ninchworms\\nfeedforward plus feedback control scheme\",\"427\":\"shape\\nsoft robotics\\nstrain\\ncomputational modeling\\noptimization\\nnumerical models\\ndeformable models\\ncalibration\\ndeformation\\ndesign engineering\\nelasticity\\ngeometry\\nmanipulators\\nmotion control\\noptimisation\\npneumatic actuators\\nshapes (structures)\\nthree-dimensional printing\\nmaterial properties\\ndeformation simulation\\ndeformed shape\\ngeometry-based direct simulation\\nmultimaterial soft robots\\nsoft materials\\nmotion simulation\\nrobots fabrication\\nnumerical optimization\\ncable-driven\\n3d-printing\",\"428\":\"muscles\\nskeleton\\nstrain\\nshape\\nsoft robotics\\nforce\\nfrequency modulation\\nactuators\\nbiomechanics\\nmuscle\\nrobot kinematics\\nshear strength\\nwearable robots\\nshear forces\\nmuscle arrangements\\nincompressible property\\nbiological hydrostatic skeletons\\nlongitudinal muscles\\ntransverse muscles\\noblique arrangement\\nshape-independent load-carrying capability\\nactuation mechanisms\\noblique muscle contractions\\nflexibility\",\"429\":\"computational modeling\\nactuators\\ndeformable models\\nfinite element analysis\\ndata models\\nobject oriented modeling\\nrobots\\ndexterous manipulators\\npneumatic actuators\\nsoft manipulation\\nsoft hands\\nenvironmental constraints\\nobject surfaces\\nsimulation technologies\\ntriple-layered simulation framework\\ndynamic properties\\nlumped parameter model\\nfast simulate soft fingers\\nsoft pneumatic fingers\\nsoft robots modeled\\nkinematic chains\\nrobotic manipulation\\ngrasping\\nforce closure\\nsingle posture\\ncontact-rich\\nfem-based simulation\\nfem simulation data\",\"430\":\"task analysis\\nmeasurement\\nforce\\nrobots\\ngrasping\\nfriction\\npredictive models\\ndexterous manipulators\\ngrippers\\nmanipulator dynamics\\nmobile robots\\nnonprehensile balancing grasps\\nwrench-based quality metric\\nforce-closure grasps\\nautonomous robotic applications\\nmanipulation\\nprediction capability\\ndexterity\",\"431\":\"shape\\ngrasping\\nstrain\\naerospace electronics\\nthree-dimensional displays\\nrobots\\ncoherence\\nimage registration\\ninference mechanisms\\nintelligent robots\\nshape recognition\\nlatent space nonrigid transformation\\ncoherent point drift approach\\nclass-level knowledge\\ngrasping motions\\nshape parameters\\nlow-dimensional latent space\\nsubspace methods\\nnonrigid registration method\\ngrasping skills\",\"432\":\"shape\\ntaxonomy\\nrobots\\ngrasping\\nplanning\\nvideos\\ndata collection\\nbiomechanics\\ndexterous manipulators\\nmotion control\\nhuman heuristics\\ngrasp-type\\nobject size\\nonline data collection method\\nhuman intuition\\nsurvey questions\\nadopted taxonomy\\nwrist orientation\\ncommon grasps\\nobject height\\nrobot hand size\\nconfidence-interval based polytope\\nobject shape space\\npotential pre-grasps\\ngrasping objects\\nfundamental object shapes\",\"433\":\"grasping\\nrobots\\ntraining\\nfeature extraction\\nadaptation models\\ncameras\\ntask analysis\\nimage colour analysis\\nmanipulators\\nneurocontrollers\\nrobot vision\\ndeep robotic grasping\\noff-the-shelf simulators\\nground-truth annotations\\nrandomized simulated environments\\ndomain adaptation methods\\ngrasping system\\nraw monocular rgb images\\npixel-level domain adaptation\\nreal-world grasping performance\\nannotated visual grasping datasets\\ngraspgan\\ngenerative adversial network\",\"434\":\"robot kinematics\\ngrasping\\nwrist\\nmanipulators\\nprotocols\\nkinematics\\nadaptive control\\ngrippers\\nmotion control\\nhand capabilities\\nsoft rbo hand 2\\npredefined motion\\nfinger movements\\ncompliant robot\\nsoft robotic grasping\\nhuman grasping\\nmovement patterns\\nadaptive intrinsic\\/extrinsic motion\",\"435\":\"robots\\ntrajectory\\ntask analysis\\nplanning\\nheuristic algorithms\\nlearning (artificial intelligence)\\napproximation algorithms\\ngrippers\\nintelligent robots\\nmanipulator dynamics\\nmotion control\\npath planning\\nhigh-level discrete actions\\nq-learning\\nrhythmic dynamic movement primitives\\n4-finger-gripper manipulator\\nhierarchical planning\\nreinforcement learning\\n4-finger-gripper manipulation\\nhierarchical-planning approach\",\"436\":\"kernel\\nheating systems\\nrobots\\njamming\\nactuators\\nforce\\nwires\\ncompressive strength\\nfriction\\ngranular flow\\ngranular materials\\ngrippers\\nignition\\nmixtures\\nrobot dynamics\\ninter-granular friction\\ngranular fluids\\njamming actuators\\npopcorn-driven actuation\\nrobotics\\npopcorn-driven robotic actuators\\npopcorn kernels\\nexpansion ratio\\ntransition temperature\\ncompression strength\\nhot oil\\nhot air\\ndirect contact\\nheated nichrome wire\\npopping force\\nbiodegradability\",\"437\":\"damping\\nresistance\\nactuators\\ndc motors\\ntask analysis\\nrobots\\nelectromagnetic devices\\nenergy conservation\\nregenerative braking\\nvibration control\\nvia\\nvariable damping module design\\nnumerical simulations\\nenergy consumption\\nenergy efficiency\\nvariable impedance actuators\\ndynamic-regenerative damping scheme\\ndynamic braking\\nregenerative braking effect\\nenergy regeneration\\ndissipated energy\",\"438\":\"fasteners\\nfriction\\nforce\\nglass\\nmedia\\nyoung's modulus\\npropulsion\\naerospace propulsion\\nblades\\ndesign engineering\\ndiscrete element method\\ngranular materials\\nplanetary rovers\\npropellers\\nshafts\\nspace vehicles\\ntracked vehicles\\ntransportation\\nvehicle dynamics\\nthrust force\\ngranular media\\nscrew-powered propulsion\\nindustrial processes\\npontoon shaft\\narctic media\\nscrew design\\nsoda-lime glass beads\\nscrew-propelled vehicles\\ndewatering\\nblades damage\\nblade sinkage\\nlunar rover design\\naqueous media\\nangular velocity\\ndouble-helix archimedes screw generating propulsive force\\nminiaturized exploration vehicle\\ndiscrete element modeling software\\nsize 5.0 cm\\nsize 8.0 cm\\nsize 10.0 cm\\nsize 1.8 mm to 2.2 mm\\nsize 4 cm\",\"439\":\"actuators\\nimplants\\nsoft robotics\\nesophagus\\nsurgery\\nmathematical model\\nbiological tissues\\nbiomedical materials\\ncellular biophysics\\nelastomers\\nmedical robotics\\npneumatic actuators\\nprosthetics\\nradially expandable modular helical soft actuator\\naxially expandable modular helical soft actuator\\nelastomeric strands\\nelongation\\ntissue regeneration\\nlong-gap esophageal atresia condition\\nsoft pneumatic actuator\\nsoft robots\\nmodular soft basic constituents\\nhuman body\\nbiomedical engineering\\nrobotic implantables\\nsoft medical robots\\npressure 19.0 kpa\",\"440\":\"optimization\\ntopology\\nforce\\npiezoelectric actuators\\nsensitivity\\nmathematical model\\nfinite element analysis\\noptimisation\\nrhombus mechanism\\nsimp topology optimization method\\ndisplacement range\\ninherent crystalline properties piezoelectric actuators\\nsimp topology optimization approach\\npiezoelectric actuators design\\ndisplacement amplifier mechanism\\noptimal design\\ncompliant structure\\nsimp topology optimization\",\"441\":\"roads\\npath planning\\ngeometry\\nautonomous vehicles\\nwheels\\nkinematics\\ncomputational modeling\\ncurve fitting\\nmobile robots\\nroad traffic\\nroad vehicles\\nurban scenario\\nintelligent vehicles\\nkinematic constraints\\ncontinuous-curvature paths\\nlow curvature derivatives\\nclothoid-based global path planning\\nroad network representation\",\"442\":\"surface reconstruction\\ncomputational modeling\\nthree-dimensional displays\\nrobot sensing systems\\ninspection\\nmobile robots\\nsolid modeling\\npath planning\\nsolid modelling\\n3d models\\nexploration algorithm\\nautonomous 3d modeling\\npath planning problem\\nexploration path\\nlow-confidence surfaces\\nreconstructed surfaces\\nvolumetric model\\nvolumetric map\\nmobile robot\",\"443\":\"robot kinematics\\nplanning\\naircraft\\ncollision avoidance\\nairports\\nautomobiles\\naircraft control\\nmobile robots\\nmulti-robot systems\\npath planning\\nautomatic aircraft taxiing coordination\\ndriver-less cars coordination\\nno-backward-movement constraint\\ncomplex conflict situations\\ncharles de gaulle airport\\nmultirobot path coordination\",\"444\":\"planning\\ntask analysis\\naerospace electronics\\nlegged locomotion\\nsuperluminescent diodes\\nhumanoid robots\\npath planning\\nplanning (artificial intelligence)\\nsearch problems\\nplanning efforts\\nplanning process\\nsingle-planner approach\\nmultimodal humanoid mobility\\nconfiguration space\\nhumanoid robot\\nsingle search process\\nsearch spaces\\nadaptive dimensionality\",\"445\":\"robot sensing systems\\ndispersion\\natmospheric modeling\\nunmanned aerial vehicles\\nposition measurement\\nwind speed\\nmathematical model\\nair pollution\\natmospheric chemistry\\natmospheric techniques\\nbayes methods\\nchemical sensors\\ndisperse systems\\nhazardous materials\\ninverse problems\\nmobile sensor planning\\nbayes' theorem\\nstatic sensors\\nsingle mobile sensor\\nchemical sensor\\ndispersion parameters\\ngaussian puff dispersion model\\nmeteorological information\\ninverse problem\\nhazardous material\\nnoncontinuous atmospheric release\\nsource term estimation\",\"446\":\"robots\\ncollision avoidance\\nforce\\nplanning\\nthree-dimensional displays\\ncollaboration\\ntask analysis\\ncontrol system synthesis\\nend effectors\\nhuman-robot interaction\\nimage segmentation\\nkalman filters\\nnearest neighbour methods\\nrobot vision\\nhuman-robot collaborative safety\\ncartesian constraint\\nreal-time motion planning\\ncontrol design\\nrobotic arm\\nmultiple kinectv2 depth cameras\\nrobot workspace\\nrobot end effector\\ncollision-free motion planning method\\n6-dof robot arm\\nkalman filter\\nk-nearest neighbor searching algorithm\",\"447\":\"calculus\\nplanning\\nmodel checking\\ntrajectory\\nreactive power\\nlattices\\nboundary-value problems\\nfeedback\\nformal verification\\nlinear quadratic control\\nmotion control\\npath planning\\nprobability\\nsampling methods\\ntemporal logic\\nsampling-based motion planning\\ntemporal logic specifications\\nlinear dynamics\\ntwo-point boundary value problem\\nasymptotically optimal planning algorithm sst\\nlocal deterministic \\u03bc-calculus model\\nmotion planning algorithm\\ndeterministic \\u03bc-calculus specifications\\nmultiple kripke structures\\nabstracted kripke structure\\nstate-space\\nlinear-quadratic regulator feedback control policy\\ncomplex liveness specification\\nsteering function\\nkinodynamic planning algorithm sst\\nlqr feedback control policy\",\"448\":\"trajectory\\nvibrations\\nacceleration\\nmotion segmentation\\nsystem dynamics\\nnumerical simulation\\nbang-bang control\\npath planning\\nposition control\\nconfiguration dependent dynamics\\nvibration-free rtr trajectory generation\\nbang-coast-bang trajectory\\npiece wise shaping\\ntrajectory segmentation strategy\\n3-segmented input shaping\\nconfiguration dependent dynamic systems\\nfree rest-to-rest trajectories\",\"449\":\"task analysis\\nforce\\nlegged locomotion\\naerospace electronics\\ndynamics\\njacobian matrices\\ncontrol system synthesis\\nforce control\\nmotion control\\nposition control\\nrobot dynamics\\nrobot kinematics\\nrobust control\\nmodel-based hierarchical controller\\nmodel-based controller\\nprojected inverse dynamics controller\\ncontrol law\\nconstrained space controller\\nunknown external disturbances\\nimpedance controller\\nlegged systems\\nunconstrained component\\ncontact forces\\nforce sensors\\ntorque sensors\\nanymal quadruped platform\\ncontact locations\\nlegged robots\",\"450\":\"legged locomotion\\ntrajectory\\nforce\\nsprings\\nvehicle dynamics\\nrobot kinematics\\nrobot dynamics\\ntrajectory control\\nfore-aft leg specialization controller\\nrunning animals\\nrobotic counterparts\\nfunctional dynamic decomposition\\ndynamic quadruped\\ntrajectory-based controller\",\"451\":\"legged locomotion\\nrobot sensing systems\\nforce\\ndisturbance observers\\nrobustness\\ndiscrete time systems\\nfinite state machines\\nforce control\\nkalman filters\\nmotion control\\nobservers\\nrobot dynamics\\nrobot kinematics\\nrobust control\\ncontact detection\\ncontact state estimation\\nmit cheetah 3 robot\\ndynamic modeling\\nkinematic\\nevent-based finite state machine\\nkalman filtering\\ncontact priors\\nproprioceptive force control estimates\\ngeneralized-momentum disturbance observer\\ndiscrete-time extension\\ncontact initiation\\nterrain geometry\\ncontact models\\ncontact transitions\\nunstructured environments\\nlegged robots\\nunstructured terrains\\nevent-based locomotion\\ncontact model fusion\\ntime 4.0 ms to 5.0 ms\",\"452\":\"cameras\\nlegged locomotion\\nplanning\\nrobot kinematics\\nthree-dimensional displays\\nrocks\\nconvolution\\nfeedforward neural nets\\nimage colour analysis\\nintelligent robots\\nlearning (artificial intelligence)\\npath planning\\nprediction theory\\nsensors\\nversatile legged locomotion\\nrobots\\nlongterm routes\\nhorizontal terrain\\nvertical terrain\\nonboard sensors\\nvantage points\\nstrongly foreshortened images\\nterrain features\\nviewing angle\\nconvolutional neural network method\\narbitrary tilt angles\\nroute planner\\nplausible plans\\nrock climbing gyms\\nwalking robots\\nclimbing robots\\nsingle image footstep prediction\\ndistance angle\\nvalid handhold prediction\\nfoothold locations prediction\\nsingle rgb+d images\\nlearning techniques\\nflat ground\\nstairs\\nwalls\",\"453\":\"robot sensing systems\\nlegged locomotion\\nkinematics\\noptimization\\nfoot\\ncameras\\ngraph theory\\nobject tracking\\noptimisation\\nrobot kinematics\\nrobot vision\\nstate estimation\\nrobotic perception systems\\nrobot state-estimation\\nagility robotics\\ncassie-series robot\\nforward kinematic factor\\nfactor graph framework\\npreintegrated contact factor\\nkinematic factors\\nlegged robots\\nstate-estimation technique\\nvisual tracking\",\"454\":\"natural languages\\nsemantics\\ntask analysis\\nplanning\\nrobots\\ntrajectory\\nnavigation\\ngrammars\\nlambda calculus\\nlearning (artificial intelligence)\\nnatural language processing\\ntrees (mathematics)\\nparse weights\\nvalidation-driven perceptron weight updates\\ngoal-condition learning approach\\ngrounded reward functions\\nlanguage representations\\nweighted linear combinatory categorial grammar semantic parser\\nccg lexicon\\nparse trees\\nrobot behaviors\\ncleanup world domain\\nnatural language parsing\\ngoal-state reward functions\",\"455\":\"robot sensing systems\\nhaptic interfaces\\nend effectors\\nclothing\\npredictive models\\nassisted living\\ncontrol engineering computing\\nhandicapped aids\\nhuman-robot interaction\\nlearning (artificial intelligence)\\noptimal control\\npredictive control\\nservice robots\\nphysical human-robot interaction\\npeople with disabilities\\ncontroller objective function\\ndeep predictive model\\nprediction horizon\\npr2 robot\\nphysics-based simulation\\ndressing assistance\\ngarment\\ndeep recurrent model\\nnonrigid garments\\nphysical implications\\nrobot-assisted dressing\\ndeep haptic model predictive control\",\"456\":\"acoustics\\nrobots\\nadaptation models\\nlogic gates\\nfeature extraction\\npredictive models\\nrecurrent neural networks\\nacoustic signal processing\\nemotion recognition\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nneural nets\\nemorl model\\naudio signal\\ncontinuous acoustic emotion classification\\ndeep reinforcement learning\\nacoustically expressed emotions\\ndeep neural network-based models\\naffective state evaluation\\nreal-time communication scenario\",\"457\":\"semantics\\ngrounding\\npallets\\nnatural languages\\ntires\\nservice robots\\ncontrol engineering computing\\nhuman-robot interaction\\nnatural language processing\\namazon mturk\\ntesis\\nnatural language sentences\\nextended sentence structure\\nspatial context information\\nhuman listeners\\ntemporal spatial inverse semantics\\ntemporal context\",\"458\":\"navigation\\nrobot sensing systems\\ntask analysis\\ntelepresence\\nbrain-computer interfaces\\nbrain\\ncollision avoidance\\ncontrol engineering computing\\ngeriatrics\\nhandicapped aids\\nlearning (artificial intelligence)\\nmedical robotics\\nmedical signal processing\\nmobile robots\\noperating systems (computers)\\npatient rehabilitation\\nposition control\\nrobot programming\\ntelerobotics\\nvideo streaming\\nnoninvasive brain-computer interface\\nrobot operating system\\ntelepresence robot\\nmobile device\\nhuman brain signals\\nsevere physical disabilities\\nelderly people\\nbci user\\nrobot position control\\nobstacle avoidance\",\"459\":\"prosthetics\\nmuscles\\nelectroencephalography\\ndc motors\\ngrasping\\nbones\\nthumb\\nbrain\\nbrain-computer interfaces\\nfinite automata\\nhandicapped aids\\nlearning (artificial intelligence)\\nmedical robotics\\nmedical signal processing\\nmuscle\\nneural nets\\nneurophysiology\\nbrain-machine interface\\ncentral nervous system\\nbiomedical signal\\nfinite automata theory\\nneucube evolving spiking neural network architecture\\nrobust prosthetic control\\nanthropomorphic mechanical design\\nnoninvasive bmi\\nmuscle atrophy\\nmotor control framework\\nanthropomorphic design\\nprosthetic limbs\\nlimb amputation\\nprosthetics control\",\"460\":\"electroencephalography\\ntraining\\ngraphical user interfaces\\nmicrosoft windows\\ntesting\\nrobot learning\\nbrain\\ncontrol engineering computing\\nhuman-robot interaction\\nintelligent robots\\nlearning (artificial intelligence)\\nrobot programming\\ntarget reaching task\\nassistive technologies\\nshared control\\nintelligent robotic device\\nelectrophysiological measures\\nerror detection\\nerror-related potentials\\nsemiautonomous system\\nonline robot learning task\\ndetected errp\\nrobot learning loop\\noptimal policy learning\\nshared autonomy\\nreinforcement learning framework\",\"461\":\"training\\ntask analysis\\nadaptation models\\nrobots\\ngallium nitride\\nmathematical model\\nlighting\\nfeature extraction\\nimage segmentation\\nunsupervised learning\\nmachine learning models\\nrobotics applications\\nalignment step\\nfeature distribution\\ngan training\\ncontinuous appearance shifts\\ncontinually changing environments\\nincremental adversarial domain adaptation\\ngenerative adversarial network\\ntraversable-path segmentation task\\nunsupervised domain adaptation\",\"462\":\"roads\\ncameras\\ngoogle\\nmachine learning\\nvideos\\nimage segmentation\\ncomputer architecture\\nconvolution\\nedge detection\\nfeedforward neural nets\\nimage classification\\nlearning (artificial intelligence)\\nobject detection\\nalgorithmic vanishing point detector\\ndeepvp\\ngoogle street view image dataset\\ncamera parameters\\ndeep learning\\ndeep vanishing point system\\ncnn classification problem\\ninferred ground-truth vanishing points\\nconvolutional neural network\\nvanishing point detection\",\"463\":\"laser radar\\ntask analysis\\nvehicle dynamics\\nthree-dimensional displays\\ndynamics\\nmachine learning\\nsemantics\\nimage colour analysis\\nimage sensors\\nimage sequences\\nlearning (artificial intelligence)\\nneural net architecture\\noptical radar\\nsemantic networks\\npretext tasks\\ntest time\\nincluding distilled image information\\nstandard image-based optical flow\\nnovel lidar-flow feature\\nsemantic information\\nimage data\\nconsecutive lidar scans\\ntesting time\\ncnn architecture\\nexternal observed vehicles\\nobserver vehicle\\nproprio-motion\\nautonomous cars\\ndeep learning solutions\\nrgb images\\nsemantically rich information\\nautonomous driving\\nperception technologies\\ndeep lidar cnn\",\"464\":\"measurement\\nthree-dimensional displays\\nrobots\\noptimization\\ntransforms\\nshape\\nconvolution\\ndistance learning\\nfeedforward neural nets\\ngradient methods\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\npose estimation\\nsolid modelling\\narbitrary spatial relations\\nsizes\\nshapes\\ndistance metric learning\\n3d point clouds\\nmetric space\\nobject poses\\narbitrary target relation\\ndomestic environments\\ngradient based optimization\\nneural network\\ngeometric models\\ncontinuous spectrum\\nend to end metric learning\",\"465\":\"solid modeling\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nobject oriented modeling\\npipelines\\ntwo dimensional displays\\nshape\\ncameras\\nfeature extraction\\nmobile robots\\nobject detection\\npose estimation\\nrobot vision\\nslam (robots)\\ncategory-specific models\\nreal-time object-oriented slam\\nmonocular camera\\nobject-level models\\ncategory-level models\\nobject deformations\\ndiscriminative object features\\ncategory models\\nobject landmark observations\\ngeneric monocular slam framework\\n2d object features\\nsparse feature-based monocular slam\\nobject instance retrieval\\ninstance-independent monocular object-slam system\\nfeature-based slam methods\\ntime 2.0 d\\ntime 3.0 d\",\"466\":\"decoding\\ncomputer architecture\\nsemantics\\ntask analysis\\nexplosions\\nimage segmentation\\nforestry\\nconvolutional codes\\nimage classification\\nimage coding\\nneural net architecture\\ndense connections\\ncam vid dataset\\nfreiburg forest dataset\\nconvolutional encoders\\nmultiple segmentation tasks\\nfeature map explosion\\nresidual network architecture\\ndense block\\ndpdb-net\\ndual-path dense-block network\\nencoder-decoder architectures\\nfeature re-usage\\ndense networks\\nmultiple classification tasks\\nfeature exploration\\ndensely connected networks\",\"467\":\"robot sensing systems\\nlegged locomotion\\ncameras\\ntracking\\ntrajectory\\npredictive models\\ncollision avoidance\\nmobile robots\\nmotion control\\nobject detection\\ntrajectory control\\nautonomous mobile robot\\nwalking user\\nautonomous push-carts\\nmultimodal person detection\\nhuman-motion model\\nobstacle mapper\\nhuman tracker\\nhuman motion model\\nrobot motion planner\\nrobot motion controller\\nindustrial entertainment applications\\ndomestic entertainment applications\\nhands-free push-cart\\npredicting user trajectory\",\"468\":\"target tracking\\nsensors\\ncameras\\nrobustness\\ntask analysis\\nhead\\nobject detection\\nobject tracking\\npose estimation\\nrobot vision\\ninner city train station\\nrobotic technologies\\n2d pose\\nmotion capture system\\nperson tracking framework\\nhuman environments\\nshoulder pose estimates\\ncrowded environments\\npose errors\\nlab environment\",\"469\":\"trajectory\\ntask analysis\\nrobot kinematics\\npredictive models\\ncomputational modeling\\ntraining data\\nhuman-robot interaction\\nimage coding\\nimage colour analysis\\nimage motion analysis\\nimage representation\\nprobability\\nrobot vision\\nmotion patterns\\nkinematic cues\\nnatural human motion\\nonline human motion prediction\\ntarget prediction\\nrgb depth images\\nskeletal data\\nconditional variational autoencoder\\ntime 300.0 ms to 500.0 ms\",\"470\":\"trajectory\\nprediction algorithms\\nforce\\npredictive models\\nplanning\\nstochastic processes\\nrobots\\ncollision avoidance\\nmarkov processes\\nmobile robots\\nmotion control\\nmulti-agent systems\\nmulti-robot systems\\nrandom processes\\nmotion trajectories\\nplanning-based approach\\ndynamic objects\\nplanning-based social force approach\\njoint long-term prediction\\nindividual agent velocities\\nsocial forces\\nweighted random walk algorithm\\nstochastic motion policies\\nlong-term predictions\\nmultiple agents\\njoint motion\\nlocal interactions\\nlong-term human motion prediction\\ndynamic environments\\nintelligent vehicles\",\"471\":\"robots\\nsenior citizens\\npsychology\\ngames\\nspeech recognition\\ntracking\\nhuman-robot interaction\\ncontrol engineering computing\\ngeriatrics\\nman-machine systems\\nuser interfaces\\npersuasive communication skills\\nsocial psychology theory\\npromotion behavior\\nprevention behavior\\nneutral behavior\\ncompanion robots\\ncaregivers\\nelderly people\\ndecisions taking\\nregulatory focus behavior\\nprevention focus\\nbody gestures\\nnegotiation scenario\",\"472\":\"speech recognition\\ntrajectory\\nrobot sensing systems\\nmicrophone arrays\\ncomputer games\\ncontrol engineering computing\\neducational robots\\ngesture recognition\\nhuman-robot interaction\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nrobotic platforms\\nchild-robot interaction scenarios\\nmulti3\\nrobotic sensory\\nperception capabilities\\nspeech recognition modules\\ngesture recognition modules\\nmodular multirobot architecture\\naction recognition modules\\nindoors interaction scenarios\\nchild-robot interaction scene\\nmultiple kinect-based system\\nmultiple robots\\nmultimodal child interaction\\nmultisensory perception system\",\"473\":\"robot kinematics\\ntask analysis\\nnavigation\\nplanning\\nresource management\\nmeasurement\\nhuman-robot interaction\\nmobile robots\\nmulti-robot systems\\npath planning\\nmarket-based framework\\ncoordination mechanism\\nsocial costs\\nbid evaluations\\nrealistic environment\\nhuman-aware navigation\\nhuman-agnostic planning\\nsocial constraints\\nmultirobot coordination\\ndynamic environments\\nsocial human-populated environments\\nmultirobot task allocation problem\\nstatic humans\\nmoving humans\\nhigh-fidelity simulator\\nlocalization noise\\nstatic people\\nblocked passages\\nhuman-aware planning\\nhuman-agnostic navigation\\nrobot experiments\\nmrta metrics\",\"474\":\"trajectory\\nnavigation\\npredictive models\\nrobots\\ncollision avoidance\\ndynamics\\ntask analysis\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\npublicly available crowd datasets\\ntrained attention model\\nsocial attention\\nhuman crowds\\nhuman predictable trajectories\\nhuman trajectory prediction\",\"475\":\"roads\\nfeature extraction\\nlaser radar\\nthree-dimensional displays\\ntask analysis\\nfuses\\nnetwork architecture\\nconvergence\\nconvolution\\nfeedforward neural nets\\ngradient methods\\nimage colour analysis\\nlearning (artificial intelligence)\\nmobile robots\\noptical radar\\nposition control\\nconvolutional neural networks\\nmultiple cues integration\\nautonomous driving\\ndeep learning\\nroad detection algorithms\\npre-trained resnet-lol\\nrgb images\\ncnn\\nfeature maps extraction\\nlidar scanner\\nposition map\\nimage gradient\\nkitti benchmark\",\"476\":\"foot\\ntrajectory\\ncomputational modeling\\nsensors\\nlegged locomotion\\nvisualization\\nestimation\\ncameras\\ngait analysis\\nimage motion analysis\\ninertial navigation\\npose estimation\\nhead-mounted imu-camera\\nbatch least-squares algorithm\\nhuman motion models\\ninertial data\\nvisual data\\nhuman gait estimation\\nminimal sensors-based system\\nvicon motion capture system\\ngait models\\ninertial measurement units\",\"477\":\"navigation\\nsemantics\\nvisualization\\nsimultaneous localization and mapping\\nrobustness\\nmeasurement\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nnavigational behaviors\\ndeep learning architectures\\nsemantic abstraction\\nnavigation tasks\\nnavigational missions\\nbehavioral approach\\nindoor autonomous navigation\\nsemantically rich graph representation\\nindoor robotic navigation\\nsemantic locations\",\"478\":\"planning\\nthree-dimensional displays\\nrough surfaces\\nsurface roughness\\nrobot sensing systems\\nnavigation\\nhumanoid robots\\nlegged locomotion\\noff-road vehicles\\npath planning\\nrobot vision\\nrough terrain\\nrough terrain stepping\\nwalk-man humanoid robot\\nflat foothold contact analysis\\nrough local terrain surfaces\\ncurved patch modeling system\\n6dof footstep sequences\\nblack box walking controller\\nproper environment modeling\\nvisual perception\\nfoothold placements\\nexteroceptive perception\\ncurved contact patches\\nbipedal robots\\nfootstep planning\",\"479\":\"laser radar\\nthree-dimensional displays\\nestimation\\nsensors\\nglobal navigation satellite system\\nautonomous vehicles\\nrobustness\\nglobal positioning system\\nkalman filters\\noptical radar\\nroad vehicles\\nsatellite navigation\\nsensor fusion\\nlidar\\nlocalization system\\ngnss rtk module\\nurban downtown\\ncomplementary sensors\\nprecise localization system\\nrobust localization system\\nprecise vehicle localization\\nlocalization measurements\\nerror-state kalman filter\\nambiguity resolution success rate\\nmultisensor fusion framework\\nsize 60.0 km\\nsize 5.0 cm to 10.0 cm\",\"480\":\"collision avoidance\\nrobots\\nvehicle dynamics\\naerospace electronics\\ntraffic control\\nautonomous vehicles\\nplanning\\ncomputational geometry\\ndecision making\\nfeedback\\nmobile robots\\nposition control\\nroad safety\\nroad vehicles\\nsafe distributed lane change maneuvers\\nmultiple autonomous vehicles\\nreciprocal collision avoidance method\\nautonomous cars\\nlinear dynamics\\nbuffered input cell\\nvoronoi cell\\nvoronoi diagrams\\nvehicles control input\\ncontrol stack\\nfreeway driving scenario\\ndecision-making layer\\ntrajectory planning layer\\nfeedback controller\\nbic method\\nhuman-driven car\",\"481\":\"predictive models\\naccidents\\nstochastic processes\\nuncertainty\\nbayes methods\\ncameras\\ntensile stress\\ncollision avoidance\\ndecision making\\ndriver information systems\\nimage colour analysis\\nlearning (artificial intelligence)\\nrecurrent neural nets\\nrisk management\\nvideo streaming\\ndeep predictive models\\ncollision risk assessment\\nautonomous driving\\npredictive approach\\nassisted driving\\ndeep predictive model\\nvideo streams\\nrgb images\\ntemporal information\\nmulti-modal information\\nproprioceptive state\\nbayesian convolutional lstm\",\"482\":\"robot sensing systems\\ntask analysis\\nvehicles\\ncameras\\nroads\\nnavigation\\ncollision avoidance\\nlearning systems\\nmobile robots\\nroad traffic control\\ndriving policy functions\\nconditional imitation learning\\nsensorimotor coordination\\nvision-based driving\\nrobotic truck\\ndriving policies\\ndeep networks\\nhigh-level navigational commands\\nurban driving\\nhigh-level command input\\ncondition imitation\",\"483\":\"neurons\\nvisualization\\ndeconvolution\\ntools\\nbiological neural networks\\nroads\\ndata visualization\\ndata visualisation\\nfeedforward neural nets\\nlearning (artificial intelligence)\\nobject detection\\ntraffic engineering computing\\nvideo signal processing\\nconvolutional neural network\\nirrelevant information\\nprediction decision\\ncnn-based systems\\nsteering self-driving cars\\nvisualization method\\nvaluable debugging tool\\ntheoretical arguments\\ninput pixels\\nindividual pixels\\nvisualization tool\\nnvidia neural-network-based end-to-end learning system\\nautonomous driving\\nvisualbackprop\\npublic road video data\\nlayer-wise relevance propagation approach\\nsimilar visualization results\\npilotnet steering decision\\nrelevant object capture\",\"484\":\"predictive models\\nvehicles\\nsensors\\nroads\\ntrajectory\\ndata models\\nmotion measurement\\ndriver information systems\\nfeature extraction\\nimage motion analysis\\nlearning (artificial intelligence)\\nneural nets\\nroad vehicles\\nstatic vehicle environment\\ngrid-based prediction\\nvarying assistance tasks\\nbaseline approaches\\nenvironmental observations\\ndeep neural network\\nadvanced driver assistance systems\\npredictive model\\nroad topologies\\nenvironmental properties\\npath extraction\\nego-vehicle path prediction\\nego-vehicle motion\",\"485\":\"autonomous vehicles\\nnavigation\\nneural networks\\nprobability distribution\\ndecision making\\nmachine learning\\nbayes methods\\ncameras\\ncontrol engineering computing\\ngaussian processes\\nlearning (artificial intelligence)\\nmixture models\\nmobile robots\\nneural nets\\npath planning\\nroad vehicles\\nrobot vision\\nsteering systems\\nparallel autonomous systems\\ndeep learning\\nautonomous driving task\\ncamera data input\\nautonomous navigation\\nvehicle control\\ncontinuous control probability distribution\\ndeep neural network based algorithm\\nsteering angles\\nparallel autonomy setting\\ndriving conditions\\nvariational bayesian methods\\nsteering bounds learning\\nend-to-end learning\\nsteering control options\\ngaussian mixture models\",\"486\":\"voltage control\\ntask analysis\\nrobot sensing systems\\ntraining\\nsynapses\\nneurons\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nroad vehicles\\nrobot vision\\nslam (robots)\\nspiking neural network\\nlane keeping vehicle\\nmobile applications\\nmobile robot applications\\nreward-modulated spike-timing-dependent-plasticity\\nreinforcement learning\\npioneer robot\\nlane information\\nrobot tasks control\\nend to end learning approach\\nr-stdp\\nsnns training\\nneuromorphic vision sensor\\nlateral localization accuracy\",\"487\":\"force\\ntactile sensors\\nneurons\\ncameras\\nlight emitting diodes\\nbackpropagation\\nccd image sensors\\nelastomers\\nforce measurement\\nimage texture\\nneural nets\\nrobots\\nshape recognition\\nrobotic hand grasping\\nforce vector distribution\\ntransparent elastomer\\ntransparent acrylic board\\nccd camera\\nreflective membrane\\nmarkers array\\nobject contact surface\\nbackpropagation neural network\\nlocal binary pattern algorith\\nforce magnitude\\nforce direction\\nsurface texture sensing\\ndual-modal vision-based tactile sensor\\ntexture recognition rate\\ntexture information\",\"488\":\"measurement\\nprototypes\\ngrippers\\nservice robots\\nrobot sensing systems\\nplanning\\ngoals\\/questions\\/metrics method\\nrobot design\\nadvanced robots\\nresource-intensive activity\\nresearch teams\\ncomplex robot systems\\ndesign metrics\\ndesign tool\\ndesign-orientated gqm method\\nbespoke robotic gripper\\nservice robot\\ngqm principles\\nrobotics applications\",\"489\":\"robots\\nontologies\\nsemantics\\ntask analysis\\ncontainers\\ncloud computing\\ncognition\\nontologies (artificial intelligence)\\nopenease cloud engine\\nweb-based user interface\\nfetch robot\\npr2 robots\\nexecution logs\\nknowledge exchange\\nencyclopedic knowledge\\ncloud application\\ncrowd-sourcing\\ncloud robotics\",\"490\":\"shape\\nstacking\\nplanning\\nstability analysis\\ntwo dimensional displays\\nbuilding materials\\nassembly planning\\nbrick\\nbuilding management systems\\nbuildings (structures)\\nconstruction industry\\ndisasters\\nheuristic programming\\nmechanical stability\\nrobotic assembly\\nstatistical analysis\\nstructural engineering\\nautomated construction\\nirregular objects\\ndry stacked structures\\ndisaster areas\\nremote environments\\nassembly planning process\\nbricks\\nheuristics programming\",\"491\":\"vibrations\\nmicroscopy\\nlenses\\nhistograms\\nmicrochannels\\nlighting\\nmachine vision\\nbrightness\\noptical microscopes\\nhigh-speed well-focused image-capturing microscope system\\nluminance histogram-based algorithm\\nhigh-speed microobject system\\nvibration machine\\nmicrochannel\\nvision-based analysis systems\\nobjective lens\\nsize 10 mum to 100 mum\\nsize 1 mum to 4 mum\",\"492\":\"training\\nlaser radar\\nimage reconstruction\\nestimation\\nprediction algorithms\\nsimultaneous localization and mapping\\nimage colour analysis\\nimage resolution\\nimage sampling\\nimage segmentation\\nimage sensors\\nlearning (artificial intelligence)\\nmean square error methods\\noptical radar\\nrandom processes\\nregression analysis\\nslam (robots)\\nsparse matrices\\nprediction root-mean-square error\\nsparse maps\\ndense maps\\nsparse-to-dense\\ndense depth prediction\\nsparse set\\ndepth measurements\\nsingle rgb image\\ndepth estimation\\nmonocular images\\nlow-resolution depth sensor\\nsingle deep regression network\\nrgb-d raw data\\nsparse depth samples\\nvisual simultaneous localization and mapping algorithms\\nplug-in module\\nnyu-depth-v2 indoor dataset\\nlidars\",\"493\":\"three-dimensional displays\\ntarget tracking\\nreal-time systems\\nsolid modeling\\ninterpolation\\nvehicle dynamics\\nlaser radar\\nestimation theory\\nimage matching\\nobject detection\\nobject tracking\\nstatistical distributions\\nstereo image processing\\ntarget object clouds\\nsparse point clouds\\npoint-to-distribution matching technique\\ntracking algorithm\\n3d point clouds\\ndirect point-to-point matching method\\nreal-time object tracking\\nestimation of vertical distributions\\nobject-tracking strategy\\nevd\\ninterpolation method\\n3d interpolation\",\"494\":\"three-dimensional displays\\nmixture models\\niterative closest point algorithm\\nrobustness\\nprobabilistic logic\\ngaussian mixture model\\ncomputerised tomography\\nexpectation-maximisation algorithm\\ngaussian processes\\nimage registration\\nmedical image processing\\noptimisation\\nprobability\\nprobabilistic approach\\nexpectation-maximization algorithm\\nct images\\nfmm\\ngmm\\nem algorithm\\noptimization problem\\nvon-mises-fisher mixture model\\nrobust point cloud registration method\\nhybrid mixture model\\npoint cloud registration\\nvon-mises-fisher distribution\",\"495\":\"yarn\\ninstruction sets\\nsurgery\\nsplines (mathematics)\\nimage reconstruction\\nrobots\\ntask analysis\\nblood vessels\\nconvolution\\nimage recognition\\nimage segmentation\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nrecurrent neural nets\\nthread centerline reconstruction\\nmultistage suture detection\\nrobot assisted anastomosis\\ndeep learning\\nrobust suture detection\\nsuture augmentation\\nrobotic-assisted surgery\\nfully convolutional neural networks\\ntrainee suturing skill evaluation\\ncurvilinear structure detector\",\"496\":\"clothing\\ntactile sensors\\nshape\\ngrippers\\ncontrol engineering computing\\nconvolution\\nfeedforward neural nets\\nimage sensors\\nintelligent robots\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nactive clothing material perception\\ntactile sensing\\ndeep learning\\nintelligent robot\\nrobot system\\nobject properties\\ncommon object category\\nexternal kinect sensor\\ngelsight tactile sensor\\ntactile data\\nphysical properties\\ndurability\\nsemantic properties\\nclothing properties\\nactive tactile perception system\\nvision-touch system\\nrobots\\nvaried clothing related housework\\nconvolutional neural networks\",\"497\":\"path planning\\ncomputational modeling\\noceans\\npredictive models\\nforecast uncertainty\\ndrag\\ngraph theory\\nmarine control\\nmarkov processes\\noptimisation\\nmarkov decision process\\nuncertain flow model\\nocean environment\\ntime-varying flows\\nminimum energy paths\\nuncertain flow field\\ntransition probability model\\nminimum expected cost path\\ngraph search based method\\nminimum expected cost policy\\nmarine environments\",\"498\":\"path planning\\nmonitoring\\noceans\\nrobot sensing systems\\ntask analysis\\nfrequency modulation\\ncomputational geometry\\ngraph theory\\ngreedy algorithms\\nimage segmentation\\nmarine control\\nmobile robots\\nrobot vision\\ntopological graph\\ngreedy-coverage algorithm\\ninformative path planning problem\\ntopological hotspot identification\\nmarine robot\\ntopological map\\nbiological hotspots\\naquatic environment\\nfast marching-based voronoi segmentation\\nscheduling problem\",\"499\":\"pollution measurement\\ngeophysical measurements\\nrobot sensing systems\\nreal-time systems\\ntime measurement\\nwater pollution\\necology\\nhydrological equipment\\nhydrological techniques\\nmicroorganisms\\nmulti-robot systems\\nremotely operated vehicles\\nreservoirs\\nwater quality\\ndata-driven behavior\\nreal geophysical data\\nmodis measurements\\nwater-sampling apparatus\\nwater quality sensor\\nplankton-rich water samples\\nchlorophyll density\\nautonomous surface vehicles plan\\nwater-sampling behavior\\nefficient measurement\\nfresh-water systems\\nmeasuring contamination levels\\ndrinking water\\nphysical sampling\\nstrategic water sampling\\nheterogeneous multirobot system\\nwater reservoir\\nexplorer robot\\nwater sampling apparatus\\nasv\",\"500\":\"receivers\\ntransmitters\\nlight emitting diodes\\nrobots\\nthree-dimensional displays\\nestimation\\noptical fiber communication\\nkalman filters\\nmobile robots\\noptical communication\\nextended kalman filter\\nproportional-integral controller\\nreceiver-transmitter line\\nactive alignment control system\\ntransmitting device\\nactive alignment system\\nline-of-sight\\nunderwater communication\\nled communication\\nactive-alignment control\",\"501\":\"navigation\\nmathematical model\\naccelerometers\\ndamping\\nuncertainty\\nacoustics\\nacceleration\\nautonomous underwater vehicles\\nc++ language\\ndrag\\ninertial navigation\\nkalman filters\\nmarine navigation\\nnonlinear filters\\nvehicle model parameter error\\nadcp-aiding\\ndvl bottom-lock loss\\nimu biases\\nnavigation filter\\ndvl dropouts\\nrobust model-aided inertial localization\\nunscented kalman filter\\ninertial model-aiding\\nacoustic doppler current profiler measurement incorporation\\nearth rotation\\ntactical grade imu\\nheading convergence\\ndata denial\\nthrust model\\nmtk\\nrock\\nflatfish auv\",\"502\":\"acoustics\\nunderwater vehicles\\nkinematics\\nglobal positioning system\\nsensors\\nattitude control\\nautonomous underwater vehicles\\nmarine control\\nmobile robots\\npath planning\\nrobot dynamics\\nrobot kinematics\\nvelocity measurement\\npreliminary evaluation\\ndynamic process model\\nfully dynamic vehicle process model\\nacoustic modem\\nsurface vehicle\\nat-sea experimental trials\\njhu iver3 autonomous underwater vehicle\\nunderwater vehicle navigation\\ndvl acoustic bottom-lock range\\nkinematic process model\\ndynamical process model\\nsubmerged vehicle\\nunderwater communication\\nvelocity measurements\\nattitude sensor\",\"503\":\"robot sensing systems\\nthree-dimensional displays\\nmanipulators\\ncalibration\\ncameras\\nkinematics\\ntransforms\\ncomputer graphics\\nend effectors\\nimage registration\\nimage sensors\\nmanipulator kinematics\\nmobile robots\\nrobot vision\\nmobile manipulator platform\\nmobile manipulator kinematic parameters\\ncalibration rigs\\ncentimetre-level post-calibration accuracy\\nend effector\\nregistration algorithm\\nsensor extrinsic parameters\\ncontact-based interaction\\nmobile manipulator self-calibration\\npoint cloud registration\\nfixed vision sensor\\nmobile base\\nsensor calibration\\nmanipulator kinematic model parameters\\nnonrigid registration process\\non-board sensing\\nexternal measurement devices\",\"504\":\"robot kinematics\\ncalibration\\nprobes\\nmetrology\\nbars\\ndata acquisition\\ncoordinate measuring machines\\ngeometry\\nindustrial robots\\nparticle swarm optimisation\\nindustrial setting\\nkinematic calibration methodology\\nexternal metrology device\\nkinematic calibration model\\noptimal parameters\\/characteristics\\nparticle swarm optimization technique\\ncoordinate measurement machine\\npso\\ncmm\\nyaskawa motoman mhs-hi robot\\ngeometry based self kinematic calibration method\\nanthropomorphic robots\",\"505\":\"humanoid robots\\nforce sensors\\ndynamics\\nrobot sensing systems\\nmathematical model\\noptimization\\ncad\\nlegged locomotion\\nmotion control\\nparameter estimation\\npath planning\\nrobot dynamics\\nfix force sensor\\nmodel-based controller\\nmotion planning\\ndynamic identification\\ndynamic motions\\nsafe fix base tree structure robot\\noptimal exciting motions\\nhoap3 humanoid robot\\n6-axis force sensor\\ncomputer aided design data\\ninertial parameter identification\",\"506\":\"calibration\\nvehicle dynamics\\nmotion segmentation\\ndynamics\\nwheels\\nfriction\\nmobile robots\\nvehicles\\nhigh-fidelity dynamical model\\nconstant time algorithm\\nautonomous ground vehicles\\ndynamic models\\nonline system identification\\nscale four wheel drive vehicle\\nestimated parameter\\nmodel parameters\\ninformative motion segments\\nrobotic platform\",\"507\":\"calibration\\ncameras\\nestimation\\ntask analysis\\nsimultaneous localization and mapping\\ngeometry\\nnumerical analysis\\nsensors\\nextrinsic sensor calibration methods\\nrobotics\\nnumerical simulation\\nabstract geometric model\",\"508\":\"manipulator dynamics\\nactuators\\nvehicle dynamics\\ngears\\nfriction\\nmathematical model\\nautonomous underwater vehicles\\nmanipulators\\nmobile robots\\nposition control\\ntelerobotics\\narms parameters\\nlinear actuators\\nidentification procedure\\nmanipulator arms\\ndynamic modeling\\nheterogeneously actuated underwater manipulator arm\\nelectrically driven underwater robot manipulator\\nifremer's hrov ariane underwater vehicle\\nhybrid remotely operated vehicle\",\"509\":\"three-dimensional displays\\nrobot sensing systems\\ncameras\\niterative closest point algorithm\\nminimization\\nintegrated circuit modeling\\nlaser radar\\nimage registration\\nimage sensors\\nmobile robots\\nsensor fusion\\nflexible framework\\ngeneral framework\\nexplicit data association\\nflexible multicue photometric point cloud registration\\nmapping systems\\nrecorded sensor data\\nphotometric registration\\nmultiple modalities\\nimage data streams\\npixel-wise difference\\nmultichannel images\",\"510\":\"image reconstruction\\nsimultaneous localization and mapping\\nvisualization\\nthree-dimensional displays\\nreal-time systems\\nuncertainty\\nconvolution\\nfeature extraction\\nimage colour analysis\\nimage fusion\\nimage sensors\\niterative methods\\nneural nets\\npose estimation\\nrecurrent neural nets\\nrobot vision\\nslam (robots)\\nstereo image processing\\ncrf model\\nrgb image\\nconfidence-based fusion\\nrealtime inpainting\\nconvolutional neural networks\\ncnn\\norb-slam\\nkinect\\nconditional depth error distributions\\npixel-wise confidence weights\\ninput depth map\\nfused depth map\\nvirtual depth sensor\\nsingle-view depth prediction network\\nsparse sensor\\nmonocular visual slam system\\nfully dense depth map\\nrealtime image-guided inpainting\\njust-in-time reconstruction\\nsingle view depth predictors\\nscale-invariant depth error\\noutlier input depth\\nlidar depth maps\\narbitrary scale\\nsparse map\",\"511\":\"labeling\\nestimation\\nminimization\\nimage reconstruction\\ncameras\\npipelines\\nsurface texture\\nspatial variables measurement\\nplanar extraction\\nurban environment\\nvision-only method\\ndepth map estimation techniques\\nfast global labelling\",\"512\":\"image segmentation\\nmerging\\nrobot kinematics\\nrobot sensing systems\\nmeasurement\\ntwo dimensional displays\\ncomputational geometry\\nconvolution\\nrobot vision\\nhand-drawn sketch maps\\nsegmentation evaluation metric\\nground-truth segmentations\\nvoronoi-based segmentation method\\ndude segmentation method\\nground truth segmentations\\nsegment maps\\nfree space layout maoris\\nnavigation maps\\nsemantic representations\\ncircular kernel\\nripple-like patterns\\nmatthews correlation coefficient\\nmap of ripples segmentation\",\"513\":\"three-dimensional displays\\nmeasurement by laser beam\\noptimization\\ntrajectory\\nlaser modes\\nsimultaneous localization and mapping\\ncontinuous time systems\\nentropy\\ngraph theory\\nimage registration\\nimage resolution\\nlaser ranging\\noptical radar\\nslam (robots)\\nsolid modelling\\nstereo image processing\\nlaser-range scanners\\nhigh data rate\\n3d laser scanner\\nsurfel-based registration\\nrecursive state estimation\\nmultiresolution maps\\ncontinuous-time slam\\n3d lidar-based online mapping\\nonline simultaneous localization and mapping\",\"514\":\"robot sensing systems\\ncomputational modeling\\nmathematical model\\ntraining\\nmutual information\\ncomputer graphics\\ngaussian processes\\nindoor environment\\nmarkov processes\\nmobile robots\\nsampling methods\\nefficient computation algorithm\\ngmrf model hyperparameters\\ninformation gain\\nefficient mobile robot exploration\\nautonomous exploration\\nunknown indoor environments\\nmi\\ninformative sensing location\\nsampling method\\nrandom sensing patches\\nsensing patch\\ninformative locations\\ntraining sample patches\\nestablished gmrf model\\ngaussian markov random fields\\ngaussian process model\",\"515\":\"task analysis\\nheuristic algorithms\\nclustering algorithms\\nresource management\\nrouting\\nservice robots\\ncomputational complexity\\nindustrial robots\\nmobile robots\\nmulti-robot systems\\nnearest neighbour methods\\npattern clustering\\nvehicle routing\\nwarehouse automation\\ncvrp instance\\nncar\\nscalable multirobot task allocation algorithm\\nmodern warehouses\\ndocking station\\nroute planning\\ncapacity-constrained vehicle routing problem\\nnearest-neighbor based clustering and routing\",\"516\":\"task analysis\\nrobot sensing systems\\ntime factors\\nschedules\\ncommunication networks\\ncontrol engineering computing\\ndistributed control\\nmobile robots\\nmulti-robot systems\\nprotocols\\nscheduling\\ntime-critical dynamic tasks\\noffline schedules\\nmobile robot networks\\ndistributed intermittent communication control\\ntask accomplishment\\ntask planning\\ncommunication events\\ndistributed control framework\\ncommunication constraints\\nintermittent communication protocols\\nconnected networks\\nreliable networks\\nrobot communication capabilities\",\"517\":\"robot kinematics\\nrobot sensing systems\\nnavigation\\ndispersion\\nmeasurement\\ndistance measurement\\nglobal positioning system\\nmobile robots\\nmulti-robot systems\\npath planning\\nrobot vision\\nsensor fusion\\nlandmark-based exploration\\nresource constrained robots\\nautonomous exploration\\ntopological representation\\ntopological information\\nexploitation strategy\\nrobot swarm\\ngps-denied environment\\nsensing capabilities\\nrange sensor\\ndense landmarks\\nbearing angles\\nmetric information\\nlocal navigation\",\"518\":\"wires\\nminimization\\nrobot sensing systems\\noptimization\\nmotion control\\npower transmission lines\\ngradient methods\\nminimisation\\nmobile robots\\npath planning\\ncontinuous constrained coverage control problem\\ncow map\\ncontinuous onto wires map\\nconstrained locational cost minimization\\nfinal projection step\\nlloyd descent algorithm\\nplanar environment\\ncontinuous motion\\none-dimensional manifolds\\ntwo-dimensional motion\\nwire-traversing robots\",\"519\":\"mobile robots\\nforce\\nwheels\\nrobot kinematics\\nbrakes\\ntorque\\nbraking\\nmotion control\\nmulti-robot systems\\nbraking control region analysis\\npassive mobile robot\\nwheel\\nformation control\\ncontrol law\\npassive robot\\nfundamental control method\\nactive leader\\nmultiple mobile robots\\nexternal pulling force\\nservo brakes\\nmultiple passive-follower type robots\",\"520\":\"cameras\\nsensors\\nheuristic algorithms\\nvisualization\\nresource management\\nimage resolution\\noptimization\\ncomputational geometry\\nevent distribution\\nactivity distribution\\nreactive coverage control algorithm\\ngreedy gradient algorithms\\npan camera network\\ntilt camera network\\nzoom camera network\\ncontinuous-and discrete-time first-order ptz camera dynamics\\ncoverage algorithms\\nlocally optimal coverage configuration\\nfirst-order ptz camera dynamics\\ncamera network allocation problem\\nsensing quality measures\\nconic voronoi diagrams\\nvisual sensing quality\\ntotal coverage quality\\ncamera orientations\\nptz camera networks\\nautomated active network reconfiguration\\nflexible visual monitoring\",\"521\":\"legged locomotion\\ntraining\\nwheels\\nhardware\\nhip\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nrobust control\\ntemporary modifications\\nphysical hardware\\nrobot leg\\nreward landscape\\nfast hopping\\nrobot controllers\\nengineering effort\\npotentially unstable parameters\\ntraining wheels\\nvideo synopsis\\nboom learning\\nrobustness\",\"522\":\"task analysis\\nuncertainty\\ncovariance matrices\\nadaptation models\\ncomputational modeling\\nparametric statistics\\npredictive models\\nbayes methods\\ngeneralisation (artificial intelligence)\\nlearning (artificial intelligence)\\nregression analysis\\nsearch problems\\nglobal parametric model\\nmodel-free policy search agent\\nmodel selection\\nbayes method\\nonline incremental learning\\ndata uncertainty\\nreinforcement learning\\nprobabilistic models\\nmotor primitives\\ndata efficient guided exploration\",\"523\":\"stochastic processes\\nrobots\\nsearch methods\\ntask analysis\\nentropy\\ncomputational modeling\\nkernel\\nlearning systems\\nnonparametric statistics\\noptimisation\\nsearch problems\\nlearning control\\ndirect policy search\\ncomplex problems\\nnonparametric methods\\nrobot skill learning\\nmemory-based learner\\nhybrid controller\\nmemory-based non-parametric stochastic search methods\\ncomputing schedules\\nrobot controller parameter optimisation\",\"524\":\"mathematical model\\ndata models\\nlearning (artificial intelligence)\\ncomputational modeling\\nmobile robots\\ngenetic programming\\ngenetic algorithms\\ninternet\\npendulums\\ntime-varying systems\\ntime varying dynamics\\ncontrolling systems\\ndata driven construction\\nonline\\nsingle node genetic programming\\nsngp\\npendulum swing up problem\\ntraining data\\naccurate models\\nreal-time experiments\\nsimulated mobile robot\\nrealtime robot control\\nanalytic equations\\nparsimonious models\\nsymbolic regression\\nacceptable policy\\nrl\\nreinforcement learning\\nsymbolic process models\\nmodel learning for control\\nai-based methods\\noptimal control\",\"525\":\"task analysis\\nrobot sensing systems\\nindoor navigation\\naerospace electronics\\nlearning (artificial intelligence)\\nmobile robots\\nnavigation\\nneural nets\\npath planning\\nprobability\\nrobot dynamics\\nrobot vision\\nsampling methods\\nsampling based planner\\nhierarchical method\\nsampling based path planning\\nlarge scale topology\\nprobabilistic roadmaps\\nfeature based deep neural net policies\\ncontinuous state\\naction spaces\\nsimulation\\noffice environments\\naerial cargo delivery\\nurban environments\\nload displacement constraints\\ntrajectories\\nnoisy sensor conditions\\nflights\\ntraining\\nprm rl\\nlong range robotic navigation tasks\\npoint to point navigation policies\\nend to end differential drive indoor navigation\\nnontrivial robot dynamics\\nrobot configurations\\ntask constraints\\ncapture robot dynamics\\nrl agent\\nreinforcement learning\",\"526\":\"robots\\ndata models\\nmathematical model\\ncomputational modeling\\nheuristic algorithms\\nanalytical models\\ntask analysis\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nlegged locomotion\\noptimisation\\nsearch problems\\nparameterized black-box priors\\nrobotics\\ndata-efficient algorithms\\nreinforcement learning\\ndynamical model\\nblack-box optimization algorithm\\nmodel-based policy search approaches\\nmodel learning procedure\\nhigh-dimensional systems\\nphysical hexapod robot\\nblack-drops algorithm\",\"527\":\"computational modeling\\nnavigation\\nlearning (artificial intelligence)\\nrobots\\ntask analysis\\nprediction algorithms\\nplanning\\nmobile robots\\npath planning\\nrobot vision\\ndouble q-learning\\nself-supervised deep reinforcement learning\\nself-supervised training\\nmodel-based methods\\nvalue-based model-free methods\\nlearning-based methods\\nplanning method\\ninternal map\\nrobot navigation\\ngeneralized computation graph\",\"528\":\"feature extraction\\nip networks\\ncameras\\noptimization\\nvisual odometry\\nsimultaneous localization and mapping\\ncomputational efficiency\\ndistance measurement\\nrobot vision\\nslam (robots)\\ndirect line guidance odometry\\npixel intensities\\nline-based features\\npoint-based direct monocular visual odometry method\\nvisual odometry algorithms\\nkeypoint selection\",\"529\":\"cameras\\nlaser radar\\nthree-dimensional displays\\nvisualization\\nsimultaneous localization and mapping\\noptimization\\nimage matching\\nmotion estimation\\nmotion measurement\\noptical radar\\noptical sensors\\noptical tracking\\noptical windows\\nportable instruments\\nslam (robots)\\nsparse depth information\\npose-graph slam\\nkitti odometry benchmark datasets\\ndirect visual slam\\nmonocular camera\\nlight detection and ranging\\nportable camera-lidar mapping system\\ndirect visual simultaneous localization and mapping\\nsliding window-based tracking method\\ndepth-integrated frame matching\\nfeature-based visual lidar mapping\\nsensors\",\"530\":\"simultaneous localization and mapping\\ncameras\\nthree-dimensional displays\\ntrajectory\\nbayes methods\\nobject detection\\nimage reconstruction\\nlearning (artificial intelligence)\\nrobot vision\\nslam (robots)\\nmonocular slam system\\nbayesian framework\\ndeep-learning based generic object detector\\ndetection region\\nscale drift\\nmonocular systems\\nbayesian scale estimation\\ngeneric object detection\\nlocal scale correction\\nobject class detection\\nkitti dataset\\nquantitative evaluations\",\"531\":\"optimized production technology\\nsimultaneous localization and mapping\\nuncertainty\\ntask analysis\\nrobot kinematics\\ncomputational complexity\\nconvex programming\\nleast squares approximations\\nminimisation\\nmobile robots\\npath planning\\npredictive control\\nquadratic programming\\nrobot vision\\nslam (robots)\\ngraph topology\\nactive slam problem\\nrobot trajectory\\narea coverage task\\nmodel predictive control framework\\nuncertainty minimization mpc problem\\ngraphical structure\\n2d feature-based slam\\nvariable substitutions\\nconvex optimization method\\nmpc framework\\nsequential quadratic programming method\\nlinear slam\\nsubmap joining approach\\nplanning\\nnonconvex constrained least-squares problem\",\"532\":\"simultaneous localization and mapping\\ntwo dimensional displays\\nestimation\\nneural networks\\npredictive models\\nkalman filters\\nimage representation\\nmobile robots\\nmultilayer perceptrons\\npose estimation\\nrobot vision\\nslam (robots)\\nautonomous mobile robots\\nlarge scale urban environments\\nsimultaneous location and mapping\\nhybrid correction module\\nlikelihood distributions\\n2d likelihood slam approaches\\nsuccessive estimated poses\\nensemble multilayer perceptron model\\nslam estimations\\nsystematic errors\\nsensor measurement errors\\nslam map representation\\nobservation model\\nmotion model\\nprobabilistic formulation\",\"533\":\"feature extraction\\nsensor phenomena and characterization\\nsimultaneous localization and mapping\\noptimization\\nmobile robots\\nobject recognition\\nrobot vision\\nsensor fusion\\nslam (robots)\\nomnidirectional multisensory perception fusion\\nlong-term place recognition\\nlong-term autonomy\\nomnidirectional sensors\\nomnidirectional observation\\nmultidirectional place recognition\\nomnidirectional multisensory data\\nappearance variations\",\"534\":\"gyroscopes\\ncameras\\nquaternions\\naccelerometers\\ncalibration\\nsimultaneous localization and mapping\\ngravity\\ninertial navigation\\niterative methods\\nmobile robots\\noptimisation\\nrobot vision\\nslam (robots)\\nextrinsic orientation\\nextrinsic translation\\naccelerometer bias\\ncamera-imu extrinsic parameters\\ninitial values\\nvisual scale\\ninitialization stage\\nmechanical configuration\\nsensor suite changes\\nonline initialization method\\ntranslation calibration\\ninitialization procedure\\ngyroscope bias\\nmonocular visual-inertial slam techniques\\ngyroscope\\ngravitational magnitude\",\"535\":\"sonar\\ncameras\\nvisualization\\nsonar navigation\\nsimultaneous localization and mapping\\nunderwater structures\\noceanographic techniques\\nslam (robots)\\nunderwater sound\\nunderwater vehicles\\nacoustic range data\\nsonar visual inertial slam\\nvisual-inertial state estimation package\\nresource management\\nmarine archaeology\\nunderwater acoustic sensor\\nunderwater cave\\nunderwater wrecks\\nunderwater domain\",\"536\":\"trajectory\\nattitude control\\nstandards\\nacceleration\\naerospace electronics\\naustralia\\nrobustness\\nhelicopters\\nmobile robots\\nstability\\ntrajectory control\\nflight envelope\\nhierarchical control\\nquadrotor flight\\ndifferential flatness transformation\\nstability issues\",\"537\":\"task analysis\\npropellers\\nwebcams\\nunmanned aerial vehicles\\ninspection\\nforce\\ncontrol systems\\naircraft control\\nautonomous aerial vehicles\\nfinite state machines\\nmobile robots\\nrobot vision\\ntarget tracking\\ninteracting-boomcopter uav\\nremote sensor mounting\\nsensor package\\nvertical surface\\nunmanned aerial vehicle\\non-board webcam\\nreversible propeller\\naerial manipulation task\\nvehicle design\\nimage processing algorithms\\nextended finite state machine\\nhigh-level autonomous control\\nautonomous control strategy\\ni-bc platform\\nautonomous sensor\",\"538\":\"heuristic algorithms\\ntrajectory\\ncost function\\nmathematical model\\nvehicle dynamics\\nload modeling\\ncomputational modeling\\naerospace robotics\\ncollision avoidance\\nhelicopters\\nmobile robots\\npredictive control\\nrobot dynamics\\npath planning\\ntrajectory generation algorithms\\nmpc\\nsequential linear quadratic\\nslq\\nobstacle-avoidance algorithm\\nmodel predictive control\\ndynamic environments\\nplanning algorithms\\nmultirotor\\nsuspended load\",\"539\":\"bars\\nforce\\nunmanned aerial vehicles\\ndynamics\\nmathematical model\\ntrajectory\\nautonomous aerial vehicles\\nstability\\nthree-term control\\nasymmetric collaborative bar stabilization tethered\\nrigid links\\ntensile forces\\ncontrol objective\\npid control law\\ndecoupled motions\\ncascaded motions\\nsystem asymmetries\\ncable lengths\\nuav\\nsystems physical parameters\",\"540\":\"surgery\\nneedles\\nprobes\\nretina\\nimpedance\\nbiosensors\\nblood vessels\\ndiseases\\nelectric impedance\\nelectric impedance measurement\\neye\\nmedical image processing\\npatient diagnosis\\nvision defects\\nthrombolytic agent flushing\\npatient eye lens\\ndouble puncture event detection\\nretinal vein occlusion treatment\\ninnovative bio-impedance sensor\\neye surgery\\nclotted retinal vessels\\nsize 50 micron to 400 micron\",\"541\":\"force\\nrobot sensing systems\\ntendons\\nfiber gratings\\nbragg gratings\\nend effectors\\nendoscopes\\nfeedback\\nfibre optic sensors\\nforce sensors\\nfriction\\nhaptic interfaces\\nmedical robotics\\nsurgery\\ndistal end haptic sensing\\ncompression force\\ntension force\\nsurgical end-effectors\\nmechanics analysis\\nverification tests\\ntendon-sheath driven grasper\\ntsms-driven systems\\ndistal end force sensing\\noptical fiber bragg gratings\\ntendon-sheath mechanisms\\nhaptic feedback\\nendoscopic surgical robots\\ntransmission systems\\nnonlinear friction profiles\\nnitinol tube\\nfbg fiber\\nrobotic fingers-hands\\nwearable devices\\nrehabilitation devices\\nhaptic sensing\\nfiber bragg gratings\\nflexible surgical endoscopic robot\",\"542\":\"trajectory\\nrobot sensing systems\\noptimization\\nuncertainty\\nbayes methods\\ntumors\\nend effectors\\ngaussian processes\\nknowledge acquisition\\nlearning (artificial intelligence)\\nmedical robotics\\nmobile robots\\nposition control\\nrobot kinematics\\nsurgery\\ntrajectory optimisation (aerospace)\\ntumours\\nuncertain systems\\nstiffness distribution\\npalpation path\\nacquisition function\\nactive learning algorithm\\nincorporate uncertainties\\nrobot position\\nsensor measurements\\nrobot-kinematics\\ntrajectory-optimized sensing\\ntissue abnormalities\\nda vinci research kit\\ninsertable robotic effector platform\\nrobotic surgery\\n6-dof industrial arm\\ndvrk\\nirep\",\"543\":\"quaternions\\nsurgery\\ntools\\ntask analysis\\nkinematics\\nmanipulators\\nbrain\\ncollision avoidance\\nmedical robotics\\nneurophysiology\\nvectors\\ndeep brain neurosurgery\\ntremor-free procedures\\nrobotic assistance\\nsurgical robots\\nmanipulator-boundary collisions\\nvector field inequality\\nactive constraints\\nsurgical tool tips\\nendonasal surgery\",\"544\":\"knee\\nprosthetics\\noptimization\\ntorque\\nneuromuscular\\ntrajectory\\ngait analysis\\nhandicapped aids\\nmedical robotics\\nadvanced prosthesis controls\\ncontrol parameters\\noptimization method\\noffline portion\\nintact subject gait data\\nneuromuscular control policy\\nankle prosthesis\\nhigh-dimensional parameter spaces\\nparameter selection process\\noffline optimization procedure\\ndueling bandits problem\\nassistive lower-limb devices\\ncontrol policies\\nlower limb assistive devices\\nonline optimization\",\"545\":\"robot sensing systems\\nmagnetic separation\\nmagnetic levitation\\nsensor fusion\\nmagnetic cores\\nmagnetic resonance imaging\\nendoscopes\\nlearning (artificial intelligence)\\nmagnetic sensors\\nmedical robotics\\nsensor fusion techniques\\nendo-vmfusenet\\nasymmetric sensor data\\nasynchronous sensor data\\ndeep learning\\nactive medical robots\\npassive capsule endoscopes\\nmedical device companies\\nendoscopic capsule robots\\ndeep visual-magnetic sensor fusion approach\",\"546\":\"robot sensing systems\\nswitches\\nkalman filters\\nproposals\\nendoscopes\\nmagnetic resonance imaging\\ndistance measurement\\nlearning (artificial intelligence)\\nmedical robotics\\nparticle filtering (numerical methods)\\npose estimation\\nrecurrent neural nets\\nrobot vision\\nsensor fusion\\nmultisensor fusion\\nendoscopy robots\\nendoscopic capsule robot trajectories\\nrecurrent neural network\\nnonlinear kinematic model\\nsensor reliability\\nonline estimation\\nparticle filter\\ngastrointestinal tract\\ntherapeutic technology\\nswitching state-space model\\nparticle filtering-based multisensory data fusion\",\"547\":\"legged locomotion\\nforce control\\nparallel robots\\nforce\\naerospace electronics\\nactuators\\ncontrol system synthesis\\nmotion control\\nrobot kinematics\\ntorque control\\nspatial force control algorithm\\nseries elastic actuators-driven parallel robot\\nvirtual ground robot\\nrfseas\\nreaction force-sensing series elastic actuator\\ntorque generation\\nforce generation\\nkinematics\\nvgr motions\",\"548\":\"stability criteria\\nrobots\\ntendons\\nforce\\nloading\\nactuators\\ncontrol system synthesis\\ndexterous manipulators\\nelasticity\\nforce control\\nmanipulator kinematics\\nposition control\\nstability\\nseries elastic tendon-driven robotic hands\\ndexterous manipulation\\nrobotic hand design\\nfingertip force directions\\ncartesian stiffness control\\nposition dependent fingertip forces\\nstability conditions\\ncartesian stiffness controllers\\npassive joint coupling\\ngeneralized passivity based stability boundary\\ncartesian stiffness controlled series elastic tendon-driven robotic fingers\",\"549\":\"dynamics\\nimpedance\\nforce\\naerospace electronics\\nrobots\\ntask analysis\\njacobian matrices\\nforce control\\nfriction\\nmanipulator dynamics\\nmotion control\\nunknown object dynamics\\nprojected inverse dynamics approach\\nmultiarm cartesian impedance control\\nmodel-based control framework\\nmultiarm manipulation\\ncontrol law\\nconstrained subspaces\\nunconstrained subspaces\\nunconstrained components\\nmotion task\\ncartesian impedance behaviour\\nconstrained component enforces contact\\nfriction constraints\\ncontact forces\\nconstrained subspace\\ncontact points\\ndual-arm platform\",\"550\":\"robot sensing systems\\nforce\\nmobile robots\\ncollision avoidance\\noscillators\\ncompliance control\\nfiltering theory\\nforce sensors\\nhuman-robot interaction\\nmotion control\\nnavigation\\nneurocontrollers\\npath planning\\ntorque control\\ncompliant mobile robots\\nmobile robot navigation\\ndirect physical contact\\nintuitive communication\\nphysical interaction\\ndisturbance forces\\nmobile platform\\nneural network approach\\ndistance sensors\\nmobile robot applications\\nwhole-body sensory\\nmodel-free filtering approach\\n6-dof force-torque sensor\\nrobot-human interaction\",\"551\":\"planning\\nrobots\\ntrajectory\\nuncertainty\\ntask analysis\\naerospace electronics\\ndynamics\\nassembling\\ncad\\nfriction\\ngeometry\\nmobile robots\\noptimal control\\npath planning\\nplanning (artificial intelligence)\\nprobability\\nrobot dynamics\\nuncertain systems\\ncompliant assembly\\noptimal belief space planning\\nautomated manufacturing\\nnonlinear contact-dynamics\\nmodel parameters\\nbelief space planning problem\\ncompliant system\\nasymptotically optimal belief space planner\\nkinodynamic motion planner\\nasymptotic optimality\\nmultiple assembly tasks\\ncad models\\nstate spaces\\nobject poses\\nimpedance-control\\nnondeterministic domains\\nprobabilistic completeness\",\"552\":\"task analysis\\nrobot kinematics\\nmanipulators\\nuncertainty\\nkinematics\\nestimation\\ndual-arm robot\\ndual-arm manipulation\\nmotion directions\\nmotion constraints\\ncoordinated task space frameworks\\nredundancy exploitation\\nrobot arms\\ntwo degrees-of-freedom articulated object\",\"553\":\"iron\\nactuators\\nskin\\nfabrics\\nforce\\nshape\\nstrain\\nbending\\ncontrol system synthesis\\nelasticity\\nforce control\\nmedical robotics\\nmotion control\\npatient monitoring\\npneumatic actuators\\npolymers\\nmanual lymphatic drainage\\nhuman arm\\nlateral force\\npolymer element\\nhyperelastic polymer\\nmechanical elements\\nrobotic device\\nsoft pneumatic fabric-polymer bending actuator\\nrigid actuators\\nsoft actuators\\nwearable biomedical devices\\nsoft pneumatic fabric-polymer actuator\\nlymphedema treatment\\nactuator motion\\nhyperelastic beam\\npolymer beam\\nfabric element\",\"554\":\"sensors\\nsoft robotics\\nforce\\nmuscles\\nbiological tissues\\nactuators\\nbiomechanics\\nclosed loop systems\\nforce control\\nmedical robotics\\npatient treatment\\nforce tracking variability\\nforce-controlled actuation patterns\\nsoft robotic system\\nopen-loop pressure-based control\\nsoft robotic force control device\\nsinusoidal force profiles\\nclosed-loop force control methodology\\nmanual mechanotherapy practices\\nmassage-magnitude forces\\nclosed-loop force control system\\nfully soft sensors\\nmuscular tissue\\ntissue regeneration\\njudicious force application\\nsoft tissues\\nmechanotherapeutic applications\\nsoft robotic wearable devices\\nbiomedical applications\\nsoft robotic devices\\ntextile-based soft wearable robots\",\"555\":\"actuators\\nhaptic interfaces\\nsoft robotics\\nskin\\nelectron tubes\\npneumatic systems\\nforce feedback\\npneumatic actuators\\npolymers\\nhapwrap\\nsoft growing wearable haptic device\\nlightweight wearable haptic devices\\nflexible low density polyethylene\\ndirectional force feedback\\nhaptic feedback device\\nmechanoreceptors\\nair flow control\\ndistributed touch feedback\",\"556\":\"legged locomotion\\nhip\\nthigh\\nexoskeletons\\nacceleration\\nforce\\nactuators\\nbiomechanics\\ngait analysis\\nmedical robotics\\npatient rehabilitation\\nportable hip-only\\naugmenting human walking\\ndifferent fixed assistance profiles\\nonline classification algorithm\\nmass potential energy fluctuations\\nabdomen-mounted imu\\nmaximum hip extension\\nautonomous wearable robot\\nassistance profile individualization\\nhip extension assistance\\nonline walking\\nautonomous hip-only\",\"557\":\"solid modeling\\ncollaboration\\nmanipulators\\nelbow\\nload modeling\\nprototypes\\nergonomics\\ngroupware\\nhuman computer interaction\\nhuman-robot interaction\\nmanipulator kinematics\\nmedical robotics\\nservice robots\\nuser interfaces\\ncollaborative tool\\nhuman-human collaboration\\nhuman ergonomic wear limits\\nrobot autonomy\\nlightweight wearable robotic augmentation device\\nhuman-wearable collaboration\\nwearable robotic forearm\\nclose-range human-robot collaboration\\nlightweight supernumerary third arm\\nshared workspace activities\\nfunctional prototype\\niterative design process\\nreachable workspace\\nnatural human reach\",\"558\":\"trajectory\\nheuristic algorithms\\nservomotors\\nreal-time systems\\naerodynamics\\nrobot sensing systems\\naerospace components\\naerospace robotics\\nlearning (artificial intelligence)\\nsearch problems\\nefficient lift generation\\npolicy search algorithm\\nreal-time robotic learning problem\\ndynamically scaled flapping robotic wing\\ndegrees-of-freedom\\nmineral oil\\nreynolds number\\noptimal wing pitching amplitude\\nstroke-pitch phase difference\\naerodynamic efficiency\\nquasisteady aerodynamic mechanism\\nwing rotation\\nstroke reversal\\nunsteady lift generation mechanisms\\nstroke amplitude range\",\"559\":\"tendons\\nstrain\\nadaptation models\\nhardware\\nrobot kinematics\\ninspection\\nmobile robots\\nmotion control\\nposition control\\nshape control\\nmotion generation\\nrobot tendril hardware\\ninternational space station\\nnasa johnson space center\\ncontinuum robot backbones\\ntheoretical plant growth-inspired approach\\ninspection operations\\nlong thin continuum robot exploration\\nvine-inspired movement strategies\\nrobot access\\nthin-stemmed plants\\nvine-inspired continuum robots\",\"560\":\"robots\\naerodynamics\\ntrajectory\\nheuristic algorithms\\nelbow\\naerospace components\\nbiomimetics\\nmobile robots\\nposition control\\nrobot dynamics\\nrolling torques\\npitch torque generation\\nmassive morphing wings\\nbio-inspired bat-like robot\\ninertial effects\\nwing shape\\nrobotic platform\\nmassive morphing-wings\\nwingbeats\",\"561\":\"perturbation methods\\ntask analysis\\ntrajectory\\nrobots\\nmathematical model\\nstability analysis\\njacobian matrices\\nhuman-robot interaction\\npendulums\\nperturbation techniques\\nrobust control\\ntrajectory control\\nhuman control strategy\\nsuspended pendulum\\ncart-pendulum system\\nassistive perturbations\\nresistive perturbations\\ntrajectory stability\\ncart trajectories\\nrobust control strategies\\ndynamically complex physical interactions\\nstability properties\\nhuman-object interaction\\nsimplified 2d model\\nvirtual implementation\",\"562\":\"task analysis\\nrobot sensing systems\\nnavigation\\ncollision avoidance\\ncameras\\nimage color analysis\\nautonomous aerial vehicles\\nimage colour analysis\\nmobile robots\\npath planning\\nrobot vision\\nstereo image processing\\nmultiroom exploration task\\ndelfly explorer\\nautonomous indoor exploration mission\\nroom exploration\\nstereo-vision based droplet algorithm\\nheading-based door passage algorithm\\nflapping wing vehicles\\nautonomous exploration tasks\\nautonomous multiroom exploration\\nwing vehicle\\nmavs\\nautonomous indoor navigation\\nrotary wings\\nflapping wing mav\\nstereo vision system\\nmicroair vehicles\\nmonocular color based snake-gate algorithm\",\"563\":\"trajectory\\nwheels\\nspinning\\nmobile robots\\nmathematical model\\nrobustness\\nadaptive control\\ncontrol system synthesis\\nfeedback\\nlegged locomotion\\nmotion control\\nnonlinear control systems\\nrobust control\\ntrajectory control\\nvariable structure systems\\nrobust trajectory control\\nspinning robot mechanism\\nrobot stability\\ntrajectory following accuracy\\nwheel velocity response\\nasmc controller\\ntrajectory following control\\nminiature spherical rolling robot\\nnonlinear adaptive sliding mode\\nlocomotory rolling patterns\\nroll angle stability\\nismc controller\\nintegral sliding controller\\nspherical robot\\nrolling gait\\ncentral pattern generator (cpg)\\ntrajectory following\\nadaptive sliding mode (asmc) control\",\"564\":\"knee\\nlegged locomotion\\njoints\\nbiological system modeling\\nhip\\nmuscles\\nbiomechanics\\nmanipulator kinematics\\nmotion control\\ntensegrity flexural manipulator\\nopensim simulation environment\\ntension analysis\\nhuman leg behavior\\ntensegrity manipulator\\nrevolute joint\\nrobotics literature model\\nbio-inspired tensegrity flexural joints\",\"565\":\"measurement\\nforce\\nkinematics\\nmanipulators\\nellipsoids\\nrandom variables\\ngrippers\\nmanipulator kinematics\\npath planning\\nprobability\\nrobust control\\narm configurations\\nforce closure region\\nkinematic robot structure\\narm kinematic noise propagation\\ngrasp quality evaluation\\nlocal robustness\\narm configuration\\ngrasp quality metric\\nredundant robot\",\"566\":\"planning\\ndeformable models\\nrobots\\nfinite element analysis\\nstrain\\ncomputational modeling\\ntask analysis\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nneurocontrollers\\npath planning\\nposition control\\nrealtime planning\\nhigh-dof deformable bodies\\narbitrarily-shaped volumetric deformable bodies\\ncomplex environments\\nhigh-dimensional configuration spaces\\ndynamics constraints\\ntwo-stage learning method\\nmultitask controller\\ndynamic movement primitives\\nneural-network controller\\ndmp task\\nfinite element method\\ncontact invariant optimization\\ngradient-based method\\ntwo-stage learning algorithm\\ntrained dmp controller\\ndifferent navigation tasks\\nlearned motion planner\\nwalking deformable robots\\nobstacle avoidance\\ndeep q-learning\",\"567\":\"grasping\\nrobots\\nthree-dimensional displays\\ncolor\\ngeometry\\ngrippers\\nvisualization\\nconcave programming\\ndexterous manipulators\\nnonconvexity\\nsupport surface\\ngrasp strategy\\nenvironmental contact\\nnonconvex geometry\\nobject-surface combination\\ndomestic flat objects grasping\",\"568\":\"grasping\\ngrippers\\nrobots\\nshape\\ngeometry\\ntopology\\nrobustness\\nobject detection\\nrobot vision\\nshape embedding space\\nrobot gripper\\nsurface geometry\\ncaging grasps\\ncaging loops\\ntarget object\",\"569\":\"three-dimensional displays\\nrobustness\\nrobots\\nanalytical models\\nseals\\ncomputational modeling\\nplanning\\nconvolution\\ndexterous manipulators\\nend effectors\\nfeedforward neural nets\\ngrippers\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmanipulator dynamics\\ndeep learning\\nvacuum-based end effectors\\nmultifinger grippers\\nsuction cup\\nexternal wrenches\\npneumatic suction gripper\\npoint clouds\\ngrasp quality convolutional neural network\\nrobust vacuum suction grasp targets\\ngravity wrench\\nparallel-jaw grippers\\nobject pose\\nmaterial properties\\ngq-cnn\\nabb yumi\\nadversarial\",\"570\":\"robots\\ntask analysis\\nneural networks\\nthree-dimensional displays\\nhead\\nvisualization\\ngrippers\\ncontrol engineering computing\\nhuman-robot interaction\\nlearning by example\\nmanipulators\\nneural nets\\nrobot programming\\nrobot vision\\ntelerobotics\\nvirtual reality\\nvirtual reality teleoperation\\nrobot skill acquisition\\nraw pixels\\nconsumer-grade virtual reality headsets\\nhand tracking hardware\\ndeep neural network policies\\nmanipulation tasks\\ndeep imitation learning\\npr2 robot\\nrgb-d images\",\"571\":\"testing\\ntraining\\nautonomous vehicles\\ntrajectory\\nadaptation models\\nhistory\\ncollision avoidance\\nlearning (artificial intelligence)\\nlife testing\\nmobile robots\\nneurocontrollers\\nregression analysis\\nautonomous vehicle\\nimitation learning\\nsurrogate agents\\ntest scenario generation\\nperformance modes\\ndeep neural networks\\nimitator surrogates\\nmission performance\\nsimulation-based testing\\non-line imitation\\ncomplex mission\\ntarget vehicle\\nbehavioral modes\\ndataset aggregation\",\"572\":\"robots\\nthree-dimensional displays\\npredictive models\\nsolid modeling\\nkernel\\nprobability density function\\ntraining\\ncad\\nlearning (artificial intelligence)\\nmanipulators\\ncontact models\\nmotion models\\npoint cloud object model\\ncad model\\ncontact-based predictors\\nrobotic push manipulation\\nfeature-based transfer learning\",\"573\":\"grammar\\nrobots\\ntask analysis\\nprobabilistic logic\\nsequential analysis\\nmarkov processes\\noptimization\\nbayes methods\\ncontext-free grammars\\nformal languages\\ngrammars\\nhumanoid robots\\nlearning (artificial intelligence)\\nmonte carlo methods\\nmotion control\\nprobability\\nrobot dynamics\\nrule-based nature\\nformal grammars\\ncomplex robot policies\\ncomposing primitives\\nmodern robotics\\ninducing probabilistic context-free grammars\\nsimple movement primitives\\ncomplex sequences\\ndegree-of-freedom lightweight robotic arm\\nmarkov chain monte carlo optimization\\ngrammar space\\nrobot movement primitives\\nphysical nature\\nyet unsolved challenge\\ncomplicated challenge\\nway robot policies\\nhierarchical concept\\nrecursively structured tasks\\nhierarchically tasks\",\"574\":\"task analysis\\ntraining\\nneural networks\\nrobot sensing systems\\nrobustness\\nstacking\\nimage colour analysis\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\nrobots\\nconvolutional pose machines\\nhuman-readable plans learning\\ndomain randomization\\nbaxter robot\\nimage space\\nsynthetic images\\nperception network\\nprogram execution\\nprogram generation\\nhuman-readable program\\nsynthetically trained neural networks\\nworld space\",\"575\":\"task analysis\\ntrajectory\\nrobot kinematics\\noptimization\\nfeature extraction\\nrobot sensing systems\\ngaussian processes\\nhumanoid robots\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmanipulators\\nmixture models\\nmotion control\\nrobot programming\\ngeneralized task-parameterized skill learning\\nhuman skills\\ntask-parameterized gaussian mixture model\\ntp-gmm\\nhuman-robot collaboration\\ndual-arm manipulation\\nlearning framework\\ntask parameters\\nrobot joint limits\\ntask-parameterized learning\\nlearned skills\\nreal robotic systems\\ntask constraints\\nlearning perspective\\nprogramming by demonstration\",\"576\":\"task analysis\\neducation\\ntrajectory\\nrobot sensing systems\\nvisualization\\nservice robots\\nautomatic programming\\ncomputer aided instruction\\ncontrol engineering computing\\ndata visualisation\\nfeedback\\nhuman-robot interaction\\nrobot programming\\nrobots\\nteaching\\nrobot learners generalisable skills\\ndemonstration data sets\\nambiguous demonstrations\\nteaching phase\\ninteractive teaching process\\nrobust teaching process\\nhuman teachers\\nheuristic rules\\nrobot learners teaching\\nundemonstrated states\\nvisual feedback\\nprogramming by demonstration\\npbd\\nfeedback visualisation\",\"577\":\"robot sensing systems\\nplanning\\ntask analysis\\ngrippers\\nlaser radar\\ncollision avoidance\\nmobile robots\\nsensors\\ndifferential drive robot\\nlidar sensor\\npassive objects\\ndeliberative planner\\nsymbolic commands\\nobstacle avoidance\\nreactive planner\\nsensor-based reactive symbolic planning\\nnonconvex environments\\nconvex obstacles\\nhigh-level commands\",\"578\":\"entropy\\nmutual information\\nrobot sensing systems\\nprediction algorithms\\nreal-time systems\\nperiodic structures\\nautonomous underwater vehicles\\nenvironmental science computing\\nmobile robots\\noptimisation\\nsampling methods\\nset theory\\nspatiotemporal phenomena\\noptimal irrevocable sample selection\\nperiodic data streams\\nmarine robotics\\nclassical secretary problem\\nrandom order\\nenvironmental monitoring domains\\nspatiotemporal structure\\nrepresentative samples\\nperiodic structure\\nmonotone submodular utility function\\nmartha's vineyard coastal observatory\\nphytoplankton sample locations\\ninformation-theoretic sense\\nperiodic secretary algorithm\\ntheoretical performance guarantees\\nsample selection algorithm\\nenvironmental dataset\\noptimal sample set\",\"579\":\"temperature measurement\\nlakes\\nmicroorganisms\\nrobot sensing systems\\ntrajectory\\ntemperature sensors\\nautonomous underwater vehicles\\nmobile robots\\nreal-world operation\\nadaptive sampling missions\\nauv\\nautonomous feature tracing\\nreal-world underwater environments\\nunderwater environmental sensing\\ncompact high resolution\\ntemperature sensing module\\nmicrostructure\\nturbulence measurements\\nsensing requirements\\nhorizontal variation capture\\nwater bodies\",\"580\":\"planning\\nlevel set\\nnavigation\\nvehicle dynamics\\nautonomous vehicles\\ncollision avoidance\\ncost function\\nmobile robots\\nmulti-agent systems\\nroad vehicles\\nrisk level set\\ncongested environment navigation\\ncluttered environment\\ncongestion cost\\noccupancy risk\\nplanning space\\nagent planning\\nautonomous vehicle driving\\nrisk threshold\\nconservative behavior\\naggressive behavior\",\"581\":\"batteries\\nunmanned aerial vehicles\\ncharging stations\\nland vehicles\\noptimization\\nmonitoring\\nplanning\\nautonomous aerial vehicles\\nbattery powered vehicles\\ncomputational complexity\\ntravelling salesman problems\\nvehicle routing\\nmobile recharging stations\\nenergy-limited unmanned aerial vehicle\\nstationary recharging stations\\nunmanned ground vehicles\\nugv\\ntraveling salesperson problem\\nstationary charging stations\\nuav mission\\nrouting\\nnp-hard\\ngeneralized tsp\",\"582\":\"planning\\nrobot kinematics\\nsimultaneous localization and mapping\\nlinear programming\\naerospace electronics\\nbayes methods\\ngraph theory\\nmulti-robot systems\\npath planning\\ntopology\\ngraph pruning\\ntopological properties\\nfactor graphs\\ntopological space\\nembedded state space\\nhigh-dimensional state spaces\\nannounced path approach\\ntopological multirobot belief space planning\\nbsp approaches\\nfactor graph representation\\nposterior beliefs\",\"583\":\"legged locomotion\\nfoot\\ngravity\\nhip\\ndamping\\ntorso\\nstability analysis\\ngait analysis\\nmotion control\\nstability\\ntrajectory control\\nbiped walking\\nstabilization\\ncontrol law\\nankle torques\\ncounterweight joint\\nenergy input\\nnumerical simulations\\nsemicircular feet\\ncompliant legs\\npassive fixed-point trajectories\\nbipedal robots\\nzero-slope walking\\npassive gaits\\nstable gaits\\nnonlinear pd terms\\nvirtual-gravity components\",\"584\":\"legged locomotion\\niterative closest point algorithm\\ntrajectory\\nplanning\\nacceleration\\nfoot\\nhumanoid robots\\nquadratic programming\\nreachability analysis\\nrobot kinematics\\nstraight-leg walking\\nwhole-body control\\nnatural gait\\nbipedal robots\\nstraightened legs\\ncomplex height planning\\nwhole-body controller\\nstraightest possible leg configuration\\nrun-time\\ncontroller solutions\\nleg joint angle objectives\\nnull-space\\nquadratic program motion objectives\\ntoe-off motion\\nkinematic reachability\",\"585\":\"legged locomotion\\nvalves\\ncomputational modeling\\nactuators\\natmospheric modeling\\nforce\\npneumatic actuators\\nhop height every step\\ndiscontinuous terrain\\nsafe footholds\\nbipedal running\\nquadrupedal running\\nconstrained vertical hopping\\npneumatic robot\\nvertical height\\nhopping robot\\npneumatically actuated hopper\",\"586\":\"legged locomotion\\nrobot sensing systems\\nplanning\\ncollision avoidance\\nsurface treatment\\nmobile robots\\nmotion control\\noptimisation\\npath planning\\npose estimation\\nrobot dynamics\\nrobot kinematics\\nterrain mapping\\nrobust rough-terrain locomotion\\nnatural settings\\nindustrial settings\\nmotion planner\\nperceptive rough-terrain locomotion\\nsafe footholds\\ncollision-free swing-leg motions\\nacquired terrain map\\noptimization approach\\nsignificant obstacles\\nquadrupedal robot anymal\\nlocomotion planner\\ndynamic environments\\nurban settings\\npose optimization approach\",\"587\":\"legged locomotion\\nrobot sensing systems\\nlimit-cycles\\nrobot kinematics\\nadaptation models\\noscillators\\nfeedback\\nmotion control\\nneurophysiology\\nsensory feedback\\ninertial feedback\\nopen-loop control strategy\\ncomplexity\\nterrain steepness\\nchallenging terrains\\nsteep terrains\\nhexapod robot\\nsteep terrain\\nbody posture\\ncpg framework\\nlevel terrain\\nopen-loop gait generation\\ncpg models\\nlocomotive performance\\ngait adaptation\\nswimming legged robots\\ncrawling legged robots\\narticulated robots\\ngaits\\ncentral pattern generator models\\nunstructured terrain\\nstable locomotion\",\"588\":\"optimization\\ndynamics\\nrobots\\nkinematics\\ntorque\\nmathematical model\\ntrajectory\\nangular momentum\\nconcave programming\\nconvex programming\\nhumanoid robots\\nminimisation\\nmobile robots\\npath planning\\nposition control\\nrobot dynamics\\ntime optimal control\\nfixed timing\\nmotion plans\\ntiming optimization\\nnonconvex problem\\ntime-optimized dynamically consistent trajectories\\ncentroidal dynamics\\ntime variables\\nnonconvexity\\ncontact forces\\nmomentum trajectories\\nconvex relaxation\\ntrajectory optimization techniques\\nmulticontact scenarios\\ndynamically consistent motions\\ncentroidal momentum dynamics\",\"589\":\"needles\\naerospace electronics\\ntask analysis\\nkinematics\\nhaptic interfaces\\nmeasurement\\nsensors\\nbiomechanics\\ncognition\\nforce feedback\\nmedical robotics\\nrobot kinematics\\nsteering systems\\nsurgery\\ntelerobotics\\nrobotically steered needles\\njoint space control\\ncartesian space control\\nhub-centered steering\\nuser experience\\nuser cognitive workload\\nmuscle fatigue\\nhuman-centric metrics\\nhuman-centric evaluation\\nteleoperation algorithms\\nteleoperated systems\\nphysiological metrics\\ncognitive metrics\\nteleoperation performance\\nintuitive teleoperation\\nrobotic needle steering\\nkinematic metrics\\nteleoperation mappings\\nteleoperation strategies\\nsteering control mapping\",\"590\":\"robots\\nmicroscopy\\noptical microscopy\\npotential energy\\nbiomedical optical imaging\\ncollision avoidance\\ndecision making\\nmanipulators\\nmicromanipulators\\nmulti-robot systems\\nradiation pressure\\nhuman-guided optical manipulation\\nmultiple microscopic objects\\ncontrol systems\\nmultiple microobjects\\nrobotic control technique\\nautomated optical manipulation system\\nprecise manipulation\\nproductive manipulation\",\"591\":\"robots\\nimpedance\\ntask analysis\\ncollision avoidance\\npayloads\\ntrajectory\\ncorrelation\\nhuman-robot interaction\\nmobile robots\\ntelerobotics\\nhuman intervention\\ntele-interaction\\nshared-autonomy tele-interaction control approach\\nimpendance colliding\\nimpedance setting\\nphysical interactions\\nslave robot\\nhuman pilot\\nshared-autonomy control principles\\nautonomous impedance regulation\\nautonomous manner\\nphysical constraints\\nrobot platform\\ninteraction forces\\nphysical obstacles\\nremote robot\\nimpedance modulators\\nautonomous motion\\nmotion commands\\nremote workspace\\nhuman operator\\nremote environment\\nhazardous environments\\nrobotics teleoperation\\nimpedance regulation principles\",\"592\":\"aerospace electronics\\ngrasping\\nkinematics\\ntask analysis\\nteleoperators\\nshape\\ndexterous manipulators\\nmotion control\\ntelerobotics\\nintuitive control method\\npose spaces\\nlow-dimensional teleoperation subspace\\ncontinuous teleoperation subspace\\nnonanthropomorphic robot\\nteleoperation subspaces\\nteleoperation subspace mapping\\nintuitive hand teleoperation\\nnovice operators\\nhuman-in-the-loop manipulation\\nautonomous grasping\\ninput device\\nteleoperation methods\",\"593\":\"collision avoidance\\nhaptic interfaces\\nrobot kinematics\\nnavigation\\nlegged locomotion\\npredictive models\\nhuman-robot interaction\\nmobile robots\\nservice robots\\ntelerobotics\\ncollision scenario\\nhaptic communication channel\\nhuman movement\\nhuman-robot collisions\\nautonomous navigation\\npopulated environments\\nhuman-robot communication\\nnavigation tasks\\nhuman users\\nwearable haptic interface\\ndistinct haptic cues\\nvibration amplitudes\\nsingle human-single robot orthogonal encounter scenario\",\"594\":\"foot\\nmaster-slave\\nhumanoid robots\\ninterpolation\\ndynamics\\nreal-time systems\\nmotion control\\nrobot dynamics\\nsmoothing methods\\nstability\\ntelerobotics\\nmaster-slave operations\\nfoot landing delay prediction\\ntrajectory smoothing method\\nmaster-slave tennis swing experiment\\nhigh kick motion experiment\\nlife-sized humanoid robot jaxon\\nhigh speed whole body dynamic motion experiment\\nonline real time whole body master-slave control\\ndynamic whole body master-slave experiment\\nflexible master-slave operation\\nreal time master-slave humanoid robot system\",\"595\":\"dogs\\ncameras\\nrobot vision systems\\nnavigation\\nmobile robots\\nbiomimetics\\ncontrol engineering computing\\nconvolution\\nfeedforward neural nets\\nhandicapped aids\\nlearning (artificial intelligence)\\nrobotic guide dog\\ninterclass trail variations\\ndeep convolutional neural network\\nvirtual worlds\\nman-made trails\\npedestrian environments\\ncontact feedback\\ntactile trails\\nautonomous trail-following\\nvirtual real-world environments\\nvisually impaired\",\"596\":\"roads\\nimage segmentation\\nstrips\\nsemantics\\ntraining\\nautonomous vehicles\\ntask analysis\\nimage colour analysis\\nlearning (artificial intelligence)\\nneural net architecture\\nobject tracking\\ntraffic engineering computing\\nlost and found dataset\\ncomplementary features\\nrgbd input\\nhigh level features\\nlow level features\\nweight-sharing\\nmultistage training procedure\\nannotation process\\nautonomous driving\\non-road scenes\\nnovel network architecture\\nsmall obstacle discovery\\ndeep net architecture\\nmergenet\",\"597\":\"trajectory\\ndifferential equations\\nneural networks\\ncost function\\ntraining\\nrobot kinematics\\nbackpropagation\\nhandwriting recognition\\nhandwritten character recognition\\nneural nets\\ndynamic movement primitives\\ncost functions\\nraw image mapping\\nmnist database\\ndeep encoder-decoder network\\nassociated movement trajectories\\nperception-action couplings\\nencoder-decoder networks\\ncalculated movements\\nhandwriting movements\",\"598\":\"training\\ntask analysis\\nrobots\\ncomputational modeling\\ncontext modeling\\nestimation\\ncognition\\nboltzmann machines\\nimage classification\\nlearning (artificial intelligence)\\nrestricted boltzmann machines\\nhybrid deep boltzmann machine\\nscene classification dataset\\nbaseline models\\ndifferent objects\\nvisible nodes\\nbm\\nhybrid boltzmann machine\\ncontextualized scene modeling\\nscene reasoning tasks\",\"599\":\"proposals\\nrobots\\nobject detection\\nobject recognition\\nthree-dimensional displays\\nsemantics\\nfeature extraction\\ncontrol engineering computing\\nconvolution\\nfeedforward neural nets\\nimage classification\\nimage colour analysis\\nmanipulators\\nmobile robots\\noperating systems (computers)\\nrobot programming\\nrobot vision\\nscene classification\\nunified architecture\\nglobal scene features\\nregional object features\\ncontinuous robot beliefs\\nrobotics applications\\nrobot operating system\\nmobile manipulator\\nobject locations\\nnetwork predictions\\nsun rgbd dataset\\n3d space\\nunified convolutional neural network\",\"600\":\"feature extraction\\nrobots\\ncomputer architecture\\nobject detection\\ntraining\\nimage segmentation\\nmachine learning\\nimage classification\\nlearning (artificial intelligence)\\nmultiple object detection\\naffordancenet\\nobject localization\\nobject classification\\naffordance label\\nrobust resizing strategy\\ndeconvolutional layer sequence\\nreal-time robotic applications\\ntesting environments\\nend-to-end architecture\\nmultitask loss function\\naffordance mask\\nrgb images\\nobject affordance detection\\nend-to-end deep learning approach\",\"601\":\"feature extraction\\ncameras\\ncontext modeling\\ncomputer vision\\nlogic gates\\nneural networks\\nconvolution\\nfeedforward neural nets\\nlearning (artificial intelligence)\\npose estimation\\nslam (robots)\\ncnn-lstm model\\nsingle monocular image\\nconvolutional neural networks\\nimage-based localization\\ncontextualnet\\nimage content\\nindoor office space\",\"602\":\"robots\\nnavigation\\ntask analysis\\ntrajectory\\ncost function\\nfeature extraction\\nconvolution\\nfeedforward neural nets\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrandom processes\\ntrees (mathematics)\\nrobot social navigation\\nfully convolutional neural networks\\nhuman-aware path planning learning\\noptimal rapidly-exploring random tree planner\\nrobot navigation\\nclassification problem\",\"603\":\"planning\\nnetwork topology\\ntopology\\nconvolution\\nlearning (artificial intelligence)\\ntrajectory\\nprediction algorithms\\ncollision avoidance\\nmobile robots\\nneural nets\\npedestrians\\ntraffic engineering computing\\nmonolithic neural network\\ninverse reinforcement learning\\npedestrian prediction\\ndeep neural networks\\nautonomous vehicles\\ngoal-directed planning\\nmixture density function\\nmotion prediction\\nconvolutional network\\ntraffic participant prediction\\ntrajectories\",\"604\":\"predictive models\\ndecoding\\nhidden markov models\\nrecurrent neural networks\\nrobot kinematics\\ntrajectory\\nfeature selection\\nhuman-robot interaction\\nimage motion analysis\\nlearning (artificial intelligence)\\nmobile robots\\npose estimation\\nrecurrent neural nets\\nstochastic processes\\nhuman robot cooperation scenario\\nprediction model\\naction prediction dataset\\nhuman motion data\\nhuman-robot cooperation\\nrecurrent neural network approach\\nmultiple action sequences prediction\\nassistive applications\\nnonverbal cues\\nneural networks\\nhuman action prediction problem\\ncontinuous spaces\\ndiscrete spaces\\nencoder-decoder recurrent neural network topology\\ndiscrete action prediction problem\\naction sequences\\nstochastic reward\",\"605\":\"decoding\\ngallium nitride\\nhidden markov models\\ngenerative adversarial networks\\nrobots\\nrecurrent neural networks\\ngenerators\\nrecurrent neural nets\\ntext analysis\\nvideo signal processing\\nsequence to sequence model\\nbaxter robot\\nvirtual agent\\ngenerative adversarial synthesis\\ntext2action\\nmsr-video-to-text\\naction decoder rnn\\ntext encoder recurrent neural network\\ngenerative network\\nseq2seq\\nsequence model\\ngenerative adversarial network\\nhuman behavior\\nsentence\\nhuman action sequence\\ngenerative model\",\"606\":\"predictive models\\nrobots\\ntrajectory\\nadaptation models\\nnavigation\\nplanning\\nneural networks\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nmotion estimation\\nneural nets\\npedestrians\\nhuman motion behavior\\nprediction accuracy\\ndata-driven model\\ninteraction-aware pedestrian motion prediction\\nobject cluttered environments\\ninteraction-aware motion prediction approach\\nhuman navigation behavior\\nlong-short term memory neural networks\\nstatic obstacles\\ntrajectory forecasting\\npolar angle space\",\"607\":\"three-dimensional displays\\nrobot sensing systems\\nfeature extraction\\nsolid modeling\\ntraining data\\nspatiotemporal phenomena\\nhidden markov models\\nfeedforward neural nets\\ngesture recognition\\nimage classification\\nimage motion analysis\\nlearning (artificial intelligence)\\nmotion recognition\\nspatiotemporal features\\ndense occupancy grids\\n3d point cloud data\\nend-to-end spatiotemporal gesture learning approach\\ndynamic gestures\\nspatiotemporal learning\\npoint cloud data augmentation\\n3d convolutional neural network\\ngestures sample data\",\"608\":\"task analysis\\nrobots\\nmerging\\nsun\\nknowledge representation\\nmirrors\\nneurons\\nobject-oriented methods\\nmanipulation sequences\\nknowledge retrieval algorithm\\nobject categories\\nfunctional units\\nobject similarity\\nvideo sources\\nknowledge spanning\\nobject-motion affordances\\nstructured knowledge representation\\nfunctional object-oriented network\",\"609\":\"trajectory\\ncameras\\nrobot kinematics\\nrobot vision systems\\ntwo dimensional displays\\nmobile robots\\ncodecs\\nimage annotation\\nimage coding\\nlearning (artificial intelligence)\\nobject detection\\npedestrians\\npose estimation\\nrobot vision\\nservice robots\\nslam (robots)\\nlong-term temporal information\\nsequence-to-sequence lstm encoder-decoder\\non-the-fly prediction\\nglobal coordinate system\\nt-pose-lstm model\\nhuman trajectory prediction\\nlong-term mobile robot deployments\\n3dof pedestrian trajectory prediction learned\\nlong-term autonomous mobile robot deployment data\\nautonomous mobile service robots\\nmonocular camera images\\nrange-finder sensors\\n3dof pedestrian trajectory prediction approach\\ntemporal 3dof-pose long-short-term memory\\nrobust human detection\",\"610\":\"probabilistic logic\\nrobots\\nmeasurement errors\\ncomputational complexity\\nprobability distribution\\nintegrated circuits\\nimage matching\\nslam (robots)\\nstatistical distributions\\ntree searching\\nchi-square test\\ngating threshold\\njoint compatibility branch and bound\\nconditional compatibility branch and bound\\nfeature cloud matching\\nincremental posterior joint compatibility\\nipjc\\nfastjcbb\\nglobal optimal data association\\njoint compatibility test\\njc test based search algorithm\\ncc test\\nconditional probability distribution\\nfeature pairing\\nconditional compatibility test\\nscan matching\\ndata association\",\"611\":\"visualization\\nliquid crystal displays\\nrobots\\ndatabases\\npipelines\\nfeature extraction\\nvocabulary\\nimage matching\\nimage recognition\\nimage representation\\nimage segmentation\\nmobile robots\\nprobability\\nrobot vision\\nslam (robots)\\nsimultaneous localization and mapping\\nimage stream\\nimage match\\ndynamic segmentation\\nnearest neighbor voting scheme\\nimage descriptors\\nquery time\\non-line clustering algorithm\\nvisual vocabulary construction\\nrobotic applications\\nlcd\\nplace recognition\\nloop closure detection\\nvisual words\",\"612\":\"roads\\nrobustness\\nthree-dimensional displays\\nstereo vision\\nsplines (mathematics)\\ncameras\\nimage edge detection\\ncomputer vision\\ngraph theory\\nimage segmentation\\nobject detection\\nstereo image processing\\ntraffic engineering computing\\nroad detection\\nnonparametric method\\nimproved v-disparity map\\nvanishing point\\nroad region\\nhorizon information\\nsource node\\nweighted graph\\nleft stereo-image\\nadjacency relationships\\nadjacent pixels\\ndisparity information\\nroad borders\\ndijkstra algorithm\\ndijkstra model\\ngray-scale information\\nimage pairs\\nroad scenes\\nweighted-sampling ransac-like method\\nkitti dataset\",\"613\":\"sensors\\nthree-dimensional displays\\noptimization\\natmospheric measurements\\nparticle measurements\\nfrequency measurement\\nlaser radar\\ngraph theory\\nintelligent transportation systems\\noptimisation\\npose estimation\\nsensor fusion\\nstereo image processing\\n3d localization\\nfactor graph-based optimization\\nautonomous driving\\n3d pose measurement\\nasynchronous multisensor fusion\\nasynchronous-measurement alignment\\ngraph nodes\\nout-of-sequence measurement alignment\\nmultiple navigation sensors\\nmodular sensor-fusion system\\nautonomous vehicles\\n3d mapping\\nasynchronous sensors\\nmultiple heterogeneous sensors\",\"614\":\"hidden markov models\\ncomputational modeling\\ntopology\\nroads\\nuncertainty\\nmeasurement\\nvehicle dynamics\\nmobile robots\\nposition control\\nremotely operated vehicles\\nroad traffic control\\nvsm-hmm\\ntopological uncertainty\\nlane membership\\ntopological localization process\\ntopological structure estimation\\nav lane estimation\\nlane identification\\nautonomous vehicles\\ntopological location\\ndecision-making\\npublic roads\\nvariable structure multiple hidden markov model\\nmetric location\\nearth mover distance\",\"615\":\"autonomous vehicles\\nvehicle dynamics\\nstability criteria\\noptimization\\nmathematical model\\nfrequency-domain analysis\\nintelligent transportation systems\\nlinear systems\\nmobile robots\\nnonlinear programming\\noptimal control\\nroad safety\\nroad traffic control\\nroad vehicles\\nstability\\nautonomously controlled vehicles\\nhuman-driven vehicles\\ntraffic stabilization\\nsafer roads\\nenergy savings\\nsingle-lane system stabilization\\nlinear string stability\\noptimality conditions\\nnonlinear optimization problem\\nsafety constraint\\noptimal linear controller\\ntraffic conditions\\nhuman driver behavior\",\"616\":\"prediction algorithms\\npredictive control\\ntransportation\\npricing\\ncontrol systems\\nsteady-state\\nreal-time systems\\ndemand forecasting\\nintelligent transportation systems\\nrecurrent neural nets\\nroad traffic control\\nend-to-end performance\\ncustomer demand\\ndata-driven model predictive control\\nlstm neural network\\ntravel demand\\nautonomous mobility-on-demand systems control\\ndidi chuxing\\nmpc algorithm\\ntransportation system\\noptimal rebalancing strategy\\namod system\",\"617\":\"three-dimensional displays\\ntwo dimensional displays\\nurban areas\\ncameras\\nrobustness\\nnoise measurement\\nsemantics\\ndistributed processing\\nimage reconstruction\\nstereo image processing\\ntraffic engineering computing\\nvalue\\nautomatic localisation\\nstatic 3d objects\\nnew york city\\nurban environments\\nvoting-based automatic labelling\\ndistributed voting schema\\ntraffic lights\",\"618\":\"semantics\\ncameras\\nroads\\nimage segmentation\\nsensors\\nproposals\\ntrajectory\\nmobile robots\\nobject detection\\npath planning\\nroad traffic\\nroad vehicles\\nrobot vision\\npath prediction model\\ncamera sensors\\nautonomous vehicle systems\\nvision systems\\nreal-time semantic segmentation\\nintelligent vehicles\\nodometry\\nmonocular camera\\ncar-width drivable lane\\npath proposal category\\nintelligent vehicle system\\ndrivable path information\\nhuman operation\\nclear lane markings\\nurban roads\",\"619\":\"sensors\\nfeature extraction\\nradar imaging\\nazimuth\\nmotion estimation\\nrobustness\\ncw radar\\ndistance measurement\\nfm radar\\nglobal positioning system\\nimage matching\\nmillimetre wave radar\\nprecise ego-motion estimation\\nmillimeter-wave radar\\ncameras\\nlidars\\nproprioceptive sensors\\nradars\\nlong-range objects\\nmobile autonomous systems\\nfrequency-modulated continuous-wave scanning radar\\ngps\\/ins\\nradar odometry\",\"620\":\"semantics\\nrobot sensing systems\\nthree-dimensional displays\\ntwo dimensional displays\\nrobustness\\nfeature extraction\\nimage colour analysis\\nimage matching\\nimage representation\\nmobile robots\\npose estimation\\nrobot vision\\nvision-based approaches\\nhigh level semantic cues\\nfloorplan\\nglobal localisation approach\\nrange measurements\\nrobotic scan-matching algorithms\\nsedar\\nsemantic detection and ranging\\n2d geometry\\n2d representation\\ndiscriminative landmarks\\nrgb images\",\"621\":\"legged locomotion\\nactuators\\nknee\\npulleys\\ntorque\\nenergy storage\\ncontrol system synthesis\\nelasticity\\nmotion control\\nsimilar mass\\nmass distribution\\nbiarticulated actuation configuration\\nmechanical design\\nactuation configuration principles\\n3-dof leg\\nparallel compliant actuation\\nseries-elastic main actuators\\nleg design\\nenergy efficient articulated robots\",\"622\":\"legged locomotion\\nkinematics\\ncouplings\\nprototypes\\nhip\\nhardware\\nend effectors\\ngait analysis\\nhumanoid robots\\nmanipulator kinematics\\nmedical robotics\\nmotion control\\nposition control\\ntrajectory tracking\\ngait trajectory\\nbar-linkage mechanism\\nserial mechanism\\ntwin 3 dof serial chains\\nparallel mechanisms\\nserial mechanisms\\n6 dof leg mechanism\\nhumanoid robot\\nserial-parallel hybrid leg\\ninverse kinematics\\nforward kinematics\\nconventional serial leg\\ncommercial robot\\nkinematic specification\\nhardware prototype\\nknee pitch rotation\",\"623\":\"grippers\\npins\\nrobot kinematics\\nlegged locomotion\\ncouplings\\nsubstrates\\nactuators\\nadhesion\\naerospace robotics\\nbone\\ngait analysis\\nimpact (mechanical)\\ndynamic penetration\\nsteep jumps\\nhigh impact forces\\nlow duty cycles\\nmonopedal jumping robots\\nslipping foot\\ndynamic jumping robot\\nsurface approach speed\\ncycle time\\npenetrable substrate\\ngripper mechanism\\nrobot salto\\nangled spines\\npenetrable inclines\\nholding angles\\nleg crouch-extension\\nstatic adhesion\\nceiling\\nself engaging spined gripper\\nkinematics\",\"624\":\"batteries\\nmagnetic resonance\\nintegrated circuit modeling\\ngeometry\\nanalytical models\\ncouplings\\nwireless power transfer\\nbattery powered vehicles\\ncoils\\ninductive power transmission\\nwpt system\\ntransmit coil\\nwpt circuit design\\npower-transfer model\\ntwo-coil system\\nwpt circuitry\\nwirelessly powered mav\\ninductive resonant coupling\\nmicroaerial vehicle\\npower transfer system\",\"625\":\"aerodynamics\\ntorque\\nrobots\\nfasteners\\npredictive models\\ndrag\\nforce\\naerospace components\\napproximation theory\\nbending strength\\nhinges\\nnavier-stokes equations\\nrobot kinematics\\nmodel predictions\\nflapping-wing robots\\nflexural passive wing hinges\\naerodynamic forces\\nbalanced torque\\nquasisteady aeromechanic model\\nrotating hinge\\nmechanical complexity\\nstroke-averaged forces\",\"626\":\"planning\\ncomputational modeling\\ndensity measurement\\ngeometry\\nsurface treatment\\nthree-dimensional displays\\ncomputational geometry\\nfeature extraction\\nimage reconstruction\\nmesh generation\\nsolid modelling\\nsurface edge explorer\\nbest view planning\\nnbv planning approaches\\nvoxel grids\\ntriangulated meshes\\nsurface geometry\\nhigh-resolution models\\nsurface representations\\nmultiple survey stages\\nscene-model-free nbv planning approach\\ndensity representation\\ncurrent measurements\\nobserved surface boundaries\\nsurface coverage\\nevaluated state-of-the-art volumetric approaches\\nnext best views\\ntime 3.0 d\",\"627\":\"three-dimensional displays\\nimage reconstruction\\nsolid modeling\\nunmanned aerial vehicles\\nplanning\\npipelines\\ncameras\\nautonomous aerial vehicles\\ndata acquisition\\nimage sensors\\nsolid modelling\\nstereo image processing\\niterative linear method\\nmultiview stereo problem\\nonline model reconstruction\\ntoy unmanned aerial vehicle\\ntoy drone\\nimage-based modeling techniques\\nphoto-realistic 3d models\\nmulti-view stereo algorithm\\nactive image-based modeling\",\"628\":\"feature extraction\\nobject detection\\nrobots\\ntask analysis\\nmachine learning\\nproposals\\nimage recognition\\nimage classification\\nimage fusion\\nlearning (artificial intelligence)\\nobject context\\ndeep learning\\nobject detection dataset\\ncurrent object detection framework\\nfunctional area detection\\nfunctionality-related feature\\nobject-related\\npotential image regions\\ndeep-model-based classifier\\nfunctional area image dataset\\narea detection problem\\ncognitive robot\",\"629\":\"pose estimation\\nmanifolds\\ntask analysis\\ntraining\\nrobustness\\nobject recognition\\nthree-dimensional displays\\nconvolution\\nfeedforward neural nets\\nimage matching\\nnearest neighbour methods\\nregression analysis\\nmanifold learning\\npose regression\\nnn descriptor matching\\nmanifold descriptor learning\\nmultitask learning framework\\nnearest neighbor search\\nconvolutional neural networks\",\"630\":\"cameras\\nthree-dimensional displays\\nestimation\\nsparse matrices\\nshape\\nshape measurement\\nsensors\\nimage reconstruction\\nimage resolution\\nlight reflection\\noptical projectors\\nmultiple light paths\\ncomplex light reflection objects\\nlight transport matrix estimation\\nlt matrix estimation\\nhigh resolution measurement\\nsparse matrix representation\\nultra-fast multiscale shape estimation\\ntarget objects\\nspecular reflection\\nlight path\\nmemory efficiency\\n256 \\u00d7 256 resolution projector\\ncamera system\\n3d measurement methods\",\"631\":\"robots\\npipelines\\nimage segmentation\\ngenerators\\nthree-dimensional displays\\ngallium nitride\\ncameras\\ncrops\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nstalk segmentation\\ngrasp point generation pipeline\\nhigh stalk density\\nlighting variation\\ncustom-built ground robot\\nend-to-end system\\naverage grasping accuracy\\ngenerative adversarial network\\nin-situ sorghum stalk detection\\nonline pipeline\\ndeep learning-based high throughput\\nlabor-intensive phenotyping processes\\nrobotic solutions\\nplant attributes\\nprecise measurements\\nfast measurements\\ndeep learning-based stalk grasping pipeline\\npixel-wise stalk detection\\ngrasp point detection\\nstalk detection f1 score\",\"632\":\"task analysis\\nplanning\\nrobot sensing systems\\nuncertainty\\nsemantics\\ncameras\\nprobabilistic logic\\nassembling\\ngraph theory\\nplanning (artificial intelligence)\\nproduction engineering computing\\nsensor fusion\\nsensors\\nperception systems\\nfactor graph\\nautomated systems\\ndata processing algorithms\\nsensor noise\\ncalibration errors\\nplanning problem\\nprobabilistic graphical models\\nconfiguration space\\nperceptions systems\\nperception steps\\nsensor data fusion\\nindustrial assembly\",\"633\":\"sonar\\nproposals\\nclustering algorithms\\npipelines\\nfeature extraction\\nsoftware\\narchaeology\\nautonomous underwater vehicles\\ncontrol engineering computing\\nimage processing\\nintelligent control\\nmarine control\\nmobile robots\\npath planning\\narchaeological survey\\nintelligent shipwreck search\\nautonomous robot system\\nmultistep pipeline\\nhigh altitude scan\\nlow-resolution side scan sonar data\\nimage processing software\\nauv path planner\\narchaeological sites\\nunderwater archaeological sites\\nranking algorithm\",\"634\":\"oceans\\nrobots\\nunderwater vehicles\\nmathematical model\\nvehicle dynamics\\nenergy consumption\\ncomputational modeling\\nautonomous underwater vehicles\\ncollision avoidance\\nmobile robots\\nnonlinear control systems\\npredictive control\\nrobust control\\nrobust model predictive control approach\\nconstrained workspace\\nunderwater robotic vehicles\\nstatic obstacles\\nworkspace boundary\\nthruster saturation\\nvehicle velocity\\ncontrol design\\nocean currents\\ncontrol inputs\\nway-point tracking mission\\ncontrol strategy\\nconstrained test tank\\nnonlinear model predictive control scheme\\nway points\\nunderwater robotic vehicle\",\"635\":\"boats\\nvehicle dynamics\\nsymmetric matrices\\nglobal positioning system\\nheuristic algorithms\\nrobot kinematics\\nautonomous underwater vehicles\\nhydrodynamics\\nindoor environment\\nmatrix algebra\\nmobile robots\\nmotion control\\nnonlinear control systems\\npredictive control\\ntracking\\ntrajectory control\\nnonlinear model predictive tracking control\\nautonomous surface vehicle\\nautonomous robotic boat\\nindoor environments\\noutdoor environments\\ncross type four-thruster configuration\\nrobot prototype\\nnonlinear dynamic model\\nnmpc algorithm\\nsurface swarm robotics testbeds\\ntrajectory tracking\\nholonomic motions\\nfiberglass\\ncentripetal matrix\\ncoriolis\\nhydrodynamic\\ndamping\\ngps modules\\ninertial measurement unit\\nimu\\nswimming pool\\nnatural river\\ncode generation strategy\",\"636\":\"computational modeling\\nlearning (artificial intelligence)\\ntask analysis\\nrobot kinematics\\nheuristic algorithms\\nforce\\nembedded systems\\nmicrorobots\\nmulti-agent systems\\nrobot programming\\nunderwater vehicles\\nmodel-based value-function rl algorithm\\nmicro underwater agents\\nunderwater robotics\\nunderwater depth stabilization\\nlight embedded systems\\ncontrol tasks\\nmicrodiving agent\\nreinforcement learning\",\"637\":\"image reconstruction\\nfeature extraction\\nreal-time systems\\nthree-dimensional displays\\nimaging\\nsonar measurements\\ncameras\\nfeedforward neural nets\\nimage classification\\nimage resolution\\nimage sensors\\nsonar imaging\\nunderwater vehicles\\nunderwater environments\\nsonar images\\nlow-resolution imagery\\nstandard cameras\\nautomatic feature extractors\\nsonar imagery\\nenvironment reconstructions\\nhigh data capture rates\\nstandard imaging sonars\\nhigh-quality frames\\nfeature annotation\\nreal-time reconstruction capability\\nunderwater vehicle\\ntime underwater 3d reconstruction\\nreal-time 3d reconstruction\\nconvolutional neural network\",\"638\":\"robustness\\ntask analysis\\nvisualization\\nrobot sensing systems\\nunmanned underwater vehicles\\ngesture recognition\\nfeedforward neural nets\\nfinite state machines\\nhuman-robot interaction\\nmobile robots\\nhand gestures\\nhand gesture recognition\\ngesture-to-instruction mapping\\nfinite-state machine\\nconvolutional neural network\\nhuman-robot collaborative tasks\\nautonomous underwater robots\\nparameter reconfiguration method\\nreal-time programming\\nunderwater human-robot collaboration\\nmission parameters\\ndynamic reconfiguration\",\"639\":\"robot sensing systems\\nadaptation models\\noptimization\\npredictive models\\ntrajectory\\nuncertainty\\nbathymetry\\nentropy\\ngaussian processes\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\npath planning\\nsampling methods\\nsingle roi\\ndeepest region\\ncoastal bathymetry mapping mission validate\\nefficient sampling strategy\\nlatest sensory measurements\\nsampling density\\nce trajectory optimization\\nhigher spatial variability\\nexhibit extreme sensory measurements\\nexploring learning\\ninitial stage\\ngp-ucb\\ngp upper confidence\\nreceding-horizon cross-entropy trajectory optimization\\nenvironmental sensing\\ncross-entropy method\\ngaussian process adaptive sampling\",\"640\":\"robot kinematics\\nneural networks\\ntask analysis\\noptimization\\nlearning (artificial intelligence)\\ntraining\\ncontrol engineering computing\\ndecision making\\nmanipulators\\nneural nets\\noptimisation\\ndecision-making problems\\nreinforcement learning architecture\\noptlayer\\nneural network\\nclosest actions\\nsafe actions\\nrobot reaching tasks\\npractical constrained optimization\\ndeep reinforcement learning techniques\",\"641\":\"entropy\\nrobots\\nlearning (artificial intelligence)\\nneural networks\\nmachine learning\\ntask analysis\\ntraining\\ncontrol engineering computing\\nmanipulators\\ncomposable deep reinforcement\\nmodel-free deep reinforcement learning\\nsimulated robotic manipulation\\nmodel-free methods\\nreal-world robotic tasks\\nmaximum entropy policies\\nsoft q-learning\\nreal-world robotic manipulation\",\"642\":\"collision avoidance\\nrobot sensing systems\\nrobot kinematics\\nnavigation\\nrobustness\\ntraining\\ndecentralised control\\ngradient methods\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\nmultiscenario multistage training framework\\noptimal policy\\npolicy gradient\\nreinforcement learning algorithm\\nlearned sensor-level collision avoidance policy\\nfinal learned policy\\ncollision-free paths\\nlarge-scale robot system\\ndeep reinforcement learning\\nsafe collision avoidance policy\\nefficient collision avoidance policy\\noptimally decentralized multirobot collision avoidance\\nagent-level feature extraction\\ndecentralized methods\\nmaps raw sensor measurements\\nmultirobot systems\\ndecentralized sensor-level collision avoidance policy\\nlocal collision-free action\\ndistributed multirobot collision avoidance systems\",\"643\":\"robot sensing systems\\nneural networks\\naerospace electronics\\nhardware\\nnasa\\ntraining\\nlearning systems\\nmobile robots\\nmotion control\\nneurocontrollers\\nnonlinear dynamical systems\\nsearch problems\\nstate-space methods\\nnonlinear dynamics\\nhigh-dimensional state space\\nrobotic systems\\nspace exploration\\ntensegrity robot locomotion\\ndeep reinforcement learning algorithms\\npolicy learning process\\nlocomotion control policies\\nneural network policies\\nmirror descent guided policy search\\nend-to-end locomotion policies\\ntensegrity robotics\",\"644\":\"mobile robots\\nlearning (artificial intelligence)\\nmachine learning\\nneural networks\\nsensors\\ntraining\\ncontrol engineering computing\\ngame theory\\nneural nets\\npath planning\\npattern classification\\ngaming reinforcement learning-based motion planner\\nmobile robotic platform\\ndeep classifier\\nasynchronous deep classification network\\nvisual recognition\\nmotion planning\\ndeep learning-based algorithms\\ntt2-bot\\nembedded neural networks\",\"645\":\"training\\nnavigation\\nacceleration\\nrobot kinematics\\nmachine learning\\ntask analysis\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nthree-term control\\ndrl network\\nstandard deep deterministic policy gradient network\\ntraining wheels\\ndeep reinforcement learning\\nrobotic applications\\nrobot applications\\nassisted reinforcement learning\\npid controller\\nlocal planning\\nnavigation problems\\nsimple control law\",\"646\":\"grasping\\nrobots\\ntask analysis\\nbenchmark testing\\nmonte carlo methods\\nmachine learning\\ntraining\\nlearning (artificial intelligence)\\nneural nets\\nrobot vision\\ndeep neural network models\\noff-policy correction\\nvision-based robotic grasping\\noff-policy methods\\ndeep reinforcement learning algorithms\\noff-policy learning\",\"647\":\"task analysis\\nrobots\\nlearning (artificial intelligence)\\nstacking\\ntraining\\nmathematical model\\ngames\\ncontrol engineering computing\\nmanipulators\\nreinforcement learning\\nreward function\\ntask horizon\\nrl methods\\nexploration problem\\nmultistep robotics tasks\\nrobot arm\\ndeep deterministic policy gradients\\nhindsight experience replay\\nsimulated robotics tasks\",\"648\":\"three-dimensional displays\\nsolid modeling\\ncomputational modeling\\ncameras\\nrobots\\ntwo dimensional displays\\nuncertainty\\nimage reconstruction\\nimage sequences\\nobject detection\\nsolid modelling\\nstereo image processing\\nfast image-based geometric change detection\\nmultiple images\\nself-recorded image sequences\\nrobotic applications\\n3d model\\n3d location\",\"649\":\"three-dimensional displays\\ncameras\\nsolid modeling\\nvisualization\\nfeature extraction\\ntwo dimensional displays\\nsensors\\ngeometry\\nimage registration\\nsolid modelling\\nmultimodal 2d image\\n3d model registration\\ndense visual features\\n2d\\/3d registration methods\\ngeometric features\\nfeature type\\nhybrid registration framework\\nvisual sensors\\n3d model\\nmutual alignment\\ngeometric visual features\\nsparse visual features\\n2d\\/3d alignment\",\"650\":\"three-dimensional displays\\nreal-time systems\\ndata integration\\nmesh generation\\nrendering (computer graphics)\\ndata structures\\nsensors\\ncomputational geometry\\nimage reconstruction\\nimage representation\\nsensor fusion\\nsolid modelling\\nreal-time scene reconstruction\\nspatial hashing\\nflexible data structures\\n3d data fusion\\nmesh refinement\\nmesh quality\\nmesh memory consumption\\nvolumetric mesh representation\\nhamming distance\\n3d reconstruction\",\"651\":\"three-dimensional displays\\ncameras\\nneural networks\\nmeasurement by laser beam\\nlasers\\nlaser beams\\nimage segmentation\\ncontrol engineering computing\\nlaser ranging\\nmobile robots\\nnavigation\\nneural nets\\nprobability\\nrobot vision\\nslam (robots)\\nkitti dataset\\nmapping process\\nmapping module\\ndynamic object\\npointwise probability\\nneural network\\nlaser range data\\n3d grid map\\nnavigation functions\\nrobot perceptions\\ndynamic environments\\nsafe navigation\\nrobust navigation\\nautonomous robotic systems\\nsingle 3d range scans\\ndynamic-object probabilities\\ntime 3.0 d\",\"652\":\"interpolation\\nmemory management\\nthree-dimensional displays\\npose estimation\\nextrapolation\\ntwo dimensional displays\\nimage resolution\\ncameras\\ncomputational geometry\\ndistance measurement\\nimage reconstruction\\nmedical image processing\\nrobot vision\\nslam (robots)\\ncamera trajectories\\ntrilinear interpolation method\\ndepth-camera pose tracking\\nperformance degradation\\ntruncated signed distance field\\nvoxel-based map representations\\ngeometric interpolation methods\\nintermediate options\\nnearest neighbors\\nvoxel volumes\\nvolumetric map-based visual odometry\\nvoxel interpolation methods\",\"653\":\"laser radar\\nthree-dimensional displays\\nglobal positioning system\\ntwo dimensional displays\\nsensor systems\\nurban areas\\ngraph theory\\nmobile robots\\noptical radar\\npose estimation\\nradar computing\\nslam (robots)\\ncomplex urban environments\\nlight detection and ranging data set\\nfiber optic gyro\\ninertial measurement unit\\nvehicle pose estimation\\ngraph simultaneous location and mapping algorithm\\ngraph slam algorithm\\nrobot operating system environment\\nraw sensor data\\n2d lidar\\n16-ray 3d lidars\\nlidar sensors\\nthree-dimensional lidar\\nbuilding complexes\\nhigh-rise buildings\\ncomplex urban lidar data set\\nfrequency 100.0 hz\",\"654\":\"shape\\nthree-dimensional displays\\nsimultaneous localization and mapping\\ncameras\\ncomputational modeling\\nhistory\\nestimation\\nimage colour analysis\\nimage fusion\\nimage texture\\nrobot vision\\nslam (robots)\\nsolid modelling\\nlive structural modeling\\ndense point cloud\\nshape map\\nsingle point cloud\\nmetric primitive modeling\\nrgb-d slam\\nprimitive shape localization\\nshape fusion\",\"655\":\"robot sensing systems\\nadaptation models\\ndensity functional theory\\ntemperature distribution\\ndata models\\ngaussian processes\\nlearning (artificial intelligence)\\nmulti-robot systems\\noptimisation\\nonline learning\\nmultirobot sensor coverage\\nonline environmental sampling\\nmultirobot coverage control\\nenvironmental phenomenon\\nrobot team\\ngaussian process\\nlocally learned gaussian processes\\ncollective model learning\\nsimultaneous adaptive sampling\\ndensity function\\nsensing performance optimization\",\"656\":\"drones\\nmathematical model\\natmospheric modeling\\noscillators\\nacceleration\\ntask analysis\\nroads\\nair traffic control\\nautonomous aerial vehicles\\ncollision avoidance\\ndecentralised control\\nmulti-robot systems\\ncoordinated dense aerial traffic\\ngeneral air traffic control solution\\ndecentralized air traffic control solution\\npackage-delivery scenarios\\nintelligent collective collision avoidance\\nmotion planning\\njam-free optimal traffic flow\\nforce-based distributed multirobot control model\\nbehaviour-driven velocity alignment\\nself-organized queueing\\nconflict-avoiding self-driving\",\"657\":\"switches\\nplanning\\ntask analysis\\nrobot kinematics\\nprobabilistic logic\\ndecision making\\nmulti-agent systems\\nmulti-robot systems\\nplanning (artificial intelligence)\\nmultirobot\\nsmall-scale synchronous decision-making scenarios\\nasynchronous agents\\nmultiple strategic adversaries\\nadversary strategies\\noptimized stratagems\\nunified policy\\nnear-optimality\\noptimal adversarial policy switching\\ndecentralized asynchronous multiagent systems\\ncommunication capabilities\",\"658\":\"emulation\\nconvergence\\naustralia\\nradio frequency\\ndata models\\nrobot kinematics\\nmobile robots\\nmulti-robot systems\\nharsh communication environments\\nuas\\nhuman-swarm interaction\\ndata dissemination capabilities\\nindoor flight facilities\\nphysical swarm robotic platforms\\nradio-frequency communications\\nswarm members\\ntactical defence networks\",\"659\":\"drones\\nrobot kinematics\\nrecurrent neural networks\\nhardware\\nrobot sensing systems\\ndistance measurement\\nlearning (artificial intelligence)\\nparticle filtering (numerical methods)\\nrecurrent neural nets\\nremotely operated vehicles\\ndeep q-learning network\\nrecurrent network\\nuwb-distance information\\nneural networks\\ndistance-based multirobot coordination\\npocket drones\\nmicroaerial vehicles\\nrecurrent neural network\",\"660\":\"task analysis\\nsafety\\nrobot kinematics\\nautomata\\nplanning\\nnavigation\\ncontrol engineering computing\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npath planning\\nplanning (artificial intelligence)\\ntemporal logic\\nplan adaptation scheme\\nshort-term tasks\\niterative inverse reinforcement learning algorithm\\nhuman preference\\nplan synthesis\\nhuman-in-the-loop simulations\\nmixed-initiative control\\ntask planning problem\\nhigh-level tasks\\nlinear temporal logic\\nhard constraints\\nsoft constraints\\nrobot autonomy\\nadditive terms\\ncontingent task assignments\\nonline coordination scheme\\nmixed-initiative continuous controller\\ntemporal tasks\\nhuman initiatives\",\"661\":\"adaptive control\\nmanipulators\\nconvergence\\nrobot sensing systems\\ncloud computing\\ntrajectory\\ncontrol engineering computing\\ncontrol system synthesis\\ndecentralised control\\nlyapunov methods\\nmulti-robot systems\\nrobot manipulators\\nsynchronous centralized update laws\\nparameter convergence\\ntime-varying network topologies\\nnonidealized networked conditions\\nplanar manipulator\\ncloud-based robotics\\ninertial parameters\\ncollective sufficient richness notion\\ndecentralized update laws\",\"662\":\"cameras\\nsonar\\nplanning\\nrobot sensing systems\\ninspection\\nthree-dimensional displays\\nautonomous underwater vehicles\\nimage reconstruction\\nmobile robots\\npath planning\\nquadtrees\\ntree data structures\\nviewpoint generation process\\nconsistent maps\\nnoisy sonar data\\noptimized environment exploration\\nautonomous underwater\\nautonomous robotic environment exploration\\nunderwater domain\\nnoisy acoustic sensors\\nhigh localization error\\ncontrol disturbances\\nrobotic exploration algorithm\\nunderwater vehicles\\nview planning\\npath planning algorithms\\nexploration approach\\nquadtree data structure\\nrelevant queries\\nnatural environments\\nunderwater maps\\nmap representation\\noptical coverage\\ntime 3.0 d\",\"663\":\"adaptation models\\ndata models\\nkernel\\nrobots\\ngaussian processes\\nestimation\\noptimization\\naerospace control\\nenvironmental factors\\nmobile robots\\npath planning\\nregression analysis\\nsampling methods\\nsampling trajectory\\ngaussian process regression\\npilot surveys\\nadaptive informative sampling\\ngp hyperparameter estimation\\npath planning decisions\\nenvironmental field modeling\\ninformative samples\\nadaptive sampling techniques\\ngp regression\",\"664\":\"robots\\nbayes methods\\ninstruments\\nlinear programming\\noptimization methods\\nmaster-slave\\nadaptive control\\nhuman-robot interaction\\nmedical robotics\\noptimisation\\nsurgery\\ntelerobotics\\ngaze-assisted adaptive motion scaling optimization\\nbayesian approaches\\nmaster-slave surgical systems\\nslave robot\\ngaze-assisted intention recognition scheme\\nbayesian approach\\nhuman-robot interface\\nbayesian optimization methods\",\"665\":\"optimization\\nrobot kinematics\\nbayes methods\\nsearch problems\\nkernel\\nlinear programming\\nautomobiles\\nhilbert spaces\\nmobile robots\\noptimisation\\nrobot dynamics\\nvehicle dynamics\\ndynamical model\\nrobot\\ncar racing simulation\\ndescent bayesian optimisation\\nrace track\\nkernel hilbert space\\nbayesian optimisation\",\"666\":\"tools\\ntask analysis\\nkinematics\\nautomobiles\\nrobot sensing systems\\ndynamics\\ncognition\\nobject recognition\\nrobot vision\\nautomatic tool recognition\\nhuman dexterity\\nskill transfer\\nrobots cognition\\nrobots capabilities\\ntools embodiment\",\"667\":\"task analysis\\nrobots\\nfeature extraction\\nswitches\\nneurons\\ntraining\\nconvolution\\nhumanoid robots\\nlearning (artificial intelligence)\\nmanipulators\\nrecurrent neural nets\\nmultiple discrete tasks\\ndeep learning\\ndeep neural networks\\nrobot manipulation model\\ndnns\\nlong sequential dynamic tasks\\nmultiple short sequential tasks\\nmultiple timescale recurrent neural network\\nmtrnn\\ninitial motion steps\\nfinal motion steps\\ninitial image input\\nsubtask\\nput-in-box task\",\"668\":\"aerospace electronics\\ngrasping\\ntraining\\ntask analysis\\nrobots\\nsensitivity analysis\\ngrippers\\nlearning (artificial intelligence)\\nadaptive-underactuated multifingered gripper\\ncurriculum accelerated self-supervised learning\\nvariance-based global sensitivity analysis\\ncontrol parameters\\ncontrol dimensions\\ntraining data\\ncassl orders\\nhigher-dimensional action\\nmap visual information\\nclever sampling strategy\\ndata collection efforts\\nhigher-dimensional control\\nlow-dimensional action\\nself-supervised learning approaches\\ncomplete end-to-end learning\\nstaged curriculum learning\\ncassl framework\",\"669\":\"muscles\\njoints\\nrobots\\nbiological system modeling\\ntorque\\nbiomechanics\\nbiomimetics\\nbone\\nhumanoid robots\\nlearning (artificial intelligence)\\nmuscle\\nneural nets\\nnonlinear control systems\\nphysiological models\\nquadratic programming\\nmachine learning approaches\\nmuscle stimulations\\nhigh actuator redundancy\\nlearned forward model\\nneural network\\nbiomimetic muscle-driven robot show\\nnonlinearity\\nbiomechanical musculoskeletal systems\\nsqp\",\"670\":\"robot kinematics\\ngames\\nmonte carlo methods\\ntask analysis\\nplanning\\nstochastic processes\\niterative methods\\nlearning (artificial intelligence)\\nlearning systems\\nmobile robots\\nmulti-robot systems\\nnavigation\\npath planning\\nstate-space methods\\nstochastic games\\ntree searching\\nuncertain systems\\nmultirobot systems\\nmanifold applications\\nunstructured scenarios\\nstate dimensionality\\nmodel-based reinforcement learning algorithm\\nq-learning\\ncurse-of-dimensionality\\ncooperation scenario\\nrobot behaviors\\nuncertainties\\nstate space exploration\\naction values learning\\nstochastic cooperative games\\ncooperative navigation problem\\ncooperative planning\\nmonte-carlo tree search iterations\\ngeneral-sum games\\nkuka youbots\\nrobot hand-overs\\ncoordination task\",\"671\":\"semantics\\ncameras\\nroads\\nimage segmentation\\nvisualization\\nrobustness\\nfeature extraction\\nparticle filtering (numerical methods)\\ntransforms\\nparticle filter based semantic localization solution\\nsift-features\\nvehicle localization\\nsemantically labeled 3d point maps\\nautonomous vehicles\\nlong-term visual navigation\\nrobust cross-seasonal localization\\nsemantically segmented images\\nlong-term visual localization\\nimage segmenter\\nhand-crafted feature descriptors\",\"672\":\"reliability\\nlaser radar\\nrobot sensing systems\\noptical sensors\\nglass\\nadaptive optics\\noptical variables measurement\\nmobile robots\\noptical radar\\npose estimation\\nrobot vision\\nmobile robot\\nlidar measurements\\nlocalization errors\\nlidar sensor-based localization\\nrobust localization\\nrange measurements\\nlight detection and ranging sensor\",\"673\":\"trajectory\\nsimultaneous localization and mapping\\nestimation\\nplanning\\nsparse matrices\\ngaussian processes\\ncontinuous time systems\\nlie groups\\nmatrix algebra\\nmobile robots\\nmotion control\\noptimisation\\npath planning\\nregression analysis\\nstate estimation\\ntrajectory control\\nnonparametric representation\\ntrajectory distributions\\nsparse gp regression\\nrobot state\\nlocally linear gps\\nmotion planning tasks\\nsparse gaussian processes\\ncontinuous-time trajectories\\ntrajectory optimization\\nmatrix lie groups\\nrobot motion reasoning\",\"674\":\"simultaneous localization and mapping\\ncomplexity theory\\noptimization\\nsparse matrices\\nextraterrestrial measurements\\nlinear algebra\\ncomputational complexity\\ngraph theory\\niterative methods\\nnewton method\\noptimisation\\nslam (robots)\\ncomplexity analysis\\nglobally-efficient structure\\nfavorable global structures\\ngauss-newton iteration\\nfactorization step\\nprimary computational bottleneck\\ngraph structure\\nexisting analytic gap\\nquantitative metric called elimination complexity\\nsignificant computation reductions\\nmeasurement decimation\\nsimple heuristics\\naggressive pruning\\nsignificant computational savings\\nstructurally-na\\u00efve techniques\\nglobal level\\nedge count\\nslam graph\\ngraph-based slam\\nhigh-rate graph\\nefficient measurement selection primitives\",\"675\":\"simultaneous localization and mapping\\noptimization\\nthree-dimensional displays\\nreal-time systems\\nestimation\\nvisualization\\ndistance measurement\\nimage reconstruction\\nleast squares approximations\\noptimisation\\nrobot vision\\nslam (robots)\\ndense visual odometry estimation\\nplanar measurements\\nslam framework\\nimu biases\\nplanar landmarks\\nincremental smoothing\\nbayes tree\\nimu data\\nvisual information\\nmodeling planes\\nimu states\\nreconstruction results\\nslam algorithms\\nstructural constraints\\ndpi-slam system\\nplanar-inertial slam system\\nnovel dense planar-inertial slam\\ndense 3d models\\nindoor environments\\nhand-held rgb-d sensor\\ninertial measurement unit\\npreinte-grated imu measurements\\nfactor graph\\nincremental mapping\\nprobabilistic global optimization\",\"676\":\"scintillators\\ncalibration\\nrobot sensing systems\\ndetectors\\nunmanned aerial vehicles\\nradiation detectors\\ngamma-ray detection\\nglobal positioning system\\nmobile robots\\nphotomultipliers\\nradioactive sources\\nradioactivity measurement\\nsolid scintillation detectors\\nradiation measurements\\nradioactive source localization\\nradiation mapping\\nthallium-doped cesium iodide scintillator\\nindoor gps-denied environments\\ncesium-137 radiation source\\ngps-denied localization\\nvisual-inertial localization\\nautonomous nuclear radiation source localization\\naerial robot\",\"677\":\"cameras\\npayloads\\nunmanned aerial vehicles\\ninspection\\nlaser radar\\ntask analysis\\nmachine vision\\naircraft landing guidance\\nautonomous aerial vehicles\\nelectrical maintenance\\nhelicopters\\noptical radar\\npower overhead lines\\nremotely operated vehicles\\nrobot vision\\nsensor fusion\\nlinedrone technology\\nunmanned aerial vehicle landing\\nsemiautomatic landing\\nvehicle onboard vision system\\nmonocular camera\\nlidar\\nnondestructive testing\\nmultirotor unmanned aerial vehicle capable\\npower transmission lines\\nlanding assistance\",\"678\":\"surface acoustic waves\\nblades\\nprototypes\\nrotors\\nmathematical model\\nsolid modeling\\nstability analysis\\naerospace components\\ngyroscopes\\nmechanical stability\\nnumerical analysis\\nposition control\\nwind tunnels\\ndirection controlled descent\\nspinning wing\\ndirection controllability\\ncontrol schemes\\nconical spiral autorotation trajectory\\ngyroscopic stability\\nmaple trees\\ntranslational motion\\nnumerical simulations\\nmultiwing saw prototype\\nwind-tunnel\\nsamara autorotating wings\\nball joint\",\"679\":\"antenna measurements\\ndirectional antennas\\ngain\\nradio frequency\\nextraterrestrial measurements\\nrotation measurement\\nautonomous aerial vehicles\\ndirective antennas\\nhelicopters\\nmobile radio\\nmobile robots\\nomnidirectional antennas\\nradionavigation\\npseudobearing measurements\\nradio frequency sources\\nrf source\\ndirectional antenna\\nomnidirectional antenna\\nantenna theory\\nground tests\\nmultirotor uavs\\nradio sources localization\\nbearing-like measurements\\nunmanned aerial vehicles\",\"680\":\"vehicle dynamics\\nacceleration\\ntrajectory\\nquaternions\\nattitude control\\nvisualization\\nregulators\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nhelicopters\\nlinear quadratic control\\nmobile robots\\nstate estimation\\ntime-varying systems\\ntrajectory control\\nonboard state dependent lqr\\nagile quadrotors\\nquadrotor control\\nmultiple cascaded subproblems\\nrotational dynamics\\ntranslational dynamics\\ncascaded attitude controller\\nattitude dynamics\\nrobustness\\nlqr controller\\nrotational states\\ntranslational states\\ntime-varying system dynamics\\ncontrol parameters\\nlinearization\",\"681\":\"aircraft\\naerodynamics\\natmospheric modeling\\ncontrol systems\\nmathematical model\\naerospace control\\npropellers\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\nhelicopters\\nmatlab\\nmicrocontrollers\\nmobile robots\\nposition control\\nautonomous fixed-wing aerobatics\\nunmanned aerial vehicles\\nconventional fixed-wing aircraft\\nhardware-in-the-loop simulator\\npixhawk microcontroller\\nxplane physics engine\\nhil simulator\\nflight platform\\nagile fixed-wing uav\\nrotorcraft\\norientation time histories\\nposition time histories\\nmatlab-simulink high-fidelity simulation\",\"682\":\"propellers\\naerodynamics\\natmospheric modeling\\nattitude control\\naircraft\\nmathematical model\\nquaternions\\nadaptive control\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nleast squares approximations\\nlyapunov methods\\nstability\\nunified controller\\nattitude dynamics model\\nflight regimes\\nlyapunov stability theory\\ncontrol challenges\\nrotary wing uavs\\nfixed wing\\ntail-sitter unmanned aerial vehicles\\ntail-sitter uav\\nadaptive attitude control\\ncontrol scheme\\nadaptive controller\\nquaternion attitude description\\nfull-regime aerodynamics model\\nthrust vectoring model\\nsingle thrust-vectored propeller\\ncumbersome controller switchings\\ntransition flights\",\"683\":\"aerodynamics\\natmospheric modeling\\nestimation\\nuncertainty\\nreal-time systems\\naircraft\\nadaptation models\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\nleast squares approximations\\nmonte carlo methods\\nsensors\\nwind tunnels\\nsmall fixed-wing unmanned aerial vehicles\\ntotal least squares estimation\\nordinary least squares method\\nlow-cost sensor system\\ninsufficient system excitation\\nvariable forgetting factor\\nmonte carlo approach\\ncompound aerodynamic variables\\nuncertainty estimation\\nreal-time schemes\\nwind-tunnel experiments\\nreal-time estimation\\nuncertain flight data\\nonline aerodynamic model identification\",\"684\":\"robots\\ntask analysis\\ntorque\\nsafety\\ncollision avoidance\\nx-ray imaging\\naerospace electronics\\ncontrol engineering computing\\nend effectors\\nlinear quadratic control\\nmanipulator dynamics\\nmedical image processing\\nmedical robotics\\nmobile robots\\nmotion control\\nrobot vision\\ncontrol solution\\nrobotic manipulator\\ngeneric safe controller\\nlinear quadratic problem formulation\\nunified energetic formulation\\nkinetic energy\\nredundant kuka lwr4+ robot\\nx-ray medical imaging\\nend-effector\",\"685\":\"task analysis\\nnull space\\nmanipulators\\nhuman-robot interaction\\ntorque\\nsurgery\\nadaptive control\\nfuzzy control\\nmedical robotics\\ntelerobotics\\nrobot manipulator\\nteleoperated minimally invasive surgery\\nsurgical task execution\\nvirtual surgical tasks\\ncompliant null space motion\\ntele-operated mis tasks\\nimplemented impedance control\\nsafety-enhanced compliant behavior\\nteleoperation control\\nredundant robot\\nsafety-enhanced human-robot interaction control\",\"686\":\"needles\\nrobots\\ntask analysis\\nsurgery\\nimage segmentation\\nbayes methods\\natmospheric measurements\\nbiomedical optical imaging\\nendoscopes\\nmedical image processing\\nmedical robotics\\nrendering (computer graphics)\\nrobot kinematics\\n3d surgical needle localization\\nstereoendoscopic image streams\\ncomputer vision techniques\\nda vinci\\u00ae surgical robotic system\\nstereo endoscopic camera images\\nthree-dimensional tracking\\nda vinci \\u00ae robot endoscope\",\"687\":\"task analysis\\ntraining\\nrobot kinematics\\nwires\\ntools\\nsurgery\\nlearning (artificial intelligence)\\nmedical robotics\\ntelerobotics\\nvisuomotor learning\\ncomplex visuomotor training\\nda vinci research kit surgical console\\nsurgical teleoperated robots\\nsurgical practice\\nhands-on training\\nsurgical robotics training\",\"688\":\"robots\\nimaging\\nsurgery\\nthree-dimensional displays\\nlaparoscopes\\nkidney\\ntask analysis\\nbiomedical optical imaging\\nendoscopes\\nmedical image processing\\nmedical robotics\\n3d endoscope\\nrobotic surgical system\\ncutting depth\\nfreedom electro-surgical tool\\nrobotic system\\nimaging system\\nlaparoscopic camera\\nopen loop control scheme\\nporcine cadaver kidney\\nrobotic laparoscopic surgery system\\nsemiautonomous laparoscopic robotic electro-surgery\",\"689\":\"acoustics\\nimpedance\\nfinite element analysis\\ntransducers\\nbones\\nsurgery\\ncutting tools\\nbiomedical transducers\\nbiomedical ultrasonics\\nbone\\ngenetic algorithms\\nmedical robotics\\nultrasonic transducers\\nfinite element software\\ncutting phantom\\nresearch kit system\\nultrasonic system\\nmultiobjective genetic algorithm\\nultrasonic transducer\\nminimally invasive ultrasonic bone cutting tool\\nda vinci research kit\",\"690\":\"calibration\\ncameras\\ngrippers\\nrobot kinematics\\nrobot vision systems\\ntools\\nbiological tissues\\ndiseases\\nedge detection\\nend effectors\\nendoscopes\\nmedical robotics\\nneural nets\\nposition control\\nrobot vision\\nsurgery\\ntelerobotics\\nfragment phantoms\\ncable-driven robots\\ndiseased tissue fragments\\nda vinci research kit\\ncable-driven systems\\ntwo-phase coarse-to-fine calibration method\\nred calibration marker\\nend effector\\nopen-loop trajectories\\ncamera pixels\\ninternal robot end-effector configurations\\nrobotic surgical assistants\\ndeep neural network\\nend-effector position\\nrandom forest\\ntwo-phase calibration procedure\\nsurgical debridement\\nfine transformation bias\\nresidual compensation bias\\ncoarse transformation bias\\ntime 7.3 s to 15.8 s\\nsize 4.55 mm\\nsize 2.14 mm\\nsize 1.08 mm\",\"691\":\"games\\nsoftware algorithms\\nsoftware\\nhardware\\nreal-time systems\\ntesting\\ncomputational modeling\\naircraft control\\nautonomous aerial vehicles\\ngame theory\\nhelicopters\\nmobile robots\\nmulti-robot systems\\noptimal control\\npath planning\\ntrajectory control\\nmultiple quadrotors\\nflag game\\ndistributed trajectory planning algorithm\\nwifi based communication infrastructure\\nautopilot modules\\nlow power computing modules\\nsuboptimal control action\\nadversarial game\\ngazebo robot simulator\\nmultiple uavs\\nquadrotor platform\\nflight testing\\nrobot operating system\\nros\",\"692\":\"cameras\\nsimultaneous localization and mapping\\ncollaboration\\nestimation\\nunmanned aerial vehicles\\naerospace computing\\nautonomous aerial vehicles\\ngroupware\\nimage fusion\\nkalman filters\\nmulti-robot systems\\nnonlinear filters\\npose estimation\\nrobot vision\\nstereo image processing\\nmonocular-inertial odometry\\nextended kalman filter\\ncollaborative scene estimation\\nmonocular camera\\nvariable-baseline stereo rig\\ninertial sensor\\ncollaborative robot operation\\ncollaborative 6dof relative pose estimation\\nuav\",\"693\":\"quality of service\\ntarget tracking\\nmonitoring\\ncomputational modeling\\nsensor systems and applications\\ncontext modeling\\nautonomous aerial vehicles\\nbelief networks\\ndecision making\\nembedded systems\\nmarkov processes\\nplanning\\ndiagnosis modules\\nbayesian networks\\nmission specifications\\nmarkov decision processes\\nbfm model\\napplication configurations\\nembedded system level\\nuav level\\ntarget tracking mission\\napplications specifications\\nmdp model\\nembedded applications\\nresource-aware method\\nadaptive mission planning\\nexternal hazards\\nfmea tables\\nscalable model\\nmodular method\\ndecision making process\\nuav\\ninternal hazards\",\"694\":\"collision avoidance\\nrobot kinematics\\ntrajectory\\nshape\\ncost function\\napproximation theory\\ncomputational complexity\\ngraph theory\\nmulti-robot systems\\noptimisation\\npath planning\\no(n3) time complexity\\noptimal assignments\\nmultiple robots\\nfixed goal formations\\nstandard assignment problem\\ntransformed problem\\nformation parameters\\nlinear sum assignment problem\\nvariable goal formation problem\\nlocation parameters\",\"695\":\"tracking\\nrobot sensing systems\\nmotion segmentation\\nmachine learning\\ncalibration\\nkinematics\\nneural networks\\nimage motion analysis\\nimage sensors\\nmultilayer perceptrons\\nsensor positions\\nbody segments\\nstandard deviation\\ncalibration values\\nrotation matrices\\ninertial motion-capture systems\\nlatency errors\\nmotion data\\nsensor-displacement patterns\\nrotational transformations\\nkinematic algorithms\\nsensor movement\\nperformance degradation\\neuler angles\\nplacement-insensitive inertial motion capture\\njoint angles\\nsensor data\\ntime 3.0 hour\",\"696\":\"shoulder\\nactuators\\nexoskeletons\\nkinematics\\nend effectors\\nbiomechanics\\nfeedback\\nmedical robotics\\noptimisation\\npatient rehabilitation\\nrobot dynamics\\nrobot kinematics\\nwrist robots\\nanalytical stiffness model\\nbounded nonlinear multiobjective optimization\\nparallel architecture\\nwearable hip\\nankle\\nparallel-actuated robotic shoulder exoskeleton\\nsagittal plane\",\"697\":\"foot\\nlegged locomotion\\nexoskeletons\\nhip\\ntorque\\nknee\\noscillators\\nadaptive control\\nartificial limbs\\ngait analysis\\nhandicapped aids\\nmedical robotics\\nmuscle\\npatient rehabilitation\\nactive lower-limb exoskeleton\\nmetabolic impact\\nhip abduction\\/adduction\\nhip extension\\/flexion\\nknee extension\\/flexion joints\\nwalking environment\\nadaptive oscillator-based control\\nelectric actuators\",\"698\":\"exoskeletons\\nkinematics\\nsolid modeling\\ndynamics\\nthree-dimensional displays\\ncalibration\\nmathematical model\\ngait analysis\\nkalman filters\\nmedical robotics\\nwii balance board\\njoint kinematics\\nbody segment inertial parameters\\nhuman locomotor apparatus\\naugmented regressor matrix\\nground reaction force\\ndynamic identification pipeline\\nqr visual markers\\nextended kalman filter\\nhuman-exoskeleton system dynamics identification\\nlower limb exoskeleton\",\"699\":\"cameras\\ntask analysis\\nimage edge detection\\nfeature extraction\\nlegged locomotion\\nartificial limbs\\nfinite state machines\\ngait analysis\\nhandicapped aids\\nintelligent robots\\nmotion control\\northotics\\nrobot vision\\nwearable robots\\nlower-limb assistive device\\ndepth images\\nprostheses\\ndaily living activities\\nintelligent controller\\ninnovative locomotion recognition system\\nfeature extraction subsystem\\nfinite-state-machine based recognition subsystem\\nlimb movements\\nlocomotion modes\\ntransition states\\nlocomotion tasks\\npowered lower-limb orthoses\\nwearable robot\",\"700\":\"electrodes\\nadhesives\\nsubstrates\\nforce\\nrough surfaces\\nsurface roughness\\nadhesion\\nbending\\nbending strength\\nelasticity\\nshear strength\\ndry switchable adhesives\\ncompliant structures\\nhigh stored strain energy\\nshear adhesion pressures\\ncontact area\\nbending compliance\\nhybrid electrostatic-gecko-like adhesives\\nshear stiffness\\nmechanical strength\",\"701\":\"friction\\ntendons\\nanisotropic magnetoresistance\\ndc motors\\nmobile robots\\nsurface morphology\\ncontinuum robots\\nsoft-bodied robots\\nsnake robot\\nserpentine locomotion\\nwriggle soft-bodied robot\\nhigh friction material\\nlow friction material\\nsnake-like soft-bodied robots\\nfrictional 2d-anisotropy surface\\nprintable soft-bodied robots\\nanisotropic structure\",\"702\":\"robots\\nforce\\nstrain\\nfriction\\nkinetic theory\\nbiological system modeling\\nactuators\\nbiomechanics\\nbiomimetics\\ndeformation\\nrobot dynamics\\nrobot kinematics\\ncrawling locomotion behavior\\ninchworm-like crawling movement\\ndeformable properties\\nbio-inspired design\\nexperimental validation\\nactuated deformation capability\\nsoft actuation mechanisms\\ninchworm locomotion mechanism\\nself-deformable capsule-like robot\\nrigid elements-based morphing structure\\nrobot deformation\",\"703\":\"magnetometers\\nrobot sensing systems\\naerodynamics\\nestimation\\naccelerometers\\nmagnetic flux\\nmagnetomechanical effects\\naerospace components\\naerospace control\\nautonomous aerial vehicles\\nsensor fusion\\nrealtime on-board attitude estimation\\nhigh-frequency flapping wing mavs\\ninstantaneous oscillation\\nfixed wings\\nrotary wings\\nhigh-frequency wing flapping\\naerial vehicles\\nflapping wing micro aerial vehicles\\nfwmavs\\ninstantaneous oscillations\",\"704\":\"robot sensing systems\\nplastics\\ncopper\\nstrips\\nwires\\nmobile robots\\nrobotic assembly\\nself-assembly\\nmodular 2d robot\\nfull-body continuous docks\\ndocking mechanism\\nmechanical complexity\\nrobotic self-assembling structures\\ninert fireant robots\",\"705\":\"task analysis\\nhardware\\nmobile robots\\nbridges\\nbuildings\\nplanning\\ncollision avoidance\\nplanning (artificial intelligence)\\nhigh-level planner\\ndisconnected regions\\nhardware experiments\\nplanning tools\\nrobot locomotion capabilities\\nspecially-designed building blocks\\nenvironment characterization algorithm\\nmodular robot systems\\nbuilding structures\\nhigh-level tasks\\nperception-informed autonomous environment augmentation\",\"706\":\"grippers\\nforce\\nfasteners\\ncalibration\\nrobot sensing systems\\nstrain measurement\\nprototypes\\nimage sensors\\nmicroassembling\\nmicromanipulators\\nmicrosensors\\nposition measurement\\ncompact microgripper\\nmicroobject manipulation\\nflexure hinge\\nlow impedance grasping mechanism\\nkinematics analysis\\nfine element analysis\\nfea\\nfibrous microring assembling\\nvisual-based calibration method\\nposition sensors\\nlaser sensor\\nembedded sensors\\ndexterous manipulation\",\"707\":\"grasping\\nrobots\\ntraining\\ndata models\\ndatabases\\nfeature extraction\\nmachine learning\\ndexterous manipulators\\nend effectors\\nfeedforward neural nets\\ngrippers\\nhumanoid robots\\nlearning (artificial intelligence)\\npose estimation\\nrendering (computer graphics)\\nrobot vision\\ndeep convolutional neural networks\\ntraining input\\nhigh-quality grasps\\nanalytical grasp planners\\nrendered depth images\\ntraining objects\\ndeep learning techniques\\nrobotic grasping\\napproach directions\\ngrasping setup\\nbig data grasping database\\nqualitative grasping experiments\\nhumanoid robot armar-iii\\nunknown objects\\ndata-driven\\ndeep learning approach\",\"708\":\"task analysis\\nrobot kinematics\\nplanning\\ncollaboration\\nforce\\nquadratic programming\\ndexterous manipulators\\nforce control\\ngrippers\\nhuman-robot interaction\\nindustrial manipulators\\nlifting\\nmanipulators\\ncollaborative manipulation\\nmanipulation task\\ngrasp location\\nhuman robot collaborative lifting task\\ngrasp planning\\ngrasp analysis approach\\nload sharing\\npartial observability\\ntwo-agent decentralized set-up\",\"709\":\"robots\\nforce\\ntendons\\ntask analysis\\nmuscles\\nfrequency modulation\\nmathematical model\\nbiocontrol\\nbiomechanics\\ndexterous manipulators\\nforce control\\nmanipulator dynamics\\nmechanical stability\\nmuscle\\nposition control\\nhuman neuromuscular system\\ngrasp stability\\nhuman-inspired object manipulation control\\nanatomically correct testbed hand\\ndexterous manipulation\\nrobotic hand\\nobject-level impedance control strategies\\ngrasp forces\\nrobotic system\\nobject stiffness control gains\\nobject-space stiffness control algorithm\\nobject size\\nobject shape\\ngrasp stability bounds\\nobject-space stiffness\\nlow-level stiffness\\nact hand\",\"710\":\"grasping\\nshape\\ncomputational modeling\\nrobots\\nthree-dimensional displays\\npipelines\\noptimization\\ngrippers\\nhumanoid robots\\npattern classification\\nsuperquadric modeling\\nobject shape\\nobject modeling\\nsuperquadric functions\\nobject classifier\\nrobot hands\\nrobotic system\\nicub humanoid robot\",\"711\":\"trajectory\\nrobots\\nlearning (artificial intelligence)\\nbayes methods\\nentropy\\ntask analysis\\nuncertainty\\ncontrol engineering computing\\nquery processing\\nrobot programming\\ncritiques\\nactive reward learning\\nprogramming robots\\nactive bayesian inverse reinforcement learning\\ntrajectory queries\\nlabeling process\\nactive learning\",\"712\":\"uncertainty\\npredictive models\\nnoise measurement\\ndata models\\ntraining\\nestimation\\nmeasurement uncertainty\\nestimation theory\\nlearning (artificial intelligence)\\nlearning systems\\nmobile robots\\nmonte carlo methods\\nsampling methods\\nuncertainty handling\\nuncertainty estimation method utilizing\\nmonte carlo sampling\\nrobotics applications\\nautonomous driving\\nepistemic uncertainties\\naleatoric uncertainties\\nuncertainty acquisition\\ndemonstration method\\nsampling-free variance modeling\\nmixture density network\\nuncertainty-aware learning\",\"713\":\"task analysis\\nfeature extraction\\nrobots\\ntraining\\ntraining data\\nobject recognition\\nsupport vector machines\\nlearning (artificial intelligence)\\npattern classification\\nlfd scenarios\\nhuman feature selection\\nrobot learner\\ninformative features\\nmulticlass classification task\\ncomputational feature selection\\nhuman selected features\\ninformative task features\\ngeneral-purpose robot\\nlearning computation\\nrobotic agent learning classification tasks\\nhuman-driven feature selection\",\"714\":\"service robots\\nhidden markov models\\nvocabulary\\nfeature extraction\\nknowledge based systems\\ntask analysis\\ncameras\\ngraph theory\\nmobile robots\\nobject recognition\\nrobot vision\\nobject-centric approach\\nmanipulation tasks\\nhuman manipulation actions\\nobject trajectories\\ncontext specific human vocabulary\\ndirected action graph representation\\npre-computed location areas\\noffline teaching phase\\ngraph generation\\nonline action recognition phase\\nhigh-level reasoning\\nsensor observation\\nvisual sensory input\\ndepth camera\\nla\\nsector-maps\\nsm\\naction recognition\\nmotion analysis\\ngraph method\\nlocation area\\nsector-map\\nknowledge representation\",\"715\":\"task analysis\\nvisual odometry\\nestimation\\nvisualization\\ntraining\\nrobustness\\ndistance measurement\\nfeature extraction\\nlearning (artificial intelligence)\\nneural nets\\npose estimation\\nvideo signal processing\\nstate-of-the-art sift-based approaches\\ndeep learning technique\\nmultitask learning\\ngeometric consistency loss\\nvisual odometry estimation\\nglobal localization\\nparameter sharing\\nmultitask model\\nconsecutive monocular images\\nvlocnet\\nconvolutional neural networks\\naction execution\\nrobot\\nvisual localization\",\"716\":\"measurement uncertainty\\nmathematical model\\nrobot sensing systems\\nkinematics\\ninstruments\\nmanipulator kinematics\\nposition control\\ncdpr\\nextra measurements\\nextra sensors\\ncable orientations\\ndirect kinematics\\ncable-driven parallel robots\\ncable length measurements\\nmodel-based approach\\ncable tension sensors\",\"717\":\"kinematics\\nwrist\\ncouplings\\nactuators\\noptimization\\ncomputer architecture\\nend effectors\\nmanipulator kinematics\\nmotion control\\noptimisation\\nkinematic design\\ngeometric optimization\\nprismatic-revolute-universal linkage\\nprismatic-spherical-spherical linkage\\nspherical motion\\npitch-yaw-roll wrist\\narbitrary direction\\nforward kinematics\\ninverse kinematics\\nparallel 2-dof mechanism\\ndesign parameters\\nglobal transmission index\\ntorque transmissibility\\ndecoupled nature\\nyaw mechanism\\nkinematic optimization\\npartially decoupled three degree of freedom hybrid wrist mechanism\\nwrist configuration\",\"718\":\"kinematics\\ntorso\\nmathematical model\\nrobot kinematics\\nplanning\\nmanipulators\\nbars\\nflexible manipulators\\nmanipulator dynamics\\nmanipulator kinematics\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nreconfiguration analysis\\nmutual mode transition rules\\nkinematics model\\nmotion planning\\nreconfiguration rules\\nreconbot\\nflexible torso\\nstraight bar-shape base\\nmetamorphic kinematic chains\\nconfiguration states\\nreconfigurable mobile manipulator torso\\n2rer reconfigurable parallel mechanism\\ntransition handling\\nsingularity position\",\"719\":\"kinematics\\nrobot sensing systems\\nheuristic algorithms\\nrobot kinematics\\ncomplexity theory\\nreal-time systems\\ncomputational complexity\\nmanipulator kinematics\\nmatrix algebra\\nopen kinematic chains\\nevent-driven forward kinematics algorithms\\ncomputational resources\\nroot joint\\nconventional forward kinematics\\ncomputation time\\nevent-driven fk algorithms\\nsensory data\\nhomogeneous transformation matrix\\ntime-variance\\nalgebraic structures\",\"720\":\"robot sensing systems\\ncollision avoidance\\nnavigation\\nmobile robots\\nforce\\nwires\\npath planning\\nturtlebot mobile robot platform\\nlocal sensory information\\narbitrary-shaped convex environment\\nmagnetic fields\\nnonholonomic mobile robot taking inspiration\\nreactive robot navigation method\\nunknown environments\\nnonholonomic mobile robots\\nreactive magnetic-field-inspired navigation\",\"721\":\"unmanned aerial vehicles\\npropellers\\nshape\\ngrasping\\nservomotors\\nforce\\nend effectors\\naerospace control\\nautonomous aerial vehicles\\nclosed loop systems\\nmobile robots\\noptimisation\\nposition control\\nclosed-loop aerial transformation\\naerial grasping\\nshape adaptive transformation\\naerial manipulation\\nhalo\\nhorizontal plane transformable aerial robot\\nclosed-loop multilinks structure\\nflight control\\nserial-link structure\\npropeller\\noptimization planning method\",\"722\":\"propellers\\nbars\\ntrajectory\\nrobots\\ntask analysis\\nadmittance\\nforce\\nautonomous aerial vehicles\\ncontrol system synthesis\\nestimation theory\\ngeometry\\nmanipulators\\nmobile robots\\nrobust control\\ntrajectory control\\nmaintenance tasks\\ntask-driven custom design\\nexperimental validations\\ncontrol framework\\nlow-level geometric controller\\nexternal wrench estimator\\nadmittance filter\\ntrajectory generator\\nexternal force disturbances\\nflying assistant paradigm\\nothex platform\\naerial manipulation\\nlaas-cnrs\\nmultidirectional thrust platform\\nhuman operators\\nlong bars\\nassembly tasks\\nground manipulators\",\"723\":\"rotors\\nblades\\naircraft\\nforce\\nfasteners\\nactuators\\ntrajectory\\naerodynamics\\nautonomous aerial vehicles\\nhelicopters\\nposition control\\nvehicle dynamics\\nflat body attitude\\nfully actuated aerial vehicle\\nmicroair vehicles\\nquadrotors\\ndownward thrust\\nspatial trajectories\\ncoaxial helicopter\\nthrust vector\\ntranslation dynamics\\ncyclic flapping response\",\"724\":\"rotors\\nrobots\\nforce\\nloading\\ntorque\\nhydraulic systems\\nactuators\\ndecentralised control\\nhydraulic actuators\\nmachine control\\nvalves\\nrobotic system\\nlasdra\\nvalve turning\\ntrajectory tracking\\nstrong\\/sturdy base actuator\\/structure\\nhydraulic actuation\\nlarge-size aerial skeleton system\\nlarge-size dexterously-articulated robot\\ninternal actuation\\nexternal actuation\\ndistributed rotors\\ndistributed rotor actuation\",\"725\":\"grippers\\napertures\\nrobots\\nrotors\\ngrasping\\npropellers\\nshape\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\nmulti-robot systems\\nposition control\\ndegree of freedom\\nfour-bar linkage\\naperture angle\\ncuboid frame\\ndocking mechanism\\nvertical edges\\ngrasp object\\ncuboid modular robots\\nflying gripper\\nhovering performance\\ndof\",\"726\":\"cameras\\ndrones\\nthree-dimensional displays\\nskeleton\\nplanning\\nrobot vision systems\\ntrajectory\\ncinematography\\nmotion estimation\\nremotely operated vehicles\\nvideo cameras\\naction scenes\\naerial filming\\nautonomous cinematography system\\nautonomous drone cinematography system\\nstate-of-the-art drone camera system\\nreal-time dynamical camera planning strategy\\ndrone platform\\nhuman action\\nexternal motion capture systems\\ndrone cinematography systems\\naesthetic objectives\",\"727\":\"computational modeling\\nstochastic processes\\ntrajectory\\nuncertainty\\nplanning\\naerospace electronics\\ninteger programming\\nlinear programming\\nmobile robots\\npath planning\\ntree searching\\nrisk-bound stochastic path planning\\noften intractable problem\\nautonomous agents\\ncomplex stochastic processes\\nfast path planning\\nchance constraint\\nstochastic path planning problem\\nnonconvex problem\\nscales computational effort\\nmilp approach\\nparallelized sampling-based approach\",\"728\":\"space exploration\\nplanning\\nterminology\\nprincipal component analysis\\nprobabilistic logic\\nrobots\\ncollision avoidance\\neigenvalues and eigenfunctions\\nmobile robots\\nrandom processes\\nsampling methods\\ntrees (mathematics)\\nconfiguration space\\ntree expansion\\neigenvectors\\nclassical rrt algorithm\\nrapidly-exploring random vines algorithm\\nmotion planning problem\\nnarrow passage\",\"729\":\"trajectory\\nplanning\\nmonte carlo methods\\nmarkov processes\\ncost function\\nheuristic algorithms\\napproximation theory\\nconvergence of numerical methods\\nestimation theory\\nlearning (artificial intelligence)\\noptimisation\\npath planning\\nrandom processes\\nsampling methods\\nstate-space methods\\ntrees (mathematics)\\nasymptotically-optimal motion planners\\nsubsequent samples\\nmotion-planning problem\\neuclidean space\\nnoneuclidean state spaces\\ndimensional state space\\nplanning algorithm\\nsub-level-set\\nmonte carlo sampling methods\\nhigh-quality solutions\\nhigh-dimensional problems\\nmarkov chain monte carlo\\ninformed set\\ngeneralizing informed sampling\\nasymptotically-optimal sampling-based kinodynamic planning\\nhierarchical rejection sampling\",\"730\":\"planning\\napproximation algorithms\\ntrajectory\\noptimization\\nrobots\\nprobabilistic logic\\nlinear programming\\napproximation theory\\noptimisation\\npath planning\\ngrid-based approaches\\noptimization-based planner\\nresolution-complete factors\\nspatial information\\nempirical information\\nlearned information\\noptimization-based local planner\\nasymptotic optimal planners\\nsimultaneous planning\\nconfiguration free space approximation\\noptimal motion planning\\nsampling-based planner\\ndancing prm\",\"731\":\"mathematical model\\nrobot kinematics\\nplanning\\ntrajectory\\nmanifolds\\nstandards\\ncollision avoidance\\nlarge-scale systems\\nmanipulators\\nmobile robots\\nrandom processes\\nrobot dynamics\\nstate-space methods\\ntrajectory control\\ntrees (mathematics)\\nkinodynamic rrt planner\\natlas\\nstate-space manifold\\nrandomized kinodynamic planner\\nholonomic constraints\\nconstrained systems\\nhigh-dimensional dynamical systems\\nparallel manipulators\\ncomplex systems\\ntrajectories\\nrobots\",\"732\":\"planning\\nrobots\\nprobabilistic logic\\nmanifolds\\ncollision avoidance\\nfeature extraction\\nacceleration\\nmobile robots\\nsampling methods\\nrobot motion planning\\nsampling-based motion planning\\ncollision-avoidance\\nvariational autoencoder\\nbias sampling\",\"733\":\"task analysis\\nvisualization\\nsemantics\\ntrajectory\\ncomputer vision\\nstandards\\nintelligent robots\\nlearning (artificial intelligence)\\nmanipulators\\nvisual perception\\nrobotic manipulation\\ngeneralizable robot learning\\nobject-centric representations\\nreinforcement learning\\nobject-level attentional mechanism\\nperception system\\nsemantic feature space\",\"734\":\"cameras\\ngaze tracking\\nfeature extraction\\nthree-dimensional displays\\nprobabilistic logic\\nsolid modeling\\ncalibration\\nbayes methods\\nhuman-robot interaction\\nobject detection\\nstereo image processing\\nremote eye tracking\\nbayesian inference\\nhuman gaze\\ncognitive states\\ngaze-based interaction\\nhuman-robot collaboration\\ngaze estimation\\n3d glint detection\",\"735\":\"pareto optimization\\ntraining\\nautonomous vehicles\\ngenerators\\nlinear programming\\nprobability density function\\ncomputer vision\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nobject recognition\\npareto optimisation\\ngol\\ninput single one-shot objects\\nenvironment perception\\nautonomous vision\\nsemiparametric approach\\ndeep neural networks\\nvisual perception\\ndriving environment\\ntraining perceptions systems\\ngenerative framework\\nhighly autonomous driving systems\\ngenerative one-shot learning\\nhad systems\\npareto optimal solutions\\nobject detection algorithms\",\"736\":\"visualization\\ntraining\\nadaptive systems\\nadaptation models\\nmachine learning\\nservice robots\\ngeneralisation (artificial intelligence)\\nhumanoid robots\\nhuman-robot interaction\\nimage classification\\nlearning (artificial intelligence)\\nrobot vision\\ndomain shift\\nend-to-end deep domain adaptation architecture\\ntarget domain\\ntraining time\\nhuman-robot interactions\\nadaptive deep\\nvisual domain localization\\ncommercial robot\\nillumination conditions\\ndomain adaptation methods\\nrobotics applications\\ncomputer vision\\ngeneralization issue\\nicub world database\",\"737\":\"training\\ntask analysis\\nsemantics\\nimage segmentation\\ncollision avoidance\\ngallium nitride\\ntraining data\\nlearning (artificial intelligence)\\nsynthetic images\\nsynthetic data\\ndomain-specific learning tasks\\nleverage recent progress\\nimage-to-image translation\\nsimulated images\\nrealistic training data\\nreal-world images\\ngenesis-rtameliorates\\ngenesis-rtto\\nhigh-accuracy predictions\\nraw simulated data\\ngenesis-rt images\\nmission-critical tasks\\nsecondary real-world task training\\ncluttered environment\\nreactive obstacle avoidance\\nsemantic segmentation\",\"738\":\"nonlinear distortion\\ngallium nitride\\ngenerators\\nimage color analysis\\nvisualization\\nsensors\\nautonomous underwater vehicles\\ndecision making\\nimage colour analysis\\nimage denoising\\nimage fusion\\nimage restoration\\nneural nets\\nrobot vision\\ngenerative adversarial networks\\nauvs\\nintelligent decision making\\ncolor distortion\\nnoisy images\\ndistorted images\\nunderwater image restoration\\nunderwater imagery\\nvisual data quality\\nvisual underwater scene quality\",\"739\":\"proposals\\ncorrelation\\nfeature extraction\\nimage segmentation\\nmachine learning\\nrobots\\nadaptation models\\nconvolution\\ncrops\\nfeedforward neural nets\\nimage classification\\nimage fusion\\nobject detection\\nprobability\\nrecurrent neural nets\\nrobot vision\\nmultiple classifiers\\nclassifier correlation\\nsmall fruit detection\\nfaster r-cnn network\\nmultiple classifier fusion\\nobjectness classification\\nprobabilities\\nagricultural robots\",\"740\":\"force\\nrobot sensing systems\\nswitches\\nreliability\\npressing\\nservice robots\\ncalibration\\nmobile robots\\nrobot button pressing\\nhuman environments\\nmobile robot\\nswitchit\\nhand-held tablet\\nbuttons categorization\",\"741\":\"task analysis\\nnavigation\\nvisualization\\nrobot kinematics\\nservice robots\\nmanipulators\\nhuman-robot interaction\\nspeech-based user interfaces\\nuncertain spatial terms\\nuncertain qualitative terms\\nplacement location\\nobject placement\\nqualitative distance information\\nvoice commands\\npeer companions\\ndaily assistive tasks\\nassistive robot companions\\nvoice instructions\\nhuman-robot interactions\\nhuman friendly robotics\\nservice robotics\\nobject manipulation\\nspatial infor-mation\",\"742\":\"robot kinematics\\ntrajectory\\ncollision avoidance\\nrobustness\\nrobot sensing systems\\nbayes methods\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\npredictive control\\nrobot vision\\nrobust control\\nservice robots\\ntrajectory control\\nvariational techniques\\nrobocup@home 2017 social standard platform league\\nrobust functions\\nhome service robots\\nservice-oriented robots\\nhuman assistance\\ncommercial service robot\\nrgb-d camera\\ndeep learning methods\\nvariational bayesian techniques\\ndeep learning modules\\ndynamic home environment\\ndeep bayesian trajectory prediction method\\nrobust human following\\nsmooth person following capability\\nhuman cooperation\\ntarget detection\\nrobot following ability\",\"743\":\"robot sensing systems\\nestimation error\\nuncertainty\\nprobabilistic logic\\nservice robots\\nautomobiles\\ngeriatrics\\nhandicapped aids\\nmobile robots\\npath planning\\nposition control\\nslam (robots)\\nactive sensing system\\ncontrol law\\nsenior user guidance\\npath following problem\\nlandmarks\\nactuator control\\naccurate localisation\\nexact localisation\\nrobotic walking assistant\\ninformation precision\\nservice robot\\ncontrol authority\\ndesign strategy\\nmassive data collection\\nslam approaches\",\"744\":\"trajectory\\nmotion measurement\\nkernel\\ncomputational modeling\\nrobot sensing systems\\ntask analysis\\ngaussian processes\\nhuman-robot interaction\\nimage motion analysis\\nimage representation\\nimage sequences\\nlearning (artificial intelligence)\\noptimisation\\nnonparametric motion flow model\\nhuman robot cooperation method\\npartial trajectory information\\ntarget trajectories\\nlearned motion description\\nunderlying reward function\\ninteracting trajectories\\nvariance functions\\ntemporal properties\\nspatial properties\\nmotion flow similarity measure\\nmotion trajectory\",\"745\":\"robot kinematics\\ntask analysis\\ntools\\nservice robots\\ntrajectory\\nquaternions\\nforce sensors\\ngrinding\\ngrinding machines\\nindustrial robots\\niterative learning control\\npolishing\\npolishing machines\\nrobot dynamics\\nsurface finishing\\nfinishing operations\\nvirtual mechanism approach\\npassive digitizer\\noptimal robot execution\\nserial kinematic chain\\naugmented system\\npolishing tools\\ngrinding tool\\niterative learning controller\",\"746\":\"probabilistic logic\\ntask analysis\\nrobot kinematics\\nacceleration\\ntrajectory optimization\\nhumanoid robots\\nlearning systems\\nmanipulator kinematics\\nprobability\\ntrajectory control\\njoint space\\nmotion constraints\\nprobabilistic formulation\\ndynamic movement primitives\\nprobabilistic treatment\\ntrajectory constraints\\nhybrid space learning\\nmotion smoothness\\nrobot null-space\\nhybrid probabilistic trajectory optimization\\nnull-space exploration\\ncartesian space\\nlearning from demonstration\\njacobian-based inverse kinematics\",\"747\":\"cameras\\nnavigation\\ncollision avoidance\\nsimultaneous localization and mapping\\nplanning\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nsensory constraints\\niterative motion planning framework\\nonline mapping\\nassociated map points\\ndistance-optimal path planner\\ndata-driven approach\\ncontinuous identification\\nfeature-based visual simultaneous localization\\nvision-based navigation\\nfailure avoidance\\nmobile robot navigation\\nfeature-constrained active visual slam\",\"748\":\"transforms\\ncameras\\nvisualization\\nrobot sensing systems\\nrobustness\\nnavigation\\nfeature extraction\\nmobile robots\\npath planning\\nrobot vision\\nrough terrain\\nunstructured terrain\\nborder patrol\\nagricultural work\\nsensor-based navigation\\nerratic motion\\nfeature-poor environments test feature tracking\\nrepeat matching\\nsalient point features\\ngrizzly robotic utility vehicle\\nactively gimbaled camera\\nimage motion\\nsearch-and-rescue\\nfield-deployable ground robot\\nvision-based route-following\",\"749\":\"cameras\\ntracking\\nthree-dimensional displays\\nestimation\\nvisual odometry\\nfeature extraction\\nimage segmentation\\ndistance measurement\\nmobile robots\\nmotion control\\nmotion estimation\\nposition control\\nrobot vision\\nmotion estimation process\\npositioning inaccuracy\\nstructured environments\\nrotational motion\\ndrift-free rotation\\nso(3)-manifold constrained mean shift algorithm\\nmultiple orthogonal planes\\nrotation estimate\\nstructural regularities\\ndrift-free rotational motion\\nlow-drift visual odometry algorithm\\ntranslational motion\",\"750\":\"visualization\\nfeature extraction\\nelectronic mail\\ncost function\\nmeasurement\\nclosed-form solutions\\nintelligent robots\\ncomputational complexity\\nimage matching\\nmotion estimation\\nguided locality preserving matching\\nglpm\\npanoramic images\\nlinear space complexities\\nvisual homing problem\\nsparse feature matches\\nhoming directions\\nfeature matching\\nmismatch removal\\ndense motion flow estimation\\ntikhonov regularization\",\"751\":\"visualization\\nfeature extraction\\nrobustness\\nurban areas\\ntransforms\\npose estimation\\nrobots\\noutlier removal\\nlogos\\nlocal geometric support\\nhigh-outlier spatial verification\\nvisual localization\\norientation information\\ninlier points\\nsecondary localization verification\\nbenchmark localization datasets\\nlocal neighbourhoods\",\"752\":\"visualization\\nencoding\\nsimultaneous localization and mapping\\nfeature extraction\\ntask analysis\\nimage coding\\nfeature selection\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nslam (robots)\\nfeature selection stage\\nremote visual slam\\nautonomous robotics\\ncollaborative slam approaches\\nmultiple robots\\nfeature coding scheme\\nvisual sensors\\nembedded devices\\nlocal binary features extraction\\ncentralized powerful processing node\",\"753\":\"training\\ncameras\\nmachine learning\\nthree-dimensional displays\\nestimation\\nimage sequences\\nvisual odometry\\nfeature extraction\\nlearning (artificial intelligence)\\nneural nets\\npose estimation\\nstereo image processing\\nunsupervised learning\\nmonocular visual odometry system\\nmonocular camera\\ndeep neural networks\\nunsupervised deep learning scheme\\nundeepvo\",\"754\":\"robots\\nnatural languages\\nplanning\\ncomputational modeling\\ncharging stations\\nmodel checking\\nmarkov processes\\ncontrol engineering computing\\nformal verification\\ninteger programming\\nlinear programming\\npath planning\\ncomplex automaton\\nmixed-integer linear programming\\nmarkov decision processes\\nwarehouse robots planning\\nrobotic mission plan\\nmdp model\\nrobotic behavior\\nstructured natural language sentences\",\"755\":\"roads\\nplanning\\ndelays\\ntask analysis\\nsensors\\ntrajectory\\nautomobiles\\ncollision avoidance\\nmobile robots\\noptimal control\\noptimisation\\nroad traffic\\nscheduling\\npotential collision situations\\nroad geometries\\njoint motion plans\\nmultivehicle motion planning\\nroad network\\nvienna convention\\ndesired deadlines\\nintegrated route\\nmotion planning problem\\nsocial optimal mobility-on-demand\\nself-driving cars\\nbubble spaces\\nqueue scheduling\",\"756\":\"safety\\nuncertainty\\nrobots\\ntesting\\ntrajectory\\noptimization\\nbayes methods\\ngaussian processes\\nlearning (artificial intelligence)\\noptimisation\\ncomplex safety specifications\\ncomplex controllers\\nbayesian optimization\\nadversarial examples\\ncoherent optimization framework\\ngaussian process prior\\nindividual functions\\nreinforcement learning\\nreward functions\\nsmooth functions\\ncomplex boolean combinations\\nadversarial counter examples\\nsafety constraints\\nactive-testing framework\\nsafety-critical applications\",\"757\":\"minimization\\nrobots\\ntask analysis\\ncolor\\ncomputational modeling\\ncognition\\npartitioning algorithms\\nbisimulation equivalence\\ncomputational complexity\\nfiltering theory\\nminimisation\\npolynomials\\nequivalence relation\\nfilter minimization problem\\npolynomial time\\nbisimulation relations\\nbisimilarity relation -the union\\nequivalent behavior-is np-hard\\ncombinatorial filter reduction\\nbisimulation quotient operation\\ninput filter\",\"758\":\"predictive control\\nrobustness\\ncollision avoidance\\ntask analysis\\ngaussian processes\\nservice robots\\ncontrol system synthesis\\nlearning (artificial intelligence)\\nregression analysis\\ntemporal logic\\nlearning-based model predictive control method\\ndifferential constraints\\ndynamical systems\\ncontrol strategy synthesis method\\nsignal temporal logic specifications\\nspecific rules\\nmodel predictive control procedure\\nlearned margin\\nsignal temporal logic formula\\ndesigned controller\\ntraditional control scheme\",\"759\":\"robot kinematics\\ntask analysis\\nuncertainty\\nplanning\\nresource management\\nprobabilistic logic\\ncontrol engineering computing\\nformal specification\\nmobile robots\\nmulti-robot systems\\noperating systems (computers)\\npath planning\\ntemporal logic\\nprobabilistic options\\ntemporal logic-based multirobot cooperation\\ntemporal dependencies\\ntask specification\\nrobot team\\ntemporal logic specifications\\ngoal specification\\nros implementation\",\"760\":\"clustering methods\\ntopology\\nclustering algorithms\\ntoy manufacturing industry\\nrobots\\nprogramming\\nsupport vector machines\\ncomputational complexity\\nmesh generation\\npattern clustering\\ntriangulated mesh\\npath clustering\\npairwise distance calculations\\ntriangle inequality\\nminimum homology area\",\"761\":\"grasping\\nbelts\\nactuators\\ntask analysis\\ngrippers\\nmanipulators\\ndrives\\nlegged locomotion\\nmanipulator kinematics\\nmotion control\\nsprings (mechanical)\\nfinger\\nsynchronous belt drive\\nunderactuated graspers\\nserial kinematic chain\\nscalable kinematic structure\\nkinematic analysis\\nprototype robot\\nmultimodal locomotion\\nrobotic platform\\nactive gripping surface\\nunderactuated fingers\\nmultipurpose grasper\\nmanipulation\\ngraspman\",\"762\":\"grippers\\nservomotors\\ncontainers\\nsensors\\nrobots\\nmechanical cables\\npulleys\\nagriculture\\nclosed loop systems\\ninfrared detectors\\nmanipulators\\nposition control\\nrobot vision\\nrobust control\\ngripper design\\ncable-driven gripper\\nautonomous harvesting\\nir sensors\\nmanipulator arm\\nvision algorithm\\nrobustness\\npositional error tolerance\\nhigh-level closed-loop control\\nstrawberry picking robots\",\"763\":\"tendons\\nforce\\noptimization\\nsprings\\nmanifolds\\nkinematics\\ngrasping\\nactuators\\ndexterous manipulators\\ngrippers\\nmanipulator kinematics\\nstability\\nhand synergies\\nkinematic hand model\\nphysical underactuation mechanism\\nhand posture\\nsingle-actuator hand\\nunderactuated hand design\\nmechanically realizable manifolds\\njoint coordination patterns\\nplanning algorithms\\nrobotic grasping\\nmechanically realizable manifold\",\"764\":\"liquids\\nrobots\\ntrajectory\\ncontainers\\ntask analysis\\nacceleration\\noptimization\\nindustrial manipulators\\nmaterials handling\\nmotion control\\npath planning\\nsloshing\\nhandling liquids\\nservice robotic applications\\nmotion planning\\nliquid transfer\\nsloshing control\\nanti spilling constraint\\nspilling avoidance constraint\\nconstraint-based control\\nsloshing suppression\\nindustrial abb robot\\nrobotic manipulators\",\"765\":\"cameras\\nmanipulators\\ntask analysis\\nplanning\\nservice robots\\nrobot vision systems\\ncalibration\\ngrippers\\nindustrial manipulators\\nlearning (artificial intelligence)\\npath planning\\nrobot vision\\nfeature-based comparison\\ngripper system\\ngrasping strategy\\nrobust performance\\ntarget items\\nrobot system\\ndual 6 degrees of freedom industrial arms\\nerror recovery strategies\\nfixed calibrated frame\\nmultiple stereo cameras\\nvision system\\ncustom-designed top-open extendable shelf\\ncalibrated table\\nfixed bases\\nmodule designs\\ncomponent selection\\nmotion planning\\nsystem requirements\\namazon robotics challenge\\nreliable system\\nstable system\\nitem picking\\nrobust robot design\",\"766\":\"kinematics\\ntask analysis\\ndynamics\\nrobot sensing systems\\nshape\\nforce\\nmobile robots\\nrobot dynamics\\nrobot kinematics\\nvisual perception\\nphysics-based selection\\ninformative action\\nforceful interactions\\ntask-relevant information\\ninformative interactions\\narticulated mechanisms\\naction selection task\\ninteractive perception methods\\ninformation gain\\nrobust manipulation\",\"767\":\"shape\\nhistory\\ntask analysis\\nrobot sensing systems\\nthree-dimensional displays\\ngeometry\\nlearning (artificial intelligence)\\nmanipulators\\ngeometric object models\\nrobotic pick\\ndeep reinforcement learning problem\\ndeep rl\\nrobotic manipulation frame\\nlow level states\\npick-place\\nregrasping problems\\nexact geometry\\nsensor perception\",\"768\":\"fabrics\\nestimation\\nrobots\\ngrippers\\nmaterial properties\\nclothing\\nmeasurement by laser beam\\nindustrial robots\\niterative methods\\nlaser ranging\\noptimisation\\nautomatic material properties estimation\\nphysics-based robotic garment folding\\nfabric material property\\niterative strategy\\noptimisation task\\nlaser range finder\",\"769\":\"force\\nrobot sensing systems\\ngrippers\\nfriction\\ntask analysis\\ndynamics\\ndeformation\\ndexterous manipulators\\nforce measurement\\nforce sensors\\nmechanical contact\\ntactile sensors\\ntorque measurement\\nobject manipulation\\nsensorized parallel grippers\\nparallel jaw grippers\\nin-hand manipulation tasks\\ncontrolled sliding motion\\ngrasped object\\nrotational sliding maneuver\\ngrip force\\ntranslational sliding\\nrotational slippage\\nlinear slippage\\nfragile objects\\ndeformable objects\\ncontrolled rotational sliding\\nin-hand manipulation actions\\nsensorized gripper\\nsix-axis force\\/tactile sensor\\ncontact force\\ntorque measurements\\nslipping control algorithms\\nin-hand manipulation action\",\"770\":\"task analysis\\nsemantics\\nrobot programming\\nestimation\\nplanning\\ndetectors\\ngraph theory\\nimage colour analysis\\nlearning (artificial intelligence)\\nmanipulators\\npath planning\\npose estimation\\nrobot vision\\nobject geometries\\nsemantic robot programming\\ntask planning\\nmotion planning\\ndiscriminatively-informed generative estimation of scenes and transforms\\ndigest method\\nrgbd images\\ngoal-directed manipulation\\ncluttered scene dataset\\nmichigan progress fetch robot\\nobject poses\\nrobot manipulator\\nsrp\\nsemantic mapping\",\"771\":\"laser radar\\nsoftware\\nthree-dimensional displays\\nrobot sensing systems\\ntraining\\ngeophysics computing\\nmobile robots\\noptical radar\\nremote sensing by laser beam\\nvegetation\\nperception algorithms\\ngeometric terrain representation\\nlidar rays\\noff-road lidar simulation\\ntrees\\nshrubs\\ndata logs\\noff-road environments\\nstate estimation algorithms\\nhigh-fidelity sensor-realistic simulation\\nscale off-road robot applications\\ndata-driven terrain primitives\\nlidar observations\\nnatural terrain\",\"772\":\"marine vehicles\\nnavigation\\nartificial intelligence\\nnoise measurement\\nplanning\\npath planning\\ndatabases\\nautonomous underwater vehicles\\ncollision avoidance\\nmarine control\\nmarine navigation\\nmobile robots\\nships\\nnavigation feature\\nhistorical data\\nnavigation planning\\ndata driven route generation\\nautonomous ship\\nautomated generation\\nautonomous surface vessel\\nrobotic surface vessel\\nhistorical automatic identification system data\\nais locations\\nnearest neighbour based path retrieval\\nship feature\\nais records\",\"773\":\"satellite navigation systems\\nmarine vehicles\\nsea ice\\nsonar navigation\\nacoustics\\nautonomous underwater vehicles\\nkalman filters\\nmarine navigation\\nmobile robots\\nnavigation\\nnonlinear filters\\noceanographic techniques\\nposition control\\nships\\nunder-ice navigation methods\\nextended kalman filter\\nsufficient satellite beacon separation\\nvehicle position\\nice velocities\\nvehicle trajectory\\nice survey\\nnavigation sensors\\nship\\nprecision vehicle\\nsatellite navigation beacons\\nprecision navigation capabilities\\nunder-ice robotic vehicles\\nmoving stationary sea ice\\nunderwater robotic vehicle navigation\\nvehicle navigation beneath moving sea ice\\nice-relative\\nsize 7.6 km\\nsize 1.2 km\",\"774\":\"robots\\npropellers\\naerodynamics\\nshock absorbers\\nbatteries\\ncontrol engineering computing\\nindustrial robots\\nmass production\\nmicrorobots\\nmotion control\\nmulti-robot systems\\nsoft robot\\nrandom exploration\\nterrestrial environments\\nunknown terrains\\nadequate locomotion strategy\\nfast exploration\\nobstacles negotiation\\nmass manufacturing\\nminimalistic design\\nroll\\nsoft cage\\nswarm operations\\nrandomly moving miniature robots\",\"775\":\"vehicle dynamics\\nhippocampus\\nattitude control\\nhydrodynamics\\nforce\\nmonitoring\\ndrones\\nautonomous underwater vehicles\\ncontrol system synthesis\\nmarine control\\nmicrorobots\\nmobile robots\\npendulums\\nrobust control\\nstability\\nsubmerged furuta pendulum\\nhippocampus microunderwater vehicle\\nfluid volumes\\ntightly constrained settings\\nagile vehicle dynamics\\nrobust attitude control scheme\\naerial drones\\nunderwater domain\\ncontrol method\\nmicrounderwater vehicle hydrobatics\\nsubmerged furuta pendulum stabilization\",\"776\":\"task analysis\\ntrajectory\\nkinematics\\nsatellite communication\\nexoskeletons\\nengines\\nrobustness\\nartificial satellites\\nend effectors\\nmanipulator kinematics\\nmobile robots\\nremotely operated vehicles\\nsatellite links\\ntelerobotics\\nunderwater vehicles\\nuvms\\nunderwater vehicle-manipulator system\\nsatellite link\\ntask-priority-based inverse kinematics algorithm\\nsatellite-based tele-operation\\neuropean project\\nunderwater intervention\\nremote control room\\nsatellite communication link\\ndexrov\\ncognitive engine\\ncommunication latency\\nend effector\\nsize 2017.0 inch\",\"777\":\"three-dimensional displays\\ncameras\\nsemantics\\nvehicle dynamics\\ndynamics\\nreal-time systems\\nheuristic algorithms\\nimage motion analysis\\nimage reconstruction\\nimage segmentation\\nmobile robots\\nmotion control\\nobject detection\\npath planning\\npose estimation\\nrobot vision\\nstereo image processing\\nrobust dense mapping\\nlarge-scale dynamic environments\\nstereo-based dense mapping algorithm\\nlarge-scale dynamic urban environments\\nstatic background\\nhigh-level mobile robotic tasks\\ncrowded environments\\ninstance-aware semantic segmentation\\nsparse scene flow\\nvisual odometry\\ndepth maps\\nstereo input\\nmap pruning technique\\nreconstruction accuracy\\nstationary objects\\nmoving objects detection\\ncamera poses estimation\\nfrequency 2.5 hz\",\"778\":\"trajectory\\nrobot sensing systems\\nsafety\\nunmanned aerial vehicles\\nreachability analysis\\nschedules\\nadaptive control\\nautonomous aerial vehicles\\ncollision avoidance\\nenergy consumption\\nhelicopters\\nmobile robots\\nnoise\\nrisk analysis\\nscheduling\\ntrajectory optimisation (aerospace)\\nobstacles avoidance\\ntrajectory curvature\\nself-triggered adaptive planning\\nquadrotor uav motion planning\\nobstacles detection\\ntime consumption\\nconstant periodic sensor measurements\\nonline speed adaptation policy\\nrisk-based analysis\",\"779\":\"task analysis\\nadaptation models\\nautomobiles\\nmanifolds\\nbicycles\\nlearning (artificial intelligence)\\ncomplexity theory\\ngeneralisation (artificial intelligence)\\ntarget apprentice learning\\ncross-domain transfer\\nreinforcement learning\\ncross-domain tasks\\ntarget task learning\\ntarget domain\\npolicy augmentation\",\"780\":\"planning\\nprediction algorithms\\nautomata\\nvehicles\\nhistory\\ncomputational modeling\\naerospace robotics\\ncontrol engineering computing\\ndecision theory\\nfunction approximation\\nlearning (artificial intelligence)\\nmarkov processes\\nmulti-agent systems\\nplanning (artificial intelligence)\\nrobot dynamics\\nlow-level planning algorithms\\nintent-aware multiagent reinforcement learning\\nlearning algorithm\\nplanning process\\npartially observable markov decision process\\nlinear function approximation\\nintent-aware multiagent planning\\naerial robots\\nhuman interaction\\ndynamic process\\npomdp\",\"781\":\"perturbation methods\\nhip\\nadaptation models\\nrobots\\ntraining\\nlips\\nlearning (artificial intelligence)\\nhumanoid robots\\noptimal control\\nsampling methods\\ncontrol signals\\nadaptive sampling\\nmodel-based balance controllers\\nhumanoid model\\nin-place balancing\\ntraining disturbances\\nstandard reinforcement learning formulations\\ndeep reinforcement learning techniques\\ncontrol policy\\nroa\\nmodel-based optimal controller\\nlearning framework\\nfull-body actions\\nnonplanar pushes\\nfull-body dynamics\\noptimal control theory\",\"782\":\"planning\\noptimization\\nprediction algorithms\\ntask analysis\\npredictive models\\ncomputer architecture\\nuncertainty\\ndecision making\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nsupervised autonomous exploration\\noffice environments\\nexploration region selection\\nautonomous robot exploration task\\ngreedy methods\\nlong-term planning\\ndeep reinforcement learning\\nexploration knowledge\\noffice blueprints\\ndrl model\\nnext-best-view selection approach\\nstructural integrity measurement\\noffice maps\\ndecision making process\",\"783\":\"optimization\\nbayes methods\\nlegged locomotion\\ntask analysis\\npredictive models\\ncomputational modeling\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\nmaximum likelihood estimation\\nmotion control\\noptimisation\\nsearch problems\\nmost likely expected improvement\\n5dof planar arm\\n6-legged robot\\ntransfer learning task\\nacquisition function\\ndata-efficient direct policy search\\nautomatic prior selection\\nbayesian optimization\",\"784\":\"task analysis\\npredictive models\\nneural networks\\ndata models\\nheuristic algorithms\\nmachine learning\\ncomplexity theory\\ncomputational complexity\\nlearning (artificial intelligence)\\nneural nets\\npredictive control\\nmodel-free learning\\nmodel-free fine-tuning\\nmodel-free deep reinforcement learning algorithms\\nmodel-based algorithms\\nmodel predictive control\\nmodel-based reinforcement learning algorithm\\ncomplex locomotion tasks\\ndeep neural network dynamics models\\nmodel-free learner\\nmodel-based approaches\\nmodel-free methods\\nsample complexity\\nmodel-based deep reinforcement learning\\nrobotic skills\\nmpc\\nplausible gaits\\nstable gaits\",\"785\":\"collision avoidance\\nrobot sensing systems\\nrobustness\\nrobotic assembly\\ntrajectory\\nbayes methods\\ngaussian processes\\nintelligent robots\\nlearning (artificial intelligence)\\noptimisation\\nservomotors\\nsignal classification\\niterative learning\\nonline collision detection\\nrobust robot system\\nuncertainty-tolerant motions\\ngaussian process learning\\nbayesian optimization\\nrobot motor currents\\nassembly process\\nmedium-sized productions\\nparameterized motions\",\"786\":\"splines (mathematics)\\ntrajectory\\noptimization\\nreal-time systems\\nprocess control\\nplanning\\ncomplexity theory\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nelasticity\\nhelicopters\\nmobile robots\\noptimal control\\npath planning\\npipes\\npredictive control\\nquadratic programming\\nrobot dynamics\\nrobot kinematics\\nrobot vision\\nsearch problems\\ntrajectory control\\nquadratically constrained quadratic programming problem\\nreceding horizon replanner design\\ntrajectory replanning\\ngrid structure\\ndynamically feasible time-parameterized trajectory\\nb-spline based kinodynamic search algorithm\\ngreedy search\\nb-spline parameterization\\nposition-only shortest path search\\nmonocular vision-based quadrotor\\nreplanning system\\nlocal control property\\nexpanded elastic tube\\noptimal control point placement\\neo approach\\nrbk search\\npost-optimization process\\nelastic optimization approach\",\"787\":\"collision avoidance\\ncharge coupled devices\\nplanning\\ncomputational modeling\\nmanipulators\\nmotion segmentation\\nmobile robots\\nmotion control\\npath planning\\nbisection continuous collision checking method\\nspherical joints\\ncontinuous collision detection method\\ntight motion bounds\\nsampling-based motion planning technique\\nbaxter research robot\\nminimum distance extension\\nobstacle minimum distance\\nrobotic systems\",\"788\":\"collision avoidance\\ncomputational modeling\\ntrajectory\\ncost function\\nmathematical model\\nvehicle dynamics\\npropulsion\\nmarine control\\npredictive control\\nmpc-based collision avoidance strategy\\nmarine vessel guidance systems\\ncolregs\\nmpc colav algorithm\\nsimulation-based model predictive control\",\"789\":\"collision avoidance\\nrobot kinematics\\nprediction algorithms\\nheuristic algorithms\\nrobot sensing systems\\ndynamics\\nmobile robots\\nvelocity control\\ntime horizon\\nobstacle avoidance algorithm\\ntwo-period velocity obstacle algorithm\\nhigh-speed obstacles avoidance\",\"790\":\"uncertainty\\nrobot sensing systems\\nplanning\\nthree-dimensional displays\\ncollision avoidance\\nhistory\\ncurrent measurement\\ncartography\\nmobile robots\\nnavigation\\nsensors\\nnanomap\\nuncertainty-aware proximity queries\\nlazy search\\nlocal 3d data\\nlocal 3d information\\nrobustly plan motions\\nlocal map structure\\nmapping approaches\\nglobal map fusion\\nmotion planner\\npose-uncertainty-aware local 3d geometric information\\nnoisy relative pose transforms\\ndepth sensor measurements\\nminimum-uncertainty view\\nmotion planning\\nfast 3d obstacle avoidance\\nmapping techniques\",\"791\":\"collision avoidance\\ntask analysis\\nmanipulators\\nforce\\ntwo dimensional displays\\nrobot sensing systems\\nforce control\\nvirtual contact points\\nforce-sensorless collision detection approach\\nvirtual contacts\\nvirtual instantaneous powers\\n2d collision detection tasks\\n3d collision detection tasks\\ndof spatial manipulator validate\\ncontact link\",\"792\":\"robot sensing systems\\nprobabilistic logic\\nobservers\\nsecurity\\nboolean functions\\nbinary decision diagrams\\ngraph theory\\nmobile robots\\nmulti-robot systems\\nprobability\\nnetworked multirobot systems\\nrobot interactions\\nexisting control-theoretic notion\\nnetwork attacks\\nleft invertibility\\ndynamical system\\nprobabilistic robot communication\\nadversarial influence\\nprobabilistic graph security problem\\nsystem reliability\\nefficient graphical representation\\nnetworked mrs\\nmobile multirobot teams\\nmobile mrs\\nreduced order bdd\",\"793\":\"robots\\ncouplings\\nmulti-robot systems\\ndynamics\\nforce\\ndamping\\ncollision avoidance\\nforce control\\nexternal entities\\ndynamic interaction model\\nmultirobot system\\ninteraction control\\nlocal deformations\\ncoupling actions\\npassivity property\\nsafety guarantees\",\"794\":\"trajectory\\nrobots\\nnetwork topology\\nacceleration\\naustralia\\nmathematical model\\natmospheric measurements\\ngraph theory\\ninference mechanisms\\nmulti-robot systems\\nnetwork theory (graphs)\\nswarm intelligence\\nnetwork topology inference\\nswarm robotics\\ntopological graph\",\"795\":\"shape\\ntask analysis\\nhardware\\nrobot sensing systems\\nlegged locomotion\\ncollision avoidance\\nmobile robots\\nmulti-robot systems\\nwork distribution\\nhardware specialization\\nclassical distributed robotics problem\\nrobotic swarms\\nsimulated environment\\nshape formation\",\"796\":\"task analysis\\nrobot kinematics\\nnavigation\\neigenvalues and eigenfunctions\\ncomputational modeling\\nmulti-robot systems\\ndistributed control\\nmobile robots\\noptimisation\\nscheduling\\ntask coverage\\nrobot swarms\\nconnectivity constraints\\nswarm robotics\\ncomplex tasks\\ncontrol algorithms\\nglobally coordinated behaviours\\nspatial coverage\\nglobal connectivity\\ndistributed robot navigation controller\\nrnc\\nglobal task scheduling controller\\nminimal computational load\\nconnectivity assessment\\nreal-life robot experiments\\ncoverage optimality\",\"797\":\"heuristic algorithms\\nrobot kinematics\\nrobot sensing systems\\ncollision avoidance\\nmulti-robot systems\\nnavigation\\nmobile robots\\nswarm intelligence\\nmultirobot algorithms\\nproximal neighbors\\nemergent collective behaviors\\nall-to-all communication\\ndeliberative collaboration\\nsupervisory operator\\nmission constraints\\napplication domains - navigation\\ndynamic area coverage\\nonline algorithm selection decisions\\noffline system design decisions\\nrobotic swarms\\ninformation invariants\",\"798\":\"robot sensing systems\\ntask analysis\\nlight sources\\nrobustness\\nkernel\\ntrajectory\\nhilbert spaces\\nlearning systems\\nmobile robots\\nmulti-robot systems\\nrobotic assembly\\nrobust control\\nsearch problems\\nswarm size\\nrobust policies learning\\nhilbert space embeddings\\npolicy search methods\\nlow-level object movement policy\\nhigh-level assembly plan\\nassembly process\\npolicy search method\\nautonomous object assembly\\nswarm robotics\\nrobot swarms\\nobject manipulation\",\"799\":\"gravity\\nsprings\\ntorque\\ndc motors\\nwires\\nrobots\\npayloads\\ncompensation\\nflexible manipulators\\nlinear systems\\nsprings (mechanical)\\nspring based mechanism\\nflexible link arm\\nsingle link flexible arm\\nlinear springs\\nflexible link robotics\\npassive gravity compensation\\nspring-based compensation\\nlumped-mass methodology\\nvibrational frequency\",\"800\":\"manipulator dynamics\\nmathematical model\\nconvergence\\nkinematics\\njacobian matrices\\nclosed loop systems\\ncontrol system synthesis\\nmanipulators\\nnonlinear control systems\\npd control\\nvariable structure systems\\nextensible continuum robots\\nclosed-loop control\\nadaptation-based control law\\nrigid-link control device\\nextensible continuum manipulator\\nnonlinear control strategy\\nset-point tracking\",\"801\":\"friction\\ntorque\\nrobots\\nactuators\\ntendons\\nmathematical model\\nclutches\\nenergy conservation\\ngears\\nmobile robots\\nmuscle\\ncontrollable series clutches\\nefficient robot actuation\\nenergy efficiency potential\\ncontinuously controllable clutches\\nrobot joints\\nbiological muscles\\npurely gravity driven robot link motion phases\\ngear friction\\nmotor effort\\nenergetic benefits\\ndirect drives\\nunforced motion phases\\nhigh torque density\\nconventional geared robotic drive technology\\nmature geared robotic drive technology\\nforced motion phases\\ngeneral functional principle\\nparticular clutch implementation\\nharmonic link motions\\nfriction torque\",\"802\":\"modulation\\nsprings\\nactuators\\nforce\\nmuscles\\nbiomechanics\\nelastic constants\\nmedical robotics\\nmuscle\\nportable instruments\\nsit-to-stand-task\\ncocontracted antagonistic muscles\\nmetabolic energy cost\\nself-contained stiffness modulator\\nmuscle activity\\ncompliant actuators\\nstiffness modulation\\nhuman augmentation\\nhuman knee joint\\nstiffness augmentation\\nportable stiffness modulator\",\"803\":\"task analysis\\nmanipulators\\ntools\\ngrippers\\nplanning\\nrobustness\\nindustrial manipulators\\nrobot vision\\nwarehouse automation\\nautonomous warehousing\\nrobotic vision\\nmanipulation\\ncartesian robot system cartman\\nexperience-centred design methodology\\nlow-cost cartesian manipulator\\namazon robotics challenge\\npick-and-place robot\",\"804\":\"grippers\\ngrasping\\ncameras\\ntactile sensors\\nforce\\nfeature extraction\\nforce control\\nimage capture\\nimage sequences\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\npattern classification\\nrobot vision\\nrobot arm\\ngrasping positions\\nslip detection\\nvisual information\\nrobotic manipulation\\ndeep neural network\\ngelsight tactile sensor\\ngrasping forces\\ndnn training\\ngrasp stability\\ntactile information\\ngripper\\ncamera-based tactile sensor\",\"805\":\"robot sensing systems\\nvisualization\\ncost function\\nstate estimation\\ncameras\\nend effectors\\nfeedback\\nobject detection\\npose estimation\\nposition control\\nrobot vision\\nslam (robots)\\ntouch (physiological)\\ntactile input\\nvisual input\\nincremental smoothing\\nvisual sensing\\ncontact sensing\\nend-effector\\nrealtime state estimation\\nrobust object state estimation\\nvisual sensor\\nvisual feedback\\nobject shapes\\nobject manipulation\\nincremental smoothing and mapping\\nisam\\nplanar manipulation\\nobject poses estimation\",\"806\":\"robot sensing systems\\nrobot kinematics\\nacceleration\\nwrist\\ntask analysis\\nactuators\\ndexterous manipulators\\nend effectors\\ngrippers\\nhuman-robot interaction\\nmobile robots\\ntactile sensors\\ngrasp response\\nhand adaptability\\nhuman inspiration\\nautonomous grasp sensory-motor primitives\\nsimple touch-based approach\\nsoft end effectors\\nsoft robotic hands\\nadaptable hands\\nhuman-to-robot handover tasks\\ntouch-based grasp primitives\\nsoft robotic manipulation\\nhuman maneuvering\\nhuman wrist\\nhand closure commands\\narm motions\\ngrasping\\ncontact\\nrobotic arm\\nunder-actuated soft anthropomorphic robotic hand\\npisa\\/iit softhand\",\"807\":\"navigation\\nplanning\\ntrajectory\\nvisualization\\nentropy\\npredictive models\\nlearning (artificial intelligence)\\ndecision theory\\ngraph theory\\nmarkov processes\\nmobile robots\\npath planning\\nprobability\\nsearch problems\\ncontrol problem\\nsingle follower robot\\nvisual contact\\nmoving target\\nplausible predictions\\ndiscrete hypotheses\\ncombinatorial search\\nphysical space\\nmodel target behavior\\nlearned navigation reward function\\nsemantic terrain features\\nsearch methods\\npredictive pursuit algorithm\\nmultiple satellite maps\\nsimulation scenarios\\ninverse reinforcement learning\\nlong term behavior\\nshort term behavior\\nplanning pursuit paths\\nlocations\\ngraph representation\\nlatent destination\\nposition\\npomdp solvers\\ndomain specific knowledge\\nmodel based probabilistic pursuit\",\"808\":\"unmanned aerial vehicles\\nsociology\\nstatistics\\nglobal positioning system\\nrobots\\nmonitoring\\nwind tunnels\\nautonomous aerial vehicles\\ndiseases\\ngraph theory\\ninteger programming\\npath planning\\ntravelling salesman problems\\ndestructive surveys\\nmosquito population\\nelectrified screen\\nuav path\\nmosquito elimination\\ntrajectory planning\\ntraveling salesman problem\\nmilling with turn cost\\nlawn mower problem\\ngrid graph\\noptimized energy consumption\\nmosquito-borne diseases\\nmosquito-killing uav\",\"809\":\"robot sensing systems\\ncurrent\\noptical fiber sensors\\noptical fibers\\ncurrent measurement\\nfiber gratings\\nautonomous aerial vehicles\\ndiffraction gratings\\nelectric current measurement\\nfibre optic sensors\\npermanent magnets\\nrobot operating system package\\nhysteresis\\noptical fiber-based sensor\\nsensing technology\\nunmanned aerial vehicles electric motors\\nros interface\\nsensing system\\nelectric current measurements\\nlinear electric current sensitivity\\nflexible sensing scheme\\npermanent neodymium magnet\\nlong-period fiber grating sensor\\ncurrent 0.22 a\\ncurrent 0.08 a\",\"810\":\"wind speed\\nrobot sensing systems\\nwind tunnels\\nfabrication\\nmarket research\\nhair\\naerodynamics\\naerospace components\\naircraft testing\\nautonomous aerial vehicles\\ndesign engineering\\nelastomers\\nflow measurement\\nflow sensors\\nhelicopters\\nturbulence\\nwinds speeds\\ncrazyflie 2.0 quadcopter\\nturbulent flow\\nthrust level\\nquadcopter ground effect sensing\\ncompliant contact-resistance-based airflow sensor\\nlightweight contact-resistance-based airflow sensor\\nwind tunnel characterization\\nsensor deflection\\nair flow speeds\\nflexible conductive pillar\\nelastomeric contact-resistance-based airflow sensor\\nnonobstructive solutions\\npower 42.0 muw\",\"811\":\"cameras\\nnavigation\\nrobot vision systems\\nlaser radar\\npayloads\\naircraft control\\naircraft navigation\\nautonomous aerial vehicles\\ncollision avoidance\\nhelicopters\\nmobile robots\\nmixed indoor environments\\noutdoor environments\\nspecific component technologies\\nhigh speed navigation capability\\nhigh speed autonomous flights\\nobstacle rich environments\\ngps-denied quadrotor flight\\nunknown environments\\nrobotics\\nfast computation\\ntight integration\\nsubsystems\\nlatency\\nperception-action loop\\naerial robots\\npayload capacity\\nnavigation system\\nquadrotor system\",\"812\":\"collision avoidance\\nrobot sensing systems\\nestimation\\ncameras\\nrobot kinematics\\nautonomous aerial vehicles\\ndistance measurement\\nfeedback\\nglobal positioning system\\nhelicopters\\nimage sequences\\nkalman filters\\nmobile robots\\npath planning\\nprobability\\nsensors\\nstate estimation\\ntelerobotics\\ntracking\\non-board state-estimation\\nindoor obstacle avoidance\\nunmanned aerial vehicles\\ngps signal\\nteleoperated quadrotor uav platform\\nonboard miniature computer\\nlinear velocity\\nkalman filter integration\\ninertial flow\\noptical flow\\ndepth measurements\\nrobo-centric obstacle model\\ncollision-free navigation\\ndistance measurements\\ncramped spaces\\nrgb-d camera\\nvisual feedback\\nprobabilistic\",\"813\":\"safety\\ntrajectory\\ncollision avoidance\\nvehicle dynamics\\ndynamics\\nrobots\\ntask analysis\\naircraft control\\nautonomous aerial vehicles\\nhelicopters\\nhuman-robot interaction\\nlyapunov methods\\nquadratic programming\\nassistive training solution\\nsafe human teleoperated flight\\ncontrol approach\\nmotion capture environment\\nsafe teleoperation\\ncontrol barrier functions\\nhuman operators\\nhighly dynamic systems\\nconstrained environment\\nquadrotor systems\\npotential obstacles\\npresented supervisory controller\\nsafety constraints\\ndynamic uav\\nexponential control barrier function\"},\"Benchmark Setup\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1},\"Experimental Results\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1},\"Code Link\":{\"0\":null,\"1\":null,\"2\":null,\"3\":null,\"4\":null,\"5\":null,\"6\":null,\"7\":\"'We design the multiple SSL output coding as the likelihood of a sound source being in each direction. Specifically, the output is encoded into a vector\\\\n{\\\\no\\\\ni\\\\n}\\\\nof 360 values, each of which is associated with an individual azimuth direction\\\\n\\u03b8\\\\ni\\\\n. The values are defined as the maximum of Gaussian-like functions centered around the true DOAs:'\\n\\n'During the test phase, we decode the output by finding the peaks that are above a given threshold\\\\n\\u03be\\\\n:'\",\"8\":null,\"9\":null,\"10\":null,\"11\":\"'To overcome this cumulative drift, iMag uses MI measurements to detect loop closures and hence correct longterm errors in inertial odometry. An MI transmitter consists of three orthogonal coils which are tuned to resonance and are driven with a coded binary phase shift keying (BPSK) message. This code is chosen to have good cross-correlation properties, to increase the range of detectability. The MI receiver similarly consists of three orthogonal sensors (again coils in our case), which are connected to low noise amplifiers and a wide dynamic range ADC. The wide dynamic range is necessary to handle the high path-loss exponent - due to the near-field coupling, the field roll-off is 60 dB\\/decade, rather than the more typically encountered 40 dB\\/decade for electromagnetic propagation. The triaxial signals are then cross-correlated with the template code in order to estimate the channel matrix.'\",\"12\":null,\"13\":\"'Distance measurement precision and accuracy was characterized within the measurement window of 0 mm to 1.5 mm, corresponding to a measurement range of 1.5 mm ahead of the needle tip. The instrument handle is fixated vertically to a precision single axis spindle drive (LX 26 series, Misumi) in combination with a position controlled DC-motor and encoder combination (RE30, Maxon Motors). This spindle and motor combination provides a positional resolution of\\\\n1 \\u03bcm\\\\n. A sinusoidal cyclic loading pattern with a period of 30 s providing a translation interval between 0 and 1.5 mm is applied for a duration of 10 minutes. Translational speed is limited to\\\\n500 \\u03bcm\\/s\\\\n. Figure 6(d) shows the relationship between the applied and calculated displacement during the application of a cyclic load as described above. It is shown that the sensor in combination with the implemented distance sensing algorithm offers high repeatability and minimal hysteresis within the operated range. Figure 6(e) shows the residual error between the applied and calculated displacement in function of the applied displacement. Minor nonlinear behaviour can be observed. The histogram depicted in Figure 6(f) shows that the 5th and 95th percentile points are respectively \\u22120.021 mm and 0.043 mm, indicating that 90 percent of the samples remain within a range of 0.064 mm of the applied reference. Mean error shows a deviation of 0.010 mm from the applied reference across the measurement range.'\\n\\n'The instrument handle is fixated vertically to a precision single axis spindle drive (LX 20 series, Misumi) in combination with a position controlled DC-motor and encoder combination (EC30, Maxon Motors). This spindle and motor combination provides a positional resolution of\\\\n0.25 \\u03bcm\\\\n. A sinusoidal cyclic loading pattern with a period of 30 s providing a translation interval between 0 and 1.5 mm was applied for a duration of 10 minutes. Translational speed was limited to\\\\n500 \\u03bcm\\/s\\\\n. Four ex-vivo enucleated porcine eyes were used in two different orientations, providing 8 datasets. Figure 9 shows the outcomes of the ex-vivo experimental validation of the distance estimation algorithm. For each dataset, a boxplot of the residual error between the applied and calculated displacement is shown. The characterisation dataset is shown for relative comparison. The mean precision (5\\u201395 percentile range) across all ex-vivo datasets is determined to be 0.239 mm, as opposed to the 0.064 mm precision achieved during characterisation. It is concluded that the implemented algorithm suffers a factor 3.7 drop in precision when implemented ex vivo.'\",\"14\":null,\"15\":null,\"16\":null,\"17\":null,\"18\":null,\"19\":null,\"20\":null,\"21\":\"'If the initial state is in free space, we begin by searching for the best feedback target. This is the only relatively computationally-intensive step of the procedure, but is rarely required after the first step. Once this first feedback target is reached, we simply take the optimal action encoded in the closest samples to the current state until the goal is reached. Feedback targets can also be fed to external controllers to achieve, as is done with the Baxter experiments in Section V-C.'\",\"22\":null,\"23\":\"'https:\\/\\/robot-learning.cs.utah.edu\\/project\\/in_hand_manipulation'\",\"24\":null,\"25\":\"'https:\\/\\/www.dropbox.com\\/s\\/0s2711n8i186qoa\\/ICRA_Hogan_Grau_Rodriguez_2018.mp4?dl=0'\",\"26\":null,\"27\":null,\"28\":\"'https:\\/\\/youtu.be\\/qrocpjaw5ny'\",\"29\":null,\"30\":\"'https:\\/\\/youtu.be\\/Ao-0W9chAd4'\\n\\n'https:\\/\\/youtu.be\\/Ao-0W9chAd4'\",\"31\":null,\"32\":null,\"33\":null,\"34\":null,\"35\":\"'https:\\/\\/www.youtube.com\\/watch?v=dn6pxl3gqey&t=1s'\\n\\n'https:\\/\\/github.com\\/hkust-aerial-robotics\\/mockamap'\\n\\n'In this paper, we propose a framework for online quadrotor motion planning for autonomous navigation in unknown environments. Based on the onboard state estimation and environment perception, we adopt a fast marching-based path searching method to find a path on a velocity field induced by the Euclidean signed distance field (ESDF) of the map, to achieve better time allocation. We generate a flight corridor for the quadrotor to travel through by inflating the path against the environment. We represent the trajectory as piecewise B\\u00e9zier curves by using Bernstein polynomial basis and formulate the trajectory generation problem as typical convex programs. By using B\\u00e9zier curves, we are able to bound positions and higher order dynamics of the trajectory entirely within safe regions. The proposed motion planning method is integrated into a customized light-weight quadrotor platform and is validated by presenting fully autonomous navigation in unknown cluttered indoor and outdoor environments. We also release our code for trajectory generation as an open-source package.'\\n\\n'Real-time implementation of the proposed motion planning method and system integration in a complete quadrotor platform. Autonomous navigation flights in unknown indoor and outdoor complex environments are presented and the source code will be released.'\\n\\n'Autonomous flights in indoor unknown environments. Two tests in different environments are given in figs. 9(a) and 9(b). The color code indicates the height of the obstacles. The red curve is the (re)generated trajectories and the green curve is the path that the quadrotor tracked. The re-plan mechanism is triggered once new obstacles are detected, as is shown in the medium of the figures. The complete map and trajectory are shown on the right side of the figures. More indoor trials of our proposed method are presented in the video.'\\n\\n'https:\\/\\/github.com\\/hkust-aerial-robotics\\/mockamap'\",\"36\":null,\"37\":null,\"38\":null,\"39\":\"'We replicated the simulation on a hardware testbed using the Crazyflie 2.0 open source quadrotor platform, shown in Fig. 8. We obtained position and orientation measurements at \\u223c 235 Hz from an OptiTrack infrared motion capture system. Given state estimates, we send control signals over a radio to the quadrotor at 100 Hz. As shown in our accompanying video,7 the quadrotor successfully avoids the obstacles while remaining inside the TEB for each planner the meta-plan.'\",\"40\":null,\"41\":\"'The main contribution of the paper is as follows: First, the disturbance from the modeling errors and unexpected contact forces can be observed using our DOB based estimator equipped with the encoders and motor model. Thus, our system can estimate the disturbance by using the motor model, even if the measured elastic deformation is noisy. Furthermore, no additional sensor such as a force\\/torque sensor is required for the proposed framework. Second, the DOB based linear feedback controller can not only minimize vibration but also ensure compliant motion of each joint. This controller moves each joint of the robot in the direction of estimated disturbance. Finally, the proposed algorithm is applied to the human-sized humanoid robot and its performance was demonstrated by experiments. We could ensure compliant motion for stable walking of the humanoid robot despite unexpected contact forces between its foot and other objects.'\\n\\n\\\"The estimator discussed in this paper does not involve a force\\/torque sensor, but it is designed using information from the motor's encoders. More precisely, the disturbance from modeling errors and external forces in the treatment of (4) could be estimated by designing the DOB based estimator for each joint as shown in Fig. 3, where\\\\nP(s),\\\\nP\\\\nn\\\\n(s),Q(s)\\\\nand\\\\nK\\\\ndist\\\\ndenote the real system, the nominal system, the first order low pass filter and the weighting gain, respectively.\\\"\",\"42\":null,\"43\":null,\"44\":null,\"45\":null,\"46\":null,\"47\":null,\"48\":\"'We used the open-source Finite Element Method Magnetics (FEMM) software to obtain estimates of the output force as a function of inductor current for the two actuator configurations for several commercially available inductor families. Fig. 4 shows representative simulations of the EM and PM configurations based around identical bobbin-type ferrite inductors from Coilcraft (LPS3010 series with removed shielding).'\",\"49\":null,\"50\":null,\"51\":\"\\\"The force-feedback control system used the Robotic Systems Integration RMP EtherCAT. Advanced Motion Controls DZEANTU-020B080-2A motor drivers for the EtherCAT regulated motor currents using PD control. The force sensor data and the motor current were updated at a rate of approximately 1 kHz by the EtherCAT system with a nominal latency of 1 ms. CHAI3D was used to render the virtual environment. Interaction forces were computed from the user's position measured by the Omega.3. Desired motor torques for the skin deformation device were calculated from the desired force of the end-effector through the forward kinematics of the delta mechanism, and then translated into motor currents. Motor optical encoders measured the joint angles for each of the joints in the delta mechanism. The force output by the end-effector of the skin deformation device was measured by the 3-axis force sensor. PID control was used to control the force of the tactile device end-effector.\\\"\",\"52\":null,\"53\":\"'However, the previously described evolution would need any involved robot programmer to implement a comparable software architecture within native robot code. Considering the door-handle insertion task, the ability to combine several turning processes with different rotational axes cannot be considered out-of-the-box and would be a sophisticated implementation challenge.'\",\"54\":\"'https:\\/\\/github.com\\/watson-intu\\/self'\\n\\n'http:\\/\\/ibm.biz\\/MaestROB'\\n\\n'http:\\/\\/ibm.biz\\/MaestROB'\\n\\n'While different sensors are used by MaestROB, in this section we will focus on the vision sensor (camera). Camera is used for recognizing a human in front of the robot, estimating the age and gender, recognizing objects and their poses etc. For accurate visual perception in industrial settings, we assume that an object database provides the properties (shape, size etc.) of all the objects of interest. Visual perception computes the pose for each instance of the object in the visible world. For the demonstration, we have used a barcode based pose estimation technique proposed by H. Kato [11].'\\n\\n'The demo starts with a human performing the task and having conversation with Pepper robot at the same time. Understanding the intent of a command by using conversation service, the robot can understand when to capture key frames and when the demonstration is over. In the example, the robot records the initial state and the final state of the demonstration. From the conversation, it remembers the name of the task it is learning to perform (peg assembly task). It also understands that the final state is the last frame of demonstration. The initial and final frame are sent to perception service that uses barcode pose detection to detect the location of all the barcodes. The barcode number of each object and the transformation between the barcodes and the objects are defined separately in the object database. The state of the final frame is determined by the relationship extractor. As the domain for the task is predefined to be insertion, the appropriate ontologies and the relationship database are loaded.'\\n\\n\\\"In this scenario, the goal is not used by Pepper to compute a plan for itself, rather Pepper implements the scenario on UR5. Pepper uses predefined locations of the demo table and UR5 to move from one place to the other. When Pepper arrives at UR5, it captures an image of the initial state. This image is used to calibrate Pepper's camera w.r.t the position of UR5 robot, the barcode pose location for all the objects are also computed in UR5 coordinate reference system. Pepper uses the initial state observed from the image and the goal state learned from the human demonstration to generate an executable plan for the manipulator robot. However, before this can be done, the skill database of UR5 is shared with Pepper. The common sense ontology is used by the planner to check if an operation is permitted or not. For example, it is not permitted to insert a big peg into a smaller hole. The plan is then transmitted to UR5; it starts performing the task based on position control. Once the task succeeds Pepper can return while UR5 continues to perform the task. In this demo we have a human helper who put the pegs back to the initial state before every UR5 iteration. In a factory environment this is usually done by conveyer belts or other machines.\\\"\\n\\n'This paper describes a framework called MaestROBe It is designed to make the robots perform complex tasks with high precision by simple high-level instructions given by natural language or demonstration. To realize this, it handles a hierarchical structure by using the knowledge stored in the forms of ontology and rules for bridging among different levels of instructions. Accordingly, the framework has multiple layers of processing components; perception and actuation control at the low level, symbolic planner and Watson APIs for cognitive capabilities and semantic understanding, and orchestration of these components by a new open source robot middleware called Project Intu at its core. We show how this framework can be used in a complex scenario where multiple actors (human, a communication robot, and an industrial robot) collaborate to perform a common industrial task. Human teaches an assembly task to Pepper (a humanoid robot from SoftBank Robotics) using natural language conversation and demonstration. Our framework helps Pepper perceive the human demonstration and generate a sequence of actions for UR5 (collaborative robot arm from Universal Robots), which ultimately performs the assembly (e.g. insertion) task.'\\n\\n'To achieve this higher level of cognitive capabilities, in this paper we present a robotics framework \\u2013 MaestROB. Different components of MaestROB communicate through a novel robotics middleware named Project Intu. Intu is provided as an open source project at https:\\/\\/github.com\\/watson-intu\\/self. Intu uses IBM Watson APIs [1] to provide a seamless access to many services including conversation, image recognition etc. With Intu at its core, MaestROB introduces a hierarchical structure to planning by defining several levels of instructions. By using the knowledge and ontology of physical constraints and relationships, these abstractions allow the grounding of human instructions to actionable commands. The framework performs symbolic reasoning at higher level, which is important for long term autonomy and to make the whole system accountable for its actions. Individual skills are allowed to use machine learning or rule based systems. We provide a mechanism to extend the framework by developing new services or connecting it with other robot middlewares and scripting languages. In MaestROB the primitive intelligence of each component is orchestrated to demonstrate complex behaviors. Important features of MaestROB include but are not limited to a skill acquisition and cloud based sharing service, task planning with physical reasoning, perception service, ability to learn from demonstration, multi-robot collaboration, and communication by using natural language.'\\n\\n'Project Intu is provided as an open source platform for embodied cognition1. It is based on a cognitive architecture called Self. Self is an agent-based architecture that combines connectionist and symbolic models of computation, using blackboards for opportunistic collaboration. Project Intu provides a mechanism for connecting and orchestrating cognitive services in a manner that brings higher level cognition to an embodied system.'\\n\\n'We present a framework to support the next generation of robots to help solve the problems that are not solvable by conventional programming methods. The robot middleware (Project Intu) presented in this paper is now available as an open source project. We also presented several key services that enable us to demonstrate sophisticated scenarios involving collaboration between multiple robots and human. MaestROB is especially useful for small and medium-sized enterprises (SMEs), which need relatively quick time to market, frequent changes in manufacturing lines and have low production volumes. The workers can communicate with the robot in natural language and teach it new skills or execute existing skills.'\\n\\n'https:\\/\\/github.com\\/watson-intu\\/self'\\n\\n'Open source version of Intu comes with many components that allow a seamless access to IBM Watson services [1]. Intu is devised to be applicable to a multitude of use cases, from avatar to concierge to retail to elder care to industrial robots. It is available for a number of platforms, including Linux, Windows, macOS, Nao, Pepper, and Raspberry Pi.'\",\"55\":\"\\\"These software abstractions hide the hardware layer from software developers on other layers. With such abstractions, end users do not need to consider hardware particularities and signal sources. They can use the robots' signals directly, facilitating code development and integration in a centralized application. However, many developments are made as decentralized applications due to the expertise of researchers and the integration of different programs and libraries. This causes difficulty when integrating all of them in a single centralized application.\\\"\\n\\n\\\"This generates a proper user-developer interaction since both users know beforehand the way they should handle input\\/output data. To encapsulate the data we implement two modules, the robot, and the control states. The first one centralizes the robot's information including measured data from encoders, IMU data, etc, as well as estimated data from external modules. The second module centralizes the controller's references and efforts. The two modules are decoupled such that the final user can consider the controller's information according to the particular needs.\\\"\\n\\n'The robot state is a class that contains a set of get\\/set methods. The data are stored in maps of vectors with the corresponding string key identifier. Four different maps of data are used through the code:'\\n\\n'To have a wide range functionalities, additional features have been considered. This way we got a flexible use of different controllers while keeping the code of the users clean. These functionalities also minimize the code integration effort. The additional functionalities that were included are:'\\n\\n'Below we show an example of the final user code when implementing the attitude controller. It shows that the code is robot independent and the migrations from robot to robot is transparent, since it depends only on the data encapsulation, config files names and required control signals. This controller was implemented in three different robots using the same attitude controller with a slight change in the way IMU data and pelvis reference are considered.'\\n\\n\\\"By using Ctrl-MORE the effort during the integration of the desired controllers is reduced, and allows the user to perform actions such as: use cascade control strategies, use individual controllers or modify various controller simultaneously. This way, the performance of the user's module increase once additional control strategies are used and active cooperation simplify. The open source files are available at1\\\"\",\"56\":\"'http:\\/\\/www.st.com\\/'\\n\\n'https:\\/\\/3dr.com\\/'\\n\\n\\\"As a rapidly growing cyber-physical platform, unmanned aerial vehicles are facing more security threats as their capabilities and applications continue to expand. Adversaries with detailed knowledge about the vehicle could orchestrate sophisticated attacks that are not easily detected or handled by the vehicle's control system. In this work, we purpose a generic security framework, termed BlueBox, capable of detecting and handling a variety of cyber-physical attacks. To demonstrate an application of BlueBox in practice, we retrofitted an off-the-shelf quadcopter. A series of attacks were then launched by embedding malicious code in the control software and by altering the vehicle's hardware with the specific targeting of sensors, controller, motors, vehicle dynamics, and operating system. Experimental results verified that BlueBox was capable of both detecting a variety of cyber-physical attacks, while also providing the means in which to recover from such attacks.\\\"\\n\\n\\\"In this work, we propose a generic security framework called BlueBox, which can be retrofitted to a range of multi-rotor UAVs. Instead of changing the original control system, an external piece of hardware was used to monitor the vehicle with minimum modification to the original system. From vehicle source code or through binary reverse engineering [19], [20], the vehicle's control logic can be extracted. Independently implementing the control and sensing logic on the external hardware enables high-accuracy error detection. A smooth variable structure filter (SVSF) was used to estimate system states and identify system parameters, which is proven to be robust to model uncertainty and noise. Combining the software and hardware redundancy, the framework was able to detect a variety of attacks on the sensor, controller, vehicle dynamics, actuator, and controller operating system. The vehicle was able to fully recover to normal operation once the attacks were detected. We implemented this framework on a 3DR IRIS+ quadcopter running ArduPilot 3.3 on NuttX operating system. Five different types of attacks were tested on the system and the attacks were identified and nullified.\\\"\\n\\n'Refers to the information trustworthiness in the system. For UAVs, various methods like trojan code or exploit subroutines of the software, jamming\\/capturing\\/editing signals can compromise the data integrity, with or without compromising the confidentiality first. With various detection mechanisms already available, this type of attack is hard to launch without sufficient knowledge of the target system, but will cause the most significant damage to the vehicle if successful. Loss of integrity can lead to the vehicle being affected or even controlled by the attacker while the operator receives false or deceptive information.'\\n\\n\\\"BlueBox's detection mechanism is described as follows. At vehicle takeoff, the estimator will update the vehicle's physical parameters. After a short period of time (<30 seconds in our testing), the vehicle can proceed its mission and the BlueBox starts fault\\/attack detection. As the foundation of a feedback control system, safeguard of feedback is critical. Based on binary code reverse engineering and Platform Independent Executable Trace technology [20], we can infer the control and sensing algorithms from the original system and independently implement them for BlueBox. Through a direct wiring from the sensor, BlueBox extracts sensors raw data for reference states calculation. It is used to be compared with the feedback from original system to determine if the sensor fusion result or code in the original system has been modified. The decision engine will integrate the error between the two measurements within a fixed time window and identifies sensor attacks using thresholding. BlueBox sensor fusion results are also fed to its internal controller and SVSF based estimator for further security diagnosis and attacks detection. To increase the level of security, extra redundant sensors can be added and sensor hardware failure can be detected [6].\\\"\",\"57\":\"'Identify the essential elements that are needed as the basic building blocks to compose complex assembly skills, separating what has to be coded from what can be modeled. Avoid \\u2018glue code\\u2019 and extensive \\u2018copy-pasting\\u2019 for filling out code-generated templates.'\\n\\n'codels'\",\"58\":\"'This paper exemplifies the design process for legged machines capable of dynamic behaviors. In order to achieve high performance robots, it is crucial to guarantee harmonious integration between software and hardware. Hence, the development of such capable robotic platforms must address design requirements that meet the assumptions of typical model-based controllers but also respect the physical limitations of a real system. First, we show that proper hardware design choices can greatly aid the control algorithm by approximating the physical robot to the template assumptions. We include actuation and sensing design examples that allows a simple model to capture a major portion of the natural dynamic behavior of the physical machine. Results are applied to a real robot (Figure 1) and we show that the adopted methodology is able to address typical problems in legged robots such as high bandwidth force control and robustness to impact. Finally, a simple model-based balance controller that takes advantage of the fidelity of the template model to the real machine is implemented. These are examples of software-hardware codesign processes that vastly facilitate robotic control.'\",\"59\":null,\"60\":null,\"61\":null,\"62\":\"'https:\\/\\/unl.box.com\\/v\\/ICRA-duncan-videos'\",\"63\":\"\\\"In this work, we formulate the patient's motion as a Markov Decision Process, where each state represents a pose, and the negative of its reward value encodes all the factors affecting the patient's cost to visit this state, including the invariant factors, the physician's instructions, and the spinal stimulations. We then adopt inverse reinforcement learning (IRL) algorithms to estimate the reward value of each state from the observed motion of the patient.\\\"\",\"64\":\"'We consider planning the path from a start to a goal in a robotic roadmap encoded as a graph. A user specification is given as a set of constraints, however without an explicitly defined weight. We want to minimize a cost that captures time and constraint violation. To learn the importance of a constraint relative to the time saved by violating it, we can ask for user feedback on alternative paths.'\",\"65\":null,\"66\":\"\\\"Visualization of a set of trajectories produced by the tracker over 15 frames. Trajectories are color coded, such that having the same color means it's the same object.\\\"\",\"67\":null,\"68\":\"'http:\\/\\/www.dabi.temple.edu\\/~hbling\\/data\\/POT-210\\/planar_benchmark.html'\",\"69\":\"'http:\\/\\/www.dabi.temple.cdu\\/~hbling\\/code\\/ccm\\/index.html'\\n\\n'http:\\/\\/www.dabi.temple.cdu\\/~hbling\\/code\\/ccm\\/index.html'\",\"70\":\"'Experiments were conducted using a Barrett WAM manipulator, a cable-actuated robotic arm. Data was captured from raw joint-encoder traces and a statically-positioned camera, collecting\\\\n640\\u00d7480\\\\nRGB frames at 30 fps using a PrimeSense camera (the infra-red depth modality was not used for this study). Due to non-linear effects arising from joint flexibility and cable stretch, large joint velocities and accelerations induce discrepancies between recorded joint values and actual positions seen in the images [27]. In order to mitigate aliasing, the joint velocities were kept under 10\\u00b0\\/\\\\ns\\\\n. This and other practical constraints imposed limitations on obtaining an adequate sampling density of the four-dimensional joint space. As such, the training data was collected while executing randomly generated trajectories using only the first four joints (trajectories were made linear for simplicity). A total of 500 trajectories were executed, resulting in 225,000 captured camera frames and corresponding joint values. 50,000 data points were reserved for the nearest-neighbor dataset, and the remaining for training data. Test data was collected for arbitrary joint trajectories with varying velocities and accelerations (without concern for joint flexibility and stretch).'\",\"71\":null,\"72\":\"'http:\\/\\/www.aslatech.com\\/'\\n\\n'http:\\/\\/www.bluebotics.com\\/'\",\"73\":null,\"74\":\"\\\"The authors would like to thank Martin Gerdzhev and Professor Joelle Pineau's team at McGill for sharing their Arduino code and shield card to interface with the Omni+ input port, Emma Smith and Professor William C. Miller's team at UBC for their work securing the PWC platform, and Justin Reiher and previous members of the AGEWELL WP3.2 and CanWheel engineering teams for their work on the hardware and software systems used in this research.\\\"\",\"75\":null,\"76\":null,\"77\":null,\"78\":null,\"79\":null,\"80\":null,\"81\":null,\"82\":null,\"83\":\"'https:\\/\\/youtu.be\\/NKx_zi975Fs'\\n\\n'encoder-decoder'\\n\\n'A. Encoder'\\n\\n'The encoder network consists of a set of purely convolutional layers that transform the input image, into a more reduced representation of feature vectors, suitable for a specific classification task. Due to the complexity of training from scratch [23], a standard approach is to initialize the model with the weights of a pre-trained model, known as fine-tuning. This has several advantages, as models trained with massive amount of natural images such as VGGNet [24], a seminal network for image classification, usually provide a good performance and stability during the training. Moreover, as initial layers closer to the input image provide low-level information and final layers are more task-specific, it is also typical to employ the first layers of a well-trained CNN for different purposes, i.e. place recognition [25]. This was also the approach in [18], where authors employed the first 8 layers of VGGNet to initialize their network, keeping their weights fixed during training, while the remaining layers were trained from scratch with random initialization. Therefore, in this work, we first fine-tuned the very deep model in [18], depicted in Figure 1a.'\\n\\n'However, since our goal is to estimate the VO with the processed sequences, a very deep network, such as the fine-tuned model, is less suitable for usual robotic applications, where the computational power must be saved for the rest of modules. Moreover, depth estimation requires a high level of semantic abstraction as it needs some spatial reasoning about the position of the objects in the scene. In contrast, VO algorithms are usually based on tracking regions of interest in the images, which largely relies on the gradient, i.e., the first derivatives of the images, information that it is usually present in the shallow layers of CNNs. Therefore, we also propose a smaller and less deep CNN to obtain faster performance, whose encoder is formed by three layers (dimensions are in Figure 1b), each one of them formed by a convolution with a\\\\n5\\u00d75\\\\nkernel, followed by a batch-normalization layer [26] and a pooling layer.'\\n\\n'encoder'\\n\\n'decoder'\\n\\n'B. Long Short Term Memory (LSTM)\\\\nWhile it is feasible to use a feedforward neural network to increase the information in images for VO, the input sequence may contain non-ignorable brightness variation. More importantly, the brightness constancy is not enforced in a feedforward network, hence the output sequence is expected to break the brightness constancy assumption for many VO algorithms. To overcome this, we can exploit the sequential information to produce more stable and temporally consistent images, i.e. reducing the impact of possible illumination change to ease the tracking of interest points. Therefore, we exploit the Recurrent Neural Networks (RNNs), more specifically, the LSTM networks first introduced in [27]. In these networks, unlike in standard CNNs where the output is only a non-linear function\\\\nf\\\\nof the current state\\\\ny\\\\nt\\\\n=f(\\\\nx\\\\nt\\\\n)\\\\n, the output is also dependent on the previous output:\\\\ny\\\\nt\\\\n=f(\\\\nx\\\\nt\\\\n,\\\\ny\\\\nt\\u22121\\\\n)\\\\n(1)\\\\nView Source as the layers are capable of memorizing the previous states. We introduce two LSTM layers in the fine-tuned network between the encoder and the decoder part, in order to produce more stable results for a better odometry estimation.'\\n\\n'C. Decoder'\\n\\n'Finally, the decoder network is formed by three deconvolutional layers, each of them formed by an upsampling, a convolution and a batch-normalization layer, as depicted in Figure 1. The deconvolutional layers increase the size of the intermediate states and reduce the length of the descriptors.'\\n\\n'Typically, decoder networks produce an output image of a proportional size of the input one containing the predicted values, which is in general blurry and noisy thus not very convenient to be used in a VO pipeline. To overcome this issue, we introduce an extra step which merges the raw output of the decoder with the input image producing a more realistic image. For that, we concatenate both the input image in grayscale and the decoder output into a 2-channel image then applying a final convolutional filter with a\\\\n1\\u00d71\\\\nkernel and one channel.'\",\"84\":\"\\\"B. Industrial Robot\\\\nIn order to have pure rotations about the optical center, the camera was mounted on the end-effector of a 6 DOF St\\u00e4ubli TX60 robot located in a\\\\n10.05 m\\u00d77.03 m\\u00d72.70 m\\\\nroom with neon lighting (see Fig. 5). We used the Tsai & Lenz's algorithm in the ViSP-library2 implementation, to extrinsically calibrate the camera with respect to the robot end-effector, i.e. to compute\\\\ne\\\\nM\\\\nc\\\\n\\u2208SE(3)\\\\n, the rigid transformation between the camera frame\\\\nF\\\\nc\\\\nand the end-effector frame\\\\nF\\\\ne\\\\n. The calibration rig was observed by the camera from six different poses (see Fig. 4(b)), leading to:\\\\ne\\\\nM\\\\nc\\\\n=\\\\n\\u23a1\\\\n\\u23a3\\\\n\\u23a2\\\\n\\u23a2\\\\n\\u23a2\\\\n\\u22120.0136\\\\n0.0142\\\\n0.9998\\\\n0\\\\n0.9997\\\\n\\u22120.0220\\\\n0.0139\\\\n0\\\\n0.0222\\\\n0.9996\\\\n\\u22120.0139\\\\n0\\\\n\\u22120.0401\\\\n0.0000\\\\n0.2372\\\\n1\\\\n\\u23a4\\\\n\\u23a6\\\\n\\u23a5\\\\n\\u23a5\\\\n\\u23a5\\\\n,\\\\nView Source where the translation is expressed in meters. In our tests, we considered 5 Collection Points (CPs) located on the same plane parallel to the ground (the camera center being 60 cm above the base of the St\\u00e4ubli robot). At each CP, the camera was rotated of 360\\u00b0 about the vertical axis, with a step size of 2.5\\u00b0, yielding 144 images. The distance between CP0 and CP1, CP2, CP3 and CP4 is 40, 120, 240 and 400 cm, respectively. The encoders of the St\\u00e4ubli robot provided us with an accurate ground truth for the validation of our visual gyroscope.\\\\n1) Yaw-Angle Estimation (1 DOF)\\\\nTo study the shape of the cost function\\\\nC\\\\nMPP\\\\nand to easily evaluate the impact of the expansion parameter\\\\n\\u03bb\\\\ng\\\\nand of the subdivision level\\\\nN\\\\n, on the magnitude of the angular estimation error, we rotated the camera about a single axis, the (vertical)\\\\nx\\\\n-axis of\\\\nF\\\\nc\\\\n(see Fig. 5). In our first series of tests, we focused on CP2 and ran the Levenberg-Marquardt algorithm (with\\\\n\\u03b3=1\\\\n) in conjunction with a redescending M-estimator with Cauchy's score function [30, Sect. 4.8] to minimize\\\\nC\\\\nMPP\\\\n(r)\\\\n(as stopping criterion, we set a 10\\u22126 threshold on the stability of residuals). For the sake of uniformity,\\\\nC\\\\nMPP\\\\n(r)\\\\nwas normalized between 0 and 1 (henceforth denoted by\\\\nC\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\nMPP\\\\n(r))\\\\n: in fact,\\\\nmax(\\\\nC\\\\nMPP\\\\n(r))\\\\nvaried over a large range (up to three orders of magnitude). Fig. 6 shows the width of the convergence domain of\\\\nC\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\nMPP\\\\n(r)\\\\nfor different values of\\\\nN\\\\nand\\\\n\\u03bb\\\\ng\\\\n. In particular, Fig. 6(a) reports the MPP cost function for\\\\nN=2\\\\nand\\\\n\\u03bb\\\\ng\\\\n=0.01\\\\nwithout the M-estimator, Fig. 6(b) for\\\\nN=3\\\\nand\\\\n\\u03bb\\\\ng\\\\n=0.4\\\\nand Fig. 6(c) for\\\\nN=5\\\\nand\\\\n\\u03bb\\\\ng\\\\n=0.3\\\\nwith M-estimator. The width of the convergence domain is 115\\u00b0, 312.5\\u00b0 and 360\\u00b0, respectively (see the vertical dashed lines). Note that for\\\\n\\u03bb\\\\ng\\\\n=0.01\\\\n, the MPP cost function essentially reduces to the SSD cost function in (1). More insight into the shape of\\\\nC\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\nMPP\\\\n(r)\\\\nis provided by Figs. 6(d)\\u20136(e) which report the width of the convergence domain against\\\\n\\u03bb\\\\ng\\\\nfor\\\\nN\\u2208{2,3,\\u2026,6}\\\\n(the values of\\\\n\\u03bb\\\\ng\\\\nconsidered are marked with a cross). The values of\\\\nC\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\nMPP\\\\n(r)\\\\nfor\\\\n\\u03bb\\\\ng\\\\n>1\\\\nare not shown in the figures, since we observed no changes with respect to the case of\\\\n\\u03bb\\\\ng\\\\n=1\\\\n. Some conclusions can be drawn from Fig. 6:\\\\nThe bigger\\\\nN\\\\n, the smoother\\\\nC\\\\nMPP\\\\n(r)\\\\nand the larger the convergence domain,\\\\nThe bigger\\\\n\\u03bb\\\\ng\\\\n, the larger the convergence domain: however, the effect vanishes for\\\\n\\u03bb\\\\ng\\\\n>0.5 (N>2)\\\\n,\\\\nThe M-estimator has a \\u201clinearizing effect\\u201d on\\\\nC\\\\nMPP\\\\n(r)\\\\n(the effect is more pronounced for large\\\\n\\u03bb\\\\ng\\\\n's). As a consequence, the convergence radius might increase (especially for\\\\n\\u03bb\\\\ng\\\\n\\u2208[0.2,0.4]\\\\n). However, for small\\\\nN\\\\n, the M-estimator perturbs\\\\nC\\\\nMPP\\\\n(r)\\\\nas well, which might result in a loss of estimation accuracy (see the \\u201cbumps\\u201d in Fig. 6(a)),\\\\nUnder ideal conditions (i.e. no image noise),\\\\nC\\\\nMPP\\\\n(r)\\\\nis perfectly symmetrical about the origin.\\\\nFig. 5.\\\\nThe Ricoh theta mounted on the end-effector of the St\\u00e4ubli TX60 robot.\\\\ne\\\\nM\\\\nc\\\\nis the rigid transformation between the camera frame\\\\nF\\\\nc\\\\nand the end-effector frame\\\\nF\\\\ne\\\\n.\\\\nShow All\\\\nFig. 6.\\\\n[1 DOF St\\u00e4ubli] normalized cost function\\\\nC\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\nMPP\\\\n(r)\\\\nfor: (a)\\\\nN=2, \\\\n\\u03bb\\\\ng\\\\n=0.01\\\\n, without the M-estimator; (b)\\\\nN=3, \\\\n\\u03bb\\\\ng\\\\n=0.4\\\\nwith the M-estimator; (c)\\\\nN=5, \\\\n\\u03bb\\\\ng\\\\n=0.3\\\\nwith the M-estimator (the width of the convergence domain is indicated by the vertical dashed lines). (d), (e) Width of the convergence domain of\\\\nC\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\nMPP\\\\n(r)\\\\nagainst\\\\n\\u03bb\\\\ng\\\\nfor\\\\nN\\u2208{2,\\u2026,6}\\\\nwithout and with M-estimator, respectively.\\\\nShow All\\\\nThe estimation accuracy and computational complexity of our gyroscope are evaluated in Fig. 7. In the experiments, we set\\\\n\\u03bb\\\\ng\\\\n=0.325, \\u03b3=1\\\\n, and considered two candidate initial conditions,\\\\nr\\\\n(0)\\\\n\\u2208{0,\\u03c0}\\\\n, retaining the one that minimizes\\\\nC\\\\nMPP\\\\n(r)\\\\n. This strategy is simple but effective to reject outliers, and it noticeably reduces the estimation error. Fig. 7(a, top) reports the mean and standard deviation of the magnitude of the angular estimation error, and Fig. 7(a, bottom) the mean CPU time in seconds for\\\\nN\\u2208{2,3,4,5}\\\\nwith (\\u201cM\\u201d) or without M-estimator (we ran the gyroscope on a MacBook Pro with 2.4 GHz Intel Core i7 processor and 8 GB RAM). Fig. 7(b) complements Fig. 7(a, top) by displaying the empirical cumulative distribution function obtained from the magnitude of the angular estimation error for\\\\nN\\u2208{2,3,4,5}\\\\n. The vertical bars in Fig. 7(b) show the ratio of the elements of the error vector whose value is smaller than 0.5\\u00b0, 1\\u00b0, 2\\u00b0, 5\\u00b0 and 10\\u00b0. For instance, if a 5\\u00b0- error is allowed, about 75% of the estimates are acceptable for\\\\nN=3\\\\nand about 95% for\\\\nN=4\\\\n. The latter result is all the more remarkable, since for\\\\nN=4\\\\nthe gyroscope uses two\\\\n37 pixels\\u00d737 pixels\\\\ngrayscale images (one per fisheye lens). Obviously, the bigger\\\\nN\\\\n, the more accurate the gyroscope, to the detriment of the computation time. In fact, since\\\\nP=\\\\n1\\\\n2\\\\n(20\\u00d7\\\\n4\\\\nN\\\\n)+2\\\\n(cf. Sect. II), the runtime grows exponentially with\\\\nN\\\\n. The gyroscope turned out to be also robust to occasional image occlusions caused by the motion of the robot arm.\\\\nA final study was performed to evaluate the impact of the translational motion on the estimation accuracy of the gyroscope. We tuned the Levenberg-Marquardt algorithm (with M-estimator) as in Fig. 7, and we estimated the angle between image 71 at CP0 and all the other images in CPO, CP1, \\u2026, CP4. Fig. 8 reports the mean and standard deviation of the magnitude of the angular estimation error for a growing distance from CP0 and for\\\\nN\\u2208{3,4,5}\\\\n. The translational motion has a negligible effect on the estimation error: however, for\\\\nN=5\\\\n(and higher) the gyroscope appears to be more sensitive to translations. For\\\\nN\\u2208{3,4,5}\\\\n, the mean estimation error (over the 5 CPs) is 2.95\\u00b0, 2.49\\u00b0, 1.69\\u00b0, respectively.\\\\n2) Attitude Estimation (3 DOFs)\\\\nThe gyroscope was also used to estimate the three rotational DOFs of the Ricoh Theta. To this end, we considered a single collection point, CP2, where we obtained the maximum number of distinct 3D orientations of the camera (94 overall), which did not violate the mechanical constraints of the robot. We set\\\\n\\u03bb\\\\ng\\\\n=0.275,\\u03b3=1\\\\n, and initialized the Gauss-Newton algorithm (without M-estimator) with\\\\nr\\\\n(0)\\\\n=0\\\\n. Table I reports the statistics of the estimation error\\\\n\\u2225r\\u2212\\\\nr\\\\n^\\\\n\\u2225\\\\nover the 94 images for\\\\nN\\u2208{3,4,5}\\\\n, and the corresponding mean CPU time and mean number of iterations of the optimization algorithm.\\\\nFig. 7.\\\\n[1 DOF St\\u00e4ubli] (a, top) mean and standard deviation of the magnitude of the estimation error for\\\\nN\\u2208{2,3,4,5}\\\\nwith (\\u201cM\\u201d) or without M-estimator; (a, bottom) mean CPU time for\\\\nN\\u2208{2,3,4,5}\\\\n(note the logarithmic scale on the vertical axis); (b) Empirical cumulative distribution function obtained from the magnitude of the angular estimation error for\\\\nN\\u2208{2,3,4,5}\\\\n.\\\\nShow All\\\\nTable I [3 DOFs St\\u00e4ubli] statistics of\\\\n\\u2225r\\u2212\\\\nr\\\\n^\\\\n\\u2225\\\\n, mean CPU time and mean number of iterations of the gyroscope over the 94 images.\\\"\",\"85\":null,\"86\":\"'https:\\/\\/github.com\\/wanq-chen\\/correlation_flow'\\n\\n'Robust velocity and position estimation is crucial for autonomous robot navigation. The optical flow based methods for autonomous navigation have been receiving increasing attentions in tandem with the development of micro unmanned aerial vehicles. This paper proposes a kernel cross-correlator (KCC) based algorithm to determine optical flow using a monocular camera, which is named as correlation flow (CF). Correlation flow is able to provide reliable and accurate velocity estimation and is robust to motion blur. In addition, it can also estimate the altitude velocity and yaw rate, which are not available by traditional methods. Autonomous flight tests on a quadcopter show that correlation flow can provide robust trajectory estimation with very low processing power. The source codes are released based on the ROS framework.'\\n\\n'In this paper we propose a robust and computationally efficient optical flow method, called correlation flow for robot velocity estimation using a monocular camera. We introduce a kernel translation correlator and a kernel scale-rotation correlator for the camera motion prediction. Due to the high efficiency of fast Fourier transform, our method is able to run in real-time on an ultra-low-power processor. Experiments on velocity estimation show that correlation flow provides more reliable results than PX4Flow. Autonomous flight and hovering tests demonstrate that correlation flow is able to provide robust trajectory estimation at very low computational cost. The source codes are released for research purpose.'\\n\\n'https:\\/\\/github.com\\/wanq-chen\\/correlation_flow'\",\"87\":null,\"88\":\"'We make five main observations. First, in comparison to the FCN monocular estimator, stereo vision in general gives more accurate depth estimates, also at the larger depths. Second, it can be seen that the monocular estimator provides depth values that are closer than stereo vision, which was limited to a maximal disparity of 64 pixels. Third, the accuracy of stereo vision becomes increasingly \\u2018wavy\\u2019 towards 80 meters. This is due to the nature of stereo vision, in which the distance per additional pixel increases nonlinearly. The employed code determines subpixel disparity estimates up to a sixteenth of a pixel, but this does not fully prevent the increasing error when between pixel disparities further away. Fourth, stereo vision has a big absolute error peak at the low distances. This is due to large outliers, where stereo vision finds a better match at very large distances. Fifth, one may think that the monocular depth estimation far away is too bad for fusion. However, one has to realize that these results are made without scaling the monocular estimates - which can go beyond 80 meters, resulting in large errors. Moreover, investigation of the error\\\\n(y\\u2212\\\\ny\\\\n\\u2217\\\\n)\\\\nshows that the monocular estimate is not biased in general. Finally, one has to realize that the majority of the pixels in the KITTI dataset lies close by, as can be seen in fig. 7. Hence, the closer pixels are most important for the fusion result.'\",\"89\":null,\"90\":null,\"91\":null,\"92\":null,\"93\":null,\"94\":\"'https:\\/\\/youtu.be\\/acI0axPgJCY'\",\"95\":null,\"96\":null,\"97\":null,\"98\":null,\"99\":\"'In the paper we presented a novel dynamic control method for the control of CDPRs, with experimental results on a planar manipulator. The main idea of the work was to find a solution for the control of these robots, with the usage of the minimum number of measures. This means exploiting basic sensors, such as motor encoders (position and velocity) and motor torque.'\",\"100\":null,\"101\":null,\"102\":null,\"103\":null,\"104\":null,\"105\":\"'https:\\/\\/github.com\\/TimingSpace\\/MVOScaleRecovery'\\n\\n'https:\\/\\/github.com\\/TimingSpace\\/MVOScaleRecovery'\",\"106\":null,\"107\":null,\"108\":\"'https:\\/\\/www.youtube.com\\/watch?v=s8XV1Y6opig'\",\"109\":null,\"110\":\"'Our algorithm to solve the communication planning problem has two key components: 1) evolve and evaluate the belief over a finite time-horizon using a particle filter, then 2) find the optimal communication schedule over this time-horizon using dynamic programming. Pseudocode is provided in Alg. 2. We describe the approach from the perspective of robot\\\\ni\\\\n, who is deciding to request information from robot\\\\nj\\\\nat iteration 0. This process is repeated for all robots at every iteration.'\",\"111\":\"\\\"This paper considers the problem of multi-robot realization. A realization is a set of robot positions where pairwise distances are either bounded from above or from below by a given adjacency threshold - depending on whether the respective robot pairs are to be adjacent or not. In the realization problem, unlike the related coordinated navigation or formation control problems, exact goal positions or relative distances need not be specified. Rather, only pairwise adjacency constraints are given and the robots' positions are required to satisfy these constraints. Applications of realization problem include multi-robots involved in team games (playing soccer, etc), patrolling and area coverage. We present a novel solution to this problem in which the robots simultaneously navigate to find a realization of a given adjacency matrix without colliding with each other along the way. In this solution, complete information about pairwise distances and free configuration space are encoded using an artificial potential function over the cross product space of the robots' simultaneous positions and proximity variables. The closed-loop dynamics governing the motion of each velocity-controlled robot take the form of the appropriate projection of the gradient of this function while pairwise distances are adjusted accordingly. Our extensive simulations demonstrate that the proposed approach has considerably higher realization percentage and shorter movement distances in comparison to a standard 2-stage approach.\\\"\\n\\n\\\"This paper presents a novel approach to the realization problem that also provides an inexact solution. In our approach, the combinatorial problem is transformed to a set of coupled dynamical systems defined over the cross product space of the robots simultaneous positions and proximity variables. Its advantages are two-fold: First, the robots do not have to wait for a realization to be found - as would be case if they were using one of the previously proposed inexact approaches. Rather, the robots simultaneously navigate to find a realization of a given adjacency matrix without colliding with each other. Second, as the robots' initial positions are taken into account, robots' paths are expected to be shorter in comparison to previous approaches in which that is not the case. The proposed approach is based on the construction of an artificial potential function that encodes robots' pairwise distances considering the given adjacency matrix, proximity variables and freespace. The constructed artificial potential function is used to set up two coupled dynamical systems with respect to robots' positions and their proximity relaxation variables. The first system defines a set of coupled closed loop gradient dynamics that governs the motion of the robot ensemble via projecting the vector field onto the coordinate slice corresponding to each individual robot. The second system defines a gradient system that enables pairwise proximities to be relaxed as needed while adhering to the given adjacency matrix. As such, control inputs that enable the robots to move towards a realization of the goal adjacency matrix are defined - in contrast to the existing inexact solutions that only aim to provide a realization.\\\"\\n\\n'This paper has presented a novel inexact approach for multi-robot planar realization. In this problem, unlike the related coordinated navigation or formation control problems, exact goal positions or relative distances need not be specified. Rather, they are given pairwise adjacency constraints in which pairwise distances should be bounded from above or from below by a given adjacency threshold - depending on whether the respective pair of robots is to be adjacent or not. As such, the realization problem can be viewed as formation control with very loose constraints on the final formation. In the proposed approach, an artificial potential function defined over the cross product space of the robots relative positions and proximity variables encodes complete information about pairwise distances and free space. It should be noted that the robots are assumed to have this knowledge either via communicating with each other or via inquiring from a central knowledge depository. The resulting pair of coupled closed-loop dynamics governs the motion of each velocity-controlled robot while adjusting pairwise distance accordingly. Our extensive simulations demonstrate that both realization percentage and travelled distances are improved considerably - in comparison to the classical 2-stage approach. We are currently working on the mathematical analysis of the proposed approach. For future work, we plan to consider other obstacles and relaxing the assumption regarding complete knowledge of other robots.'\",\"112\":\"'https:\\/\\/youtu.be\\/myogharjuoy'\",\"113\":null,\"114\":null,\"115\":null,\"116\":null,\"117\":\"'We used the open source physics engine DART [6], which provides APIs to implement user-defined constraints without modifying the core code that formulates and solves LCP. For feedforward and back-propagation operations on a neural network, we incorporated the light-weight, C++ library tiny-dnn [23], with the trained weights imported from TensorFlow. To clearly visualize the effect of joint-limit constraints, the following experiments only simulate one limb with the torso fixed in place.'\",\"118\":null,\"119\":\"'https:\\/\\/goo.gl\\/42yf6f'\",\"120\":\"'https:\\/\\/sites.google.com\\/site\\/imitationfromobservation\\/'\\n\\n'https:\\/\\/sites.google.com\\/site\\/imitationfromobservation\\/'\\n\\n'The model consists of four components: a source observation encoder\\\\nEnc\\\\n1\\\\n(\\\\no\\\\ni\\\\nt\\\\n)\\\\nand a target initial observation encoder\\\\nEnc\\\\n2\\\\n(\\\\no\\\\nj\\\\n0\\\\n)\\\\nthat encode the observations into source and target features, referred to as\\\\nz\\\\n1\\\\nand\\\\nz\\\\n2\\\\n, a translator\\\\nz\\\\n3\\\\n=T(\\\\nz\\\\n1\\\\n,\\\\nz\\\\n2\\\\n)\\\\nthat translates the features\\\\nz\\\\n1\\\\ninto features for the context of\\\\nz\\\\n2\\\\n, which are denoted\\\\nz\\\\n3\\\\n, and finally a target context decoder\\\\nDec(\\\\nz\\\\n3\\\\n)\\\\n, which decodes these features into\\\\no\\\\n^\\\\nj\\\\nt\\\\n. We will use\\\\nF(\\\\no\\\\ni\\\\nt\\\\n,\\\\no\\\\nj\\\\n0\\\\n)=\\\\nz\\\\n3\\\\nto denote the feature extractor that generates the features\\\\nz\\\\n3\\\\nfrom an input observation and a context image. The encoders Enc1 and Enc2 can have either different weights or tied weights depending on the diversity of the demonstration scenes. To deal with the complexities of pixel-level reconstruction, we include skip connections from Enc2 to Dec. The model is supervised with a squared error loss\\\\nL\\\\ntrans\\\\n=\\u2225(\\\\no\\\\n^\\\\nj\\\\nt\\\\n)\\\\ntrans\\\\n\\u2212\\\\no\\\\nj\\\\nt\\\\n\\u2225\\\\n2\\\\n2\\\\non the output\\\\no\\\\nj\\\\nt\\\\nand trained end-to-end.'\\n\\n'For encoders Enc1 and Enc2 in simulation we perform four\\\\n5\\u00d75\\\\nstride-2 convolutions with filter sizes 64, 128, 256, and 512 followed by two fully-connected layers of size 1024. We use LeakyReLU activations with leak 0.2 for all layers. The translation module\\\\nT(\\\\nz\\\\n1\\\\n,\\\\nz\\\\n2\\\\n)\\\\nconsists of one hidden layer of size 1024 with input as the concatenation of\\\\nz\\\\n1\\\\nand\\\\nz\\\\n2\\\\n. For the decoder Dec in simulation we use a fully connected layer from the input to four fractionally-strided convolutions with filter sizes 256, 128, 64, 3 and stride\\\\n1\\\\n2\\\\n. We have skip connections from every layer in the context encoder Enc2 to its corresponding layer in the decoder Dec by concatenation along the filter dimension. For real world images, the encoders perform 4 convolutions with filter sizes 32, 16, 16, 8 and strides 1, 2, 1, 2 respectively. All fully connected layers and feature layers are size 100 instead of 1024. The decoder uses fractionally-strided convolutions with filter sizes 16, 16, 32, 3 with strides\\\\n1\\\\n2\\\\n, 1,\\\\n1\\\\n2\\\\n, 1 respectively. For the real world model only, we apply dropout for every fully connected layer with probability 0.5, and we tie the weights of Enc1 and Enc2.'\",\"121\":\"'The edges of the graph are encoded through the following two functions.'\\n\\n'Experiment 1 (a), ITMCD selected the smallest model that encodes each color sort demonstration. Experiment 2 (b), ITMCD opted to expand the initiation classifier to encompass the whole table (LR) instead of adding a new primitive (L\\/R). Experiment 3 (c), ITMCD opted to add an edge (RB) to model the blue sort demonstration instead of adding a new primitive (R\\/B).'\",\"122\":\"https:\\/\\/github.com\\/sermanet\\/sermanet.github.io\\/blob\\/master\\/assets\\/bib\\/Sermanet2017TCN.bib\",\"123\":\"'https:\\/\\/youtu.be\\/7Dx5imylKcw'\\n\\n'Expected sensor traces acquisition: Still with the tilt stage at 0\\u00b0 roll angle, we unroll the nominal primitives 15 times and record the tactile sensor traces. We encode each dimension of the 38-dimensional sensor traces as\\\\nS\\\\nexpected, using the standard DMP formulation.'\\n\\n\\\"Feedback model learning: Now we vary the tilt stage's roll-angle to 2.5\\u00b0, 5\\u00b0, 7.5\\u00b0, and 10\\u00b0, one-at-a-time, to encode different environmental settings. At each setting, we let the robot unroll the nominal behavior. Beside the downward force control for contact maintenance, now we also activate the roll-orientation PI torque control at 0 Newton-meter throughout primitives 2 and 3. This allows the human demonstrator to perform the roll-orientation correction demonstration, to maintain full flat tool-tip contact relative to the now-tilted scraping board. We recorded 15 demonstrations for each setting, from which we extracted the supervised dataset for the feedback model, i.e. the pair of the sensory trace deviation\\\\n\\u0394S\\\\nand the target coupling term\\\\nC\\\\ntarget\\\\nas formulated in Equation 9. Afterwards, we learn the feedback models from this dataset using the PMNN.\\\"\\n\\n'(Left) Comparison of regression results on primitives 2 and 3 using different neural network structures; (Middle) Comparison of regression results on primitives 2 and 3 using separated feature learning (PCA or Autoencoder and phase kernel modulation) versus embedded feature learning (PMNN); (Right) The top 10 dominant regular hidden layer features for each phase RBF in primitive 2, roll-orientation coupling term, displayed in yellow.'\\n\\n'In this experiment, we used PCA which retained 99% of the overall data variance, reducing the data dimensionality to 7 and 6 (from originally 38) for primitive 2 and 3, respectively. In addition, we also implemented an autoencoder, a nonlinear dimensionality reduction method, as a substitute for PCA in representation learning. The dimensions of the latent space of the autoencoders were 7 and 6 for primitive 2 and 3, respectively. For PMNNs, we used two kinds of networks: one with one regular hidden layer of 6 nodes (such that it is become comparable with the PCA counterpart), and the other with one regular hidden layer of 100 nodes.'\",\"124\":\"'Environment features are encoded as binary occupancy grid maps, where a grid cell in a feature map denotes the presence\\/absence of the feature it is modeling. Our set of feature maps model obstacles,\\\\nm\\\\nterrain classes found in the environment, and blurred versions of the obstacle and terrain feature maps.'\\n\\n'The grid feature maps are blurred and used as additional features to encode the distance to the nearest cell of the classes. Fig. 3 shows an example of a feature map (left) and its corresponding blurred map (right). The blurring effect extends the degree to which a cell represents the feature based on the values of cells within a certain radius. Cells closer to positive values in the grid map are weighted more heavily. Typically, for each feature we use a conventional grid map and at least one blurred map. Maps blurred at larger radii are used when feature information at larger distances is needed.'\",\"125\":\"'The monocular visual-inertial system (VINS), which consists one camera and one low-cost inertial measurement unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate visual-inertial odometry is prone to drift and cannot provide absolute pose estimation. Leveraging history information to relocalize and correct drift has become a hot topic. In this paper, we propose a monocular visual-inertial SLAM system, which can relocalize camera and get the absolute pose in a previous-built map. Then 4-DOF pose graph optimization is performed to correct drifts and achieve global consistent. The 4-DOF contains x, y, z, and yaw angle, which is the actual drifted direction in the visual-inertial system. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. Current map and previous map can be merged together by the global pose graph optimization. We validate the accuracy of our system on public datasets and compare against other state-of-the-art algorithms. We also evaluate the map merging ability of our system in the large-scale outdoor environment. The source code of map reuse is integrated into our public code, VINS-Monol 11 https:\\/\\/github.com\\/HKUST-Aerial-Robotics\\/VINS-Mono.'\\n\\n'Open-source code of map reuse.'\",\"126\":\"'(a) The experimental handheld 3D spinning LiDAR for mobile mapping. It contains a 2D laser, an IMU, an encoder, a color camera and a thermal camera. Note that the thermal camera is not used in the paper. (b) System block diagram of our method. The device local trajectory is tracked in the local mapping stage, while the global consistent map is maintained in the second global mapping stage.'\\n\\n'A hand-held 3D spinning LiDAR is utilized for the real data experiments. The device consists of a spinning Hokuyo UTM-30LX laser, an encoder, a Microstrain 3DM-GX3 IMU, Grasshopper3 2.8 MP color camera and Optris PI 450 thermal-infrared\\\\n382\\u00d7288\\\\npixel camera (Figure 2 (a)).'\",\"127\":null,\"128\":null,\"129\":null,\"130\":null,\"131\":null,\"132\":null,\"133\":null,\"134\":null,\"135\":null,\"136\":null,\"137\":\"'https:\\/\\/youtu.be\\/s_rvaHvTn64'\\n\\n\\\"We present a real-time vehicle detection and tracking system to accomplish the complex task of driving behavior analysis in urban environments. We propose a robust fusion system that combines a monocular camera and a 2D Lidar. This system takes advantage of three key components: robust vehicle detection using deep learning techniques, high precision range estimation from Lidar, and road context from the prior map knowledge. The camera and Lidar sensor fusion, data association and track management are all performed in the global map coordinate system by taking into account the sensors' characteristics. Lastly, behavior reasoning is performed by examining the tracked vehicle states in the lane coordinate system in which the road context is encoded. We validated our approach by tracking a leading vehicle while it performed usual urban driving behaviors such as lane keeping, stop-and-go at intersections, lane changing, overtaking and turning. The leading vehicle was tracked consistently throughout the 2.3 km route and its behavior was classified reliably.\\\"\\n\\n'Tracking results of the target vehicle for the entire 2.3 km route with 16537 numbers of detection in 479 s. The maximum speed of the target vehicle is 9.4 m\\/s and the maximum distance from the ego vehicle is 22.5 m. The behavior is color coded blue for lane keeping, red for lane changing and green for stopping.'\",\"138\":null,\"139\":\"'https:\\/\\/youtu.be\\/x75RpjoJ9HM'\\n\\n'https:\\/\\/youtu.be\\/R090Tjr4s40'\",\"140\":\"'We present a novel method of measurement covariance estimation that models measurement uncertainty as a function of the measurement itself. Existing work in predictive sensor modeling outperforms conventional fixed models, but requires domain knowledge of the sensors that heavily influences the accuracy and the computational cost of the models. In this work, we introduce Deep Inference for Covariance Estimation (DICE), which utilizes a deep neural network to predict the covariance of a sensor measurement from raw sensor data. We show that given pairs of raw sensor measurement and ground-truth measurement error, we can learn a representation of the measurement model via supervised regression on the prediction performance of the model, eliminating the need for hand-coded features and parametric forms. Our approach is sensor-agnostic, and we demonstrate improved covariance prediction on both simulated and real data.'\",\"141\":\"\\\"This study investigates the effects of inertial sensor placement and noise characteristics on the accuracy of robot pose estimation. Of course, most robots are equipped with joint angle encoders for pose estimation and end-effector positioning. However, in some situations, it's not possible or not desirable to introduce encoders on all joints. Such common examples include legged locomotion, dual arm co-manipulation, and prosthetic limbs. To tackle such situations, one solution is to embed inertial measurement units (IMUs) into artificial skin patches placed on robots' limbs and body. This work analyzes the effects of design parameters such as the number of sensors, their placement on the robot, and noise properties on the quality of robot pose estimation and its signal-to-noise Ratio (SNR). We study the benefits of using a large number of IMUs, which is possible due to the proliferation of inexpensive micro-machined sensors. We use Monte-Carlo simulations and experiments with a two-link robot arm to obtain the distributions of expected estimation error metric values for several accelerometer configurations, which are then compared to determine the optimal number and placement for the IMUs. Results show that the placement of at least two accelerometers on each link has the most significant impact on the pose estimation error, while using a larger number of accelerometers plays a less significant role in reducing the arm pose estimation error and resultant SNR.\\\"\\n\\n'Virtual accelerometer readings were generated from the encoder data acquired from the experimental section for the same trials. First, velocities and accelerations of the robot during experimental runs\\\\n\\u03b8\\\\n\\u02d9\\\\n\\u2217\\\\nand\\\\n\\u03b8\\\\n\\u00a8\\\\n\\u2217\\\\nwere approximated as:'\\n\\n'(Blue) Encoder values, (red) filtered estimated encoder values\\\\n(\\\\n\\u03b8\\\\n^\\\\n\\u2217,s\\\\n)\\\\n, and (green) unfiltered estimated encoder values\\\\n(\\\\n\\u03b8\\\\n^\\\\n\\u2217\\\\n)\\\\nfrom the experiment.'\\n\\n'(Blue) Encoder values and, (red) filtered estimated encoder values\\\\n(\\\\n\\u03b8\\\\n^\\\\n\\u2217,s\\\\n)\\\\n, and (green) unfiltered estimated encoder values\\\\n(\\\\n\\u03b8\\\\n^\\\\n\\u2217\\\\n)\\\\nfrom the simulation.'\",\"142\":\"'Encoder-Camera-Ground Penetrating Radar Tri-Sensor Mapping for Surface and Subsurface Transportation Infrastructure Inspection'\\n\\n'We report system and algorithmic development for a sensing suite comprising multiple sensors for both surface and subsurface transportation infrastructure inspection focusing on multi-modal mapping for inspection. The sensing suite contains a camera, a ground penetrating radar (GPR), and a wheel encoder. We design the sensing suite and propose a data collection scheme using customized artificial landmarks (ALs). We use ALs to synchronize two types data streams: camera images that are temporally evenly-spaced and GPR\\/encoder data that are spatially evenly-spaced. We also employ pose graph optimization with synchronization as penalty functions to further refine synchronization and perform data fusion for 3D reconstruction. We have implemented the system and tested it in physical experiments. The results show that our system successfully fuses three sensory data and product metric 3D reconstruction. The sensor fusion approach reduces the end-to-end distance error from 7.45cm to 3.10cm.'\\n\\n'Transportation infrastructure such as bridge decks, freeways, and airport runways requires periodic inspections for maintenance purposes. Manual inspections are labor-intensive and costly. A more viable approach is to employ a robot. The inspection tasks often include both surface and subsurface mapping to assist searching for cracks, voids, or other damages. The ability to combine surface images with subsurface scans is important for further inspections or future repairs. Therefore, it is necessary to combine multiple inspection sensors such as a regular camera, a light Detection and Ranging (LIDAR) device, and a ground penetrating radar (GPR) together along with navigational sensors such as a wheel encoder and\\/or a global position system (GPS) receiver. Combining data from heterogeneous sensors is challenging because there are challenges in system design, synchronization, correspondence, and data fusion.'\\n\\n'We report our recent development on systems and algorithms that enable encoder-camera-GPR tri-sensor fusion for transportation infrastructure inspection which can be viewed as a multi-modal mapping process, a classic problem in robotics with a new sensor combination. We build a sensing suite consisting of the aforementioned sensors (see Fig. 1).'\\n\\n'Left: Our sensing suite comprises a camera and a GPR; a GPR includes GPR control unit, wheel encoder, and GPR antenna. Right: Artificial landmarks. Black and red colored side is the upper side while the metal side is downside (best viewed in color).'\\n\\n'We also design visualization software and a data collection scheme using artificial landmarks (ALs) to simplify the scanning process. Moreover, ALs allow us to synchronize two types of data streams: camera images that are temporally evenly-spaced and GPR\\/encoder readings that are spatially evenly-spaced. Our algorithm takes advantage of ALs to identify moments when the camera center, the GPR frame origin, and the intersection point between AL edges and the GPR trajectory are vertically collinear. The moments allow us to align data streams and then refine them in pose graph optimization with synchronization constraints.'\\n\\n'The camera (DS-CFMT1000-H) is used not only for surface inspection but also for visual SLAM because it can provide more accurate pose and trajectory estimation than that from a GPS receiver in local region. In addition, GPS may not always be available due to terrain or high-rise buildings. The LIDAR (Hokuyo UST-20LX) can also be used for surface crack detection when cracks cannot be distinguished from background images. The GPR (GSSI SIR-3000) is used for substructure inspection and it is installed a wheel encoder which is also an important sensor for data fusion. The wheel encoder data is pre-synchronized with GPR data by a hardware trigger.'\\n\\n'Since camera images are taken according to a fixed interval determined by camera internal clock and GPR scans are triggered by its wheel encoder based on a preset fixed distance traveled, there is no inherent synchronization between the two sensors. However, this would lead to significant issues when fusing the data streams. Therefore, we design a data collection procedure (see Fig. 2) to deal with the synchronization problem by using our ALs.'\\n\\n'1) Identify TSSM from the GPR and Wheel Encoder Readings'\\n\\n'ALs help us synchronize camera image stream and GPR\\/encoder data streams. Four TSSMs are shown here. Camera poses represented by small triangles and GPR poses represented by small rectangles are displayed on the top of the radargram. The poses drawn in dashed lines are virtual poses corresponding to TSSMs (best viewed in color).'\\n\\n'2) Synchronizing Camera Poses to GPR\\/Encoder Data'\\n\\n'With the scale rectified, we can align camera poses with the GPR\\/encoder data streams through distance matching. Let\\\\nt\\\\n\\u2032\\\\nk\\\\nbe GPR frame origin for the corresponding camera center position\\\\nt\\\\nk\\\\n. The fixed frame mapping relationship is'\\n\\n'At each camera\\/GPR frame, we can incorporate encoder readings to capture the traveled distance. Note that encoder error accumulates linear to the distance traveled. We verify distances traveled between adjacent camera\\/GPR poses and formulate the follow objective function by considering relative error'\\n\\n'The proposed algorithm has been validated in physical experiments. The parameters for GSSI SIR-3000 are given as follows: the horizontal sample rate for the wheel encoder is 390 pulses per meter, the two-way travel time of a radar signal is 8 ns, the GPR sample rate is 1024 samples per scan, and the dielectric constant in air is 1. The resolution for the wheel encoder is 1785 pulses per meter, and the distance error does not exceed \\u00b12% under ideal conditions (e.g. smooth surface and no skid). We have scanned a bridge deck at the Ernest Langford architecture center at Texas A&M University to test our system. The attached video illustrates the dataset and the 3D reconstruction result which shows that our algorithms are able to synchronize these data streams to create successful 3D metric reconstruction.'\\n\\n'We built an encoder-camera-GPR tri-sensor transportation infrastructure inspection sensing suite and developed a tri-sensor mapping algorithm. Our system design included hardware configuration, software interface, ALs, and a data collection scheme. We designed ALs to assist the synchronization between two types of data streams: camera images that are temporally evenly-spaced and GPR\\/encoder data that are spatially evenly-spaced. We identified synchronization events created by ALs and used them as inputs to synchronize sensory inputs. The results lead to 3D metric reconstruction for synchronized data streams that covers both surface and subsurface structure. We tested our system in real physical experiments. Our system and algorithm have successfully achieved data synchronizations and metric reconstruction. In the future, we will conduct more physical experiments, especially in field tests. We will provide complexity analysis for the algorithm. We will further improve speed and accuracy of the proposed algorithm.'\\n\\n'Encoder-Camera-Ground Penetrating Radar Sensor Fusion: Bimodal Calibration and Subsurface Mapping'\",\"143\":\"'ARM Cortex M4 -powered STM32F407 Discovery micro-controller development board. Used for reading the IMU through a serial peripheral interface (SPI) and sending the read data forward to a computer with a Ethernet user datagram protocol (UDP). Programmed graphically in Simulink with the Wai-jung third-party blockset, which compiles the Simulink model to STM32F407-compatible C code.'\",\"144\":null,\"145\":\"'as it encodes our assumptions about the function which we wish to learn'\\n\\n'Two impedance controllers are required to control each motor independently. Each impedance controller uses an external position controller with an internal torque controller. The external position controller tracks a reference trajectory (generated via LE method) and uses the angle feedback from the encoders on each motor\\\\n\\u03b8\\\\nm\\\\n. The block diagram of the impedance controller is shown in fig. 7. The motors actuate the ankle in dorsifiexion-plantarfiexion (DP) and inversion-eversion (IE) directions using Bowden cables that form a differential drive mechanism.'\",\"146\":null,\"147\":null,\"148\":null,\"149\":null,\"150\":null,\"151\":null,\"152\":null,\"153\":null,\"154\":null,\"155\":null,\"156\":\"'We use servo motors to change the shape of the curved beam robot which are controlled through a DFRduino UNO V3.0 board with motor shield. It is a simple microcontroller board fully compatible with Arduino UNO R3 and Arduino IDE open-source development environment. Figure 3(b) shows the GUI of the software used to control the servos which change the shape of the robot. In the images the servo angle had been set to 40\\u00b0 and an input voltage of 2.20V is fed to the DC motor. Figure 3(c), (d) and (e) show IUCBR in different shapes.'\",\"157\":null,\"158\":null,\"159\":null,\"160\":\"'https:\\/\\/youtu.be\\/w33QfRjKFs8'\\n\\n'In many industrial robotics applications, such as spot-welding, spray-painting or drilling, the robot is required to visit successively multiple targets. The robot travel time among the targets is a significant component of the overall execution time. This travel time is in turn greatly affected by the order of visit of the targets, and by the robot configurations used to reach each target. Therefore, it is crucial to optimize these two elements, a problem known in the literature as the Robotic Task Sequencing Problem (RTSP). Our contribution in this paper is two-fold. First, we propose a fast, near-optimal, algorithm to solve RTSP. The key to our approach is to exploit the classical distinction between task space and configuration space, which, surprisingly, has been so far overlooked in the RTSP literature. Second, we provide an open-source implementation of the above algorithm, which has been carefully benchmarked to yield an efficient, ready-to-use, software solution. We discuss the relationship between RTSP and other Traveling Salesman Problem (TSP) variants, such as the Generalized Traveling Salesman Problem (GTSP), and show experimentally that our method finds motion sequences of the same quality but using several orders of magnitude less computation time than existing approaches.'\\n\\n'Our second contribution is to provide an open-source implementation2 of the above algorithm. In particular, we carefully benchmark different key components of the algorithm (underlying task-space TSP solver, configuration-space metric, discretization step-size for the free-DoF), so as to come up with an efficient, ready-to-use, software solution.'\",\"161\":null,\"162\":\"'An end-to-end implementation of our approach on a PR2 and Fetch robot (to released as open-source).'\",\"163\":\"\\\"We haven't optimize the code for speed, yet the average computation time for solving one problem (or declare failure) is 43ms in single-thread Matlab, measured on a desktop with Intel Xeon 3.10GHz CPU. The off-line computation described in Algorithm 2 takes several minutes per object, depending on the complexity of the object shape.\\\"\",\"164\":null,\"165\":null,\"166\":null,\"167\":null,\"168\":null,\"169\":\"'The tuned propulsion system of the hex-rotor is E800 of DJI powered by a 6 cells Li-po battery. The flight stack of the hex-rotor is an open source controller for autonomous drones named PixHawk [17]. The robotic arm is composed of Dynamixel smart actuators. The computer used as the arm controller is Intel NUC with a core-i7 processer. The under-actuated gripper is designed based on the Yale OpenHand Project by changing some structure and parameters [18]. The gripper is passively compliant and is capable of grasping objects adaptively [19].'\",\"170\":null,\"171\":null,\"172\":null,\"173\":null,\"174\":\"'the paper is accompanied by open source implementations of six coverage planning algorithms 1;'\",\"175\":null,\"176\":\"'We show that our forward simulation can be encoded in a differentiable deep network and propose an efficient procedure using backpropagation to compute an accurate gradient of the objective function without the need for heuristics (Sec. IV). Unlike Stochastic Gradient Ascent (SGA) [3], our computed gradient accounts for interactions with other agents and static obstacles.'\\n\\n'We have presented a differentiable deep network that can encode a forward simulation, allowing effective backpropagation to compute the gradient of a complex cost function efficiently. Our approach quickly finds influential outcomes even in challenging scenarios with multiple agents. We overcome the limitations of our previous approach (SGA) [3] eliminating the need for heuristics, and its limitations. Unlike SGA, the gradient computed using Backprop-MPDM accounts for interactions with other agents and static obstacles.'\",\"177\":null,\"178\":\"'The calculation time maximum was 0.013 sec and the average was 0.010 sec for all calculations of the real-time planner including a maximum of 0.005 sec and an average of 0.002 sec for the simultaneous optimization of ZMP, COG and foot placements using an Intel Core i7 2.4 GHz CPU. In the implementation, the planner included the optimization based IK to commonly use source code with offline motion generation which was designed for kinematically difficult movements. The calculation of IK is not necessary in the real-time planner, because kinematic difficulties rarely occur when a robot is walking in a wide area. Therefore, the sampling time of the real-time planner can be faster by eliminating the IK calculation.'\",\"179\":\"'https:\\/\\/youtu.be\\/hpxnfregara'\",\"180\":null,\"181\":null,\"182\":\"'The CMP encodes the total linear force acting on the CoM'\",\"183\":null,\"184\":\"'In order to actuate these prostheses, gesture recognition is commonly performed by using sEMG measurements as a simple interface. These are taken by placing electrodes on the skin surrounding the muscle of interest, allowing for a noninvasive monitoring of user muscle activity. Combining this with an embedded gesture recognition system enables user intentions to be decoded that can then be used to provide actuation control signals to a device.'\\n\\n'To evaluate the potential of the textile electromyography system as a prosthesis control interface, the data need to be decoded to recognise the desired gesture. This involves (i) automatic segmentation of the data to extract individual gestures, (ii) computation of segment-wise features, and (iii) classification of the gesture through a pattern recognition algorithm. The following describes these post-processing steps in detail.'\\n\\n'Once segmentation is complete, individual gestures are decoded through a statistical pattern recognition approach based on features computed segment-wise from the data. Specifically, in this paper, a one-versus-one multi-class [20] Support Vector Machine (SVM) [21] with a Gaussian kernel [22] is used for classification. The latter is selected because it has shown good performance in previous work, outperforming comparable one-versus-all methods in general [23], [24].'\",\"185\":null,\"186\":\"'To permit volumetric reconstruction of the ultrasound data, the spatial location of the probe was tracked from a set of four optical markers using a PhaseSpace active motion capture system (PhaseSpace Inc., San Leandro, CA, USA). Prior to data collection, the transformation between the ultrasound probe location and the measured image was calculated (both spatially and temporally) using the open-source PLUS calibration toolkit [21], with a reported probe calibration error of 1.6mm. During data collection, data were streamed to an external computer at a rate of 30fps through an OpenIGTLink server [22] and later reconstructed using the volume reconstruction application provided by the PLUS toolkit.'\",\"187\":\"\\\"Many grasp-training robots (GTRs) move their user's fingers in response to his or her motion intentions as decoded from motion triggers or EMG or electroencephalogram (EEG) signals, for example. This is a very important factor for people with neurological disorders who are recovering from motion paralysis (e.g., post-stroke patients) because stimulating motion intentions and the corresponding actual movements simultaneously can enhance the re-connections of damaged neural systems.\\\"\",\"188\":null,\"189\":\"'The size of the image is repeatedly reduced by a factor of 2 in the encoder path to increase the receptive field of the filters. Consequently, they become invariant to tiny deformations of the road markings and are able to take contextual information and long-range interactions into account. The decoding path is identical to the encoding path except that the feature maps are now repeatedly upsampled to generate an output image of the same resolution as the input. The upsampling is performed with trainable filters. Skip connections concatenate high-resolution features from the encoding path to the decoding path, so that fast convergence is ensured and a fine-grained segmentation output can be achieved. We modified the original U-Net to include batch normalization after every convolutional filter, and added zero-padding to the sides so that the output resolution is equivalent to the input resolution.'\",\"190\":\"'https:\\/\\/matterport.com\\/'\\n\\n'Table I: Used subset of places 365 scene types. Classes in regular font were merged into the color-coded ones.'\",\"191\":null,\"192\":\"'We address semantic segmentation of road-objects from 3D LiDAR point clouds. In particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and cyclists. We formulate this problem as a point-wise classification problem, and propose an end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then refined by a conditional random field (CRF) implemented as a recurrent layer. Instance-level labels are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point clouds from the KITTI [1] dataset, and our point-wise segmentation labels are derived from 3D bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime (8.7\\u00b10.5 ms per frame), highly desirable for autonomous driving. Furthermore, additionally training on synthesized data boosts validation accuracy on real-world data. Our source code is open-source released 1 . The paper is accompanied by a video 2 containing a high level introduction and demonstrations of this work.'\\n\\n'It is worth noting that GTA-V used very simple physical models for pedestrians, often reducing people to cylinders. In addition, GTA-V does not encode a separate category for cyclists, instead marking people and vehicles separately on all accounts. For these reasons, we decided to focus on the \\u201ccar\\u201d class for KITTI evaluation when training with our synthesized dataset.'\",\"193\":\"'We adopt a convolutional encoder-multi-decoder network architecture to predict both disparity and ephemerality masks from a single image, as illustrated in Fig. 5, by adding an additional decoder to the architecture in [21].'\",\"194\":null,\"195\":\"'An example of semantic-agnostic object segmentation. Top row: Input colour, raw depth and inpainted depth images. Bottom row: Predicted boundary map and segmentations at two different scales. Different colours encode different object candidate regions, rather than semantic classes.'\",\"196\":null,\"197\":null,\"198\":\"'http:\\/\\/hourdakisemmanouil.com\\/GTMconcept.mp4'\\n\\n'http:\\/\\/www.entiment.eu\\/'\",\"199\":null,\"200\":null,\"201\":\"\\\"To analyze the pair-wise comparisons between the same motion sequence re-targeted with and without optimization, we coded participants' responses as follows: +1 for responses considering the video showing the optimized condition to be more natural\\/in sync, \\u22121 for responses favoring the control condition, and 0 when participants considered both videos to be equally natural\\/in sync. We then averaged all the 6 responses of each participant to obtain two continuous variables ranging between \\u22121 and 1 that represented a participant's preferential method in our two measures of interest, naturalness and synchrony. One-sample t-tests were run to determine whether one of the conditions (optimized or control) was significantly different to the neutral option of both videos appearing equally natural\\/in sync, defined as a score of 0. The mean naturalness score (\\\\nM=.49, SD=.36\\\\n) was significantly higher than the neutral score,\\\\nt(37)=8.38, p<.0005\\\\n, such that participants found the motion resultant from the proposed optimization approach to be more natural than the control condition. A similar trend was found in the synchrony measure, where the mean synchrony score (\\\\nM=.49, SD=.37\\\\n) was significantly different than the neutral score,\\\\nt(37)=8.29, p<.0005\\\\n, in the direction of the optimized condition.\\\"\",\"202\":null,\"203\":\"\\\"The scanned playdough model is a smaller mock-up of the desired structure where the user has to specify the size of the desired final structure. In other words, the user is giving only the shape information with the playdough and the final size of the desired object is another parameter. There could be a hard-coded constant gain to scale up the 3D model to get the real-life object size. However, making an exactly scaled model means more effort for the user since removing or adding material to change the scale of the sculpture may not be simple. Additionally, the user may want to change the size of the object after some use. In such scenarios, having an adjustable parameter\\\\n\\u03ba\\\\neases the user's role. In this paper,\\\\n\\u03ba\\\\nis assumed to be the maximum length of the object along any x-y-z axes and it can be set on the fly. We envision that the user can use the GUI or a physical object such as knob or slider bar to define the scale. In summary,\\\\n\\u03ba\\\\nis the only user parameter other than the playdough shape in the proposed interface. Fig. 3b shows the scaled model with\\\\n\\u03ba=420mm\\\\n.\\\"\",\"204\":\"'As increasing attention is paid on human action recognition from skeleton data, this paper focuses on such tasks by proposing a hierarchical model to discover the structure information of body-parts involved in human actions. Considering human actions as simultaneous motions of different body-parts of the human skeleton, we propose a hierarchical model to simultaneously apply discriminative body-parts selection at a same scale and group coupling of bundles of body-parts at different scales, while we decompose the human skeleton into a hierarchy of body-parts of varying scales. To represent such hierarchy of body-parts, we accordingly build a hierarchical RRV (Rotation and Relative Velocity) descriptors. The hierarchical representations encoded by Fisher vectors of the hierarchical RRV descriptors are properly formulated into the hierarchical model via the proposed hierarchical mixed norm, to apply sparse selection of body-parts and regularize the structure of such hierarchy of body-parts. The extensive evaluations on three challenging datasets demonstrate the effectiveness of our proposed approach, which achieves superior performance compared to state-of-the-art results on different sizes of datasets, showing it is more widely applicable than existing approaches.'\\n\\n'In the context of our hierarchical framework, given a HRRV descriptor extracted from an action sequence, the fisher vectors of each body-part are aggregated along a temporal pyramid dimension to constitute the Fisher code. The temporal pyramid dimension is introduced to take temporal information into account, similar with the Fourier temporal pyramid approach. Specifically, we recursively partition the action into a temporal pyramid from 0 to\\\\nZ\\\\nscale, where at\\\\nz\\\\n-th scale there are\\\\n2\\\\nz\\\\nsegments in temporal dimension. We then do FV encoding on all the segments. We call these locally aggregated FVs on each segment as local FVs. FV codes\\\\nu\\\\nl,\\\\nk\\\\nl\\\\nfor each body-part is the concatenation of these local FVs from all the segments.'\",\"205\":null,\"206\":null,\"207\":\"'Our program code as well as the configurations we used for evaluations are publicly available via our web pages1 and GitHub 2.'\\n\\n'GitHub'\\n\\n'In this work we presented a real-time capable person detection approach for indoor and outdoor environments. Unlike many other solutions, it is possible to execute the complete algorithmic pipeline on a single CPU core. Our configurable modular computation pipeline achieves good results and provides high flexibility regarding 3D sensor support. Fast and robust 3D obstacle detection in combination with efficient sparse voxel grid clustering generates the input for visual person classification. In the evaluation we show feature-classifier combinations that work best for different application scenarios. Furthermore, we are able to outperform state-of-the-art person detection approaches in terms of clustering quality, classification performance and runtime. Alongside this work, we provide a large-scale dataset compilation of indoor and outdoor sequences acquired using different 3D sensors. All data, configuration files used for the evaluation and our framework are available on our web pages and via GitHub.'\",\"208\":null,\"209\":\"'https:\\/\\/goo.gl\\/REMZFB'\",\"210\":\"'https:\\/\\/github.com\\/daniilidis-group\\/drocap'\\n\\n'https:\\/\\/github.com\\/daniilidis-group\\/drocap'\\n\\n'https:\\/\\/github.com\\/daniilidis-group\\/drocap'\",\"211\":\"'We showed a first system that uses Deep Q-Networks for the specific problem of intersection handling and show that it is capable of learning exploratory behaviors to more fully understand the scene. In addition to having better task efficiency and success rates than the rule based method, we identify occlusions as a fail case for rule based methods that lack hand-coded accommodations. While networks can learn both departure times and creeping behaviors, and have some ability to generalize to novel domains and out-of-sample data, they do occasionally result in collisions. Consequently more research is required to increase robustness.'\",\"212\":null,\"213\":\"'https:\\/\\/youtu.be\\/NpLNJ5kC_G0'\\n\\n'https:\\/\\/github.com\\/AMZ-Driverless\\/fsd-resources#amz_driverless_2017'\\n\\n'The car is fitted with an Inertial Navigation System, an optical Ground Speed Sensor (GSS), a LiDAR and a self-developed visual-inertial stereo camera system. Furthermore, individual wheel speeds are determined by reading out each wheel encoder. Consumer-grade GPS is used (no differential GPS or RTK) as an absolute position sensor. Cones that mark the race-track are detected by both LiDAR and camera to create redundancy in the perception pipelines.'\\n\\n'Finally, a real-time capable ECU runs the low level controllers and low level state machine of the car. The torque vectoring and traction controllers developed for the original car are used to distribute individual torques to all 4 wheels at 200Hz. Their target is the desired throttle calculated on the master computer. The car relies on regenerative braking encoded as a negative throttle input during normal operation and the mechanical brakes are only used for emergency stops. Lastly, the ECU forwards the desired steering angle from the master computer to the internal controller of the steering actuator after a simple integrity check.'\\n\\n'To ensure code quality while keeping the validation process efficient, special attention was paid to the testing methodology and tools.'\\n\\n\\\"Git was used to efficiently code as a team and keep an integral record of the entire project history. A three-stage branching model was used to develop and validate functional components. For each new major feature, a Git branch was forked from master into the Development stage. Once the new code was written and ready, it was scheduled for testing by moving it to a branch of the Validation stage corresponding to the test type and date. After the code had been tested and proven to be stable, it was merged back into the Stable stage's only branch: master.\\\"\\n\\n'Gazebo was used in combination with a dynamic model written in python to simulate new features. If new code passed this test, it was ready to be validated on the car. The simulation also proved to be a useful tool for preliminary controller tuning.'\\n\\n'Testing the autonomous system generated a considerable amount of data from different sources. A custom web browser based tool has been developed to efficiently manage all data through one interface. The information was structured as experiments with report annotation fields and nested test runs. Each test run contains a link to the source code (Git commit hash) that was run and the data (rosbag) it generated.'\\n\\n'https:\\/\\/github.com\\/AMZ-Driverless\\/fsd-resources#amz_driverless_2017'\",\"214\":\"'https:\\/\\/youtu.be\\/iw-eXnnjfj0'\\n\\n'Prediction results for a (possibly) turning vehicle (dashed, red ellipse) before turning maneuver. The excerpt of the input grid map (bottom left) shows the cell velocity estimates with orange lines, illustrating that it is not clear at input time, if the vehicle will turn or go straight. Predicted occupancy from 0.5 s to 3.0 s is illustrated in RGB images in the bottom row. True occupancy is encoded in the red channel, learned prediction in the green, and prediction with particles in the blue channel. Overlapping results are encoded as mixed RGB color, e.g., white in static regions. Starting from 1.0 s, the multi-modality of the predicted spatial occupancy distribution is visible in the learned prediction result, while the particles only cover the straight maneuver. Also, the static edges of the wall, misclassified as dynamic, are predicted as moving by the particle approach but not by the learning approach.'\",\"215\":null,\"216\":\"'http:\\/\\/team.inria.fr\\/rits\\/drl'\\n\\n'(a) Overview of our end-to-end driving in the WRC6 rally environment. The state encoder learns the optimal control commands (steering, brake, gas, hand brake), using only 84\\u00d784 front view images and speed. The stochastic game environment is complex with realistic physics and graphics. (b) Performance on 29.6km of training tracks exhibiting a wide variety of appearance, physics, and road layout. The agent learned to drive at 73km\\/h and to take sharp turns and hairpin bends with few crashes (drawn in yellow).'\\n\\n'In addition to its top performance, the choice of this A3C algorithm is justified because of its ability to train small image encoder CNNs without any need of experience replay for decorrelation. This allows training in different environments simultaneously, which is useful for our particular case as the WRC6 environment is undeterministic.'\\n\\n'B. State Encoder'\\n\\n'The state encoder CNN+LSTM architecture used in our approach (2a). Compared to the one used in [13] (2b), our network is slightly deeper and comes with a dense filtering for finer feature extraction and far-away vision.'\\n\\n'Preliminary research highlighted that naively training an A3C algorithm with a given state encoder does not reach optimal performances. In fact, we found that control, reward shaping, and agent initialization are crucial for optimal end-to-end driving. Although the literature somewhat details control and reward shaping, the last one is completely neglected but of high importance.'\\n\\n'The softmax layer of the policy network outputs the probabilities of the 32 control classes given the state encoded by the CNN and LSTM.'\\n\\n'To evaluate our contribution as compared to the state of the art we evaluate separately the proposed choices of state encoder, reward shaping and respawn strategies. The study of each factor is carried out by retraining the whole network while only changing the element of study.'\\n\\n'1) State Encoder'\\n\\n'Evaluation of our state encoder versus the encoder used in mnih et al. [13]. In 6a, the smaller CNN from [13] (cyan) converges faster than ours (orange). In 6b the racing performance of both networks are comparable though slightly more exploratory with our network, as highlighted at location A and b. Refer to section IV-C.1 for details.'\\n\\n'To summarize, this comparative evaluation showed that our state encoder, respawn strategy and reward shaping improve greatly the end-to-end driving performance. Comparison of discrete versus continuous control was also conducted but not reported as they exhibit similar performances.'\",\"217\":null,\"218\":null,\"219\":null,\"220\":null,\"221\":null,\"222\":\"'Finally, the computation time at each iteration of our algorithm for on-line optimal sensing control, implemented in MATLAB\\u00ae\\/Simulink\\u00ae with not fully optimized code, is around 23 ms on an Intel Core i7-6600U running at 2.60 GHz. This confirm the possibility of a real-time implementation.'\",\"223\":\"'Encoderless Gimbal Calibration of Dynamic Multi-Camera Clusters'\\n\\n'Dynamic Camera Clusters (DCCs) are multi-camera systems where one or more cameras are mounted on actuated mechanisms such as a gimbal. Existing methods for DCC calibration rely on joint angle measurements to resolve the time-varying transformation between the dynamic and static camera. This information is usually provided by motor encoders, however, joint angle measurements are not always readily available on off-the-shelf mechanisms. In this paper, we present an encoderless approach for DCC calibration which simultaneously estimates the kinematic parameters of the transformation chain as well as the unknown joint angles. We also demonstrate the integration of an encoderless gimbal mechanism with a state-of-the art VIO algorithm, and show the extensions required in order to perform simultaneous online estimation of the joint angles and vehicle localization state. The proposed calibration approach is validated both in simulation and on a physical DCC composed of a 2-DOF gimbal mounted on a UAV. Finally, we show the experimental results of the calibrated mechanism integrated into the OKVIS VIO package, and demonstrate successful online joint angle estimation while maintaining localization accuracy that is comparable to a standard static multi-camera configuration.'\\n\\n'encoderless'\\n\\n\\\"Existing encoderless approaches found in literature mainly attempt to perform joint motor control without the use of velocity and position sensors mounted on the motor shaft. Kawamura et al. [14] propose augmenting the motor control signal of a manipulator joint with a high frequency component, then measuring the changes in the motor's back-EMF. This approach estimates the joint angle positions through the use of a known dynamic model for the motor and external measurements of the end-effector position. Baik et al. [15] suggest a method to perform motor angle estimation of a Switched Reluctance Motor (SRM) using a neural network. Using a temporary external reference system, the system is trained to estimate the motor angle using the measured motor flux and current as inputs to the network. Since a typical off-the-shelf gimbal system does not provide measurements of the motor back-EMF and motor flux, these existing approaches are not suitable to apply to our joint angle estimation problem. The work of Kormushev et al. [16] demonstrates a learning algorithm capable of positioning the end-effector of a manipulator without the knowledge of joint angles or the robot's kinematic parameters. By generating random control signals and observing the end-effector, they show that the local kinodynamics of the manipulator can be approximated and the required control signal for end effector positioning can be estimated. Although their approach is able to perform accurate positioning of the end-effector without joint angle measurements, the proposed learning algorithm requires occasional exploratory motions of the end-effector in order to calculate the local kinodynamics. Application of the learning algorithm to our gimbal joint angle estimation problem is impractical, as the required exploratory motions would interrupt the calibration or localization process.\\\"\\n\\n'A. DCC Calibration with Encoder Feedback'\\n\\n'B. DCC Calibration Without Encoder Feedback'\\n\\n'In order to calibrate the DCC without the use of encoder feedback, the angles are added to the optimization as part of the estimated parameters. As a result, the size of the estimation parameters increases by\\\\nL\\\\nwith each new measurement. In addition to estimating the kinematic parameters,\\\\n\\u0398\\\\n, we now simultaneously estimate the joint angles for each measurement configuration. We denote the estimated joint angles for the\\\\ni\\\\nth\\\\nmeasurement set as\\\\n\\u03b2\\\\ni\\\\n=[\\\\n\\u03b8\\\\ni\\\\n1\\\\n \\u22ef \\\\n\\u03b8\\\\ni\\\\nL\\\\n]\\\\n. For the\\\\ni\\\\nth\\\\nmeasurement set, we can write the full transformation from the static to the dynamic camera as'\\n\\n'To validate our proposed approach, we perform two sets of experiments. In the first set, we demonstrate the successful encoderless calibration of a 2-DOF gimbal both in simulation and on physical hardware. In the second set of experiments, we perform visual inertial odometry using the calibrated gimbal, and show that our gimballed DCC VIO configuration performs comparably to a standard SCC setup.'\\n\\n'A. Encoderless Gimbal DCC Calibration'\\n\\n'Simulation: The encoderless calibration approach outlined in Section IV-B is first validated in simulation. We generate a DCC consisting of a 2-DOF gimbal and collect synthetic measurements of a 9\\u00d77 chessboard fiducial target. Zero-mean error with 0.4 pixel standard deviation is added to the pixel measurements to simulate real-world image noise. To perform the calibration, we collect a calibration set of 81 independent image pairs from a wide range of gimbal configurations. In order to ensure sufficient configuration excitation for the calibration, we collect measurement sets using approximate uniform sampling of the 2-DOF joint angle space. An approach similar to [6] would optimize the required number of images to achieve adequate coverage of the configuration space. Prior to optimization, we initialize the kinematic and joint angle estimates with noisy measurements of the true value; this error is characterized as zero-mean noise with a standard deviation of 0.03 m and 10 degrees for the translation and rotation parameters, respectively.'\\n\\n'Encoderless calibration of the gimballed DCC is performed using the method outlined in Section IV-B. To perform calibration, a calibration set of 83 independent image pairs is collected. Similar to the simulation study, the gimbal configurations are sampled uniformly over the 2-DOF joint angle space. An AprilGrid is used as the fiducial target for calibration, as accurate detection of this target is more robust to large viewpoint changes and partial target observations in comparison to a chessboard [29]. For validation of the gimbal calibration, an independent verification set of 70 image pairs is collected, also using the AprilGrid fiducial target. For both the calibration and validation set, we ensure that there is overlap of the fiducial target between the static and gimbal camera. Initialization of the joint angle values are provided by the gimbal IMU that is used for camera stabilization. Due to the gimbal geometry, the estimated roll and pitch values reported by the gimbal IMU roughly correspond to the gimbal joint angle rotation. The results for the calibration and verification sets are summarized in Table IV.'\\n\\n'Fig. 5 compares the estimated gimbal joint angles to the values obtained using encoders mounted on the gimbal for ground truth collection, and Table VI reports the RMSE of the estimated joint angles. We see that as the gimbal changes configuration, our extended VIO algorithm is able to accurately estimate the joint angles. It is important to state that no motion model or input from the gimbal IMU was used for the estimation, and only visual data from the camera images were used. A component of the errors reported in Table VI are due to a noticeable time lag in the joint angle estimation, which can be improved through hardware synchronization in the DCC.'\\n\\n'This work presents a method to perform encoderless calibration of a dynamic camera cluster. Our approach simultaneously estimates kinematic parameters of the DCC, as well as the joint angles of the mechanism for each measurement of a fiducial target. We demonstrate successful calibration of a 2-DOF gimbal DCC both in simulation and using physical hardware. The calibrated values are then used to perform VIO on a custom quadrotor, using an extended implementation of OKVIS which also performs online estimation of the gimbal joint angles. We demonstrate that the extended VIO is able to successfully estimate the joint angles, and that it performs comparably to a VIO solution using a standard static camera configuration. Future work will include testing on a wider range of actuated mechanisms, degeneracy analysis of both the encoderless calibration and online gimbal joint angle estimation, as well as incorporating the calibration into an active Visual SLAM framework.'\\n\\n'Estimated gimbal joint angles compared to ground truth provided by the gimbal encoders.'\",\"224\":null,\"225\":null,\"226\":\"'We present a deep convolutional neural network (CNN) architecture for high-precision depth estimation by jointly utilizing sparse 3D LiDAR and dense stereo depth information. In this network, the complementary characteristics of sparse 3D LiDAR and dense stereo depth are simultaneously encoded in a boosting manner. Tailored to the LiDAR and stereo fusion problem, the proposed network differs from previous CNNs in the incorporation of a compact convolution module, which can be deployed with the constraints of mobile devices. As training data for the LiDAR and stereo fusion is rather limited, we introduce a simple yet effective approach for reproducing the raw KITTI dataset. The raw LiDAR scans are augmented by adapting an off-the-shelf stereo algorithm and a confidence measure. We evaluate the proposed network on the KITTI benchmark and data collected by our multi-sensor acquisition system. Experiments demonstrate that the proposed network generalizes across datasets and is significantly more accurate than various baseline approaches.'\\n\\n'Our overall network consists of two cascade sub-networks, including LiDAR-Stereo fusion and refinement. Our architecture design is inspired by two intuitions that: 1) the 3D LiDAR disparity and stereo disparity encode different aspects of 3D geometric configuration, such that information about one provides complementary cues that can assist to reconstruct high-precision disparity information, and 2) the guidance of color information can be utilized to boost the disparity estimation performance.'\\n\\n'We presented the CNN architecture for high-precision depth estimation. Our network started from the fusion process to encode the complementary characteristics of sparse 3D LiDAR and dense stereo depth in a boosting manner. Our network has also employed the compact convolution module that can be applied to various real time systems. To the best of our knowledge, this network is the first CNN model specifically designed for LiDAR and stereo depth fusion. We also built large-scale datasets using the KITTI raw LiDAR data, and augmented the raw LiDAR scans by adapting off-the-shelf stereo algorithm and confidence measure. We also collected data by our own multi-sensor acquisition system, and demonstrated that our network outperforms the state-of-the-art algorithms. In future work, our network can benefit from the incorporation of additional stereo matching network by improving the input depth information.'\",\"227\":null,\"228\":\"'https:\\/\\/www.acin.tuwien.ac.at\\/en\\/vision-for-robotics\\/software-tools\\/autonomous-robot-indoor-dataset\\/'\\n\\n'https:\\/\\/www.acin.tuwien.ac.at\\/en\\/vision-for-robotics\\/software-tools\\/autonomous-robot-indoor-dataset\\/'\",\"229\":null,\"230\":null,\"231\":\"'As noted earlier, we perform a search on test points located all over the scene due to the fact that we want to remain agnostic about complex features on objects or surfaces in the scene that enable the affordance. In order to perform this search efficiently we implemented the most computationally expensive parts of our code on a GPU. With this implementation we can perform nearest-neighbor search, vectors comparison and scores computation for all orientations in 10 ms on average using a NVIDIA Titan X GPU.'\",\"232\":null,\"233\":null,\"234\":null,\"235\":\"'Since our specific task of crop vs. weed classification has a much narrower target space compared to the general architectures designed for 1000+ classes, we can design an architecture which meets the targeted speed and efficiency. We propose an end-to-end encoder-decoder semantic segmentation network, see Fig. 4, that can accurately perform the pixel-wise prediction task while running at 20+ Hz, can be trained end-to-end with a moderate size training dataset, and has less than 30, 000 parameters. We design the architecture taking some design cues from Segnet [1] and Enet [17] into account and adapt them to the task at hand. Our network is based on the following building blocks:'\\n\\n'Detailed encoder-decoder architecture used for the pixel-wise semantic segmentation. Best viewed in color.'\\n\\n'The unpooling operations in the decoder are performed sharing the pooling indexes of the symmetrical pooling operation in the encoder part of the network. This allows the network to maintain the information about the spatial positions of the maximum activations on the encoder part without the need for transposed convolutions, which are comparably expensive to execute. Therefore, after each unpooling operation, we obtain a sparse feature map, which the subsequent convolutional layers learn how to densify. All pooling layers are [2 \\u00d7 2] with stride 2.'\\n\\n'One way to analyze the generalization performance of the approach is to analyze the amount of data that needs to be labeled in a new field for the classifier to achieve state-of-the-art performance. For this, we separate the Zurich and Stuttgart datasets in halves, and we keep 50% of it for testing. From each of the remaining 50%, we extract sets of 10, 20, 50 and 100 images, and we retrain the last layer of the network trained in Bonn, using the convolutional layers in the encoder and the decoder as a feature extractor. We further separate this small sub-samples in 80%-20% for training and validation and we train until convergence, using early stopping, which means that we stop training when the validation error starts to increase. This is to provide an automated approach to the retraining, so that it can be done without supervision of an expert.'\\n\\n'In this paper, we presented an approach to pixel-wise semantic segmentation of crop fields identifying crops, weeds, and background in real-time solely from RGB data. We proposed a deep encoder-decoder CNN for semantic segmentation that is fed with a 14-channel image storing vegetation indexes and other information that in the past has been used to solve crop-weed classification tasks. By feeding this additional, task-relevant background knowledge to the network, we can speed up training and improve the generalization capabilities on new crop fields, especially if the amount of training data is limited. We implemented and thoroughly evaluated our system on a real agricultural robot operating using data from three different cities in Germany and Switzerland. Our results suggest that our system generalizes well, can operate at around 20 Hz, and is suitable for online operation in the fields.'\",\"236\":null,\"237\":null,\"238\":null,\"239\":\"\\\"The artificial finger consists of 3D printed parts, torsion springs, and encoders(Fig. 7). Torsion springs are installed on all three joints to mimic hand spasticity. A proportion of the spring constants is 3.5:76.9:54.9 for the DIP, PIP, and, MCP joint respectively, which is similar to 0.03:0.66:0.46 from Cruz and Kamper's work [20]. Hall effect rotary encoders(AS5600) are placed on the side of the PIP and MCP joint. The DIP joint, which is coupled with the PIP joint is excluded from the measurements for the sake of simplicity. Since four fingers other than the thumb have similar structures and exhibit identical movements experiments with one fingered device should suffice.\\\"\",\"240\":\"\\\"The 12 actuated cables of the superball are color-coded and mapped to nodes of the CPG architecture. Locomotion is achieved by periodically contracting and expanding the lengths of the longest actuation chain (orange\\/center ring). This action shifts the system's com and causes it to \\u201croll\\u201d onto the next surface. Steering is achieved by contracting the cables on the system's sides (green, blue), which causes a lateral motion. In light gray: The nodes and interconnections of the cpg. In black: The free parameters of the reduced-dimensionality CPG control, and the nodes they affect.\\\"\\n\\n\\\"The efficiency of the presented framework (BOkNN) is compared against: Monte Carlo sampling of the CPG parameters (MC), kNN classification-based region biasing (kNN) and multi-objective Bayesian optimization (BO). All algorithms were executed for 10 repeated trials with different random seeds for 6, 000 seconds per trial. The simulation was performed using NTRT, an open source tensegrity simulation software package developed at NASA, which has been experimentally validated using the SUPERBall robot [4]. Initial experiments were executed for even longer durations but it was observed that results largely leveled-off prior to 6, 000 seconds. Each method was given the objective of maximizing the velocities of 9 individual gaits with angular velocities consisting of a non-overlapping discretization of the range\\\\n[\\u2212\\\\n10\\\\no\\\\n\\/s, \\\\n10\\\\no\\\\n\\/s]\\\\n, and was correspondingly allowed 9 parallel processes for each trial. Every 25 iterations the best gaits for each objective in terms of the velocity of SUPERBall's CoM found up to that point were recorded. To aggregate and compare results from the repeated trials, a running average was computed from the 10 runs for each of the methods.\\\"\",\"241\":null,\"242\":null,\"243\":\"'https:\\/\\/youtu.be\\/5zelvj_xPzM'\",\"244\":null,\"245\":null,\"246\":null,\"247\":\"https:\\/\\/github.com\\/zswang666\\/Omni-CNN\",\"248\":\"'The task of a simple GAN is to capture the distribution of a given domain. This is achieved by training the generator (G) to map random samples from a hidden space to images of the particular domain. For the task at hand, we are interested in encoding an image into the latent space, and a normal GAN does not have the provision for it. We need a generator that can take an input image, instead of a random sample, and generates an image at the output. Therefore, we use a simple encoder-decoder network in place of a generator, where the encoder part maps the image to an encoding space, and the decoder generate the output image. Such a setup can then be used to map one domain to another by having different inputs to the encoder-decoder (source domain) and discriminator (target domain).'\\n\\n'In this work, we explored the possibility of using GAN s for the task of image translation and subsequently using the features spaces learned in the discriminator for place recognition. We show that these features encode visually similar images close in the space and can be used for place recognition using generated images in the domain of interest. In addition to using the learned features, the generated images can be used in a traditional place recognition system to get an improvement in performance.'\",\"249\":null,\"250\":null,\"251\":null,\"252\":\"'Let X encode the status of the edges of the graph, i.e.,\\\\nX\\\\ni\\\\n=1\\\\nindicates that edge\\\\ni\\\\nis selected to generate a spanning forest. Then, a number-fixed balanced connected partition problem\\\\nBC\\\\nP\\\\nq\\\\ncan be abstracted as Eq. (1):'\\n\\n'As previously mentioned, we encode a partition with a binary array X of size\\\\n|E|\\\\n. The edges in the graph are numbered as edge 1, edge 2\\u2019\\\\n\\u22ef\\\\n, edge\\\\n|E|.\\\\nX\\\\ni\\\\n=1\\\\nmeans that the edge\\\\ni\\\\nis presented in the forest.'\",\"253\":\"'In this paper we have introduced a joint behavior estimation and trajectory planning algorithm integrating the strengths of POMDP and MPC. As shown in multiple simulations with up to five vehicles, the method accounts for the uncertainty on the motion intentions of other traffic participants and produces safe trajectories for the ego-vehicle. In particular, the chance constrained multi-policy MPC planner is general and can be employed together with other behavior estimators. Furthermore, the method scales well with the number of obstacles thanks to the parallelization of the behavior estimation and the efficiency of code generation for MPC. Future work will seek to improve the performance of the estimator and test the method on autonomous vehicles.'\",\"254\":null,\"255\":null,\"256\":null,\"257\":null,\"258\":null,\"259\":\"'Pseudo code for our EM based learning of coupled forward-inverse models is presented in Algorithm 1.'\",\"260\":\"'We propose a novel deep network that combines an autoencoder and a conditional GAN, tightly coupled with an FEM-based physics simulator. Given a single RGB-D depth image of the deformable object (e.g. from a side scan of the object to be traversed) and conditioning input which includes the type of material (e.g. aluminium), the size of the force (e.g. 50 N), and the location of the force (e.g. 10 cm), the network is able to output a predicted 3-D deformation of the solid. This prediction can then be used by a path-planner to determine which is the fastest or safest path to take, given terrain information and robot payload.'\\n\\n'Since a traditional generator from a GAN does not have the ability to map a 3-D point cloud to its latent representation, the generator is implemented as an autoencoder network\\\\nE\\\\n, that is able to learn a latent representation from input voxel grids. The encoder enables the network to explore the latent space by interpolating or making variations on it. By concatenating a condition to the latent representation, explicitly controlled variations can be made as conditional information, such as the size of the force.'\\n\\n'In order to facilitate the propagation of local structures in the input voxel grids, the autoencoder has skip connections between the encoder and the decoder. The encoder has five 3-D convolutional layers with a bank of\\\\n4\\u00d74\\u00d74\\\\nfilters with strides of\\\\n1\\u00d71\\u00d71\\\\n, followed by a leaky ReLU activation function and a max pooling layer with\\\\n2\\u00d72\\u00d72\\\\nfilters and 2 \\u00d7 2 \\u00d7 2 strides. The encoder is followed by two fully-connected layers which flatten the 3-D representation into a 1-dimensional vector representing the latent encoding. The condition is also encoded as a 1-dimensional vector. The condition encapsulates three properties: the magnitude of the force, the location of the force, and the material. Each of these properties is discretized into a range of values and represented as a one-hot vector. The condition vector is of the form (f2, a2, m2), where f2, a2, m2 are the binary representations of the discretized conditions. In our application scenario, we use 2 bits for the force, 7 for the point of application and 2 for the material. The decoder largely follows the inverse of the encoder, composed of five up-convolutional layers which are followed by ReLU activations except for the last layer which is followed by a sigmoid function.'\",\"261\":\"'Being aware of our body has great importance in our everyday life. It helps us to complete difficult tasks, such as movement in a dark room or grasping a complex object. These skills are important for robots as well, however, robotic bodily awareness is still an open question, and the nonlinearity of soft robots adds even more complexity. In this paper, we address this problem and present a novel method to implement bodily awareness into a real soft robot by the integration of its exteroceptive and proprioceptive sensors. We use an octopus-inspired arm as an example where the proprioceptive representation is approximated by four bend sensors integrated into the soft body, while a camera records the movement of the arm capturing its exteroceptive representation. The internal sensory signals are mapped to the visual information using a combination of a stacked convolutional autoencoder (CAE) and a recurrent neural network (RNN). As a result, the soft robot can learn to estimate and, therefore, to imagine its motion even when its visual sensor is not available.'\\n\\n'The system architecture and the learning process. First, we used a stacked convolutional model to reduce the dimensionality of the exteroceptive (visual) representation of the body. Both the encoder (a) and decoder (b) components had five hidden layers. (c) After training them, we mapped the temporal data received from stretch sensors to this low-dimensional representation (shown in green). Finally, by attaching the previously trained decoder component the robot could imagine its body even when the visual sensor was not available.'\\n\\n\\\"The original (a), the preprocessed (b) and the decoded (output) (c) images. The original video was recorded at 29.97 frames per second. The decoded and the preprocessed images are almost identical, which means that the CAE is able to find a low-dimensional representation of the soft arm's movement. This allows us to create a black box type of model about the arm and its environment without using complex modeling techniques.\\\"\\n\\n'We trained and tested the system using the data collected over a 600 s long (15000 time steps) measurement. We used the first 510 s (12750 frames) to train, the next 60 s (1500 frames) to test the stacked convolutional autoencoder and the last 30 s (750 frames) to validate the whole system. All the images were black and white and contained 124\\u00d760 pixels. The hidden layers had ReLu activation functions and the ADADELTA [26] algorithm was used to optimize the autoencoder. In order to measure the error of the trained model, we calculated the binary cross-entropy testing loss between the original and reconstructed images, which is defined by'\\n\\n'The encoded features predicted by the recurrent neural network are shown in Figure 7c. Mean squared error was used as loss function and its convergence is shown in Figure 7b. As shown in Figure 4, the predicted signals were fed into the previously trained decoder and the images representing the soft body could be reconstructed. We note that this was achieved using the data of the last 30 s of the measurement, that have not been used before for training. Since the recurrent neural network predicts the next value of the signal using the previous 30 data points, we used the remaining 725 to generate the plot.'\\n\\n'(a) Loss function of the convolutional autoencoder over the training. (b) Loss function of the recurrent neural network training. (c) Comparison between the simulated and predicted signals of two encoded features. The figure also shows the images reconstructed (shown in red) by our system from the bend sensor input and the original images (shown in blue). We note that these data points have not been used before for training. The root mean square error (rmse) between the two images are shown in yellow. Two identical images produce RMSE=0, two completely different images produce RMSE=l.'\\n\\n'In this paper, we implemented bodily awareness in a soft robot by the integration of exteroceptive and proprioceptive sensors. We fabricated an octopus-inspired soft robotic arm equipped with four integrated flexible bend sensors. The proprioceptive data were collected from these sensors, while for exteroceptive representation we used visual data captured by a camera. The dimensionality of the video frames was reduced by a stacked convolutional autoencoder. We connected and trained a recurrent neural network to the previously trained decoder component and mapped the temporal data received from the proprioceptive sensors of the robot to the encoded features. The predicted features were fed into the decoder and the motion of the soft arm was reconstructed. Using this system the robot could imagine its body moving.'\\n\\n'The images reconstructed by the convolutional autoen-coder are very close to the images of the real robot arm. This suggests that the complex motion of a soft body has a low dimensional representation which can be found by encoding techniques. This method can be used for complex, nonperiodic and nonlinear movements produced by soft robotic bodies. Its advantage is that not only the shape of the body but also the material properties, the excitation function and the environmental effects are captured and stored in the encoded representation.'\",\"262\":null,\"263\":null,\"264\":\"'Decentralized visual simultaneous localization and mapping (SLAM) is a powerful tool for multi-robot applications in environments where absolute positioning is not available. Being visual, it relies on cheap, lightweight and versatile cameras, and, being decentralized, it does not rely on communication to a central entity. In this work, we integrate state-of-the-art decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and optimization, existing decentralized visual SLAM systems exchange the full map data among all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: first, a compact full-image descriptor is deterministically sent to only one robot. Then, only if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-the-art decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available datasets and we provide open access to the code. Supplementary Material Data and code are at: https:\\/\\/github.com\\/uzh-rpg\\/dslam_open.'\",\"265\":null,\"266\":null,\"267\":null,\"268\":\"'We would like to thank Mladen Mazuran for his NFR source code and datasets used in our experiments as well as fruitful discussions regarding the parameterization and optimality of the pose composition approach, Philip Fong for introducing us to this interesting problem, and Christopher Jones for his support on this paper.'\",\"269\":\"'Thanks to Stefan Leutenegger, Michael Bloesch, and Tong Qin for their help in tuning OKVIS, ROVIO, and VINS-Mono, respectively. Special thanks to Kenneth Chaney, Alex Zhu, and Kostas Daniilidis for their assistance with the MSCKF code and experiments.'\\n\\n\\\"Visual-inertial odometry (VIO) is currently applied to state estimation problems in a variety of domains, including autonomous vehicles, virtual and augmented reality, and flying robots. The field has reached a level of maturity such that many commercial products now utilize proprietary VIO algorithms, and there are several open-source software packages available that offer off-the-shelf visual pipelines that can be deployed on an end-user's system of choice.\\\"\",\"270\":\"'We thank Jakob Engel for releasing the code of DSO and for his helpful comments on First Estimates Jacobians, and the authors of [21] for providing their numbers for the comparison in Fig. 7.'\",\"271\":\"'http:\\/\\/cvrs.whu.edu.cn\\/projects\\/Struct-PL-SLAM\\/'\\n\\n'The structural features in Manhattan world encode useful geometric information of parallelism, orthogonality and\\/or coplanarity in the scene. By fully exploiting these st...'\\n\\n'The structural features in Manhattan world encode useful geometric information of parallelism, orthogonality and\\/or coplanarity in the scene. By fully exploiting these structural features, we propose a novel monocular SLAM system which provides accurate estimation of camera poses and 3D map. The foremost contribution of the proposed system is a structural feature-based optimization module which contains three novel optimization strategies. First, a rotation optimization strategy using the parallelism and orthogonality of 3D lines is presented. We propose a global binding method to compute an accurate estimation of the absolute rotation of the camera. Then we propose an approach for calculating the relative rotation to further refine the absolute rotation. Second, a translation optimization strategy leveraging coplanarity is proposed. Coplanar features are effectively identified, and we leverage them by a unified model handling both points and lines to calculate the relative translation, and then the optimal absolute translation. Third, a 3D line optimization strategy utilizing parallelism, orthogonality and coplanarity simultaneously is proposed to obtain an accurate 3D map consisting of structural line segments with low computational complexity. Experiments in man-made environments have demonstrated that the proposed system outperforms existing state-of-the-art monocular SLAM systems in terms of accuracy and robustness.'\\n\\n'Lines encode more structural information than points, so we express the 3D environment using a \\u201cstructural map\\u201d which consists of 3D structural lines. We use Plucker matrix [14] to represent a 3D line\\\\nL\\\\nm\\\\nas'\\n\\n'To demonstrate the performance of the proposed structural feature-based SLAM system, we conduct experiments on both synthesized data and real image sequence. We compare our methods with existing state-of-the-art approaches in terms of accuracy and efficiency. Additional experimental results and source code are available at http:\\/\\/cvrs.whu.edu.cn\\/projects\\/Struct-PL-SLAM\\/.'\",\"272\":\"'A) initialization of VSEP in a saliency-enhanced mockup environment, 3D pointcloud and volumetric height-map, b) 1stlayer: Next-best-view random sampling for pure volumetric exploration, c) 2ndlayer: Saliency-aware resampling in local configuration space, robot deviates and directs its attention towards visually salient regions (voxels on the right) while respecting mission time-constraints, d) saliency-encoded octomap with inhibition of return: At a later mission step, saliency regions that were observed first (paintings on the right) have become inhibited, while the latest discovered ones (painting and colored box at the far end) are still salient, e) pointcloud representation of previous phase and sample saliency image frame, f) mission completion: Post-exploration phase focuses on the last upper side regions that have not become inhibited yet, g) pointcloud at 2ndlayer snapshot: Part of the salient regions are in fact a human (mannequin) standing in front of a wall painting.'\\n\\n'In order to evaluate the proposed Visual Saliency-aware Exploration Planner (VSEP) prior to its experimental verification, multiple simulation studies were conducted. The open-source RotorS Simulator [28] was employed, as it can provide both state feedback as well as the necessary onboard localization and mapping information. A hexacopter aerial robot equipped with a stereo camera providing raw and depth images\\\\n([\\\\na\\\\nv\\\\n,\\\\na\\\\nh\\\\n]=[60,90\\\\n]\\\\n\\u2218\\\\n,\\\\n\\u03b7\\\\nm\\\\n=\\\\n15\\\\n\\u2218\\\\ndownward) was simulated. Environments of varying density of salient entities (paintings, furniture and a human) were considered. All environments are initially unknown to the robot.'\\n\\n\\\"The proposed saliency-aware exploration strategy is evaluated both in simulation, as well as in challenging experimental studies. Computational complexity analysis is further provided. All experiments were conducted indoors with the robot relying only on onboard visual-inertial localization. The planned paths, the derived saliency-based volumetric maps, and metrics demonstrating the increased focus of the robot's attention towards salient areas are presented. The algorithm implementation is released as an open source package accompanied with a relevant dataset [12].\\\"\",\"273\":\"'https:\\/\\/youtu.be\\/761zxZMeQNo'\",\"274\":null,\"275\":\"'https:\\/\\/skfb.ly\\/sq7j'\",\"276\":null,\"277\":null,\"278\":null,\"279\":null,\"280\":null,\"281\":null,\"282\":\"'https:\\/\\/www.youtube.com\\/watch?v=LwUEuM8vSSs'\",\"283\":null,\"284\":\"\\\"This corresponds to a linear assignment problem and can be solved using the Jonker-Volgenant algorithm that needs expensive computations, especially for the singular value decompositions. In the application of learning shared representations of vision and tactile sensing, the dimension of learned features required to encode the rich information in camera and GelSight images is in the order of 103, for example, we have\\\\nD=4,096\\\\nfor the hidden representations of camera images. To make DWCA practically applicable to our application, we implement both the feature learning and MCA phases on a GPU with the CUBLAS and CUSOLVER libraries distributed as part of NVIDIA's CUDA Programming Toolkit1 to compute linear algebra subroutines.\\\"\",\"285\":null,\"286\":null,\"287\":\"'Due to the nonlinear cable stretch and motor-side encoders on our robot, our robot has poor accuracy (1.5cm) as well as significantly different accuracies for different regions inside the robots workspace. Simply executing a sequence of skills will tend to result in successful replays if they are executed at the same pose as the demonstrations but failed replays if they are executed at different robot poses as the demonstrations. A sensory model for monitoring the progress of each motor primitive is therefore essential to achieve robust execution performance. The sensory model is used to confirm whether a sensory subgoal of the task has been reached by the end of each skill.'\",\"288\":\"'The magnetic encoders (As5045 from Austria microsystem,\\\\n\\u00b10.01\\\\ndeg) are assembled in a similar manner on each joint, as shown in the detailed view in Figure 3, and are connected to each other and to the boards through standard FFC connectors. A separate board is used for each finger, and all five of them are located in the box, which is assembled to the base representing the starting point of all five kinematic chains. All the electronic boards used in the device were developed under the Natural Machine Motion Initiative (more information is available in [31]).'\\n\\n'An InvenSense MPU-9250 IMU is placed on the box, connected to its own board. The five encoder boards are connected in daisy chain and are then plugged to a laptop through a USB cable; the IMU board is instead connected directly to a laptop through USB. Finally, the ATI nano 17 Force\\/Torque sensors [32] used by the ThimbleSenses are connected to a switch through ethernet and data from them is fed to the laptop by an ethernet cable. The overall sampling frequency of the system is 12 Hz. The structural components of the exoskeleton are all 3D printed and built in ABS.'\\n\\n'To have correct readings from the encoders a calibration procedure is needed. This is obtained using of a suitable calibration structure (Figure 4a), which can also be employed to store the exoskeleton safely when it is not being used.'\\n\\n'Proper reference for each end effector is obtained through supports placed on the structure, each having a slot corresponding to a negative of the inner shell of a ThimbleSense. To ensure proper placement of the shells on the structure, blocking plates are assembled on each inner shell through their open slots, and they are fixed on the calibration structure with screws, blocking the system completely. Calibration is then performed by setting the zeros of the encoders on each board. Figure 4 shows the physical prototype assembled on the calibration structure: this is the zero configuration of ExoSense, which will be used as a reference for all data acquisitions.'\\n\\n'The encoder readings were fed to the Blender game engine which calculated the forward kinematic to visualize the movement, while the data from the IMU was used in a passive complementary filter [35] to estimate orientation of the base, which is used for visualization purposes and to know the direction of gravity. Knowledge of the forward kinematics makes it possible to know position and orientation of each ThimbleSense, which are rigidly attached to the end effectors of each finger. Position of contacts on the ThimbleSense shells is also estimated by using the intrinsic tactile sensing algorithm described in [13], and contact centroids as well as forces are visualized.'\\n\\n'The second phase of the validation aims to evaluate the overall performance of ExoSense when it comes to measuring grasp, considering all measurements (encoder readings, orientation estimation from the IMU, force\\/torque measurements and contact point estimation) simultaneously. We followed a procedure similar to the one used in [12], relying on the framework presented in [6].'\",\"289\":\"\\\"This paper introduces an approach that robotizes an ankle-foot orthosis (AFO). In particular, toward post-stroke gait rehabilitation, we robotize a double-bar AFO, which is widely used in rehabilitation facilities, by newly designing a modular joint, a pneumatic actuator, and a Bowden cable force-transmission system. Our modular joint system, called the Modular Exoskeletal Joint (MEJ), has a hollow shaft for simple attachment to an AFO's pivot. We designed MEJ to compactly house an encoder that is built in a bearing in a pulley. We adopted Bowden cables to transmit contraction forces from an actuator to the MEJ. As an actuation scheme, we developed the Nested-cylinder Pneumatic Artificial Muscle (NcPAM) system. Even though PAMs are mechanically compliant and lightweight, they can still generate a large force. Therefore, they can provide an ideal actuation system for exoskeletal robots. The nested-cylinder in NcPAM houses a cable-tensioning spring to properly maintain small cable tension for passive movements and a cable stopper to connect the PAM and the cable for properly transmitting the large force generated by PAM. We show the ankle-joint trajectory tracking performances of this integrated system using iterative learning control.\\\"\\n\\n'We designed the MEJ so that all necessary components, including a pulley, a Bowden cable holder, a cable stopper, rotational\\/thrust bearings, and an encoder, could be compactly contained. In addition, we tried to design the MEJ in such a way that the entire system would be lightweight.'\\n\\n'Figure 4 illustrates the connection mechanism of the exoskeletal joint and the double-bar AFO with a pulley and a Bowden cable. The MEJ contains a thrust bearing to support the axial load due to the large force transmitted by the Bowden cable. The encoder and the reflective code wheel are placed inside the thrust bearing. The outer housing of the MEJ has grooves to guide the Bowden cable.'\\n\\n'Connection mechanism of exoskeletal joint and double-bar AFO: (a) Exoskeletal joint is attached to double-bar afo. (b) Internal structure of exoskeletal joint: MEJ contains thrust bearing to support axial load due to large force transmitted by bowden cable. Encoder and reflective code wheel are placed inside thrust bearing.'\",\"290\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/30598854'\\n\\n\\\"Sensor feedback is critical for both the control and safety features of the device. Each actuator has one Optical Quadrature Encoder with 4096 cycles per revolution, US Digital, Washington, USA. Fixed to the motor shaft, the encoder sends motor position data to the motor driver and system controller. Once at the controller, this data is multiplied by the transmission reduction ratio for position and velocity feedback. Although only one encoder was used per actuator, the leg's design allows for a second encoder to be used at each actuator output, allowing direct joint measurements for position and velocity feedback. For this reason, some renderings show two encoders per actuator. Additionally, both motors contain two Pt1000 thermistors embedded in the stator. These monitor the internal temperature of the stator to ensure that the motor is not damaged during use. A M3564F 6-axis load cell, Sunrise Instruments, Nanning, China, located below the ankle joint axis, provides force and moment information usable for ground detection and reaction forces during gait. It is capable of reading 2500 N\\/200 Nm along the x and y axes and 5000 N\\/100 Nm along the z axis.\\\"\\n\\n\\\"CAD design of the knee actuator. The exploded view on the left displays the components\\/sub-assemblies of the knee actuator, such as the upper\\/lower hinges, encoders, transmission, motor, and pylon. The image on the right presents the assembled knee actuator. The pyramid adapter on top connects to the user's socket, and the length-adjustable pylon on bottom connects to the ankle actuator module.\\\"\",\"291\":null,\"292\":\"'ATALANTE is a lower-limb exoskeleton designed by the French startup company Wandercraft. It is intended to be used by paraplegics in medical center settings for rehabilitation. The exoskeleton has 12 degrees of freedom (6 per leg). Except for the ankle, where a special mechanism is mounted, each degree of freedom is independently actuated by a brushless DC motor. A digital encoder is mounted on each motor to estimate joint position and velocity. Sensors installed in the feet allow for impact detection when the feet hit the ground. The exoskeleton is controlled by a central computer board running a real-time operating system and in charge of all high-level computations. The dimensions of ATALANTE can be easily adjusted to fit the patient through mechanical adjustments. The patient is secured to the exoskeleton by means of fasteners located at the ankle, the shin, the thigh, and the abdomen. ATALANTE is self-powered with a battery pack.'\\n\\n\\\"Embedded systems structure. The exoskeleton is controlled by a central computer board running a real-time operating system and in charge of all high level computations. This central computer then sends commands to motor controllers that handle the low-level operation of brushless motors. Digital encoders are present on all the joints of the exoskeleton. IMU's are located in different parts of the exoskeleton allowing for a better estimation of the pose of the robot. Furthermore, sensors are present in the feet and provide a measurement of the center-of-pressure during walking.\\\"\",\"293\":null,\"294\":null,\"295\":null,\"296\":\"'We do not list all of the specific details and parameters in the implementation of the SCAs. However, all the codes used to run the numerical tests are available upon request by contacting the corresponding author.'\",\"297\":null,\"298\":\"\\\"In MLN, rules are expressed as first-order formulas with an associated weight which represents the degree to which the formula will be true. Formulas with a large weight(often > 10) are considered hard formulas and are always true. Soft Formulas, in contrast, are more or less likely to be true depending on the value of their weight. These formulas are hand-coded, though structure learning techniques exist, in our view, we do not have enough hindsight about the relevance of the produced rules. We formulate the model's formulas as follows.\\\"\",\"299\":null,\"300\":null,\"301\":null,\"302\":\"\\\"In this paper, we consider motion as a means of sending messages between robots. We focus on a scenario in which a message is encoded in a sending robot's trajectory, and...\\\"\\n\\n\\\"In this paper, we consider motion as a means of sending messages between robots. We focus on a scenario in which a message is encoded in a sending robot's trajectory, and decoded by a receiver robot equipped with a monocular camera. The relative pose between the robots is unknown. We introduce an online Bayesian estimation algorithm based on the Multi-hypothesis Extended Kalman Filter for the receiving robot to simultaneously estimate its relative pose to the sender, and the trajectory class of the sender. The difficulty in this problem arises from the monocular vision model of the receiver and the unknown relative pose between robots, which brings inherent ambiguity into the trajectory identification, and hence the message decoding. An active vision-based control policy is derived and combined with the Bayesian estimation in order to deal with this difficulty. The policy is constructed online based on Monte Carlo Tree Search and aims at reducing the entropy over the trajectory class distribution. The algorithm has broad applications, e.g., to intent modeling and motion prediction for autonomous driving and autonomous drone operations. Simulation results demonstrate that the proposed estimation algorithm and the control policy result in an accurate trajectory classification.\\\"\\n\\n'Robots that interact with one another are often assumed to communicate over a wireless network. However, in many instances a wireless network is not available, or cannot be relied upon. In this paper we consider motion-based communication, in which a sender robot encodes a message in its trajectory, which is decoded by a receiver robot. The receiving robot has only a monocular camera with which to observe the motion, and the relative pose between the two robots is unknown. This can make the identification of the trajectory difficult or impossible without an active perception strategy. We present a Bayesian estimation algorithm by which the receiving robot classifies the trajectory from the sending robot, and simultaneously estimates the relative pose between the two robots. We also propose an active perception algorithm based on Monte Carlo Tree Search with Double Progressive Widening. The search is performed in the belief space and drives the receiving robot to move so that it can disambiguate between similar trajectory classes. We use entropy as the measure of uncertainty in the trajectory class distribution. We present theoretical results as well as experimental results evaluated in a realistic simulation environment.'\\n\\n'(Left) A sample 2D trajectory codebook with potential ambiguities between entries. (Right) A noisy camera projection of a complete trajectory. The trajectory may be either an \\u201cl\\u201d \\u201cor a \\u201cv\\u201d from the image. Further inference requires the receiver to move to obtain a more accurate estimate.'\\n\\n'Our estimation and control algorithm may be used for robots to literally exchange messages through their trajectories, or more generally, as an abstraction for motion classification and prediction. The sender (whether intentionally or not) sends a \\u201cmessage\\u201d with its trajectory, which the receiving robot must \\u201cdecode.\\u201d For example, an autonomous car must categorize an observed vehicle as aggressive or defensive based on its observed trajectory. Thus our algorithm has applications to autonomous driving, where vehicles have to predict the motion of other vehicles, pedestrians, and bikers by observing their motion, and to autonomous drones, which have to avoid collisions with other drones and pedestrians in their airspace by observing and predicting their trajectories. The algorithm may also model motion-based communication in animals, for example the \\u201cwaggle dance\\u201d of honey bees.'\\n\\n'The sender robot chooses a pre-specified trajectory to encode a message. The sender then executes this trajectory, while the receiver observes the motion and controls its viewing positions. In this work we let the receiver move and estimate the message simultaneously while the sender executes a single trajectory.'\\n\\n'2D\\\\ntrajectory codebook used in the experiment. The sender robot moves clockwise. The trajectories are intentionally ambiguous from different observer angles, to make trajectory classification difficult for the receiver.'\\n\\n'The trajectory codebook presented in Fig. 4 consists of three different elliptic trajectories. A trajectory is represented by 6 waypoints. The sender takes 7 seconds to transition between successive waypoints, which corresponds to a single timestep. The codebook was deliberately designed to make the classification challenging. Indeed, the prior initialization with the Levenberg-Marquardt algorithm resulted in an ambiguous classification as depicted in Fig. 5.'\",\"303\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34336368'\",\"304\":\"'https:\\/\\/bitbucket.org\\/brettlopez\\/snap'\\n\\n\\\"A second contribution of this work is the release of an open source flight stack for Qualcomm's Snapdragon Flight platform that includes a nonlinear observer, attitude controller, and motor controller interface. The code can be found at https:\\/\\/bitbucket.org\\/brettlopez\\/snap.\\\"\",\"305\":null,\"306\":\"'https:\\/\\/youtu.be\\/CbyCa7aeC7k'\\n\\n'Additional constraints can also be considered (typical state limits, obstacle avoidance, etc are supposed to be encoded in\\\\nX\\\\nand\\\\nU\\\\n).'\\n\\n'The previous developments lead to the Iterative Roadmap Expansion and Policy Approximation algorithm (IREPA). The goal of IREPA is to compute a fully connected PRM (i.e. every pair of nodes must be directly connected), together with the corresponding approximators. A consequence of this output is that the resulting steering method (guided by the approximators) can directly connect any pair of nodes in the graph. The algorithm is summarized in pseudo-code in Alg. 1.'\",\"307\":\"'We develop a general control framework where a low-level optimizer is built into the robot dynamics. This optimizer together with the robot constitute a goal directed dynamical system, controlled on a higher level. The high level command is a cost function. It can encode desired accelerations, end-effector poses, center of pressure, and other intuitive features that have been studied before. Unlike the currently popular quadratic programming framework, which comes with performance guarantees at the expense of modeling flexibility, the optimization problem we solve at each time step is non-convex and non-smooth. Nevertheless, by exploiting the unique properties of the soft-constraint physics model we have recently developed, we are able to design an efficient solver for goal directed dynamics. It is only two times slower than the forward dynamics solver, and is much faster than real time. The simulation results reveal that complex movements can be generated via greedy optimization of simple costs. This new computational infrastructure can facilitate teleoperation, feature-based control, deep learning of control policies, and trajectory optimization. It will become a standard feature in future releases of the MuJoCo simulator.'\\n\\n'Illustration of the proposed framework. A human operator or a high-level controller sends goals encoded as cost functions. An optimizer maps these instantaneous goals to control signals. The optimizer together with the robot constitute a goal directed dynamical system.'\",\"308\":null,\"309\":null,\"310\":null,\"311\":null,\"312\":null,\"313\":null,\"314\":null,\"315\":null,\"316\":\"\\\"Our experimental platform is the ROBOTIS Darwin Mini, a small humanoid robot with 16 actuated joint motors (Dynamixel XL 320). The motor encode can read the motor's present angle and angular velocity. These motors can be directly controlled by the OpenCM 9.04 C board featuring a 32-bit ARM Cortex-M3 processor. The baudrate between the board and motor is configured to be 1Mbps. This board supports the serial data transmission via Tx, Rx and Bluetooth (BT210) with a maximum baudrate of 115200bps. Two ROBOTIS touch sensors are connected to this board to monitor the status of the robot hand contact.\\\"\",\"317\":\"'Further details and a pseudo code of the 3D model-aided SIS PF can be found in [8].'\\n\\n'Both experiment 1 and 2 use the same set of parameters. Details of the grasping and 3D model-aided particle filter implementation can be found on Github1. The visual servoing gains with gain scheduling are\\\\nK\\\\ne\\\\nt,1\\\\n=0.5,\\\\nK\\\\ne\\\\nt,2\\\\n=0.25,\\\\nK\\\\ne\\\\no,1\\\\n=3.5\\\\nand\\\\nK\\\\ne\\\\no,2\\\\n=0.5\\\\n, with distance thresholds\\\\n\\u03c4\\\\ne\\\\nt\\\\n=\\\\n\\u03c4\\\\ne\\\\no\\\\n=10\\\\n[pixel]. The termination condition for visual servoing is achieved when the\\\\n\\u2113\\\\n2\\\\n-norm of\\\\ne\\\\nfalls below 1 pixel.'\",\"318\":\"'y\\\\nk\\\\ndenote quadratic objectives that encode desired motion, which we will call \\u201ctasks\\u201d.\\\\ny\\\\n\\u00a8\\\\nd\\\\nk\\\\nis the desired acceleration of the task, which can be derived from a desired trajectory or set-point of the task.\\\\nw\\\\nk\\\\nis the weight\\/soft priority of the task. The tasks can be mapped from the configuration space to any kind of \\u2018operational space\\u2019 with Jacobians, as shown in [11]. One possible mapping is with PD regulation towards a set-point:'\\n\\n\\\"Balance is encoded in a center-of-mass (CoM) task, which encourages the robot to keep its CoM above the center of its support polygon, and a CoM bound constraint, which limits\\\\np\\\\n\\u00a8\\\\ndes\\\\nCoM\\\\nto ensure that the ground projection of the robot's CoM does not go outside its support polygon. The CoM bounds are defined in hyperplane representation\\\\n{\\\\np\\\\nCoM\\\\n|A\\\\np\\\\nCoM\\\\n\\u2265b}\\\\n, and implemented as a damping behavior that slows down the CoM as it nears the boundaries of the convex hull:\\\"\",\"319\":null,\"320\":\"\\\"A humanoid robot is aware of its joint configuration thanks to its joint encoder, but there is usually no measurement on the position and the orientation of the floating base. Instead, there is often a modeled position of this configuration described in an inertial frame. This is usually obtained through a model of the environment, of the contact positions and orientations and using joint encoders similarly to a fixed-base kinematic chain. This configuration can be described by the transformation matrix between the base and an ideal frame\\\\nI\\\\n. Let's denote this matrix\\\\nI\\\\nM\\\\nb\\\\n. In the ideal case, the ideal frame and the world frame W are identical. However, even when contact positions and orientations are perfectly known, no robot is perfectly stiff and the compliance of the robot leads the real configuration to differ. A robot controller is a priori not aware of this deformation, this is why we were referring to this frame as the control frame in our former publications [1], [18], [2]. The real configuration can be described as a transformation matrix denoted\\\\nW\\\\nM\\\\nb\\\\n.\\\"\",\"321\":null,\"322\":null,\"323\":null,\"324\":\"'While the problem of inverse kinematics on serial kinematic chains is well researched, solving motion tasks quickly on more complex robots remains an open problem. Examples include dual-arm manipulation, grasping with multi-finger hands, and full-body motion generation for humanoids. In this paper, we introduce an open-source software package for ROS and MoveIt! that solves inverse kinematics and motion tasks on robots with arbitrary kinematic trees. The underlying memetic algorithm integrates evolutionary optimization, particle swarm optimization, and gradient methods. The optimization respects joint limits, effectively avoids local minima, and achieves fast convergence to accurate solutions. More importantly, the overall motion goal is specified using a set of weighted sub-goals, providing great flexibility and control of secondary objectives. Several application examples demonstrate how to combine the predefined sub-goals to achieve complex motion tasks.'\\n\\n'The ROS package described in this paper has been released as open-source; please visit.'\\n\\n'The ROS package described in this paper has been released as open-source; please visit github.com\\/TAMS-Group\\/bio_ik'\\n\\n'In this paper, we introduced the BioIK open source package for inverse kinematics, fully integrated into ROS and the MoveIt! motion planning framework. Based on a novel memetic optimization algorithm, the tool can be used as a direct replacement for existing IK solvers, but it can also be applied to more complex problems, including multi end-effector tasks on shared kinematics chains and full body posture control on humanoid robots.'\",\"325\":null,\"326\":\"'The pseudo-code for the proposed algorithm is given in Algorithm 1. Modifying AdaDelta such that the learning rate can be changed adaptively at each time step of each trial to improve the tracking performance yields'\\n\\n\\\"Fig. 3 shows the experimental setup for planar in-hand manipulation. Two fingers, each 15 cm in length, are actuated by direct-drive motors with rotary encoders. The length between the joint axes of the fingers is 20 cm. A support plane, which is tilted slightly from the vertical plane, is used to restrict the motion of the object to planar motion and to simplify the experiments. The object's posture is estimated by measuring the positions of three markers on its surface with a high-speed camera.\\\"\",\"327\":null,\"328\":null,\"329\":\"'https:\\/\\/www.dropbox.com\\/s\\/5hzjvw911xa5mye\\/kitti_3d.avi?dl=0'\\n\\n\\\"General fusion pipeline. All of the point clouds shown are in 3D, but viewed from the top (bird's eye view). The height is encoded by color, with red being the ground. A subset of points is selected based on the 2D detection. Then, a model fitting algorithm based on the generalised car models and score maps is applied to find the car points in the subset and a two-stage refinement CNN is designed to fine tune the detected 3D box and re-assign an objectiveness score to it.\\\"\\n\\n'Self-occlusion can be easily determined from the view direction. This is encoded online when doing the model fitting since view direction changes for different 3D box proposals. Negative scores are assigned to the car surface elements if they are self-occluded. Furthermore, for simplicity, only the four vertical facets are considered for self-occlusion analysis while car roof and bottom are not considered.'\",\"330\":null,\"331\":\"'https:\\/\\/youtu.be\\/NBL-hFQRh4k'\\n\\n'https:\\/\\/github.com\\/roatienza\\/densemapnet'\\n\\n'Disparity estimation is a difficult problem in stereo vision because the correspondence technique fails in images with textureless and repetitive regions. Recent body of work using deep convolutional neural networks (CNN) overcomes this problem with semantics. Most CNN implementations use an autoencoder method; stereo images are encoded, merged and finally decoded to predict the disparity map. In this paper, we present a CNN implementation inspired by dense networks to reduce the number of parameters. Furthermore, our approach takes into account semantic reasoning in disparity estimation. Our proposed network, called DenseMapNet, is compact, fast and can be trained end-to-end. DenseMapNet requires 290k parameters only and runs at 30Hz or faster on color stereo images in full resolution. Experimental results show that DenseMapNet accuracy is comparable with other significantly bigger CNN-based methods.'\\n\\n'Current CNN-based implementations are designed either to mimic the classical correspondence technique at the pixel level or to predict the disparity image from feature maps of stereo images. Pixel level correspondence is unfavorable since it generally does not understand semantics of images being processed. It is just emulating the patch-based correspondence technique of classical computer vision using deep learning techniques. Using feature maps to predict disparity image is preferable since it takes into account the meaning of stereo images. Current feature maps-based approaches process stereo images into a latent representation using a CNN encoder. The latent representation is decoded by a transposed CNN to arrive with dense disparity estimates. Since stereo images have generally high resolution, the resulting autoencoder is deep and requires millions of parameters. An undesirable consequence of deep networks is gradient decay; weights and biases updates vanish as they propagate down to the shallow layers. Without residual network connections [7] [8], the network is difficult to train.'\\n\\n'The image-based method uses an autoencoder style model that processes stereo images into dense disparity estimates, GC-Net [1] and DispNet [2]. The CNN encoder generates feature maps that feed the CNN decoder. To process stereo images simultaneously, a siamese network of CNN encoder with shared weights combines the feature maps. In the case of GC-Net, the feature maps are combined into a 3D cost volume which is then processed by a 3D convolution-deconvolution decoder. The model is trained by minimizing cross-entropy or MSE loss. Within the autoencoder, additional layers are used to increase the disparity estimate accuracy like the correlation network in DispNet and residual network in GC-Net. Both networks are deep with 26 layers for DispNet and 37 layers for GC-Net and up to 1024 feature maps per CNN. Both image-based methods use cropped images only as input to the encoder. This is understandable considering the KITTI datasets images are at least\\\\n1240\\u00d7376\\\\npixels in size [16], [15], [14]. and MPI Sintel dataset images are\\\\n1024\\u00d7436\\\\npixels [18]. Using cropped images as input reduces the size of the network. However, this could have a negative impact on the prediction of disparity near the boundary of the cropped images. The output dense disparity map is of the same size for GC-Net or half the size as the cropped image for DispNet. Up sampling or super resolution is applied to get the full disparity map in case the predicted image is of lower dimensions.'\\n\\n'https:\\/\\/github.com\\/roatienza\\/densemapnet'\\n\\n'DenseMapNet demonstrates that it is possible to arrive at a compact and fast CNN model architecture by taking advantage of semantics and interconnection in feature maps. Our proposed model is suitable for computation and memory constrained machines like drones and other autonomous robots. Codes and other results of DenseMapNet can be found in our project repository: https:\\/\\/github.com\\/roatienza\\/densemapnet.'\",\"332\":null,\"333\":null,\"334\":\"'We present the Signature of Topologically Persistent Points (STPP), a global descriptor that encodes topological invariants of 3D point cloud data. These topological inva...'\\n\\n'We present the Signature of Topologically Persistent Points (STPP), a global descriptor that encodes topological invariants of 3D point cloud data. These topological invariants include the zeroth and first homology groups and are computed using persistent homology, a method for finding the features of a topological space at different spatial resolutions. STPP is a competitive 3D point cloud descriptor when compared to the state of art and is resilient to noisy sensor data. We demonstrate experimentally on a publicly available RGB-D dataset that STPP can be used as a distinctive signature, thus allowing for 3D point cloud processing tasks such as object detection and classification.'\\n\\n'Shape recognition is performed by either a local or global approach. Local descriptors rely on keypoints extracted from surfaces. The aim of descriptors using local methods is to try to single out points that are distinctive in order to allow for effective description and matching. Within the local neighborhood of each keypoint, geometric information is encoded to obtain a compact representation of the input data invariant up to a predefined transformation (translation, rotation, scaling, point density variations, etc.). Global descriptors encode object geometry. They are not computed for individual points, but for a whole cluster of points that represents an object.'\\n\\n'The evolution of a scale parameter defining the neighborhood radius about each point (left) and the corresponding barcode diagram (right). When the neighborhoods of two points overlap, one dies while the other survives. At the end of this process, the lone surviving point (red) is a point of infinite persistence and is the representative of the set of points which forms a connected component. This procedure of computing topological persistence over varying spatial resolutions is known as a filtration.'\\n\\n'In contrast to the aforementioned global descriptors, STPP encodes the topological information of a 3D point cloud. Our work is inspired by an early study of shape description and classification via persistent homology [21]. Additional inspiration comes from the results of Li et al. [22] where persistence diagrams built from functions defined on objects serve as compact and informative descriptors for images and shapes.'\\n\\n'The birth-death pairings of the homology generators are encoded in a feature vector where the indices of the vector correspond to the birth of a\\\\nk\\\\n-simplex and the entries of the vector correspond to the index of a destroying\\\\n(k+1)\\\\n-simplex. Unpaired simplices, denoted by \\u22121 entries, represent\\\\nk\\\\n-dimensional holes.'\\n\\n'A barcode or persistence diagram encapsulates a concise description of the topological changes that occur during a filtration. Intuitively, a\\\\nk\\\\n-dimensional hole born at time\\\\nb\\\\nand filled at time\\\\nd\\\\ngives rise to a point\\\\n(b,d)\\\\nin the\\\\nkth\\\\npersistence diagram or an interval in the\\\\nkth\\\\nbarcode diagram. Therefore, a persistence diagram is a multiset of points in\\\\nR\\\\n2\\\\nwhile a barcode diagram is an equivalent multiset of intervals in\\\\nR\\\\n.'\\n\\n'The use of distances between barcode\\/persistence diagrams has received much attention lately in applications [24]\\u2013[26] where they serve as topological proxies for the input data. Distances between the diagrams serve as measures of the similarity between datasets. These distances can be expressed as a bottleneck or Wasserstein distance between two planar point sets using the\\\\nL\\\\n\\u221e\\\\nmetric.'\\n\\n'Although the distance between barcode\\/persistence diagrams has been shown to measure the similarity between some datasets, we observe that a single scalar value is not enough to discriminate between massive 3D point clouds. In contrast to the work mentioned in the previous subsection, we construct a vector of topologically persistent features as follows.'\",\"335\":\"'In this paper we tackle this problem by developing an open-source pipeline that vastly reduces the amount of human annotation time needed to produce labeled RGBD datasets for training image segmentation neural networks. The pipeline produces ground truth segmentations and ground truth 6DOF poses for multiple objects in scenes with clutter, occlusions, and varied lighting conditions. The key components of the pipeline are: leveraging dense RGBD reconstruction to fuse together RGBD images taken from a variety of viewpoints, labeling with ICP-assisted fitting of object meshes, and automatically rendering labels using projected object meshes. These techniques allow us to label once per scene, with each scene containing thousands of images, rather than having to annotate images individually. This reduces human annotation time by several orders of magnitude over traditional techniques. We optimize our pipeline to both collect many views of a scene and to collect many scenes with varied object arrangements. Our goal is to enable manipulation researchers and practitioners to generate customized datasets, which for example can be used to train any of the available state-of-the-art image segmentation neural network architectures. Using this method we have collected over 1,000,000 labeled object instances in multi-object scenes, with only a few days of data collection and without using any crowd sourcing platforms for human annotation.'\\n\\n'The next step is to extract a dense 3D reconstruction of the scene, shown in Figure (lb), from the raw RGBD data. For this step we used the open source implementation of ElasticFusion [17] with the default parameter settings, which runs in realtime on our desktop with an NVIDIA GTX 1080 GPU. ElasticFusion also provides camera pose tracking relative to the local reconstruction frame, a fact that we take advantage of when rendering labeled images. Reconstruction performance can be affected by the amount of geometric features and RGB texture in the scene. Most natural indoor scenes provide sufficient texture, but large, flat surfaces with no RGB or depth texture can occasionally incur failure modes. Our pipeline is designed in a modular fashion so that any 3D reconstruction method that provides camera pose tracking can be used in place of ElasticFusion.'\\n\\n'This paper introduces LabelFusion, our pipeline for efficiently generating RGBD data annotated with per-pixel labels and ground truth object poses. Specifically only a few minutes of human time are required for labeling a scene containing thousands of RGBD images. LabelFusion is open source and available for community use, and we also supply an example dataset generated by our pipeline [29].'\",\"336\":\"'http:\\/\\/cocodataset.org\\/#explore'\",\"337\":\"'To encode the multimodal inputs, one SNN is constructed for each of the\\\\nM\\\\nchannels. The outcomes from each SNN are combined in the end using a decision-level fusion. This approach follows the human brain mechanism (i.e., vision and hearing are independently processed and are fused in later cognitive decision-making stage).'\\n\\n'This training phase aims to tune the synaptic weights for SNN so that it can properly encode the input spatio-temporal signals. For that purpose, the STDP training is used. Under STDP, the synaptic weights are updated based on timing differences of the neural firings [26]. The synaptic weights between those neurons which always fire together are strengthened. More specifically, the weight of synaptic connection from pre-to postsynaptic neuron is increased if the post-neuron fires after the presynaptic spike and is decreased otherwise. Parameters for STDP training are set based on [23]. During this training stage, each quantized training data\\\\nX\\\\n~\\\\nk\\\\n\\u2208\\\\nQ\\\\nL\\\\nk\\\\n\\u00d7M\\\\nV\\\\nis mapped to its corresponding neurons in the SNN, and the synaptic weights are updated in each 1ms interval based on the STDP rules. The time allocated to simulating each\\\\nX\\\\n~\\\\nk\\\\nis 250ms. Since the input data length\\\\nL\\\\nk\\\\nis less than 40, it requires less than 200ms to stimulate the network. Then the network continues to run for 50ms without being provided any input, to allow the spike trains to propagate the network. Patterns\\\\nX\\\\n~\\\\nk\\\\nfor both turn-taking classes\\\\n(\\\\ny\\\\nk\\\\n\\u2208{0,1})\\\\nare presented to the SNN network during this training phase, following a random repeated order. The network was simulated for 250s, which includes a total of 1000 training inputs\\\\nX\\\\n~\\\\nk\\\\n, each of which takes 250ms to simulate. After the simulation, the synaptic weights in the network converge into a steady state.'\",\"338\":null,\"339\":\"'For a safe and successful daily living assistance, far from the highly controlled environment of a factory, robots should be able to adapt to ever-changing situations. Programming such a robot is a tedious process that requires expert knowledge. An alternative is to rely on a high-level planner, but the generic symbolic representations used are not well suited to particular robot executions. Contrarily, motion primitives encode robot motions in a way that can be easily adapted to different situations. This paper presents a combined framework that exploits the advantages of both approaches. The number of required symbolic states is reduced, as motion primitives provide \\u201csmart actions\\u201d that take the current state and cope online with variations. Symbolic actions can include interactions (e.g., ask and inform) that are difficult to demonstrate. We show that the proposed framework can adapt to the user preferences (in terms of robot speed and robot verbosity), can readjust the trajectories based on the user movements, and can handle unforeseen situations. Experiments are performed in a shoe-dressing scenario. This scenario is particularly interesting because it involves a sufficient number of actions, and the human-robot interaction requires the handling of user preferences and unexpected reactions.'\\n\\n'Example of hidden semi-markov model encoding the spatio-temporal characteristics of the action of approaching the shoe to the foot. (top) A set of continuous distributions, named states, encodes positions, velocities, orientations, etc. Each state is linked to a duration distribution. A transition distribution indicates the possible sequencing of the states.'\\n\\n'The features encoded in the model of the action are\\\\ny\\\\n~\\\\nt\\\\n=[\\\\nx\\\\n\\u22a4\\\\nt\\\\n x\\\\n\\u02d9\\\\n\\u22a4\\\\nt\\\\n q\\\\n\\u22a4\\\\nt\\\\n]\\\\n\\u22a4\\\\n, where\\\\nx\\\\nt\\\\nis the position of the end-effector,\\\\nx\\\\n\\u02d9\\\\nt\\\\nits velocity and\\\\nq\\\\nt\\\\nits orientation in quaternion form. The distribution of the data\\\\ny\\\\nt\\\\n, given the current cluster\\\\nz\\\\nt\\\\n, is the product of normal distributions of the data under\\\\nP\\\\ndifferent transformations\\\\nT\\\\nj\\\\nt\\\\n, varying at each time step\\\\nt'\\n\\n'In order to provide data-efficient adaptation to varying poses of objects, the model (in red and green) is encoded in different coordinate systems, linked to various objects of interest. This can be interpreted as several experts observing the data under different projections. The combination of their knowledge provides the desired adaptation (in yellow).'\",\"340\":\"'In order to make robot programming more easy and immediate, walk-through programming techniques can be exploited. However, a modification of a portion of the trajectory usually means to execute the path from the beginning. In this paper we propose a passivity-based framework to modify the trajectory online, manually driving the robot throughout the desired correction. The system follows the initial trajectory, encoded with Dynamical Movement Primitives, by setting high gains in the admittance control. When the human operator grabs the end-effector, the robot becomes compliant and the user can easily teach the desired correction, until he\\/she releases it at the end of the modification. Finally, the correction is optimally joined to the initial trajectory, restarting the path tracking. To avoid unsafe behaviors, the variation of the admittance parameters is performed exploiting energy tanks, in order to preserve the passivity of the interaction.'\\n\\n'According to the proposed method, the trajectory is taught to the robot and then it is encoded using DMPs. The robot is then able to track the generalized trajectory by setting high gains in the admittance control. When the user needs to change a portion of the trajectory, he\\/she grabs the end-effector of the manipulator. The controller detects this operation and it makes the robot easily drivable by the user. The operator moves the end-effector through the desired correction and releases it when the critical part of the correction has been done. Finally, the controller detects the release phase, the correction is optimally joined to the trajectory generated by the DMPs, the admittance control is made stiff and the modified trajectory is tracked.'\",\"341\":\"'The first challenge we addressed was topology selection. We focused on selecting a geometry with a broad and convex workspace to allow for the combined rotations required for smooth motion of the robotic limb. Due to space constraints, we cannot offer a complete description of all details of the optimization described below, however all MATLAB code used in this analysis is accessible via the git repository located at [11].'\\n\\n'Feedback control diagram for the low level cable controller. The target position is supplied by the high level controller. Cp is the proportional controller for position, which feeds cf, a proportional controller for force. This creates a linear spring behaviour for low frequency (less than 100 hz) disturbances in the closed loop system. Both the position and force measurements are obtained through encoders.'\\n\\n'Fig. 4 shows an overview of our cable-drive system, which is identical for each of the six actuated cables in our joint. A 20 Watt DCX Maxon motor with a 72:1 gearbox affixed with an 18 millimeter diameter spool is used for actuation of the cable. This system is capable of generating over 400 Newtons of cable-tension, and a maximum cable speed of approximately 10 cm\\/s. This motor is also equipped with an encoder to track the amount of cable spooled. After leaving the spool, the cable is fed through a series of mechanisms for sensing and routing described below.'\\n\\n'Depicted above is our series compliance mechanism. The cable (a) enters from the motor spool and wraps around fixed pulley (b) then continues to moving pulley (c) which is attached to compression springs (d). The cable continues over fixed pulley (e) and exits toward the omnidirectional routing mechanism. Magnetic encoder sensor (f) and magnetic strip (g) provide spring displacement measures with limit switch (h) giving an absolute reference.'\\n\\n'Depicted above is the omnidirectional cable routing mechanism. The cable (1) enters from the series compliance mechanism passes through axial\\/thrust bearing (2) before passing over pulley (3) which can passively rotate about the axis of bearing (2). The cable exit (4) is then able to point in arbitrary directions as the pulley (3) follows the cable direction. Magnetic encoder sensor (5) and magnetic ring (6) track the angle of the cable exit with limit switch (7) providing an absolute displacement when it makes contact with nub (8).'\\n\\n'Information flow diagram for embedded system. The string rest length is determined by the maxon ENX 16 encoder embedded with the motors. The string angle and string force are both determined by the ams5304 multi-pole magnetic strip position sensor. Each of these signals are counted by the ls7336R encoder counters, which communicate with the dspic33 through 3 parallel SPI buses. The limit switches connect to GPIO pins on the dspic.'\\n\\n'In total, our design incorporates eighteen quadrature encoder signals to track six tension sensors, six cable angle sensors, and six motor positions. This, combined with the need for a relatively high control frequency, places stringent real-time requirements on our embedded system. As a decentralized network of microcontrollers would be challenging to synchronize, we have utilized a dedicated quadrature decoder IC which tracks the quadrature signals from each sensor, and communicates with the single microcontroller over an SPI interface. The information flow diagram for the system is shown in fig. 9.'\",\"342\":null,\"343\":null,\"344\":\"'This work considers all of these properties while generating the hypothesis set. The pseudocode for hypothesis generation is presented in Algorithm 1.'\\n\\n'To effectively utilize the constrained expansion of states, an order of object placements needs to be considered. This information is encoded in a dependency graph, which is a directed acyclic graph that provides a partial ordering of object placements but also encodes the interdependency of objects. The vertices of the dependency graph correspond to the objects in the observed scene. Simple rules are established to compute this graph based on the detected segments\\\\nP\\\\n1:N\\\\nfor objects\\\\nO\\\\n1:N\\\\n.'\",\"345\":\"'In this work, we present an approach to deep visuomotor control using structured deep dynamics models. Our model, a variant of SE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an encoder-decoder structure. Unlike prior work, our model is structured: given an input scene, our network explicitly learns to segment salient parts and predict their pose embedding and motion, modeled as a change in the pose due to the applied actions. We train our model using a pair of point clouds separated by an action and show that given supervision only through point-wise data associations between the frames our network is able to learn a meaningful segmentation of the scene along with consistent poses. We further show that our model can be used for closed-loop control directly in the learned low-dimensional pose space, where the actions are computed by minimizing pose error using gradient-based methods, similar to traditional model-based control. We present results on controlling a Baxter robot from raw depth data in simulation and RGBD data in the real world and compare against two baseline deep networks. We also test the robustness and generalization performance of our controller under changes in camera pose, lighting, occlusion, and motion. Our method is robust, runs in real-time, achieves good prediction of scene dynamics, and outperforms baselines on multiple control runs. Video results can be found at: https:\\/\\/rse-lab.cs.washington.edu\\/se3-structured-deep-ctrl\\/.'\\n\\n'encoder'\\n\\n'The encoder has three components, the first is a convolutional network that generates a latent representation of the input point cloud (x). This network has five convolutional layers, each followed by a max pooling layer. The latent representation is further used as input for the mask and pose predictions.'\\n\\n'Given the encoded latent representation, we use a three layer fully-connected network to predict the 6D pose\\\\np\\\\nk\\\\nof each of the\\\\nK\\\\nsegmented parts. We represent each pose by 6 numbers: a 3D position\\\\n(y\\u2208\\\\nR\\\\n3\\\\n)\\\\nand an orientation\\\\n(R\\u2208SO(3))\\\\n, represented as a 3-parameter axis-angle vector. As we show later, our pose network learns to predict consistent poses which can be used to data-associate observations over long sequences of motions.'\\n\\n'encoder.'\",\"346\":\"'Generated synthetic scenes. All scenes were generated with the same annotated background frame (left column) for easier comparison. Top row: rgb. Bottom row: color-coded generated segmentation ground truth.'\",\"347\":\"'Mechanical sensors used for actuation and control, such as pressure\\/barometric sensors (pneumatic systems) and cable encoders (tendon-driven systems), are precise, reliable, and can provide explicit, easy-to-interpret information on mechanical changes in soft systems. These sensors, however, generally require special mechanical design considerations to ensure that the large, complex deformations of soft robots can be captured. The influence of external forces in particular (e.g. gravitational loading or contact with an object in the environment) may be largely invisible to these sensors [7].'\",\"348\":\"'Depth representation obtained from structure-from-motion (SFM) and our proposed mono-stixel approach. While the SFM-based reconstruction fails for the preceding vehicle (left) and oncoming vehicle (right), the mono-stixels provide a reliable and compact representation of the whole dynamic scene. The color encodes the inverse depth from close (red) to far (dark blue).'\\n\\n'Example depth and semantic representation of the mono-stixels based on the optical flow and semantic segmentation. The color encodes the inverse depth from close (red) to far (dark blue) or the semantic class following [16].'\",\"349\":\"'DTLD contains labels on the stereo cameras left raw image. We include the cameras distortion, rectification and project matrix along the disparity images, as well as example code for mapping between raw image and disparity image. For easy of use, our label format groups all additional sensors such as GPS, velocity, yaw rate and disparity image together with the left raw camera image. While DTLD does not come with a benchmark for detection or recognition, a sequence-considering split into a training and a test set (70% vs. 30% of sequences) is provided. This ensures that traffic lights inside the test set are physically other instances than in the training set.'\",\"350\":null,\"351\":\"'https:\\/\\/github.com\\/StanfordASL\\/TrafficWeavingCVAE'\\n\\n'This paper presents a method for constructing human-robot interaction policies in settings where multimodality, i.e., the possibility of multiple highly distinct futures, plays a critical role in decision making. We are motivated in this work by the example of traffic weaving, e.g., at highway on-ramps\\/off-ramps, where entering and exiting cars must swap lanes in a short distance-a challenging negotiation even for experienced drivers due to the inherent multimodal uncertainty of who will pass whom. Our approach is to learn multimodal probability distributions over future human actions from a dataset of human-human exemplars and perform real-time robot policy construction in the resulting environment model through massively parallel sampling of human responses to candidate robot action sequences. Direct learning of these distributions is made possible by recent advances in the theory of conditional variational autoencoders (CVAEs), whereby we learn action distributions simultaneously conditioned on the present interaction history, as well as candidate future robot actions in order to take into account response dynamics. We demonstrate the efficacy of this approach with a human-in-the-loop simulation of a traffic weaving scenario.'\\n\\n\\\"Methods for autonomous decision making under uncertainty may be classified as model-free, whereby human action possibilities and associated likelihoods are implicitly encoded in a robot control policy learned from trial experience, or model-based, whereby a probabilistic understanding of the interaction dynamics is used as the basis for policy construction [5]. In this paper we take a model-based approach to pairwise human-robot interaction, seeking to explicitly characterize a possibly multimodal distribution over human actions at each time step conditioned on interaction history as well as future robot action choices. By decoupling action\\/reaction prediction from policy construction, we aim to achieve a degree of transparency in a planner's decision making that is typically unavailable in model-free approaches. Conditioning on history allows a robot to reason about hidden factors like experience, mood, or engagement level that may influence the distribution, and conditioning on the future takes into account response dynamics. We develop our work around a traffic weaving case study (Fig. 1) for which we adapt methods from deep neural network-based language and path prediction [6], [7], [8], [9] to learn a Conditional Variational Autoencoder (CVAE) generative model of human driver behavior. We employ this class of models because it encodes probability distributions in terms of latent variables that, for our purposes, may represent multiple modes of behavior. Additionally, CVAEs admit efficient ancestral sampling for drawing human responses to candidate robot action sequences. We validate our learned CVAE as the basis for a limited-lookahead autonomous driver policy, applied in a receding horizon fashion, the behavior of which we explore with human-in-the-loop testing.\\\"\\n\\n'In our model, the discrete latent variable z has the responsibility of representing high-level behavior modes, while a second level of multimodality within each such high-level behavior is facilitated by an autoregressive RNN sequence decoder (light purple cells, Fig. 2). The RNN maintains a hidden state to allow for drawing each future human action conditioned on the actions drawn at previous future times:'\\n\\n'The human-human traffic weaving dataset and source code for all results in this section, including all network architecture details and hyperparameters, are available at https:\\/\\/github.com\\/StanfordASL\\/TrafficWeavingCVAE. All simulation and computation was done on a computer running Ubuntu 16.04 equipped with a 3.6GHz octocore AMD Ryzen 1800X CPU and an NVIDIA GeForce GTX 1080 Ti GPU.'\\n\\n'https:\\/\\/github.com\\/StanfordASL\\/TrafficWeavingCVAE'\",\"352\":\"'Iterative Closest Point (ICP) is a widely used method for performing scan-matching and registration. Being simple and robust, this method is still computationally expensive and may be challenging to use in real-time applications with limited resources on mobile platforms. In this paper we propose a novel effective method for acceleration of ICP which does not require substantial modifications to the existing code. This method is based on an idea of Anderson acceleration which is an iterative procedure for finding a fixed point of contractive mapping. The latter is often faster than a standard Picard iteration, usually used in ICP implementations. We show that ICP, being a fixed point problem, can be significantly accelerated by this method enhanced by heuristics to improve overall robustness. We implement proposed approach into Point Cloud Library (PCL) and make it available online. Benchmarking on the real-world data fully supports our claims.'\\n\\n'We implement AA-ICP as part of the widely used Point Cloud Library (PCL), and the source code is freely available in our fork repository1. Finally, section 4 summarizes the paper.'\\n\\n'To measure the performance of AA-ICP, we implemented it as a part of Point Cloud Library (PCL). In our work AA-ICP works with point-to-point ICP. The modified code is freely available in our fork repository4 and is intended for the inclusion into the upstream.'\",\"353\":\"'Implementation of GICP is taken from the open source version available at [32] and 3D-NDT is taken from Point Cloud Library (PCL) [33]. Grid resolution\\\\n(\\\\nR\\\\nv\\\\n)\\\\nfor 3D-NDT, MI-VARZ and MI-N is set as 1m (i.e. voxel is\\\\n1m\\u00d71m\\u00d71m)\\\\n. In our implementation we chose the initial simplex for optimization as\\\\n[\\\\ns\\\\nx\\\\n=8,\\\\ns\\\\ny\\\\n=8,\\\\ns\\\\nz\\\\n=1,\\\\ns\\\\nroll\\\\n=0.1,\\\\ns\\\\npitch\\\\n=0.1,\\\\ns\\\\nyaw\\\\n=0.8]\\\\n.'\",\"354\":\"'The IDT is the de facto standard spatio-temporal model for hand-crafted video representation. The settings were based on the original implementation. To generate a codeword vector, motion boundary histograms (MBH) (192-d), histograms of optical flow (HoF) (108-d), and histogram of oriented gradients (HoG) (96-d) were captured at each trajectory sampling. The combined vector consisted of the MBH, HoF, and HoG features.'\",\"355\":null,\"356\":\"'https:\\/\\/goo.gl\\/b489PX'\\n\\n'https:\\/\\/goo.gl\\/WbWFCa'\",\"357\":\"'There are no known engineering attempts that replicate this small-field neuronal processing strategy. The main contribution of this work is the demonstration of a computationally-efficient method for small object perception and avoidance through the creation of an engineering analogue for the FD neuron system, which can be deployed on small autonomous aerial microsystems for indoor and outdoor applications. The engineering equivalent of the CH cells obtains the low spatial frequency component of the input stimulus, which is then removed from the stimulus to the FD cells. The resultant operation retains the high spatial-frequency component of the stimulus which directly encodes information about the relative range and bearing of small-field objects in the local environment. These motion cues are then extracted and combined with an artificial potential function based steering controller to achieve safe, robust navigation in an obstacle field laden with small-field clutter. The proposed navigation strategy then relies entirely on the extraction of the small-field optic flow signal, and thus renders extraction of either local environment structure or estimation of vehicle velocity states superfluous. Thus, the proposed scheme is inherently more robust to variation in local environment structure and reference flight condition.'\",\"358\":null,\"359\":null,\"360\":null,\"361\":null,\"362\":\"'We have implemented a particle filter on episode (PFoE) without a reward system. This implementation can be used for teach-and-replay of a mobile robot, and is published on GitHub as a ROS package with a robot control program for teaching.'\",\"363\":\"'The fast expanding 3C (Computer, Communication, and Consumer electronics) manufacturing leads to a high demand on the fabrication of USB cables. While several commercial machines have been developed to automate the process of stripping and soldering of USB cables, the operation of manipulating USB wires according to the color code is heavily dependent on manual works because of the deformation property of wires, probably resulting in the falling-off or the escape of wires during manipulation. In this paper, a new vision-based controller is proposed for robotic grasping and manipulation of USB wires. A novel two-level structure is developed and embedded into the controller, where Level-I is referred to as the grasping and manipulation of wires, and Level-II is referred to as the wire alignment by following the USB color code. The proposed formulation allows the robot to automatically grasp, manipulate, and align the wires in a sequential, simultaneous, and smooth manner, and hence to deal with the deformation of wires. The dynamic stability of the closed-loop system is rigorously proved with Lyapunov methods, and experiments are performed to validate the proposed controller.'\\n\\n'In the first experiment, all wires were not fixed at the grooves initially, and the robotic gripper was controlled to grasp then manipulate the wires, and carry out the alignment according to the color code sequentially. The desired positions were specified as:\\\\nx\\\\nd1\\\\n=[531,535\\\\n]\\\\nT\\\\npixel,\\\\nx\\\\nd2\\\\n=[538,610\\\\n]\\\\nT\\\\npixel,\\\\nx\\\\nd3\\\\n=[530,665\\\\n]\\\\nT\\\\npixel, and\\\\nx\\\\nd4\\\\n=[531,719\\\\n]\\\\nT\\\\npixel respectively. The parameters of the regions\\\\nf\\\\ni\\\\n(x, \\\\nx\\\\ni\\\\n)\\u22640\\\\nand\\\\nh\\\\ni\\\\n(\\\\nx\\\\ni\\\\n, \\\\nx\\\\ndi\\\\n)\\u22640\\\\nwere set as\\\\n\\u03b4\\\\nI\\\\n=15pixel\\\\nand\\\\n\\u03b4\\\\nII\\\\n=30\\\\npixel respectively, and the parameters in the weighting functions\\\\nw\\\\ni\\\\n(x, \\\\nx\\\\ni\\\\n)\\\\nand\\\\na\\\\ni\\\\n(\\\\nx\\\\ni\\\\n, \\\\nx\\\\ndi\\\\n)\\\\nwere set as:\\\\n\\u03ba\\\\nI\\\\n=\\\\n\\u03ba\\\\nII\\\\n=0.7\\\\n. The control parameters in (10) were set as:\\\\n\\u03b1=1,\\\\nK\\\\ns\\\\n=0.1\\\\nI\\\\n3\\\\n, and\\\\nK\\\\np\\\\n=0.5\\\\nI\\\\n2\\\\n, where\\\\nI\\\\n3\\\\n\\u2208\\\\nR\\\\n3\\u00d73\\\\nand\\\\nI\\\\n2\\\\n\\u2208\\\\nR\\\\n2\\u00d72\\\\ndenote identity matrices. The position errors for each wire are given in Fig. 4, which shows that Red, White, Green, Black wires were grasped at\\\\nt\\u22486,16,28,39s\\\\nrespectively then manipulated to the desired positions. The duration of Grasping and Manipulation was around 10 s. The snapshots are shown in Fig. 5.'\\n\\n'By specifying the control objective in a two-level structure, the proposed controller allows the robot to automatically grasp and manipulate the USB wires, and properly align the wires by following the color code. Such formulation guarantees the feasibility and the autonomous capability of the manipulation of USB wires. The stability of the closed-loop system has been rigorously proved using Lyapunov methods, and experimental results in different scenarios have been presented to demonstrate the performance of the proposed controller.'\\n\\n'Experiment 1: All wires were grasped and manipulated at the grooves by following the USB color code. (a) Position errors for Red wire\\\\nx\\\\n1\\\\n\\u2212\\\\nx\\\\nd1\\\\n; (b) position errors for White wire\\\\nx\\\\n2\\\\n\\u2212\\\\nx\\\\nd2\\\\n; (c) position errors for Green wire\\\\nx\\\\n3\\\\n\\u2212\\\\nx\\\\nd3\\\\n;. D) position errors for Black wire\\\\nx\\\\n4\\\\n\\u2212\\\\nx\\\\nd4\\\\n.'\",\"364\":null,\"365\":\"'At this stage, we obtain several conflicting segmentation hypotheses in the time domain, including over- and under-segmentations, as well as outliers from the static background, such as segments that are parts of buildings. We perform near-online inference using a CRF model to resolve this ambiguity. Intuitively, hypotheses that are supported by consistent segmentations are more prominent object candidates. Two additional cues that we use are the segmentation scores of the associated segmentation masks and the classification scores. As a counterweight, hypotheses are mutually penalized for occupying the same physical space in the image domain. We encode these intuitions by performing a MAP inference using a CRF model (c.f. [18], [26]) by minimizing the following energy function for each time frame\\\\nt\\\\ne\\\\n:'\",\"366\":null,\"367\":\"https:\\/\\/github.com\\/JunaidCS032\\/MOTBeyondPixels\",\"368\":\"'https:\\/\\/sites.google.com\\/view\\/multi-task-domain-adaptation\\/'\\n\\n'The dataset for training the grasp prediction network is collected by controlling the gripper to repeatedly attempt to grasp objects from a tray with randomly selected objects and drop them back. This process starts with exploring a random policy at the beginning, and then switches to the CEM with the updated network parameters. In the real world, the ground truth success label is automatically determined by a hard coded perception system after each grasping episode. This is done by taking images of the tray and comparing pixel differences before and after dropping the object. In simulation, we determine the success label by checking the object position. The grasp prediction network is then trained using a log loss against the ground truth success labels.'\",\"369\":\"'For policy search, we used the GPS code implementation [41], and modified the cost function, initialization, and neural network structure according to Section IV. To further automate the learning process, we also learned reset controllers using iLQG, as described in [42]. This was important since learning to pull out the parts from a partly assembled state can require a non-trivial control policy. The GPS implementation sends torque control commands at 20Hz, which are repeated on the robot real time controller at 1KHz. We compare our method to the standard MoveIt! [43] controller, which tracks a joint trajectory at 1KHz. Note that the finer granularity of the MoveIt! controller gives it an advantage over our controller, but in practice the 20Hz controller is sufficient to solve the tasks considered here.'\",\"370\":null,\"371\":\"'The challenges in personalization of carpentered items are twofold. First, designs must ensure proper functionality and performance (structural stability, durability, etc.) while being feasible to manufacture and assemble. Second, fabrication of a design requires skilled tool use and dexterous assembly. We address these challenges by an end-to-end system that handles all the stages from conceptual design, through verification, to fabrication. We leverage expert knowledge for design, robots for fabrication, and code to tie it all together.'\",\"372\":null,\"373\":null,\"374\":null,\"375\":null,\"376\":null,\"377\":\"'One approach to achieve multiple DOFs in the homogeneous field case is through time-encoded signals. Becker et al., for example, suggest a mechanical decoding system for modulated magnetic control signals [8]. However, this method requires relatively large agents in an MRI which is not feasible in microscale and cannot be generalized to other microrobotic systems. In this work, on the other hand, we consider quasistatic magnetic actuation; in other words, where magnetic fields are in steady state and actuation forces and torques can be seen as linearly dependent on field and field gradient inputs. Thus the results of this study can be applied to more complex actuation types including both quasistatic and time-encoded magnetic fields.'\",\"378\":null,\"379\":\"'The Loader connects the user interface with algorithms. The Loader is a critical part of the infrastructure in the context of collaboration with industry, since it allows SLAM libraries to be dynamically loaded without source code being made available. This means that commercially-developed SLAM algorithms can be integrated into SLAMBench2 and compared against other commercial or open source algorithm implementations in a consistent and reproducible manner.'\\n\\n'For ORB-SLAM2, the required functions can be implemented using around 350 lines of code. This is certainly a straightforward task for the authors of the algorithm, and a simple task for a person familiar with SLAM algorithms. Once compiled, this library (i.e. liborbslam2.so) can be directly used with the SLAMBench2 loader:'\\n\\n'In this paper we introduce SLAMBench2, which is designed to enable the evaluation of open source or proprietary SLAM systems, over an extensible sets of datasets and clearly specified metrics. SLAMBench2 is inspired by the work in [2], [4], [5]. It currently supports eight different algorithms (dense, semi-dense, and sparse) and three datasets. It is a tool that enables the reproducibility of results for existing SLAM systems and allows the integration and evaluation of new SLAM results. Through a series of use-cases (Sec. V-B), we demonstrate its simplicity of use as well as a variety of results that have already been collected. We make the following contributions:'\",\"380\":null,\"381\":\"'Place recognition is a key element of mobile robotics. It can assist with the \\u201cwake-up\\u201d and \\u201ckidnapped robot\\u201d problems, where the robot position needs to be estimated without prior information. Among the different sensors that can be used for the task (e.g., camera, GPS, LiDAR), LiDAR has the advantage of operating in the dark and in GPS-denied areas. We propose a new method that uses solely the LiDAR data and that can be performed without robot motion. In contrast to other methods, our system leverages intensity information (as opposed to only range information) which is encoded into a novel descriptor of LiDAR intensities as a group of histograms, named DELIGHT. The descriptor encodes the distributed histograms of intensity of the surroundings which are compared using chi-squared tests. Our pipeline is a two-stage solution consisting of an intensity-based prior estimation and a geometry-based verification. For a map of 220k square meters, the method achieves localisation in around 3s with a success rate of 97%, illustrating the applicability of the method in real environments.'\\n\\n\\\"Several methods for global localisation in 3D point clouds have been proposed. Among techniques based on global descriptors where each point cloud is encoded using a single feature vector, R\\u00f6hling et al. [2] proposed to describe places by a histogram of points' elevation. Another method is to create the descriptor by dividing the point cloud a into a cubic grid and calculating the density function which describes the shapes that are later gathered into histograms [3]. One issue with global descriptors is the fact that the map is interpreted as a discrete set of places along the trajectory, and each place has a defined origin forming a graph-like structure. In this case the only possible robot locations are in the graph vertices. When the robot is between the vertices or off the original trajectory the exact location cannot be retrieved.\\\"\\n\\n'In this section we outline the details of the DELIGHT descriptor, initially providing insights on the use of intensity, followed by a presentation of the descriptor structure. The use of the intensity information encoded into the DELIGHT descriptor is the main novelty of the proposed system.'\",\"382\":null,\"383\":null,\"384\":\"\\\"We have implemented the proposed algorithm in the C++ programming language, as well as Bresenham's Line, ray marching, and the LUT approach for comparison. Our source code1 is available for use and analysis, and Python wrappers are also provided. All synthetic benchmarks were performed on a computer with an Intel Core i5-4590 CPU@3.30GHz with 16GB of 1333MHz DDR3 ram, running Ubuntu 14.04.\\\"\\n\\n'The main contribution of this paper is a new acceleration data structure, called the Compressed Directional Distance Transform (CDDT) which allows near constant time two dimensional ray casting queries for an occupancy grid map of fixed size. The algorithm is benchmarked against several common ray casting methods. We provide an open-source implementation of CDDT and the other methods evaluated in a library called RangeLibc 3. The CDDT algorithm has been applied to a particle filter localization algorithm 4, allowing 2500 particles to be maintained at 40Hz with 61 ray casts per particle on a NVIDIA Jetson TX1 embedded computer.'\",\"385\":\"\\\"With increasing maturity and robustness in this field, two state of the art methods for Visual-Inertial Odometry (VIO) open-sourced their implementations, namely OKVIS [15] and ROVIO [3]. Such systems permit reliable state estimation even during complicated UAV maneuvers. However, as these algorithms are only local, the current UAV pose that is being estimated is prone to drift over longer trajectories. Aiming to address drift during real-time monocular state estimation, ORB-SLAM [16] pushed the state of the art, tackling large-scale loop correction at an unprecedented robustness and accuracy in monocular systems. Incorporating additional inertial data to the monocular setup, the most recent VI-ORB-SLAM [17] was the first VI-SLAM system capable of correcting drift via loop-closure detection and optimization, while maintaining an estimate of metric scale with high accuracy. Despite constituting a milestone, VI-ORB-SLAM remains closed source and based on the authors' evaluation [17] as the only source of information, its accuracy is reportedly fluctuating across different datasets, highlighting the need for deeper analysis in VI-SLAM.\\\"\\n\\n'While the aforementioned open-sourced VIO systems have been very influential in robot navigation, their inevitable tendency to drift, limits their applicability in real scenarios, where global state estimation is required. In this spirit, we present a carefully designed back-end, which in combination with a nominal VIO system enables the generation of a globally consistent map at comparable accuracy with the state of the art VI-SLAM systems - at times even achieving error reduction of over 50%, solely considering the backend optimization. Moreover, here we go a step further to illustrate the use of the proposed back-end with two UAVs to achieve collaborative mapping, while correcting for drift upon loop-closures, both within each trajectory as well as across trajectories of different UAV s as shown in Figure 1. This paper outlines a new, complete back-end system in enough detail to enable reproducability of the proposed system, employable in combination with an off-the-shelf VIO system requiring only minimal modification. Furthermore, our evaluation on benchmarking datasets reveals that the proposed framework can achieve significant improvement in accuracy over the state of the art.'\",\"386\":null,\"387\":null,\"388\":\"'B\\\\np\\\\nis a logical expression representing the preconditions that need to be satisfied for the task to be executed. We encode these constraints into a task precedence graph\\\\nP\\\\nm\\\\n. Based on the precedence constraints, we can determine when a task is ready for execution. This effectively decomposes the mission into stages consisting of possibly multiple mission tasks.'\",\"389\":null,\"390\":null,\"391\":\"\\\"The main devices in each rope-climbing robots are, a brush DC motor with 40:1 reduction gears, which is with 250 walt rating power and 120 round per minute rating speed; an embedded system which here is STM32F103C8T6; a brush motor driver which can use PWM to change the motor speed, it varies from \\u22125000 to 5000; and an rotary incremental optical encoder with 1200 segments which is installed at the shaft of the DC motor; a Lithium-ion Battery with 24Voltage rating voltage and 15Ampere-Hour capacity. Two robots are connected by CAN bus wire. and numbered as robot1 and robot2. Robot1 gets the two robots' sensors datas from the CAN bus wire, and sends out the data to the ground control station which is a personal computer(PC) of intel is, windows 7 system through tansparent wireless module which you can think the robot1 is communicated with PC through serial ports. To verify the stability of the control scheme, setpoint control and tracking control experiments are implemented as follows.\\\"\",\"392\":\"'http:\\/\\/arc.cs.princeton.edu\\/'\\n\\n'https:\\/\\/youtu.be\\/6fG7zwGfIkI'\\n\\n'https:\\/\\/arxiv.org\\/abs\\/1710.01330'\\n\\n'This paper presents a robotic pick-and-place system that is capable of grasping and recognizing both known and novel objects in cluttered environments. The key new feature of the system is that it handles a wide range of object categories without needing any task-specific training data for novel objects. To achieve this, it first uses a category-agnostic affordance prediction algorithm to select and execute among four different grasping primitive behaviors. It then recognizes picked objects with a cross-domain image classification framework that matches observed images to product images. Since product images are readily available for a wide range of objects (e.g., from the web), the system works out-of-the-box for novel objects without requiring any additional training data. Exhaustive experimental results demonstrate that our multi-affordance grasping achieves high success rates for a wide variety of objects in clutter, and our recognition algorithm achieves high accuracy for both known and novel grasped objects. The approach was part of the MIT-Princeton Team system that took 1st place in the stowing task at the 2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are available online at http:\\/\\/arc.cs.princeton.edu.'\\n\\n'All code, datasets, and pre-trained models are available online at http:\\/\\/arc.cs.princeton.edu [1]. We also provide a video summarizing our approach at https:\\/\\/youtu.be\\/6fG7zwGfIkI, and a supplementary appendix with more details on our system at https:\\/\\/arxiv.org\\/abs\\/1710.01330.'\",\"393\":\"'https:\\/\\/github.com\\/rrahmati\\/roboinstruct-2'\\n\\n'https:\\/\\/youtu.be\\/AqQFzoVsJfA'\\n\\n'A closer look at the architecture shows that we have a VAE-GAN autoencoder that shares its encoder with the visual feature extractor of a controller network that sends commands to the robot. The autoencoder tries to fully reconstruct the images while the controller network will try to focus on some relevant features from the image such as the pose of the gripper and relevant objects. This competition\\/collaboration between these two networks will result in a more regularized visual feature extractor. This idea is similar to the semi-supervised learning with generative models [36] where they use a generative model via the VAE decoder and discriminative training via the action branch to improve sample efficiency. However, in contrast to this work, we observe an improvement in generalization simply from including the reconstruction objective, without including any additional unlabeled data.'\\n\\n'Note that the extracted features from the encoder are in the form of a probability distribution that is encouraged to be close to the unit Gaussian by a KL-divergence penalty in the loss function. The noise in the LSTM input caused by sampling from the encoded latent features helps to regularize the LSTM. In addition, we use dropout [37] with a probability of 0.5 to further avoid overfitting.'\\n\\n'Our proposed architecture for multi-task robot manipulation learning. The neural network consists of a controller network that outputs joint commands based on a multi-modal autoregressive estimator and a VAE-GAN autoencoder that reconstructs the input image. The encoder is shared between the VAE-GAN autoencoder and the controller network and extracts some shared features that will be used for two tasks (reconstruction and controlling the robot).'\\n\\n'The autoencoder consists of three neural networks. The first network is an encoder that encodes the data sample\\\\nx\\\\nto the latent representation\\\\nz\\u223cenc(x)=q(z|x)\\\\n. Then the generator network decodes the latent representation\\\\nz\\\\nand reconstructs the input image\\\\nx\\\\n~\\\\n\\u223cgen(z)=p(x|z)\\\\n. The last part of the autoencoder network is a GAN discriminator that takes a real image or a reconstructed image and tries to discriminate whether the given image is real or reconstructed.'\\n\\n'Finally, the error of the autoencoder network can be described as the sum of errors formulated before:'\\n\\n'To train the controller and autoencoder networks, we alternate between them at each iteration with probability of 0.5 for the selection of each network. The parameters of the autoencoder are set and initialized based on the recommendations in [29], with the latent space size set to 256. All other parameters including the LSTM parameters are initialized uniformly between \\u22120.08 to 0.08 following the recommendation in [40]. Each LSTM layer has 100 memory cells and is connected to a mixture of Gaussians with 50 components.'\\n\\n\\\"Another reason for using a recurrent neural network is that there will be timesteps where the model will fail to extract enough information from the current input to decide on what to do next. For instance, the manipulation object might be occluded or not encoded correctly due to the imperfection of the visual encoder. The LSTM recurrent neural networks are able to store this information and the controller can continue to act based on the network's memory until it regains the sight of the object.\\\"\\n\\n'In order to evaluate our method, we consider several manipulation tasks that are frequent components of ADLs found in assistive robotics. The tasks involved the manipulation of objects on a table, by a robotic arm with a fixed base. The robot used was a 6-axis Lynxmotion AL5D robot with a two-finger gripper. The input video is recorded by a camera mounted facing the robot arm. This arrangement minimizes but does not completely eliminate the instances where the objects are occluded by the robot arm. The entire setup, including the arm, the camera, the Leap Motion and Playstation controllers used for demonstration, costs about $500, making it affordable and accessible. The source code of the system is available as open source at https:\\/\\/github.com\\/rrahmati\\/roboinstruct-2. We encourage readers to reproduce or extend our experiements.'\\n\\n'B. Validating the Autoencoder'\\n\\n'Our network architecture uses an autoencoder to reduce the visual input to a latent space of 256 features. An advantage of this approach is that we can use the network to reconstruct the input images, and thus intuitively confirm that the features sufficiently capture the details of the scene to make possible the manipulation task. Figure 3 shows the pairs of the original and reconstructed images for several images chosen from the manipulation tasks. We find that in most cases, the objects and the arm itself are captured and encoded reasonably well. Therefore, the LSTM has useful information to generate a trajectory to accomplish the task.'\\n\\n'Original input images to the network are shown on the top row. For each original image, the corresponding reconstructed image by the autoencoder is shown in the bottom row.'\\n\\n'https:\\/\\/github.com\\/rrahmati\\/roboinstruct-2'\",\"394\":\"'https:\\/\\/goo.gl\\/gPzPhm'\\n\\n'In this work, we design an end-to-end deep geometry-aware grasping network for learning this representation. Our geometry-aware network has two components: a shape generation network and a grasping outcome prediction network. The shape generation network learns to recognize and reconstruct the 3D geometry of the scene with an image encoder and voxel decoder. The image encoder transforms the RGBD input into a high-level geometry representation that involves shape, location, and orientation of the object. The voxel decoder network takes in the geometry representation and outputs the occupancy grid of the object. To further hallucinate the local view from gripper perspective, we propose a novel learning-free image projection layer similar to [41], [30]. Building upon the shape generation network, our grasping outcome prediction network learns to produce a grasping outcome (e.g., success or failure) based on the action (i.e. gripper pose), the current visual state (e.g., object and gripper), and the learned geometry-aware 3D representation. Unlike our end-to-end multi-objective learning framework, existing data-driven grasping pipelines [29], [22], [21] can be viewed as models without a shape generation component. They require either an additional camera to capture the global object shape or extra processing steps, such as object detection and patch alignment. Furthermore, these methods learn over a constrained grasp space, typically either 3-DOF or 4-DOF. We relax this constraint to learn fully generalized 6-DOF grasp poses.'\\n\\n'Illustration of DGGN (deep geometry-aware grasping network). Our DGGN has a shape generation network and an outcome prediction network. The shape generation network has a 2D CNN encoder, 3D CNN decoder, and a global sampling layer (detailed in sec. Iii-b). Our outcome prediction network has a 2D CNN encoder, a local sampling layer (detailed in sec. III-d), and a fully-connected prediction network.'\\n\\n\\\"We adopt the current data-driven framework as our grasping baseline by removing the shape encoder and shape decoder from our deep geometry-aware grasping model. This baseline can be interpreted as the grasping quality CNN [21] without an additional view from a top-down camera. We trained the model using the ADAM optimizer with a learning rate of 10\\u22125 for 200K iterations and a mini-batch of size of 4. As an ablation study, we added view and static scene as an additional input channel on top of the baseline model but didn't observe significant improvements.\\\"\\n\\n'We adopted a two-stage training procedure: First, we pre-trained the shape generation model (shape encoder and shape decoder) using the ADAM optimizer with a learning rate of 10\\u22125 for 400K iterations and a mini-batch of size of 4. In each batch, we sample 4 random viewpoints for the purpose of multi-view supervision in the training time. We observed that this setting led to a more stable shape generation performance compared to single-view training. In addition, we used\\\\nL\\\\n1\\\\nloss for foreground depth prediction and\\\\nL\\\\n2\\\\nloss for silhouette prediction with coefficients\\\\n\\u03bb\\\\nD\\\\n=0.5\\\\nand\\\\n\\u03bb\\\\nM\\\\n=10.0\\\\n. In the second stage, we fine-tuned the state encoder and outcome predictor using the ADAM optimizer with a learning rate of 3 * 10\\u22126 for 200K iterations and a mini-batch of size of 4. We used cross-entropy as our objective function since the grasping prediction is formulated as a binary classification task.'\\n\\n'In our experiments, all the models are trained using 20 GPU workers and 32 parameter servers with asynchronized updates. Both baseline and our geometry-aware model adopt convolutional encoder-decoder architecture with residual connections. The bottleneck layer (e.g., the identity unit in the geometry-aware model) is a 768 dimensional vector.'\\n\\n'We evaluate the quality of the shape generation model by visualizing the geometry representations through the shape encoder and decoder network. In our evaluations, we used single-view RGBD input and corresponding camera view matrix as input to the network. As shown in Figure 4(a), our shape generation model is able to generate a detailed 3D occupancy grid from single-view input without 3D supervision during training. As shown in Figure 4(b), our model demonstrates reasonable generalization quality even on novel object instances.'\\n\\n'We believe the proposed deep geometry-aware grasping framework has many potentials in advancing robot learning in general. One interesting future direction is to apply the learned geometry-aware representation to perform tasks using other types of hands (e.g., hands with very different kinematics). In addition, we would like to explore some alternative model designs (e.g., learn to grasp without the auxiliary state encoder) such that the learned geometry-aware representation might be easily adapted to other domains (e.g., real robot setup).'\",\"395\":\"'Identify the target object by using an encoder model that takes the textual instruction and the image (within the bounding box) as input.'\",\"396\":\"'https:\\/\\/sites.google.com\\/site\\/video2command\\/'\\n\\n'We present a new method to translate videos to commands for robotic manipulation using Deep Recurrent Neural Networks (RNN). Our framework first extracts deep features from the input video frames with a deep Convolutional Neural Networks (CNN). Two RNN layers with an encoder-decoder architecture are then used to encode the visual features and sequentially generate the output words as the command. We demonstrate that the translation accuracy can be improved by allowing a smooth transaction between two RNN layers and using the state-of-the-art feature extractor. The experimental results on our new challenging dataset show that our approach outperforms recent methods by a fair margin. Furthermore, we combine the proposed translation module with the vision and planning system to let a robot perform various manipulation tasks. Finally, we demonstrate the effectiveness of our framework on a full-size humanoid robot WALK-MAN.'\\n\\n'An overview of our approach. We first extract the deep features from the input frames using cnn. Then the first lstm\\/gru layer is used to encode the visual features. The input words are fed to the second lstm\\/gru layer and this layer sequentially generates the output words.'\\n\\n'Our architecture is based on the encoder-decoder scheme [8] [27] [20], which is adapted from the popular sequence to sequence model [28] in machine translation. Although recent approaches to video captioning problem use attention mechanism [20] or hierarchical RNN [16], our proposal solely relies on the neural architecture. Based on the input data characteristics, our network smoothly encodes the input visual features and generates the output commands, achieving a fair improvement over the state of the art without using any additional modules.'\\n\\n'In particular, given an input video, we first extract visual features from the video frames using the pretrained CNN network. These features are encoded in the first RNN layer to create the encoder hidden state. The input words are then fed to the second RNN layer, and this layer will decode sequentially to generate a list of words as the output command. Fig. 2 shows an overview of our approach. More formally, given an input sequence of features\\\\nX=(\\\\nx\\\\n1\\\\n, \\\\nx\\\\n2\\\\n, \\u2026, \\\\nx\\\\nn\\\\n)\\\\n, we want to estimate the conditional probability for an output command\\\\nY=(\\\\ny\\\\n1\\\\n, \\\\ny\\\\n2\\\\n, \\u2026, \\\\ny\\\\nm\\\\n)\\\\nas follows:'\\n\\n'Since we want a generative model that encodes a sequence of features and produces a sequence of words in order as a command, the LSTM\\/GRU is well suitable for this task. Another advantage of LSTM\\/GRU is that they can model the long-term dependencies in the input features and the output words. In practice, we conduct experiments with the LSTM and GRU network as our RNN, while the input visual features are extracted from the VGG16, Inception_v1, and ResNet50 network, respectively.'\\n\\n'In the decoding stage, the second LSTM\\/GRU layer converts the list of hidden encoder vectors\\\\nH\\\\ne\\\\ninto the sequence of hidden decoder vectors\\\\nH\\\\nd\\\\n. The final list of predicted words\\\\nY\\\\n^\\\\nis achieved by applying a softmax layer on the output\\\\nH\\\\nd\\\\nof the LSTM\\/GRU decoder layer. In particular, at each time step\\\\nt\\\\n, the output\\\\nz\\\\nt\\\\nof each LSTM\\/GRU cell in the decoder layer is passed though a linear prediction layer\\\\ny\\\\n^\\\\n=\\\\nW\\\\nz\\\\nz\\\\nt\\\\n+\\\\nb\\\\nz\\\\n, and the predicted distribution\\\\nP(\\\\ny\\\\nt\\\\n)\\\\nis computed by taking the softmax of\\\\ny\\\\n^\\\\nt\\\\nas follows:'\\n\\n'In this way, the LSTM\\/GRU decoder layer sequentially generates a conditional probability distribution for each word of the output command given the encoded features representation and all the previously generated words. In practice, we preprocess the data so that the number of input words\\\\nm\\\\nis equal to the number of input frames\\\\nn\\\\n. For the input video, this is done by uniformly sampling\\\\nn\\\\nframes in the long video, or padding the extra frame if the video is too short. Since the number of words\\\\nm\\\\nin the input commands is always smaller than\\\\nn\\\\n, we pad a special empty word to the list until we have\\\\nn\\\\nwords.'\",\"397\":\"'https:\\/\\/github.com\\/gsartoretti\\/Deep-SEA-Snake'\\n\\n'https:\\/\\/goo.gl\\/FT6Gwq'\\n\\n'httPs:\\/\\/github.com\\/gsartoretti\\/Deep-SEA-Snake'\",\"398\":\"\\\"We collect a dataset that consists of execution traces from multiple types of tasks and their task specifications. For each specification, we provide the ground-truth hierarchical decomposition of the specification for training by rolling a hard-coded expert policy. We use cross-entropy loss at every temporal location of the task specification to supervise the scoping labels. We also adopted the idea of adaptive curriculum from NPI [24], where the frequency of each mini-batch being fetched is proportional to the model's prediction error with respect to the corresponding program.\\\"\\n\\n'We use an expert policy to generate program execution traces as training data. An expert policy is an agent with hard-coded rules that call programs (move_to, pick_and_drop, etc.) to perform a task. In our experiment, we use the demonstration of a robot carrying out a task as the task specification. For all experiments, unless specified, the state representation in the task demonstrations is in the form of object position trajectories relative to the gripper frame. In the Block Stacking experiment, we also report the results of using a learned object detector to predict object locations and the results of directly using RGB video sequence as state observations and task demonstrations.'\\n\\n\\\"We introduced Neural Task Programming (NTP), a meta-learning framework that learns modular and reusable neural programs for hierarchical tasks. We demonstrate NTP's strengths in three robot manipulation tasks that require prolonged and complex interactions with the environment. NTP achieves generalization towards task length, topology, and semantics. This work opens up the opportunity to use generalizable neural programs for modeling hierarchical tasks. For future work, we intend to 1) improve the state encoder to extract more task-salient information such as object relationships, 2) devise a richer set of APIs such as velocity and torque-based controllers, and 3) extend this framework to tackle more complex tasks on real robots.\\\"\\n\\n'In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task specification (e.g., video demonstration of a task) and recursively decomposes it into finer sub-task specifications. These specifications are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well towards unseen tasks with increasing lengths, variable topologies, and changing objectives.stanfordvl.github.io\\/ntp\\/.'\",\"399\":null,\"400\":null,\"401\":\"'SLAM approaches evolve with the development of sensors and computation platforms. At first, SLAM was mostly applied on robots equipped with wheel encoders and range sensors. Such SLAM system uses a Kalman Filter [2] with the assumption of linearly approximated model with Gaussian noise to jointly estimate the robot pose and a map (e.g., a set of landmarks), or a particle filter [3] to build multiple hypotheses to localize within a global map. Among all range sensors, laser range finder was especially popular because of its accuracy and stability. For example, in the 2005 DARPA Grand Challenge [4], five Sick AG LIDARs were mounted on the roof with a GPS for localization and mapping. Due to hardware limitations, vision-based SLAM algorithms were not popular at that time.'\\n\\n'On top of the PIRVS hardware, we design the PIRVS system running an Android system without middleware layer which means: 1) it supports native C\\/C++ code through Android NDK, and 2) it does not have the typical Android overhead (e.g., garbage collection from Java layer). The dependencies have been pre-installed in the image. Our SDK is cross-compiled on Linux. The potential acceleration via ARM NEON instrument sets is fully available for the users to take advantage of the heterogeneous ARM-based system. Fig. 3 illustrates our system architecture. In the native layer, in contrast to ROS, our design ensures that sensor data acquisition and algorithm execution are in a real-time fashion with minimum overhead. Therefore, a main thread is created to invoke each module in the pipeline at a rate which is fast enough for the most frequent data (e.g., IMU data). The modules which require intensive computation runs in their own threads to avoid blocking the main thread.'\",\"402\":\"'Table II: Size of the source code and average per frame processing speed over 10 runs on each KITTI sequence'\\n\\n'In this paper we present ProSLAM, a lightweight open-source stereo visual SLAM system designed with simplicity in mind. This work stems from the experience gathered by th...'\\n\\n'In this paper we present ProSLAM, a lightweight open-source stereo visual SLAM system designed with simplicity in mind. This work stems from the experience gathered by the authors while teaching SLAM and aims at providing a highly modular system that can be easily implemented and understood. Rather than focusing on the well known mathematical aspects of stereo visual SLAM, we highlight the data structures and the algorithmic aspects required to realize such a system. We implemented ProSLAM using the C++ programming language in combination with a minimal set of standard libraries. The results of a thorough validation performed on several standard benchmark datasets show that ProSLAM achieves precision comparable to state-of-the-art approaches, while requiring substantially less computation.'\\n\\n'In this paper we present ProSLAM (Programmers SLAM), a complete open-source 1 stereo visual SLAM system that combines well known techniques and encapsulates them into a single pipeline with separated components and clear interfaces. We further provide multiple code snippets that realize the core functionality of our system. ProSLAM is implemented in pure C++ and relies on very few publicly available external libraries such as Eigen and OpenCV to perform basic operations. In contrast to our previous work [4] this paper focuses more on the concepts beyond the proposed architecture rather than on coding aspects.'\\n\\n\\\"In this paper, we presented ProSLAM, a stereo visual SLAM system developed to be extended and used both as a learning tool, and as a framework to improve particular SLAM aspects. ProSLAM achieves high computational speed by using a simple yet effective combination of algorithms and data structures. In terms of accuracy, our approach is comparable to more sophisticated systems. Our statements are supported by comparative experiments on standard benchmarking datasets. All results of this paper can be reproduced by running the open-source implementation, spanning over about 5'000 lines of documented C++code.\\\"\",\"403\":null,\"404\":\"'http:\\/\\/www.edinburgh-robotics.org\\/students\\/raluca-scona'\\n\\n'The code and the demonstration video can be found here: http:\\/\\/www.edinburgh-robotics.org\\/students\\/raluca-scona.'\",\"405\":\"'Finally, we define our optimization function as a weighted combination of the above metrics, where a minimum value indicates the best set of viewpoints. The weights were set equally in our implementation, but can be adjusted to make the algorithm prioritize one factor highly over the others. This combined optimization function takes a set of candidate poses for all vehicles being used as its input and outputs a value of the objective function that encodes all the heuristics.'\",\"406\":\"'https:\\/\\/youtu.be\\/gUllR-CXwSM'\",\"407\":\"'Consider the task of an aerial vehicle charged with collecting images of a given set of locations. Such tasks arise in many applications such as crop monitoring, animal tracking and road inspection. In this paper, we study a novel coverage problem inspired by this scenario. We associate each measurement with an inverted cone apexed at the location of the interest. The height of the cone is associated with the desired resolution and the apex angle corresponds to camera field of view. In other words, each cone encodes the set of view points from which a target can be imaged at a desired location. See Figure 1. The task is to visit a given set of cones so as to ensure that all locations are covered.'\",\"408\":null,\"409\":null,\"410\":null,\"411\":null,\"412\":null,\"413\":null,\"414\":\"'We use strain gauges to measure joint torque. Even if we employ the highest resolution encoder currently available like a Renishaw optical sensor, shapes of torque sensors are limited such as long torsional bars or compliant spokes to ensure sufficient displacement. With these shapes, it is difficult to achieve compactness. Our sensors are designed to have a thin cylindrical flexure part with strain gauges attached and flanges to connect gears and links as shown in Fig. 3 and Fig. 5. The thickness of the flexure part is determined to be 0.15-0.2 mm in order to maximize the resolution while satisfying strength requirements.'\",\"415\":null,\"416\":null,\"417\":null,\"418\":\"'http:\\/\\/robotart.org\\/'\",\"419\":null,\"420\":\"'To capture both pose and force in hand-object interactions, we utilize an open-source tactile glove [3]. The tactile glove employs a network of 15 IMUs to measure the rotations between individual phalanxes. Hand pose is reconstructed using forward kinematics. With 6 customized force sensors using Velostat, a piezoresistive material, the force exerted by hand is recorded in two regions (proximal and distal) on each phalange and a\\\\n4\\u00d74\\\\nregions on the palm. The data is collected and visualized using the Robot Operating System (ROS).'\",\"421\":null,\"422\":null,\"423\":\"'We use different strategies to control the patient-side manipulator gripper. When we use the dVRK manipulator force feedback, the gripper jaws are controlled according to the measurement of the magnetic encoders located at the last joint of the master manipulator. When we use the skin deformation device for sensory substitution, the gripper jaws are controlled using the grip force sensor mounted on the skin deformation tactile device. The gripping angle was calculated from the grip force as'\",\"424\":null,\"425\":\"'Sensors are widely used in robotic systems to provide feedback to achieve a precise control of operations. However, the compliance of soft actuators precludes the use of many conventional sensors such as encoders, metal \\/ semiconductor strain gauges, or inertial measurement units for proprioception purposes [1]. As soft robots can deform in all directions throughout their entire body, sensors should be able to detect these unpredictable and large deformations. Moreover, ideal sensors for soft robots shall be bendable and\\/or stretchable, and shall have little influence on the performance of actuators. No off-the-shelf sensor meets these requirements.'\",\"426\":null,\"427\":null,\"428\":null,\"429\":null,\"430\":null,\"431\":null,\"432\":\"'https:\\/\\/goo.gl\\/s8vwdk'\",\"433\":null,\"434\":\"\\\"A motion capture system and the robot encoders recorded the hand movement; while the pressure sensors in RBO SoftHand 2 tracked the finger motion. The two force\\/torque sensors were placed at the robot end-effector and between the hand and the stick. Four cameras recorded the scene. Figure 2 displays the objects in the order they were grasped, from the subjects' point of view. The objects were diverse in size, weight and shape, and yet graspable by the soft hand.\\\"\",\"435\":null,\"436\":null,\"437\":null,\"438\":\"'A 3D model of the setup including the screw, load cell, and the frame submerged in glass beads. Particle velocities are color coded in this figure according to legend.'\",\"439\":null,\"440\":\"\\\"To address the first matter, we propose here to use SIMP (Solid Isotropic Material Penalization) formulation of topology optimization to design a novel amplification mechanism. This formulation uses penalization power law to make material intermediate density unattractive and therefore avoids the 0\\u20131 problem. Otherwise, it is mathematically well-posed and easy to implement. Indeed, a Matlab implementation including 99 code lines was proposed by Sugmund in [22]. To address the second matter we propose to substitute the OC (Optimality Criteria) method originally implemented in [22] by MMA method (Method Moving Asymptotes) in order to take into account more than one constraint. Finally once the optimization is realized under Matlab software, we propose an automatic procedure to extract the resulted structures. This step is important to convert the structures to a format compatible with CAO software's. To resume, this paper deals with the optimal design of an amplification mechanism for piezoelectric actuator. The contributions of the paper are:\\\"\\n\\n'an improvement of the 99 code of topology optimization presented in [22] by using MMA method,'\\n\\n'The base code is taken from the renowned article [22], using the compliance objective presented plotted in [8]. However several changes, apart from the design specification, have been made for this paper. First of all, the optimization of supports have been added by implementing the formulation of the optimization problem-I. Next, the MMA optimization algorithm was implemented, the one originally present being the OC algorithm. Finally a post processing was added and is discussed in the next section.'\\n\\n\\\"The design and the fabrication of a mechanism devoted to magnify the deformation of a piezoelectric actuator is presented. Commonly known as \\u201camplification mechanism\\u201d this design is obtained by utilizing the SIMP formulation of topology optimization method. This formulation has been chosen for many reasons: (i) unlike classical approaches, it leads to a systematic design (ii) it allows to avoid the 0\\u20131 material density problem (iii) it is mathematically well-posed and easy to implement. To take advantage of this method, we started by improving the SIMP code presented by Sigmund in [22]. Basically, we substituted the OC method reported in the original code by the MMA method [23] in order to extend the algorithm to problems with multiple constraints. Then, optimal designs were derived according to the amplifying specification. An analysis was carried out on the influence of the supports and volume using a post processing to select an optimal design. We observed that supports have a weak influence as above 10% designs don't take advantage of having more available supports. Moreover lower volume give better designs as high volume create bulky designs resulting in low performances. A finite element simulation and a experiment with a 3D printing realization were proposed to validate the performances of the selected optimal design. We obtained a theoretical amplification factor of 4.47 and a lower simulation and experimental result of 4.05. This difference is explained from the post processed.\\\"\",\"441\":null,\"442\":null,\"443\":\"'https:\\/\\/youtu.be\\/hfjeukpeedo'\\n\\n'https:\\/\\/youtu.be\\/hfJeUKpeeDO'\",\"444\":null,\"445\":null,\"446\":null,\"447\":null,\"448\":null,\"449\":\"'https:\\/\\/mistrygroup.bitbucket.io\\/pages\\/videos.html'\",\"450\":\"\\\"Many running animals, unlike their robotic counterparts, have distinct morphologies and functional roles for their front and rear legs. In this paper we present a new control approach for a 5kg autonomous dynamic quadruped that explicitly encodes separate roles for each contralateral pair of legs. This controller utilizes a functional dynamic decomposition similar to Raibert's three part control law, but focuses on fore-aft leg specialization to regulate the robot's performance. The velocity of this controller, which exceeds 5 body lengths per sec, is compared with an improved trajectory-based controller and shown to be significantly more robust to changes in environment.\\\"\",\"451\":\"\\\"Realistically, we cannot simply trust that the controller will adhere to the perfect timing of the scheduler by pulling the foot off the ground or placing it down again on time. Similarly, the ground cannot be assumed to be perfectly flat and may contain unexpected ground height due to rough terrain or unseen objects. Without an external force sensor, we must estimate the forces at the leg from the encoder data, proprioceptive forces, and the dynamics of the leg's model. Encoders will have discretization error and model inaccuracies will cause an external force to show when there is none. None of these single measurements will tell us the leg state, but we can take the information from each and use them to find a better estimate for whether or not a particular foot is likely to be in contact with the ground,\\\\ns\\\\n^\\\\n\\u2208{0=swing,1=contact}\\\\n. A perfect detection algorithm will have\\\\ns=\\\\ns\\\\n^\\\\n.\\\"\",\"452\":\"'Tab. I shows that our model achieves high accuracy across multiple terrain types. Region proposal and footstep prediction across an entire image takes around 2 seconds in unoptimized Python code on a 4.0 GHz Intel Core i7-6700K Quad-Core CPU. Fig. 6 shows some examples of predicted footholds on the testing set. We observe that it performs quite well at predicting that hands should not be used on flat ground and the synthetic \\u201cuneven terrain\\u201d of the rotated-down wall. Surprisingly, it also produces fairly reasonable estimates on the Furniture environment, which is drastically different from anything seen in training. We also observe that the predictions are more accurate for nearby terrain, which is possibly caused by the smaller region size and greater noise in depth at longer distance.'\\n\\n'Fig. 7 shows the results on some test terrains. We manually set a starting position at the bottom of the image and the goal to arrive anywhere above a given horizontal line. The planner takes around 30 seconds using unoptimized MATLAB code, with the running time dependent on the total number of predicted footholds. Observe especially in Figs. 7a, 7b, and 7f that the route correctly avoids areas that are impassable. In Figs. 7c, 7d, and 7e, there are many routes available, and the planned routes are relatively short.'\",\"453\":\"'https:\\/\\/youtu.be\\/QnFoMR47OBI'\\n\\n\\\"State-of-the-art robotic perception systems have achieved sufficiently good performance using Inertial Measurement Units (IMUs), cameras, and nonlinear optimization techniques, that they are now being deployed as technologies. However, many of these methods rely significantly on vision and often fail when visual tracking is lost due to lighting or scarcity of features. This paper presents a state-estimation technique for legged robots that takes into account the robot's kinematic model as well as its contact with the environment. We introduce forward kinematic factors and preintegrated contact factors into a factor graph framework that can be incrementally solved in real-time. The forward kinematic factor relates the robot's base pose to a contact frame through noisy encoder measurements. The preintegrated contact factor provides odometry measurements of this contact frame while accounting for possible foot slippage. Together, the two developed factors constrain the graph optimization problem allowing the robot's trajectory to be estimated. The paper evaluates the method using simulated and real sensory IMU and kinematic data from experiments with a Cassie-series robot designed by Agility Robotics. These preliminary experiments show that using the proposed method in addition to IMU decreases drift and improves localization accuracy, suggesting that its use can enable successful recovery from a loss of visual tracking.\\\"\\n\\n'Legged robots, unlike ground, flying, and underwater platforms, are in direct and switching contact with the environment. Leg odometry involves estimating relative transformations and velocity using kinematic and contact information, which can be noisy due to the encoder noise and foot slip [9]. Typically, legged robots are equipped with additional sensors (IMUs, cameras, or LiDARs) which also provide independent, noisy odometry measurements (shown in Fig. 1). Therefore, without a sound sensor fusion framework, the estimated trajectory can quickly become inaccurate as a consequence of significant drift over the traveled distance.'\\n\\n'Experiments were conducted on a cassie-series robot designed by agility robotics. The biped robot has 20 degrees of freedom, 10 actuators, joint encoders, an IMU, and a multisense s7 stereo camera.'\\n\\n\\\"Although the factor graph framework has been successful, most methods heavily rely on visual information and are prone to failure when visual tracking is lost, often due to lighting or scarcity of features. In these scenarios, leg odometry is a way to reduce drift; and hereby, the incorporation of contact and encoder measurements into the factor graph framework are addressed. In this paper, we develop two novel factors that integrate the use of multi-Iink Forward Kinematics (FK) and the notion of contact between the robotic system and the environment into the factor graph smoothing framework. The forward kinematic factor relates a sensor frame (such as a camera or IMU) to a contact frame through noisy encoder measurements. On the other hand, the contact factor preintegrates high-frequency foot contact measurements to describe the contact frame's movement over time. When combined, these novel factors constrain the robot's net movement, leading to improved state estimation. In particular, this work has the following contributions:\\\"\\n\\n'An FK factor that incorporates noisy encoder measurements to estimate an end-effector pose at any time-step;'\\n\\n'In this section, we formulate the state estimation problem of the legged robot. The biped robot is equipped with a stereo camera, an IMU, joint encoders, and binary contact sensors on the feet. Without loss of generality, we assume the IMU and camera are collocated with the base frame of the robot. In current state-of-the-art visual-inertial navigation systems, the state includes the camera pose and velocity along with system calibration parameters such as IMU bias [14]. In the factor graph framework, independent measurements from additional sensors can be incorporated by introducing additional factors based on the associated measurement models. Foot slip is the major source of drift in leg odometry; as such, to isolate the noise at the contact point we augment the state at time-step\\\\ni\\\\nto include the contact frame pose of both feet (in the world frame)\\\\nC\\\\ni\\\\n\\u25b3={\\\\nR\\\\nWC,l\\\\n(i)\\\\n}\\\\n2\\\\nl=1\\\\nand\\\\nd\\\\ni\\\\n\\u225c{w\\\\np\\\\nWC,l\\\\n(i)\\\\n}\\\\n2\\\\nl=1\\\\n. However, without loss of generality, all following derivations are for a single contact frame. Thus, the state at any time-step\\\\ni\\\\nis represented as:'\\n\\n'Changes in joint angles affect the orientation of all link frames further down the kinematic tree. The following Lemma shows how the angle offsets propagate through the FK functions which is important for dealing with encoder noise.'\\n\\n'A. Contact Pose Through Encoder Measurements'\\n\\n'The encoder measurements are assumed to be affected by additive Gaussian noise,\\\\n\\u03b7\\\\n\\u03b1\\\\n\\u223cN(0,\\\\n\\u03a3\\\\n\\u03b1\\\\n)\\\\n.'\\n\\n'In addition to the encoders, it is assumed that a separate binary sensor can measure when the robot is in contact with the static world. If the contact is rigid (6-DOF constraint), then both the angular and linear velocity of the contact frame are zero; i.e\\\\nc\\\\n\\u03c9\\\\nwc\\\\n(t)=w\\\\nv\\\\nC\\\\n(t)=\\\\n0\\\\n3,1\\\\n. Therefore,'\\n\\n'The encoder measurements at time\\\\nt\\\\ni\\\\nare already being used for the forward kinematic factor (Section V). Therefore, to prevent information double counting, the first term in the summation can be replaced with the state estimate at time\\\\nt\\\\ni\\\\n. After a first-order approximation of the forward kinematics function, we arrive at the preintegrated point contact position measurement\\\\n\\u0394\\\\nd\\\\n~\\\\nij\\\\nand its noise\\\\n\\u03b4\\\\nd\\\\nij\\\\n:'\\n\\n'We evaluated our factor graph implementation using real measurement data collected from a Cassie-series robot. This data included IMU measurements and joint encoders values. Cassie has two springs, located on each leg, that are compressed when the robot is standing on the ground. The binary contact measurement was computed using measurements of these spring deflections. The data was collected at 2KHz.'\\n\\n'We are currently in the process of developing the open source implementation of the proposed techniques and performing additional evaluations in real experiments. In the future, we plan to use these techniques to build up both local and global maps that can be used for path planning as well as to inform the robot controller about the surrounding terrain.'\",\"454\":\"'https:\\/\\/youtu.be\\/YChlga1wwAc'\\n\\n'Commanding mobile robot movement based on natural language processing with RNN encoder\\\\xaddecoder'\",\"455\":null,\"456\":\"'The emorl model consists of 4 components: Gated recurrent unit (GRU), emotion classification (EC), action selection (AS) and baseline reward estimator (BRE). The GRU encodes the acoustic information as a fixed-length vector allowing to model long-term dependencies of a speech signal which is used as a state representation in our system. EC is a single layer module which uses the state representation to evaluate the probability of the human speaker being in an angry state. AS and BRE are also single layer modules which determine the probability distribution over possible actions and the estimation of the baseline reward.'\",\"457\":\"'We would like to thank Dr. Stefanie Tellex for providing the code for modeling natural language command understanding [22] and the dataset [1] that was adapted in this work. This research is supported in part by the NASA grant NNXI7 AD06G.'\",\"458\":\"\\\"In details, the default behaviour of the robot consists in moving forward and avoiding obstacles when necessary. The user can control it by his\\/her brain activities, delivering voluntary commands (left and right) to turn it to the corresponding direction. The user's intention is decoded by the BCI system and the related command is sent to the ROS node dealing with navigation through an UDP packet.\\\"\\n\\n\\\"The logic of our algorithm is described in the following pseudo-code. At every iteration, our algorithm sends new navigation goals to the robot to ensure the robot capability of avoiding obstacles in the environment-especially the dynamic obstacles not represented in the static map. This way, the planner in the navigation stack, can (re)compute the best trajectory to reach the target destination even if dynamic obstacles are presented in the path. In details, we used the Dynamic Window Approach [21] for local planner and Dijkstra's algorithm to compute the global planner function. Furthermore, before sending a new goal to the robot, the corresponding position in the map is checked: the goal is sent to the robot only if it matches with a free cell in the map, which means that that cell is not occupied by an obstacle. Otherwise, the RECOVERYBEHAVIOUR() procedure is called to avoid deadlocks by slightly moving the robot. In detail, the RECOVERYBEHAVIOUR() makes the robot go back (if it is possible) and keep turning counterclockwise until required. The recovery rotation takes place always in the same direction (by fixed angle A) to make the robot able to rotate around itself and, thus, to escape from this undesirable situation. Furthermore, the rotation is carried out incrementally by sending velocity commands to the robot. If the robot cannot go back and\\/or turn due to obstacles, the on-board short-range sonars will stop it.\\\"\",\"459\":\"'An Electroencephalography (EEG)-based non-invasive BMI is an appropriate method of prosthetic control for a majority of amputees. However, there are computational barriers for such BMI-decoder that limits its support for manipulating objects during the activities of daily living. Due to the variety of 3D shape, prosthesis requires computation of kinematics and kinetics of fingertips for safe grasping of these objects. A natural hand achieves this through precise temporal activation of relevant forearm muscle synergies. For a BMI-based prosthesis to calculate such kinematics, the decoder requires distinguishing EEG patterns that represent the cortical activities during the activation of these synergies. This is challenging as EEG suffers from poor spatial resolution and volume conductance effect [4], [5]. The EEG channels which monitor the primary motor cortex sub-serving the hand muscles, receive a mixture of neural activity from several nearby neuron populations that activate different hand muscles. Thus, distinguishing the activity of different forearm muscle synergies from EEG results in less accuracy.'\\n\\n'To address this issue, first, the prosthetic hand was mechanically improved by enabling adaptive grasping so that it eliminates the need for explicit calculation of kinematics for grasping. Secondly, a novel BMI decoder based on a simplified model of motor control during hand opening and closing through neuro-muscular synergies was developed. The integrated framework was experimentally validated to prove its feasibility on voluntary object grasping through the non-invasive BMI.'\\n\\n'Figure 4 depicts the finite automata used by the FaNeu-Robot framework. The SNN was trained to decode e1 and e2 from EEG. The model was applied for online control of a robot arm through BMI.'\\n\\n'This research presents the feasibility of continuous voluntary control of a hand prosthesis through a non-invasive BMI. The mechanical aspects of the hand and as well the computational aspects of the BMI decoder were improved to facilitate safe and accurate grasping.'\\n\\n\\\"Proper positioning of fingers is necessary for secure grasping of an object. However, decoding this kinematics from EEG is challenging for a non-invasive BMI decoder. Thus, the mechanical design of the prosthetic hand was improved to facilitate adaptive grasping of objects. Force supplied by monofilament nylon wires throughout the fingers provided differential adaptive grasping. This approach eliminated the need for kinematics and path planning. Predicting the specific grasp type that is more associated with the object's shape and orientation from EEG was not required. Thus, predicting only the opening and closing of fingers was adequate for grasping. This mechanical enhancement reduced the computational complexity of the classification problem.\\\"\\n\\n'Proper positioning of fingers of the prosthetic hand is necessary for secure grasping of an object. However, decoding these finger kinematics from EEG is challenging for a non-invasive BMI decoder due to the poor spatial resolution and volume conductance effect of EEG. In this research, we have shown that this limitation can be overcome by improving the mechanical design of the hand. The adaptive grasping and proper positioning of the thumb described in this paper enabled secure grasping of many objects regardless of its shape and orientation. In addition, we have shown that a simplified behavioural model which model the dynamic behaviour of neuro-muscular synergies can reduce the computational complexity of the BMI decoder. It is equally important to choose the proper model from all possible models which will maximize the inter-class separability.'\",\"460\":null,\"461\":\"'In recent years, there has been a steady trend towards applying deep networks for various robotics tasks, where early layers act as a feature encoder with a supervised loss for the desired task on the output of the network. Unfortunately, even such powerful models still suffer from the problem of shifts in domain appearance. This has prompted a number of works which try to address this issue [3], [9], [16], [5]\\u2013.'\\n\\n'Network architecture and information flow for iada. After the optimisation of source encoder and supervised model, the target encoder is trained to confuse the domain discriminator, leading to domain invariant feature representations. During deployment, the target encoder is connected to the supervised module. Dotted arrows represent only forward passes while solid lines display forward and gradient backward pass.'\\n\\n'The target encoder weights are initialised with parameters from the source encoder trained on the supervised task. These inchoate parameters are then subsequently adapted to align to the currently encountered target data by optimising both the target encoder and discriminator using the objectives in Equations 1 and 2 respectively. Intuitively, this procedure entails using the optimised parameters from the previously encountered target domain as initialisation for adapting to the current domain. The currently encountered unsupervised data is hereby utilised to fill a buffer from which is continuously sampled for the domain adaption training procedure.'\\n\\n'Network architecture and information flow for training with a generative model approximating the marginal source feature distribution. The approach additionally trains a GAN during the source training procedure but does not propagate gradients for the adversarial loss to the source encoder to ensure unmodified source domain performance. Subsequently the target encoder is trained to mimic the feature distribution of the - now fixed - gan. Dotted arrows represent only forward passes while solid lines display forward and gradient backward pass.'\\n\\n'Subsequently during the domain adaptation procedure, the target encoder is optimised to align to the feature distribution of the GAN, whose parameters remain static to model the source domain. Instead of optimising the discriminator to classify between source and target domain, in this scenario it learns to distinguish between synthetically generated source features and actual target features encoding target images. Target encoder and discriminator are optimised towards the objectives\\\\nL\\\\nE\\\\nt\\\\nand\\\\nL\\\\nD\\\\nin Equations 5 and 6 respectively.'\\n\\n'Table I shows the target domain classification accuracy of 1-step adaptation methods against their incremental counterparts which continue optimising the target encoder across domains with incrementally increasing domain shift. Furthermore, we test the methods in combination with GAN-based Source Domain Modelling (SDM) introduced in Section III-A.'\",\"462\":null,\"463\":\"'Finally, we encode each vehicle ground-truth motion vector attending to its angle and magnitude according to the color-code typically used to represent optical flow. Figure 2b shows a frame sample of the described dataset, where the corresponding RGB image of the scene is also shown just for comparison purposes.1.'\",\"464\":\"'http:\\/\\/spatialrelations.cs.uni-freiburg.de\\/'\\n\\n'https:\\/\\/github.com\\/philjd\\/generalize_spatial_relations'\\n\\n'http:\\/\\/spatialrelations.cs.uni-freiburg.de\\/'\\n\\n'https:\\/\\/github.com\\/philjd\\/generalize_spatial_relations'\",\"465\":null,\"466\":\"'DPDB-Net: Exploiting Dense Connections for Convolutional Encoders'\\n\\n'Densely connected networks for classification enable feature exploration and result in state-of-the-art performance on multiple classification tasks. The alternative to dense networks is the residual network which enables feature re-usage. In this work, we combine these orthogonal concepts for encoder-decoder architectures, which we call Dual-Path Dense-Block Network (DPDB-Net). We introduce a dense block which incorporates feature re-usage and new feature exploration in the encoder. Moreover, we discuss that feature re-usage by the residual network architecture leads to a feature map explosion in the decoder and, thus, is not advantageous in this part of the network. We evaluated our proposed architecture in multiple segmentation tasks and report state-of-the-art performance on the Freiburg Forest dataset and competitive results on the Cam Vid dataset.'\\n\\n'In order to recover the resolution loss induced by pooling layers, all current techniques make use of skip connections between their encoder and decoder parts. Skip connections between encoder and decoder help the upsampling path to recover fine-grained information from downsampling layers. Although with a different motivation, modern classification architectures like ResNet and DenseNet [12], [13] take advantage of skip connections, too, by propagating information from lower directly to higher layers within the encoder. Such connections facilitate gradient back-propagation to the bottom layers without magnitude reduction, thereby reducing the vanishing gradient problem.'\\n\\n'encoder'\\n\\n'decoder'\\n\\n'In contrary to previous works we will explore the potential of densely connected blocks, through our DPDB block, as an encoder-decoder architecture for semantic segmentation. Differently from previous approaches that use the same class of dense blocks homogeneously in the whole network, we found that different characteristics are required for the encoder and decoder part of FCNs. Our DPDB-Net is a new architecture that incorporates such demands for semantic segmentation tasks.'\\n\\n'DPDB-net architecture. Only convolutional, DPDB block, transition down, transition up and dense block layers are visualized. The network before the first transition up layer is considered the encoder part. Such part is constituted by DPDB blocks, while the rest of the network constitutes the decoder part and uses densely connected layers. Below the architecture we present a short description of each of the main building blocks of the proposed architecture.'\\n\\n'Table I DPDB architecture in more detail. For brevity transition down and transition up modules are not shown. The network has 217 convolutional layers. Most of them are in the encoder part (123 layers). The remaining layers are in the decoder (84 layers) and in the transitions with 5 layers each. In the table we use the following notation: DPDB stands for the dual-path dense-block module, DB for dense block and\\\\nN\\\\nC1\\\\nfor the number of classes'\\n\\n'Table II reports the obtained results and comparisons to the baseline. The experiments also points out the importance of the DPDB block in the encoder side of the proposed architecture. Even the recent proposed Global Convolution Network (GCN) [26] underperforms in comparison to our DPDB-Net. Additionally, we tested our architecture using only densely connected layers. We can notice that such architecture yields competitive results with GCN, however it is still not able to outperform to our complete approach.'\\n\\n'In this paper, we presented a new architecture called Dual-Path Dense-Block Network (DPDB-Net). We introduced a dense block that incorporates feature re-usage and new feature exploration in the encoder of fully convolutional networks. The resulting DPDB-Net is an architecture with 217 layers. It improves the state-of-the-art performance on a challenging unstructured semantic segmentation dataset (Freiburg Forest) and presents competitive results on the Cam Vid dataset without requiring post-processing, pre-training, or temporal regularization modules.'\\n\\n'Cascade Decoder: A Universal Decoding Method For Biomedical Image Segmentation'\\n\\n'Efficient data hiding in JPEG2000 images using sequential decoding of convolutional codes'\",\"467\":\"'We propose a \\u201cfollowing in front of the leader\\u201d robot behavior. Our implementation improves on previous work by featuring a motion model which predicts the trajectory of the user by reasoning about walking direction in the context of the local navigable surroundings. We use state-of-the-art CNN-based object detection, and all our code is freely available online as ROS modules. We proposed a simple error metric for this behavior and evaluated our system in easy and harder settings. The results are qualitatively good, especially for the easy setting. A user study would be required to make a formal claim, but informally we believe our following behavior feels natural and easy in our experiments.'\",\"468\":\"'Object Detection And Autoencoder-Based 6d Pose Estimation For Highly Cluttered Bin Picking'\",\"469\":\"'https:\\/\\/www.youtube.com\\/watch?v=L93X3zh1sQo'\\n\\n\\\"Fluent and safe interactions of humans and robots require both partners to anticipate the others' actions. The bottleneck of most methods is the lack of an accurate model of natural human motion. In this work, we present a conditional variational autoencoder that is trained to predict a window of future human motion given a window of past frames. Using skeletal data obtained from RGB depth images, we show how this unsupervised approach can be used for online motion prediction for up to 1660 ms. Additionally, we demonstrate online target prediction within the first 300-500 ms after motion onset without the use of target specific training data. The advantage of our probabilistic approach is the possibility to draw samples of possible future motion patterns. Finally, we investigate how movements and kinematic cues are represented on the learned low dimensional manifold.\\\"\\n\\n'B. Background of Variational Autoencoders'\\n\\n'C. Conditional Variational Autoencoders'\\n\\n'While VAEs are pure autoencoders, i.e. the likelihood of the data depends only on a single observed variable, we are interested in predicting future time steps given past observations. Thus, we require a sequential generative model that encodes the dynamics of the time series. In order to allow for online evaluations of the model, we aim at a feedforward model under the markov assumption. We propose an approach similar to the ideas presented in [12] and [21] but do not consider recurrent connections and extensions. Instead, we assume that the generative process consists of three components: an encoder\\\\nh\\\\ne\\\\n, a transitioner\\\\nh\\\\nt\\\\nand a decoder\\\\nh\\\\nd\\\\n, depending on parameters\\\\n\\u0398\\\\ne\\\\n,\\\\n\\u03b8\\\\nt\\\\nand\\\\n\\u03b8\\\\nd\\\\nrespectively. In general terms, the encoding and decoding of an observation\\\\nx\\\\nt\\u2212l\\\\nto predict the next observation\\\\nX\\\\nt\\\\nis described by'\\n\\n'encoded'\\n\\n'Five different samples of possible futures. The samples were generated by propagating the past motion window through the network, sampling from the encoder and transitioner distributions and visualizing the mean output of the decoder. We depict the past 800 ms and samples of the next 800 ms.'\\n\\n'The advantage of CVAEs over common autoencoders is that they model a probability distribution over future poses instead of a point estimate. Therefore, we can sample natural human motion from our model. In HRI it is of importance to be able to anticipate more than a single future. Even if the target of a human motion is fixed, the trajectories towards this goal can vary over time. The structure of the temporal CVAE allows us to sample from three different distributions, the encoder, the transitioner and the decoder. When propagating a window of past motion frames through the network at each of these layers, we can either use the mean estimate or a sample of the resulting distribution. When applying the mean in all layers, the output of the CVAE is a prediction. When sampling of at least one layer in the hierarchy, the output is a sample of the predictive distribution. To be of value for the robot, these samples should be in accordance with the current body configuration and represent natural motion patterns. In Figure 6 we showcase five samples of future motion given a past sequence. These samples were generated by propagating the input through the three models corresponding to the torso, the left arm and the right arm and sampling at both the encoder and transitioner stage. The resulting samples are in accordance with the input data, while varying slightly from sample to sample. Online demonstrations of this method can be found in the at https:\\/\\/www.youtube.com\\/watch?v=L93X3zh1sQo.'\\n\\n\\\"Due to the encoder-decoder structure of temporal CVAEs, these models do not only provide a mechanism to predict and generate natural human motion but they also learn a low dimensional manifold of the data. Ideally, this manifold should disentangle factors of variation. For example, when moving an arm up along a line, the underlying dynamics could be explained by a single dimension. When the body rotates and the arm movement is kept along the same line, the first dimension must not change, but an additional dimension needs to account for the rotation. The same principle should hold for more subtle kinematic cues such as legible and predictable motion. A legible motion lets an observer infer the goal given an observed trajectory, while a predictable motion is the trajectory that an observer would expect given a goal [13]. Being able to distinguish these kinematic cues could facilitate a natural human-robot interaction. As discussed above, predictable and legible motion of a reaching movement should live in the same low dimensional space but be distinguishable within this space. A well separated low dimensional space would facilitate the robot's decision making compared to noisy signals of high dimensional arm trajectories.\\\"\",\"470\":\"'Prediction results in simulated interactive scenarios. Predicted distributions are color-coded, augmented with the ground truth position shown as a dot in contrasting color. Top row: Schematic depiction of the situation, dashed lines show the path of each person. Left: Two people walking together side-by-side, sharing a common goal ahead of them. Right: People walking in opposite flows, two of them make room for the third person walking in between.'\",\"471\":null,\"472\":\"'For encoding purposes, codebooks for each descriptor (Trajectory, HOG, HOF, MBHx, MBHy) are constructed during the training phase from a subset of randomly selected training features using K-means. The centroid of each cluster is defined as a visual word, and each trajectory is assigned to its closest visual word using the Euclidean distance. We use Bo Vw encoding, i.e., a histogram of visual word occurrences, yielding a sparse\\\\nK\\\\n-dimensional video representation, which is essentially the histogram of visual word occurrence frequencies over the space-time volume. Videos are classified based on their Bo Vw representation, using non-linear support vector machines (SVMs) with the\\\\n\\u03c7\\\\n2\\\\nkernel [30]. In addition, different descriptors are combined, by computing distances between their corresponding BoVW histograms as:'\",\"473\":null,\"474\":null,\"475\":null,\"476\":null,\"477\":\"'We present a semantically rich graph representation for indoor robotic navigation. Our graph representation encodes: semantic locations such as offices or corridors as no...'\\n\\n'We present a semantically rich graph representation for indoor robotic navigation. Our graph representation encodes: semantic locations such as offices or corridors as nodes, and navigational behaviors such as enter office or cross a corridor as edges. In particular, our navigational behaviors operate directly from visual inputs to produce motor controls and are implemented with deep learning architectures. This enables the robot to avoid explicit computation of its precise location or the geometry of the environment, and enables navigation at a higher level of semantic abstraction. We evaluate the effectiveness of our representation by simulating navigation tasks in a large number of virtual environments. Our results show that using a simple sets of perceptual and navigational behaviors, the proposed approach can successfully guide the way of the robot as it completes navigational missions such as going to a specific office. Furthermore, our implementation shows to be effective to control the selection and switching of behaviors.'\\n\\n'An open source implementation that will be available online to foster further research in this area.'\",\"478\":\"\\\"In order to compare our results with the original planner's, we run the footstep planner on flat surfaces (using the same point clouds that we acquired), to produce 6 steps. In this case, ~2, 000 states were expanded in average, for a total planning time of ~10sec. From the statistical results in Tb. I, we can first see that our algorithm expands a small number of states, since it finds optimal paths through the rough terrain. Notice also that any planning of the original planner around an obstacle, may cause a significant increase in the number of states expansions. Secondly, we notice that the number of patches that are required are not that many, but most of them are declined due to bad fitting and, thus, representation of the underlying surface. Moreover, the overall planning is sufficiently fast, compared also to the original planner. The timing increases with the number of expanded states, the retrieved steps, and the fitted patches. Our code is not optimized and even though the patch fitting process does not take much time (l ms per patch), the implementation of the lookup tables\\/matrices and the conversions between the world and the camera frame are not computational optimal and should be further improved.\\\"\",\"479\":null,\"480\":null,\"481\":null,\"482\":\"'https:\\/\\/github.com\\/carla-simulator\\/imitation-learning'\\n\\n'https:\\/\\/github.com\\/carla-simulator\\/imitation-learning'\\n\\n'Felipe Codevilla'\\n\\n'We now describe a practical implementation of command-conditional imitation learning. Code is available at https:\\/\\/github.com\\/carla-simulator\\/imitation-learning.'\\n\\n'Antonio M. L\\u00f3pez and Felipe Codevilla acknowledge the Spanish project TIN2017-88709-R (Ministerio de Economia, Industria y Competitividad) and the Spanish DGT project SPIP2017-02237, the Generalitat de Catalunya CERCA Program and its ACCIO agency. Felipe Codevilla was supported in part by FI grant 2017FI-B1-00162. Antonio and Felipe also thank Germ\\u00e1n Ros who proposed to investigate the benefits of introducing route commands into the end-to-end driving paradigm during his time at CVC.'\",\"483\":\"'https:\\/\\/github.com\\/mbojarski\\/VisualBackProp'\\n\\n'http:\\/\\/benchmark.ini.rub.de\\/?section=gtsdb&subsection=dataset'\\n\\n'http:\\/\\/image-net.org\\/challenges\\/LSVRC\\/2016\\/'\\n\\n'We first demonstrate the performance of VisualBackProp on the task of end-to-end autonomous driving, which requires real-time operation. The codes of VisualBackProp are already publicly released at https:\\/\\/github.com\\/mbojarski\\/VisualBackProp. The first set of experiments is conducted on the PilotNet and aims at validating whether VisualBackProp is able to capture the parts of the image that are indeed relevant for steering the self-driving car. The next set of experiments were performed on the Udacity self-driving car data set (Udacity Self Driving Car Dataset 3-1: El Camino). We qualitatively compare our method with LRP, the state-of-the-art deep learning visualization technique, (we use implementation as given in Equation 6 from [15]; similarly to the authors, we use\\\\n\\u03f5=100\\\\n) and we also compare their running times4. We finally show experimental results on the task of the classification of traffic signs on the German Traffic Sign Detection Benchmark data set (http:\\/\\/benchmark.ini.rub.de\\/?section=gtsdb&subsection=dataset) and also ImageNet data set (http:\\/\\/image-net.org\\/challenges\\/LSVRC\\/2016\\/). Therefore we demonstrate the applicability of the technique to a wide-range of learning problems.'\\n\\n'https:\\/\\/github.com\\/mbojarski\\/VisualBackProp'\",\"484\":\"\\\"An example of an urban driving scenario with additional information on the future ego-vehicle motion provided by its static environment. The ego-vehicle (blue box) is positioned in the center of the grid heading to the top. Its static environment is represented in shades of gray depending on the measured value of each grid cell's probability for being free space, while the red line indicates the previously driven path color-coded with respect to its velocity profile.\\\"\\n\\n'Schematic illustration of the architecture of the proposed CNN with examples of the input channels, the prediction map representing the path prediction and the ground truth label. The output of each functional layer of the network is represented by a rectangle color-coded according to the type of layer that produced it. The shape of the output data structures is written on each rectangle in the following format: batch size (b) x height x width x channel. Black arrows represent the transfer of the stored pooling indices from each pooling layer to their corresponding unpoolig layer.'\\n\\n'The proposed architecture is a convolutional feed forward neural network (CNN) that follows this paradigm as it is designed as a symmetric encoder-decoder structure. It is build with four types of functional layers: convolutional layers,\\\\n2\\u00d72\\\\nmax pooling layers,\\\\n2\\u00d72\\\\nunpooling layers and a softmax output layer. All convolutional layers are two- dimensional convolutions with a kernel of\\\\n3\\u00d73\\\\nand a stride of 1. They are all, except the last one, followed by a batch normalization [13] and ReLU activation [14]. The output of the proposed architecture are two grids, one for each class, of the same spatial resolution as the input that are normalized using a softmax function over the two classes within each grid cell. Furthermore, symmetric padding is applied to the spatial dimensions of all convolutional layers and to the first and last unpooling layer to match spatial resolutions. Fig. 2 gives an overview of the proposed architecture and lists all functional layers with their output data structure resolution. The proposed model uses the same architectural concept as SegNet [15], but some adjustment for the given problem were made: The total number of free parameters (approximately 2.5 million) is kept smaller to control overfitting and the depth of the network was set such that the receptive field of each grid cell in the output covers the whole input grid.'\",\"485\":\"'Deep learning has been successfully applied to \\u201cend-to-end\\u201d learning of the autonomous driving task, where a deep neural network learns to predict steering control commands from camera data input. However, the learned representations do not support higher-level decision making required for autonomous navigation, nor the uncertainty estimates required for parallel autonomy, where vehicle control is shared between human and robot. This paper tackles the problem of learning a representation to predict a continuous control probability distribution, and thus steering control options and bounds for those options, which can be used for autonomous navigation. Each mode of the distribution encodes a possible macro-action that the system could execute at that instant, and the covariances of the modes place bounds on safe steering control values. Our approach has the added advantage of being trained on unlabeled data collected from inexpensive cameras. The deep neural network based algorithm generates a probability distribution over the space of steering angles, from which we leverage Variational Bayesian methods to extract a mixture model and compute the different possible actions in the environment. A bound, which the autonomous vehicle must respect in our parallel autonomy setting, is then computed for each of these actions. We evaluate our approach on a challenging dataset containing a wide variety of driving conditions, and show that our algorithm is capable of parameterizing Gaussian Mixture Models for possible actions, and extract steering bounds with a mean error of only 2 degrees. Additionally, we demonstrate our system working on a full scale autonomous vehicle and evaluate its ability to successful handle various different parallel autonomy situations.'\\n\\n'End-to-end deep learning of control distributions. The output of the classification network encodes a distribution of control values, rather than a single value in previous end-to-end approaches [1]. Consequently, we can use the output for navigation and decision making. Furthermore, we can use our approach as part of a \\u201cparallel autonomy\\u201d shared control system in which sets of upper and lower bounds for steering trajectories can be computed and used as controller takeover points.'\\n\\n'In addition to offline testing of our algorithm, we also implemented our entire codebase to run onboard a laptop controlling the car using drive-by-wire. This includes running the entire inference and posterior estimation in real time (15Hz) to compute an autonomous steering command, as well as the corresponding bounds. We use an Intel i5 CPU with 4 cores to perform our computation, and in the future plan to also install an NVIDIA Drive PX2 inside of our vehicle to increase neural network inference speeds further.'\\n\\n'This paper presents a novel deep learning based algorithm for autonomous driving that computes an intermediate probability distribution of steering angles. While previous work in end-to-end learning presents a form of reactionary control, lane following, and object avoidance, this technique encodes a much richer set of information in a probability distribution, thereby making it an attractive algorithm for incorporating navigation and decision making capabilities. Our results indicate the ability to dynamically compute the number of potential actions at any point in time and to accurately extract steering bounds from each of these mixtures with a mean deviation error of approximately 2 degrees.'\",\"486\":\"'Learning-based methods have demonstrated clear advantages in controlling robot tasks, such as the information fusion abilities, strong robustness, and high accuracy. Meanwhile, the on-board systems of robots have limited computation and energy resources, which are contradictory with state-of-the-art learning approaches. They are either too lightweight to solve complex problems or too heavyweight to be used for mobile applications. On the other hand, training spiking neural networks (SNNs) with biological plausibility has great potentials of performing fast computation and energy efficiency. However, the lack of effective learning rules for SNNs impedes their wide usage in mobile robot applications. This paper addresses the problem by introducing an end to end learning approach of spiking neural networks for a lane keeping vehicle. We consider the reward-modulated spike-timing-dependent-plasticity (R-STDP) as a promising solution in training SNNs, since it combines the advantages of both reinforcement learning and the well-known STDP. We test our approach in three scenarios that a Pioneer robot is controlled to keep lanes based on an SNN. Specifically, the lane information is encoded by the event data from a neuromorphic vision sensor. The SNN is constructed using R-STDP synapses in an all-to-all fashion. We demonstrate the advantages of our approach in terms of the lateral localization accuracy by comparing with other state-of-the-art learning algorithms based on SNNs.'\\n\\n'For communicating with robot sensors and motors in SNNs, the sensory information should be encoded into input spikes and the output spikes should be decoded into motor commands. Similar processing procedure for the encoding and decoding can be found in [9] The same model is implemented in this paper as well with only one change. Instead of steering angles, turn speeds are computed and added or subtracted for left and right motor. First, the output spike count\\\\nn\\\\nleft(right)\\\\nt\\\\nis scaled by the maximum possible output\\\\nn\\\\nmax\\\\n:'\",\"487\":null,\"488\":null,\"489\":null,\"490\":\"'The contribution of this paper is twofold. First, we introduce a quantitative analysis of object irregularity and how the planning strategy is closely coupled with the object shape. Second, we propose a \\u201cnext best step\\u201d planning method that autonomously chooses assembly actions to allow construction with objects that contain a significant amount of variation. Also, the geometric analysis and reduction process, that are essential components of the proposed planning method, are concepts that are transferable to other systems, whereas the heuristic approaches and other hard-coded implementations may differ for other object categories and targets.'\",\"491\":null,\"492\":\"'We found in our experiments that many bottleneck architectures (with an encoder and a decoder) could result in good performance. We chose the final structure based on [3] for the sake of benchmarking, because it achieved state-of-the-art accuracy in RGB-based depth prediction. The network is tailed to our problem with input data of different modalities, sizes and dimensions. We use two different networks for KITTI and NYU-Depth-v2. This is because the KITTI image is triple the size of NYU-Depth-v2 and consequently the same architecture would require 3 times of GPU memory, exceeding the current hardware capacity. The final structure is illustrated in Figure 2.'\",\"493\":null,\"494\":null,\"495\":\"'Deep Neural Networks We adapt the architecture for our encoder networks from He et al. [17] who have shown impressive results for image classification tasks. The fully connected layers are removed which makes the networks can cope with arbitrary sizes of input images. The decoders for gradient map and overlap map are based on the UNet [18] which introduces the skip connection between down-sampling and up-sampling layers. We use the Adam solver [19] with a batch size of 1. The Pytorch1 framework is adopted to implement the deep convolution neural networks and the experiment platform is a workstation with Xeon E5-2630 and NVIDIA GeForce Titan Xp.'\",\"496\":\"'http:\\/\\/data.csail.mit.edu\\/active_clothing\\/Data_ICRA18.tar'\",\"497\":null,\"498\":null,\"499\":null,\"500\":null,\"501\":null,\"502\":null,\"503\":null,\"504\":null,\"505\":\"'The OEM were re-sampled at 500Hz and played onto the HOAP3 robot. Because there is no interaction with the environment the trajectories were played with a top coder accuracy. Actual force sensor data and joint angles were measured synchronously and low-pass filtered (zero-phase forward and reverse filter Butterworth 5th order, 10Hz with backward difference) to calculate joint velocities and accelerations.'\",\"506\":null,\"507\":\"'However, the setup of a reference frame common to all sensors often requires control points, known landmark, or reference objects with known geometry. Hence, we refer to Model A as marker-based approach. The requirement of a common frame is a major disadvantage as they are hard or even impossible to achieve in some cases. Consider hand-eye calibration problem for example, where the encoder of the robot arm measures nothing other than its own rotation. Thus a direct shared measurement frame with camera is not possible. Calibration with the wheeled odometry of mobile robot has similar problems. Another generally known example is camera to camera calibration where the cameras have non-overlapping views. In this case, a single checkerboard is not sufficient and a more elaborate infrastructure is required. Beside the landmarks may be hard to setup, we also need to make sure their pose information are error free, because otherwise the estimation result will be biased and may contain systematic errors.'\",\"508\":null,\"509\":\"'https:\\/\\/gitlab.com\\/srrg-software\\/srrg_mpr'\\n\\n'The ability to build maps is a key functionality for the majority of mobile robots. A central ingredient to most mapping systems is the registration or alignment of the recorded sensor data. In this paper, we present a general methodology for photometric registration that can deal with multiple different cues. We provide examples for registering RGBD as well as 3D LIDAR data. In contrast to popular point cloud registration approaches such as ICP our method does not rely on explicit data association and exploits multiple modalities such as raw range and image data streams. Color, depth, and normal information are handled in an uniform manner and the registration is obtained by minimizing the pixel-wise difference between two multi-channel images. We developed a flexible and general framework and implemented our approach inside that framework. We also released our implementation as open source C++ code. The experiments show that our approach allows for an accurate registration of the sensor data without requiring an explicit data association or model-specific adaptations to datasets or sensors. Our approach exploits the different cues in a natural and consistent way and the registration can be done at framerate for a typical range or imaging sensor.'\\n\\n'To further stress the generality of our method, we conducted an additional experiment using exactly the same code and parameters using the sequence 10 of the KITTI dataset [4] where we used the 3D scans obtained with a Velodyne HDL-64E LIDAR. As in the previous experiment, the range and normals cues are used. Fig. 6 illustrates the error reduction after the registration of two scans.'\\n\\n'Our method is a general and efficient framework for multi-cue sensor data registration. We release a C++ implementation that closely follows the description in this paper as open source. We implemented our general methodology for the RGBD Kinect sensor and 3D laser scanners using the following cues: intensity, depth, range, and surface normal and thus the evaluation is done based on these sensors and cues. Note, that further cues or similar sensors can be added easily.'\",\"510\":null,\"511\":\"'By using a global energy approach, our proposed approach is able to detect multiple homographies in images as shown in the top row. These homographies encode planar priors which are used as input into the planar labelling of the image pixels. The bottom row shows the results of the pixel labelling using the extracted planar priors.'\",\"512\":null,\"513\":\"'The resulting 3D map from an out\\/in-door environment. Color encodes height from the ground.'\",\"514\":null,\"515\":null,\"516\":null,\"517\":null,\"518\":\"'http:\\/\\/www.robotarium.org\\/'\",\"519\":null,\"520\":null,\"521\":\"'Another approach is to shape the reward landscape. A common method of shaping is to encode more information of the task in the reward [10]. The drawback is that it requires more prior knowledge of the task, and goes against the attractiveness of being able to choose rewards based on achieving a task rather than specifying a behavior1. It is also possible to shape the landscape by proper mechanical design. For example, walking robots designed after passive-dynamic walkers [11] have good stability properties for a wide range of policy parameters, allowing quick and reliable learning from even poor initializations [6]. The drawback is that designing the system around one specific behavior can be limiting in terms of versatility and design options.'\\n\\n'Each DoF of the boom and leg is measured with a rotary encoder (CUI ATMI02-V). The boom arm has a length of\\\\n1.5[m]\\\\nfrom pivot to the leg, and is counterweighted to completely offset its own mass without the leg. The ankle joint of the leg (Fig. 3) is mechanically limited to 130\\u00b0 in one direction, and has a spring with a stiffness of\\\\n6 [\\\\nN\\\\nmm\\\\n]\\\\nattached to a cam mechanism with a radius of 15 [mm]. This spring is slightly preloaded such that it always returns to the resting angle of 130\\u00b0. The upper and lower leg segments measure 110 [mm] and 136 [mm] respectively, and the virtual leg length from hip to foot is 223 [mm] at rest. The hip is actuated with a brushless outrunner motor (T-motor MN-4006) with a 1:5 gearbox. The motor control board (Texas Instruments TMS320F28069M with DRV8305EVM booster packs) uses field-oriented control for direct torque control of the motor. A Xenomai real-time linux operating system handles high-level control. Electric power and computational power are both off-loaded via tether. A representative mass is directly attached on the boom just behind the leg. With the entire payload, the robot has a body weight of\\\\n600[g]\\\\n. For our two training wheel environments the representative mass is replaced with an intermediate mass or completely removed. This results in a body weight of\\\\n505 [g](0.84 \\\\ng\\\\n0\\\\n)\\\\nand\\\\n415 [g](0.69 \\\\ng\\\\n0\\\\n)\\\\nrespectively.'\\n\\n'We would like to thank \\u00d6zge Drama, Felix Grimminger and Julian Viereck for fruitful discussions, advice and help along the way. We also thank Felix Widmaier who wrote the code to run and interface with the motor control boards. We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting the academic development of Felix Ruppert. This work was partially supported by an MPI Grassroots grant provided to Ludovic Righetti, Felix Grimminger and Alexander Spr\\u00f6witz in 2017.'\",\"522\":\"'We selected the ball-in-a-cup task to assess how effective our incremental learning framework is in boosting up the learning speed when generalizing MPs to a new situation using a model-free RL method. The kinematics of the task are encoded using Dynamic Movement Primitives (DMPs). Our experiments demonstrated that the proposed model selection has correctly identified the required complexity for GPDMP outperforming Locally Weighted Regression (LWR) significantly. Above all, the proposed empirical Bayes method has led to a statistically significant improvement of model-free RL speed.'\",\"523\":null,\"524\":null,\"525\":null,\"526\":\"'https:\\/\\/youtu.be\\/HFkZkhGGzTO'\\n\\n'https:\\/\\/github.com\\/resibots\\/blackdrops'\\n\\n'https:\\/\\/github.com\\/resibots\\/blackdrops'\\n\\n'Code for replicating the experiments: https:\\/\\/github.com\\/resibots\\/blackdrops.'\",\"527\":\"'Given the outputs of the navigation computation graph, we now need to define how the task of collision-free robot navigation is encoded into the policy evaluation function\\\\nJ\\\\n.'\\n\\n'Experiment videos and code are provided on our website github.com\\/gkahn13\\/gcg.'\\n\\n'The RC car learning system was set up to maximize time spent gathering data, minimize computational burden for the car, and be fully autonomous. The computer onboard the RC car is an NVIDIA Jetson TX1, which is intended for embedded deep learning applications. However, all model training is performed offboard on a laptop for computational efficiency, while model inference is performed onboard. We therefore made the system fully asynchronous: the car continuously runs the reinforcement learning algorithm and sends data to the laptop, while the laptop continuously trains the model and periodically sends updated model parameters to the car. For full autonomy, the car automatically detects collisions using the onboard IMU and wheel encoder, and automatically backs up if a collision occurs. The only human intervention required is if the car flips over, which occurred approximately every 30 minutes.'\\n\\n'github.com\\/gkahn13\\/gcg'\",\"528\":null,\"529\":\"'Given a rich LiDAR data (64 laser beam from HDL64-E), we simulate a sparse LiDAR by sampling laser. The color is coded by the depth value.'\",\"530\":null,\"531\":null,\"532\":\"'A Fast Pose Estimation Method Based on New QR Code for Location of Indoor Mobile Robot'\",\"533\":\"'http:\\/\\/hcr.mines.edu\\/code\\/MOLP.html'\\n\\n'http:\\/\\/hcr.mines.edu\\/code\\/MOLP.html'\",\"534\":null,\"535\":null,\"536\":\"'https:\\/\\/youtu.be\\/SrqrGweKQAU'\\n\\n'https:\\/\\/youtu.be\\/M-ljAlKCqb8'\",\"537\":null,\"538\":null,\"539\":\"'https:\\/\\/youtu.be\\/rgweowQ8fAE'\",\"540\":\"'In order to alert the surgeon when he punctures a vessel, a peak detection algorithm is proposed. It is described in pseudo code in Algorithm 1 and further explained in the following text. The basic idea relies on catching up the moment when the impedance derives abnormally far from its current value. Considering the data is acquired continuously, a framed window takes into account enough samples to compute the mean and standard deviation over this frame, respectively noted\\\\n\\u03bc\\\\ns\\\\nand\\\\n\\u03c3\\\\ns\\\\n. Every new data leads to update the mean and standard deviation while the oldest data gets out of the framed window. Two user-defined parameters impact the behaviour of the detection method, namely\\\\n\\u03b1\\\\nt\\\\nand\\\\nw\\\\nt\\\\n. On one hand,\\\\n\\u03b1\\\\nt\\\\nis the number of\\\\n\\u03c3\\\\ns\\\\nthe current data can get away from the previous samples before a peak is detected. On the other hand,\\\\nw\\\\nt\\\\nis used if the current value is detected as a peak. It is a weight between 0 and 1 that balances the current data and previous data such that\\\\n\\u03bc\\\\ns\\\\nand\\\\n\\u03c3\\\\ns\\\\ndo not get too much impacted by an abnormal data. In other words, 0 means the current detected data will have no influence when updating\\\\n\\u03bc\\\\ns\\\\nand\\\\n\\u03c3\\\\ns\\\\n.'\",\"541\":null,\"542\":\"'However, none of these approaches explicitly encode the goal of extracting the shape of the stiff inclusion. The only goal that is encoded through a Bayesian optimization framework is to find the global maximum. As a consequence, the robot ends up mainly exploring around high stiffness regions before expanding to the boundary of the inclusion and other regions. Prior works commonly demonstrate results using a single stiff inclusion (single maximum) [15], [16]. When multiple inclusions are present (multiple global and local maxima) the algorithm is initialized with a coarse grid to ensure exploration of all regions [14].'\\n\\n\\\"In this work, we present a formulation that leverages state-of-the-art active learning methods as the objective to optimize robot's trajectories and explicitly encodes search of stiff regions and their boundaries. Compared to the existing works on active learning [19], [20], our formulation incorporates constraints due to the robot's motion model, restricts areas inhe search domain, and captures uncertainty in the measurements. We show experimental results with a variety of robotic platforms both using discrete probing and along a continuous path that is optimized using stochastic trajectory optimization 1.\\\"\\n\\n'We use the open source da Vinci Research Kit (dVRK) [32] for evaluating our approach on silicone tissue samples. The dVRK serves as a realistic surgical platform for evaluating the efficacy of tumor search algorithms. In order to perform palpation, we attach a custom 3D printed spherical-head tip to the 8mm needle driver tool of the robot. The silicone tissue sample with embedded stiff inclusions (see Fig. 1) is placed on top of an ATI Nano25 F\\/T sensor. Fig. 1 shows the stiffness map as estimated by our approach as well as the superimposed palpation trajectory. The stiffness map accurately reveals the stiff inclusions without wasting time exploring the softer regions in the bottom half of the tissue sample.'\",\"543\":null,\"544\":\"'To compensate for kinematic differences between the prosthesis and the training data, before sending prosthesis joint angles to the neuromuscular control, we add constant bias angles to the joint encoder readings so that at joint\\\\nj\\\\n,'\",\"545\":null,\"546\":\"'The visual odometry is performed by minimization of a multi-objective cost function, which includes terms that measure photometric and volumetric correlation. For every input RGB image, we create its depth image using the source code of the perspective shape-from-shading under realistic lighting conditions project [24]. Once the depth map is obtained, the framework uses both RGB and depth map information to jointly estimate camera pose. An energy minimization-based pose estimation technique is applied containing both sparse optical flow (OF) based correspondence establishment, and dense volumetric and photometric alignment [25]. Inspired from the pose estimation strategies proposed by [25], for a parameter vector'\",\"547\":null,\"548\":\"'The robotic finger testbed was designed using laser cut acrylic to keep the weights and inertias as low as possible. Bearings were utilized to minimize the effects of damping\\/friction. High strength fishing line was used for tendons to avoid tendon stretch. Series springs with a stiffness of 718 N\\/m were attached to each tendon which were then connected to two brushed DC gearmotors (Maxon motors) in a belt drive arrangement. The motors were controlled by Maxon EPOS motor controllers which were setup for closed loop position control tuned to behave like first order systems with a rise time of 0.044 seconds. Two high resolution rotary encoders (US Digital) were used to estimate joint angles. The higher level Cartesian stiffness controller was written in Labview (National Instruments) and was executed on an FPGA controller (NI Compact RIO). Motor pulleys were 57 mm in diameter and joint pulleys were all 34 mm in diameter. The finger was loaded by hanging weights using a pulley attached sufficiently far such that the angle of the load was within \\u00b15\\u00b0 of the target of \\u00b130\\u00b0.'\",\"549\":null,\"550\":null,\"551\":null,\"552\":null,\"553\":null,\"554\":null,\"555\":null,\"556\":\"'Each DoF consists of an electronically commutated 4-pole power motor (#305013, Maxon, Switzerland) that is connected to a gearbox of ratio 51:1 (#326664, Maxon, Switzerland) and drives a 40-mm radius multi-wrap pulley. An incremental encoder (#225778, Maxon, Switzerland) that has 2000 pulses per revolution is mounted on the motor to measure the motor position. A servomotor driver (Gold Twitter, Elmo Motion Control Ltd., Israel) integrated into a custom electronic board controls each motor in a closed loop. A removable battery consisting of two 6-cell Li-Po units (3.7Ah) is placed on the abdomen and it lasts approximately 7.5 km with a peak force of 300N. The overall system weight is 4.7 kg including 2.6 kg of the actuation unit, 1.1 kg of suit components and sensors, and 1.0 kg of battery.'\",\"557\":null,\"558\":null,\"559\":\"'The actuator package comprises nine DC motors concentric to the robot backbone. Each motor feature encoder feedback, allowing for direct feedback control. The motors are operated using an Arduino Due via a motor driver, which allows for speed control using a PWM signal.'\\n\\n'Each of the motors rests on a load cell, allowing for measurement of approximate tension feedback. The combination of encoder feedback and tension sensing allows for a more accurate estimation of tendon length than previous designs, to determine robot shape [24].'\\n\\n'In order to implement circumnutation motion primitives with the tendril robot, the tendons needed to be actuated in sequence. Such sequencing was achieved using a pattern of changing encoder counts. A small example portion is given in Table I. Each row in the table represents a set of encoder counts sent to the robot at once. Every time this occurs, the robot moves each motor for which the target encoder counts changes, i.e. implementing in hardware, the theory of circumnutation in Section II-B. This particular sequence of counts gives rotation and extension thus the appearance of growing, much like a plant would.'\\n\\n'Table I: Example encoder count sequence that results in circumnutation movement.'\",\"560\":null,\"561\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/32346494'\",\"562\":\"'https:\\/\\/www.youtube.com\\/playlist?list=pl_ksx9gon2p9rfuje04kqymlbeop-ivh2'\",\"563\":null,\"564\":\"\\\"OpenSim is an open source biomechanics simulation and analysis environment developed by Stanford-based Simbios [26] to simulate the behavior of multi-body dynamic and kinematic systems, effects of surgical procedures, static optimization of models, etc. For our particular area of interest, OpenSim provides a forward dynamics multi-body dynamics solver that uses a fifth order Runge-Kutta-Feldberg integrator to solve the system's governing equations [27].\\\"\\n\\n'The ultimate goal of the proposed research is to develop assistive wearable device that maintains stability and guidance for users through rehabilitation. Robust tensegrity flexural joints allow prosthetics or wearables to handle unanticipated impedances and increase joint strength with higher quality actuators or materials. This paper presents a tenseg-rity manipulator that consist of 3 compression elements (\\u201chip\\u201d, \\u201cfemur\\u201d and \\u201ctibia\\u201d) connected by two flexural joints (\\u201chip\\u201d and \\u201cknee\\u201d). The design is inspired by a human leg, and therefore, its behavior was simulated in the biomechanics open source environment OpenSim. A prototype was built and its kinematic behavior was monitored using an OptiTrack motion capture environment and compared with a human leg. It was found that the behavior of the proposed manipulator is comparable with a biological leg. Although the proof of concept prototype presented here was not intended to carry load, it could be envisaged that future research would lead to load carrying tensegrity legs.'\",\"565\":null,\"566\":\"\\\"We present a method for planning the motion of arbitrarily-shaped volumetric deformable bodies or robots through complex environments. Such robots have very high-dimensional configuration spaces and we compute trajectories that satisfy the dynamics constraints using a two-stage learning method. First, we train a multitask controller parameterized using dynamic movement primitives (DMP), which encodes various locomotion or movement skills. Next, we train a neural-network controller to select the DMP task to navigate the robot through environments while avoiding obstacles. By combining the finite element method (FEM), model reduction, and contact invariant optimization (CIO), the DMP controller's parameters can be optimized efficiently using a gradient-based method, while the neural-network's parameters are optimized using Deep Q-Learning (DQL). This two-stage learning algorithm also allows us to reuse the trained DMP controller for different navigation tasks, such as moving through different environmental types and to different goal positions. Our results show that the learned motion planner can navigate swimming and walking deformable robots with thousands of DOFs at realtime.\\\"\",\"567\":\"'https:\\/\\/youtu.be\\/poQytQ0PRd4'\\n\\n'https:\\/\\/goo.gl\\/PSb9zD'\\n\\n'The code for producing the approaching directions is available at https:\\/\\/goo.gl\\/PSb9zD.'\",\"568\":null,\"569\":\"'http:\\/\\/bit.ly\\/2xMcx3x'\\n\\n'http:\\/\\/goo.gl\\/6M5rfw'\\n\\n'We encode wrench resistance as a binary variable\\\\nW\\\\nsuch that\\\\nW=0\\\\nif\\\\nu\\\\nresists\\\\nw\\\\nand\\\\nW=0\\\\notherwise.'\\n\\n'Pipeline for generating the dex-net 3.0 dataset (left to right). We first sample a candidate suction grasp from the object surface and evaluate the ability to form a seal and resist gravity over perturbations in object pose, gripper pose, and friction. The samples are used to estimate the probability of success, or robustness, for candidate grasps on the object surface. We render a point cloud for each object and associate the candidate grasp with a pixel and orientation in the depth image through perspective projection. Training datapoints are centered on the suction target pixel and rotated to align with the approach axis to encode the invariance of the robustness to image locations.'\\n\\n'This research was performed at the AUTO LAB at UC Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab, the Real-Time Intelligent Secure Execution (RISE) Lab, and the CITRIS People and Robots (CPAR) Initiative. The authors were supported in part by donations from Siemens, Google, Honda, Intel, Comcast, Cisco, Autodesk, Amazon Robotics, Toyota Research Institute, ABB, Samsung, Knapp, and Loccioni, Inc and by the Scalable Collaborative Human-Robot learning (SCHooL) Project, NSF National Robotics Initiative Award 1734633. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Sponsors. We thank our colleagues who provided helpful feedback, code, and suggestions, in particular Ruzena Bajcsy, Oliver Brock, Peter Corke, Chris Correa, Ron Fearing, Roy Fox, Bernhard Guetl, Menglong Guo, Michael Laskey, Andrew Lee, Pusong Li, Jacky Liang, Sanjay Krishnan, Fritz Kuttler, Stephen McKinley, Juan Aparicio Ojea, Michael Peinhopf, Peter Puchwein, Alberto Rodriguez, Daniel Seita, Vishal Satish, and Shankar Sastry.'\\n\\n'Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98%, 82%, and 58% respectively, improving to 81% in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http:\\/\\/berkeleyautomation.github.io\\/dex-net.'\",\"570\":null,\"571\":\"'In this paper, we investigate the use of surrogate agents to accelerate test scenario generation for autonomous vehicles. Our goal is to train the surrogate to replicate the true performance modes of the system. We create these surrogates by utilizing imitation learning with deep neural networks. By using imitator surrogates in place of the true agent, we are capable of predicting mission performance more quickly, gaining greater throughput for simulation-based testing. We demonstrate that using on-line imitation learning with Dataset Aggregation (DAgger) can not only correctly encode a policy that executes a complex mission, but can also encode multiple different behavioral modes. To improve performance for the target vehicle and mission, we manipulate the training set during each iteration to remove samples which do not contribute to the final policy. We call this approach Quantile-DAgger (Q-DAgger) and demonstrate its ability to replicate the behaviors of an autonomous vehicle in a collision avoidance scenario.'\\n\\n'One of the primary objectives of this work was determining if our system could encode multiple behaviors in an imitator policy. Three of the primary performance modes resulting from these behaviors are depicted in Figure 6. We are particularly concerned with how well the algorithms learn failure modes such as the collision mode depicted. To determine how well the imitator was learning each of these behaviors, the resulting performance modes for the expert system were compared to the predicted performance mode by the imitator in the form of a confusion matrix (Figure 7). In other words, this matrix shows the success of the imitator system in predicting the same performance mode that the expert system experiences.'\\n\\n'In this paper, we explored the application of imitation learning to the creation of surrogate agents for validation of autonomous vehicles in simulation. We demonstrated that behavioral cloning using DAgger can successfully create surrogate agents which accurately encode both the decision-making of the real agent and its resulting performance landscape. In addition, we demonstrated that a single multilayer perception architecture is capable of not only learning a regression of the control actions, but can also encode multiple mission-level behaviors. Finally, by applying percentile downsampling, we can further reduce the time it takes to train the imitation agent.'\",\"572\":\"\\\"This paper presents a data-efficient approach to learning transferable forward models for robotic push manipulation. Our approach extends our previous work on contact-based predictors by leveraging information on the pushed object's local surface features. We test the hypothesis that, by conditioning predictions on local surface features, we can achieve generalisation across objects of different shapes. In doing so, we do not require a CAD model of the object but rather rely on a point cloud object model (PCOM). Our approach involves learning motion models that are specific to contact models. Contact models encode the contacts seen during training time and allow generating similar contacts at prediction time. Predicting on familiar ground reduces the motion models' sample complexity while using local contact information for prediction increases their transferability. In extensive experiments in simulation, our approach is capable of transfer learning for various test objects, outperforming a baseline predictor. We support those results with a proof of concept on a real robot.\\\"\\n\\n'In particular, we learn two types of experts: 1) static contact models and 2) motion models. The former models encode the contact between the pusher and the object, and between the object and the environment. Meanwhile, the latter models make predictions on how the object will behave under a specific push. In other words, instead of learning motions for a specific object, we break down the problem to learn how few local points of interest (i.e. robot-object and object-environment contacts) will change under a push. We then use those predictions to estimate the global motion of the object.'\\n\\n'A central aspect of our approach is the probabilistic modelling of surface features, extracted from three-dimensional (3D) object point clouds. Features are composed of a 3D position, a 3D orientation, and a 2D local surface descriptor that encodes local curvatures. Let us denote by\\\\nSO(3)\\\\nthe group of rotations in three dimensions. A feature belongs to the space\\\\nSE(3)\\u00d7\\\\nR\\\\n2\\\\n, where\\\\nSE(3)=\\\\nR\\\\n3\\\\n\\u00d7SO(3)\\\\nis the group of 3D poses, and surface descriptors are composed of two real numbers. We thus represent an object as a set\\\\nS\\\\nof\\\\nN\\\\nS\\\\nsurface features\\\\nx\\\\nj'\\n\\n\\\"A contact model encodes the joint probability distribution of surface features and of the 3D pose of the robot's link in contact. At prediction time, we obtain a point cloud\\\\nO\\\\nof the novel object from a single shot taken from a depth camera. Given a set of\\\\nN\\\\nO\\\\nsurface features\\\\n{\\\\nx\\\\nj\\\\n}\\\\nN\\\\nO\\\\nj=1\\\\n, with\\\\nx\\\\nj\\\\n=(\\\\nv\\\\nj\\\\n,\\\\nr\\\\nj\\\\n)\\\\nand\\\\nv\\\\nj\\\\n=(\\\\np\\\\nj\\\\n,\\\\nq\\\\nj\\\\n)\\\\n, a robot-object contact model\\\\nC\\\\nL\\\\nis constructed from features from the object's surface. Surface features close to the link surface are more important than those lying far from the surface. Features are thus weighted, to make their influence on\\\\nC\\\\nL\\\\ndecrease with their squared distance to the link. Let us denote by\\\\nu\\\\nj\\\\n=(\\\\np\\\\nj\\\\n,\\\\nq\\\\nj\\\\n)\\\\nthe pose of\\\\nL\\\\nrelative to the pose\\\\nv\\\\nj\\\\nof the\\\\nj\\\\nth\\\\nsurface feature. In other words,\\\\nu\\\\nj\\\\nis defined as\\\"\\n\\n'We evaluated our approach empirically in experiments on a simulated and a real Pioneer 3DX mobile robot equipped with a bumper for pushing objects. Our experiments focus on object transfer, the main aspiration of this paper. We learned contact and motion models in simulation for one object (i.e. the cube), and then evaluated the predictive performance of the learned models on a set of five test objects in simulation, and two test objects in a real world scenario. We also compared our approach to a baseline predictor which encodes the planar sliding constraints (Sec. V-D).'\",\"573\":\"'We evaluated the proposed approach on two real robot tasks. First, we induced a grammar producing turns of the tic-tac-toe game. Second, we learned a grammar that assists a human with the assembly of a simple toolbox. In both tasks the necessary primitives were encoded as Probabilistic Movement Primitives [1]. For each of the tasks we compare the posterior resulting from our proposed prior, Grammar Poisson, with the one resulting from three common structure prior choices, MDL, Poisson + MDL, Avg. Poisson. The MDL prior is simply defined as an exponential distribution with the MDL as its energy [13]. The Poisson + MDL prior weights the description language for every production with the Poisson probability over the length of the production [20]. Finally, the Avg. Poisson prior discards the MDL completely and is solely represented by a Poisson distribution over the average length of all productions [11]. A major difference of the Grammar Poisson prior to the other discussed priors is that we do not model the distribution over the grammar parameters as a Dirichlet distribution but rather use them as a weighting for the average production length.'\",\"574\":null,\"575\":null,\"576\":\"'A model parameter to be selected by the researcher is the number of components the TP-GMM should use to encode the task trajectory. Based on preliminary tests a mixture of 11 components provided reasonable reproduction behaviour.'\",\"577\":null,\"578\":null,\"579\":null,\"580\":null,\"581\":null,\"582\":null,\"583\":\"\\\"Gait stabilization is very important for legged robots, as they are often used on rough or irregular terrain. Moreover, encoder feedback errors introduce some level of disturbance to the system. For these reasons, the biped robot's controller must guarantee its orbital gait stability.\\\"\",\"584\":\"'Diagram showing the relationship between important ground reaction points. The ICP encodes the com position and velocity, and is controlled by the ecmp. The CMP is lies at the intersection between of the line passing through the com and the ecmp and the ground. For a more detailed definition of the DCM and VRP, see [22]'\",\"585\":null,\"586\":\"'https:\\/\\/youtu.be\\/cpzqu25ila0'\",\"587\":\"'https:\\/\\/goo.gl\\/w9JybR'\\n\\n'The robot is blind, meaning that no on-board vision system is used to close the loop; on-board sensing is provided by the joint modules themselves, each containing an inertial measurement unit (IMU) and encoders [20].'\",\"588\":\"'https:\\/\\/youtu.be\\/ZGhSCILANDw'\\n\\n'The problem at hand is nonconvex and thus hard to solve. The proposed heuristics help us reduce the effort required to find a solution by searching in a convex space an approximate solution (because of the constraint violations); however, the approach has also limitations. For instance, the trust region method could start with a bad initial guess, and the trust region built around it could render the local solution non optimal or even worse, it might overly restrict the problem making it primal infeasible. In the case of soft-constraint approximations, the penalty affects competing objectives, namely the amount of constraint violation and conditioning of the problem, which leads to problems such as having low constraint violation but poor problem conditioning or high constraint violation with faster convergence, therefore a reasonable tradeoff needs to be found. Source code provides more detail on the implementation and heuristics used in our experiments to speed up convergence, to find a solution realiably and robustly, and on how we build and refine trust region constraints and soft-constraint penalties.'\",\"589\":\"'A simulated needle steering teleoperation environment was developed using C++ code and the CHAI3D library. The objective of the teleoperation task was to steer the simulated needle to reach one of four predefined targets (5 mm in diameter and mirrored vertically), as defined by one of the four teleoperation algorithms presented, by moving a haptic stylus (Geomagic Touch, 3D Systems, SC, USA). Figure 3 shows a human subject interacting with the simulated haptic and graphical environment. The necessary input commands to the haptic device for each algorithm are shown in Fig. 4. The target layout is shown in Fig. 5(a).'\\n\\n'Real-time sensor signals (human physiological response, user kinematics, and performance data) were acquired simultaneously, using custom Robotic Operating System (ROS) and C++ codes, at sampling rates defined by the hardware.'\",\"590\":null,\"591\":null,\"592\":null,\"593\":\"'We used a Turtlebot 2 as the mobile robot platform. Turtlebot 2 is a low-cost, personal robot kit with open-source software. We used a Microsoft Kinect as the 3D sensor, and an ASUS Zenbook UX303UB as the onboard computer to process RGB-D data from the Kinect and to control the robot. All the algorithms described in this and the next section were implemented in Ubuntu 16.04 LTS with the Robot Operating System (ROS Kinetic Kame).'\",\"594\":null,\"595\":null,\"596\":\"'We pose the problem of obstacle detection as that of segmenting the road scene into multiple classes. The proposed model consists of three key networks, namely the stripe-net, the context-net and the refiner-net. Stripe-net is a fully convolutional encoder-decoder model which is trained with column-wise strips of RGBD input (each training image is divided into a set of non overlapping vertical strips and fed to the network individually). The key idea behind Stripe-net is twofold: (a) learning discriminative features at a low-level by only attending to the vertical pathway of a road scene and (b) sharing parameters across the stripes to ensure lower model complexity and in turn reducing susceptibility to overfit even on small datasets. Context-net is also a fully convolutional encoder-decoder, but is trained on the full image. The role of this network is to incorporate global features which typically span a width higher than the stripe-width used in the previous network. Global coherence and important contextual cues are more suitably learnt using this network. Finally, the refiner-net is used for aggregating both the low and high level features and making the final prediction. Figure 1 illustrates a motivating example, showing the results at different stages of the proposed architecture.'\\n\\n'The core intuition behind the MergeNet model is that learning the low-level features of a road scene, combined with high-level features learnt from the context of the scene can jointly produce better segmentation maps. The MergeNet consists of three individual CNNs, each implemented as a fully convolutional network. The encoder and decoder blocks used for upsampling and downsampling of the image are similar to Segnet (basic version) [16], differing only in the number of layers and channels. Each encoder downsamples the input resolution through a series of convolution, batch normalization and pooling layers and then upsamples the encoded features though a series of deconvolution, batch normalization and unpooling layers, back to the original resolution. The first two networks learn at different levels of spatial abstraction while the third is a refiner network which exploits complementary information from the first two networks to produce a refined semantic map. These three models are denoted by functions\\\\ng\\\\nstripe\\\\n(\\\\nw\\\\ns\\\\n, \\\\nx\\\\ns\\\\n),\\\\ng\\\\ncontext\\\\n(\\\\nw\\\\nc\\\\n,\\\\nx\\\\nc\\\\n)\\\\nand\\\\ng\\\\nrefine\\\\n(\\\\nw\\\\nr\\\\n,\\\\nx\\\\nr\\\\n)\\\\nin the following sub-sections. Figure 2 shows the detailed structure of the whole pipeline and its various modules.'\\n\\n'The network consists of two parallel encoding and decoding branches, one for the RGB channel and another for the depth channel. Each branch contains four layers of convolutional downsampling in the encoder followed by 4 layers of upsampling in the decoder. These branches are subsequently fused across the channel dimension to obtain a combined map which is used for pixel-wise predictions of the three classes using a softmax layer.'\",\"597\":\"'Deep Encoder-Decoder Networks for Mapping Raw Images to Dynamic Movement Primitives'\\n\\n'DEEP Encoder-Decoder Networks'\\n\\n'In this paper we propose a new approach for learning perception-action couplings. We show that by collecting a suitable set of raw images and the associated movement trajectories, a deep encoder-decoder network can be trained that takes raw images as input and outputs the corresponding dynamic movement primitives. We propose suitable cost functions for training the network and describe how to calculate their gradients to enable effective training by back-propagation. We tested the proposed approach both on a synthetic dataset and on a widely used MNIST database to generate handwriting movements from raw images of digits. The calculated movements were also applied for digit writing with a real robot.'\\n\\n'An example encoder-decoder network architecture with five hidden layers'\\n\\n'We are interested to apply deep neural networks in the context of imitation learning. Previous works have shown that autoencoders [6] or variational autoencoders [7] can be used to reduce the dimensionality of the movements obtained by human demonstration and effectively train dynamic movement primitives in a low-dimensional latent space. In these works, the training data were acquired either synthetically or by motion capture. Deep autoencoders have also been shown to significantly outperform other technologies, e. g. principal component analysis, in computing low dimensional latent spaces from raw character images [8]. A shift invariant autoencoder was developed [9] to extract a typical spatial subpattern independent of its relative position in an image.'\\n\\n'Our focus is on learning direct mappings between images and actions. So instead of autoencoder networks, we are interested in encoder-decoder networks, where transformation and generalization of the incoming image data into the output robot actions occurs through a low-dimensional latent space. Encoder-decoder networks in combination with convolutional layers have proven to be useful in computer vision. One of the best known examples is SegNet deep convolutional encoder-decoder architecture for semantic pixel-wise segmentation [10]. A similar architecture have proven to be successful at object contour detection [11]. Convolutional neural networks have also been employed to generate task parametrized dynamic movement primitives [12].'\\n\\n'Unlike the above-mentioned encoder-decoder networks, which convert raw images into another pixel-wise image representation, in this work we investigate the conversion of raw images into movements associated with the images. As a practical example we consider the problem of reproducing handwritten digits, i. e. we propose to train an encoder-decoder network which calculates a movement that generated a handwritten digit directly from its image. Other possible applications of our work are discussed in Section VI, Conclusions and future directions.'\\n\\n'As autoencoders have shown good performance for the calculation of low-dimensional latent space representations of human movements [6], [7] as well as for the reproduction of images of handwritten characters [8], and encoder-decoder networks have shown to be useful for converting raw images into different image representations, we designed our architecture for decoding movements from images as a fully-connected encoder-decoder architecture. A simple example encoder-decoder network is shown in Fig. 1.'\\n\\n'A simple cost function for evaluating the output of the encoder-decoder neural network would be to convert the example movements\\\\nM\\\\nj\\\\ninto DMPs and define the cost function for the\\\\nj\\\\n-th DMP as follows'\\n\\n'We were also able to use the DMPs computed by the proposed encoder-decoder network for writing all 10 digits with a Kuka LightWeight Robot arm (LWR). For writing with a pen, we used admittance control with a specified contact force in direction of the pen tip while following a trajectory specified by the output DMP. A few screenshots from this experiments are shown in Fig. 2.'\\n\\n'The results of reconstruction for digits from MNIST dataset. The manually generated and transformed trajectories are shown in blue, while the DMP trajectories calculated by a neural network are shown in red. These data were used only for testing, not for training. Hence these results show the generalization performance of the proposed encoder-decoder network.'\\n\\n'We presented a new approach to convert greyscale images into the associated dynamic movement primitives using deep encoder-decoder neural networks. Our experimental results show that with the proposed approach, handwriting movements for digits can effectively be reconstructed from raw images of digits.'\\n\\n'There are quite a few possibilities to extend the proposed approach. Our plan for the immediate future work is to explore the application of convolutional neural networks for the encoder part of the proposed architecture. This will enable more effective processing of real images, which should be beneficial especially for images with cluttered background. In connection to this we will investigate how pretrained convolutional neural networks can be exploited for faster training of the proposed encoder-decoder networks. Another possible extension is movement reproduction from image sequences, which requires an effective dimensionality reduction to be feasible.'\\n\\n'While in this paper we focused on the reproduction of handwriting movements, our work has many potential applications beyond this task. For example, in the future we plan to associate assistive robot actions with pointing gestures observed by a robot. The image or image sequence containing the pointing gesture will in this case be fed as input to the encoder-decoder network and the output will be the robot action associated with the gesture. This way an effective human-robot interaction system could be created.'\",\"598\":null,\"599\":\"'Environment understanding, object detection and recognition are crucial skills for robots operating in the real world. In this paper, we propose a Convolutional Neural Network with multi-task objectives: object detection and scene classification in one unified architecture. The proposed network reasons globally about an image to understand the scene, hypothesize object locations, and encodes global scene features with regional object features to improve object recognition. We evaluate our network on the standard SUN RGBD dataset. Experiments show that our approach outperforms state-of-the-arts. Network predictions are further transformed into continuous robot beliefs to ensure temporal coherence and extended to 3D space for robotics applications. We embed the whole framework in Robot Operating System, and evaluate its performance on a real robot for semantic mapping and grasp detection.'\",\"600\":\"'https:\\/\\/sites.google.com\\/site\\/affordancenetwork\\/'\\n\\n'We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https:\\/\\/github.com\\/nqanh\\/affordance-net.'\",\"601\":null,\"602\":\"'The implementation code of the network and all the datasets used in the experiments can be found in the Github repository of the UPO Service Robotics Lab, in the module upo_fcn_learning 1.'\",\"603\":null,\"604\":\"\\\"Close human-robot cooperation is a key enabler for new developments in advanced manufacturing and assistive applications. Close cooperation require robots that can predict human actions and intent, understanding human non-verbal cues. Recent approaches based on neural networks have led to encouraging results in the human action prediction problem both in continuous and discrete spaces. Our approach extends the research in this direction. Our contributions are three-fold. First, we validate the use of gaze and body pose cues as a means of predicting human action through a feature selection method. Next, we address two shortcomings of existing literature: predicting multiple and variable-length action sequences. This is achieved by applying an encoder-decoder recurrent neural network topology in the discrete action prediction problem. In addition, we theoretically demonstrate the importance of predicting multiple action sequences as a means of estimating the stochastic reward in a human robot cooperation scenario. Finally, we show the ability to effectively train the prediction model on an action prediction dataset, involving human motion data, and explore the influence of the model's parameters on its performance.\\\"\\n\\n\\\"This section introduces the discrete encoder-decoder recurrent neural network topology which seeks to solve the shortcomings enumerated in section II. The first part of the model is a contextual information encoder. The encoder condenses past information into a fixed length context vector through a Long Short Term Memory (LSTM) cell. The embedding is a fully connected layer (FeatureVectorDim\\\\n\\u00d7 50\\\\n), where FeatureVectorDim is the size of the feature vector. The embedding layer includes dropouts which act as a regularization to the model [21]. The encoder LSTM's hidden state dimension is 20. This context vector, the internal state of the encoding LSTM, is the initial state of the second part of the model, the decoder.\\\"\\n\\n\\\"The decoder is responsible for generating a coherent future sequence of actions. At each step the decoder, an LSTM cell, returns a discrete distribution over possible future actions. This distribution is obtained by projecting the decoder's internal state and normalizing it using a softmax layer. The decoding process samples an action from the distribution and feeds it back as an input to the next decoding iteration. The projection is a fully connected layer (HiddenStateDim\\\\nx\\\\nVocabDim), where HiddenStateDim is the size of the hidden state, 20, and VocabDim the dimension of the action discrete possible actions vocabulary, 11. The decoder LSTM's hidden state dimension is 20.\\\"\\n\\n'Encoder-decoder model'\\n\\n'The previous section hints at the complexity underlying the decoding process. At every decoding step, the decoder samples one or more actions from the output distribution as possible actions at a given time step; it then expands these actions by branching and feeding them individually as input to the next decoder iteration. There are two strategies that could be applied to this decoding process.'\",\"605\":\"'In this paper, we propose a generative model which learns the relationship between language and human action in order to generate a human action sequence given a sentence describing human behavior. The proposed generative model is a generative adversarial network (GAN), which is based on the sequence to sequence (SEQ2SEQ) model. Using the proposed generative network, we can synthesize various actions for a robot or a virtual agent using a text encoder recurrent neural network (RNN) and an action decoder RNN. The proposed generative network is trained from 29,770 pairs of actions and sentence annotations extracted from MSR-Video-to-Text (MSR-VTT), a large-scale video dataset. We demonstrate that the network can generate human-like actions which can be transferred to a Baxter robot, such that the robot performs an action based on a provided sentence. Results show that the proposed generative network correctly models the relationship between language and action and can generate a diverse set of actions from the same sentence.'\\n\\n'The text encoder\\\\nE\\\\n, the generator\\\\nG\\\\n, and the discriminator\\\\nD\\\\nconstituting the proposed generative model. The pair of\\\\nE\\\\nand\\\\nG\\\\n, and the pair of\\\\nE\\\\nand\\\\nD\\\\nare seq2seq model composed of the RNN-based encoder and decoder. Each rectangular denotes the LSTM cell of the rnn. (a) The text encoder\\\\nE\\\\nprocesses the set of word embedding vectors\\\\ne={\\\\ne\\\\n1\\\\n,\\u2026\\\\ne\\\\nT\\\\ni\\\\n}\\\\ninto its hidden states\\\\nh\\\\nh={\\\\nh\\\\n1\\\\n,\\u2026\\\\nh\\\\nT\\\\ni\\\\n}\\\\n. The generator\\\\nG\\\\ntakes\\\\nh\\\\nand decodes it into the set of feature vectors\\\\nc\\\\nc\\\\nand samples the set of random noise vectors\\\\nz=\\\\nz={\\\\nz\\\\n1\\\\n,\\u2026\\\\nz\\\\nT\\\\no\\\\n}\\\\n. It receives\\\\nc\\\\nc\\\\nand\\\\nz\\\\nz\\\\nas inputs and generates the human action sequence\\\\nx\\\\nx={\\\\nx\\\\n1\\\\n,\\u2026\\\\nx\\\\nT\\\\no\\\\n}\\\\n. (b) After the text encoder\\\\nE\\\\nencodes the input sentence information into its hidden states\\\\nh\\\\n, the discriminator\\\\nD\\\\nalso takes\\\\nh\\\\nand decodes it into the set of feature vectors\\\\nc\\\\n. By taking\\\\nc\\\\nand\\\\nx\\\\nx\\\\nas inputs,\\\\nD\\\\nidentifies whether\\\\nx\\\\nx\\\\nis real or fake.'\\n\\n'A. Rnn-Based Text Encoder'\\n\\n'The RNN-based text encoder\\\\nE\\\\nshown in Figure 2 encodes the input information\\\\ne\\\\ninto its hidden states of the LSTM cell [12]. Let us denote the hidden states of the text encoder\\\\nE\\\\nas\\\\nh\\\\nh={\\\\nh\\\\n1\\\\n, \\u2026, \\\\nh\\\\nT\\\\ni\\\\n}\\\\n, where'\\n\\n'The desired performance was not obtained properly when we tried to train the entire network end-to-end. Therefore, we pretrain the RNN-text encoder\\\\nE\\\\nfirst. Regarding this,\\\\nE\\\\nis trained by training an autoencoder which learns the relationship between the natural language and the human action as shown in Figure 3. This autoencoder consists of the text-to-action encoder that maps the natural language to the human action, and the action-to-text decoder which reconstructs the human action back to the natural language description. Both the text-to-action encoder and the action-to-text decoder are Seq2seq models based on the attention mechanism [13].'\\n\\n'The encoding part of the text-to-action encoder corresponds to\\\\nE\\\\n. Based on\\\\nc\\\\n(see (3)\\u2013(5)), the hidden states of its decoder\\\\ns\\\\ns={\\\\ns\\\\n1\\\\n,\\u2026\\\\ns\\\\nT\\\\no\\\\n}\\\\nis calculated as\\\\ns\\\\nt\\\\n=\\\\n\\u03b3\\\\nt\\\\n[\\\\ns\\\\nt\\u22121\\\\n, \\\\nx\\\\nt\\u22121\\\\n, \\\\nc\\\\nt\\\\n, \\\\nw\\\\nt\\\\n]\\\\n(see (6)\\u2013(11)), and\\\\nx\\\\nis generated (see (12)). Here,\\\\nw\\\\nt\\\\n\\u2208\\\\nR\\\\nn\\\\nz\\\\nis the zero vector instead of random vector. The action-to-text decoder works similar as the text-to-action encoder. After the information of\\\\nx\\\\nx\\\\nis encoded into\\\\ns\\\\ns={\\\\ns\\\\n1\\\\n,\\u2026, \\\\ns\\\\nT\\\\n0\\\\n}\\\\n(see Figure 3), its decoding part decodes\\\\ns\\\\ninto the set of feature vectors\\\\nc\\\\n\\u2032\\\\n(see (3)-(5)). Based on\\\\nc\\\\n\\u2032\\\\n, the hidden states of its decoder\\\\nh\\\\nh\\\\n\\u2032\\\\n={\\\\nh\\\\n\\u2032\\\\n1\\\\n, \\u2026, \\\\nh\\\\n\\u2032\\\\nT\\\\ni\\\\n}\\\\n, is calculated as\\\\nh\\\\n\\u2032\\\\nt\\\\n=\\\\n\\u03b3\\\\nt\\\\n[\\\\nh\\\\n\\u2032\\\\nt\\u22121\\\\n, \\\\ne\\\\n\\u2032\\\\nt\\u22121\\\\n, \\\\nc\\\\n\\u2032\\\\nt\\\\n, \\\\nw\\\\nt\\\\n]\\\\n. (see (6)-(11)). From the set of hidden states\\\\nh\\\\nh\\\\n\\u2032\\\\n, the word embedding representation of the sentence\\\\ne\\\\nis reconstructed as\\\\ne\\\\nprime\\\\n={\\\\ne\\\\nprime\\\\n1\\\\n,\\u2026\\\\ne\\\\nprime\\\\nT\\\\ni\\\\n}\\\\n(see (12)).'\\n\\n'In order to train this autoencoder network, we have used a loss function\\\\nL\\\\na\\\\ndefined as follows:'\\n\\n'After training the autoencoder network, the part of\\\\nE\\\\nis extracted and passed to\\\\nG\\\\nand\\\\nD\\\\n. In addition, in order to make the training of\\\\nG\\\\nmore stable, the weight matrices and bias vectors of\\\\nG\\\\nthat are shared with the autoencoder are initialized to trained values. When training\\\\nG\\\\nand\\\\nD\\\\nwith the GAN value function shown in equation (1), we do not train the text encoder\\\\nE\\\\n. It is to prevent the pretrained encoded language information from being corrupted while training the network with the GAN value function. Regarding training the generator\\\\nG\\\\n, we choose to maximize\\\\nlogD(G(z, c), c)\\\\nrather than minimizing\\\\n1\\u2212logD(G(z, c), c)\\\\nsince it has been shown to be more effective in practice from many cases [8], [9]. For more details, we recommend the readers to refer the longer version [18] and the released code.1'\\n\\n'The overall structure of proposed network. First, we train an autoencoder which maps between the natural language and the human motion. Its encoder maps the natural language to the human action, and decoder maps the human action to the natural language. After training this autoencoder, only the encoder part is extracted to generate the conditional information related to the input sentence so that\\\\nG\\\\nand\\\\nD\\\\ncan use.'\\n\\n\\\"We enable a Baxter robot to execute the give action trajectory defined in a 3D Cartesian coordinate system by referring the code from [19]. Since the maximum speed at which a Baxter robot can move its joint is limited, we slow down the given action trajectory and apply it to the robot. Figure 10 shows how the Baxter robot executes the given 3D action trajectory corresponding to the input sentence \\u2018A man is throwing something out\\u2019. Here, the time difference between frames capturing the Baxter's pose is about two seconds. We can see that the generated 3D action takes the action as throwing something forward.\\\"\\n\\n\\\"Results of applying generated action sequence to the baxter robot based on the baxter-teleoperation code from [19]. The time difference between frames capturing baxter's pose is about two seconds.\\\"\",\"606\":\"\\\"Architecture of the LSTM model. The inputs are the query agent's state (velocity), the occupancy grid and the angular pedestrian grid (apg), all centered at the query agent's position and aligned with its coordinate frame. Each of the three different input channels gets processed separately through embedding, CNN and LSTM layers. The CNN \\/ FC combination which pre-processes the occupancy grid is pre-trained with an auto-encoder. The concatenation of the extracted features from each channel is followed by another LSTM layer, a FC and an output layer. The output of the model is a sequence of velocities predicted for the query agent's future.\\\"\\n\\n'Third, there is the information about the other agents surrounding the query agent, encoded in a special hybrid grid. In order to be able to capture well the dynamics of other pedestrians, a high resolution grid would be needed, increasing the dimensionality a lot. Therefore, we introduce a new way of encoding the information of other pedestrian agents with our angular pedestrian grid (APG), as shown in Figure 3. The output of the APG encoding is a one dimensional vector,\\\\nr\\\\ni\\\\n, where each element\\\\nr\\\\ni,k\\\\nis defined by:'\\n\\n'1) Pre-Training of the CNN\\\\nAs the extraction of relevant features from the occupancy grid (grey box in Figure 2) is mostly independent of the rest of the overall LSTM model, we propose to pre-train the CNN by using a convolutional auto-encoder (AE) inspired by [29] and [30] as shown in Figure 4. By doing so, the overall training time can be significantly reduced since the weights of the CNN and FC encoding are directly affected and not only through backpropagation based on the overall loss.\\\\nIn the encoding part, the 2D occupancy grid gets reduced to the latent space of the AE. This CNN encoding phase is incorporated into the full LSTM neural network structure from Figure 2. The second part of the AE is the decoding phase, where the latent space has to be decoded such that the output represents the 2D occupancy grid. The CNN weight parameters between encoding and decoding are shared and the training error is defined by\\\\nL(\\\\ng\\\\nin\\\\n,\\\\ng\\\\nout\\\\n):=\\\\n\\u2211\\\\ni=1\\\\nD\\\\nx\\\\n\\u2211\\\\nj=1\\\\nD\\\\ny\\\\n(\\\\ng\\\\nin\\\\n(i,j)\\u2212\\\\ng\\\\nout\\\\n(i,j)\\\\n)\\\\n2\\\\n,\\\\n(4)\\\\nView Source namely the squared error between the encoded-decoded output\\\\n(\\\\ng\\\\nout\\\\n)\\\\nand the original input grid\\\\n(\\\\ng\\\\nin\\\\n)\\\\n, where\\\\ni\\\\nand\\\\nj\\\\ndefine the grid indices along the\\\\nx\\\\nand\\\\ny\\\\naxis.\\\\nD\\\\nx\\\\nand\\\\nD\\\\ny\\\\nrepresent the grid dimensions along the axes. By pre-training the feature extraction for the occupancy grid in this way, it can be assured that the latent space consists of meaningful features, since the original grid needs to be reconstructed from the latent space.\\\\nFig. 4:\\\\nArchitecture of the convolutional AE network for grid feature extraction. By applying three convolutional layers and a FC layer, the original occupancy grid\\\\n(\\\\ng\\\\nin\\\\n)\\\\ngets compressed to a latent space size of 64 during the encoding phase. The decoder reconstructs a grid\\\\n(\\\\ng\\\\nout\\\\n)\\\\nwhich ideally perfectly matches the input grid. The encoding part of this AE network is later on used in the full LSTM motion and interaction model.\\\\nShow All'\\n\\n'Supervised training is conducted for the motion and interaction model using pedestrian trajectory demonstrations as ground truth. Using the training data, the model is trained end-to-end, while keeping the CNN weights of the encoder (obtained from pre-training) fixed. The FC weights of the encoder are initialized with the values obtained during pre-training but then optimized with the rest of the network.'\\n\\n'Since the inputs to the model are encoded in grids, the evaluation complexity per agent is constant. This also results in an almost constant (depending on the other processes running on the computer) and predictable query time of the model. For this example, where 20 pedestrians need to be predicted all the time, the average evaluation time for all agents was 51 ms on a standard laptop with an Intel\\u00ae Core\\u2122 i7-4810MQ CPU processor with 2.80GHz. This results in an average prediction time of 2.6 ms per pedestrian.'\\n\\n'In this paper, we introduced a new approach to model pedestrian dynamics and interactions among them. The model architecture is based on an LSTM neural network which is trained from demonstration data. To the best of our knowledge, we present the first approach which is able to predict pedestrian-pedestrian interactions and the avoidance of static obstacles at the same time with a neural network model. In addition, we introduce a new way of handling dynamic objects, the angular pedestrian grid (APG), which encodes the information of the surrounding pedestrians. In a multi-pedestrian scenario, we use one LSTM network per pedestrian, and therefore the computational complexity only increases linearly with the number of agents. This is especially important when the crowd density gets higher. The presented model does not require a known destination per pedestrian or a predefined set of destination candidates, which makes it well applicable to many real-world applications.'\",\"607\":null,\"608\":\"'The knowledge represented in a FOON is taken from a collection of video sources found on YouTube. A subgraph will be created for each source video, where functional units are directly constructed through manual annotation. The annotation process simply involves the recording of actions occurring in videos, specifically the time they occur, the objects being manipulated, the changes in their states (if any), and the type of motion occurring. With these individual subgraphs, we can then combine the knowledge into a single, larger FOON through a merging procedure. The merging process is very simple in intuition: we perform a union operation with all functional units while removing any duplicate units. A duplicate in this sense means that two units have the exact same input, object and motion nodes down to the smallest detail. Before merging, we parse these files to ensure that all labels are consistent with the object and motion indices kept as reference. For more details on the algorithm, including pseudocode, we refer readers to [21].'\",\"609\":\"'https:\\/\\/lcas.lincoln.ac.uk\\/wp\\/3dof-pedestrian-trajectory-dataset\\/'\\n\\n'This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.'\\n\\n\\\"Our LSTM model is a triple-layer sequence-to-sequence model with output size of 128. Using a multi-layer LSTM can enlarge the receptive field of the short-memory and thereby preserve the short-memory against vanishing too quickly. In our approach, we used a shared LSTM cell for all three layers in order to achieve a longer-term short memory without increasing the model's parameters. The LSTM is trained as an encoder-decoder from the pose at the current time\\\\nt\\\\nto the pose of the next observation at\\\\nt+\\u25b3t\\\\n. Hence for each observation in the training sequence, the next observation is connected with the loss of LSTM as the prediction ground truth.\\\"\\n\\n'The architecture of our temporal pose-LSTM network. A shared-triple-layer LSTM is trained in a sequence-to-sequence encoder-decoder form.'\",\"610\":\"'https:\\/\\/youtu.be\\/ucZG3rafeUc'\",\"611\":null,\"612\":\"'https:\\/\\/github.com\\/kiki0378\\/Dijkstra-based-Road-Detection.git'\\n\\n'https:\\/\\/github.com\\/kiki0378\\/Dijkstra-based-Road-Detection.git'\",\"613\":null,\"614\":null,\"615\":null,\"616\":\"\\\"In this paper, we built forecasting model based on an LSTM neural network following the encoder-decoder architecture [19]. The model's input is a multivariate time series describing the demand for each origin and destination pair for the last\\\\nT\\\\nback\\\\ntime steps. The model forecasts demand for each origin and destination pair for the following\\\\nT\\\\nforward\\\\ntime steps for each region. That is, for a trained model\\\\nf\\\\nwe forecast demand as follows:\\\"\",\"617\":null,\"618\":\"'Vision systems are widely used in autonomous vehicle systems due to the rich information that camera sensors provide of the surrounding environment. This paper presents an automatic algorithm to obtain the drivable path of a vehicle operating in urban roads with or without clear lane markings. The developed system projects trajectories obtained during human operation of the vehicle and utilizes these to generate automatic labels for training a semantic based path prediction model. The system segments an urban scenario into 13 categories including vehicles, pedestrian, undrivable road, other categories relevant to urban roads, and a new class for a path proposal. The drivable path information is essential particularly in unstructured scenarios, and is critical for an intelligent vehicle system to make sound driving decisions. The path proposal category is a car-width drivable lane estimated to be safe to drive for the vehicle under consideration. The data collection, model training and inference process requires only images from a monocular camera and odometry from a low-cost IMU combined with a wheel encoder. The algorithm has been successfully demonstrated on the Sydney University campus, which is a challenging environment without clear road markings. The algorithm was demonstrated to run in real-time, proving its applicability for intelligent vehicles.'\\n\\n'Camera\\/MU generated safe paths: A single camera, IMU and wheel encoder are the only required sensors in the generation of training data for proposed paths. The existing pixel level semantic scene understanding removes the requirement for additional sensors to detect obstacles in the vehicle path.'\\n\\n'Research into vision based algorithms have already contributed to autonomous vehicles due to the rich information the sensor can provide. In this paper, we proposed a weakly-supervised semantic segmentation process to comprehensively classify objects at a pixel level in an urban environment, and to incorporate an additional class that will propose a safe drivable path. The process uses a prior semantic segmentation model to classify each image at the pixel level to one of twelve categories relevant to our road environment. We extend the existing label set adding an additional class for a \\u2018Path Proposal\\u2019 intended to predict the safe drivable path. The ground truth of this class was automatically generated by projecting the recorded trajectory onto the camera images, where those trajectories are considered to be reliable since they were driven by human drivers in a safe manner. During the process of path projection, the car-width polygon is formed by projecting the four vehicle wheels. This included the appropriate transforms to determine the relative poses and 3D orientations for each wheel as they have different turning rates during driving. This proposed automatic label generation and path projection avoid tedious manual labeling work which allows us collect a large number of training images. The whole process involved only a monocular camera and a low-cost IMU combined with a wheel encoder. This low -cost sensor configuration can be generalized for most robotics platforms.'\",\"619\":\"'In order to confidently travel through its environment, an autonomous vehicle must achieve robust localization and navigation despite changing conditions and moving objects. Currently, most platforms employ lidar, vision, GPS, internal sensors, or a combination of these systems to obtain information about their surroundings and perform motion estimation. While extremely fast and high-resolution, lidar is sensitive to weather conditions, especially rain and fog. Vision systems are versatile and cheap but easily impaired by scene changes, like poor lighting or the sudden presence of snow. Both optical sensors only yield dependable results for short-range measurements. A typical GPS on its own guarantees at best 3-m accuracy, experiences reception difficulties near obstructions, and relies on an external infrastructure. Additionally, proprioceptive sensors, like wheel encoders and IMUs, suffer from significant drift among other detrimental effects.'\",\"620\":\"'We propose a fundamentally different approach that is inspired by how humans perform the task. Instead of discarding valuable semantic information, we use a Convolutional Neural Network (CNN)-based encoder-decoder to extract high-level semantic information. We then collapse all semantic information into 2D in order to reduce the assumptions about the environment. We then use these labels, image geometry and (optionally) depth along with a semantically labelled floorplan to create a state-of-the-art sensing and localisation framework.'\",\"621\":null,\"622\":null,\"623\":null,\"624\":null,\"625\":null,\"626\":null,\"627\":null,\"628\":null,\"629\":null,\"630\":\"'Our method is compared with the phase-shift method and the Gray-coded structured lights method in terms of their ability to obtain 3D measurements of a measurement scene [1]. These two methods are spatial encoding methods, and they are widely used in practice. The number of steps was specified as 4 in the phase-shift method. The 3D points measured from three different perspectives are shown in Fig. 11. As we do not have a reference result (the measurement objects cannot be measured by using traditional methods), we cannot provide a numerical comparison. The results are presented in the form of a comparison between the camera view and the 3D points rendered by the results of the measurement. The planar part of the scene, which consisted of paper, could be measured by all methods. The Gray-code method yielded incorrect 3D points on the plastic bottle surface such as a saw-tooth shape, and it could not measure the shape of the metallic L-shape parts. The 3D points reconstructed by the phase-shift method are smooth for almost all parts, although the surfaces of the plastic bottle and the plastic cap collapsed. Although the proposed method used without the post-filter produces images that include shot noise and missing correspondence it succeeds in measuring the basic shape. The results led us to conclude that satisfactory measurement of the 3D points of these complex light reflection objects is possible provided the post-filter is used.'\",\"631\":null,\"632\":\"'The modeling and planning environment is a self-developed system that we intend to use in larger extent for machine knowledge management and robot autonomy. More detailed publications about this system will follow. For the factor graph representation and optimization we build upon the open source library GTSAM [26].'\",\"633\":null,\"634\":null,\"635\":\"'In this paper, we present the design, modeling, and real-time nonlinear model predictive control (NMPC) of an autonomous robotic boat. The robot is easy to manufacture, highly maneuverable, and capable of accurate trajectory tracking in both indoor and outdoor environments. In particular, a cross type four-thruster configuration is proposed for the robotic boat to produce efficient holonomic motions. The robot prototype is rapidly 3D-printed and then sealed by adhering several layers of fiberglass. To achieve accurate tracking control, we formulate an NMPC strategy for the four-control-input boat with control input constraints, where the nonlinear dynamic model includes a Coriolis and centripetal matrix, the hydrodynamic added mass, and damping. By integrating \\u201cGPS\\u201d modules and an inertial measurement unit (IMU) into the robot, we demonstrate accurate trajectory tracking of the robotic boat along preplanned paths in both a swimming pool and a natural river. Furthermore, the code generation strategy employed in our paper yields a two order of magnitude improvement in the run time of the NMPC algorithm compared to similar systems. The robot is designed to form the basis for surface swarm robotics testbeds, on which collective algorithms for surface transportation and self-assembly of dynamic floating infrastructures can be assessed.'\\n\\n'Finally, we would like to highlight that the implemented NMPC algorithm is extremely efficient because of C\\/C++ code generation based on ACADO toolkit. In particular, the computation time needed to determine the next control action is always below 1 ms for our algorithm, meaning that the control frequency can reach up to 1000 Hz, which will be very beneficial to high-speed and high-accuracy applications. By contrast, the computation time of the NMPC method in [29] is about 100 ms with a similar computer configuration, a difference of almost two orders of magnitude.'\",\"636\":null,\"637\":null,\"638\":\"'3) FSM-Based Gesture to Instruction Decoder'\",\"639\":null,\"640\":\"'Having trained control policies in simulation, we execute them on a real robot. In these experiments, we position obstacle and target with respect to the robot using physical handles marked with 2D barcodes and tracked with an RGB camera. First, we consider a normal instance of\\\\nE\\\\n3D\\\\nwith static target and obstacle. We execute the unconstrained policy learned with UP,\\\\nN\\\\nUP\\\\n, to reach a target behind a balloon (Fig. 1a). While reaching was successful, the balloon was knocked down, illustrating that collision avoidance was not effectively learned. We do not use\\\\nN\\\\nUP\\\\nfurther. In contrast, using the constrained policy learned with CPC,\\\\nN\\\\nCPC\\\\n, results in successfully reaching the target while avoiding the balloon (Fig. 1b). As a qualitative example, we consider the case where the target is moving, carried by a toy train around a balloon (Fig. 8, left). Although\\\\nN\\\\nCPC\\\\nwas not explicitly trained on this task, the target was successfully followed without hitting the balloon. We then consider the case where the obstacle is also moving. Through the collision avoidance constraints of Eq. (19), we verify that bringing the obstacle close to the robot forces the latter to move away from it. These results illustrate that combining control policies learned on specific tasks with OptLayer can help adapt to changing conditions, while always maintaining safety. Finally, although position control policies can be learned in simulation for\\\\nE\\\\n3D\\\\n, real-world training can be necessary, e.g., for tasks involving force control [20] and physical interaction with the environment [13]. We thus learn a simplified version of\\\\nE\\\\n3D\\\\n, in which the real robot has to reach a static target without touching a static obstacle. For ease of visualization, we use a cake for the latter (Fig. 8, right), never hitting it over 6 h of training. Our approach can thus be used to learn a robot control policy from scratch without active human supervision. We present our complete experimental results in the supplementary material2.'\",\"641\":null,\"642\":null,\"643\":\"'While agile mobility is desirable in space exploration tasks, locomotion control for soft robotic systems remains a challenging problem due to highly nonlinear, coupled dynamics caused by frequent interaction between connecting members. We generally have three methods to tackle this problem: hand-engineered\\/hard-coded open-loop controllers such as lookup tables; optimal control methods such as linear quadratic regulator (LQR) or model predictive control (MPC); and model-free or model-based reinforcement learning methods.'\\n\\n'Hard-coded controllers are typically time-consuming and counter-intuitive. These controllers generally achieve tenseg-rity locomotion either through crawling or step-by-step punctuated rolling. These approaches boil down to hard-coded look-up tables and tend to follow a common order of commands: 1) contract one cable on the bottom triangle until the robot tips onto the next face, 2) reset the robot by returning all cables to rest length, 3) repeat. A lookup table can then be created from the set of actuations that resulted in successful rolls. The major problem with this approach is that it can be tediously slow and the control can be suboptimal on terrains without hard-coded control policies.'\\n\\n'Figure 4 shows a speed comparison between the learned neural network policy, a hand-made controller, and an LQR controller using linearized dynamics. To evaluate our learned policies, we calculate the average distance traveled in 60 seconds by running each policy from its respective triangular base. Our MDGPS policy was able to successfully travel over 2.5x faster than the policy determined by the hard-coded lookup table, and over 17x faster than that of LQR.'\\n\\n'Average distance traveled by each policy over 60 seconds. Notice that the MDGPS policy can travel over 2.5x faster then the hard-coded locomotion policy, and over 17x faster than that of lqr.'\\n\\n'Per the qualification in the prior paragraph, it is important to note that while the three-dimensional gravity-vector observation space works well when the system is at rest, it is not sufficient to know solely the direction of gravity when the system is in motion. To account for varying degrees of cable contraction and varying relative rod positioning, it is useful to know the geometry of the systems rods themselves. We hypothesize that observations which define both the position as well as the orientation of the rods contribute to understanding the structural geometry of the system more successfully, and thus are more effective observations for learning dynamic tensegrity locomotion. To validate this hypothesis, we trained policies using observation spaces from which we can deduce structural geometry, but not gravity, for example vectors tracking the rods or motor encoder counts. We further validated the necessity of structural geometry input by training policies using observation spaces that contain no gravitational or structural-geometric information. We can see in Figure 7, that the policy learns effective locomotion only when presented with information from which it can deduce structural geometry. In order to explicitly encode structural geometry information, we define \\u201crod vectors: a directional vector representing the position and orientation of each tensegrity rod. For any given rod with endpoint positions (a, b), this is the three-dimensional difference between them:\\\\n(\\\\nx\\\\nb\\\\n\\u2212\\\\nx\\\\na\\\\n, \\\\ny\\\\nb\\\\n\\u2212\\\\ny\\\\na\\\\n, \\\\nz\\\\nb\\\\n\\u2212\\\\nz\\\\na\\\\n)\\\\n. Not surprisingly, our most successful policy is that which has access to this explicit encoding of structural geometry information: the policy whose observation space is defined to be the rod vectors (see Figure 7).'\\n\\n\\\"For implementation of this algorithm, we used NASA's open source simulator designed for tensegrity robots, NTRT [21], and the guided policy search framework from the Berkeley Robot Learning Lab [19]. Figure 3 shows the training process in our simulator.\\\"\",\"644\":null,\"645\":null,\"646\":\"'The first contribution of this paper is a simulated grasping benchmark for a robotic arm with a two-finger parallel jaw gripper, grasping random objects from a bin. This task is available as an open-source Gym environment2 [3]. Next, we present an empirical evaluation of off-policy deep RL algorithms on vision-based robotic grasping tasks. These methods include the grasp success prediction approach proposed by [24], Q-learning [28], path consistency learning (PCL) [29], deep deterministic policy gradient (DDPG) [25], Monte Carlo policy evaluation [39], and Corrected Monte-Carlo, a novel off-policy algorithm that extends Monte Carlo policy evaluation for unbiased off-policy learning.'\",\"647\":\"'http:\\/\\/ashvin.me\\/demoddpg-website'\",\"648\":\"'3D models of the environment are used in numerous robotic applications and should reflect the current state of the world. In this paper, we address the problem of quickly finding structural changes between the current state of the world and a given 3D model using a small number of images. Our approach finds inconsistencies between pairs of images by re-projecting an image onto another one by passing through the given 3D model. This process leads to ambiguities, which we resolve by combining multiple images such that the 3D location of the change can be estimated. A focus of our approach is that it can be executed fast enough to allow the operation on a mobile system. We implemented our approach in C++ and released it as open source software. We tested it on existing datasets as well as on self-recorded image sequences and 3D models, which we publicly share. Our experiments show that our method quickly finds changes in the geometry of a scene.'\\n\\n'The main contribution of this paper is a new and fast approach for identifying differences between an existing 3D model and a small sequence of images recorded in the environment. Our approach identifies the approximate area of change fast enough to be executed on a navigating robot, which sets it apart from several related other techniques. We identify inconsistencies by comparing the acquired images to re-projected images that would have been obtained assuming the 3D model is correct, in combination with a forward intersection of the potentially inconsistent regions. We implemented our approach in C++ and tested it on both self-recorded and existing datasets. Our experiments show that our method quickly finds the approximate location of the change in the scene and is fast enough to potentially guide an exploring ground robot or UAV seeking to map the changes in the environment. We publicly share both our open source implementation 1and our own dataset including the 3D models2.'\",\"649\":null,\"650\":null,\"651\":\"'Localization and navigation using QR code for mobile robot in indoor environment'\",\"652\":null,\"653\":\"'https:\\/\\/youtu.be\\/iguzjmlf5v0'\\n\\n'Most sensors were mounted externally on the vehicle with the exception of the 3-axis FOG, which was installed inside the vehicle as shown. Magnetic rotary encoders were used to gauge wheel rotation, and were installed inside each wheel. The vehicle was equipped with 18-inch tires. All sensor data was logged using a personal computer (PC) with an i7 processor, a 512GB SSD, and 64GB DDR4 memory. The sensor drivers and logger were developed on the Ubuntu OS. Additional details are listed in Table. I.'\\n\\n'For accurate odometry measurements, odometry calibration was performed using high-precision sensors: VRS GPS and FOG. The calibration was conducted in a wide and flat open space that guaranteed precision of the reference sensors, VRS GPS and FOG. As two-wheel encoders were mounted on the vehicle, the forward kinematics of the platform can be calculated using three parameters\\\\nw=(\\\\nd\\\\nl\\\\n,\\\\nd\\\\nr\\\\n,\\\\nw\\\\nb\\\\n)\\\\n: the left and right wheel diameters, and the wheel base between the two rear wheels. To obtain relative measurements from global motion sensors, a 2D pose graph\\\\nx=(\\\\nx\\\\n1\\\\n,\\u22ef\\\\nx\\\\nn\\\\n)\\\\nwas constructed whenever accurate VRS GPS measurements were received. The coordinates of the VRS GPS and FOG are globally synchronized, and a node is added from hard-coupled measurements\\\\nx\\\\ni\\\\n=(\\\\nx\\\\ni\\\\n, \\\\ny\\\\ni\\\\n, \\\\n\\u03b8\\\\ni\\\\n)\\\\nwritten in vehicle center coordinate. Least square optimization was used to obtain optimized kinematic parameters w* using relative motion from the graph\\\\nz\\\\ni\\\\n=\\\\nx\\\\ni+1\\\\n\\u2296\\\\nx\\\\ni\\\\nand forward motion from the kinematics\\\\nk\\\\ni\\\\n(w)\\\\n. The mathematical expression of the objective function is'\\n\\n'5) Encoder Data'\\n\\n'The sensor_data\\/encoder. csv file stores the incremental pulse count values of the wheel encoder in the form of (timestamp, left count, right count).'\\n\\n'In this study, baselines were generated via pose-graph SLAM. Our strategy is to incorporate highly accurate sensors (VRS GPS, FOG and wheel encoder) in the initial baseline generation. Further refinement over this initial trajectory is performed using semi-automatic ICP for the revisited places (Fig. 7). Manual selection of the loop-closure proposal is piped into the ICP as the initial guess, and the baseline trajectory is refined using ICP results as the additional loop-closure constraint.'\\n\\n'To support the ROS community, a File player that publishes sensor data as ROS messages is provided. New message types were redefined to convey more information, and were released via the GitHub webpage. In urban environments, there are many stop periods during data logging. As most of the algorithm does not require data in stop periods, the player can skip the stop period for convenience, and control data publishing speeds.'\",\"654\":null,\"655\":null,\"656\":null,\"657\":\"'https:\\/\\/youtu.be\\/EGLprD6MMGE'\\n\\n\\\"In addition to the domain specification above, the allied robots also have access to a set of black-box simulators of the opposition's fundamental tactics upon which more advanced strategies might be built. In our experiments, these are constructed as tuples of individual hand-coded tactics (see Table I below) that include: (a) DL and DR which script the robot to play defensively on left and right flank of its territory using Sentry and Move macro-actions, respectively; (b) DC which scripts the robot to play defensively on the middle-front of the allied territory; (c)\\\"\",\"658\":null,\"659\":null,\"660\":null,\"661\":null,\"662\":null,\"663\":null,\"664\":null,\"665\":\"'In this section, we present the experimental evaluation of the performance of our method in simulation. We performed tests with a robot car driving on race tracks performing realistic full physics simulation using the race engine of an open-source game, called Speed Dreams1, which is based on TORCS [30]. In all the tests, we compared the performance of the algorithm against:'\",\"666\":null,\"667\":\"'For robots to have a wide range of applications, they must be able to execute numerous tasks. However, recent studies into robot manipulation using deep neural networks (DNN) have primarily focused on single tasks. Therefore, we investigate a robot manipulation model that uses DNNs and can execute long sequential dynamic tasks by performing multiple short sequential tasks at appropriate times. To generate compound tasks, we propose a model comprising two DNNs: a convolutional autoencoder that extracts image features and a multiple timescale recurrent neural network (MTRNN) to generate motion. The internal state of the MTRNN is constrained to have similar values at the initial and final motion steps; thus, motions can be differentiated based on the initial image input. As an example compound task, we demonstrate that the robot can generate a \\u201cPut-In-Box\\u201d task that is divided into three subtasks: open the box, grasp the object and put it into the box, and close the box. The subtasks were trained as discrete tasks, and the connections between each subtask were not trained. With the proposed model, the robot could perform the Put-In-Box task by switching among subtasks and could skip or repeat subtasks depending on the situation.'\\n\\n'Robots that can combine appropriate actions in a dynamic environment to complete a complex task that comprises multiple subtasks are more flexible compared with those that cannot combine actions. Nowadays, most factories are not fully automated. Typically, in a changing work environment, e.g., small quantity production lines for customizable products, people perform various complicated tasks because robot manipulation using a modeling approach often requires experts to manually extract environmental features for various conditions and plan corresponding motion trajectories. As the number of situations and task types increase, it becomes extremely difficult to define the features and design appropriate motions for each condition. Furthermore, hard-coded robot manipulations are often unstable in uncontrolled environments. To address these problems, robot manipulation methods that can extract features easily, function in unknown situations, and generate and combine various tasks to suit changing work environments are desirable.'\\n\\n\\\"In this section, we describe the proposed model for generating and switching among short tasks based on image data. The proposed model uses two types of DNNs, i.e., a convolutional autoencoder (CAE) and a modified multiple timescale RNN (MTRNN), to extract image features and generate the robot's next motion based on previous images and motions (Fig. 1). The core of the proposed model is the modified MTRNN which ensures its internal state and the robot return to the similar state at the beginning and end of each subtask. This makes the transitions between multiple subtasks easier to execute.\\\"\\n\\n'A. Convolutional Autoencoder'\",\"668\":null,\"669\":null,\"670\":null,\"671\":\"'The top level algorithm is summarized in pseudo code in Algorithm 2.'\",\"672\":\"'A Fast Pose Estimation Method Based on New QR Code for Location of Indoor Mobile Robot'\",\"673\":null,\"674\":null,\"675\":null,\"676\":null,\"677\":null,\"678\":null,\"679\":null,\"680\":\"'https:\\/\\/youtu.be\\/80VsJNgNfa0'\\n\\n'https:\\/\\/youtu.be\\/80VsJNgNfa0'\",\"681\":\"'The absence of outliers and noisy signals, leads to large control gains in the traditional simulation. This would be very problematic and likely cause crashes if an attempt was made to transition directly to flight testing. As well, tracing and resolving these issues during flight testing would be a difficult process. Using the HIL simulation as an intermediate step allows the resolution of these issues in the lab, avoiding many crashes. Ultimately, we achieved stable flight in experiment without any modifications to the code used in the HIL simulation. To date, the aircraft has not crashed during a flight test, largely due to the use of the HIL simulation in pre-flight planning.'\\n\\n'The artificial sensor measurements and actuations are sent as UDP messages to and from Qgrouncontrol (QGC) in binary format. The structure of these messages can be deduced from the Qgroundcontrol source code in the file \\/src\\/comm\\/QGCXPlaneLink.cc.'\\n\\n\\\"Our test platform consists of an off-the-shelf RC aircraft, modified with hardware to render it autonomous. The WM Parkflyers McFoamy aircraft is equipped with a Rimfire 400 brushless DC motor and an Electrify PowerFlow 10\\u00d74.5 propeller. It is retrofitted with a Pixhawk flight controller running the open-source PX4 flight stack that allows data logging, sensor integration, and state estimation. The default PX4 state estimator (EKF2), fuses readings from the Pixhawk's embedded IMU, barometer, and magnetometer, as well as a 3DR uBlox GPS with compass, to provide an estimate of the 12 state variables (position, attitude, velocity, and body rates). The flight controller uses a 168 MHz Cortex M4F CPU which executes the control loop at 200 Hz. Due to the extra loading caused by additional hardware, additional carbon fiber reinforcements are used for extra rigidity. The McFoamy test platform is shown in Fig. 6 and its physical properties are given in Appendix II.\\\"\\n\\n'Now, using the index one can see the data being sent from the Pixhawk. With the default QGC, when the index is 8, the first single is the aileron, the second single is the negative elevator, and the third single is the rudder. When the index is 25, all the singles are the throttle. It should be noted that this can be easily changed considering QGC is open source. It should also be noted that different indices would be used for a quadrotor or VTOL aircraft.'\",\"682\":\"'https:\\/\\/www.youtube.com\\/watch?v=gHTqOqz_Zyg'\",\"683\":\"'github.com\\/Georacer\\/last_letter'\",\"684\":null,\"685\":\"'http:\\/\\/www.orocos.org\\/'\\n\\n'http:\\/\\/www.ros.org\\/'\",\"686\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34075324'\\n\\n'The proposed method was validated on a da Vinci \\u00ae IS-1200 Surgical Robotic System, upgraded with the open-source\\/open-hardware da Vinci Research Kit (dVRK) [29], which allows direct computer-based control of the system.'\",\"687\":null,\"688\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/31475074'\",\"689\":null,\"690\":\"'http:\\/\\/robotics.citris-uc.org\\/'\\n\\n'Automating precision subtasks such as debridement (removing dead or diseased tissue fragments) with Robotic Surgical Assistants (RSAs) such as the da Vinci Research Kit (dVRK) is challenging due to inherent nOnlinearities in cable-driven systems. We propose and evaluate a novel two-phase coarse-to-fine calibration method. In Phase I (coarse), we place a red calibration marker on the end effector and let it randomly move through a set of open-loop trajectories to obtain a large sample set of camera pixels and internal robot end-effector configurations. This coarse data is then used to train a Deep Neural Network (DNN) to learn the coarse transformation bias. In Phase II (fine), the bias from Phase I is applied to move the end -effector toward a small set of specific target points on a printed sheet. For each target, a human operator manually adjusts the end -effector position by direct contact (not through teleoperation) and the residual compensation bias is recorded. This fine data is then used to train a Random Forest (RF) to learn the fine transformation bias. Subsequent experiments suggest that without calibration, position errors average 4.55mm. Phase I can reduce average error to 2.14mm and the combination of Phase I and Phase II can reduces average error to 1.08mm. We apply these results to debridement of raisins and pumpkin seeds as fragment phantoms. Using an endoscopic stereo camera with standard edge detection, experiments with 120 trials achieved average success rates of 94.5 %, exceeding prior results with much larger fragments (89.4%) and achieving a speedup of 2.1x, decreasing time per fragment from 15.8 seconds to 7.3 seconds. Source code, data, and videos are available at https:\\/\\/sites.google.com\\/view\\/calib-icra\\/.'\",\"691\":\"'https:\\/\\/youtu.be\\/z7IfTLO_Fyg'\\n\\n'https:\\/\\/github.com\\/mzahana\\/dlp'\\n\\n'Also, an open-source code that is used in the simulation and hardware testings is available at https:\\/\\/github.com\\/mzahana\\/dlp'\\n\\n'We use quadrotors as our testing platforms as they provide high maneuverability even in constrained test spaces. Each quadrotor is controlled by two levels of controllers. A low-level controller of attitude, velocity, and position stabilization is handled by the open-source PX4 autopilot firmware1. A high-level controller executes the proposed algorithm and provides position setpoints to the low-level controller.'\\n\\n\\\"Distributed linear program algorithm: An implementation of the proposed algorithm as a C++ class. The algorithm's class provides convenience functions to easily setup problem's parameters and inputs, executes the proposed LP algorithm. The algorithm uses the open-source linear program C++ library, GLPK 2. Also, a ROS C++ node is available to interface between the algorithm class and ROS environment.\\\"\\n\\n'https:\\/\\/github.com\\/mzahana\\/dlp'\\n\\n\\\"PX4: Open-source autopilot software, which provides low-level control to the quadrotor's states including orientation, velocity, and position. The same software is used in both simulation and hardware testings. The PX4 firmware used in the simulations is called software-in-the-loop, SITL. The reader is referred to the PX4 developer guide3. on how to setup the SITL simulation for multiple UAVs.\\\"\\n\\n'Pixhawk: Open-source autopilot'\",\"692\":null,\"693\":null,\"694\":null,\"695\":\"'When presented with motions that are not part of the training set, directly estimating joint angles with the neural network is less accurate. However, this is not the case with the approach of modeling sensor displacement. In fact, in this case the performance is slightly increased; error reduces from 16.42\\u00b0 to 14.54\\u00b0. We hypothesize that this performance difference is due to the nature and amount of training data that we use. Given the finite set of motion and subject samples, our training set is only able to encode a subset of variance in the large space of possible Euler angles. Thus, a completely data-driven approach, such as the direct joint-angle estimator, suffers from low accuracy on unseen test data. This exposes the limited generalizability of such a method. In contrast, utilizing machine learning to model sensor displacement within a kinematics framework is a hybrid approach that relies on patterns that have lower variance; the space of sensor displacements is smaller than that of Euler angles. Thus, our training dataset potentially has more coverage over this domain. Therefore, unless there is access to sufficient labeled training data, the approach of directly estimating joint-angles may be more suitable for structured tasks such as recognizing specific activities rather than tracking pose. On the other hand, the hybrid approach of modeling sensor displacement may be a good candidate for motion capture in general.'\",\"696\":\"'https:\\/\\/youtu.be\\/vm9iL-SyoS8'\\n\\n'Each actuator substructure was built using a linear actuator (FA-PO-35, Firgelli, WA). The top two actuators have a stroke length of 15.2 cm and the bottom actuator has a stroke of 10.2 cm, Each actuator was stripped of its stock motor and gearbox and modified to include a drill motor (393111\\u201301, DeWalt, WI), custom gearbox, motion coupling mechanism, and feedback sensors. The coupled pitch angle and linear stroke length of each actuator are measured using an encoder (E6C2, Karlsson Robotics, FL) with a resolution of 1024 pulses\\/rotation. The roll is measured using a 10K ohm potentiometer (3590S, Bourns, CA). For testing purposes, each actuator substructure is mounted to a stationary frame. For the configuration and global frame orientation shown in Fig. 1, the Cartesian location of the top, middle and bottom mounting joint with respect to C is [-33, \\u221210, 19]T cm, [-28, \\u221217, \\u221220] cm and [-10, \\u221212, \\u221224]T cm, respectively.'\\n\\n'Shown on left is one of the three actuated substructures with its active (green) and passive (red) dof depicted. Key components are as follows: a) motor, b) pitch and stroke feedback encoder, c) roll feedback potentiometer, d) motion coupling mechanism with actuated slider and pitch control arm, e) 3-dof platform mounting joint. Shown on right is the actuated substructure equivalent nodal diagram.'\",\"697\":null,\"698\":null,\"699\":\"\\\"Powered lower-limb orthoses and prostheses are attracting an increasing amount of attention in assisting daily living activities. To safely and naturally collaborate with human users, the key technology relies on an intelligent controller to accurately decode users' movement intention. In this work, we proposed an innovative locomotion recognition system based on depth images. Composed of a feature extraction subsystem and a finite-state-machine based recognition subsystem, the proposed approach is capable of capturing both the limb movements and the terrains right in front of the user. This makes it possible to anticipate the detection of locomotion modes, especially at transition states, thus enabling the associated wearable robot to deliver a smooth and seamless assistance. Validation experiments were implemented with nine subjects to trace a track that comprised of standing, walking, stair ascending, and stair descending, for three rounds each. The results showed that in steady state, the proposed system could recognize all four locomotion tasks with approximate 100% of accuracy. Out of 216 mode transitions, 82.4% of the intended locomotion tasks can be detected before the transition happened. Thanks to its high accuracy and promising prediction performance, the proposed locomotion recognition system is expected to significantly improve the safety as well as the effectiveness of a lower-limb assistive device.\\\"\",\"700\":null,\"701\":null,\"702\":\"'http:\\/\\/slets.org\\/tracker\\/'\",\"703\":\"'http:\\/\\/www.vicon.com\\/'\\n\\n'Magnetometer calibration for motor and magnetic encoder introduced offset compensation.'\",\"704\":null,\"705\":\"'https:\\/\\/youtu.be\\/oo4CWKWyfvQ'\",\"706\":null,\"707\":\"'The DCNN used for this experiment has a six layer architecture, with two Convolutional Layers and three Fully Connected Layers. The input layers size fits the image size of 200 by 200 pixels and accepts floating point values, which encode the depth at this position. Both Convolutional Layers have a kernel size of 5\\u00d75 with a stride of 1\\u00d71 and each include 64 independent convolutions. Pooling and Normalisation is used after each Convolutional Layer. The Pooling Layers use Max - Pooling with a kernel size of 3\\u00d73 and a stride of 2\\u00d72. The Normalisation parameters are\\\\n\\u03b1=0.001\\/9.0,\\u03b2=0.75,bias=1.0\\\\nwith a depth radius of 4. See [13] for additional information about Normalisation. The last Convolutional Layer is followed by a Fully Connected Layer of size 384, which subsequently connects to another Fully Connected Layer of size 192. The final output layer connects the last Fully Connected Layer as a linear combination\\\\n(Wx+b)\\\\nwith 6 output neurons. The 6 output neurons encode the grasp in camera coordinates, using a Roll-Pitch-Yaw representation.'\",\"708\":null,\"709\":null,\"710\":null,\"711\":null,\"712\":null,\"713\":null,\"714\":null,\"715\":\"'http:\\/\\/deeploc.cs.uni-freiburg.de\\/'\\n\\n'http:\\/\\/deeploc.cs.uni-freiburg.de\\/'\\n\\n'Approaches learn a set of feature descriptors from training images and build a codebook of 3D descriptors against which a query image is matched [22], [8]. To efficiently find feature correspondences within the codebook, Shotton et al. [23] and Valentin et al. [25] train regression forests on 3D scene data and use RANSAC to infer the final location of the query image. Donoser et al. propose a discriminative classification approach using random ferns and demonstrate improved pose accuracy with faster run times [7]. Despite the accurate pose estimates provided by these methods, the overall run-time depends on the size of the 3D model and the number of feature correspondences found. The approach presented in this paper does not suffer from these scalability issues as the learned model is independent of the size of the environment. Moreover, since it does not involve any expensive matching algorithm, it has a time complexity of\\\\nO(1)\\\\n.'\",\"716\":null,\"717\":null,\"718\":null,\"719\":null,\"720\":null,\"721\":null,\"722\":null,\"723\":null,\"724\":null,\"725\":\"'In the proposed modular platform we use the Crazyflie 2.0 as the chosen aerial vehicle due to its size and ease in making adaptations. The quadrotor itself weights 27g and its dimensions are 92\\u00d792\\u00d729mm. Its battery life lasts around seven minutes, though in our case battery life time is reduced due to the extra payload. The platform is open-source and open-hardware. The motor mounting was modified from the standard design, we tilted the rotors\\\\n\\u03b1=\\\\n15\\\\n\\u2218\\\\n. This was necessary as more yaw authority was required to enable grasping as a four-bar. However, adding this tilt reduces the quadrotor lifting thrust by 3% [17].'\",\"726\":null,\"727\":null,\"728\":\"'All algorithms have been written in C++, the nearest neighbor search has been coded by the support of FLANN library [21], while the collision detection procedure was based on the FCL library [22]. All simulations are conducted on a PC with Intel Core i7-4500U 1.8 GHz and 8 GB of RAM. All algorithms have been run 30 times for each of the three MP problems to collect the relevant statistics. The expansion step for all algorithms and problems has been set to\\\\n\\u03f5=0.1\\\\n. To get the set of random configurations\\\\nQ\\\\nt\\\\n(\\\\nq\\\\nnode\\\\n, N, R)\\\\n, we have used a fixed\\\\nR=5\\u03f5(\\u03f5\\\\nis the expansion step), while the number of configurations\\\\nN\\\\nwas different for each scenario, and will be given later.'\\n\\n'The paper explains the RRV algorithm and provides relevant pseudo-codes sufficient to reproduce the results presented in the paper. The algorithm covers all different cases the vine might encounter during the expansion, including vine in free space, vine in front of a convex obstacle, vine in front of a narrow passage, and vine inside narrow passage. We also explain why the RRV deals well in all these cases.'\",\"729\":null,\"730\":\"'The pseudocode of the proposed algorithm called Dancing\\\\nPR\\\\nM\\\\n\\u2217\\\\nis depicted in Alg. 2. The main flow of the algorithm is based on PRM\\\\n\\u2217\\\\n, and we also apply a lazy collision checking [20] to minimize the number of local plannings by skipping unnecessary ones.'\",\"731\":\"'https:\\/\\/youtu.be\\/yV7bDj5zFUs'\\n\\n'https:\\/\\/youtu.be\\/yv7bdj5zfus'\\n\\n'We shall encode the forces and torques of the actuators into an action vector\\\\nu\\\\nu\\\\nof dimension\\\\nn\\\\nu\\\\n. Given a starting state\\\\nx\\\\nx\\\\ns\\\\n\\u03f5X\\\\n, and the vector\\\\nu\\\\nu\\\\nas a function of time,\\\\nu\\\\nu=\\\\nu\\\\nu(t)\\\\n, the time evolution of the constrained system is determined by a differential-algebraic equation of the form'\\n\\n'Algorithm 1: The Top-Level Pseudocode of the Planner'\\n\\n'Algorithm 1 gives the top-level pseudocode of the planner we propose. It can be seen that, at this level, the algorithm is almost identical to the one proposed in [6], the only difference being that we use an atlas\\\\nA\\\\nof\\\\nX\\\\nin our case (initialized in line 4 with one chart centered at\\\\nx\\\\nx\\\\nS\\\\nand another at\\\\nx\\\\nx\\\\ng\\\\n)\\\\nto support the lower-level sampling and simulation tasks. As in [6], the algorithm implements a bidirectional RRT where one tree is extended (line 8) towards a random sample (generated in line 6) and then the other tree is extended (line 10) towards the state just added to the first tree. The process is repeated until the trees become connected with a user-specified accuracy (parameter\\\\n\\u03b2\\\\nin line 12). Otherwise, the trees are swapped (line 11) and the process is repeated. Tree extensions are always initiated at the state in the tree closer to the target state (lines 7 and 9). Different metrics can be used without affecting the overall structure of the planner. As in [6], we shall use the Euclidean distance in state space for simplicity.'\",\"732\":\"'https:\\/\\/github.com\\/StanfordASL\\/LearnedSamplingDistributions'\\n\\n'https:\\/\\/goo.g1\\/E3JPWn'\\n\\n'https:\\/\\/goo.gl\\/E3JPWn'\\n\\n'A defining feature of sampling-based motion planning is the reliance on an implicit representation of the state space, which is enabled by a set of probing samples. Traditionally, these samples are drawn either probabilistically or deterministically to uniformly cover the state space. Yet, the motion of many robotic systems is often restricted to \\u201csmall\\u201d regions of the state space, due to e.g. differential constraints or collision-avoidance constraints. To accelerate the planning process, it is thus desirable to devise non-uniform sampling strategies that favor sampling in those regions where an optimal solution might lie. This paper proposes a methodology for nonuniform sampling, whereby a sampling distribution is learned from demonstrations, and then used to bias sampling. The sampling distribution is computed through a conditional variational autoencoder, allowing sample generation from the latent space conditioned on the specific planning problem. This methodology is general, can be used in combination with any sampling-based planner, and can effectively exploit the underlying structure of a planning problem while maintaining the theoretical guarantees of sampling-based approaches. Specifically, on several planning problems, the proposed methodology is shown to effectively learn representations for the relevant regions of the state space, resulting in an order of magnitude improvement in terms of success rate and convergence to the optimal cost.'\\n\\n'The goal of this work is to develop a methodology capable of identifying subspaces of the state space containing optimal trajectories with high probability, and generating samples from this subspace to improve the performance of sampling-based motion planning (SBMP) algorithms. These regions may be arbitrary and potentially complex, defined by internal or external factors. Specifically, we refer to intrinsic properties of the robotic system, independent of the individual planning problem (e.g., the system dynamics) as internal factors, and we refer to properties specific to the planning problem itself (e.g., the obstacles, the environment, the initial state, and the goal state) as external factors. At the core of our methodology is a Conditional Variational Autoencoder (CVAE), as it is expressive enough to represent very complex, high-dimensional regions and general enough to admit arbitrary problem inputs. The CVAE is an extension of the standard variational autoencoder, which is a class of generative models that has seen widespread application in recent years, particularly in the image and video processing communities [25]. This extension allows conditional data generation by sampling from the latent space of the model [1]; in the motion planning context, the conditioning variables represent external factors. Lastly, these samples are used, along with uniform samples that ensure state space coverage, as the sampling distribution for SBMP algorithms. The method thus leverages previous robotic experience (motion plans and demonstrations) to inform planning algorithms. This combination of learning and SBMP allows both the generality of learning and the exhaustive exploration ability and theoretical guarantees of SBMP.'\\n\\n'We will briefly discuss the theory behind the variational autoencoder (VAE), and point out connections to concrete features of our methodology. We aim to construct a distribution for the set of sampled points lying on a nearly optimal trajectory, conditioned on a given planning problem. We will refer to a sampled point as\\\\nx\\\\n. We will denote a finite dimensional encoding of the planning problem and other external features as\\\\ny\\\\n. For example, a map of obstacles in a workspace can be encoded as an occupancy grid. We will denote the conditional density of sample points, conditioned on\\\\ny\\\\n, as\\\\np(x|y\\\\n. While we will not directly address the degree of \\u201cnear-optimality\\u201d, we note that this is caused by the near-optimality of training data generated by solving motion planning problems. This distribution may be formulated as a latent variable model, where we write the joint density of sampled points and the latent variable as\\\\np(x\\\\n|\\\\n\\u00af\\\\nz, y)p(z|y)\\\\n, where\\\\nz\\\\nis a latent variable. We may then write parameterized forms of these densities as\\\\n0\\\\n\\u03b8\\\\n(x|z, y)\\\\nand\\\\np\\\\n\\u03b8\\\\n(z|y)\\\\nrespectively, where\\\\n\\u03b8\\\\nis a vector of parameters. Given this formulation, the maximum likelihood approach aims to maximize the expectation of'\\n\\n'The standard construction of the conditional VAE (CVAE) consists of neural networks for the encoder\\\\nq\\\\n\\u03d5\\\\n(z|x, y)\\\\nand the decoder\\\\np\\\\n\\u03b8\\\\n(x|z,y)\\\\n. Once trained, the decoder allows us to approximately generate samples from\\\\np(x|y)\\\\nby simply sampling from the normal distribution of the latent variable\\\\n\\u2032o(z1v1=N(0.I\\\\n(see Fig. 2b). While one iteration of this offline phase is often sufficient, with problems that are expensive to solve and thus expensive to acquire data for, the entire methodology may be performed iteratively. Thus, a partially trained CVAE may generate samples that result in better planning performance, allowing more, high-quality data and subsequently allowing the CVAE to be further trained.'\\n\\n'Conditional variational autoencoder (cvae) setup [27]. In the context of this work,\\\\nxj\\\\nrepresents training states,\\\\ny\\\\nthe conditioning variable (possibly initial state, goal state, and workspace information), and\\\\nz\\u03b3\\\\nthe latent state. The decoder (b) is used online to project conditioned latent samples into our distribution.'\\n\\n'Conclusions: In this paper we have presented a methodology to bias samples in the state space for sampling-based motion planning algorithms. In particular, we have used a conditional variational autoencoder to learn subspaces of valid or desirable states, conditioned on problem details such as obstacles. We have compared our methodology to several state of the art methods for sample biasing, and have demonstrated it on multiple systems, showing approximately an order of magnitude improvement in cost and success rate over uniform sampling. This learning-based approach is promising due to its generality and extensibility, as it can be applied to any system and can leverage any problem information available. Its ability to automatically discover useful representations for motion planning (as seen by the near immediate convergence) is similar to recent results from deep learning in the computer vision community, which require less human intuition and handcrafting, and exhibit superior performance.'\\n\\n'https:\\/\\/github.com\\/StanfordASL\\/LearnedSamplingDistributions'\",\"733\":\"'https:\\/\\/github.com\\/cdevin\\/objectattention'\\n\\n'https:\\/\\/sites.google.com\\/berkeley.edu\\/object-representations'\\n\\n'https:\\/\\/sites.google.com\\/berkeley.edu\\/oblect-reoresentations'\\n\\n'The main contribution of our work is a perception framework that facilitates generalization over objects and environments while requiring minimal data or supervision per task. Our method incorporates general-purpose object-centric priors in the form of an object attention trained on a large, generic computer vision dataset, and combines it with an extremely sample efficient task-specific attention mechanism that can either be learned from a very small number of demonstrations, or even specified directly by a human operator. We show that this framework can be combined with reinforcement learning to enable a real-world robotic system to learn vision-based manipulation skills. Our experiments demonstrate that our approach achieves superior generalization to an end-to-end trained approach, through the incorporation of prior visual knowledge via the general-purpose attention. We illustrate how the user can control the degree of generalization by including or excluding other objects in the demonstrations. Our source code is available online in a stand-alone ROS package: https:\\/\\/github.com\\/cdevin\\/objectattention. A video of results is available here: https:\\/\\/sites.google.com\\/berkeley.edu\\/object-representations.'\\n\\n'https:\\/\\/github.com\\/cdevin\\/objectattention'\",\"734\":null,\"735\":null,\"736\":\"'An illustration of our localized adaptive (load) network. The colors are used to code different part of the architecture. With yellow we indicate that the multiplicative and concatenation layers execute operations that do not need parameter learning. The layers that are instead directly involved in the network parameter learning procedure are the green and the blue ones, corresponding to the convolutional and fully connected layers. Note that the final softmax performs object classification on the annotated source data, while the pre-trained domain localization network needs as input both the source and the target data but is unsupervised with respect to the object category labels.'\",\"737\":\"'We use the open-source implementation of the Cycle-GAN algorithm [3] for our experiments. The cycle loss weighting, which determines the magnitude of the cycle consistency loss, was reduced from 10 to 4, so as to increase the relative importance of the discriminator network-this slightly reduced convergence time but made little difference in overall conversion quality. We also used the 6-resent-block configuration for the structure for the generative networks. We refer the reader to the work of Zhu et al [3] for further details on the network architecture and parameters.'\",\"738\":\"'Our generator network is a fully convolutional encoder-decoder, similar to the work of [13], which is designed as a \\u201cU - Net\\u201d [23] due to the structural similarity between input and output. Encoder-decoder networks downsample (encode) the input via convolutions to a lower dimensional embedding, which is then upsampled (decode) via transpose convolutions to reconstruct an image. The advantage of using a \\u201cU-Net\\u201d comes from explicitly preserving spatial dependencies produced by the encoder, as opposed to relying on the embedding to contain all of the information. This is done by the addition of \\u201cskip connections\\u201d, which concatenate the activations produced from a convolution layer\\\\ni\\\\nin the encoder to the input of a transpose convolution layer\\\\nn\\u2212i+1\\\\nin the decoder, where\\\\nn\\\\nis the total number of layers in the network. Each convolutional layer in our generator uses kernel size 4\\u00d74 with stride 2. Convolutions in the encoder portion of the network are followed by batch normalization [24] and a leaky ReLU activation with slope 0.2, while transpose convolutions in the decoder are followed by a ReLU activation [25] (no batch norm in the decoder). Exempt from this is the last layer of the decoder, which uses a TanH nonlinearity to match the input distribution of [-1, 1]. Recent work has proposed Instance Normalization [26] to improve quality in image-to-image translation tasks, however we observed no added benefit.'\",\"739\":null,\"740\":\"'SwitchIt is a spherical robot equipped with an RGB-D camera. Button panels are annotated using a QR code sticker affixed during a manual setup phase. Shown here mounted on a tripod, the robot is preparing to press buttons on an electronic passcode panel.'\\n\\n\\\"With the above data in mind, the SwitchIt robot accessory is designed to operate a large number of switches and to be easily mounted on a mobile robot platform. Setting up the system for use in a new environment requires a human to first perform a calibration procedure, which involves affixing QR codes to button panels and annotating reference models of the panels using a hand-held tablet. Afterward, the system will autonomously recognize any visible panel, suggest a reference position for the robot's base, and once in position, press a requested button or sequence of buttons.\\\"\\n\\n'The user affixes a QR code on or near the button panel.'\\n\\n'The panel identifier, QR code, RGB-D information, button names, type, location, size, and areas of interest are saved to a database.'\\n\\n\\\"One omission of the current procedure is that we do not store a 3D map of button panel locations. As a result, to use our system, a mobile base must be able to first position the camera to observe the panel's QR code. Future iterations of our system might record panel location, and incorporate simultaneous localization and mapping (SLAM) software to guide a mobile base to a desired panel.\\\"\\n\\n'Recognition and localization consists of an imprecise QR code localization followed by a more accurate point cloud registration via Iterative Closest Points (ICP) algorithm [16], [17]. When a QR code is detected, the panel reference RGB-D image and all button annotations become available. A first guess is obtained from the QR code, which gives an estimated affine transformation between the reference image coordinates and the current camera coordinates. Since QR codes are relatively small, this estimate is often inaccurate.'\\n\\n'To improve accuracy, we then apply ICP to match the point cloud corresponding to the reference RGB-D image to the currently observed point cloud. The QR code localization gives a reasonable initial guess for this optimization. It should be noted when depth data is missing or corrupted by dark or highly reflective surfaces, ICP is not as effective, and localization relies more on the QR code and surrounding non-reflective surfaces.'\\n\\n'Push: Approach the center of the marked zone and push down. Pushing stops if one of the following conditions have been met: 1) the linear actuator has fully extended by 5mm, or 2) encoder readings indicate that the linear actuator has been stopped for 0.2s.'\\n\\n'Pull: First, approach the side of the button with the \\u201chook\\u201d pointing inward and then move inward by 5mm. Interpolate toward a point 5mm in front of the pull button surface center, or until the encoder reading indicates that the linear actuator motor has been stopped for 0.2s.'\\n\\n'To isolate performance of our panel localization system we performed 500 localization readings on a different test panel that has several buttons (slider, rocker, push button, and switch) and a color-based fiducial on the push button. We compare the estimated button position from panel localization against the \\u201cground truth\\u201d location from color tracking. Testing repeatability for a static panel (i.e., the effect of camera noise) gives a maximum euclidean distance error of 2.56mm using only QR localization, which is reduced to 0.5mm with ICP. Repeating this experiment while shifting and rotating the panel between each reading, localization with QR code only had a maximum error 2.76mm, reduced to 1.3mm with ICP.'\\n\\n'Initial location calibration of home service robot based on 2-dimensional codes landmarks'\",\"741\":null,\"742\":\"'https:\\/\\/youtu.be\\/apRcOadMAo0'\\n\\n'For the purpose of providing our framework as an open-source 1, we connected all the modules with ROS for integrated control for the robots. To control the following person procedure, we implemented a pipeline with three different control flows. The dynamics control, recursive path planning navigation control and the reflex control.'\",\"743\":null,\"744\":null,\"745\":\"'The learned compensation signal and the tracking error from the previous cycle are encoded as linear combinations of radial basis functions in exactly the same way as the demonstrated forces. This has several benefits: a) more compact representation of trajectories reduces computer memory requirements, b) we can apply velocity scaling of trajectories and control signals provided by speed scaled dynamic movement primitives framework, c) we can omit filtering, i.e. we can set\\\\nQ=1\\\\n, since the appropriate filtering is provided by encoding control signals with DMPs [27].'\\n\\n'The PC computer handles the learning by demonstration process and calculates contact points and relative coordinates from the demonstrated motion. It is also used to encode relative coordinates as DMPs and forces as a linear combination of radial basis functions. DMPs encoding the relative coordinates and the demonstrated forces encoded as a linear combination of radial basis functions are passed to the MOTOMAN FS100 controller. Redundancy resolution (17) is implemented on the robot controller FS100 in Yaskawa MotoPlus programming language (C library for robot control).'\",\"746\":null,\"747\":\"'https:\\/\\/github.com\\/XinkeAE\\/Active-ORB-SLAM2'\\n\\n'The main contributions of this paper are the identification of the particular tracking failure and the proposition of a navigation framework to avoid such failure. We propose to exploit internal data of the SLAM to approximate the number of associated map points and use it to constrain the path in a real-time distance-optimal planner. The implementation of the framework in ROS is open-sourced and available online.'\\n\\n'Significant progress has been made in VSLAM over the last two decades. There are some successful open source VSLAM systems [5], [15], and VSLAM has been practically applied in fields such as autonomous robot navigation and, virtual and augmented reality [15], [16]. One major breakthrough in VSLAM is the PTAM by Klein and Murray [15], which separates the VSLAM problem into two threads: tracking and mapping to ensure accuracy and real-time performance. However, most of the VSLAM systems are performed passively. Active VSLAM deals with the problem of motion planning concurrently with VSLAM.'\\n\\n'https:\\/\\/github.com\\/XinkeAE\\/Active-ORB-SLAM2'\",\"748\":null,\"749\":\"'The proposed LPVO written in unoptimized MATLAB codes is able to run at 13.5 Hz on a desktop computer with an Intel Core i5 (3.20 GHz) and 8 GB memory, suggesting potential when implemented in C\\/C++ in the near future.'\",\"750\":\"'We use the open source VLFeat toolbox [27] to extract the SIFT features, and then the putative correspondences are constructed based on the similarity of feature descriptors with nearest neighbor strategy. As the image resolution in this database is relative low, we modify the default parameter of SIFT to generate more features. Specifically, the number of layers in each octave is increased from default 3 to 6.'\",\"751\":\"'Each SURF feature extracted from an image is used to calculate the resulting image code. Each feature is associated to a particular word in the VLAD model, and potential inliers are defined as follows. If database image\\\\nI\\\\nd\\\\nis the most likely location match for query image\\\\nI\\\\nq\\\\n, and feature\\\\nf\\\\nin\\\\nI\\\\nq\\\\nis assigned to word\\\\nw\\\\n, then for any feature\\\\nf\\\\n\\u2032\\\\nin\\\\nI\\\\nd\\\\nalso assigned to word\\\\nw\\\\nthe pair\\\\n(f, \\\\nf\\\\n\\u2032\\\\n)\\\\nis a potential inlier pair.'\",\"752\":\"'Transmission path from a sensor node to a central processing node. The sensor node is responsible for capturing the image data, performing feature extraction and transmitting the coded features over a channel to the processing node. A visual analysis task such as ORB-slam2 is responsible of interpreting the data and providing feedback to the sensor node.'\\n\\n'We add a feature selection step to the encoder to prioritize useful features by means of a probabilistic model, which includes the current tracking state, coding mode decision and keypoint properties, to meet the real-time constraints and to stay within the available transmission capacity. In order to adapt the feature coding to the tracking state, we add a feedback channel to signal the current tracking state.'\\n\\n'For the CTA approach, usually the whole image or video is coded and transmitted. At the receiver node, the analysis task is carried out on the decoded image. Although there are several choices for feature-preserving image [12] and video coding approaches [13], most parts of the image might be irrelevant for the targeted task and keypoint locations are not well preserved through the compression step [14]. In consequence, we apply an ATC-based scheme optimized for visual SLAM. For visual feature coding, the Compact Descriptors for Visual Search (CDVS) standard has been proposed by MPEG [15], focusing mainly on descriptor compression for computationally demanding SIFT-like [16] features optimized for visual search. A visual SLAM system relies on accurate keypoint locations for triangulation, whereas in visual search the keypoint position is often only used for a subsequent geometric consistency check. The proposed coding method for the keypoint locations within the MPEG-CDVS standard uses a location histogram [17], [18], which includes a quantization step leading to a loss in accuracy. Apart from the MPEG-CDVS approach, there are other approaches for coding both descriptors and keypoints. Starting with real-valued descriptors like SIFT and SURF [19], Baroffio et al. proposed to code visual features extracted from video sequences using a scheme close to hybrid video coding [20]. Their approach supports intra- and inter-frame coding to exploit both redundancies within the descriptor itself and correlation between matching features of successive frames. While this work was based on real-valued descriptors, binary descriptors such as ORB [1] became a faster alternative suitable for most computer vision tasks. Following this development some work has been done to compress binary features [21], [3]. We use the general structure of [3] for coding binary features in our remote ORB-SLAM2 framework.'\",\"753\":\"'The depth estimator is mainly based on an encoder-decoder architecture to generate dense depth maps. Different from other depth estimation methods [17], [18] which produce disparity images (inverse of the depth) from the network, the depth estimator of UnDeepVO is designed to directly predict depth maps. This is because training trails report that the whole system is easier to converge when training in this way.'\",\"754\":\"'The proposed solution is based on mixed-integer linear programming (MILP). The MILP objective is to minimize the number of sentences (i.e., the instantiations of structured language templates) in the explanation. The constraints encode the requirement violation (i.e., the reachability probability exceeds the threshold), the MDP transition relation and nondeterministic choices of actions, and the connection between sentences and MDP states and actions. The MILP results in a minimal set of sentences that should be included in the counterexample explanation, and a set of states to form a critical subsystem. Therefore, the approach seeks to search for a counterexample and its structured language explanation simultaneously. The set of sentences identified in the MILP are unordered. To make it easier to follow the explanation, we propose an algorithm to order these sentences based on the topological sorting of counterexample states. Finally, we demonstrate the usefulness of the approach on a case study of warehouse robots planning.'\",\"755\":\"\\\"In this paper we consider the problem of deploying a fleet of self-driving vehicles providing rides to customers to meet their demands within certain deadlines, and at the same time obeying the rules of the road, such as speed limits, construction areas, and traffic lights. The two goals may not be always compatible, since the rules of the road may slow down the autonomous vehicles and preclude the satisfaction of their respective customer demands by their deadlines. Moreover, by operating in the same environment, the vehicles interact with each other which might induce further delays. Our goal is to compute motion plans for all vehicles that are guaranteed to meet the customers' demands within their deadlines while obeying the rules of the road or, if this is not possible, with optimal social cost that encodes a fair distribution of individual delays in servicing the demands among the vehicles.\\\"\\n\\n\\\"The vehicles operate in a road network which is captured as a hierarchical model describing the vehicles' dynamics, the road segments the vehicles traverse, and a high-level finite abstraction of roads and intersections. For simplicity, the customer demands are given as goal locations that need to be reached by desired deadlines. While servicing the demands, the vehicles must avoid colliding with each other and satisfy the rules of the road which are encoded as syntactically co-safe Linear Temporal Logic (scLTL) formulae. The choice of scLTL is motivated by its resemblance to natural language, rigorousness, and expressiveness allowing to formalize a variety of reachability and sequencing tasks, such as \\u201cPick me up at work, then go to the school to pick up the kids and then bring us home. Somewhere on our way, stop by at a shopping mall or a bakery.\\u201d Here we consider the motion planning in a road segment, in other words a part of the overall plan. The scLTL formalism's expressiveness is necessary in our larger framework which includes routing and motion planning [3].\\\"\",\"756\":\"'https:\\/\\/github.com\\/shromonag\\/adversarial_testing.git'\\n\\n'https:\\/\\/github.com\\/shromonag\\/adversarial_testing.git'\",\"757\":null,\"758\":null,\"759\":\"'The theoretical framework presented in this paper has been implemented as a multi-robot planning system in ROS. We use the open-source components Spot2[8] for LTL translation and\\\\nFlexBE\\\\n3[22] for action implementation and execution, as well as Gazebo4 for simulation. The robots communicate via ROS topics for the auction algorithm and for environment updates, e.g., by using a multi-master setup when executed on the real robots.'\",\"760\":\"'http:\\/\\/csc.kth.se\\/~jfpbdc\\/projects\\/homology_clustering'\",\"761\":null,\"762\":null,\"763\":\"'To search the optimal spring stiffnesses and preload angles, we also formulated a dual-layer framework, where the outer-layer is an exhaustive search over all combinations of spring stiffnesses and preload angles, and the inner-layer is to calculate the stability metric (unbalanced joint torques before contact) for all grasps. Similar to the Force Optimization, the outer-layer is also a non-convex problem. The pseudo code is shown as Algorithm 4.'\",\"764\":null,\"765\":null,\"766\":null,\"767\":\"'We propose a new method of structuring robotic pick-place and regrasping tasks as a deep RL problem, i.e., as a Markov decision process (MDP). Our key idea is to formulate the problem using reach actions where the set of target poses that can be reached using these actions is sampled on each time step. Each reach action is represented by a descriptor that encodes the volumetric appearance of the scene in the local vicinity of the sampled reach target. In order to formulate the MDP, we note our problem is actually a partially observable MDP (POMDP) where object shape and pose are hidden state and the images or point clouds produced by the sensors are the observations. In order to solve this problem as an MDP, we encode belief state as a short history of recently taken reach actions expressed using the volumetric descriptors used to encode the reach action.'\\n\\n'Since we are sampling candidate parameters for REACH-GRASP, we need a way to encode these choices to the action-value function. Normally, in RL, the agent has access to a fixed set of action choices where each choice always results in the same distribution of outcomes. However, since we are now sampling actions, this is no longer the case, and we need to encode actions to the action-value function differently. In this paper, we encode each target pose candidate for REACH-GRASP by the corresponding REACH-GRASP descriptor,\\\\nD\\\\ni\\\\n=D(C,\\\\nT\\\\ni\\\\n),i\\u2208[1,m]\\\\n. Essentially, the descriptor encodes each target pose candidate by the image describing what the point cloud nearby the target pose looks like. The total action set consists of the set of descriptors corresponding to sampled reach-grasps, REACH-GRASP\\\\n(D(C,\\\\nT\\\\ni\\\\n)),i\\u2208[1, m]\\\\n, and the discrete set of reach-places adopted from the underlying POMDP, REACH-PLACE\\\\n(i),i\\u2208PLACE\\\\n:\\\\nA=[1,m]\\u222a\\\\nPLACE.'\\n\\n'We encode state as the history of the\\\\nM\\\\nmost recent reach actions where REACH-GRASP actions are represented using the corresponding descriptors. In all of our experiments,\\\\nM\\u22642\\\\n. Figure 3 illustrates the resulting state-action space. The set of blue circles on the right labeled \\u201cSpace of all grasp descriptors\\u201d denotes the set of states where an object has been grasped. This is a continuous space of states equal to the set of REACH-GRASP descriptors resulting from the most recent REACH-GRASP action,\\\\n{trun\\\\nc\\\\n\\u03b3\\\\n(C)|C\\u2282\\\\n\\u0393\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n}\\\\n. The set of blue circles on the left labeled \\u201cSpace of object placements\\u201d represents the set of states where an object has been placed somewhere in the environment. These states are encoded as the history of the two most recent reach actions: the REACH-PLACE action taken on the last time step and the descriptor that encodes the REACH-GRASP action taken two time steps ago. All together, a state in this new MDP is a point in\\\\nS={trun\\\\nc\\\\n\\u03b3\\\\n(C)|C\\u2282\\\\n\\u0393\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00d7PLACE. The state labeled \\u201cGoal!\\u201d in Figure 3 denotes an absorbing state where the object has been placed correctly, and the state labeled \\u201cFell over!\\u201d denotes an absorbing state where the object has been placed incorrectly. When the agent reaches either of these states, it obtains a final reward and the episode ends.'\\n\\n'The state component of the input is also comprised of a REACH-GRASP descriptor and a place vector. However, here these two parameters encode the recent history of actions taken (Section IV-B). After executing a grasp action, the grasp descriptor component of state is set to a stored version of the descriptor of the selected grasp and the place vector is set to zero. After executing a place action, the grasp descriptor retains the selected grasp and the place component is set to the just-executed place command, thereby implicitly encoding the resulting pose of the object following the place action. Each grasp image (both in the action input and the state input) is processed by a CNN similar to LeNet [24], except the output has 100 hidden nodes instead of 10. These outputs, together with the place information, are then concatenated and passed into two 60-unit fully connected, inner product (IP) layers, each followed by rectifier linear units (ReLU). After this there is one more inner product to produce the scalar output.'\\n\\n'Convolutional neural network architecture used to encode the action-value function, i.e., the Q-function.'\\n\\n'This paper proposes a new way of structuring robotic pick-place and regrasping tasks as a deep RL problem. Importantly, our learned policies can place objects very accurately without using shape primitives or attempting to model object geometry in any way. Our key insight is to encode a sampled set of end-to-end reaching actions using descriptors that encode the geometry of the reach target pose. We encode state as a history of recent actions and observations. The resulting policies, which are learned in simulation, simultaneously perceive relevant features in the environment and plan the appropriate grasp and place actions in order to achieve task goals. Our experiments show that the method consistently outperforms a baseline method based on shape primitives.'\",\"768\":null,\"769\":null,\"770\":null,\"771\":\"'We implemented a baseline simulator in which object models were open-source 3D mesh models. The sensor model consisted of raycasting with the object models, followed by adding Gaussian noise to the nominal range. This baseline is representative of what can be implemented in current general-purpose robotics simulators. We created a primitive set for the baseline simulator, using mesh models from TurboSquid4. We briefly note that the number of models we worked with was limited due to: scarcity of freely available mesh models of natural terrain; use of proprietary file types; large-size (> 10MB) mesh models. The raw mesh models were transformed, scaled and sorted into appropriate classes. Example baseline primitives are shown in Figure 8. Given an annotation, scene objects are constructed from mesh model primitives for the baseline in the same way as for our hybrid geometric scene simulator.'\\n\\n'For the purpose of Lidar simulation, we have shown that data-driven object models derived from our approach generalize from a training to a new test scene better than open-source mesh models. Our approach can also benefit efforts to create point cloud maps of off-road geographical sites. A detailed data log can be gathered in a representative section, and then extrapolated to other sections. To conclude, we mention steps which we believe will further improve simulation. First, we used a fixed clustering density to obtain ellipsoids. A different density for each class might be more appropriate. Second, we are working on automating scene generation, using well-studied tools for semantic segmentation of point cloud data [27], [28]. In this work, segmentation and labeling was manual, which serves as ground truth for future work.'\",\"772\":\"'https:\\/\\/www.operations.amsa.gov.au\\/spatial\\/dataservices\\/mapproduct'\",\"773\":null,\"774\":null,\"775\":null,\"776\":\"'Several tests have been executed with the system in different configurations, accepting the end-effector trajectories by different means, i.e., by local code or joystick, by remote code or joystick and finally by remote exoskeleton. In the remote configuration, the trajectory is generated in Brussels (Belgium) and then transmitted via satellite communications to the vessel in Marseilles (France) and then through the umbilical to the vehicle. Initially, during the early debugging phases, the driving commands were not directly sent to the physical system but the graphical simulator instead. Noticeably, the Brussels operator and the code running on board of the vessel are transparent to this configuration.'\",\"777\":\"'We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work. The source code is available from the project Website a .'\",\"778\":null,\"779\":null,\"780\":null,\"781\":\"'The simulations are conducted with an open-source physics simulation engine PyDART [26]. It handles contacts and collisions by formulating velocity-based linear-complementarity problems to guarantee non-penetration and approximated Coulomb friction cone conditions. The simulation and control time steps are set to 0.002 s (500 Hz), which is enough to be executed on the actual COMAN hardware. The computations are conducted on a single core of 3.40GHz Intel i7 processor.'\",\"782\":null,\"783\":\"'https:\\/\\/github.com\\/resibots\\/pautrat_2018mlei'\\n\\n'https:\\/\\/github.com\\/resibots\\/pautrat_2018mlei'\\n\\n'Code of the experiments: https:\\/\\/github.com\\/resibots\\/pautrat_2018mlei'\",\"784\":null,\"785\":null,\"786\":null,\"787\":null,\"788\":null,\"789\":\"\\\"The experiment was accomplished in the laboratory for a robot to avoid two moving obstacles by using the two-period VO algorithm. The obstacles are dummy toy cars manipulated remotely by lab members. The robot is an omni-directional four-wheeled vehicle with maximum speed 0.7 m\\/s. The robot's velocity is estimated by measurements from the encoders connected to the wheels. A 2-dimentional\\\"\",\"790\":\"'We would like robots to be able to safely navigate at high speed, efficiently use local 3D information, and robustly plan motions that consider pose uncertainty of measurements in a local map structure. This is hard to do with previously existing mapping approaches, like occupancy grids, that are focused on incrementally fusing 3D data into a common world frame. In particular, both their fragile sensitivity to state estimation errors and computational cost can be limiting. We develop an alternative framework, NanoMap, which alleviates the need for global map fusion and enables a motion planner to efficiently query pose-uncertainty-aware local 3D geometric information. The key idea of NanoMap is to store a history of noisy relative pose transforms and search over a corresponding set of depth sensor measurements for the minimum-uncertainty view of a queried point in space. This approach affords a variety of capabilities not offered by traditional mapping techniques: (a) the pose uncertainty associated with 3D data can be incorporated in motion planning, (b) poses can be updated (i.e., from loop closures) with minimal computational effort, and (c) 3D data can be fused lazily for the purpose of planning. We provide an open-source implementation of NanoMap, and analyze its capabilities and computational efficiency in simulation experiments. Finally, we demonstrate in hardware its effectiveness for fast 3D obstacle avoidance onboard a quadrotor flying up to 10 m\\/s.'\\n\\n'atgithub.com\\/peteflorence\\/nanomap_ros'\",\"791\":null,\"792\":null,\"793\":null,\"794\":null,\"795\":null,\"796\":\"'When a robot finally aligns with the prominence, its control is taken over by the Lennard-Jones potentials of its neighbours. The repulsive forces allow the prominence to erupt towards the desired direction, while the attractive ones prevent the break-down of the swarm. If a robot loses its alignment (e.g., because it was pushed by a neighbouring robot), its control goes back into the spiraling step. A Jupyer Notebook version of the Python code is available under the MIT license on GitHub2.'\",\"797\":null,\"798\":\"'https:\\/\\/youtu.be\\/kuu8wsr9dd4'\\n\\n'https:\\/\\/youtu.be\\/kuu8wsr9dd4'\\n\\n'Motivated by this application, we present an approach based on policy learning to find a control strategy for the light source. We split the assembly process in two subtasks: generating a top-level assembly plan using simple planning strategies, and learning a low-level object movement policy. The assembly plan encodes waypoints for each object while the object movement policy controls the trajectory execution by guiding the Kilobots with the light source. In this study we treat the assembly plan as given and only learn object movement policies through policy search.'\",\"799\":null,\"800\":null,\"801\":null,\"802\":null,\"803\":null,\"804\":\"'https:\\/\\/www.dropbox.com\\/sh\\/i39pjxfqiwhbuln\\/AAD5FNjk-Nt28UZ81sVuVd4ja?dl=0'\",\"805\":null,\"806\":null,\"807\":\"\\\"The observation model of the particle filter incorporates a maximum range and field of view limitation to model depth sensors such as the Kinect, but also to reflect the fact that today's target trackers are not generally reliable at large distances. The observation model may also encode particularities of the detector, such as its susceptibility to false negative errors as well as dependence to viewpoint, which is becoming less crucial given recent advances in object detection using supervised deep learning. The observation model we use in the particle filter is the one presented in Eqn. 15.\\\"\",\"808\":\"'https:\\/\\/youtu.be\\/sfyomdgdnao'\",\"809\":null,\"810\":null,\"811\":\"'Unfortunately, only a few of these algorithms have efficient open-source implementations available and most of them only work with monocular cameras. Very few algorithms are designed specifically for stereo or multi-camera setups mainly due to higher computational cost of feature detection and matching across the cameras. Hence we developed our own stereo VIO system based on the MSCKF algorithm [19] while incorporating improvements proposed in [25]. A filter based approach was chosen due to its computational efficiency compared to optimization based methods. This is because we need to run our full navigation stack on the onboard computer of our aerial robot, hence computational efficiency of each of the algorithms running on the robot was an important factor for us.'\\n\\n'A comparison of our algorithm against some open-source tightly-coupled VIO algorithms, namely OKVIS2, ROVIO3 and VINS-Mono4, on the EuRoC dataset is shown in Figure 3. From the figure, we can see that our method provides a good compromise between estimation accuracy and low computational requirement.'\\n\\n'A comparison of the accuracy and computational efficiency our VIO system with various open source packages on the openly available euroc dataset. Our method fails on the v2_03 dataset due to significantly different exposure times on the two cameras in some parts of the dataset.'\",\"812\":null,\"813\":\"'https:\\/\\/youtu.be\\/zvgpn6EK_qi'\"},\"Stars\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":20,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":27,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":15,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":0,\"123\":-1,\"124\":-1,\"125\":3770,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":1,\"211\":-1,\"212\":-1,\"213\":62,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":21,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":253,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":133,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":23,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":12,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":179,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":15,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":4,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":6,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":115,\"483\":13,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":17,\"527\":33,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":117,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":0,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":16,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":13,\"733\":9,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":58,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":4,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":157,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1},\"Forks\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":79,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":28,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":31,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":0,\"123\":-1,\"124\":-1,\"125\":1857,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":7,\"211\":-1,\"212\":-1,\"213\":292,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":3,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":68,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":56,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":64,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":33,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":40,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":40,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":4,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":14,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":344,\"483\":45,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":64,\"527\":83,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":53,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":4,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":31,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":3,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":50,\"733\":17,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":102,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":2,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":53,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1},\"Citations\":{\"0\":11,\"1\":-1,\"2\":14,\"3\":5,\"4\":3,\"5\":11,\"6\":20,\"7\":98,\"8\":4,\"9\":9,\"10\":6,\"11\":6,\"12\":23,\"13\":15,\"14\":5,\"15\":14,\"16\":0,\"17\":0,\"18\":1,\"19\":3,\"20\":7,\"21\":12,\"22\":4,\"23\":20,\"24\":24,\"25\":46,\"26\":22,\"27\":31,\"28\":30,\"29\":6,\"30\":20,\"31\":21,\"32\":3,\"33\":15,\"34\":-1,\"35\":91,\"36\":31,\"37\":7,\"38\":5,\"39\":53,\"40\":5,\"41\":9,\"42\":15,\"43\":2,\"44\":6,\"45\":13,\"46\":17,\"47\":3,\"48\":5,\"49\":5,\"50\":7,\"51\":0,\"52\":88,\"53\":6,\"54\":9,\"55\":0,\"56\":25,\"57\":9,\"58\":18,\"59\":3,\"60\":3,\"61\":18,\"62\":9,\"63\":11,\"64\":14,\"65\":33,\"66\":94,\"67\":4,\"68\":30,\"69\":4,\"70\":15,\"71\":43,\"72\":13,\"73\":6,\"74\":3,\"75\":3,\"76\":4,\"77\":3,\"78\":-1,\"79\":7,\"80\":0,\"81\":13,\"82\":2,\"83\":32,\"84\":10,\"85\":10,\"86\":14,\"87\":1,\"88\":14,\"89\":13,\"90\":4,\"91\":8,\"92\":0,\"93\":8,\"94\":8,\"95\":1,\"96\":5,\"97\":8,\"98\":0,\"99\":3,\"100\":3,\"101\":10,\"102\":4,\"103\":9,\"104\":0,\"105\":17,\"106\":9,\"107\":17,\"108\":70,\"109\":7,\"110\":42,\"111\":0,\"112\":21,\"113\":10,\"114\":13,\"115\":8,\"116\":2,\"117\":20,\"118\":28,\"119\":118,\"120\":229,\"121\":8,\"122\":450,\"123\":17,\"124\":27,\"125\":36,\"126\":58,\"127\":6,\"128\":11,\"129\":17,\"130\":17,\"131\":2,\"132\":12,\"133\":35,\"134\":0,\"135\":3,\"136\":4,\"137\":12,\"138\":41,\"139\":13,\"140\":29,\"141\":3,\"142\":11,\"143\":3,\"144\":1,\"145\":14,\"146\":1,\"147\":3,\"148\":9,\"149\":3,\"150\":5,\"151\":21,\"152\":8,\"153\":0,\"154\":6,\"155\":20,\"156\":2,\"157\":22,\"158\":0,\"159\":8,\"160\":20,\"161\":0,\"162\":11,\"163\":35,\"164\":6,\"165\":8,\"166\":2,\"167\":5,\"168\":3,\"169\":21,\"170\":1,\"171\":17,\"172\":11,\"173\":15,\"174\":28,\"175\":23,\"176\":13,\"177\":4,\"178\":19,\"179\":45,\"180\":13,\"181\":15,\"182\":17,\"183\":1,\"184\":13,\"185\":5,\"186\":8,\"187\":5,\"188\":12,\"189\":22,\"190\":14,\"191\":8,\"192\":447,\"193\":51,\"194\":13,\"195\":42,\"196\":24,\"197\":8,\"198\":6,\"199\":12,\"200\":40,\"201\":3,\"202\":6,\"203\":1,\"204\":2,\"205\":89,\"206\":11,\"207\":2,\"208\":6,\"209\":7,\"210\":21,\"211\":181,\"212\":60,\"213\":29,\"214\":99,\"215\":16,\"216\":100,\"217\":32,\"218\":12,\"219\":3,\"220\":10,\"221\":5,\"222\":14,\"223\":7,\"224\":6,\"225\":39,\"226\":31,\"227\":8,\"228\":20,\"229\":6,\"230\":4,\"231\":7,\"232\":18,\"233\":8,\"234\":17,\"235\":203,\"236\":11,\"237\":7,\"238\":1,\"239\":14,\"240\":6,\"241\":12,\"242\":4,\"243\":31,\"244\":25,\"245\":10,\"246\":12,\"247\":42,\"248\":27,\"249\":2,\"250\":7,\"251\":30,\"252\":8,\"253\":22,\"254\":3,\"255\":20,\"256\":8,\"257\":8,\"258\":16,\"259\":2,\"260\":7,\"261\":36,\"262\":20,\"263\":109,\"264\":107,\"265\":3,\"266\":156,\"267\":11,\"268\":4,\"269\":237,\"270\":109,\"271\":29,\"272\":48,\"273\":29,\"274\":23,\"275\":6,\"276\":27,\"277\":3,\"278\":3,\"279\":2,\"280\":3,\"281\":1,\"282\":1,\"283\":4,\"284\":65,\"285\":8,\"286\":23,\"287\":14,\"288\":2,\"289\":5,\"290\":36,\"291\":2,\"292\":40,\"293\":28,\"294\":59,\"295\":1,\"296\":13,\"297\":10,\"298\":1,\"299\":76,\"300\":6,\"301\":6,\"302\":6,\"303\":20,\"304\":9,\"305\":9,\"306\":42,\"307\":4,\"308\":1,\"309\":5,\"310\":9,\"311\":9,\"312\":4,\"313\":2,\"314\":2,\"315\":6,\"316\":7,\"317\":5,\"318\":16,\"319\":7,\"320\":9,\"321\":5,\"322\":3,\"323\":12,\"324\":15,\"325\":4,\"326\":3,\"327\":1,\"328\":5,\"329\":104,\"330\":6,\"331\":16,\"332\":32,\"333\":8,\"334\":3,\"335\":67,\"336\":109,\"337\":7,\"338\":13,\"339\":23,\"340\":16,\"341\":12,\"342\":62,\"343\":3,\"344\":32,\"345\":26,\"346\":49,\"347\":6,\"348\":5,\"349\":20,\"350\":11,\"351\":113,\"352\":33,\"353\":0,\"354\":15,\"355\":26,\"356\":4,\"357\":9,\"358\":15,\"359\":8,\"360\":0,\"361\":10,\"362\":3,\"363\":9,\"364\":6,\"365\":47,\"366\":4,\"367\":113,\"368\":79,\"369\":93,\"370\":24,\"371\":10,\"372\":2,\"373\":3,\"374\":5,\"375\":60,\"376\":5,\"377\":21,\"378\":19,\"379\":41,\"380\":45,\"381\":39,\"382\":8,\"383\":21,\"384\":10,\"385\":5,\"386\":2,\"387\":8,\"388\":4,\"389\":14,\"390\":3,\"391\":3,\"392\":310,\"393\":146,\"394\":65,\"395\":80,\"396\":30,\"397\":7,\"398\":130,\"399\":727,\"400\":59,\"401\":29,\"402\":33,\"403\":17,\"404\":88,\"405\":5,\"406\":5,\"407\":5,\"408\":1,\"409\":12,\"410\":8,\"411\":0,\"412\":9,\"413\":1,\"414\":6,\"415\":9,\"416\":8,\"417\":7,\"418\":19,\"419\":10,\"420\":6,\"421\":6,\"422\":5,\"423\":5,\"424\":30,\"425\":12,\"426\":20,\"427\":14,\"428\":2,\"429\":23,\"430\":2,\"431\":25,\"432\":5,\"433\":457,\"434\":4,\"435\":2,\"436\":4,\"437\":4,\"438\":3,\"439\":13,\"440\":8,\"441\":10,\"442\":28,\"443\":0,\"444\":12,\"445\":4,\"446\":30,\"447\":2,\"448\":3,\"449\":27,\"450\":6,\"451\":74,\"452\":2,\"453\":22,\"454\":43,\"455\":48,\"456\":12,\"457\":13,\"458\":21,\"459\":3,\"460\":9,\"461\":80,\"462\":21,\"463\":12,\"464\":13,\"465\":36,\"466\":3,\"467\":16,\"468\":2,\"469\":49,\"470\":30,\"471\":7,\"472\":13,\"473\":7,\"474\":360,\"475\":5,\"476\":9,\"477\":25,\"478\":14,\"479\":131,\"480\":18,\"481\":32,\"482\":575,\"483\":46,\"484\":11,\"485\":17,\"486\":32,\"487\":21,\"488\":1,\"489\":3,\"490\":15,\"491\":0,\"492\":340,\"493\":1,\"494\":21,\"495\":8,\"496\":55,\"497\":12,\"498\":12,\"499\":28,\"500\":3,\"501\":14,\"502\":5,\"503\":8,\"504\":3,\"505\":0,\"506\":4,\"507\":9,\"508\":2,\"509\":17,\"510\":22,\"511\":0,\"512\":22,\"513\":100,\"514\":5,\"515\":20,\"516\":7,\"517\":3,\"518\":3,\"519\":3,\"520\":13,\"521\":5,\"522\":5,\"523\":0,\"524\":6,\"525\":179,\"526\":32,\"527\":198,\"528\":15,\"529\":48,\"530\":27,\"531\":4,\"532\":5,\"533\":13,\"534\":28,\"535\":36,\"536\":14,\"537\":10,\"538\":13,\"539\":7,\"540\":4,\"541\":19,\"542\":16,\"543\":12,\"544\":10,\"545\":5,\"546\":24,\"547\":3,\"548\":5,\"549\":32,\"550\":7,\"551\":17,\"552\":7,\"553\":6,\"554\":12,\"555\":25,\"556\":-1,\"557\":16,\"558\":6,\"559\":20,\"560\":3,\"561\":7,\"562\":3,\"563\":0,\"564\":9,\"565\":1,\"566\":4,\"567\":4,\"568\":6,\"569\":162,\"570\":324,\"571\":4,\"572\":17,\"573\":3,\"574\":29,\"575\":16,\"576\":11,\"577\":18,\"578\":5,\"579\":3,\"580\":24,\"581\":49,\"582\":6,\"583\":8,\"584\":27,\"585\":2,\"586\":93,\"587\":27,\"588\":40,\"589\":12,\"590\":7,\"591\":5,\"592\":21,\"593\":5,\"594\":16,\"595\":27,\"596\":15,\"597\":18,\"598\":7,\"599\":6,\"600\":168,\"601\":11,\"602\":29,\"603\":71,\"604\":33,\"605\":36,\"606\":72,\"607\":15,\"608\":36,\"609\":76,\"610\":1,\"611\":28,\"612\":4,\"613\":10,\"614\":2,\"615\":83,\"616\":51,\"617\":1,\"618\":10,\"619\":84,\"620\":21,\"621\":10,\"622\":8,\"623\":5,\"624\":8,\"625\":3,\"626\":17,\"627\":19,\"628\":0,\"629\":20,\"630\":10,\"631\":10,\"632\":4,\"633\":9,\"634\":13,\"635\":36,\"636\":3,\"637\":4,\"638\":21,\"639\":15,\"640\":65,\"641\":155,\"642\":248,\"643\":22,\"644\":2,\"645\":53,\"646\":129,\"647\":422,\"648\":21,\"649\":3,\"650\":10,\"651\":14,\"652\":4,\"653\":22,\"654\":4,\"655\":39,\"656\":10,\"657\":6,\"658\":8,\"659\":8,\"660\":9,\"661\":15,\"662\":4,\"663\":8,\"664\":2,\"665\":8,\"666\":7,\"667\":10,\"668\":30,\"669\":14,\"670\":6,\"671\":71,\"672\":7,\"673\":7,\"674\":7,\"675\":28,\"676\":30,\"677\":18,\"678\":5,\"679\":6,\"680\":29,\"681\":10,\"682\":6,\"683\":3,\"684\":6,\"685\":31,\"686\":8,\"687\":24,\"688\":-1,\"689\":4,\"690\":43,\"691\":6,\"692\":8,\"693\":6,\"694\":9,\"695\":10,\"696\":2,\"697\":15,\"698\":4,\"699\":18,\"700\":3,\"701\":9,\"702\":2,\"703\":7,\"704\":10,\"705\":16,\"706\":0,\"707\":47,\"708\":3,\"709\":1,\"710\":7,\"711\":43,\"712\":60,\"713\":7,\"714\":2,\"715\":152,\"716\":11,\"717\":2,\"718\":1,\"719\":0,\"720\":12,\"721\":18,\"722\":42,\"723\":3,\"724\":21,\"725\":28,\"726\":41,\"727\":3,\"728\":9,\"729\":17,\"730\":11,\"731\":13,\"732\":203,\"733\":62,\"734\":3,\"735\":14,\"736\":19,\"737\":24,\"738\":190,\"739\":10,\"740\":5,\"741\":0,\"742\":16,\"743\":5,\"744\":2,\"745\":9,\"746\":7,\"747\":8,\"748\":5,\"749\":35,\"750\":8,\"751\":5,\"752\":11,\"753\":340,\"754\":6,\"755\":15,\"756\":33,\"757\":9,\"758\":8,\"759\":11,\"760\":2,\"761\":2,\"762\":17,\"763\":11,\"764\":6,\"765\":15,\"766\":8,\"767\":43,\"768\":2,\"769\":13,\"770\":21,\"771\":5,\"772\":7,\"773\":1,\"774\":22,\"775\":9,\"776\":3,\"777\":83,\"778\":12,\"779\":25,\"780\":16,\"781\":5,\"782\":47,\"783\":32,\"784\":643,\"785\":2,\"786\":19,\"787\":3,\"788\":38,\"789\":7,\"790\":32,\"791\":3,\"792\":7,\"793\":1,\"794\":3,\"795\":4,\"796\":20,\"797\":2,\"798\":15,\"799\":3,\"800\":3,\"801\":1,\"802\":8,\"803\":101,\"804\":53,\"805\":25,\"806\":18,\"807\":6,\"808\":2,\"809\":1,\"810\":6,\"811\":31,\"812\":6,\"813\":15}}"