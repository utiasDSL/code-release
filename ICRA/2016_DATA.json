"{\"Conference\":{\"0\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"1\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"2\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"3\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"4\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"5\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"6\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"7\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"8\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"9\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"10\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"11\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"12\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"13\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"14\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"15\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"16\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"17\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"18\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"19\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"20\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"21\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"22\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"23\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"24\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"25\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"26\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"27\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"28\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"29\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"30\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"31\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"32\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"33\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"34\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"35\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"36\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"37\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"38\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"39\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"40\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"41\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"42\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"43\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"44\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"45\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"46\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"47\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"48\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"49\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"50\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"51\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"52\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"53\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"54\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"55\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"56\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"57\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"58\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"59\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"60\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"61\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"62\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"63\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"64\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"65\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"66\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"67\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"68\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"69\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"70\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"71\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"72\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"73\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"74\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"75\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"76\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"77\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"78\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"79\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"80\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"81\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"82\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"83\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"84\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"85\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"86\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"87\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"88\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"89\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"90\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"91\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"92\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"93\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"94\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"95\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"96\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"97\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"98\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"99\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"100\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"101\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"102\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"103\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"104\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"105\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"106\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"107\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"108\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"109\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"110\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"111\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"112\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"113\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"114\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"115\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"116\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"117\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"118\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"119\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"120\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"121\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"122\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"123\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"124\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"125\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"126\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"127\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"128\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"129\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"130\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"131\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"132\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"133\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"134\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"135\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"136\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"137\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"138\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"139\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"140\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"141\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"142\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"143\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"144\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"145\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"146\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"147\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"148\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"149\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"150\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"151\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"152\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"153\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"154\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"155\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"156\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"157\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"158\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"159\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"160\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"161\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"162\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"163\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"164\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"165\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"166\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"167\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"168\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"169\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"170\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"171\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"172\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"173\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"174\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"175\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"176\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"177\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"178\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"179\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"180\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"181\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"182\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"183\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"184\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"185\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"186\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"187\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"188\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"189\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"190\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"191\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"192\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"193\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"194\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"195\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"196\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"197\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"198\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"199\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"200\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"201\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"202\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"203\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"204\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"205\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"206\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"207\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"208\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"209\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"210\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"211\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"212\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"213\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"214\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"215\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"216\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"217\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"218\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"219\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"220\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"221\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"222\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"223\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"224\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"225\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"226\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"227\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"228\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"229\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"230\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"231\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"232\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"233\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"234\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"235\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"236\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"237\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"238\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"239\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"240\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"241\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"242\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"243\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"244\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"245\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"246\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"247\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"248\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"249\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"250\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"251\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"252\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"253\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"254\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"255\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"256\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"257\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"258\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"259\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"260\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"261\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"262\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"263\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"264\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"265\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"266\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"267\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"268\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"269\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"270\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"271\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"272\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"273\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"274\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"275\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"276\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"277\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"278\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"279\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"280\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"281\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"282\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"283\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"284\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"285\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"286\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"287\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"288\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"289\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"290\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"291\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"292\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"293\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"294\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"295\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"296\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"297\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"298\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"299\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"300\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"301\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"302\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"303\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"304\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"305\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"306\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"307\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"308\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"309\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"310\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"311\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"312\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"313\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"314\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"315\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"316\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"317\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"318\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"319\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"320\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"321\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"322\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"323\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"324\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"325\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"326\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"327\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"328\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"329\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"330\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"331\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"332\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"333\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"334\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"335\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"336\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"337\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"338\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"339\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"340\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"341\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"342\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"343\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"344\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"345\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"346\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"347\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"348\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"349\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"350\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"351\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"352\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"353\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"354\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"355\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"356\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"357\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"358\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"359\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"360\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"361\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"362\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"363\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"364\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"365\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"366\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"367\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"368\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"369\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"370\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"371\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"372\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"373\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"374\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"375\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"376\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"377\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"378\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"379\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"380\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"381\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"382\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"383\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"384\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"385\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"386\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"387\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"388\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"389\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"390\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"391\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"392\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"393\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"394\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"395\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"396\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"397\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"398\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"399\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"400\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"401\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"402\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"403\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"404\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"405\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"406\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"407\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"408\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"409\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"410\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"411\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"412\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"413\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"414\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"415\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"416\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"417\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"418\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"419\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"420\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"421\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"422\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"423\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"424\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"425\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"426\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"427\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"428\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"429\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"430\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"431\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"432\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"433\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"434\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"435\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"436\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"437\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"438\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"439\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"440\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"441\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"442\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"443\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"444\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"445\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"446\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"447\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"448\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"449\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"450\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"451\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"452\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"453\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"454\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"455\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"456\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"457\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"458\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"459\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"460\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"461\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"462\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"463\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"464\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"465\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"466\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"467\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"468\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"469\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"470\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"471\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"472\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"473\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"474\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"475\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"476\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"477\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"478\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"479\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"480\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"481\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"482\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"483\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"484\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"485\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"486\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"487\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"488\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"489\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"490\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"491\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"492\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"493\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"494\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"495\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"496\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"497\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"498\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"499\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"500\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"501\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"502\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"503\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"504\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"505\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"506\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"507\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"508\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"509\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"510\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"511\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"512\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"513\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"514\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"515\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"516\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"517\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"518\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"519\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"520\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"521\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"522\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"523\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"524\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"525\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"526\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"527\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"528\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"529\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"530\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"531\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"532\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"533\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"534\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"535\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"536\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"537\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"538\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"539\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"540\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"541\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"542\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"543\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"544\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"545\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"546\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"547\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"548\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"549\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"550\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"551\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"552\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"553\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"554\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"555\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"556\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"557\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"558\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"559\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"560\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"561\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"562\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"563\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"564\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"565\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"566\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"567\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"568\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"569\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"570\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"571\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"572\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"573\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"574\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"575\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"576\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"577\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"578\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"579\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"580\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"581\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"582\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"583\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"584\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"585\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"586\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"587\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"588\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"589\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"590\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"591\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"592\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"593\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"594\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"595\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"596\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"597\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"598\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"599\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"600\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"601\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"602\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"603\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"604\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"605\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"606\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"607\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"608\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"609\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"610\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"611\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"612\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"613\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"614\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"615\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"616\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"617\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"618\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"619\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"620\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"621\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"622\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"623\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"624\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"625\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"626\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"627\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"628\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"629\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"630\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"631\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"632\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"633\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"634\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"635\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"636\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"637\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"638\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"639\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"640\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"641\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"642\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"643\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"644\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"645\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"646\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"647\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"648\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"649\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"650\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"651\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"652\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"653\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"654\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"655\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"656\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"657\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"658\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"659\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"660\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"661\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"662\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"663\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"664\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"665\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"666\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"667\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"668\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"669\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"670\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"671\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"672\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"673\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"674\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"675\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"676\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"677\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"678\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"679\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"680\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"681\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"682\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"683\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"684\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"685\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"686\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"687\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"688\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"689\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"690\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"691\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"692\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\"},\"Year\":{\"0\":\"Date of Conference: 16-21 May 2016\",\"1\":\"Date of Conference: 16-21 May 2016\",\"2\":\"Date of Conference: 16-21 May 2016\",\"3\":\"Date of Conference: 16-21 May 2016\",\"4\":\"Date of Conference: 16-21 May 2016\",\"5\":\"Date of Conference: 16-21 May 2016\",\"6\":\"Date of Conference: 16-21 May 2016\",\"7\":\"Date of Conference: 16-21 May 2016\",\"8\":\"Date of Conference: 16-21 May 2016\",\"9\":\"Date of Conference: 16-21 May 2016\",\"10\":\"Date of Conference: 16-21 May 2016\",\"11\":\"Date of Conference: 16-21 May 2016\",\"12\":\"Date of Conference: 16-21 May 2016\",\"13\":\"Date of Conference: 16-21 May 2016\",\"14\":\"Date of Conference: 16-21 May 2016\",\"15\":\"Date of Conference: 16-21 May 2016\",\"16\":\"Date of Conference: 16-21 May 2016\",\"17\":\"Date of Conference: 16-21 May 2016\",\"18\":\"Date of Conference: 16-21 May 2016\",\"19\":\"Date of Conference: 16-21 May 2016\",\"20\":\"Date of Conference: 16-21 May 2016\",\"21\":\"Date of Conference: 16-21 May 2016\",\"22\":\"Date of Conference: 16-21 May 2016\",\"23\":\"Date of Conference: 16-21 May 2016\",\"24\":\"Date of Conference: 16-21 May 2016\",\"25\":\"Date of Conference: 16-21 May 2016\",\"26\":\"Date of Conference: 16-21 May 2016\",\"27\":\"Date of Conference: 16-21 May 2016\",\"28\":\"Date of Conference: 16-21 May 2016\",\"29\":\"Date of Conference: 16-21 May 2016\",\"30\":\"Date of Conference: 16-21 May 2016\",\"31\":\"Date of Conference: 16-21 May 2016\",\"32\":\"Date of Conference: 16-21 May 2016\",\"33\":\"Date of Conference: 16-21 May 2016\",\"34\":\"Date of Conference: 16-21 May 2016\",\"35\":\"Date of Conference: 16-21 May 2016\",\"36\":\"Date of Conference: 16-21 May 2016\",\"37\":\"Date of Conference: 16-21 May 2016\",\"38\":\"Date of Conference: 16-21 May 2016\",\"39\":\"Date of Conference: 16-21 May 2016\",\"40\":\"Date of Conference: 16-21 May 2016\",\"41\":\"Date of Conference: 16-21 May 2016\",\"42\":\"Date of Conference: 16-21 May 2016\",\"43\":\"Date of Conference: 16-21 May 2016\",\"44\":\"Date of Conference: 16-21 May 2016\",\"45\":\"Date of Conference: 16-21 May 2016\",\"46\":\"Date of Conference: 16-21 May 2016\",\"47\":\"Date of Conference: 16-21 May 2016\",\"48\":\"Date of Conference: 16-21 May 2016\",\"49\":\"Date of Conference: 16-21 May 2016\",\"50\":\"Date of Conference: 16-21 May 2016\",\"51\":\"Date of Conference: 16-21 May 2016\",\"52\":\"Date of Conference: 16-21 May 2016\",\"53\":\"Date of Conference: 16-21 May 2016\",\"54\":\"Date of Conference: 16-21 May 2016\",\"55\":\"Date of Conference: 16-21 May 2016\",\"56\":\"Date of Conference: 16-21 May 2016\",\"57\":\"Date of Conference: 16-21 May 2016\",\"58\":\"Date of Conference: 16-21 May 2016\",\"59\":\"Date of Conference: 16-21 May 2016\",\"60\":\"Date of Conference: 16-21 May 2016\",\"61\":\"Date of Conference: 16-21 May 2016\",\"62\":\"Date of Conference: 16-21 May 2016\",\"63\":\"Date of Conference: 16-21 May 2016\",\"64\":\"Date of Conference: 16-21 May 2016\",\"65\":\"Date of Conference: 16-21 May 2016\",\"66\":\"Date of Conference: 16-21 May 2016\",\"67\":\"Date of Conference: 16-21 May 2016\",\"68\":\"Date of Conference: 16-21 May 2016\",\"69\":\"Date of Conference: 16-21 May 2016\",\"70\":\"Date of Conference: 16-21 May 2016\",\"71\":\"Date of Conference: 16-21 May 2016\",\"72\":\"Date of Conference: 16-21 May 2016\",\"73\":\"Date of Conference: 16-21 May 2016\",\"74\":\"Date of Conference: 16-21 May 2016\",\"75\":\"Date of Conference: 16-21 May 2016\",\"76\":\"Date of Conference: 16-21 May 2016\",\"77\":\"Date of Conference: 16-21 May 2016\",\"78\":\"Date of Conference: 16-21 May 2016\",\"79\":\"Date of Conference: 16-21 May 2016\",\"80\":\"Date of Conference: 16-21 May 2016\",\"81\":\"Date of Conference: 16-21 May 2016\",\"82\":\"Date of Conference: 16-21 May 2016\",\"83\":\"Date of Conference: 16-21 May 2016\",\"84\":\"Date of Conference: 16-21 May 2016\",\"85\":\"Date of Conference: 16-21 May 2016\",\"86\":\"Date of Conference: 16-21 May 2016\",\"87\":\"Date of Conference: 16-21 May 2016\",\"88\":\"Date of Conference: 16-21 May 2016\",\"89\":\"Date of Conference: 16-21 May 2016\",\"90\":\"Date of Conference: 16-21 May 2016\",\"91\":\"Date of Conference: 16-21 May 2016\",\"92\":\"Date of Conference: 16-21 May 2016\",\"93\":\"Date of Conference: 16-21 May 2016\",\"94\":\"Date of Conference: 16-21 May 2016\",\"95\":\"Date of Conference: 16-21 May 2016\",\"96\":\"Date of Conference: 16-21 May 2016\",\"97\":\"Date of Conference: 16-21 May 2016\",\"98\":\"Date of Conference: 16-21 May 2016\",\"99\":\"Date of Conference: 16-21 May 2016\",\"100\":\"Date of Conference: 16-21 May 2016\",\"101\":\"Date of Conference: 16-21 May 2016\",\"102\":\"Date of Conference: 16-21 May 2016\",\"103\":\"Date of Conference: 16-21 May 2016\",\"104\":\"Date of Conference: 16-21 May 2016\",\"105\":\"Date of Conference: 16-21 May 2016\",\"106\":\"Date of Conference: 16-21 May 2016\",\"107\":\"Date of Conference: 16-21 May 2016\",\"108\":\"Date of Conference: 16-21 May 2016\",\"109\":\"Date of Conference: 16-21 May 2016\",\"110\":\"Date of Conference: 16-21 May 2016\",\"111\":\"Date of Conference: 16-21 May 2016\",\"112\":\"Date of Conference: 16-21 May 2016\",\"113\":\"Date of Conference: 16-21 May 2016\",\"114\":\"Date of Conference: 16-21 May 2016\",\"115\":\"Date of Conference: 16-21 May 2016\",\"116\":\"Date of Conference: 16-21 May 2016\",\"117\":\"Date of Conference: 16-21 May 2016\",\"118\":\"Date of Conference: 16-21 May 2016\",\"119\":\"Date of Conference: 16-21 May 2016\",\"120\":\"Date of Conference: 16-21 May 2016\",\"121\":\"Date of Conference: 16-21 May 2016\",\"122\":\"Date of Conference: 16-21 May 2016\",\"123\":\"Date of Conference: 16-21 May 2016\",\"124\":\"Date of Conference: 16-21 May 2016\",\"125\":\"Date of Conference: 16-21 May 2016\",\"126\":\"Date of Conference: 16-21 May 2016\",\"127\":\"Date of Conference: 16-21 May 2016\",\"128\":\"Date of Conference: 16-21 May 2016\",\"129\":\"Date of Conference: 16-21 May 2016\",\"130\":\"Date of Conference: 16-21 May 2016\",\"131\":\"Date of Conference: 16-21 May 2016\",\"132\":\"Date of Conference: 16-21 May 2016\",\"133\":\"Date of Conference: 16-21 May 2016\",\"134\":\"Date of Conference: 16-21 May 2016\",\"135\":\"Date of Conference: 16-21 May 2016\",\"136\":\"Date of Conference: 16-21 May 2016\",\"137\":\"Date of Conference: 16-21 May 2016\",\"138\":\"Date of Conference: 16-21 May 2016\",\"139\":\"Date of Conference: 16-21 May 2016\",\"140\":\"Date of Conference: 16-21 May 2016\",\"141\":\"Date of Conference: 16-21 May 2016\",\"142\":\"Date of Conference: 16-21 May 2016\",\"143\":\"Date of Conference: 16-21 May 2016\",\"144\":\"Date of Conference: 16-21 May 2016\",\"145\":\"Date of Conference: 16-21 May 2016\",\"146\":\"Date of Conference: 16-21 May 2016\",\"147\":\"Date of Conference: 16-21 May 2016\",\"148\":\"Date of Conference: 16-21 May 2016\",\"149\":\"Date of Conference: 16-21 May 2016\",\"150\":\"Date of Conference: 16-21 May 2016\",\"151\":\"Date of Conference: 16-21 May 2016\",\"152\":\"Date of Conference: 16-21 May 2016\",\"153\":\"Date of Conference: 16-21 May 2016\",\"154\":\"Date of Conference: 16-21 May 2016\",\"155\":\"Date of Conference: 16-21 May 2016\",\"156\":\"Date of Conference: 16-21 May 2016\",\"157\":\"Date of Conference: 16-21 May 2016\",\"158\":\"Date of Conference: 16-21 May 2016\",\"159\":\"Date of Conference: 16-21 May 2016\",\"160\":\"Date of Conference: 16-21 May 2016\",\"161\":\"Date of Conference: 16-21 May 2016\",\"162\":\"Date of Conference: 16-21 May 2016\",\"163\":\"Date of Conference: 16-21 May 2016\",\"164\":\"Date of Conference: 16-21 May 2016\",\"165\":\"Date of Conference: 16-21 May 2016\",\"166\":\"Date of Conference: 16-21 May 2016\",\"167\":\"Date of Conference: 16-21 May 2016\",\"168\":\"Date of Conference: 16-21 May 2016\",\"169\":\"Date of Conference: 16-21 May 2016\",\"170\":\"Date of Conference: 16-21 May 2016\",\"171\":\"Date of Conference: 16-21 May 2016\",\"172\":\"Date of Conference: 16-21 May 2016\",\"173\":\"Date of Conference: 16-21 May 2016\",\"174\":\"Date of Conference: 16-21 May 2016\",\"175\":\"Date of Conference: 16-21 May 2016\",\"176\":\"Date of Conference: 16-21 May 2016\",\"177\":\"Date of Conference: 16-21 May 2016\",\"178\":\"Date of Conference: 16-21 May 2016\",\"179\":\"Date of Conference: 16-21 May 2016\",\"180\":\"Date of Conference: 16-21 May 2016\",\"181\":\"Date of Conference: 16-21 May 2016\",\"182\":\"Date of Conference: 16-21 May 2016\",\"183\":\"Date of Conference: 16-21 May 2016\",\"184\":\"Date of Conference: 16-21 May 2016\",\"185\":\"Date of Conference: 16-21 May 2016\",\"186\":\"Date of Conference: 16-21 May 2016\",\"187\":\"Date of Conference: 16-21 May 2016\",\"188\":\"Date of Conference: 16-21 May 2016\",\"189\":\"Date of Conference: 16-21 May 2016\",\"190\":\"Date of Conference: 16-21 May 2016\",\"191\":\"Date of Conference: 16-21 May 2016\",\"192\":\"Date of Conference: 16-21 May 2016\",\"193\":\"Date of Conference: 16-21 May 2016\",\"194\":\"Date of Conference: 16-21 May 2016\",\"195\":\"Date of Conference: 16-21 May 2016\",\"196\":\"Date of Conference: 16-21 May 2016\",\"197\":\"Date of Conference: 16-21 May 2016\",\"198\":\"Date of Conference: 16-21 May 2016\",\"199\":\"Date of Conference: 16-21 May 2016\",\"200\":\"Date of Conference: 16-21 May 2016\",\"201\":\"Date of Conference: 16-21 May 2016\",\"202\":\"Date of Conference: 16-21 May 2016\",\"203\":\"Date of Conference: 16-21 May 2016\",\"204\":\"Date of Conference: 16-21 May 2016\",\"205\":\"Date of Conference: 16-21 May 2016\",\"206\":\"Date of Conference: 16-21 May 2016\",\"207\":\"Date of Conference: 16-21 May 2016\",\"208\":\"Date of Conference: 16-21 May 2016\",\"209\":\"Date of Conference: 16-21 May 2016\",\"210\":\"Date of Conference: 16-21 May 2016\",\"211\":\"Date of Conference: 16-21 May 2016\",\"212\":\"Date of Conference: 16-21 May 2016\",\"213\":\"Date of Conference: 16-21 May 2016\",\"214\":\"Date of Conference: 16-21 May 2016\",\"215\":\"Date of Conference: 16-21 May 2016\",\"216\":\"Date of Conference: 16-21 May 2016\",\"217\":\"Date of Conference: 16-21 May 2016\",\"218\":\"Date of Conference: 16-21 May 2016\",\"219\":\"Date of Conference: 16-21 May 2016\",\"220\":\"Date of Conference: 16-21 May 2016\",\"221\":\"Date of Conference: 16-21 May 2016\",\"222\":\"Date of Conference: 16-21 May 2016\",\"223\":\"Date of Conference: 16-21 May 2016\",\"224\":\"Date of Conference: 16-21 May 2016\",\"225\":\"Date of Conference: 16-21 May 2016\",\"226\":\"Date of Conference: 16-21 May 2016\",\"227\":\"Date of Conference: 16-21 May 2016\",\"228\":\"Date of Conference: 16-21 May 2016\",\"229\":\"Date of Conference: 16-21 May 2016\",\"230\":\"Date of Conference: 16-21 May 2016\",\"231\":\"Date of Conference: 16-21 May 2016\",\"232\":\"Date of Conference: 16-21 May 2016\",\"233\":\"Date of Conference: 16-21 May 2016\",\"234\":\"Date of Conference: 16-21 May 2016\",\"235\":\"Date of Conference: 16-21 May 2016\",\"236\":\"Date of Conference: 16-21 May 2016\",\"237\":\"Date of Conference: 16-21 May 2016\",\"238\":\"Date of Conference: 16-21 May 2016\",\"239\":\"Date of Conference: 16-21 May 2016\",\"240\":\"Date of Conference: 16-21 May 2016\",\"241\":\"Date of Conference: 16-21 May 2016\",\"242\":\"Date of Conference: 16-21 May 2016\",\"243\":\"Date of Conference: 16-21 May 2016\",\"244\":\"Date of Conference: 16-21 May 2016\",\"245\":\"Date of Conference: 16-21 May 2016\",\"246\":\"Date of Conference: 16-21 May 2016\",\"247\":\"Date of Conference: 16-21 May 2016\",\"248\":\"Date of Conference: 16-21 May 2016\",\"249\":\"Date of Conference: 16-21 May 2016\",\"250\":\"Date of Conference: 16-21 May 2016\",\"251\":\"Date of Conference: 16-21 May 2016\",\"252\":\"Date of Conference: 16-21 May 2016\",\"253\":\"Date of Conference: 16-21 May 2016\",\"254\":\"Date of Conference: 16-21 May 2016\",\"255\":\"Date of Conference: 16-21 May 2016\",\"256\":\"Date of Conference: 16-21 May 2016\",\"257\":\"Date of Conference: 16-21 May 2016\",\"258\":\"Date of Conference: 16-21 May 2016\",\"259\":\"Date of Conference: 16-21 May 2016\",\"260\":\"Date of Conference: 16-21 May 2016\",\"261\":\"Date of Conference: 16-21 May 2016\",\"262\":\"Date of Conference: 16-21 May 2016\",\"263\":\"Date of Conference: 16-21 May 2016\",\"264\":\"Date of Conference: 16-21 May 2016\",\"265\":\"Date of Conference: 16-21 May 2016\",\"266\":\"Date of Conference: 16-21 May 2016\",\"267\":\"Date of Conference: 16-21 May 2016\",\"268\":\"Date of Conference: 16-21 May 2016\",\"269\":\"Date of Conference: 16-21 May 2016\",\"270\":\"Date of Conference: 16-21 May 2016\",\"271\":\"Date of Conference: 16-21 May 2016\",\"272\":\"Date of Conference: 16-21 May 2016\",\"273\":\"Date of Conference: 16-21 May 2016\",\"274\":\"Date of Conference: 16-21 May 2016\",\"275\":\"Date of Conference: 16-21 May 2016\",\"276\":\"Date of Conference: 16-21 May 2016\",\"277\":\"Date of Conference: 16-21 May 2016\",\"278\":\"Date of Conference: 16-21 May 2016\",\"279\":\"Date of Conference: 16-21 May 2016\",\"280\":\"Date of Conference: 16-21 May 2016\",\"281\":\"Date of Conference: 16-21 May 2016\",\"282\":\"Date of Conference: 16-21 May 2016\",\"283\":\"Date of Conference: 16-21 May 2016\",\"284\":\"Date of Conference: 16-21 May 2016\",\"285\":\"Date of Conference: 16-21 May 2016\",\"286\":\"Date of Conference: 16-21 May 2016\",\"287\":\"Date of Conference: 16-21 May 2016\",\"288\":\"Date of Conference: 16-21 May 2016\",\"289\":\"Date of Conference: 16-21 May 2016\",\"290\":\"Date of Conference: 16-21 May 2016\",\"291\":\"Date of Conference: 16-21 May 2016\",\"292\":\"Date of Conference: 16-21 May 2016\",\"293\":\"Date of Conference: 16-21 May 2016\",\"294\":\"Date of Conference: 16-21 May 2016\",\"295\":\"Date of Conference: 16-21 May 2016\",\"296\":\"Date of Conference: 16-21 May 2016\",\"297\":\"Date of Conference: 16-21 May 2016\",\"298\":\"Date of Conference: 16-21 May 2016\",\"299\":\"Date of Conference: 16-21 May 2016\",\"300\":\"Date of Conference: 16-21 May 2016\",\"301\":\"Date of Conference: 16-21 May 2016\",\"302\":\"Date of Conference: 16-21 May 2016\",\"303\":\"Date of Conference: 16-21 May 2016\",\"304\":\"Date of Conference: 16-21 May 2016\",\"305\":\"Date of Conference: 16-21 May 2016\",\"306\":\"Date of Conference: 16-21 May 2016\",\"307\":\"Date of Conference: 16-21 May 2016\",\"308\":\"Date of Conference: 16-21 May 2016\",\"309\":\"Date of Conference: 16-21 May 2016\",\"310\":\"Date of Conference: 16-21 May 2016\",\"311\":\"Date of Conference: 16-21 May 2016\",\"312\":\"Date of Conference: 16-21 May 2016\",\"313\":\"Date of Conference: 16-21 May 2016\",\"314\":\"Date of Conference: 16-21 May 2016\",\"315\":\"Date of Conference: 16-21 May 2016\",\"316\":\"Date of Conference: 16-21 May 2016\",\"317\":\"Date of Conference: 16-21 May 2016\",\"318\":\"Date of Conference: 16-21 May 2016\",\"319\":\"Date of Conference: 16-21 May 2016\",\"320\":\"Date of Conference: 16-21 May 2016\",\"321\":\"Date of Conference: 16-21 May 2016\",\"322\":\"Date of Conference: 16-21 May 2016\",\"323\":\"Date of Conference: 16-21 May 2016\",\"324\":\"Date of Conference: 16-21 May 2016\",\"325\":\"Date of Conference: 16-21 May 2016\",\"326\":\"Date of Conference: 16-21 May 2016\",\"327\":\"Date of Conference: 16-21 May 2016\",\"328\":\"Date of Conference: 16-21 May 2016\",\"329\":\"Date of Conference: 16-21 May 2016\",\"330\":\"Date of Conference: 16-21 May 2016\",\"331\":\"Date of Conference: 16-21 May 2016\",\"332\":\"Date of Conference: 16-21 May 2016\",\"333\":\"Date of Conference: 16-21 May 2016\",\"334\":\"Date of Conference: 16-21 May 2016\",\"335\":\"Date of Conference: 16-21 May 2016\",\"336\":\"Date of Conference: 16-21 May 2016\",\"337\":\"Date of Conference: 16-21 May 2016\",\"338\":\"Date of Conference: 16-21 May 2016\",\"339\":\"Date of Conference: 16-21 May 2016\",\"340\":\"Date of Conference: 16-21 May 2016\",\"341\":\"Date of Conference: 16-21 May 2016\",\"342\":\"Date of Conference: 16-21 May 2016\",\"343\":\"Date of Conference: 16-21 May 2016\",\"344\":\"Date of Conference: 16-21 May 2016\",\"345\":\"Date of Conference: 16-21 May 2016\",\"346\":\"Date of Conference: 16-21 May 2016\",\"347\":\"Date of Conference: 16-21 May 2016\",\"348\":\"Date of Conference: 16-21 May 2016\",\"349\":\"Date of Conference: 16-21 May 2016\",\"350\":\"Date of Conference: 16-21 May 2016\",\"351\":\"Date of Conference: 16-21 May 2016\",\"352\":\"Date of Conference: 16-21 May 2016\",\"353\":\"Date of Conference: 16-21 May 2016\",\"354\":\"Date of Conference: 16-21 May 2016\",\"355\":\"Date of Conference: 16-21 May 2016\",\"356\":\"Date of Conference: 16-21 May 2016\",\"357\":\"Date of Conference: 16-21 May 2016\",\"358\":\"Date of Conference: 16-21 May 2016\",\"359\":\"Date of Conference: 16-21 May 2016\",\"360\":\"Date of Conference: 16-21 May 2016\",\"361\":\"Date of Conference: 16-21 May 2016\",\"362\":\"Date of Conference: 16-21 May 2016\",\"363\":\"Date of Conference: 16-21 May 2016\",\"364\":\"Date of Conference: 16-21 May 2016\",\"365\":\"Date of Conference: 16-21 May 2016\",\"366\":\"Date of Conference: 16-21 May 2016\",\"367\":\"Date of Conference: 16-21 May 2016\",\"368\":\"Date of Conference: 16-21 May 2016\",\"369\":\"Date of Conference: 16-21 May 2016\",\"370\":\"Date of Conference: 16-21 May 2016\",\"371\":\"Date of Conference: 16-21 May 2016\",\"372\":\"Date of Conference: 16-21 May 2016\",\"373\":\"Date of Conference: 16-21 May 2016\",\"374\":\"Date of Conference: 16-21 May 2016\",\"375\":\"Date of Conference: 16-21 May 2016\",\"376\":\"Date of Conference: 16-21 May 2016\",\"377\":\"Date of Conference: 16-21 May 2016\",\"378\":\"Date of Conference: 16-21 May 2016\",\"379\":\"Date of Conference: 16-21 May 2016\",\"380\":\"Date of Conference: 16-21 May 2016\",\"381\":\"Date of Conference: 16-21 May 2016\",\"382\":\"Date of Conference: 16-21 May 2016\",\"383\":\"Date of Conference: 16-21 May 2016\",\"384\":\"Date of Conference: 16-21 May 2016\",\"385\":\"Date of Conference: 16-21 May 2016\",\"386\":\"Date of Conference: 16-21 May 2016\",\"387\":\"Date of Conference: 16-21 May 2016\",\"388\":\"Date of Conference: 16-21 May 2016\",\"389\":\"Date of Conference: 16-21 May 2016\",\"390\":\"Date of Conference: 16-21 May 2016\",\"391\":\"Date of Conference: 16-21 May 2016\",\"392\":\"Date of Conference: 16-21 May 2016\",\"393\":\"Date of Conference: 16-21 May 2016\",\"394\":\"Date of Conference: 16-21 May 2016\",\"395\":\"Date of Conference: 16-21 May 2016\",\"396\":\"Date of Conference: 16-21 May 2016\",\"397\":\"Date of Conference: 16-21 May 2016\",\"398\":\"Date of Conference: 16-21 May 2016\",\"399\":\"Date of Conference: 16-21 May 2016\",\"400\":\"Date of Conference: 16-21 May 2016\",\"401\":\"Date of Conference: 16-21 May 2016\",\"402\":\"Date of Conference: 16-21 May 2016\",\"403\":\"Date of Conference: 16-21 May 2016\",\"404\":\"Date of Conference: 16-21 May 2016\",\"405\":\"Date of Conference: 16-21 May 2016\",\"406\":\"Date of Conference: 16-21 May 2016\",\"407\":\"Date of Conference: 16-21 May 2016\",\"408\":\"Date of Conference: 16-21 May 2016\",\"409\":\"Date of Conference: 16-21 May 2016\",\"410\":\"Date of Conference: 16-21 May 2016\",\"411\":\"Date of Conference: 16-21 May 2016\",\"412\":\"Date of Conference: 16-21 May 2016\",\"413\":\"Date of Conference: 16-21 May 2016\",\"414\":\"Date of Conference: 16-21 May 2016\",\"415\":\"Date of Conference: 16-21 May 2016\",\"416\":\"Date of Conference: 16-21 May 2016\",\"417\":\"Date of Conference: 16-21 May 2016\",\"418\":\"Date of Conference: 16-21 May 2016\",\"419\":\"Date of Conference: 16-21 May 2016\",\"420\":\"Date of Conference: 16-21 May 2016\",\"421\":\"Date of Conference: 16-21 May 2016\",\"422\":\"Date of Conference: 16-21 May 2016\",\"423\":\"Date of Conference: 16-21 May 2016\",\"424\":\"Date of Conference: 16-21 May 2016\",\"425\":\"Date of Conference: 16-21 May 2016\",\"426\":\"Date of Conference: 16-21 May 2016\",\"427\":\"Date of Conference: 16-21 May 2016\",\"428\":\"Date of Conference: 16-21 May 2016\",\"429\":\"Date of Conference: 16-21 May 2016\",\"430\":\"Date of Conference: 16-21 May 2016\",\"431\":\"Date of Conference: 16-21 May 2016\",\"432\":\"Date of Conference: 16-21 May 2016\",\"433\":\"Date of Conference: 16-21 May 2016\",\"434\":\"Date of Conference: 16-21 May 2016\",\"435\":\"Date of Conference: 16-21 May 2016\",\"436\":\"Date of Conference: 16-21 May 2016\",\"437\":\"Date of Conference: 16-21 May 2016\",\"438\":\"Date of Conference: 16-21 May 2016\",\"439\":\"Date of Conference: 16-21 May 2016\",\"440\":\"Date of Conference: 16-21 May 2016\",\"441\":\"Date of Conference: 16-21 May 2016\",\"442\":\"Date of Conference: 16-21 May 2016\",\"443\":\"Date of Conference: 16-21 May 2016\",\"444\":\"Date of Conference: 16-21 May 2016\",\"445\":\"Date of Conference: 16-21 May 2016\",\"446\":\"Date of Conference: 16-21 May 2016\",\"447\":\"Date of Conference: 16-21 May 2016\",\"448\":\"Date of Conference: 16-21 May 2016\",\"449\":\"Date of Conference: 16-21 May 2016\",\"450\":\"Date of Conference: 16-21 May 2016\",\"451\":\"Date of Conference: 16-21 May 2016\",\"452\":\"Date of Conference: 16-21 May 2016\",\"453\":\"Date of Conference: 16-21 May 2016\",\"454\":\"Date of Conference: 16-21 May 2016\",\"455\":\"Date of Conference: 16-21 May 2016\",\"456\":\"Date of Conference: 16-21 May 2016\",\"457\":\"Date of Conference: 16-21 May 2016\",\"458\":\"Date of Conference: 16-21 May 2016\",\"459\":\"Date of Conference: 16-21 May 2016\",\"460\":\"Date of Conference: 16-21 May 2016\",\"461\":\"Date of Conference: 16-21 May 2016\",\"462\":\"Date of Conference: 16-21 May 2016\",\"463\":\"Date of Conference: 16-21 May 2016\",\"464\":\"Date of Conference: 16-21 May 2016\",\"465\":\"Date of Conference: 16-21 May 2016\",\"466\":\"Date of Conference: 16-21 May 2016\",\"467\":\"Date of Conference: 16-21 May 2016\",\"468\":\"Date of Conference: 16-21 May 2016\",\"469\":\"Date of Conference: 16-21 May 2016\",\"470\":\"Date of Conference: 16-21 May 2016\",\"471\":\"Date of Conference: 16-21 May 2016\",\"472\":\"Date of Conference: 16-21 May 2016\",\"473\":\"Date of Conference: 16-21 May 2016\",\"474\":\"Date of Conference: 16-21 May 2016\",\"475\":\"Date of Conference: 16-21 May 2016\",\"476\":\"Date of Conference: 16-21 May 2016\",\"477\":\"Date of Conference: 16-21 May 2016\",\"478\":\"Date of Conference: 16-21 May 2016\",\"479\":\"Date of Conference: 16-21 May 2016\",\"480\":\"Date of Conference: 16-21 May 2016\",\"481\":\"Date of Conference: 16-21 May 2016\",\"482\":\"Date of Conference: 16-21 May 2016\",\"483\":\"Date of Conference: 16-21 May 2016\",\"484\":\"Date of Conference: 16-21 May 2016\",\"485\":\"Date of Conference: 16-21 May 2016\",\"486\":\"Date of Conference: 16-21 May 2016\",\"487\":\"Date of Conference: 16-21 May 2016\",\"488\":\"Date of Conference: 16-21 May 2016\",\"489\":\"Date of Conference: 16-21 May 2016\",\"490\":\"Date of Conference: 16-21 May 2016\",\"491\":\"Date of Conference: 16-21 May 2016\",\"492\":\"Date of Conference: 16-21 May 2016\",\"493\":\"Date of Conference: 16-21 May 2016\",\"494\":\"Date of Conference: 16-21 May 2016\",\"495\":\"Date of Conference: 16-21 May 2016\",\"496\":\"Date of Conference: 16-21 May 2016\",\"497\":\"Date of Conference: 16-21 May 2016\",\"498\":\"Date of Conference: 16-21 May 2016\",\"499\":\"Date of Conference: 16-21 May 2016\",\"500\":\"Date of Conference: 16-21 May 2016\",\"501\":\"Date of Conference: 16-21 May 2016\",\"502\":\"Date of Conference: 16-21 May 2016\",\"503\":\"Date of Conference: 16-21 May 2016\",\"504\":\"Date of Conference: 16-21 May 2016\",\"505\":\"Date of Conference: 16-21 May 2016\",\"506\":\"Date of Conference: 16-21 May 2016\",\"507\":\"Date of Conference: 16-21 May 2016\",\"508\":\"Date of Conference: 16-21 May 2016\",\"509\":\"Date of Conference: 16-21 May 2016\",\"510\":\"Date of Conference: 16-21 May 2016\",\"511\":\"Date of Conference: 16-21 May 2016\",\"512\":\"Date of Conference: 16-21 May 2016\",\"513\":\"Date of Conference: 16-21 May 2016\",\"514\":\"Date of Conference: 16-21 May 2016\",\"515\":\"Date of Conference: 16-21 May 2016\",\"516\":\"Date of Conference: 16-21 May 2016\",\"517\":\"Date of Conference: 16-21 May 2016\",\"518\":\"Date of Conference: 16-21 May 2016\",\"519\":\"Date of Conference: 16-21 May 2016\",\"520\":\"Date of Conference: 16-21 May 2016\",\"521\":\"Date of Conference: 16-21 May 2016\",\"522\":\"Date of Conference: 16-21 May 2016\",\"523\":\"Date of Conference: 16-21 May 2016\",\"524\":\"Date of Conference: 16-21 May 2016\",\"525\":\"Date of Conference: 16-21 May 2016\",\"526\":\"Date of Conference: 16-21 May 2016\",\"527\":\"Date of Conference: 16-21 May 2016\",\"528\":\"Date of Conference: 16-21 May 2016\",\"529\":\"Date of Conference: 16-21 May 2016\",\"530\":\"Date of Conference: 16-21 May 2016\",\"531\":\"Date of Conference: 16-21 May 2016\",\"532\":\"Date of Conference: 16-21 May 2016\",\"533\":\"Date of Conference: 16-21 May 2016\",\"534\":\"Date of Conference: 16-21 May 2016\",\"535\":\"Date of Conference: 16-21 May 2016\",\"536\":\"Date of Conference: 16-21 May 2016\",\"537\":\"Date of Conference: 16-21 May 2016\",\"538\":\"Date of Conference: 16-21 May 2016\",\"539\":\"Date of Conference: 16-21 May 2016\",\"540\":\"Date of Conference: 16-21 May 2016\",\"541\":\"Date of Conference: 16-21 May 2016\",\"542\":\"Date of Conference: 16-21 May 2016\",\"543\":\"Date of Conference: 16-21 May 2016\",\"544\":\"Date of Conference: 16-21 May 2016\",\"545\":\"Date of Conference: 16-21 May 2016\",\"546\":\"Date of Conference: 16-21 May 2016\",\"547\":\"Date of Conference: 16-21 May 2016\",\"548\":\"Date of Conference: 16-21 May 2016\",\"549\":\"Date of Conference: 16-21 May 2016\",\"550\":\"Date of Conference: 16-21 May 2016\",\"551\":\"Date of Conference: 16-21 May 2016\",\"552\":\"Date of Conference: 16-21 May 2016\",\"553\":\"Date of Conference: 16-21 May 2016\",\"554\":\"Date of Conference: 16-21 May 2016\",\"555\":\"Date of Conference: 16-21 May 2016\",\"556\":\"Date of Conference: 16-21 May 2016\",\"557\":\"Date of Conference: 16-21 May 2016\",\"558\":\"Date of Conference: 16-21 May 2016\",\"559\":\"Date of Conference: 16-21 May 2016\",\"560\":\"Date of Conference: 16-21 May 2016\",\"561\":\"Date of Conference: 16-21 May 2016\",\"562\":\"Date of Conference: 16-21 May 2016\",\"563\":\"Date of Conference: 16-21 May 2016\",\"564\":\"Date of Conference: 16-21 May 2016\",\"565\":\"Date of Conference: 16-21 May 2016\",\"566\":\"Date of Conference: 16-21 May 2016\",\"567\":\"Date of Conference: 16-21 May 2016\",\"568\":\"Date of Conference: 16-21 May 2016\",\"569\":\"Date of Conference: 16-21 May 2016\",\"570\":\"Date of Conference: 16-21 May 2016\",\"571\":\"Date of Conference: 16-21 May 2016\",\"572\":\"Date of Conference: 16-21 May 2016\",\"573\":\"Date of Conference: 16-21 May 2016\",\"574\":\"Date of Conference: 16-21 May 2016\",\"575\":\"Date of Conference: 16-21 May 2016\",\"576\":\"Date of Conference: 16-21 May 2016\",\"577\":\"Date of Conference: 16-21 May 2016\",\"578\":\"Date of Conference: 16-21 May 2016\",\"579\":\"Date of Conference: 16-21 May 2016\",\"580\":\"Date of Conference: 16-21 May 2016\",\"581\":\"Date of Conference: 16-21 May 2016\",\"582\":\"Date of Conference: 16-21 May 2016\",\"583\":\"Date of Conference: 16-21 May 2016\",\"584\":\"Date of Conference: 16-21 May 2016\",\"585\":\"Date of Conference: 16-21 May 2016\",\"586\":\"Date of Conference: 16-21 May 2016\",\"587\":\"Date of Conference: 16-21 May 2016\",\"588\":\"Date of Conference: 16-21 May 2016\",\"589\":\"Date of Conference: 16-21 May 2016\",\"590\":\"Date of Conference: 16-21 May 2016\",\"591\":\"Date of Conference: 16-21 May 2016\",\"592\":\"Date of Conference: 16-21 May 2016\",\"593\":\"Date of Conference: 16-21 May 2016\",\"594\":\"Date of Conference: 16-21 May 2016\",\"595\":\"Date of Conference: 16-21 May 2016\",\"596\":\"Date of Conference: 16-21 May 2016\",\"597\":\"Date of Conference: 16-21 May 2016\",\"598\":\"Date of Conference: 16-21 May 2016\",\"599\":\"Date of Conference: 16-21 May 2016\",\"600\":\"Date of Conference: 16-21 May 2016\",\"601\":\"Date of Conference: 16-21 May 2016\",\"602\":\"Date of Conference: 16-21 May 2016\",\"603\":\"Date of Conference: 16-21 May 2016\",\"604\":\"Date of Conference: 16-21 May 2016\",\"605\":\"Date of Conference: 16-21 May 2016\",\"606\":\"Date of Conference: 16-21 May 2016\",\"607\":\"Date of Conference: 16-21 May 2016\",\"608\":\"Date of Conference: 16-21 May 2016\",\"609\":\"Date of Conference: 16-21 May 2016\",\"610\":\"Date of Conference: 16-21 May 2016\",\"611\":\"Date of Conference: 16-21 May 2016\",\"612\":\"Date of Conference: 16-21 May 2016\",\"613\":\"Date of Conference: 16-21 May 2016\",\"614\":\"Date of Conference: 16-21 May 2016\",\"615\":\"Date of Conference: 16-21 May 2016\",\"616\":\"Date of Conference: 16-21 May 2016\",\"617\":\"Date of Conference: 16-21 May 2016\",\"618\":\"Date of Conference: 16-21 May 2016\",\"619\":\"Date of Conference: 16-21 May 2016\",\"620\":\"Date of Conference: 16-21 May 2016\",\"621\":\"Date of Conference: 16-21 May 2016\",\"622\":\"Date of Conference: 16-21 May 2016\",\"623\":\"Date of Conference: 16-21 May 2016\",\"624\":\"Date of Conference: 16-21 May 2016\",\"625\":\"Date of Conference: 16-21 May 2016\",\"626\":\"Date of Conference: 16-21 May 2016\",\"627\":\"Date of Conference: 16-21 May 2016\",\"628\":\"Date of Conference: 16-21 May 2016\",\"629\":\"Date of Conference: 16-21 May 2016\",\"630\":\"Date of Conference: 16-21 May 2016\",\"631\":\"Date of Conference: 16-21 May 2016\",\"632\":\"Date of Conference: 16-21 May 2016\",\"633\":\"Date of Conference: 16-21 May 2016\",\"634\":\"Date of Conference: 16-21 May 2016\",\"635\":\"Date of Conference: 16-21 May 2016\",\"636\":\"Date of Conference: 16-21 May 2016\",\"637\":\"Date of Conference: 16-21 May 2016\",\"638\":\"Date of Conference: 16-21 May 2016\",\"639\":\"Date of Conference: 16-21 May 2016\",\"640\":\"Date of Conference: 16-21 May 2016\",\"641\":\"Date of Conference: 16-21 May 2016\",\"642\":\"Date of Conference: 16-21 May 2016\",\"643\":\"Date of Conference: 16-21 May 2016\",\"644\":\"Date of Conference: 16-21 May 2016\",\"645\":\"Date of Conference: 16-21 May 2016\",\"646\":\"Date of Conference: 16-21 May 2016\",\"647\":\"Date of Conference: 16-21 May 2016\",\"648\":\"Date of Conference: 16-21 May 2016\",\"649\":\"Date of Conference: 16-21 May 2016\",\"650\":\"Date of Conference: 16-21 May 2016\",\"651\":\"Date of Conference: 16-21 May 2016\",\"652\":\"Date of Conference: 16-21 May 2016\",\"653\":\"Date of Conference: 16-21 May 2016\",\"654\":\"Date of Conference: 16-21 May 2016\",\"655\":\"Date of Conference: 16-21 May 2016\",\"656\":\"Date of Conference: 16-21 May 2016\",\"657\":\"Date of Conference: 16-21 May 2016\",\"658\":\"Date of Conference: 16-21 May 2016\",\"659\":\"Date of Conference: 16-21 May 2016\",\"660\":\"Date of Conference: 16-21 May 2016\",\"661\":\"Date of Conference: 16-21 May 2016\",\"662\":\"Date of Conference: 16-21 May 2016\",\"663\":\"Date of Conference: 16-21 May 2016\",\"664\":\"Date of Conference: 16-21 May 2016\",\"665\":\"Date of Conference: 16-21 May 2016\",\"666\":\"Date of Conference: 16-21 May 2016\",\"667\":\"Date of Conference: 16-21 May 2016\",\"668\":\"Date of Conference: 16-21 May 2016\",\"669\":\"Date of Conference: 16-21 May 2016\",\"670\":\"Date of Conference: 16-21 May 2016\",\"671\":\"Date of Conference: 16-21 May 2016\",\"672\":\"Date of Conference: 16-21 May 2016\",\"673\":\"Date of Conference: 16-21 May 2016\",\"674\":\"Date of Conference: 16-21 May 2016\",\"675\":\"Date of Conference: 16-21 May 2016\",\"676\":\"Date of Conference: 16-21 May 2016\",\"677\":\"Date of Conference: 16-21 May 2016\",\"678\":\"Date of Conference: 16-21 May 2016\",\"679\":\"Date of Conference: 16-21 May 2016\",\"680\":\"Date of Conference: 16-21 May 2016\",\"681\":\"Date of Conference: 16-21 May 2016\",\"682\":\"Date of Conference: 16-21 May 2016\",\"683\":\"Date of Conference: 16-21 May 2016\",\"684\":\"Date of Conference: 16-21 May 2016\",\"685\":\"Date of Conference: 16-21 May 2016\",\"686\":\"Date of Conference: 16-21 May 2016\",\"687\":\"Date of Conference: 16-21 May 2016\",\"688\":\"Date of Conference: 16-21 May 2016\",\"689\":\"Date of Conference: 16-21 May 2016\",\"690\":\"Date of Conference: 16-21 May 2016\",\"691\":\"Date of Conference: 16-21 May 2016\",\"692\":\"Date of Conference: 16-21 May 2016\"},\"Paper Title\":{\"0\":\"Exact robot navigation using power diagrams\",\"1\":\"Gaussian Process Motion planning\",\"2\":\"Topological trajectory clustering with relative persistent homology\",\"3\":\"High-dimensional Winding-Augmented Motion Planning with 2D topological task projections and persistent homology\",\"4\":\"Resource-aware motion planning\",\"5\":\"An MDP-based approximation method for goal constrained multi-MAV planning under action uncertainty\",\"6\":\"Learning high-dimensional Mixture Models for fast collision detection in Rapidly-Exploring Random Trees\",\"7\":\"Burs of free C-space: A novel structure for path planning\",\"8\":\"Path planning for robotic manipulators using expanded bubbles of free C-space\",\"9\":\"Reduced complexity multi-scale path-planning on probabilistic maps\",\"10\":\"Hierarchical rejection sampling for informed kinodynamic planning in high-dimensional spaces\",\"11\":\"A highly sensitive dual mode tactile and proximity sensor using Carbon Microcoils for robotic applications\",\"12\":\"Nanoforce sensing with magnetic springs using a differential approach to compensate external mechanical disturbances\",\"13\":\"Improved normal and shear tactile force sensor performance via Least Squares Artificial Neural Network (LSANN)\",\"14\":\"Tactile manipulation with biomimetic active touch\",\"15\":\"Narrow passage sampling in the observation of robotic assembly tasks\",\"16\":\"Robotic grasp control with high-resolution combined tactile and proximity sensing\",\"17\":\"Experience-based torque estimation for an industrial robot\",\"18\":\"Compressed sensing for tactile skins\",\"19\":\"Variability and predictability in tactile sensing during grasping\",\"20\":\"Analytic grasp success prediction with tactile feedback\",\"21\":\"Designing embroidered electrodes for wearable surface electromyography\",\"22\":\"Terrain contact modeling and classification for ATVs\",\"23\":\"Probabilistic consolidation of grasp experience\",\"24\":\"Movement primitives with multiple phase parameters\",\"25\":\"Learning optimal navigation actions for foresighted robot behavior during assistance tasks\",\"26\":\"List prediction applied to motion planning\",\"27\":\"Learning soft task priorities for control of redundant robots\",\"28\":\"Stream-based Active Learning for efficient and adaptive classification of 3D objects\",\"29\":\"Generalizing demonstrated motions and adaptive motion generation using an invariant rigid body trajectory representation\",\"30\":\"Combining model-based policy search with online model learning for control of physical humanoids\",\"31\":\"Learning movement synchronization in multi-component robotic systems\",\"32\":\"Hierarchical Interactive Learning for a HUman-Powered Augmentation Lower EXoskeleton\",\"33\":\"Drifting Gaussian processes with varying neighborhood sizes for online model learning\",\"34\":\"Automatic LQR tuning based on Gaussian process global optimization\",\"35\":\"Monitoring the evolution of clouds with UAVs\",\"36\":\"Planning for a ground-air robotic system with collaborative localization\",\"37\":\"Proxemic group behaviors using reciprocal multi-agent navigation\",\"38\":\"Real-time reciprocal collision avoidance with elliptical agents\",\"39\":\"An analysis on flexible formations of nonholonomic mobile robots: Motion planning and control perspectives\",\"40\":\"Optimal technique for manipulation of a group of cells using optical tweezers\",\"41\":\"Cooperative multi-quadrotor pursuit of an evader in an environment with no-fly zones\",\"42\":\"Risk aversion in finite Markov Decision Processes using total cost criteria and average value at risk\",\"43\":\"An efficient algorithm for fault-tolerant rendezvous of multi-robot systems with controllable sensing range\",\"44\":\"Using fractional-order differential equations for health monitoring of a system of cooperating robots\",\"45\":\"A convex polynomial force-motion model for planar sliding: Identification and application\",\"46\":\"Optimal control with learned local models: Application to dexterous manipulation\",\"47\":\"A two-fingered underactuated anthropomorphic manipulator based on human precision manipulation motions\",\"48\":\"Finger trajectory generation for planar dexterous micro-manipulation\",\"49\":\"Adaptive control for pivoting with visual and tactile feedback\",\"50\":\"Multimodal execution monitoring for anomaly detection during robot manipulation\",\"51\":\"Sensorless and constraint based peg-in-hole task execution with a dual-arm robot\",\"52\":\"A framework for fine robotic assembly\",\"53\":\"Kinematic multi-robot manipulation with no communication using force feedback\",\"54\":\"Decentralized motion control for cooperative manipulation with a team of networked mobile manipulators\",\"55\":\"Guided search for task and motion plans using learned heuristics\",\"56\":\"Adaptive object centered teleoperation control of a mobile manipulator\",\"57\":\"SHIV: Reducing supervisor burden in DAgger using support vectors for efficient learning from demonstrations in high dimensional state spaces\",\"58\":\"Robust learning from demonstration using leveraged Gaussian processes and sparse-constrained optimization\",\"59\":\"Avoiding moving obstacles with stochastic hybrid dynamics using PEARL: PrEference Appraisal Reinforcement Learning\",\"60\":\"Safe controller optimization for quadrotors with Gaussian processes\",\"61\":\"Variable duration movement encoding with minimal intervention control\",\"62\":\"Model-based reinforcement learning with parametrized physical models and optimism-driven exploration\",\"63\":\"Deep spatial autoencoders for visuomotor learning\",\"64\":\"Learning deep neural network policies with continuous memory states\",\"65\":\"Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search\",\"66\":\"Deep learning for tactile understanding from visual and haptic data\",\"67\":\"Incremental semiparametric inverse dynamics learning\",\"68\":\"Self-learning and adaptation in a sensorimotor framework\",\"69\":\"Robust tracking of unknown objects through adaptive size estimation and appearance learning\",\"70\":\"Global data association for the Probability Hypothesis Density filter using network flows\",\"71\":\"Robust camera motion estimation using direct edge alignment and sub-gradient method\",\"72\":\"Monocular 3D tracking of deformable surfaces\",\"73\":\"Articulated motion estimation from a monocular image sequence using spherical tangent bundles\",\"74\":\"Depth-based object tracking using a Robust Gaussian Filter\",\"75\":\"Robot arm pose estimation by pixel-wise regression of joint angles\",\"76\":\"Saccade Mirror 3: High-speed gaze controller with ultra wide gaze control range using triple rotational mirrors\",\"77\":\"A lightweight, multi-axis compliant tensegrity joint\",\"78\":\"Compliant actuation for energy efficient impedance modulation\",\"79\":\"Design of a variable compliant humanoid foot with a new toe mechanism\",\"80\":\"Work density analysis of adjustable stiffness mechanisms\",\"81\":\"Design of a hopping mechanism using a voice coil actuator: Linear elastic actuator in parallel (LEAP)\",\"82\":\"Design and evaluation of a novel variable stiffness spherical joint with application to MR-compatible robot design\",\"83\":\"+SPEA introduction: Drastic actuator energy requirement reduction by symbiosis of parallel motors, springs and locking mechanisms\",\"84\":\"A lightweight, low-power electroadhesive clutch and spring for exoskeleton actuation\",\"85\":\"A hybrid hydrostatic transmission and human-safe haptic telepresence robot\",\"86\":\"Image-based robotic system for enhanced minimally invasive intra-articular fracture surgeries\",\"87\":\"Plenoptic cameras in surgical robotics: Calibration, registration, and evaluation\",\"88\":\"Hubot: A three state Human-Robot collaborative framework for bimanual surgical tasks based on learned models\",\"89\":\"Toward human-robot collaboration in surgery: Performance assessment of human and robotic agents in an inclusion segmentation task\",\"90\":\"Development of a robotic system for orthodontic archwire bending\",\"91\":\"Robotic ultrasound trajectory planning for volume of interest coverage\",\"92\":\"Optical-inertial tracking of an input device for real-time robot control\",\"93\":\"Soft pop-up mechanisms for micro surgical tools: Design and characterization of compliant millimeter-scale articulated structures\",\"94\":\"A continuum manipulator with phase changing alloy\",\"95\":\"Learning binary features online from motion dynamics for incremental loop-closure detection and place recognition\",\"96\":\"Fast, robust, continuous monocular egomotion computation\",\"97\":\"Low-latency image processing for vision-based navigation systems\",\"98\":\"Made to measure: Bespoke landmarks for 24-hour, all-weather localisation with a camera\",\"99\":\"Off the beaten track: Predicting localisation performance in visual teach and repeat\",\"100\":\"Benefit of large field-of-view cameras for visual odometry\",\"101\":\"On degeneracy of optimization-based state estimation problems\",\"102\":\"PROBE-GK: Predictive robust estimation using generalized kernels\",\"103\":\"Multi-level mapping: Real-time dense monocular SLAM\",\"104\":\"On the workspace of suspended cable-driven parallel robots\",\"105\":\"Triple Scissor Extender: A 6-DOF lifting and positioning robot\",\"106\":\"Dynamic isotropy in 3-DOF Gantry Tau robots - an analytical study\",\"107\":\"On the stiffness of three\\/four degree-of-freedom parallel pick-and-place robots with four identical limbs\",\"108\":\"Determination of the wrench-closure translational workspace in closed-form for cable-driven parallel robots\",\"109\":\"A comparison of the yaw constraining performance of SCARA-tau parallel manipulator variants via screw theory\",\"110\":\"Energy efficiency of cable-driven parallel robots\",\"111\":\"Wire-driven parallel robotic system and its control for maintenance of offshore wind turbines\",\"112\":\"Ingestible, controllable, and degradable origami robot for patching stomach wounds\",\"113\":\"Considerations for follow-the-leader motion of extensible tendon-driven continuum robots\",\"114\":\"Complementary model update: A method for simultaneous registration and stiffness mapping in flexible environments\",\"115\":\"Using Bayesian optimization to guide probing of a flexible environment for simultaneous registration and stiffness mapping\",\"116\":\"Toward real-time 3D ultrasound registration-based visual servoing for interventional navigation\",\"117\":\"Dry-wireless EEG and asynchronous adaptive feature extraction towards a plug-and-play co-adaptive brain robot interface\",\"118\":\"Visual-tactile sensory map calibration of a biomimetic whiskered robot\",\"119\":\"Microneedle-based high-density surface EMG interface with high selectivity for finger movement recognition\",\"120\":\"What lies behind: Recovering hidden shape in dense mapping\",\"121\":\"Surface reconstruction from image space adjacency of lines using breadth-first plane search\",\"122\":\"Fast, accurate gaussian process occupancy maps via test-data octrees and nested Bayesian fusion\",\"123\":\"Probabilistic map fusion for fast, incremental occupancy mapping with 3D Hilbert maps\",\"124\":\"Room segmentation: Survey, implementation, and analysis\",\"125\":\"From grids to continuous occupancy maps through area kernels\",\"126\":\"Automated three-dimensional axis mapping with a mobile platform\",\"127\":\"Towards lifelong feature-based mapping in semi-static environments\",\"128\":\"Large-scale cooperative 3D visual-inertial mapping in a Manhattan world\",\"129\":\"A passivity-based approach for trajectory tracking and link-side damping of compliantly actuated robots\",\"130\":\"Simultaneous position and stiffness control for an inflatable soft robot\",\"131\":\"Region control for robots driven by series elastic actuators\",\"132\":\"Improved dynamic formulation for decoupled cartesian admittance control and RCM constraint\",\"133\":\"Implicit force control for an industrial robot based on stiffness estimation and compensation during motion\",\"134\":\"A model compensation-prediction scheme for control of micromanipulation systems with a single feedback loop\",\"135\":\"Observer based impedance control of a pneumatic system with long transmission lines\",\"136\":\"KONTUR-2: Force-feedback teleoperation from the international space station\",\"137\":\"Fast image mosaicing using incremental bags of binary words\",\"138\":\"On-board vision-based 3D relative localization system for multiple quadrotors\",\"139\":\"Cooperative sensor fault recovery in multi-UAV systems\",\"140\":\"Rate-adaptive multicast video streaming from teams of micro aerial vehicles\",\"141\":\"Collaborative localization and formation flying using distributed stereo-vision\",\"142\":\"Experiments on coordinated motion of aerial robotic manipulators\",\"143\":\"Optimal event handling by multiple unmanned aerial vehicles\",\"144\":\"Landing of a fixed-wing UAV on a mobile ground vehicle\",\"145\":\"Fast nonlinear model predictive control via partial enumeration\",\"146\":\"Assistive collision avoidance for quadrotor swarm teleoperation\",\"147\":\"Live-fly, large-scale field experimentation for large numbers of fixed-wing UAVs\",\"148\":\"Real-time loop closure in 2D LIDAR SLAM\",\"149\":\"Improving accuracy of feature-based RGB-D SLAM by modeling spatial uncertainty of point features\",\"150\":\"CPA-SLAM: Consistent plane-model alignment for direct RGB-D SLAM\",\"151\":\"Comparative design space exploration of dense and semi-dense SLAM\",\"152\":\"Pinpoint SLAM: A hybrid of 2D and 3D simultaneous localization and mapping for RGB-D sensors\",\"153\":\"Direct semi-dense SLAM for rolling shutter cameras\",\"154\":\"Tree-connectivity: Evaluating the graphical structure of SLAM\",\"155\":\"Fast depth edge detection and edge based RGB-D SLAM\",\"156\":\"Visual-inertial direct SLAM\",\"157\":\"A unified representation for application of architectural constraints in large-scale mapping\",\"158\":\"A unified resource-constrained framework for graph SLAM\",\"159\":\"Optimal configuration of series and parallel elasticity in a 2D Monoped\",\"160\":\"Optimization and stabilization of trajectories for constrained dynamical systems\",\"161\":\"Simultaneous optimization of gait and design parameters for bipedal robots\",\"162\":\"Model predictive control of autonomous mobility-on-demand systems\",\"163\":\"Evolutionary optimization for parameterized whole-body dynamic motor skills\",\"164\":\"Fast nonlinear Model Predictive Control for unified trajectory optimization and tracking\",\"165\":\"Optimal feedback linearization control of brushless motors\",\"166\":\"Optimizing gaze direction in a visual navigation task\",\"167\":\"Aggressive driving with model predictive path integral control\",\"168\":\"On reachability sets for optimal feedback controllers: Monitoring the approach of a region of attraction\",\"169\":\"3D dynamic walking with underactuated humanoid robots: A direct collocation framework for optimizing hybrid zero dynamics\",\"170\":\"Optimized and trusted collision avoidance for unmanned aerial vehicles using approximate dynamic programming\",\"171\":\"Receding Horizon \\\"Next-Best-View\\\" Planner for 3D Exploration\",\"172\":\"Aggressive quadrotor flight through cluttered environments using mixed integer programming\",\"173\":\"Online generation of collision-free trajectories for quadrotor flight in unknown cluttered environments\",\"174\":\"High speed navigation for quadrotors with limited onboard sensing\",\"175\":\"Minimum-energy path generation for a quadrotor UAV\",\"176\":\"Aggressive quadrotor flight using dense visual-inertial fusion\",\"177\":\"An analysis of wind field estimation and exploitation for quadrotor flight in the urban canopy layer\",\"178\":\"Fast radiation mapping and multiple source localization using topographic contour map and incremental density estimation\",\"179\":\"Informative soaring with drifting thermals\",\"180\":\"Fast, on-board, model-aided visual-inertial odometry system for quadrotor micro aerial vehicles\",\"181\":\"Velocity aided attitude estimation for aerial robotic vehicles using latent rotation scaling\",\"182\":\"Visual inertial odometry for quadrotors on SE(3)\",\"183\":\"Work those arms: Toward dynamic and stable humanoid walking that optimizes full-body motion\",\"184\":\"Synthesis of full-body 3-D human gait using optimal control methods\",\"185\":\"Off-line controller design for reliable walking of ranger\",\"186\":\"Walking pattern generators designed for physical collaboration\",\"187\":\"Focused online visual-motor coordination for a dual-arm robot manipulator\",\"188\":\"Robot-human balance state transfer during full-body humanoid teleoperation using Divergent Component of Motion dynamics\",\"189\":\"Fast algorithms to test robust static equilibrium for legged robots\",\"190\":\"Parametrization of Catmull-Clark subdivision surfaces for posture generation\",\"191\":\"Whole-body planning for humanoids along deformable tasks\",\"192\":\"NEOL: Toward Never-Ending Object Learning for robots\",\"193\":\"Daily activity recognition using the informative features from skeletal and depth data\",\"194\":\"Deep learning for human part discovery in images\",\"195\":\"Learning convolutional action primitives for fine-grained action recognition\",\"196\":\"Robust speech\\/non-speech discrimination based on pitch estimation for mobile robots\",\"197\":\"Multi-sensorial and explorative recognition of garments and their material properties in unconstrained environment\",\"198\":\"Color object recognition via cross-domain learning on RGB-D images\",\"199\":\"Arbitrary view action recognition via transfer dictionary learning on synthetic training data\",\"200\":\"Convolutional hypercube pyramid for accurate RGB-D object category and instance recognition\",\"201\":\"Control of microstructures propelled via bacterial baths\",\"202\":\"Feedback control of inertial focusing using real-time extraction of equilibrium positions\",\"203\":\"A fully automated robotic system for three-dimensional cell rotation\",\"204\":\"Catch, load and launch toward on-chip active cell evaluation\",\"205\":\"Magnetic microrobots with addressable shape control\",\"206\":\"Closed-loop 3D path following of scaled-up helical microswimmers\",\"207\":\"Five-degree-of-freedom magnetic control of micro-robots using rotating permanent magnets\",\"208\":\"Development of a 3D-magnetic tweezer system having magnetic pole positioning mechanism\",\"209\":\"Humanoid walking with compliant soles using a deformation estimator\",\"210\":\"Model predictive control for dynamic footstep adjustment using the divergent component of motion\",\"211\":\"Unified bipedal gait for walking and running by dynamics-based virtual holonomic constraint in PDAC\",\"212\":\"Looking for motor synergies in Darwin-OP biped robot\",\"213\":\"Real-time footstep planning using a geometric approach\",\"214\":\"Bipedal gait recharacterization and walking encoding generalization for stable dynamic walking\",\"215\":\"Realizing dynamic and efficient bipedal locomotion on the humanoid robot DURUS\",\"216\":\"Learning the odometry on a small humanoid robot\",\"217\":\"WALK-MAN humanoid lower body design optimization for enhanced physical performance\",\"218\":\"Inertial sensor-based humanoid joint state estimation\",\"219\":\"Gaussian Markov Random Fields for fusion in information form\",\"220\":\"Multi-sensor fusion of occupancy grids based on integer arithmetic\",\"221\":\"State estimation for tensegrity robots\",\"222\":\"2D-image to 3D-range registration in urban environments via scene categorization and combination of similarity measurements\",\"223\":\"Probabilistic multi-sensor fusion based on signed distance functions\",\"224\":\"A distributed MEMS gyro network for joint velocity estimation\",\"225\":\"Direct visual-inertial odometry with stereo cameras\",\"226\":\"Micro-hand positioning in consideration of the obstacle avoidance and the grasping part by using the automatic stage\",\"227\":\"Dynamics and scaling of magnetically folding multi-material structures\",\"228\":\"Optimal control of multiple magnetic microbeads navigating in microfluidic channels\",\"229\":\"Independent control of two millimeter-scale soft-bodied magnetic robotic swimmers\",\"230\":\"Sperm-shaped magnetic microrobots: Fabrication using electrospinning, modeling, and characterization\",\"231\":\"Novel In situ nanomanipulation integrated with SEM-CT imaging system\",\"232\":\"Dex-Net 1.0: A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with correlated rewards\",\"233\":\"Visual cues used to evaluate grasps from images\",\"234\":\"Reflex control of the Pisa\\/IIT SoftHand during object slippage\",\"235\":\"Rope caging and grasping\",\"236\":\"A fail-safe object handover controller\",\"237\":\"Lightweight underactuated pneumatic fingers capable of grasping various objects\",\"238\":\"Grasp detection for assistive robotic manipulation\",\"239\":\"On the evolution of fingertip grasping manifolds\",\"240\":\"Wrench resistant multi-finger hand mechanisms\",\"241\":\"Object discovery and grasp detection with a shared convolutional neural network\",\"242\":\"Precision grasping based on probabilistic models of unknown objects\",\"243\":\"Investigation of effects of dynamics on intrinsic wrench sensing in continuum robots\",\"244\":\"RobWorkPhysicsEngine: A new dynamic simulation engine for manipulation actions\",\"245\":\"A trajectory optimization formulation for assistive robotic devices\",\"246\":\"Extracting feasible robot parameters from dynamic coefficients using nonlinear optimization methods\",\"247\":\"Aiming and vaulting: Spider inspired leaping for jumping robots\",\"248\":\"Fast forward dynamics simulation of robot manipulators with highly frictional gears\",\"249\":\"Inverse kinematics and design of a novel 6-DoF handheld robot arm\",\"250\":\"Kinematic modeling and singularity treatment of steerable wheeled mobile robots with joint acceleration limits\",\"251\":\"The second generation prototype of a Duct Climbing Tensegrity robot, DuCTTv2\",\"252\":\"Power manipulability analysis of redundantly actuated parallel kinematic manipulators with different types of actuators\",\"253\":\"Design of a spherical robot arm with the Spiral Zipper prismatic joint\",\"254\":\"Robot learning with a spatial, temporal, and causal and-or graph\",\"255\":\"Robust single-view instance recognition\",\"256\":\"Deep metric learning autoencoder for nonlinear temporal alignment of human motion\",\"257\":\"Optimizing for what matters: The top grasp hypothesis\",\"258\":\"Real-time 3D scene layout from a single image using Convolutional Neural Networks\",\"259\":\"How many bits do I need for matching local binary descriptors?\",\"260\":\"Fusing LIDAR and images for pedestrian detection using convolutional neural networks\",\"261\":\"Alextrac: Affinity learning by exploring temporal reinforcement within association chains\",\"262\":\"Re-using prior tactile experience by robotic hands to discriminate in-hand objects via texture properties\",\"263\":\"An efficient probabilistic surface normal estimator\",\"264\":\"Simultaneous dense scene reconstruction and object labeling\",\"265\":\"Hierarchical graph-based discovery of non-primitive-shaped objects in unstructured environments\",\"266\":\"Cluttered scene segmentation using the symmetry constraint\",\"267\":\"Part-based room categorization for household service robots\",\"268\":\"When 2.5D is not enough: Simultaneous reconstruction, segmentation and recognition on dense SLAM\",\"269\":\"Environment exploration for object-based visual saliency learning\",\"270\":\"Efficient, dense, object-based segmentation from RGBD video\",\"271\":\"Understand scene categories by objects: A semantic regularized scene classifier using Convolutional Neural Networks\",\"272\":\"Detection of pedestrians at far distance\",\"273\":\"Integrated on-line robot-camera calibration and object pose estimation\",\"274\":\"Powered upper-limb control using passivity-based nonlinear disturbance observer for unknown payload carrying applications\",\"275\":\"Control of constrained robots subject to unilateral contacts and friction cone constraints\",\"276\":\"A hierarchical approach to minimum-time control of industrial robots\",\"277\":\"Cooperative redundant omnidirectional mobile manipulators: Model-free decentralized integral sliding modes and passive velocity fields\",\"278\":\"Robust two-degree-of-freedom iterative learning control for flexibility compensation of industrial robot manipulators\",\"279\":\"Fault tolerant control for omni-directional mobile platforms with 4 mecanum wheels\",\"280\":\"Softbot: Software-based lead-through for rigid servo robots\",\"281\":\"Model-free joint torque control strategy for hydraulic robots\",\"282\":\"Torque evaluation method of spherical motors using six-axis force\\/torque sensor\",\"283\":\"Catching the wave: A transparency oriented wave based teleoperation architecture\",\"284\":\"Sweet pepper pose detection and grasping for automated crop harvesting\",\"285\":\"Single image based camera calibration and pose estimation of the end-effector of a robot\",\"286\":\"Real-time scalable 6DOF pose estimation for textureless objects\",\"287\":\"Reshaping our model of the world over time\",\"288\":\"Vision system and depth processing for DRC-HUBO+\",\"289\":\"Recognising the clothing categories from free-configuration using Gaussian-Process-based interactive perception\",\"290\":\"A model-based approach to finding substitute tools in 3D vision data\",\"291\":\"Watch-Bot: Unsupervised learning for reminding humans of forgotten actions\",\"292\":\"Histogram of distances for local surface description\",\"293\":\"A distributed robotic vision service\",\"294\":\"Autonomous disassembly of electric vehicle motors based on robot cognition\",\"295\":\"Visual detection of occluded crop: For automated harvesting\",\"296\":\"Robust stereo visual odometry through a probabilistic combination of points and line segments\",\"297\":\"Augmented dictionary learning for motion prediction\",\"298\":\"A multistage controller with smooth switching for Autonomous Pallet Picking\",\"299\":\"Intent-aware long-term prediction of pedestrian motion\",\"300\":\"Robust homing for autonomous robots\",\"301\":\"First applications of sound-based control on a mobile robot equipped with two microphones\",\"302\":\"Route planning for active classification with UAVs\",\"303\":\"Robust sampling-based motion planning for autonomous tracked vehicles in deformable high slip terrain\",\"304\":\"Any-time path-planning: Time-varying wind field + moving obstacles\",\"305\":\"Active sensing data collection with autonomous mobile robots\",\"306\":\"Cluster-based loop closing detection for underwater slam in feature-poor regions\",\"307\":\"Neural-based underwater surface localization through electrolocation\",\"308\":\"Adaptive underwater sonar surveys in the presence of strong currents\",\"309\":\"Underwater Vehicles attitude estimation in presence of magnetic disturbances\",\"310\":\"Preliminary study of cooperative navigation of underwater vehicles without a DVL utilizing range and range-rate observations\",\"311\":\"Online fault detection and model adaptation for Underwater Vehicles in the case of thruster failures\",\"312\":\"3D underwater localization scheme using EM wave attenuation with a depth sensor\",\"313\":\"AUV behaviors for collection of bistatic and multistatic acoustic scattering data from seabed targets\",\"314\":\"Anomaly detection in unstructured environments using Bayesian nonparametric scene modeling\",\"315\":\"The efficacy of interaction behavior and internal stiffness control for embodied information gain in haptic perception\",\"316\":\"Haptic simulation of an automotive automatic gearshift: Stability analysis and design of force profiles with hysteresis\",\"317\":\"Passivity and practical considerations for the SNMF System\",\"318\":\"Unsupervised feature learning for classifying dynamic tactile events using sparse coding\",\"319\":\"User's task performance in two-handed complementary-motion teleoperation\",\"320\":\"Development and experimental validation of a minimalistic shape-changing haptic navigation device\",\"321\":\"A novel haptic device with high-force display capability and wide workspace\",\"322\":\"A unified representation to interact with simulated deformable objects in virtual environments\",\"323\":\"Closed-loop shape control of a Haptic Jamming deformable surface\",\"324\":\"Reduced dynamical equations for barycentric spherical robots\",\"325\":\"Development and experimental validation of a reorientation algorithm for a free-floating serial manipulator\",\"326\":\"Motion planning for a hoop-pendulum type of underactuated systems\",\"327\":\"A-Connect: Bounded suboptimal bidirectional heuristic search\",\"328\":\"Planning motions for a planar robot attached to a stiff tether\",\"329\":\"RRT-based nonholonomic motion planning using any-angle path biasing\",\"330\":\"Random Inspection Tree Algorithm in visual inspection with a realistic sensing model and differential constraints\",\"331\":\"The Stochastic Traveling Salesman Problem and Orienteering for kinodynamic vehicles\",\"332\":\"Human-robot collaborative high-level control with application to rescue robotics\",\"333\":\"Control and experimental validation of robot-assisted automatic measurement system for Multi-Stud Tensioning Machine (MSTM)\",\"334\":\"Verification of gait control based on reaction null-space for ground-gripping robot in microgravity\",\"335\":\"Measurement of stress distributions of a wheel with grousers traveling on loose soil\",\"336\":\"Task frame estimation during model-based teleoperation for satellite servicing\",\"337\":\"Design of four-arm four-crawler disaster response robot OCTOPUS\",\"338\":\"Motion control of a compliant wheel-leg robot for rough terrain crossing\",\"339\":\"Autonomous repositioning and localization of an in situ fabricator\",\"340\":\"Low dimensional human preference tracking for motion optimization\",\"341\":\"Laban head-motions convey robot state: A call for robot body language\",\"342\":\"Learning socially normative robot navigation behaviors with Bayesian inverse reinforcement learning\",\"343\":\"Autonomous indoor robot navigation using a sketch interface for drawing maps and routes\",\"344\":\"Singing minstrel robots, a means for improving social behaviors\",\"345\":\"transHumUs: A poetic experience in mobile robotics\",\"346\":\"Enhancing human-robot interaction by interpreting uncertain information in navigational commands based on experience and environment\",\"347\":\"A smartphone-based laser distance sensor for outdoor environments\",\"348\":\"Non-linear model-free control of flapping wing flying robot using iPID\",\"349\":\"Application of an approximate model predictive control scheme on an unmanned aerial vehicle\",\"350\":\"Trajectory generation for quadrotor based systems using numerical optimal control\",\"351\":\"From tracking to robust maneuver regulation: An easy-to-design approach for VTOL aerial robots\",\"352\":\"Adaptive Super Twisting Controller for a quadrotor UAV\",\"353\":\"Sensors model based data fusion using complementary filters for attitude estimation and stabilization\",\"354\":\"Obstacle detection, tracking and avoidance for a teleoperated UAV\",\"355\":\"Prop-hanging control of a thrust vector vehicle with hybrid Nonlinear Dynamic Inversion method\",\"356\":\"Full Attitude Control of a VTOL tailsitter UAV\",\"357\":\"On-line coaching of robots through visual and physical interaction: Analysis of effectiveness of human-robot interaction strategies\",\"358\":\"An ISO10218-compliant adaptive damping controller for safe physical human-robot interaction\",\"359\":\"Manipulator performance constraints in Cartesian admittance control for human-robot cooperation\",\"360\":\"Impedance-based Gaussian Processes for predicting human behavior during physical interaction\",\"361\":\"Cadence control of cycling wheelchair with Continuously Variable Transmission and servo brake\",\"362\":\"Essential considerations for design and control of human-interactive robots\",\"363\":\"Port-based modeling of human-robot collaboration towards safety-enhancing energy shaping control\",\"364\":\"Design and development of a hybrid Magneto-Rheological clutch for safe robotic applications\",\"365\":\"Angled sensor configuration capable of measuring tri-axial forces for pHRI\",\"366\":\"Algorithmic safety measures for intelligent industrial co-robots\",\"367\":\"Multi-constrained joint transportation tasks by teams of autonomous mobile robots using a dynamical systems approach\",\"368\":\"Recurrent Neural Networks for driver activity anticipation via sensory-fusion architecture\",\"369\":\"Unsupervised trajectory compression\",\"370\":\"Improving dependability of industrial transport robots using model-based techniques\",\"371\":\"Dynamic routing of energy-aware vehicles with Temporal Logic Constraints\",\"372\":\"A new multi-agent approach for lane detection and tracking\",\"373\":\"Optimal navigation policy for an autonomous agent operating in adversarial environments\",\"374\":\"Observability analysis and optimal sensor placement in stereo radar odometry\",\"375\":\"Traffic awareness driver assistance based on stereovision, eye-tracking, and head-up display\",\"376\":\"Exploiting fully convolutional neural networks for fast road detection\",\"377\":\"Multi-scale object candidates for generic object tracking in street scenes\",\"378\":\"High-performance and tunable stereo reconstruction\",\"379\":\"Generation and real-time implementation of high-speed controlled maneuvers using an autonomous 19-gram quadrotor\",\"380\":\"A novel hyperacute gimbal eye to implement precise hovering and target tracking on a quadrotor\",\"381\":\"Bat Bot (B2), a biologically inspired flying machine\",\"382\":\"Development of a 3.2g untethered flapping-wing platform for flight energetics and control experiments\",\"383\":\"Non-linear resonance modeling and system design improvements for underactuated flapping-wing vehicles\",\"384\":\"Thrust loss saving design of overlapping rotor arrangement on small multirotor unmanned aerial vehicles\",\"385\":\"Versatile aerial grasping using self-sealing suction\",\"386\":\"Local histogram matching for efficient optical flow computation applied to velocity estimation on pocket drones\",\"387\":\"Design, modeling and control of an omni-directional aerial vehicle\",\"388\":\"Dynamic underactuated flying-walking (DUCK) robot\",\"389\":\"A controllable flying vehicle with a single moving part\",\"390\":\"Anomalous yaw torque generation from passively pitching wings\",\"391\":\"SUAV:Q - a hybrid approach to solar-powered flight\",\"392\":\"3D gaze cursor: Continuous calibration and end-point grasp control of robotic actuators\",\"393\":\"SRAC: Self-Reflective Risk-Aware Artificial Cognitive models for robot response to human activities\",\"394\":\"Markerless perspective taking for humanoid robots in unconstrained environments\",\"395\":\"Novel planning-based algorithms for human motion prediction\",\"396\":\"Learning time series models for pedestrian motion prediction\",\"397\":\"Interpreting multimodal referring expressions in real time\",\"398\":\"Cubimorph: Designing modular interactive devices\",\"399\":\"Learning assistive strategies from a few user-robot interactions: Model-based reinforcement learning approach\",\"400\":\"Modeling communicative behaviors for object references in human-robot interaction\",\"401\":\"An architectural approach to safety of component-based robotic systems\",\"402\":\"Investigating spatial guidance for a cooperative handheld robot\",\"403\":\"Hierarchical action learning by instruction through interactive grounding of body parts and proto-actions\",\"404\":\"RoboBench: Towards sustainable robotics system benchmarking\",\"405\":\"Learning to remove multipath distortions in Time-of-Flight range images for a robotic arm setup\",\"406\":\"Exemplar-based prediction of global object shape from local shape similarity\",\"407\":\"Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours\",\"408\":\"Wavelets-based 6 DOF visual servoing\",\"409\":\"Towards Ultrasound-based visual servoing using shearlet coefficients\",\"410\":\"Visual servoing of a medical ultrasound probe for needle insertion\",\"411\":\"ViTa: Visual task specification interface for manipulation with uncalibrated visual servoing\",\"412\":\"Confidence-driven control of an ultrasound probe: Target-specific acoustic window optimization\",\"413\":\"Discriminative learning based visual servoing across object instances\",\"414\":\"A sparse snapshot-based navigation strategy for UAS guidance in natural environments\",\"415\":\"Real-time, GPU-based pose estimation of a UAV for autonomous takeoff and landing\",\"416\":\"Multi-robot visual support system by adaptive ROI selection based on gestalt perception\",\"417\":\"An information gain formulation for active volumetric 3D reconstruction\",\"418\":\"Design of a highly biomimetic anthropomorphic robotic hand towards artificial limb regeneration\",\"419\":\"Experimental implementation of underactuated potential energy shaping on a powered ankle-foot orthosis\",\"420\":\"IMU-based iterative control for hip extension assistance with a soft exosuit\",\"421\":\"Controlling negative and positive power at the ankle with a soft exosuit\",\"422\":\"SoftHand Pro-D: Matching dynamic content of natural user commands with hand embodiment for enhanced prosthesis control\",\"423\":\"A force-and-slippage control strategy for a poliarticulated prosthetic hand\",\"424\":\"Active Impedance Control of a lower limb exoskeleton to assist sit-to-stand movement\",\"425\":\"A fabric-regulated soft robotic glove with user intent detection using EMG and RFID for hand assistive application\",\"426\":\"An energy-efficient torque controller based on passive dynamics of human locomotion for a robotic transtibial prosthesis\",\"427\":\"Investigation of a cognitive strain on hand grasping induced by sensory feedback for myoelectric hand\",\"428\":\"A versatile and efficient pattern generator for generalized legged locomotion\",\"429\":\"3-DOF passive dynamic walking of compass-like biped robot with semicircular feet generated on slippery downhill\",\"430\":\"Model-based bounding on a quadruped robot\",\"431\":\"Robot locomotion on hard and soft ground: Measuring stability and ground properties in-situ\",\"432\":\"Quadruped pronking on compliant terrains using a reaction wheel\",\"433\":\"Acoustics based terrain classification for legged robots\",\"434\":\"Road following with blind crawling robot\",\"435\":\"Towards a multi-legged mobile manipulator\",\"436\":\"Beyond layers: A 3D-aware toolpath algorithm for fused filament fabrication\",\"437\":\"Robust control of automated manufacturing systems with assembly operations using petri nets\",\"438\":\"Tying knot precisely\",\"439\":\"Grasping and folding knots\",\"440\":\"Robotic folding of 2D and 3D structures from a ribbon\",\"441\":\"An automated system for investigating sperm orientation in fluid flow\",\"442\":\"Distributed supervisor synthesis for automated manufacturing systems with flexible routes and assembly operations using Petri nets\",\"443\":\"Simultaneous model identification and task satisfaction in the presence of temporal logic constraints\",\"444\":\"Optimal temporal logic planning in probabilistic semantic maps\",\"445\":\"Unilateral walking surface stiffness perturbations evoke brain responses: Toward bilaterally informed robot-assisted gait rehabilitation\",\"446\":\"Redundant kinematics and workspace centering control of AssistOn-Gait overground gait and balance trainer\",\"447\":\"Development of a walking assistance apparatus for gait training and promotion of exercise\",\"448\":\"Estimation of tremor parameters and extraction tremor from recorded signals for tremor suppression\",\"449\":\"Efficacy of coordinating shoulder and elbow motion in a myoelectric transhumeral prosthesis in reaching tasks\",\"450\":\"On the feasibility of wearable exotendon networks for whole-hand movement patterns in stroke patients\",\"451\":\"CPWalker: Robotic platform for gait rehabilitation in patients with Cerebral Palsy\",\"452\":\"Development of a polymer-based tendon-driven wearable robotic hand\",\"453\":\"Overground robot based gait rehabilitation system MOPASS - overview and first results from usability testing\",\"454\":\"Underactuated robot finger controlled by Variable Vibration Center Effect\",\"455\":\"Initial results for a ballbot driven with a spherical induction motor\",\"456\":\"Performance without tweaking differentiators via a PR controller: Furuta pendulum case study\",\"457\":\"Step climbing cooperation primitives for legged robots with a reversible connection\",\"458\":\"Online motion planning over uneven terrain with walking primitives and regression\",\"459\":\"Electroadhesive feet for turning control in legged robots\",\"460\":\"Differential jumping: A novel mode for micro-robot navigation\",\"461\":\"Control scheme of nongrasping manipulation based on virtual connecting constraint\",\"462\":\"Automatic configuration of mobile conveyor lines\",\"463\":\"Defect detection with estimation of material condition using ensemble learning for hammering test\",\"464\":\"Robust optimization of robotic pick and place operations for deformable objects through simulation\",\"465\":\"3D printing of variable stiffness hyper-redundant robotic arm\",\"466\":\"Printable hydraulics: A method for fabricating robots by 3D co-printing solids and liquids\",\"467\":\"High-performance robotic contour tracking based on the dynamic compensation concept\",\"468\":\"Differential feed control applied to corner matching in automated sewing\",\"469\":\"High accurate robotic drilling with external sensor and compliance model-based compensation\",\"470\":\"Towards manipulation planning for multiple interlinked deformable linear objects\",\"471\":\"Efficiently solving general rearrangement tasks: A fast extension primitive for an incremental sampling-based planner\",\"472\":\"A coordinate-free framework for robotic pizza tossing and catching\",\"473\":\"Rearrangement planning using object-centric and robot-centric action spaces\",\"474\":\"Considering avoidance and consistency in motion planning for human-robot manipulation in a shared workspace\",\"475\":\"Planning and execution of groping behavior for contact sensor based manipulation in an unknown environment\",\"476\":\"Planning for grasp selection of partially occluded objects\",\"477\":\"Affordance-feasible planning with manipulator wrench spaces\",\"478\":\"Folding assembly by means of dual-arm robotic manipulation\",\"479\":\"Optical manipulation of multiple microscopic objects with Brownian perturbations\",\"480\":\"Curvature control of soft orthotics via low cost solid-state optics\",\"481\":\"Hydro Muscle -a novel soft fluidic actuator\",\"482\":\"Design and implementation of a 300% strain soft artificial muscle\",\"483\":\"Comparison of kinesthetic and skin deformation feedback for mass rendering\",\"484\":\"Bayesian estimation of non-rigid mechanical parameters using temporal sequences of deformation samples\",\"485\":\"Musculoskeletal quadruped robot with Torque-Angle Relationship Control System\",\"486\":\"Hysteresis model of longitudinally loaded cable for cable driven robots and identification of the parameters\",\"487\":\"Design and analysis of a wire-driven flexible manipulator for bronchoscopic interventions\",\"488\":\"Real-time planner for multi-segment continuum manipulator in dynamic environments\",\"489\":\"A lightweight robotic arm with pneumatic muscles for robot learning\",\"490\":\"Teleoperation mappings from rigid link robots to their extensible continuum counterparts\",\"491\":\"Bimanual teleoperation with heart motion compensation on the da Vinci\\u00ae Research Kit: Implementation and preliminary experiments\",\"492\":\"Robust trocar detection and localization during robot-assisted endoscopic surgery\",\"493\":\"Robust image-based computation of the 3D position of RCM instruments and its application to image-guided manipulation\",\"494\":\"Thin-diameter chopsticks robot for Laparoscopic Surgery\",\"495\":\"Dynamic modeling of cable driven elongated surgical instruments for sensorless grip force estimation\",\"496\":\"Unscented Kalman Filter and 3D vision to improve cable driven surgical robot joint angle estimation\",\"497\":\"TSC-DL: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with Deep Learning\",\"498\":\"Unsupervised surgical data alignment with application to automatic activity annotation\",\"499\":\"Concurrent nonparametric estimation of organ geometry and tissue stiffness using continuous adaptive palpation\",\"500\":\"Path planning for robot-enhanced cardiac Radiofrequency Catheter Ablation\",\"501\":\"Automating multi-throw multilateral surgical suturing with a mechanical needle guide and sequential convex optimization\",\"502\":\"Automated in-plane OCT-probe positioning towards repetitive optical biopsies\",\"503\":\"Reactive high-level behavior synthesis for an Atlas humanoid robot\",\"504\":\"Real-time planning and execution of evasive motions for a humanoid robot\",\"505\":\"Regionally accelerated batch informed trees (RABIT): A framework to integrate local information into optimal path planning\",\"506\":\"An efficient robotic exploration planner with probabilistic guarantees\",\"507\":\"Decentralized multi-agent exploration with online-learning of Gaussian processes\",\"508\":\"Multimodal information-theoretic measures for autonomous exploration\",\"509\":\"Feedback motion planning under non-Gaussian uncertainty and non-convex state constraints\",\"510\":\"Online motion generation for mirroring human arm motion\",\"511\":\"Pursuit-evasion with fixed beams\",\"512\":\"Voronoi-based coverage control of heterogeneous disk-shaped robots\",\"513\":\"Super ray based updates for occupancy maps\",\"514\":\"The right direction to smell: Efficient sensor planning strategies for robot assisted gas tomography\",\"515\":\"Integrated control of a multi-fingered hand and arm using proximity sensors on the fingertips\",\"516\":\"Self-calibrating multi-sensor fusion with probabilistic measurement validation for seamless sensor switching on a UAV\",\"517\":\"Maximum likelihood parameter identification for MAVs\",\"518\":\"Extending kalibr: Calibrating the extrinsics of multiple IMUs and of individual axes\",\"519\":\"Local and closed-loop calibration of an industrial serial robot using a new low-cost 3D measuring device\",\"520\":\"Elasto-geometrical calibration of an industrial robot under multidirectional external loads using a laser tracker\",\"521\":\"Unsupervised calibration of wheeled mobile platforms\",\"522\":\"Instance selection for efficient and reliable camera calibration\",\"523\":\"Geometry based exhaustive line correspondence determination\",\"524\":\"Choosing a time and place for calibration of lidar-camera systems\",\"525\":\"New probabilistic approaches to the AX = XB hand-eye calibration without correspondence\",\"526\":\"Calibration of industry robots with consideration of loading effects using Product-Of-Exponential (POE) and Gaussian Process (GP)\",\"527\":\"Visual tracking of biopsy needles in 2D ultrasound images\",\"528\":\"Magnetic needle guidance for neurosurgery: Initial design and proof of concept\",\"529\":\"Snap-on robotic wrist module for enhanced dexterity in endoscopic surgery\",\"530\":\"Automate surgical tasks for a flexible Serpentine Manipulator via learning actuation space trajectory from demonstration\",\"531\":\"Catadioptric stereo tracking for three dimensional shape measurement of MRI guided catheters\",\"532\":\"Progress toward robotic surgery of the lateral skull base: Integration of a dexterous continuum manipulator and flexible ring curette\",\"533\":\"Compensation for unconstrained catheter shaft motion in cardiac catheters\",\"534\":\"Steering an actuated-tip needle in biological tissue: Fusing FBG-sensor data and ultrasound images\",\"535\":\"Needle steering fusing direct base manipulation and tip-based control\",\"536\":\"On the inseparable nature of sensor selection, sensor placement, and state estimation for continuum robots or \\u201cwhere to put your sensors and how to use them\\u201d\",\"537\":\"Planar odometry from a radial laser scanner. A range flow-based approach\",\"538\":\"Collar Line Segments for fast odometry estimation from Velodyne point clouds\",\"539\":\"Motion-based detection and tracking in 3D LiDAR scans\",\"540\":\"Hierarchical spatial model for 2D range data based room categorization\",\"541\":\"Simultaneous tracking and rendering: Real-time monocular localization for MAVs\",\"542\":\"Noise mask for TDOA sound source localization of speech on mobile robots in noisy environments\",\"543\":\"Real-time high-accuracy 2D localization with structured patterns\",\"544\":\"Probabilistic sensor data processing for robot localization on load-sensing floors\",\"545\":\"Towards a hyperbolic acoustic one-way localization system for underwater swarm robotics\",\"546\":\"RFID-enabled location fingerprinting based on similarity models from probabilistic similarity measures\",\"547\":\"Fast localization and tracking using event sensors\",\"548\":\"A practical optimal control approach for two-speed actuators\",\"549\":\"Slip-aware Model Predictive optimal control for Path following\",\"550\":\"Kinematic control of an Autonomous Underwater Vehicle-Manipulator System (AUVMS) using autoregressive prediction of vehicle motion and Model Predictive Control\",\"551\":\"Model-predictive control with stochastic collision avoidance using Bayesian policy optimization\",\"552\":\"A topology-guided path integral approach for stochastic optimal control\",\"553\":\"Fully autonomous hip exoskeleton saves metabolic cost of walking\",\"554\":\"Hierarchical planning of dynamic movements without scheduled contact sequences\",\"555\":\"Finding optimal isomorphic goal adjacency\",\"556\":\"Mechanics of a scalable high frequency flapping wing robotic platform capable of lift-off\",\"557\":\"The flying monkey: A mesoscale robot that can run, fly, and grasp\",\"558\":\"An integrated jumping-crawling robot using height-adjustable jumping module\",\"559\":\"A continuous jumping robot on water mimicking water striders\",\"560\":\"Object shape recognition using electric sense and ellipsoid's polarization tensor\",\"561\":\"OUROBOT - a self-propelled continuous-track-robot for rugged terrain\",\"562\":\"A structurally flexible humanoid spine based on a tendon-driven elastic continuum\",\"563\":\"Circumnutations as a penetration strategy in a plant-root-inspired robot\",\"564\":\"Distributed sensing and nonlinear MISO models for predicting the propulsive forces of flexible, multi-DOF robotic fins\",\"565\":\"Speed evaluation of a freely swimming robotic fish with an artificial lateral line\",\"566\":\"A tether-less Legged Piezoelectric Miniature Robot using bounding gait locomotion for bidirectional motion\",\"567\":\"Depth control of the biomimetic U-CAT turtle-like AUV with experiments in real operating conditions\",\"568\":\"Absolute pose estimation using multiple forms of correspondences from RGB-D frames\",\"569\":\"Modelling uncertainty in deep learning for camera relocalization\",\"570\":\"Object-aware bundle adjustment for correcting monocular scale drift\",\"571\":\"Localization accuracy estimation with application to perception design\",\"572\":\"Fast and effective online pose estimation and mapping for UAVs\",\"573\":\"Non-uniform sampling strategies for continuous correction based trajectory estimation\",\"574\":\"The line leading the blind: Towards nonvisual localization and mapping for tethered mobile robots\",\"575\":\"Vision-based robot localization across seasons and in remote locations\",\"576\":\"2D visual place recognition for domestic service robots at night\",\"577\":\"Point cloud descriptors for place recognition using sparse visual information\",\"578\":\"Do you see the bakery? Leveraging geo-referenced texts for global localization in public maps\",\"579\":\"Making objects graspable in confined environments through push and pull manipulation with a tool\",\"580\":\"Synergy-based interface for bilateral tele-manipulations of a master-slave system with large asymmetries\",\"581\":\"Denoising auto-encoders for learning of objects and tools affordances in continuous space\",\"582\":\"Flexible, semi-autonomous grasping for assistive robotics\",\"583\":\"Swing-up regrasping algorithm using energy control\",\"584\":\"Haptic feedback for improved robotic arm control during simple grasp, slippage, and contact detection tasks\",\"585\":\"Development of a food handling gripper considering an appetizing presentation\",\"586\":\"Free-flyer acquisition of spinning objects with gecko-inspired adhesives\",\"587\":\"Interactive computational imaging for deformable object analysis\",\"588\":\"A new interaction force decomposition maximizing compensating forces under physical work constraints\",\"589\":\"Stable simulation of underactuated compliant hands\",\"590\":\"SoRo-Track: A two-axis soft robotic platform for solar tracking and building-integrated photovoltaic applications\",\"591\":\"Auxetic metamaterial simplifies soft robot design\",\"592\":\"A geometry deformation model for compound continuum manipulators with external loading\",\"593\":\"A composite soft bending actuation module with integrated curvature sensing\",\"594\":\"Fast, compact, and lightweight shape-shifting system composed of distributed self-folding origami modules\",\"595\":\"Printable skin adhesive stretch sensor for measuring multi-axis human joint angles\",\"596\":\"Motion control of a soft-actuated modular manipulator\",\"597\":\"Improving Soft Pneumatic Actuator fingers through integration of soft sensors, position and force control, and rigid fingernails\",\"598\":\"Cross-modal adaptation for RGB-D detection\",\"599\":\"Learning anisotropic ICP (LA-ICP) for robust and efficient 3D registration\",\"600\":\"3D point cloud segmentation using topological persistence\",\"601\":\"PERCH: Perception via search for multi-object recognition and localization\",\"602\":\"Hierarchical semantic parsing for object pose estimation in densely cluttered scenes\",\"603\":\"Structure-based auto-calibration of RGB-D sensors\",\"604\":\"An integrated approach to visual perception of articulated objects\",\"605\":\"Rolling shutter and motion blur removal for depth cameras\",\"606\":\"Plantation monitoring and yield estimation using autonomous quadcopter for precision agriculture\",\"607\":\"Self-supervised weed detection in vegetable crops using ground based hyperspectral imaging\",\"608\":\"A novel framework for modeling dormant apple trees using single depth image for robotic pruning application\",\"609\":\"Towards autonomous phytopathology: Outcomes and challenges of citrus greening disease detection through close-range remote sensing\",\"610\":\"An effective classification system for separating sugar beets and weeds for precision farming applications\",\"611\":\"Image classification with orchard metadata\",\"612\":\"Texture-based fruit detection via images using the smooth patterns on the fruit\",\"613\":\"Discrete switching commands for tracking and vibration suppression using a quantized, compliant camera orientation system\",\"614\":\"A near-optimal dynamic power sharing scheme for self-reconfigurable modular robots\",\"615\":\"Modular Hydraulic Propulsion: A robot that moves by routing fluid through itself\",\"616\":\"A new meta-module for efficient reconfiguration of hinged-units modular robots\",\"617\":\"A task-driven algorithm for configuration synthesis of modular robots\",\"618\":\"Design of manually reconfigurable modular manipulator with three revolute joints and links\",\"619\":\"Simultaneous configuration formation and information collection by modular robotic systems\",\"620\":\"Full-resolution reconfiguration planning for heterogeneous cube-shaped modular robots with only sliding motion primitive\",\"621\":\"Steering micro-robotic swarm by dynamic actuating fields\",\"622\":\"Robot-assisted optical trapping and manipulation of a biological cell with stochastic perturbations\",\"623\":\"Hybrid control of multi-robot systems using embedded graph grammars\",\"624\":\"Distributed formation control of non-holonomic robots without a global reference frame\",\"625\":\"Coordinated motion for multi-robot systems under time varying communication topologies\",\"626\":\"Distributed trajectory estimation with privacy and communication constraints: A two-stage distributed Gauss-Seidel approach\",\"627\":\"Hybrid architecture for communication-aware multi-robot systems\",\"628\":\"Low latency bounty hunting and geographically adjacent server configuration for real-time cloud control\",\"629\":\"Adaptive forward error correction with adjustable-latency QoS for robotic networks\",\"630\":\"Dynamic perimeter surveillance with a team of robots\",\"631\":\"Environmental field estimation with hybrid-mobility sensor networks\",\"632\":\"Developing robotic swarms for ocean surface mapping\",\"633\":\"Distributed planar manipulation in fluidic environments\",\"634\":\"Classifying swarm behavior via compressive subspace learning\",\"635\":\"Using abstraction for swarm control of a parent system\",\"636\":\"Discrete-time distributed state feedback control for multi-robot systems\",\"637\":\"Distributed multi-robot formation control among obstacles: A geometric and optimization approach with consensus\",\"638\":\"Formalizing the impact of diversity on performance in a heterogeneous swarm of robots\",\"639\":\"Open robotics research using web-based knowledge services\",\"640\":\"Probabilistic qualitative mapping for robots\",\"641\":\"Graph-based Cross Entropy method for solving multi-robot decentralized POMDPs\",\"642\":\"Searching for physical objects in partially known environments\",\"643\":\"Safeguarding a lunar rover with Wald's sequential probability ratio test\",\"644\":\"POMDP-lite for robust robot planning under uncertainty\",\"645\":\"Neural networks and differential dynamic programming for reinforcement learning problems\",\"646\":\"Autonomous drifting using simulation-aided reinforcement learning\",\"647\":\"From human instructions to robot actions: Formulation of goals, affordances and probabilistic planning\",\"648\":\"Implicit belief-space pre-images for hierarchical planning and execution\",\"649\":\"Aerial-ground robotic system for autonomous delivery tasks\",\"650\":\"Autonomously constructing hierarchical task networks for planning and human-robot collaboration\",\"651\":\"Assembly sequence planning for constructing planar structures with rectangular modules\",\"652\":\"Multi-objective planning with multiple high level task specifications\",\"653\":\"Asynchronous multirobot exploration under recurrent connectivity constraints\",\"654\":\"Analyzing the utility of a support pin in sequential robotic manipulation\",\"655\":\"Relational activity processes for modeling concurrent cooperation\",\"656\":\"On multi-modal people tracking from mobile platforms in very crowded and dynamic environments\",\"657\":\"Real-time RGB-D based template matching pedestrian detection\",\"658\":\"GLMP- realtime pedestrian path prediction using global and local movement patterns\",\"659\":\"Hierarchical online domain adaptation of deformable part-based models\",\"660\":\"Walking compass with head-mounted IMU sensor\",\"661\":\"Automatic bone parameter estimation for skeleton tracking in optical motion capture\",\"662\":\"Compact coaxial thermal and color imaging system with silicon-glass hybrid lens\",\"663\":\"Tele-operation system with reliable grasping force estimation to compensate for the time-varying sEMG feature\",\"664\":\"Optic-flow based car-like robot operating in a 5-decade light level range\",\"665\":\"Large-scale model-assisted bundle adjustment using Gaussian max-mixtures\",\"666\":\"Hybrid driving-stepping locomotion with the wheeled-legged robot Momaro\",\"667\":\"Actively articulated suspension for a wheel-on-leg rover operating on a Martian analog surface\",\"668\":\"Recurrent Neural Networks for fast and robust vibration-based ground classification on mobile robots\",\"669\":\"FAD learning: Separate learning for three accelerations -learning for dynamics of boat through motor babbling\",\"670\":\"Skyline-based localisation for aggressively manoeuvring robots using UV sensors and spherical harmonics\",\"671\":\"Fast 6D pose estimation for texture-less objects from a single RGB image\",\"672\":\"Probabilistic traversability map generation using 3D-LIDAR and camera\",\"673\":\"Automated coding of activity videos from an OCD study\",\"674\":\"Tracking multiple rigid symmetric and non-symmetric objects in real-time using depth data\",\"675\":\"Vision-based system for welding groove measurements for robotic welding applications\",\"676\":\"Probabilistic visual verification for robotic assembly manipulation\",\"677\":\"Design and characterization of a debriding tool in robot-assisted treatment of osteolysis\",\"678\":\"Multi-sensor surface analysis for robotic ironing\",\"679\":\"Safe and robust robot maneuvers based on reach control\",\"680\":\"Robust and adaptive whole-body controller for humanoids with multiple tasks under uncertain disturbances\",\"681\":\"Adaptive control for robot navigation in human environments based on social force model\",\"682\":\"A study on the L1 optimal PD controller with application to joint motion control of a robot manipulator\",\"683\":\"Multiple-hypothesis chance-constrained target tracking under identity uncertainty\",\"684\":\"Augmented \\u22121 adaptive control of an actuated knee joint exoskeleton: From design to real-time experiments\",\"685\":\"Adaptive-Robust Control of uncertain Euler-Lagrange systems with past data: A time-delayed approach\",\"686\":\"Place categorization and semantic mapping on a mobile robot\",\"687\":\"SceneNet: An annotated model generator for indoor scene understanding\",\"688\":\"Learning to generalize 3D spatial relationships\",\"689\":\"Monocular reconstruction of vehicles: Combining SLAM with shape priors\",\"690\":\"Towards visual mapping in industrial environments - a heterogeneous task-specific and saliency driven approach\",\"691\":\"Scaling perception towards autonomous object manipulation \\u2014 in knowledge lies the power\",\"692\":\"Find my office: Navigating real space from semantic descriptions\"},\"First and Last Author Affiliations\":{\"0\":null,\"1\":null,\"2\":null,\"3\":null,\"4\":null,\"5\":null,\"6\":null,\"7\":null,\"8\":null,\"9\":null,\"10\":null,\"11\":null,\"12\":null,\"13\":null,\"14\":null,\"15\":null,\"16\":null,\"17\":null,\"18\":null,\"19\":null,\"20\":null,\"21\":null,\"22\":null,\"23\":null,\"24\":null,\"25\":null,\"26\":null,\"27\":null,\"28\":null,\"29\":null,\"30\":null,\"31\":null,\"32\":null,\"33\":null,\"34\":null,\"35\":null,\"36\":null,\"37\":null,\"38\":null,\"39\":null,\"40\":null,\"41\":null,\"42\":null,\"43\":null,\"44\":null,\"45\":null,\"46\":null,\"47\":null,\"48\":null,\"49\":null,\"50\":null,\"51\":null,\"52\":null,\"53\":null,\"54\":null,\"55\":null,\"56\":null,\"57\":null,\"58\":null,\"59\":null,\"60\":null,\"61\":null,\"62\":null,\"63\":null,\"64\":null,\"65\":null,\"66\":null,\"67\":null,\"68\":null,\"69\":null,\"70\":null,\"71\":null,\"72\":null,\"73\":null,\"74\":null,\"75\":null,\"76\":null,\"77\":null,\"78\":null,\"79\":null,\"80\":null,\"81\":null,\"82\":null,\"83\":null,\"84\":null,\"85\":null,\"86\":null,\"87\":null,\"88\":null,\"89\":null,\"90\":null,\"91\":null,\"92\":null,\"93\":null,\"94\":null,\"95\":null,\"96\":null,\"97\":null,\"98\":null,\"99\":null,\"100\":null,\"101\":null,\"102\":null,\"103\":null,\"104\":null,\"105\":null,\"106\":null,\"107\":null,\"108\":null,\"109\":null,\"110\":null,\"111\":null,\"112\":null,\"113\":null,\"114\":null,\"115\":null,\"116\":null,\"117\":null,\"118\":null,\"119\":null,\"120\":null,\"121\":null,\"122\":null,\"123\":null,\"124\":null,\"125\":null,\"126\":null,\"127\":null,\"128\":null,\"129\":null,\"130\":null,\"131\":null,\"132\":null,\"133\":null,\"134\":null,\"135\":null,\"136\":null,\"137\":null,\"138\":null,\"139\":null,\"140\":null,\"141\":null,\"142\":null,\"143\":null,\"144\":null,\"145\":null,\"146\":null,\"147\":null,\"148\":null,\"149\":null,\"150\":null,\"151\":null,\"152\":null,\"153\":null,\"154\":null,\"155\":null,\"156\":null,\"157\":null,\"158\":null,\"159\":null,\"160\":null,\"161\":null,\"162\":null,\"163\":null,\"164\":null,\"165\":null,\"166\":null,\"167\":null,\"168\":null,\"169\":null,\"170\":null,\"171\":null,\"172\":null,\"173\":null,\"174\":null,\"175\":null,\"176\":null,\"177\":null,\"178\":null,\"179\":null,\"180\":null,\"181\":null,\"182\":null,\"183\":null,\"184\":null,\"185\":null,\"186\":null,\"187\":null,\"188\":null,\"189\":null,\"190\":null,\"191\":null,\"192\":null,\"193\":null,\"194\":null,\"195\":null,\"196\":null,\"197\":null,\"198\":null,\"199\":null,\"200\":null,\"201\":null,\"202\":null,\"203\":null,\"204\":null,\"205\":null,\"206\":null,\"207\":null,\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":null,\"213\":null,\"214\":null,\"215\":null,\"216\":null,\"217\":null,\"218\":null,\"219\":null,\"220\":null,\"221\":null,\"222\":null,\"223\":null,\"224\":null,\"225\":null,\"226\":null,\"227\":null,\"228\":null,\"229\":null,\"230\":null,\"231\":null,\"232\":null,\"233\":null,\"234\":null,\"235\":null,\"236\":null,\"237\":null,\"238\":null,\"239\":null,\"240\":null,\"241\":null,\"242\":null,\"243\":null,\"244\":null,\"245\":null,\"246\":null,\"247\":null,\"248\":null,\"249\":null,\"250\":null,\"251\":null,\"252\":null,\"253\":null,\"254\":null,\"255\":null,\"256\":null,\"257\":null,\"258\":null,\"259\":null,\"260\":null,\"261\":null,\"262\":null,\"263\":null,\"264\":null,\"265\":null,\"266\":null,\"267\":null,\"268\":null,\"269\":null,\"270\":null,\"271\":null,\"272\":null,\"273\":null,\"274\":null,\"275\":null,\"276\":null,\"277\":null,\"278\":null,\"279\":null,\"280\":null,\"281\":null,\"282\":null,\"283\":null,\"284\":null,\"285\":null,\"286\":null,\"287\":null,\"288\":null,\"289\":null,\"290\":null,\"291\":null,\"292\":null,\"293\":null,\"294\":null,\"295\":null,\"296\":null,\"297\":null,\"298\":null,\"299\":null,\"300\":null,\"301\":null,\"302\":null,\"303\":null,\"304\":null,\"305\":null,\"306\":null,\"307\":null,\"308\":null,\"309\":null,\"310\":null,\"311\":null,\"312\":null,\"313\":null,\"314\":null,\"315\":null,\"316\":null,\"317\":null,\"318\":null,\"319\":null,\"320\":null,\"321\":null,\"322\":null,\"323\":null,\"324\":null,\"325\":null,\"326\":null,\"327\":null,\"328\":null,\"329\":null,\"330\":null,\"331\":null,\"332\":null,\"333\":null,\"334\":null,\"335\":null,\"336\":null,\"337\":null,\"338\":null,\"339\":null,\"340\":null,\"341\":null,\"342\":null,\"343\":null,\"344\":null,\"345\":null,\"346\":null,\"347\":null,\"348\":null,\"349\":null,\"350\":null,\"351\":null,\"352\":null,\"353\":null,\"354\":null,\"355\":null,\"356\":null,\"357\":null,\"358\":null,\"359\":null,\"360\":null,\"361\":null,\"362\":null,\"363\":null,\"364\":null,\"365\":null,\"366\":null,\"367\":null,\"368\":null,\"369\":null,\"370\":null,\"371\":null,\"372\":null,\"373\":null,\"374\":null,\"375\":null,\"376\":null,\"377\":null,\"378\":null,\"379\":null,\"380\":null,\"381\":null,\"382\":null,\"383\":null,\"384\":null,\"385\":null,\"386\":null,\"387\":null,\"388\":null,\"389\":null,\"390\":null,\"391\":null,\"392\":null,\"393\":null,\"394\":null,\"395\":null,\"396\":null,\"397\":null,\"398\":null,\"399\":null,\"400\":null,\"401\":null,\"402\":null,\"403\":null,\"404\":null,\"405\":null,\"406\":null,\"407\":null,\"408\":null,\"409\":null,\"410\":null,\"411\":null,\"412\":null,\"413\":null,\"414\":null,\"415\":null,\"416\":null,\"417\":null,\"418\":null,\"419\":null,\"420\":null,\"421\":null,\"422\":null,\"423\":null,\"424\":null,\"425\":null,\"426\":null,\"427\":null,\"428\":null,\"429\":null,\"430\":null,\"431\":null,\"432\":null,\"433\":null,\"434\":null,\"435\":null,\"436\":null,\"437\":null,\"438\":null,\"439\":null,\"440\":null,\"441\":null,\"442\":null,\"443\":null,\"444\":null,\"445\":null,\"446\":null,\"447\":null,\"448\":null,\"449\":null,\"450\":null,\"451\":null,\"452\":null,\"453\":null,\"454\":null,\"455\":null,\"456\":null,\"457\":null,\"458\":null,\"459\":null,\"460\":null,\"461\":null,\"462\":null,\"463\":null,\"464\":null,\"465\":null,\"466\":null,\"467\":null,\"468\":null,\"469\":null,\"470\":null,\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":null,\"476\":null,\"477\":null,\"478\":null,\"479\":null,\"480\":null,\"481\":null,\"482\":null,\"483\":null,\"484\":null,\"485\":null,\"486\":null,\"487\":null,\"488\":null,\"489\":null,\"490\":null,\"491\":null,\"492\":null,\"493\":null,\"494\":null,\"495\":null,\"496\":null,\"497\":null,\"498\":null,\"499\":null,\"500\":null,\"501\":null,\"502\":null,\"503\":null,\"504\":null,\"505\":null,\"506\":null,\"507\":null,\"508\":null,\"509\":null,\"510\":null,\"511\":null,\"512\":null,\"513\":null,\"514\":null,\"515\":null,\"516\":null,\"517\":null,\"518\":null,\"519\":null,\"520\":null,\"521\":null,\"522\":null,\"523\":null,\"524\":null,\"525\":null,\"526\":null,\"527\":null,\"528\":null,\"529\":null,\"530\":null,\"531\":null,\"532\":null,\"533\":null,\"534\":null,\"535\":null,\"536\":null,\"537\":null,\"538\":null,\"539\":null,\"540\":null,\"541\":null,\"542\":null,\"543\":null,\"544\":null,\"545\":null,\"546\":null,\"547\":null,\"548\":null,\"549\":null,\"550\":null,\"551\":null,\"552\":null,\"553\":null,\"554\":null,\"555\":null,\"556\":null,\"557\":null,\"558\":null,\"559\":null,\"560\":null,\"561\":null,\"562\":null,\"563\":null,\"564\":null,\"565\":null,\"566\":null,\"567\":null,\"568\":null,\"569\":null,\"570\":null,\"571\":null,\"572\":null,\"573\":null,\"574\":null,\"575\":null,\"576\":null,\"577\":null,\"578\":null,\"579\":null,\"580\":null,\"581\":null,\"582\":null,\"583\":null,\"584\":null,\"585\":null,\"586\":null,\"587\":null,\"588\":null,\"589\":null,\"590\":null,\"591\":null,\"592\":null,\"593\":null,\"594\":null,\"595\":null,\"596\":null,\"597\":null,\"598\":null,\"599\":null,\"600\":null,\"601\":null,\"602\":null,\"603\":null,\"604\":null,\"605\":null,\"606\":null,\"607\":null,\"608\":null,\"609\":null,\"610\":null,\"611\":null,\"612\":null,\"613\":null,\"614\":null,\"615\":null,\"616\":null,\"617\":null,\"618\":null,\"619\":null,\"620\":null,\"621\":null,\"622\":null,\"623\":null,\"624\":null,\"625\":null,\"626\":null,\"627\":null,\"628\":null,\"629\":null,\"630\":null,\"631\":null,\"632\":null,\"633\":null,\"634\":null,\"635\":null,\"636\":null,\"637\":null,\"638\":null,\"639\":null,\"640\":null,\"641\":null,\"642\":null,\"643\":null,\"644\":null,\"645\":null,\"646\":null,\"647\":null,\"648\":null,\"649\":null,\"650\":null,\"651\":null,\"652\":null,\"653\":null,\"654\":null,\"655\":null,\"656\":null,\"657\":null,\"658\":null,\"659\":null,\"660\":null,\"661\":null,\"662\":null,\"663\":null,\"664\":null,\"665\":null,\"666\":null,\"667\":null,\"668\":null,\"669\":null,\"670\":null,\"671\":null,\"672\":null,\"673\":null,\"674\":null,\"675\":null,\"676\":null,\"677\":null,\"678\":null,\"679\":null,\"680\":null,\"681\":null,\"682\":null,\"683\":null,\"684\":null,\"685\":null,\"686\":null,\"687\":null,\"688\":null,\"689\":null,\"690\":null,\"691\":null,\"692\":null},\"Keywords or Approach\":{\"0\":\"collision avoidance\\nnavigation\\nrobot sensing systems\\ngenerators\\nkinematics\\nstandards\\ncomputational geometry\\nconvex programming\\nrobot dynamics\\nrobot navigation\\npower diagrams\\nreactive navigation problem\\nvector field\\ncompact-convex euclidean subset\\neuclidean disk robot\\nzero-measure set\\npoint destination\\ngeneralized voronoi diagrams\\nadditive weights\\nrobot collision free convex neighborhood\\nconvex optimization problem\\nnonholonomically constrained kinematics\\nstandard differential drive vehicle\",\"1\":\"gaussian processes\\nplanning\\nrobots\\ninterpolation\\ntrajectory optimization\\ndifferential equations\\nlinear systems\\nmanipulators\\noptimisation\\npath planning\\ntime-varying systems\\ntrajectory control\\n7-dof wam arm\\n7-dof robotic arm planning problems\\ngpmp\\ngaussian process motion planner\\ngp interpolation\\ncontinuous-time trajectories\\ntime-varying-based optimization technique\\nlinear time-varying stochastic differential equation\\ncontinuous-time trajectory\\ndiscrete state parameterization\\ntrajectory optimization algorithms\\nrobotics\\nfundamental tool\\ngaussian process motion planning\",\"2\":\"trajectory\\nrobots\\nclustering algorithms\\nglobal positioning system\\ntopology\\nprobabilistic logic\\nmeasurement\\npattern clustering\\nprobability\\ntopological trajectory clustering\\nrelative persistent homology\\nglobal constraints\\nprobabilistic motion models\\nship gps trajectories\\npedestrian trajectories\",\"3\":\"trajectory\\nwindings\\nplanning\\nrobots\\napproximation algorithms\\ncouplings\\ncomputer science\\npath planning\\nhigh-dimensional winding-augmented motion planning\\n2d topological task projections\\nhomotopy inequivalent trajectories\\nrobot configuration space\\nskeletonization\\ncollision space\\nfree space\\nttp\\n2-dimensional spaces\\nsimplicial complex filtrations\\npersistent homology\\nwinding augmented rrt\\nwa-rrt-rrt* algorithms\",\"4\":\"planning\\nheuristic algorithms\\nresource management\\nhumanoid robots\\nprobabilistic logic\\nmonitoring\\nmobile robots\\npath planning\\nquality of service\\nresource allocation\\nresource-aware motion planning\\nglobal system level resource allocation\\nconcurrent algorithms\\nself-monitoring concepts\\nplanning progress\\nalgorithmic level resource allocation dynamic adaptation\\nstatic resource allocation\\nquality of service measures\\nqos measures\\nhumanoid robot armar-4\",\"5\":\"planning\\ntrajectory\\nuncertainty\\nstochastic systems\\ncomputational modeling\\nmarkov processes\\naerospace robotics\\napproximation theory\\nmicrorobots\\nmobile robots\\npath planning\\nuncertain systems\\nmdp-based approximation method\\ngoal constrained multi-mav planning\\naction uncertainty\\nmarkov decision process\\nhomogeneous micro air vehicles\\nstochastic system\",\"6\":\"collision avoidance\\nclustering algorithms\\nplanning\\nmanipulators\\nprobabilistic logic\\nheuristic algorithms\\nmachine learning algorithms\\nexpectation-maximisation algorithm\\ngaussian distribution\\nmixture models\\npattern clustering\\nrandom processes\\nsampling methods\\ntrees (mathematics)\\nbiased random sampling\\nkinematic-based collision detection routine\\nincremental expectation maximization clustering algorithm\\ngmm\\ngaussian mixture models\\nrrt motion planning\\nrapidly-exploring random trees motion planning\\nhigh dimensional configuration spaces\\nfast collision detection\\nhigh-dimensional mixture models\",\"7\":\"collision avoidance\\nrobot kinematics\\npath planning\\nmanipulators\\nplanning\\nmeasurement\\ntrees (mathematics)\\nfree c-space\\nrobotic manipulator\\nbubble\\ndistance information\\nrapidly exploring bur tree\\nspace-filling tree\",\"8\":\"manipulators\\ncollision avoidance\\nplanning\\njoining processes\\nkinematics\\ndiamond\\npath planning\\npath planning algorithm\\nrobotic manipulators\\nexpanded bubbles\\nc-space\\ncollision-free paths\\nrrt paradigm\\nmodified extend procedure\",\"9\":\"hypercubes\\ncomplexity theory\\npartitioning algorithms\\nprobabilistic logic\\ndata structures\\nrobots\\nplanning\\ncomputational complexity\\npath planning\\nsampling methods\\ntrees (mathematics)\\nreduced complexity multiscale path-planning\\nprobabilistic maps\\nmspp algorithm\\ntree structure\\nrecursive dyadic search space partitioning\\ngraph neighbors\\no(|v|2) complexity\\no(|v| log |v|) complexity\\nfailure probability\",\"10\":\"trajectory\\nplanning\\nstandards\\nvisualization\\nrobots\\ncost function\\nautomobiles\\nrandom processes\\nrobot dynamics\\nsampling methods\\nset theory\\nstate-space methods\\ntrees (mathematics)\\nrapidly exploring random tree\\nkinodynamic planning\\nrrt* planner\\nsteering method\\nhrs\\ninformed subset\\nstate space subset\\ncollision checking\\nhigh-dimensional domains\\nrejecting samples\\npruning nodes\\ndifferential constraints\\nhigh-dimensional problems\\nasymptotically optimal sampling-based planner\\nhierarchical rejection sampling\",\"11\":\"robot sensing systems\\nelectrodes\\ndielectrics\\nsensitivity\\ncapacitance\\ncapacitive sensors\\ndielectric materials\\nelectric impedance\\ninductive sensors\\nprinted circuits\\nrobots\\ntactile sensors\\nhighly sensitive dual mode tactile sensor\\nproximity sensor\\ncarbon microcoils\\ncmc\\nrobotic applications\\nmultiple electrode layers\\nflexible printed circuit board\\nfpcb\\ndielectric substrate\\ndielectric layer\\ncapacitive sensing mode\\ninductive sensing mode\\nelectrical impedance\\nsensor signal\\nprototype sensor\",\"12\":\"force\\nestimation\\nmagnetic levitation\\nforce measurement\\nforce sensors\\nsprings\\natomic force microscopy\\nelasticity\\nmicrorobots\\nsprings (mechanical)\\nnanoforce sensing\\ndifferential approach\\nexternal mechanical disturbances\\npassive magnetic springs\\nmacroscopic seismic mass\\nelastic microstructures\\natomic force microscopes\\nnoisy measurement\\nmass displacement\\nunder-damped dynamic\\nnanonewton level\",\"13\":\"sensor arrays\\nforce\\nrobot sensing systems\\npressure sensors\\nneural networks\\nforce sensors\\nleast squares approximations\\nneurocontrollers\\nregression analysis\\nrobots\\ntactile sensors\\nshear tactile force sensor performance\\nleast squares artificial neural network\\nlsann\\ntactile array sensors\\nshear forces\\nsensor characterization\\nlinear regression portion\\nmultivariate least squares regression\\nnonlinear regression portion\\nmit cheetah footpad\\nnormalized root mean squared error\\ntactile robotics\",\"14\":\"tactile sensors\\npins\\nbayes methods\\nreal-time systems\\nbiomembranes\\nbiomimetics\\nend effectors\\ntrajectory control\\ncomplex trajectory\\nrobot arm\\nend effector\\ntactile fingertip\\nbiomimetic tactile sensor\\nbiomimetic active touch\\ntactile manipulation\",\"15\":\"uncertainty\\nsolid modeling\\nkinematics\\nrobot sensing systems\\nrobot kinematics\\ntorque\\ncollision avoidance\\nfeedback\\nindustrial manipulators\\nmonte carlo methods\\nobservers\\nrobotic assembly\\nsampling methods\\ntorque control\\nnarrow passage sampling\\nrobotic assembly task observation\\nsequential monte carlo observation algorithm\\ncollision detection algorithm\\ncomplex shaped parts\\nclassic random motion model\\nprobabilistic roadmap planning\\nconfiguration space\\nsample impoverishment\\npeg-in-hole task\\nlightweight-robot arm\\njoint torque sensor\",\"16\":\"cameras\\nrobot kinematics\\nspatial resolution\\nrobot vision systems\\ndexterous manipulators\\nimage motion analysis\\nimage resolution\\nstereo image processing\\ntactile sensors\\nrobotic grasp control\\ntactile sensing\\nproximity sensing\\nmultimodal sensing\\noptical device\\nhigh spatial resolution\\nrobotic hand\\ninfrared image\\nlight conductive plate\\nstereo matching\\ncompound-eye camera\\n6 dof robotic arm\\nrobotic motion\\nadaptive grasping motion\",\"17\":\"robot sensing systems\\ntorque\\nestimation\\ntraining\\nforce\\ntraining data\\nforce control\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nsensors\\nstatistical analysis\\ntorque control\\nexperience-based torque estimation\\nindustrial robot\\nrobotic manipulation tasks\\nmachine learning approach\\nforce estimation\\ninternal sensor data\\ntransfer entropy\\nstatistical model\\nrobot kinematics\\nmass distribution\\nsensor instrumentation\",\"18\":\"compressed sensing\\ndata acquisition\\nskin\\ntactile sensors\\nhardware\\ncontrol engineering computing\\ndata compression\\nhuman-robot interaction\\nsignal detection\\ntactile skins\\nwhole body tactile perception\\nrobots\\ntactile sensor elements\\nscalable tactile data acquisition\\ndata sampling\\ndata reconstruction\\nwiring complexity reduction\\nsimulated tactile sensor networks\\nsignal acquisition\",\"19\":\"grasping\\ntactile sensors\\nindexes\\nsupport vector machines\\ncontrol engineering computing\\ncontrol system synthesis\\ndexterous manipulators\\ngrippers\\nlearning (artificial intelligence)\\nsignal processing\\ntactile sensor system\\nobject grasping\\nrobotic manipulation\\ncontroller design\\nmachine learning method\\nthree-fingered hand\",\"20\":\"force\\ntactile sensors\\nuncertainty\\npredictive models\\ncomputational modeling\\ntraining data\\nfriction\\ndexterous manipulators\\nhaptic interfaces\\ninference mechanisms\\npattern classification\\nanalytic grasp success prediction\\ntactile feedback\\nrobotic applications\\nwrench space\\ntactile information\\ncontact placement uncertainties\\ncontact modeling\\nwrench-based classifier\\nwrench-based reasoning\",\"21\":\"electrodes\\nyarn\\nbiomedical monitoring\\nskin\\nmuscles\\nfabrics\\nsensors\\nbiomedical electrodes\\nelectromyography\\ndigital embroidery\\nvariable force isometric grip exercise\\nthread-based electrodes\\ngel-based electrodes\\nsemg measurements\\ntextile-based surface emg\\nrobot interfaces\\ninjury prevention\\ngait analysis\\nmuscle activity monitoring\\nwearable surface electromyography\\nembroidered electrodes\",\"22\":\"collision avoidance\\nmanipulator dynamics\\nrobot sensing systems\\nvehicle dynamics\\ndynamics\\nfault diagnosis\\nmobile robots\\nrobot dynamics\\nsignal classification\\ntracked vehicles\\nwavelet transforms\\nterrain contact modeling\\nterrain contact classification\\natvs\\ncontact event estimation\\nsensor-free active subtracks\\nterrain surface\\nunexpected collision dynamics\\nmoving base link\\nflipper residual dynamics\\ngeneralized momenta fault detection and isolation method\\nfdi method\\nresidual signal\\ndisturbance patterns\\nwavelet packet transform\\nfeature selection\\nsparse svm\",\"23\":\"robot sensing systems\\ngrippers\\ngrasping\\ntraining\\nstability analysis\\nvisualization\\nestimation theory\\nprobability\\nstability\\nprobabilistic model\\nrobotic grasping\\nsensory modality\\naction parameter\\ngrasp stability estimation\\ngrasp correction\\ntactile imprint prediction\\nobject-relative gripper pose\",\"24\":\"shape\\ntrajectory\\nrobots\\nacceleration\\nlearning (artificial intelligence)\\nmodulation\\ncomputer science\\nlearning systems\\nmanipulators\\nvelocity control\\nmovement primitives\\nmultiple phase parameters\\nconcise movement representations\\nspeed modulation mechanisms\\ngeneral velocity constraints\\n4-dof robot arm\\nminigolf setup\\nlearning from human demonstration\",\"25\":\"trajectory\\nnavigation\\nmobile robots\\nlearning (artificial intelligence)\\nservice robots\\nmarkov processes\\ncontrol engineering computing\\nmotion control\\noptimisation\\npath planning\\nrobot programming\\ntrajectory control\\noptimal navigation action\\nreinforcement learning\\nrobot behavior\\nassistance task\\nservice robot\\nq-function\\nhuman motion prediction\\nrobot trajectory\\nmarkov decision process\\nmdp\\nmobile robot\",\"26\":\"planning\\nlibraries\\nprediction algorithms\\ntraining\\ntrajectory optimization\\nconvex functions\\nlearning (artificial intelligence)\\noptimisation\\npath planning\\nmachine learning\\nsearch based planning\\nadaptive motion planning systems\\nlist prediction\\nconseqopt\",\"27\":\"optimization\\ntuning\\nmanuals\\nhumanoid robots\\nsequential analysis\\ntorque\\nlearning (artificial intelligence)\\noptimisation\\nredundant manipulators\\nsoft task priority learning\\nredundant robot control problem\\nredundant robot planning problems\\nmultitask prioritized approaches\\nweight function\\nmachine learning techniques\\nstochastic optimization procedure\\n7 dof kuka lwr\\nkinova jaco arm\",\"28\":\"robots\\nlearning systems\\nstandards\\nsemantics\\ntraining data\\nthree-dimensional displays\\ntraining\\ncomputer graphics\\ndecision trees\\nimage classification\\nlearning (artificial intelligence)\\nmobile robots\\nobject recognition\\nstream-based active learning\\n3d objects adaptive classification\\n3d point cloud data\\nstandard online learning methods\\nmondrian forests\\nmf\\nonline learning algorithm\\ndata order\\nrandom forests\\nrobot learning applications\\ndata streams\\nkitti benchmark\",\"29\":\"trajectory\\nhidden markov models\\ncontext\\nfeature extraction\\nprobabilistic logic\\nprogramming\\noptimal control\\nautomatic programming\\nmotion control\\nrobot programming\\ntrajectory control\\ndemonstrated motion\\nadaptive motion generation\\ninvariant rigid body trajectory representation\\nprogramming by demonstration\\ndynamical system\\nconstrained optimal control problem\\ncontext-specific information\",\"30\":\"robot sensing systems\\ncomputational modeling\\nadaptation models\\ntrajectory optimization\\nneural networks\\nhumanoid robots\\nlearning systems\\nlegged locomotion\\nneurocontrollers\\noptimal control\\ntrajectory control\\nmodel-based policy search\\nonline model learning\\nphysical humanoid robot interactive control\\nmodel-free controller\\nlow-level controller\\nadaptive system\\noptimal controls\\nneural network learning\\non-board sensors\",\"31\":\"synchronization\\npetri nets\\nconcurrent computing\\nrobot sensing systems\\nmathematical model\\nautomation\\nautomatic programming\\nlearning (artificial intelligence)\\nmulti-robot systems\\nlearning movement synchronization\\nmulticomponent robotic systems\\nimitation learning\\nprogramming by demonstration\",\"32\":\"exoskeletons\\ntrajectory\\nadaptation models\\ndynamics\\nrobot sensing systems\\nlearning (artificial intelligence)\\nsensitivity\\nhuman-robot interaction\\nintelligent robots\\nregression analysis\\nrobot dynamics\\nhierarchical interactive learning\\nlearning-by-demonstration method\\nhuman-coupled robot control\\nmotion trajectories\\nphysical human-robot interaction\\nhil strategy\\nexoskeleton sensory system\\nlearning hierarchies\\nhigh-level motion learning\\nlow-level controller learning\\ndynamic movement primitives\\ndmp\\nlocally-weighted regression\\nlwr\\nreinforcement learning\\nrl\\nmodel-based controller learning\\nsingle-degree-of-freedom platform\\nsingle-dof platform\\nhuman-powered augmentation lower-exoskeleton system\\nhualex system\\ninteraction force\\ninteractive learning\\ndynamic movement primitive (dmp)\\nlower exoskeleton\\nhuman-powered augmentation lower exoskeleton (hualex)\",\"33\":\"gaussian processes\\nadaptation models\\ndata models\\ncomputational modeling\\nground penetrating radar\\nkernel\\ntrajectory\\nlearning (artificial intelligence)\\nregression analysis\\ndrifting gaussian processes\\nvarying neighborhood sizes\\nonline model learning\\nquery points\\ninverse dynamics learning tasks\",\"34\":\"tuning\\ncomputational modeling\\nrobots\\nbayes methods\\ncost function\\ngaussian processes\\nentropy\\nlinear quadratic control\\noptimisation\\nsearch problems\\nautomatic lqr tuning\\ngaussian process\\nglobal optimization\\nautomatic controller tuning\\nlinear optimal control\\nbayesian optimization\\ncontroller gains\\nentropy search\\ninformation gain\\nseven-degree-of-freedom robot arm\\ninverted pole\\ntwo-dimensional tuning problems\\nfour-dimensional tuning problems\\nrobotic platforms\\nlinear quadratic regulator\",\"35\":\"clouds\\natmospheric modeling\\ntrajectory\\ncovariance matrices\\natmospheric measurements\\nkernel\\noptimization\\nautonomous aerial vehicles\\ngaussian processes\\noptimisation\\nsampling methods\\nsteering systems\\ntrajectory control\\nuav\\ncloud evolution\\natmospheric variables\\nlow-altitude cumulus clouds\\nunmanned aerial vehicles\\non-line maps\\nsparse local measurements\\nplanning algorithm\\nadaptive data sampling process\\nenergetic-efficient flights\\nvehicle steering\\ngp\\nstochastic optimization scheme\\ntrajectories generation\\nthree-dimensional current field\",\"36\":\"planning\\nrobot sensing systems\\ncollaboration\\ntrajectory\\nlattices\\nautonomous aerial vehicles\\nvisual features\\nugv-uav team operating indoors\\nhigh-quality localization information\\npad planner\\nplanning adaptive dimensionality\\nslc planner\\ncontroller-based motion primitives\\nstate lattice planner\\npayload capacity\\nunmanned aerial vehicles\\nunmanned ground vehicles\\nrobust navigation capabilities\\ncollaborative localization\\nground-air robotic system\",\"37\":\"navigation\\ntrajectory\\nheuristic algorithms\\nshape\\nplanning\\ndynamics\\nreal-time systems\\ncollision avoidance\\ndecentralised control\\nmulti-agent systems\\nproxemic group behaviors\\nreciprocal multiagent navigation\\ndecentralized algorithm\\ngroup-based coherent multiagent navigation\\ncollision-free trajectories\\nmacroscopic group movements\",\"38\":\"collision avoidance\\nrobots\\nnavigation\\napproximation algorithms\\nheuristic algorithms\\nreal-time systems\\nlinear approximation\\napproximation theory\\nlinear programming\\nmulti-robot systems\\ntrajectory control\\nreal-time reciprocal collision avoidance\\nreal-time collision-free navigation\\nelliptical agents\\ntight-fitting 2d ellipse\\nreciprocal velocity obstacle formulation\\nconservative linear approximations\\nsufficient conditions\\ncollision-free motion\\nlow-dimensional linear programming\\nminkowski sum approximations\\nreal-time conservative collision avoidance\\nmulti-agent environments\\ncollision-free trajectory\\ncircular agents\",\"39\":\"mobile robots\\nrobot kinematics\\ntrajectory\\ntracking\\nplanning\\nnavigation\\npath planning\\nrobot dynamics\\nflexible formations\\nnonholonomic mobile robots\\nmotion planning\\ncontrol perspectives\\ncurvilinear longitudinal separations\\nlateral relative separations\\nformation maneuverability\\nkinodynamical motion constraints\\nformation point\",\"40\":\"optical imaging\\nplanning\\nbiomedical optical imaging\\nlaser beams\\ncharge carrier processes\\nnonlinear optics\\npredictive control\\ncellular biophysics\\nmanipulators\\ncell manipulation\\noptical tweezers\\nintegrated optimal planning\\nintegrated control technique\\noptical manipulation\\nplanning layer\\nmodel predictive control\\nmotorized stage\\ncontrol allocation technique\",\"41\":\"ellipsoids\\nrobustness\\nrobots\\npath planning\\ncollision avoidance\\naerospace electronics\\nprediction algorithms\\naircraft control\\ncomputational geometry\\nhelicopters\\nmulti-robot systems\\nposition control\\npredictive control\\nreachability analysis\\nrobust control\\ncooperative multiquadrotor pursuit\\nevader pursuit\\nno-fly zones\\nevader positions\\nvoronoi-based coverage control\\nevader reachable set\\nrobust model predictive control\\nrmpc tools\",\"42\":\"measurement\\nrobots\\nmarkov processes\\ntransient analysis\\nreactive power\\nrandom variables\\nhistory\\nstatistical distributions\\nrisk aversion\\nfinite markov decision processes\\ntotal cost criteria\\nrisk averse policies\\ntotal cost criterion\\naverage value at risk metric\\navar metric\\nmdp algorithms\\nrobot\\ntarget location\\ntemporal deadline\\nfailure probability\\nrisk averse policy\\nstatistical distribution\\nvaluable analysis tool\",\"43\":\"robot sensing systems\\nmulti-robot systems\\nrobot kinematics\\nfault tolerance\\nfault tolerant systems\\napproximation theory\\ntopology\\nfaulty robots\\ncomputational framework\\napproximate algorithm\\ncomputationally intractable algorithm\\ndirected network topology\\nsynchronous multirobot systems\\ncontrollable sensing range\\nmultirobot systems\\nfault-tolerant rendezvous\",\"44\":\"robots\\nmathematical model\\nfrequency response\\ntransfer functions\\nmonitoring\\nforce\\ncomplex networks\\ndifferential equations\\nparameter estimation\\nrandom processes\\nrobot dynamics\\nsystem dynamics\\nfractional-order models\\nparameter variation identification\\ninteger-order models\\nrandom scale-free agent networks\\nlarge-scale robotic formation systems\\ncooperating robot system health monitoring\\nfractional-order differential equations\",\"45\":\"friction\\nload modeling\\nrobots\\ncomputational modeling\\nellipsoids\\nforce measurement\\nfacsimile\\nconvex programming\\nforce control\\nmotion control\\npolynomials\\nconvex polynomial force-motion model\\nplanar sliding\\ngeneralized friction loads\\npolynomial sublevel set\\nconvex even-degree homogeneous polynomial\\nmodel identification procedure\\nsum-of-squares convex relaxation\\nobject pushing\\nfree sliding dynamic simulation\",\"46\":\"adaptation models\\ntrajectory\\nrobots\\ngravity\\nheuristic algorithms\\ncost function\\nadaptive control\\ndexterous manipulators\\nlearning (artificial intelligence)\\nlinear systems\\nmanipulator dynamics\\noptimal control\\ntime-varying systems\\nlearned local models\\ndexterous manipulation\\npneumatically-actuated tendon-driven 24-dof hand\\ntime-varying linear models\\ntrajectory optimization\\nmodel-based reinforcement learning\\nadaptive optimal control\\nintermittent contact dynamics\",\"47\":\"thumb\\nsprings\\nindexes\\nrobots\\niron\\ntendons\\ndexterous manipulators\\ngrippers\\nmanipulator kinematics\\nsprings (mechanical)\\naverage human workspace\\nantagonist actuators\\npalmar-dorsal axis\\nrobotic fingers\\nspring energy\\nmoment arms\\njoint axis alignment\\nlink lengths\\nflexor tendons\\nabduction-adduction tendon\\nrobot hand design\\nhuman manipulation kinematics\\nhuman precision manipulation motions\\ntwo-fingered underactuated anthropomorphic manipulator\",\"48\":\"trajectory\\nforce\\ngrasping\\nadhesives\\nfriction\\nnavigation\\nmanipulators\\ndexterous manipulators\\nmicromanipulators\\ntrajectory control\\nfinger trajectory generation\\nplanar dexterous micromanipulation\\ndexterous microhandling solutions\\namplitude rotation\\ndexterous microhands\\nadhesion force\\nautomated repositioning\\nmicromanipulation enhancement\",\"49\":\"friction\\nforce\\ngrippers\\nrobot kinematics\\nplanning\\nrobot sensing systems\\nadaptive control\\ndexterous manipulators\\nforce control\\nforce feedback\\ntrajectory control\\nobject pivoting\\nvisual feedback\\ntactile feedback\\nin-hand manipulation maneuver\\nrobot hand\\ngripping force control\\nreference trajectory\\nangular position\\nvisual pose estimation system\\nfriction coefficient\\nmultiple finger\",\"50\":\"robot sensing systems\\nhidden markov models\\nmonitoring\\nforce\\nkinematics\\nmicrowave theory and techniques\\nhuman-robot interaction\\nmanipulator kinematics\\nmultimodal execution monitoring\\nanomaly detection\\nrobot manipulation\\nanomalous execution online detection\\nmultiple complementary sensory modalities\\nlong-duration manipulation behaviors\\ndata-driven approach\\nhidden markov model\\nhaptic sensing\\nvisual sensing\\nauditory sensing\\nkinematic sensing\\npr2 robot\\nroc curves\\ndetection threshold method\",\"51\":\"trajectory\\nrobot sensing systems\\nadmittance\\ngenerators\\noptimization\\nkinematics\\ngrippers\\nmanipulators\\nobservers\\noptimisation\\nrobotic assembly\\ntrajectory control\\ngeometric uncertainty\\nabb dual-arm 7-dof lightweight prototype robot\\ninteraction force\\nmodel-based sensorless observer\\nconstraint based optimization\\nreal-time trajectory generator\\nadmittance based control\\nrobotic manipulator\\ncontraint based peg-in-hole insertion\\nsensorless peg-in-hole insertion\\ndual-arm robot\",\"52\":\"taxonomy\\nrobot sensing systems\\nservice robots\\nmanipulators\\nrobot kinematics\\ngrippers\\nforce control\\nindustrial manipulators\\nposition control\\nrobotic assembly\\nsoftware architecture\\nunstructured environment\\nindustrial robots\\ncontact interaction control\\nscalable robotic system design\\nreal-world robotic assembly tasks\\nhardware architecture\\nworkspace optimization\\nexternal wrench compensation\\nposition-based force control\\ndexterous task\\nbimanual pin insertion\\nautonomous assembly\\nready-to-assemble chair\",\"53\":\"robot kinematics\\nrobot sensing systems\\nforce\\nfriction\\nforce measurement\\nforce feedback\\nforce sensors\\nmanipulator kinematics\\nkinematic multirobot manipulation\\ncooperative manipulation task\\nrobotic fleet\\nphysics engine\\nlow-cost robots\\nvelocity sensors\\ncardboard box\\nlaboratory environment\\nhuman-swarm cooperation\",\"54\":\"decentralized control\\nmanipulators\\nestimation\\ntrajectory\\nmobile communication\\ndecentralised control\\nmobile robots\\nmotion control\\nrobust control\\ndecentralized motion control\\ncooperative manipulation\\nnetworked mobile manipulators\\ndecentralized controller\\ndecentralized estimation\\ndiscontinuous robustification\\nmanipulation settings\",\"55\":\"planning\\ntrajectory\\nlearning (artificial intelligence)\\nrobots\\nmachine learning algorithms\\nmarkov processes\\ngrasping\\ncontrol engineering computing\\ndecision theory\\nmanipulators\\nmobile robots\\npath planning\\nrandomised algorithms\\nsearch problems\\nguided search\\nlearned heuristics\\nmobile manipulation planning\\nplanning complexity\\ninfinite branching factor\\ntask and motion planning\\ntamp methods\\nlogical search\\ngeometric reasoning\\nstatistical machine learning\\nrandomized local search algorithm\\nplan refinement\\nmarkov decision process\\nmdp\\nreinforcement learning\\nhigh-level task plans\",\"56\":\"robot sensing systems\\norbits\\nend effectors\\nmobile communication\\nthree-dimensional displays\\ncad\\ncollision avoidance\\nconstraint handling\\nimage colour analysis\\nimage sensors\\nmanipulators\\nmobile robots\\nrobot programming\\ntelerobotics\\nuser interfaces\\nadaptive object centered teleoperation control\\nmobile manipulator\\nvirtual objects\\n3d design software\\nautocad\\n3d design community\\nteleoperation interface control mode\\ncomputer aided design softwares\\n3d objects\\norbit object\\npan object\\nwrist mounted rgb-d sensor\\ngripper motions\\nsystem redundancies\\nobstacle avoidance\\nconstraint based programming framework\\nvirtual object\\nmobile manipulation\\nteleoperation\",\"57\":\"robots\\nsupport vector machines\\ntrajectory\\naerospace electronics\\ncost function\\nclutter\\ntraining data\\ncontrol engineering computing\\ndigital simulation\\nlearning (artificial intelligence)\\nmanipulators\\nmedical robotics\\nsurgery\\nsupervisor burden\\ndagger\\nsupport vectors\\nonline learning from demonstration algorithms\\nhigh dimensional state spaces\\nsystem dynamics\\nmmd-il algorithm\\nshiv algorithm\\nsvm-based reduction in human intervention\\noutlier rejection\\nlevel set boundary\\none class support vector machine\\ndimensional visual feature space\\npush-grasping\\nclutter simulation\\nphysical surgical needle insertion\",\"58\":\"kernel\\ntraining data\\ngaussian processes\\noptimization\\ntraining\\nrobots\\nground penetrating radar\\nintelligent robots\\nminimisation\\npath planning\\nregression analysis\\nsparse matrices\\nrobust learning-from-demonstration\\nleveraged gaussian process regression\\nsparse-constrained leveraged optimization algorithm\\nproximal linearized minimization\\nsensory field reconstruction\\ndirect policy learning\\nplanar navigation problems\\nlfd methods\",\"59\":\"dynamics\\nplanning\\nrobot kinematics\\ncollision avoidance\\naerospace electronics\\nacceleration\\nacceleration control\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmotion control\\noptimal control\\nrobot dynamics\\nstochastic systems\\ncomplex hybrid stochastic obstacle dynamics\\nstatic obstacle\\ndynamic obstacle avoidance robotic task\\nhigh-dimensional continuous robot state space\\ndimensionality constraint\\noptimal robot motion\\npreference appraisal reinforcement learning\\npearl\\nstochastic hybrid dynamics\",\"60\":\"optimization\\nbayes methods\\nsafety\\ntuning\\nvehicle dynamics\\nnoise measurement\\ncomputational modeling\\naircraft control\\ncontrol system synthesis\\ngaussian processes\\nhelicopters\\nlearning systems\\noptimisation\\nparameter estimation\\nsafe controller optimization\\ngaussian process\\ncontroller design\\ndynamic systems\\nmachine learning\\nbayesian optimization\\nsafety-critical system failure\\nsafe optimization algorithm\\nsafeopt\\nautomatic controller parameter tuning\\nlow-performance controller\\nperformance threshold\\nprobability\\nquadrotor vehicle\",\"61\":\"hidden markov models\\nrobots\\nencoding\\nprobabilistic logic\\nlinear programming\\npredictive models\\noptimal control\\nautomatic programming\\npredictive control\\nprobability\\nrobot programming\\nvariable duration movement encoding\\nminimal intervention control\\nprogramming by demonstration\\npbd\\nlow-level controller\\ntask performance\\nmovement duration probabilistic encoding\\nhidden semimarkov model\\nhsmm\\nmodel predictive control\\nmpc\\nprobabilistic model\\nrobot experiment\",\"62\":\"robots\\nmathematical model\\ndynamics\\ncomputational modeling\\nreal-time systems\\nheuristic algorithms\\npredictive control\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\nnonlinear systems\\noptimisation\\npendulums\\nmodel-based reinforcement learning\\nmodel-based rl\\nmodel identification\\nmodel predictive control\\noptimism-driven exploration\\nfeature-based representation\\nrobot morphology\\nconnectivity structure\\ncartpole system\\ndouble pendulum system\\nrobotic control task\\n7 degree of freedom arm\",\"63\":\"visualization\\nlearning (artificial intelligence)\\nrobot sensing systems\\nrobot kinematics\\ncameras\\nunsupervised learning\\nclosed loop systems\\ncontrol engineering computing\\nmanipulators\\nmotion control\\nrobot vision\\nstate-space methods\\ndeep spatial autoencoders\\nvisuomotor learning\\nreinforcement learning\\nautomated acquisition\\nrobotic motion skills\\ntask-relevant objects\\nstate-space construction\\nstate representation\\ncamera images\\nclosed-loop control\\npr2 robot\",\"64\":\"robots\\nsupervised learning\\nneural networks\\nheuristic algorithms\\nsearch problems\\ntraining\\ntrajectory optimization\\nlearning (artificial intelligence)\\nmanipulators\\nneurocontrollers\\noptimisation\\ntrajectory control\\ndeep neural network policies learning\\ncontinuous memory states\\npartially observed control tasks\\nsalient information\\nhigh-dimensional continuous systems\\nrobotic manipulators\\ncontinuous-valued memory states\\ngeneral-purpose policies\\nmemory representation\\ntrajectory optimization phase\\nsupervised learning phase\\nguided policy search\\nmemorization actions\\ncontinuous control\\nmanipulation settings\\nnavigation settings\",\"65\":\"training\\nrobot sensing systems\\nneural networks\\nheuristic algorithms\\ntrajectory optimization\\nvehicles\\nautonomous aerial vehicles\\ncollision avoidance\\nhelicopters\\nlearning (artificial intelligence)\\nneural nets\\npredictive control\\ndeep control policies\\nmpc-guided policy search\\nmodel predictive control\\nquadcopters\\nreinforcement learning\\nexplicit state estimation\\ndeep neural network policy\\nobstacle avoidance policies\\nquadrotor\",\"66\":\"haptic interfaces\\nvisualization\\nrobots\\ntraining\\nneural networks\\nconvolution\\ndata models\\nhuman-robot interaction\\nneurocontrollers\\ntactile understanding\\ndeep learning\\nhaptic data\\nvisual data\\nfine grained tactile\\nhaptic adjectives\\nphysical interaction data\\nvisual interaction data\\nvisual predictions\\nphysical interactions\\ncognitive pattern\\nphysical interaction signals\\nhaptic classification\\ndeep neural networks\\nphysical interaction\\nvisual observations\",\"67\":\"robots\\nkernel\\nsolid modeling\\nparametric statistics\\nmathematical model\\ncomplexity theory\\ngaussian processes\\nhumanoid robots\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nnonparametric statistics\\nshear modulus\\nincremental semiparametric inverse dynamics learning\\nrigid body dynamics equations\\nparametric modeling\\nnonparametric modeling\\nincremental kernel methods\\nmechanical properties\\nicub humanoid robot arm\",\"68\":\"robot sensing systems\\nadaptation models\\ncost function\\npredictive models\\ngaussian processes\\naerospace electronics\\nintelligent robots\\nlearning (artificial intelligence)\\nmanipulators\\nrandom processes\\ntrees (mathematics)\\nsensorimotor framework\\nrobot\\nsensorimotor mapping\\nmotor components\\nhigh-dimensional sensory\\nmotor spaces\\ngp learning strategy\\nrapidly exploring random tree algorithm\\nrrt algorithm\\ngradient-based search\\njoint space controller\",\"69\":\"face\\nthree-dimensional displays\\nsolid modeling\\ncameras\\nshape\\ntracking\\nrobustness\\nimage colour analysis\\nimage motion analysis\\nlearning (artificial intelligence)\\nobject tracking\\nrobust object tracking\\nadaptive size estimation\\nappearance learning\\nadaptive learning mechanism\\nrgbd cameras\\nred-green-blue-depth cameras\\nobject motion\\ninscribed cuboid\\narbitrary shape object\",\"70\":\"target tracking\\ntrajectory\\ntime measurement\\nstate estimation\\noptimization\\nmarkov processes\\nfiltering theory\\nprobability\\nsensor fusion\\npedestrian tracking dataset\\ntransition probabilities\\nmeasurement likelihoods\\nphd recursion\\nmultitarget tracker\\nmultitarget state estimation\\nnetwork flows\\nprobability hypothesis density filter\\nglobal data association\",\"71\":\"image edge detection\\ntransforms\\nthree-dimensional displays\\ncameras\\ncost function\\nfeature extraction\\nedge detection\\ngradient methods\\nimage colour analysis\\nmotion estimation\\nobject tracking\\npose estimation\\nrobust camera motion estimation\\ndirect edge alignment\\nsubgradient method\\nfeature-less methods\\ncomputational power\\ndirect edge alignment approach\\n6-dof tracking\\nphoto-consistency based methods\\ndistance transform\\nenergy formulation\",\"72\":\"three-dimensional displays\\nimage reconstruction\\nshape\\nsurface reconstruction\\nsplines (mathematics)\\nimage segmentation\\nclustering algorithms\\nimage motion analysis\\nimage sequences\\nleast squares approximations\\nobject tracking\\nvideo signal processing\\nmonocular 3d tracking\\ndeformable 3d surface reconstruction\\nnonrigid structure from motion context\\npiecewise methods\\ntriangulated mesh model\\nvisual information\\ngeometric characteristics\\nimage domain\\ntps\\nthin-plate splines\\nnonlinear least squares approach\\nmonocular video sequences\",\"73\":\"joints\\nmanifolds\\nthree-dimensional displays\\nmathematical model\\nkalman filters\\ncameras\\nnoise measurement\\nimage filtering\\nimage sequences\\nmotion estimation\\nnonlinear filters\\narticulated motion estimation\\nmonocular image sequence\\nspherical tangent bundles\\nsecond order stochastic dynamical model\\nstate space\\nriemannian manifold\\nriemannian extended kalman filter\\nstructure estimation\\ncamera\\nsynthetic data\\ncmu mocap dataset\",\"74\":\"computational modeling\\nstandards\\ncameras\\nrobustness\\ncomputational complexity\\nsensor phenomena and characterization\\napproximation theory\\ngaussian processes\\nimage filtering\\nobject tracking\\nrobot vision\\ndepth-based object tracking\\nrobust gaussian filter\\nmodel-based object 3d-tracking\\ndense depth images\\ndepth sensors\\nfat-tailed measurement noise\\nrobustification method\\napproximation\",\"75\":\"training\\nradio frequency\\nimpurities\\nestimation\\nrobot kinematics\\nmeasurement\\ncalibration\\ncomputational complexity\\ndexterous manipulators\\ngrippers\\nimage resolution\\nimage segmentation\\nlearning (artificial intelligence)\\npose estimation\\nregression analysis\\nrobot vision\\narm segmentation\\nrandom regression forest\\nframe-by-frame method\\nangular joint position estimation\\narm depth images\\nhand-eye calibration\\nhand-eye coordination\\nrobotic arm\\nvision-based control\\njoint angle pixelwise regression\\nrobot arm pose estimation\",\"76\":\"mirrors\\ncameras\\nprototypes\\nlenses\\nvisualization\\nmachine vision\\ntime factors\\ngaze tracking\\nmicromirrors\\nmobile robots\\nrobot vision\\nsaccade mirror 3\\nultrawide gaze control range\\ntriple rotational mirrors\\noptical high-speed gaze controller\\nautomated rotational mirrors\\nhigh-speed visual tracking\\nhigh-speed vision system\\ntable tennis ball\\nflying drone\",\"77\":\"robots\\njoints\\nelbow\\nmuscles\\nactuators\\nbones\\nresists\\nmedical robotics\\nmultiaxis compliant tensegrity joint\\nhuman elbow\\ntensegrity elbow\\nrotation axis\\ncompliant robotic joints\\nfunctional redundancy\\nbiologically accurate robotic models\",\"78\":\"actuators\\nmodulation\\nsprings\\nforce\\nbiology\\ntorque\\nimpedance\\ncompliance control\\nelasticity\\nenergy conservation\\nhuman-robot interaction\\nenergy efficient impedance modulation\\nenergy efficient compliant actuation\\nnext-generation autonomous systems\\ndomestic robots\\nprosthetic devices\\northotic devices\\nwearable exoskeletons\\nenergy cost\\nbiologically-inspired compliant actuation\\nminimalistic compliant actuator\\nstiffness augmentation\\nhuman-machine collaboration\",\"79\":\"foot\\nsprings\\nrubber\\nforce\\nlegged locomotion\\nhumanoid robots\\npressure sensors\\ndamping\\nsprings (mechanical)\\nvariable compliant humanoid foot\\ntoe mechanism\\nrectangular plate sole structures\\nflat terrain locomotion\\nsmall terrain irregularities\\nrough terrains\\nvariable compliant humanoid feet design\\nhumanoids locomotion\\nuneven terrains\\nleaf spring\\nrubber balls\\nvariable stiffness range\",\"80\":\"springs\\ntorque\\nbenchmark testing\\nshafts\\nrobots\\nimpedance\\ngears\\ncompliant mechanisms\\ndesign engineering\\nwork density analysis\\nadjustable stiffness mechanisms\\nmechanical compliance\\nrobot versatility\\nasm\\naie uno\",\"81\":\"actuators\\nforce\\nsprings\\nlegged locomotion\\nmathematical model\\nmagnetic cores\\ncontrollers\\nhopping mechanism\\nlinear elastic actuator\\nlegged robots\\nrunning robots\\nterrain\\nlocomotion actuators\\nwalking robot\\nuntethered hopping robots\\nparallel-elastic actuator\\nvoice coil actuators\\nelectrically-powered direct-drive translational motors\\nlinear force output\\nelectromechanical model\\nleap mechanism\\nbang-bang hopping controller\\nspring parameters\\noptimal spring stiffness\\nweight-bearing functions\\nrobot leg\",\"82\":\"needles\\nrobots\\ngrasping\\ncontext\\nsprings\\nmagnetic resonance imaging\\nperformance evaluation\\ndesign engineering\\nelasticity\\nmedical robotics\\npneumatic control equipment\\nmr-compatible robot design\\nvariable stiffness spherical joint design\\nprestressed cable-driven mechanisms\\nzero stiffness configuration\\nprestress adjustment system\\npneumatic energy\\nmultimaterial additive manufacturing\",\"83\":\"springs\\ntorque\\nactuators\\ndc motors\\nbrakes\\ngears\\nhysteresis motors\\nenergy conservation\\nenergy consumption\\nrobot kinematics\\nsprings (mechanical)\\nspea\\ndrastic actuator energy requirement reduction\\nparallel motors symbiosis\\nlocking mechanisms\\nactuation schematics\\nseries-parallel elastic actuator\\nenergy reduction\\ncontrol strategy\\nrecruitment strategy\",\"84\":\"springs\\nelectrodes\\nexoskeletons\\nforce\\nelectrostatics\\nrobots\\ndielectrics\\nadhesion\\nclutches\\ndielectric materials\\nelastic constants\\nelastomers\\nmechanical energy storage\\npower consumption\\nsprings (mechanical)\\nlightweight low-power electroadhesive clutch\\nlightweight low-power electroadhesive spring\\nexoskeleton actuation\\nactuators\\nrobotic devices\\nspring engagement control\\nankle exoskeleton\\nelectrostatic adhesion\\nthin electrode sheets\\ndielectric material\\nelastomer springs\\nmechanical energy\\nelectrically-controllable clutches\\ntorque density\\nonline stiffness tuning\",\"85\":\"torque\\nactuators\\nbelts\\nmanipulators\\ntiming\\nshafts\\ncameras\\nhumanoid robots\\nhuman-robot interaction\\nhydraulic actuators\\nservomechanisms\\ntelerobotics\\nhumanoid robot\\nremote object manipulation\\nhead-mounted display\\n2dof servo-controlled neck stream live video\\nstereo camera\\nrotary hydraulic actuator\\nrolling-diaphragm cylinder\\nhybrid air-water configuration\\nhaptic telepresence robot\\nhydrostatic transmission\",\"86\":\"robots\\nbones\\nthree-dimensional displays\\nkinematics\\ngraphical user interfaces\\nminimally invasive surgery\\ncomputer graphics\\nimage processing\\nmedical robotics\\northopaedics\\nsurgery\\nimage-based robotic system\\nenhanced minimally invasive intra-articular fracture surgeries\\nrobotic assistance\\northopedic fracture surgery\\nfracture fragment repositioning\\ninvasive fracture surgery techniques\\nclinical outcome\\nreduced recovery time\\nhealth-related costs\\nrobot-assisted fracture surgery\\nrafs system\\nbristol robotics laboratory\\nrobotic architecture\\nreal-time 3d imaging\\nfractured anatomy\\ndistal femur fractures\\nrobot assistance\\northopedics\\nbone model\\nsurgeon\\nclinical requirements\",\"87\":\"cameras\\nrobot vision systems\\ncalibration\\nrobot kinematics\\nthree-dimensional displays\\nimage registration\\nimage sensors\\nmedical robotics\\nrobot vision\\nstatistical analysis\\nplenoptic cameras\\nsurgical robotics\\nthree-dimensional sensing\\nsurgical scenes\\ndepth sensor\\nmetric calibration\\ncoordinate frame registration\\nhand-eye calibration\\nrobotic needle insertion\\nsuture placement\\nsuture spacing\\nstandard deviation\\ninter-suture distances\\nleak-free suturing\\nrobotic tool\\nplenoptic depth imager\",\"88\":\"robots\\nsurgery\\nhaptic interfaces\\nmanuals\\ncollaboration\\ntrajectory\\ncovariance matrices\\nhuman-robot interaction\\nmedical robotics\\nthree state human-robot collaborative framework\\nhubot\\nbimanual surgical tasks\\nminimally invasive surgery\\nhuman-robot cooperative control\\nraven ii surgical robot\",\"89\":\"collaboration\\nsurgery\\nmanipulators\\nbiological tissues\\nrobot sensing systems\\ntumors\\nhuman-robot interaction\\nmedical robotics\\ntelerobotics\\ntumours\\nhuman-robot collaboration\\nhuman agents\\nrobotic agents\\nstiff inclusion segmentation\\nrobot-assisted surgery\\ntumor\\nartificial tissue\\nteleoperation\\nsupervised control\\ntraded control\\nvirtually palpated tissues\",\"90\":\"hardware\\nrobot sensing systems\\nplanning\\ncollision avoidance\\ngrippers\\nbending\\ndentistry\\ndiseases\\nmedical robotics\\northotics\\nsampling methods\\ntime-varying systems\\northodontic archwire preparation\\nphysical robotic system\\ncontrol system architecture\\ntime-varying environment\\ncollision checker\\nadaptive sampling-based bending planner\\nautomatic bending control\\northodontic archwire bending\\nros-integrated control system\\nhardware system\\nmalocclusion\\nlingual orthodontic patient treatment\\northodontics\\northodontic archwire\\nrobotic bending\\nmotion planning and control\\nros\",\"91\":\"ultrasonic imaging\\nthree-dimensional displays\\ntransducers\\nrobot kinematics\\ntrajectory\\nprobes\\nbiomedical ultrasonics\\ncameras\\nmedical image processing\\nmedical robotics\\npath planning\\nsplines (mathematics)\\ntelecontrol\\nmedical robotic ultrasound trajectory planning algorithm\\nvolume of interest coverage\\nremote control\\nultrasound probes\\ntelemanipulation systems\\nautonomous acquisitions\\nautomatic robotic ultrasound acquisition\\ndiagnostic images\\n3d patient surface point cloud\\ndepth camera\\nvolume delineation\\nparameterizable path function\\nparallel scan trajectories\\nspline\\npreliminary path points\\nlightweight robot\\nultrasound scan\\nimpedance control mode\",\"92\":\"cameras\\nrobots\\nsurgery\\nkalman filters\\nbiomedical optical imaging\\noptical devices\\nacceleration\\nimage fusion\\nmedical robotics\\nnonlinear filters\\noptical tracking\\nposition control\\nreal-time systems\\nrobot vision\\nrobust control\\nstate estimation\\noptical-inertial tracking\\nreal-time robot control\\nminimally invasive robotic surgery systems\\ntracking algorithm\\nhandheld input device\\ninertial measurements\\noptical measurements\\nrobust state estimates\\nerror state extended kalman filter\\nrobustness\\npartial device occlusions\\nactive optical markers\\n2d positions\\ncamera planes\\nfusion process\\nquality measure\",\"93\":\"actuators\\nrobot sensing systems\\nfabrication\\nsurgery\\nlaminates\\nmedical robotics\\nmicroactuators\\nrobot dynamics\\nsoft pop-up mechanisms\\nmicrosurgical tools\\ncompliant millimeter-scale articulated structure design\\nmanufacturing technique\\nsoft fluidic microactuators\\nsoft materials\\npop-up book mems paradigm\\nrigid-flex laminates\\narticulated robotic arm\\nendoscope model\\nrobotic mechanisms\\nsurgical scenario\",\"94\":\"metals\\nsprings\\nmanipulators\\nheating\\nelectron tubes\\nresistance\\nalloys\\ncables (mechanical)\\ncooling\\ndesign engineering\\ndexterous manipulators\\nphase changing alloy\\ncable-driven continuum manipulator\\nalloy thermomechanical properties\\nalloy liquid phase\\njoule heating\\nwater cooling\\nsingle-segment cm\\ntwo-segment cm\\nthermodynamic features\\ndexterity\\npayload-to-weight ratio\\ncontrollable stiffness\\nenergy efficiency\\nlumen\",\"95\":\"visualization\\nfeature extraction\\nhamming distance\\ndynamics\\nvocabulary\\nrobot motion\\nimage motion analysis\\nlearning (artificial intelligence)\\nslam (robots)\\nincremental bag-of-words system\\nmasked hamming distance\\nlearned descriptor packaging\\ncodeword generation\\ndiscriminative projection\\nmean image patch\\nbinary descriptor\\nlearning algorithm\\ncamera motion\\nbag-of-words frameworks\\nplace recognition\\nincremental loop-closure detection\\nmotion dynamics\\nbinary features online learning\",\"96\":\"cameras\\nrobustness\\noptical imaging\\nkernel\\nestimation\\nvisualization\\nnoise measurement\\nimage denoising\\nimage sequences\\nmaximum likelihood estimation\\nmotion estimation\\nmonocular egomotion computation\\ncamera egomotion estimation\\nobserver rotation\\nobserver translation\\nnonconvex cost function\\nperspective camera motion equation\\noptical flow estimation\\nexpected residual likelihood method\\nerl method\\nconfidence weights\\nlifted kernel\\nmotion estimation pipeline\",\"97\":\"feature extraction\\nnavigation\\nfield programmable gate arrays\\ncameras\\ncomputer architecture\\npipelines\\ncontrol engineering computing\\nmobile robots\\npower aware computing\\nrobot vision\\nsystem-on-chip\\nlow-latency image processing\\nlatency reduction\\nvision-based mobile robot navigation\\nvisual data\\nprocessor centric fpga-based system-on-chip design\\non-line image processing\\nautonomous vision-based navigation\\nteach-and-repeat algorithm\\nimage salient points\\ncpu-based solution\\nfpga-based implementation\\npure cpu solutions\\npower consumption\\nlatency\\nvisual navigation\\nfpga\\nsystem on chip\\nimage processing\",\"98\":\"detectors\\ntraining\\nrobustness\\nfeature extraction\\nsun\\nrobots\\nclouds\\ncameras\\ndata mining\\nimage classification\\nsupport vector machines\\nunsupervised learning\\nall-weather localisation\\ncamera-only localisation\\noutdoor environments\\nsift\\nsurf\\nbrief\\nlandmark observations\\nplace-specific linear svm classifiers\\ndistinctive elements\\nunsupervised mining algorithm\\nn-vs-n analysis\",\"99\":\"visualization\\ntrajectory\\nrobots\\ngaussian processes\\nplanning\\ncameras\\ntraining\\ninterpolation\\nrobot vision\\nappearance-based approach\\nvision-based localisation system\\ngaussian process regressor\\nvision-based localisation\",\"100\":\"cameras\\npipelines\\nvisualization\\nrobot vision systems\\nrobustness\\nthree-dimensional displays\\nmotion estimation\\nmobile robots\\nnatural scenes\\nfield-of-view cameras\\nvisual-odometry technology\\nvision-based motion estimation\\nvisual odometry algorithm\\nmotion-estimation performance\\nscene geometry\\ncamera motion\\nmobile robotics\\nindoor scene\\nvo pipeline\\nfov fisheye\\nurban canyon environments\",\"101\":\"state estimation\\nsensors\\nrobustness\\ncameras\\neigenvalues and eigenfunctions\\nlaser radar\\noptimisation\\nstate-space methods\\ndegeneracy\\noptimization-based state estimation problems\\nstate estimation methods\\nonline method\\ngeometric structure\\nproblem constraints\\nstate space\\ncamera data\\nlidar sensor\\n6-dof ego-motion\\nonline positioning\\nonline mapping\",\"102\":\"predictive models\\nrobustness\\nvisualization\\noptimization\\ntransforms\\ncameras\\nestimation\\nbayes methods\\nestimation theory\\ninference mechanisms\\nmonte carlo methods\\nrobot vision\\nprobe-gk\\npredictive robust estimation using generalized kernel\\ncomputer vision\\nrobotics\\nbayesian inference\\nsensor uncertainty\\nmonte carlo simulation\",\"103\":\"three-dimensional displays\\ncameras\\nsimultaneous localization and mapping\\nimage resolution\\nestimation\\ngraphics processing units\\nreal-time systems\\nestimation theory\\nimage reconstruction\\nimage texture\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nsmoothing methods\\nmultilevel mapping\\nslam\\nmonocular camera\\n3d geometry reconstruction\\ngraphics processing unit\\ngpu\\nmultiresolution depth estimation\\nspatial smoothing process\\nrobotic navigation\",\"104\":\"mathematical model\\nparallel robots\\nlegged locomotion\\ntrajectory\\nlimiting\\nrobot sensing systems\\ncables (mechanical)\\nrobots\\nshear modulus\\nsuspended cable-driven parallel robots\\nrigid legs\\nlimiting factor\\nmechanical equilibrium\\ncable behavior\\n6-dof cdpr\\nhorizontal cross-sections\\nplatform orientation\\nnondeformable cables\\nelastic mass-less cables\\ncable tension\\nconstant thresholds\\nsagging cables\\ncomputer intensive algorithm\\ncross-section approximation\",\"105\":\"kinematics\\nactuators\\njacobian matrices\\nrobot kinematics\\nmanipulators\\nservice robots\\neigenvalues and eigenfunctions\\nend effectors\\nindustrial manipulators\\nmanipulator kinematics\\nmaterials handling\\nposition control\\nvelocity control\\ntriple scissor extender robot\\nsix-dof lifting-and-positioning robot\\ndegrees-of-freedom\\nend effector\\nscissor mechanisms\\nlinear actuators\\nball joints\\ndifferential control\\nend-effector velocities\\nactuator velocities\\ninverse jacobian analysis\\nkinematic modeling\\narbitrary orientation\\narbitrary position\\ncoordinated motion\\ntriangular plate\\nrobot mechanism\\nscissor lift\\nparallel manipulator\\nstewart platform\\n6-psu\\njacobian\",\"106\":\"conferences\\nautomation\\nmanipulators\\nmatrix algebra\\nstiffness matrices\\njacobian matrices\\ngeometric variables\\ngantry tau robots\\nrobot geometry\\nnatural frequencies\\nparallel robot\\n3-dof gantry tau robots\\ndynamic isotropy\",\"107\":\"robot kinematics\\nmatrix decomposition\\nmanipulators\\nmobile communication\\nforce\\nsprings\\nelastic constants\\nindustrial manipulators\\nmaterials handling\\nmatrix algebra\\nparallel pick-and-place robots\\nidentical limbs\\nstiffness analysis\\ncartesian stiffness matrix\\nvirtual spring approach\\nstiffness matrix nondimensionalization\\ntranslational stiffness\\nrotational stiffness\\nmanipulator stiffness behaviors\\nelasto-static performance\",\"108\":\"robots\\ntransmission line matrix methods\\nnickel\\ncomputers\\ngeometry\\nmatrices\\ncables (mechanical)\\nnumerical analysis\\nprocess algebra\\nrobot kinematics\\nwrench-closure translational workspace determination\\ncable-driven parallel robots\\ncomputer algebra\\nnumerical routines\\ntranslational wrench-closure workspace computation\\nconstant orientation computation\",\"109\":\"manipulators\\ncouplings\\nkinematics\\njacobian matrices\\nindexes\\nprototypes\\nyaw constraining performance\\nscara-tau parallel manipulator variants\\nscrew theory\\ndelta parallel manipulator\\ninput-output jacobian\",\"110\":\"robots\\nforce\\npower cables\\npulleys\\nenergy consumption\\nfriction\\nwinches\\ncables (mechanical)\\nmobile robots\\nindustrial robot\\ninternal tension\\nmechanical losses\\nservo amplifier\\nelectrical losses\\nipanema3\\nlight weight synthetic fiber cables\\nmachine frame\\nservo drives\\nmobile platform\\ncable-driven parallel robots\",\"111\":\"blades\\npoles and towers\\nmanipulators\\nmobile communication\\ninspection\\nwind turbines\\nattitude control\\ndesign engineering\\ngrippers\\nmaintenance engineering\\nmobile robots\\nmotion control\\noffshore installations\\ntest equipment\\nultrasonic devices\\nwind power plants\\nwires\\nwire-driven parallel robotic system\\noffshore wind turbine maintenance\\nwaterjets\\nphased array ultrasonic testing devices\\npaut devices\\nmechanical design\\ntowers climbing\\nblades climbing\\ntransition motion\\nblades gripping\\ncontact motion\\nmotion analysis\\nnonholonomic constraints\\nheight control scheme\\nattitude control scheme\",\"112\":\"stomach\\nbatteries\\nesophagus\\nice\\nlegged locomotion\\nwounds\\ncomposite materials\\nmagnetic fields\\nmedical robotics\\norigami robot\\nminiature robots\\nstomach wounds patching\\nremote instructions\\nmedical professionals\\nversatile clinical procedures\\ncomposite material sheets\\nbiocompatible robot\\nbiodegradable robot\\nunderwater maneuvers\\nbiological organs\\nsimulated physical environment\",\"113\":\"robot kinematics\\nmanipulators\\nelectron tubes\\nthree-dimensional displays\\nkinematics\\nsurgery\\nmedical robotics\\nmotion control\\nposition control\\nfollow-the-leader motion\\nextensible tendon-driven continuum robots\\npath following motion\\nminimally-invasive surgery\\nhyper-redundant snake robots\\ncontinuum robots\\nmanipulator\\nforward kinematics model\\n3d path\\nb-spline curves\",\"114\":\"solid modeling\\nforce\\nforce measurement\\nposition measurement\\nsurgery\\nrobots\\ndeformable models\\nimage registration\\nmedical image processing\\ncmu\\ncontact force\\ncontact location\\nsilicone models\\nex vivo organ\\ncomplementary model update\\nphysical interaction\\nlocal deformation\\nblind exploration\\ncomputer-aided surgery\\nsurgical tool\\nflexible environments\\nstiffness mapping\\nsimultaneous registration\",\"115\":\"bayes methods\\noptimization\\nrobot sensing systems\\nsurgery\\nrobot kinematics\\ndata models\\nbiological organs\\nblood vessels\\nend effectors\\nforce measurement\\nimage registration\\nmedical image processing\\nmedical robotics\\noptimisation\\nposition measurement\\nflexible environment\\nregistration\\nstiffness mapping\\ncomputer-aided surgery\\npreoperative model\\nsurgical navigation\\nmechanical palpation\\narteries\\ncancerous lumps\\nbayesian optimization framework\\nend effector\\na priori geometric model\\nintraoperative data fusion\\ngaussian processes\\nstiffness distribution\\ncartesian robot\\nsilicone organ model\\nex vivo porcine liver\\nimaging\",\"116\":\"needles\\ntransducers\\nrobot kinematics\\nthree-dimensional displays\\nvisual servoing\\ntracking\\nbiomedical transducers\\nbiomedical ultrasonics\\ngraphics processing units\\nimage fusion\\nimage registration\\nmedical image processing\\nmedical robotics\\nreal-time 3d ultrasound registration\\ninterventional navigation\\nintraoperative imaging\\nsurgical interventions\\nautomatic robotic support\\nimage-guided navigation\\nclinical routine\\nfull image-based 3d ultrasound registration\\nreal-time servo-control scheme\\nmultimodal fusion\\npreinterventional plan\\nannotated needle insertion path\\ntarget anatomy tracking\\nneedle guide\\nmanual insertion\\nmotorized 3d ultrasound transducer\\nforce-controlled robot\\ngpu-based image processing toolkit\\ngeometric agar\\ngelatin phantom\\npositioning errors\\nspine phantom\\nlumbar spine injections\",\"117\":\"electroencephalography\\nrobots\\nheadphones\\ndecoding\\nexoskeletons\\nsynchronous motors\\ntraining\\nadaptive control\\nbrain-computer interfaces\\ncontrol engineering computing\\nfeature extraction\\nhuman-robot interaction\\nmedical robotics\\nmedical signal processing\\npatient rehabilitation\\ndry-wireless eeg\\nasynchronous adaptive feature extraction\\nplug-and-play coadaptive brain robot interface\\nasynchronous adaptive brain machine interface\\ndry-wireless headset\\nlower limb exoskeleton robot\\nfoot motor imagery\\nelectroencephalogram\\nasynchronous bmi\\nexponential moving average\\nfeature adaptation\\nsensorimotor rhythm classification\\nrobot squatting\\nadaptive bmi\\nneuromotor rehabilitation\",\"118\":\"robot sensing systems\\nservice robots\\ncalibration\\ncameras\\nhead\\nadaptive filters\\nbiomimetics\\nelectroactive polymer actuators\\nindustrial manipulators\\npneumatic actuators\\nrobot vision\\ntactile sensors\\nsensory array\\nmanufacturing tolerances\\nstandard industrial robotic manipulator\\nvisual error feedback\\ncamera\\nelectro-active polymer artificial muscles\\nbellabot\\ncerebellar function\\nadaptive filter model\\nbiomimetic whiskered robot\\nvisual-tactile sensory map calibration\",\"119\":\"electrodes\\nneedles\\nfeature extraction\\nskin\\nmetals\\nmuscles\\nindexes\\nelectromyography\\nhuman computer interaction\\nimage motion analysis\\nmedical image processing\\nhigh-density surface emg interface\\nfinger movement recognition\\nsurface electromyography\\nfinger motion recognition\\nmetal microneedle\\nmnhd semg interface\\nsemg signal extraction\\nstatic finger flexion test\\nfingertip force estimation\",\"120\":\"surface treatment\\nsurface reconstruction\\nmathematical model\\nthree-dimensional displays\\nsurface emitting lasers\\ntv\\nrobots\\nmobile robots\\npath planning\\nstatistical analysis\\nhidden shape recovery\\ndense mapping\\nmobile robotics applications\\nstatic maps generation\\nephemeral objects\\nlaser point clouds\\nback-fill dense surfaces\\nresource constraints\\nthree-dimensional voxel grid\\ntruncated signed distance function\\ntsdf\\ntv regulariser\\ntotal variation regulariser\\nkernel conditional density estimation\\nkcde\",\"121\":\"image segmentation\\nimage reconstruction\\nthree-dimensional displays\\ncameras\\nsurface reconstruction\\nrobots\\ngeometry\\nimage matching\\npath planning\\nrendering (computer graphics)\\nslam (robots)\\ntree searching\\nrobotic exploration\\n3d surface model\\nvisibility constraint\\nstructure-preserving criterion\\nbfps\\nsegment adjacency graph\\nsegment projection lines\\nimage-space analysis\\nsimultaneous localization and planning\\nline-slam\\nimage-based rendering\\nrobotic mapping\\nline segment matching\\nbreadth-first plane search\\nline adjacency\\nimage space\",\"122\":\"robot sensing systems\\noctrees\\ntraining data\\ngaussian processes\\nthree-dimensional displays\\nbayes methods\\ncomputational complexity\\nimage classification\\nimage fusion\\nmobile robots\\nregression analysis\\nrobot vision\\ngaussian process occupancy maps\\ntest-data octrees\\nnested bayesian fusion\\ndescriptive online 3d occupancy maps\\nmap cells\\nsensor data\\ncubic computational complexity\\nlarge-scale mapping\\nnested bayesian committee machine\\ngp regressions\\nvariance threshold\\nbinary classification step\\nimproved-accuracy classifier\\nautonomous navigation\",\"123\":\"robot sensing systems\\nkernel\\ngaussian processes\\ntraining\\nlogistics\\nhilbert transforms\\nsensor fusion\\nslam (robots)\\nsensor observations\\nrobot range scans\\nglobal occupancy map\\n3d map-building\\n3d hilbert maps\\noccupancy mapping\\nprobabilistic map fusion\",\"124\":\"partitioning algorithms\\nimage segmentation\\nrobots\\ntransforms\\nsemantics\\nnavigation\\ncleaning\\nmobile robots\\npath planning\\nrobot vision\\nroom segmentation\\nfloor plans division\\nnavigation maps\\nsemantic units\\ntopological mapping\\nsemantic mapping\\nplace categorization\\nhuman-robot-interaction\\nautomatized professional cleaning\\nmap partitioning algorithms\\nmap segmentation algorithm\",\"125\":\"kernel\\ngaussian processes\\nrobot sensing systems\\nmathematical model\\nlaser beams\\nimage resolution\\npath planning\\nrobot vision\\nslam (robots)\\ncontinuous occupancy maps\\narea kernels\\n2d occupancy mapping\\nrobot environment classification\\noccupancy grids\\ncontinuous gaussian process occupancy maps\\nrobotic navigation\",\"126\":\"quaternions\\nmobile communication\\nrocks\\nlaser radar\\noptimization\\nface\\nstate estimation\\ngraph theory\\nmobile robots\\noptimisation\\nposition control\\nrobot vision\\nslam (robots)\\nvectors\\nthree-dimensional axis mapping\\n3dam\\nmobile platform\\nplanar surface orientation\\npositional information\\ngraph-based optimization algorithm\\nnormal vector\\nmobile autonomous geotechnical mapping\",\"127\":\"feature extraction\\ndetectors\\ncomputational modeling\\nadaptation models\\nsimultaneous localization and mapping\\nbayes methods\\nadaptive systems\\nrecursive estimation\\nrobot vision\\nslam (robots)\\nlifelong feature-based mapping\\nfeature-based graphical approach\\nrobotic mapping\\nautonomous agent\\nlocalization system\\nadaptive system\\nexpressive probabilistic generative feature persistence model\\nabstract semistatic environmental features\\nrecursive bayesian estimator\\npersistence filter\\nexplicit bayesian belief\\nfeature persistence estimation\\ngraphical mapping\\nlifelong environmental modeling\",\"128\":\"cameras\\ntrajectory\\nnoise measurement\\nvisualization\\nthree-dimensional displays\\nbuildings\\nposition measurement\\noptimisation\\nslam (robots)\\ncooperative 3d visual-inertial mapping\\nmanhattan world\\ncooperative mapping\\ncm\\nconstrained optimization problem\\ntrajectory estimation\\ngeometric constraints\\nvisual measurement\\ninertial measurement\",\"129\":\"damping\\nrobot kinematics\\nsprings\\ndynamics\\nstability analysis\\nsymmetric matrices\\nclosed loop systems\\nmanipulators\\nnonlinear control systems\\ntrajectory control\\ntrajectory tracking\\npassivity based approach\\nlink side damping\\ncompliantly actuated robots\\ndisturbance rejection characteristics\\nlink side dynamics\\nactuated robots\\nnonlinear spring characteristics\\nmotor coordinates\\noriginal plant dynamics\\nstability\\nclosed loop dynamics\\nvariable stiffness robot arm dlr hand arm system\\nnonlinear elastic elements\",\"130\":\"robots\\nbladder\\nactuators\\njoints\\nvalves\\nhardware\\ntorque\\nelasticity\\nmanipulators\\nmechanical variables control\\npneumatic control equipment\\nposition control\\npredictive control\\nsimultaneous position-stiffness control\\ninflatable soft robot\\ndynamic environments\\nuncertain environments\\nmechanical robustness\\ncontact forces\\npneumatically actuated soft robot\\nsoft robot actuation chambers\\nmodel predictive control\\nrobot linkages\\nactuation schemes\",\"131\":\"actuators\\nmathematical model\\nrobot kinematics\\nend effectors\\nimpedance\\nrobot sensing systems\\nforce control\\nhuman-robot interaction\\nrobot dynamics\\ntorque control\\ntrajectory control\\nseries elastic actuators\\nforce\\/torque fidelity\\nsea-driven robot systems\\ntrajectory\\nimpedance model\\nexternal force\\ncontrollers\\nregion control scheme\\nfourth-order system\\ncontrol method\\nobserver\",\"132\":\"dynamics\\nmanipulator dynamics\\nredundancy\\ntorque\\naerospace electronics\\nfeedback\\nmanipulators\\nmedical robotics\\nmotion control\\nsurgery\\ndecoupled cartesian admittance control\\nrcm constraint\\ndynamic formulation\\ncac\\nremote center of motion constraint\\nrcm\\ndynamically consistent extended formulation\\ndynamically decoupled redundancy resolution\\nnull-motion feedback\\nminimally invasive surgery procedures\\nmis\\nkuka lbr 7 iiwa r800 robot arm\",\"133\":\"estimation\\nend effectors\\nforce\\nservice robots\\nrobot kinematics\\ngeometry\\nadaptive control\\ncontrol system synthesis\\nforce control\\nforce sensors\\nfriction\\nindustrial robots\\nmotion control\\nposition control\\nimplicit force control\\nindustrial robot\\nstiffness estimation\\nstiffness compensation\\nforce control algorithms\\nposition-based adaptive force control strategy\\ncontrol design\\nindustrial controller structures\\nenvironment stiffness\\njoint friction disturbances\\ncontact surface\\nidentification approach\\ncontrol approach\\nforce sensor\",\"134\":\"actuators\\nmathematical model\\ndelay effects\\nnoise measurement\\nmeasurement uncertainty\\npredictive models\\ndelays\\nfeedback\\nmicromanipulators\\nthree-term control\\nmodel compensation-prediction scheme\\nmicromanipulation system control\\nsingle feedback loop\\nunknown modeling errors\\nsingle noisy feedback measurement\\nsystem time delay compensation\\nmathematical transformation\\nsingle equivalent error\\nnoise-insensitive extended high-gain observer\\nehgo\\nsmith predictor\\ncompensation-prediction scheme\\npid controller\",\"135\":\"force\\nactuators\\nmagnetic resonance imaging\\npistons\\nobservers\\npower transmission lines\\nimpedance\\nbiomedical mri\\ndisplacement control\\nforce control\\nforce feedback\\nmean square error methods\\nmedical robotics\\npatient rehabilitation\\npneumatic actuators\\npressure control\\npressure measurement\\nobserver based impedance control\\npneumatic actuation system\\nlong-transmission lines\\npneumatically driven nonmagnetic platforms\\nrobotic rehabilitation monitoring\\nmri-compatibility requirements\\ndirect pressure measurement\\ncontrol algorithm\\npressure observer\\ndisplacement feedback\\nrobust controller\\nforce tracking\\nstable force tracking\\nmean squared error\\nsize 7 m\",\"136\":\"robots\\ndelay effects\\nstability analysis\\ndelays\\nforce\\njitter\\naerospace electronics\\naerospace robotics\\nforce feedback\\nhuman-robot interaction\\ninteractive devices\\nmanipulators\\nstability\\ntelerobotics\\nkontur-2\\nforce-feedback teleoperation\\ninternational space station\\nspace telerobotics missions\\ngerman telerobotics mission\\nrussian telerobotics mission\\nfuture planetary explorations\\ntelemanipulation\\nrobot manipulator\\nforce-feedback joystick\\nbilateral controller\\n4-channels architecture\\npassivity\\ntime delay power network\",\"137\":\"bicmos integrated circuits\\ntopology\\nvisualization\\ncameras\\nindexes\\nrobot vision systems\\nimage segmentation\\nmulti-threading\\nfast image mosaicing approach\\nbag-of-binary-words scheme\\nseamless composites\\nmultithreaded architecture\\nrobotic mapping applications\",\"138\":\"cameras\\nthree-dimensional displays\\nrobot sensing systems\\ntime-frequency analysis\\nrobot kinematics\\naircraft control\\nautonomous aerial vehicles\\nclosed loop systems\\nhelicopters\\nmulti-robot systems\\npose estimation\\nrobot vision\\non-board vision-based 3d relative localization system\\nmultiple quadrotors\\nactive markers\\non-board camera\\nmultiple quadrotor tracking\\n3d pose extraction\\ncamera field-of-view\\nmultiple targets\\nsystem visibility\\nbidirectional sensing\\nsensing capabilities\\nvisibility analysis\\nrelative localization system\\nclosed-loop experiment\\nunmanned aerial vehicles\\nmultiuav\",\"139\":\"observers\\nrobot sensing systems\\nfault detection\\nfault diagnosis\\nvehicles\\nvisualization\\nautonomous aerial vehicles\\ncooperative systems\\nimage sensors\\nposition measurement\\nrobot vision\\ncooperative sensor fault recovery\\nmulti uav systems\\nfault detection-identification-and-recovery system\\ninternal position sensor\\ninternal attitude sensor\\ninternal visual sensor\\ncvs\\nredundant position estimation\\nredundant velocity estimation\\nvision-based fdir system\\noffline fault pattern injection\\nposition measurements\",\"140\":\"streaming media\\nmulticast communication\\nieee 802.11 standard\\nreceivers\\nmobile communication\\nunicast\\nwireless communication\\nautonomous aerial vehicles\\ntelecommunication network topology\\nvideo signal processing\\nvideo streaming\\nvideo surveillance\\nmicro aerial vehicles\\nvideo multicasting\\nmav\\nsurveillance\\ndisaster management\\nmobility\\nvideo sources\\nhigh data-rate\\ntransmission rate\\nwireless link estimation\\nfrequent feedback\\nmultiple receivers\\napplication layer rate-adaptive video multicast streaming framework\\n802.11 adhoc network\\nreceiver nodes\\nmulticast group\\nchanging link conditions\\ngain feedback\\napplication layer video multicast gateway\\nalvm-gw\\nvideo encoding rate\\npacket loss\",\"141\":\"vehicles\\ncameras\\ncollaboration\\nsensors\\nfeature extraction\\nsonar\\nkalman filters\\nautonomous aerial vehicles\\ninertial systems\\nmicrorobots\\nmobile robots\\nnonlinear filters\\nposition control\\nrobot vision\\nsensor fusion\\nslam (robots)\\nstereo image processing\\ncollaborative localization\\nformation flying\\ndistributed stereo-vision\\ncollaborative stereo-vision\\nmicroair vehicles\\nmav\\nmonocular camera\\ninertial measurement units\\nsonar sensor\\nsensor fusion scheme\\nextended kalman filter\\nposition estimation\\norientation estimation\\ndistributed measurement\\nperception\\ncontrol loop\\nros ground station\\nformation control\\nmicro-air vehicles\\nstereo-vision\",\"142\":\"manipulators\\nrobot kinematics\\ntrajectory\\nvehicles\\ncomputer architecture\\nfield effect transistors\\nautonomous aerial vehicles\\ncontrol system synthesis\\nend effectors\\nmotion control\\nmulti-robot systems\\ntrajectory control\\ncoordinated motion\\nthree-layer control architecture\\nmultiple aerial robotic manipulator\\nend-effector\\nlow level motion controller\\nnull space-based behavioral approach\",\"143\":\"convergence\\noptimization\\ncontrol systems\\ngenerators\\nunmanned aerial vehicles\\ncomputer architecture\\naerospace control\\nautonomous aerial vehicles\\ncollision avoidance\\nmobile robots\\nmulti-robot systems\\noptimal control\\noptimal event handling\\nmultiple unmanned aerial vehicles\\npath planner\\nshortest path\",\"144\":\"aircraft\\nland vehicles\\natmospheric modeling\\nelevators\\naerospace control\\naerodynamics\\naerospace components\\nautonomous aerial vehicles\\ngears\\nsolar powered vehicles\\nstratosphere\\ncooperative vehicle control\\ncar-mounted landing platform\\noperational availability\\ncrosswind conditions\\npayload capacity\\nlanding gear\\ntotal takeoff mass\\nlightweight design\\nsolar-powered high-altitude uav\\nmobile ground vehicle\\nfixed-wing uav landing\",\"145\":\"robots\\nfeedback control\\npredictive control\\nstandards\\nquadratic programming\\ncomputational modeling\\nautonomous aerial vehicles\\nfeedback\\nmicrorobots\\nnonlinear control systems\\nrobot dynamics\\nnonlinear model predictive control technique\\nnonlinear partial enumeration\\nmpc\\ntime per control iteration\\nnpe\\nmav flight\\nsimulation trials\\naggressive motion\\nreusable set\\nlocal feedback controllers\\nmicroaerial vehicle\",\"146\":\"trajectory\\nrobots\\ncollision avoidance\\nvehicles\\nstandards\\nreal-time systems\\nvehicle dynamics\\nautonomous aerial vehicles\\nhelicopters\\nhuman-robot interaction\\nimage capture\\nimage motion analysis\\ninteractive devices\\nmulti-robot systems\\nshear modulus\\ntelerobotics\\nmotion capture system\\nhuman operator\\nquadrotor microaerial vehicles\\nhardware experiments\\nvector fields\\njoystick\\nhuman user\\nvirtual rigid body abstraction\\nintuitive human-swarm interface\\nquadrotor swarm teleoperation\\nassistive collision avoidance\",\"147\":\"aircraft\\npayloads\\nrobot kinematics\\naerospace electronics\\nload modeling\\naerospace control\\naerospace components\\nautonomous aerial vehicles\\nmulti-robot systems\\nfixed-wing uav\\nlive-fly field experimentation capabilities\\nfixed-wing aerial robots\\nautonomous launch test\\nmulti-uav datasets\\nlow-cost components\\nopen-source components\\nmultirobot systems\\nflight test\\nlanding test\",\"148\":\"simultaneous localization and mapping\\noptimization\\nreal-time systems\\nlaser radar\\nbuildings\\nfeature extraction\\nupper bound\\ndata visualisation\\ndistance measurement\\noptical radar\\nslam (robots)\\ntree searching\\nreal-time loop closure\\n2d lidar slam\\nportable laser range-finders\\nas-built floor plans\\nfloor plans visualization\\nbackpack mapping platform\\nbranch-and-bound approach\",\"149\":\"simultaneous localization and mapping\\nuncertainty\\noptimization\\nfeature extraction\\nmeasurement uncertainty\\ntrajectory\\ncameras\\ngraph theory\\nimage colour analysis\\noptimisation\\nslam (robots)\\nspatial variables measurement\\nfeature-based rgb-d slam\\nspatial uncertainty modeling\\npose-graph optimization approach\\ndepth measurements\\nfactor graph optimization\\nrgb-d point features\\nuncertainty models\\nimage processing\",\"150\":\"simultaneous localization and mapping\\noptimization\\nsolid modeling\\ncameras\\ntracking\\nmotion segmentation\\nvisualization\\nexpectation-maximisation algorithm\\nimage colour analysis\\nimage segmentation\\nobject tracking\\nslam (robots)\\ncpa-slam system\\nconsistent plane-model alignment\\ndirect rgb-d slam\\nred-green-blue-depth simultaneous localization and mapping\\nframe-to-keyframe alignment\\nframe-to-plane alignment\\ndirect image alignment\\ndense image information\\nshort-term tracking\\nexpectation-maximization framework\",\"151\":\"simultaneous localization and mapping\\npipelines\\noptimization\\ncameras\\nthree-dimensional displays\\nestimation\\nkernel\\nslam (robots)\\nsemidense slam\\ncomparative design space exploration\\ndense slam\\nindividual kernels\\nparameter spaces\\nslambench framework\\nkinectfusion\\nlsd-slam\\nhardware design spaces\",\"152\":\"three-dimensional displays\\nsimultaneous localization and mapping\\nfeature extraction\\ncameras\\nsensor systems\\nbenchmark testing\\nimage registration\\nimage sensors\\nrobot vision\\nslam (robots)\\nspatial variables measurement\\nrgb-d slam system\\nrgb-d sensor\\ndepth measurement\\nregistration error\\nkeypoint extraction\",\"153\":\"cameras\\nsplines (mathematics)\\nsimultaneous localization and mapping\\nimage segmentation\\ntrajectory\\ngeometry\\nrobot vision systems\\nimage sequences\\noptimisation\\nslam (robots)\\nsmoothing methods\\nstereo image processing\\nsimultaneous localization and mapping system\\nsemidense slam system\\nrolling shutter cameras\\ncamera trajectory\\nb-spline curve\\nphotometric error optimisation\\ngeneralised epipolar geometry\\nsynthetic sequences\",\"154\":\"simultaneous localization and mapping\\nlaplace equations\\nmeasurement\\ntransmission line matrix methods\\nmaximum likelihood estimation\\nslam (robots)\\ntrees (mathematics)\\ntree-connectivity\\nslam graphical structure evaluation\\nweighted graph\\ncramer-rao lower bound\\ncrlb concept\\nd-optimality concept\\nspanning trees\\ngraph connectivity metric\\npose-graph slam datasets\",\"155\":\"image edge detection\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nstreaming media\\niterative closest point algorithm\\ncameras\\nedge detection\\nimage colour analysis\\niterative methods\\nmobile robots\\nrobot vision\\nslam (robots)\\nvideo streaming\\nfast depth edge detection\\nedge based rgb-d slam\\nrgb-d video streams\\nedge features\\nimage similarity\\ncomputation time\\ndepth component\\n3d point clouds\\nicp\\nback projected edge features\\npublicly available datasets\",\"156\":\"cameras\\nsimultaneous localization and mapping\\noptimization\\njacobian matrices\\nvisualization\\nimage reconstruction\\ntracking\\nmotion estimation\\nnonlinear programming\\nobject tracking\\nrobot vision\\nslam (robots)\\nvisual-inertial direct slam\\nsparse reconstructions\\nfeature-based algorithms\\nframe rate\\ncamera motion estimaiton\\nnonlinear optimization\\nsemidense map\\nhigh-gradient areas\\ncamera tracking\\nfully dense scene reconstruction\",\"157\":\"optimization\\nthree-dimensional displays\\ncameras\\nmathematical model\\nalgebra\\nsensors\\nrobots\\narchitecture\\noptimisation\\nstereo image processing\\ntown and country planning\\narchitectural constraints\\nlarge-scale mapping\\nlarge scale 3d reconstructions\\nurban reconstructions\\nsurvey construction\\npush broom laser\\nvisual odometry\\nvertical 2d scans\\nsalient architectural constraints\\n3d laser scanners\",\"158\":\"simultaneous localization and mapping\\nbandwidth\\nnavigation\\nmemory management\\nsparse matrices\\natmospheric measurements\\nmaximum likelihood estimation\\nminimisation\\nmobile robots\\nrobot vision\\nslam (robots)\\nkld\\nkullback-liebler divergence\\nminimization problem\\nsparsification pipeline\\nnode removal\\nresource constraint requirements\\nmap estimates\\nmaximum a posteriori estimates\\nincremental solvers\\nestimation problems\\nmobile robotics\\ngraphical methods\\ngraph slam\\nunified resource-constrained framework\",\"159\":\"hip\\nsprings\\nactuators\\nmathematical model\\nlegged locomotion\\noptimization\\nelastic constants\\nelasticity\\nelectric motors\\noptimal control\\noptimisation\\nsprings (mechanical)\\n2d monoped\\nparallel elastic actuator\\npea\\nhip elastic actuator\\nseries elastic actuator\\nsea\\nspring damping\\ndc electric motor models\\npositive motor work\\nelectrical losses\\npositive electrical work\\nmotor parameters\\nmotor stiffness\\nspring pre-compression terms\\ncosts of transport\\ncot\\nparallel hip configuration\\nseries leg configuration\",\"160\":\"manifolds\\nrobot kinematics\\nheuristic algorithms\\ntrajectory optimization\\nlegged locomotion\\nfriction\\nlinear quadratic control\\nmotion control\\npath planning\\nquadratic programming\\nrobot dynamics\\ntime-varying systems\\nconstrained dynamical systems\\ncontact constraints\\nrobotic tasks\\nwhole-body dynamic motion planning\\nwhole-body dynamic motion control\\ncomplex trajectory synthesis\\ncomplex trajectory stabilization\\nfully-actuated robot\\nunderactuated robot\\ntrajectory optimization algorithm\\ndircon\\ndirect collocation method\\nthird-order integration\\ntracking control\\ntime-varying linear quadratic regulator\\nmanifold tangent plane\\nquadratic program\\nunilateral friction\\ntorque constraints\\ncomplex walking locomotion\\nclimbing locomotion\",\"161\":\"mathematical model\\noptimization\\nlegged locomotion\\ntrajectory\\nrobot kinematics\\nfoot\\napproximation theory\\noptimisation\\nsimultaneous optimization\\nbipedal robots\\nenergy efficiency\\nplanar robot\\nhybrid zero dynamics control\\ntorsion spring\\nperiodic gaits\\ncomplex step derivative approximations\",\"162\":\"predictive control\\nelectric vehicles\\nprediction algorithms\\npublic transportation\\nrobots\\ndecision theory\\ndiscrete time systems\\ninteger programming\\nlinear programming\\nlyapunov methods\\nmobile robots\\nroad vehicles\\nscheduling\\nstability\\nvehicle routing\\nmodel predictive control\\nautonomous mobility-on-demand systems\\nvehicle scheduling\\nrobotic self-driving vehicles\\namod system discrete-time model\\nelectric vehicle charging constraints\\nlyapunov method\\noptimization\\nmixed integer linear program\\ndecision variables\\nbinary variables\",\"163\":\"heuristic algorithms\\noptimization\\nlinear programming\\nrobots\\nprobability distribution\\ncovariance matrices\\ninterpolation\\nconcave programming\\nevolutionary computation\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\nregression analysis\\nstatistical distributions\\nparameterized probability distribution\\npolicy parameter space\\ncurve closed segment\\nnonconvex optimization\\ngravity\\nunderactuated system balancing\\ndiscrete contacts\\ntask-policy parameter space mapping\\nregression model\\nautonomous robots\\nparameterized whole-body dynamic motor skill learning\\nevolutionary optimization\",\"164\":\"trajectory\\noptimal control\\ncost function\\nsystem dynamics\\nfeedforward neural networks\\npredictive control\\nrobots\\nfeedback\\nfeedforward\\nlinear quadratic control\\nnonlinear control systems\\ntrajectory optimisation (aerospace)\\nnonlinear model predictive control\\nunified trajectory optimization\\nunified trajectory tracking\\nfull-state feedback\\niterative optimal control algorithm\\nsequential linear quadratic\\nslq\\nmpc\\noptimal feedforward\\noptimal feedback\\nasctec firefly hexacopter\\nball balancing robot\\nrezero\",\"165\":\"torque\\nvoltage control\\noptimal control\\ntorque control\\nbrushless motors\\nsynchronous motors\\ncopper\\nangular velocity control\\nbrushless machines\\nfault tolerant control\\nfeedback\\nlinearisation techniques\\nmachine control\\nmaximum principle\\npermanent magnet motors\\nenergy-efficient controller\\nopen-circuited phase\\nvoltage-to-torque linearization\\noutput voltage saturation\\nnonconstant operational torque\\nmaximum principle formulation\\noptimal-linearization control\\noptimal control theory\\nmultiphase nonsinusoidal pm synchronous machines\\nfault-tolerant control\\noptimal feedback linearization control\",\"166\":\"cameras\\nrobot vision systems\\nface\\nnavigation\\ncomputer vision\\ngaze tracking\\nmarkov processes\\nmobile robots\\nsensors\\nvisual navigation task\\nvision system\\nactive sensing problem\\nmobile robot gaze direction optimization\\nmachine vision cameras\\npartially observable markov decision process\\npomdp\\nmutual information\\nmi\\nreward function\\nmonocular configuration\\nstereo configuration\",\"167\":\"trajectory\\noptimal control\\nentropy\\nvehicles\\nprediction algorithms\\nq measurement\\nstochastic processes\\ncontrol engineering computing\\ngraphics processing units\\nimportance sampling\\nnonlinear control systems\\npredictive control\\nroad traffic control\\nroad vehicles\\naggressive driving\\nmodel predictive path integral control\\nnonlinear systems optimization\\ncost criteria\\nstochastic optimal control framework\\ninformation theoretic notion\\nfree energy notion\\nrelative entropy notion\\nimportance sampling scheme\\ngraphics processing unit\\ngpu\\nfifth-scale auto-rally vehicle\",\"168\":\"trajectory\\noptimization\\nlyapunov methods\\nmonitoring\\nellipsoids\\nrobots\\naerospace electronics\\nclosed loop systems\\nfeedback\\nnonlinear dynamical systems\\noptimal control\\nquadratic programming\\nreachability analysis\\nstability\\nstate-space methods\\nreachability sets\\noptimal feedback controllers\\nsums-of-squares optimization\\nlocal lyapunov function\\nnonlinear dynamic system\\nlinear quadratic regulator\\nstate space\\nregion of attraction\\nroa\\nclosed-loop system\\nlqr stabilizer\\nconic optimization\\nquadratic optimization\\nmultidimensional state trajectory\\nroa distance evolution\\ninput saturation\\ntime delay\",\"169\":\"legged locomotion\\noptimization\\nhumanoid robots\\nthree-dimensional displays\\nreliability\\njacobian matrices\\nrobot dynamics\\ntrajectory control\\n3d dynamic walking\\nunderactuated humanoid robots\\ndirect collocation framework\\nhybrid zero dynamics framework\\nhzd framework\\nbipedal walking\\ngait design\\nvirtual constraints\\ngait definition\\nmulticontact robotic walking gaits\\noptimization problem\\ntrajectory optimization\\ndirect collocation context\\nanalytic jacobians generation\\ndurus humanoid robot\",\"170\":\"vehicles\\ndynamic programming\\nvehicle dynamics\\ncollision avoidance\\nsafety\\naircraft\\ncomputational modeling\\nautonomous aerial vehicles\\nunmanned aerial vehicles\\ncivil airspace\\ntrustworthy collision avoidance system\\nresolution logic\\nstatic parameters\\nsimulation-based approximate dynamic programming method\",\"171\":\"vehicles\\nrobot sensing systems\\nspace exploration\\nplanning\\nnavigation\\nthree-dimensional displays\\nautonomous aerial vehicles\\npath planning\\ntrees (mathematics)\\nreceding horizon\\nnext-best-view planner\\n3d exploration\\npath planning algorithm\\nautonomous exploration\\naerial robotic platforms\\nrandom tree\",\"172\":\"trajectory\\niris\\nplanning\\ncomputational modeling\\nmathematical model\\noptimization\\npropellers\\nautonomous aerial vehicles\\nhelicopters\\nindoor environment\\ninteger programming\\niterative methods\\npath planning\\naggressive quadrotor flight\\nsparse environments\\nnumerical complication\\nobstacle-dense environments\\niterative regional inflation by semidefinite programming algorithm\\nmixed-integer semidefinite programming\\nmodel-based control\\nmisdp\\nplanning algorithm\\nindoor environments\\ncubic meter volume\\ninterwoven strings\\ncontrol architecture\",\"173\":\"trajectory\\noctrees\\nrobot sensing systems\\nnavigation\\nsafety\\naerospace safety\\nautonomous aerial vehicles\\ncollision avoidance\\noptimisation\\nsensors\\nstate estimation\\nonline generation\\ncollision-free trajectories\\nunknown cluttered environments\\nautonomous quadrotor flight\\nquadrotor aerial robot\\noctree-based environment representation\\nonline navigation\\nmapping\\nonboard sensing\\nhigher order dynamical constraints\\nflight corridor safety\\noptimization-based method\\noverlapping 3d grids\\nfree-space flight corridor generation\\noctree data structure\\nonboard sensors\",\"174\":\"robot sensing systems\\ntrajectory\\nplanning\\ncollision avoidance\\nreal-time systems\\nnavigation\\nautonomous aerial vehicles\\nhelicopters\\npath planning\\ntrajectory control\\nhigh speed autonomous navigation\\nquadrotor microaerial vehicles\\ndual range planning horizon method\\nshort-range planner\\ntrajectory generation\\nstopping policy\\ntrajectory planning\\ngeometric information\\nobstacle detection\\ntrajectory execution\",\"175\":\"brushless dc motors\\npropellers\\nbatteries\\noptimal control\\nnumerical models\\nacceleration\\nautonomous aerial vehicles\\nbattery powered vehicles\\nenergy consumption\\nhelicopters\\nbattery-powered quadrotor uav\\nelectrical model\\nbrushless dc motor\\nminimum-energy paths\\noptimal control problem\\nangular accelerations\\nboundary states\\nminimum-time trajectories\\nminimum-control-effort trajectories\\nunmanned aerial vehicles\\ndji phantom 2 quadrotor\",\"176\":\"tracking\\nvisualization\\ncameras\\nmeasurement uncertainty\\nweight measurement\\nrobot sensing systems\\noptimization\\naircraft control\\nautonomous aerial vehicles\\nhelicopters\\ninertial systems\\nmobile robots\\noptimisation\\nprobability\\nrobot vision\\nsensor fusion\\nstate estimation\\nstatistical analysis\\nuncertain systems\\nopen-source ros package\\noptimization-based sensor fusion framework\\nprobabilistic fusion\\nmotion blur resistance\\ninverse depth uncertainty\\ncamera pose tracking\\nuncertainty-aware direct dense visual tracking module\\nreliable aggressive motion tracking\\ndense visual-inertial state estimator\\nangular velocity\\nlinear acceleration\\nlinear velocity\\nautonomous flight\\nfully integrated quadrotor system\\nsensing modality\\nimu\\nquadrotor aerial vehicle\\ndense visual-inertial fusion\\naggressive quadrotor flight\",\"177\":\"urban areas\\nwind speed\\ntrajectory\\ncomputational fluid dynamics\\ncomputational modeling\\nplanning\\naircraft control\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\npath planning\\ntrajectory control\\nwind\\nwind-aware trajectory\\nflight time\\nenergy expenditure\\nair speed\\nmit campus\\nminimum-energy trajectory\\npower consumption model\\nwind direction\\nprevailing wind speed\\nwind heading\\nrepresentative 3d environment model\\ncomputational fluid dynamics solver\\nwind model\\ntemporal variation\\nspatial variation\\nterrain\\nbuildings\\nturbulence\\nsynoptic wind\\nvehicle flight performance\\ncomplex urban wind field\\nurban environment\\nautonomy\\nagility\\nunmanned air vehicle\\nurban canopy layer\\nquadrotor flight\\nwind field exploitation\\nwind field estimation\",\"178\":\"position measurement\\nestimation\\nbayes methods\\nalgorithm design and analysis\\narea measurement\\nrobots\\nradiation effects\\nautonomous aerial vehicles\\ngaussian processes\\ngreedy algorithms\\ninference mechanisms\\nmixture models\\nfast radiation mapping\\nmultiple source localization\\ntopographic contour map\\nincremental density estimation\\nregion of interest\\nradiation sources\\ngreedy algorithm\\nmaximum radiation value\\nsource positions\\nroi selection\\nincremental variational bayes inference\\ngaussian mixtures\\nestimation accuracy\\nuav based exploration method\\nunmanned air vehicles\",\"179\":\"clustering algorithms\\nschedules\\nmathematical model\\nprobability density function\\nalgorithm design and analysis\\nsearch problems\\nautonomous aerial vehicles\\nboundary-value problems\\nmotion control\\npath planning\\ntree searching\\ngreedy method\\nnumerical simulations\\nhigh-utility-rate itp\\nglobal planning\\nbnb tree search\\nbranch and bound tree search\\ncluster service schedule\\nthermal time windows\\nmoving thermals\\nitp generation\\ninter-thermal path segment\\nboundary value problem\\nthermal exploration map\\ninformation gain\\ninformation clusters\\ncomplex target-search scenarios\\nsearch environment\\ninformation gathering capability\\nuav\\ngliding unmanned aerial vehicle\\nifs problem\\ndrifting thermals\\ninformative soaring\",\"180\":\"cameras\\nestimation\\nvehicle dynamics\\nvisualization\\naccelerometers\\nheuristic algorithms\\ngyroscopes\\ndistance measurement\\nhelicopters\\nimage fusion\\nkalman filters\\nmicrorobots\\nmobile robots\\nnonlinear filters\\nrobot dynamics\\nrobot vision\\nstate estimation\\nembedded computer\\ngyroscope biases\\naccelerometer\\nepipolar constraints\\nfeature tracking\\nvisual information\\ndynamic model\\nmonocular camera\\nmems inertial measurement unit\\ninformation fusion\\nstate estimation algorithm\\nekf\\nextended kalman filter\\nquadrotor microair vehicles\\nquadrotor microaerial vehicles\\nmodel-aided visual-inertial odometry system\\non-board visual-inertial odometry system\\nfrequency 200 hz\\nfrequency 50 hz\",\"181\":\"observers\\nvehicles\\nvelocity measurement\\naccelerometers\\nconvergence\\nrobots\\naerospace robotics\\nattitude control\\nlyapunov methods\\nmatrix algebra\\nmobile robots\\nstability\\nstate-space methods\\nvelocity aided attitude estimation\\naerial robotic vehicles\\nlatent rotation scaling\\nflight performance\\nstate estimation\\nonboard sensor system\\nsmall scale aerial robotic system\\nlinear velocity\\naerial platform\\nnonlinear observer\\nstate space relaxation\\nscaled rotation matrices\\npositive scalar\\nobserver dynamics\\nobserver state space\\nstraightforward lyapunov stability analysis\\nglobal asymptotic convergence\\nlocal exponential convergence\",\"182\":\"sensors\\nmathematical model\\nquaternions\\nkalman filters\\nalgebra\\nkinematics\\nmanifolds\\naerospace instrumentation\\ndistance measurement\\nhelicopters\\nlie algebras\\nlie groups\\nnonlinear filters\\nvisual inertial odometry\\nquadrotors\\nunscented kalman filter\\nlie group se(3)\\nnoise\\nlie algebra\\nstandard ukf formulation\\nstate uncertainty\\nparallel transport\\naffine connection\",\"183\":\"dynamics\\nmathematical model\\noptimization\\nlegged locomotion\\nstability analysis\\nhumanoid robots\\ngait analysis\\nmotion control\\nrobot kinematics\\nstability\\ndynamic humanoid walking\\nstable humanoid walking\\nfull-body motion\\ndynamic locomotion\\nhybrid zero dynamics\\ninverse-kinematic solver\\ndrc-hubo\\nhzd+ik solver\",\"184\":\"optimal control\\ncomputational modeling\\noptimization\\nlegged locomotion\\nfoot\\nmathematical model\\ndata models\\nmotion control\\noptimisation\\nfull-body 3d human gait synthesis\\noptimal control methods\\noffline human gait synthesis\\nrigid multibody systems\\ndirect multiple-shooting method\\nspace-time optimization problem\\nfull-body 3d human model\\nhuman motion-capture motions\",\"185\":\"foot\\nlegged locomotion\\nrobot kinematics\\nhip\\ntorque\\ncomputer architecture\\ncontrol system synthesis\\nelectric motors\\nenergy consumption\\nmotion control\\noptimisation\\nrobust control\\nvelocity control\\noffline controller design\\nfall prevention\\nspeed regulation\\nwalking controller\\nenergy use minimization\\nwalking speed\\nlimb motions\\nindividual joint motors\\nlower-level joint controller\\nstep-to-step motions\\nhigh-level balance controller\\nhierarchical controller structure\\nmodel-based optimization\\nreliable walking robot cornell ranger\",\"186\":\"legged locomotion\\nfoot\\nmathematical model\\ngenerators\\npredictive control\\ncollaboration\\nhumanoid robots\\nhuman-robot interaction\\nmotion control\\nphysical collaboration\\nhumanoid walking pattern generators\\nheavy objects\\nreduced model\\nmodel predictive control\\nhrp-4 humanoid robot\\nhumanoid walking\\nhuman-humanoid physical interaction\\nmodel-predictive control\",\"187\":\"robot kinematics\\nvisualization\\ncameras\\nthree-dimensional displays\\ncalibration\\nsolid modeling\\ncomputational complexity\\ngraphics processing units\\nmanipulators\\nparameter estimation\\nparticle filtering (numerical methods)\\nsensors\\nfocused online visual-motor coordination\\ndof dual-arm robot manipulator\\nvisual sensors\\nonline parameter estimation\\nrgb-d camera\\nrobot head\\nself-observation\\npoint cloud data\\n3d mesh models\\ngpu-based particle filtering method\\nreal-time algorithm\\ncomputation time\",\"188\":\"robot kinematics\\ndynamics\\nforce\\nlegged locomotion\\nhumanoid robots\\nmanipulator dynamics\\nmechanical stability\\nrobot dynamics\\nsynchronisation\\ntelerobotics\\nrobot-human balance state transfer\\nfull-body humanoid teleoperation\\ndivergent component-of-motion dynamics\\ndcm concept\\nrobot balance synchronization\\ncom dynamics\\ncenter-of-mass dynamics\\ncop dynamics\\ncenter-of-pressure dynamics\\nlipm\\nlinear inverted pendulum model\\nreaction mass\\nequilibrium point control assumption\\nmit hermes humanoid robot\",\"189\":\"robustness\\nrobots\\nfriction\\nip networks\\nforce\\napproximation algorithms\\nface\\nlegged locomotion\\nrobust control\\nrobust static equilibrium\\nlegged robots\\nlegged systems\\ncontact-force tracking\\nslippage\\ncontacts rotation\\nbounded force-tracking errors\\nequilibrium tests\\ncontact postures\\nreachability-based multicontact planner\",\"190\":\"optimization\\nkernel\\napproximation algorithms\\nrobustness\\nhumanoid robots\\ncasting\\nmechanical contact\\noptimisation\\nrobust control\\ncatmull-clark subdivision surface parametrization\\nposture generation\\nnonsingular map\\nsmooth map\\nray-casting algorithm\\ngradient-based optimization\\ncontact constraints\\nhumanoid robot\",\"191\":\"trajectory\\nplanning\\nhumanoid robots\\nkinematics\\nhistory\\nnavigation\\ndeformation\\npath planning\\ntrajectory control\\nwhole-body motion planning\\nnao humanoid robot\\ntask trajectory\\ndeformation mechanism\\nheuristic function\",\"192\":\"robots\\ncrowdsourcing\\ntraining\\nsemantics\\nsun\\nobject recognition\\ntraining data\\nimage classification\\nlearning (artificial intelligence)\\nrobot vision\\nneol\\nnever-ending object learning\\npersonal robots\\nsemantic hierarchy\\nbackground knowledge bases\\nobject classifiers\",\"193\":\"three-dimensional displays\\nfeature extraction\\nhistograms\\ntrajectory\\nrobustness\\nskeleton\\nshoulder\\ndata privacy\\ngabor filters\\nimage classification\\nimage motion analysis\\npose estimation\\nhold\\nlocal depth motion maps\\nl-dmm-based gabor representation\\ndiscriminative posture\\nmotion cues\\nskeletal joints\\ndepth features\\nclassification ability\\nhistogram of located displacements\\ninformative body parts\\nmotion analysis\\nprivacy protection\\ndepth information\\nhuman activity recognition\\ndepth data\\nskeletal data\\ninformative features\\ndaily activity recognition\",\"194\":\"image segmentation\\nrobots\\nsemantics\\nimage resolution\\ntraining\\nproposals\\nfeature extraction\\nimage classification\\nimage colour analysis\\nneural nets\\ndeep learning\\nhuman part discovery\\nhuman body part segmentation\\nrgb images\\nconvolutional neural networks\\ncnn\\nnetwork architecture\\ndeep convolutional network\\npascal parts dataset\\npart segmentation datasets\\nfreiburg sitting people dataset\\ndisaster dataset\\nunmanned aerial vehicle\",\"195\":\"hidden markov models\\nconvolution\\nrobot sensing systems\\naccelerometers\\nsurgery\\nrobot kinematics\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nmedical robotics\\nobject recognition\\nrobot vision\\ntime series\\nconvolutional action primitives learning\\nfine-grained action recognition\\nautomated skill assessment\\ntime series sequence\\nrobotics applications\\nhand-crafted features\\ndomain knowledge\\nlatent convolutional skip chain conditional random field\\nlc-sc-crf\\ncooking tasks\\naccelerometer data\\nuniversity of dundee 50 salads dataset\\nrobotic surgery training tasks\\nrobot kinematic data\\njhu-isi gesture and skill assessment working set\\njigsaws\\nintricate domain knowledge\",\"196\":\"speech\\ndelays\\nspeech recognition\\nreverberation\\nrobustness\\ndelay effects\\nmobile robots\\ncepstral analysis\\ngaussian processes\\nmixture models\\nrobust nonspeech discrimination\\npitch estimation\\nmel-frequency cepstrum coefficients\\ngaussian mixture models\\nmfcc-gmm\\nadditive noise\\nseparation system\\ntracking system\",\"197\":\"clothing\\nrobot sensing systems\\nfeature extraction\\nfabrics\\npattern recognition\\ndata acquisition\\ngrippers\\nlearning (artificial intelligence)\\nobject recognition\\ntactile sensors\\nyarn\\nmultisensorial garment recognition\\nexplorative garment recognition\\nmaterial properties\\nrobots\\nrgb-d sensors\\nphotometric stereo sensors\\nmachine learning techniques\\nfabric pattern\",\"198\":\"dictionaries\\nobject recognition\\nlinear programming\\nimage color analysis\\ntraining\\nlearning systems\\nclassification algorithms\\nfeature extraction\\nimage colour analysis\\noptimisation\\npattern classification\\ncaltech-256 dataset\\ndomain-adaptive dictionary learning\\ncross-domain discrepancy\\nclassifier levels\\nmultiple-domain inputs\\nrgb-d images\\ncross-domain learning\\ncolor object recognition\",\"199\":\"three-dimensional displays\\ndictionaries\\nfeature extraction\\ntrajectory\\ntraining\\nrobots\\nhistograms\\ngesture recognition\\nimage classification\\nimage representation\\nrobot vision\\nview action recognition\\ntransfer dictionary learning\\nsynthetic training data\\nhuman action recognition\\nreal world video\\nview-invariant sparse representation\\nview-invariant classifier\\nn-ucla datasets\\nixmas datasets\\nrobotic vision\",\"200\":\"feature extraction\\nthree-dimensional displays\\nhypercubes\\ntraining\\nimage color analysis\\nrobots\\nobject recognition\\ncategory theory\\ncomputer vision\\nconvolution\\nimage classification\\nimage colour analysis\\nimage fusion\\nimage representation\\nlearning (artificial intelligence)\\nneural nets\\nconvolutional hypercube pyramid\\nrgb-d object category\\ninstance recognition\\ndeep learning\\nrgb-d images\\ntraining data deficiency\\nmultimodality input dissimilarity\\nrgb-d object recognition\\npoint cloud data\\nconvolutional neural network\\ncnn\\ncoarse-to-fine feature representation\\nfusion scheme\\nclassification\\nextreme learning machines\\nelm\\nnonlinear classifiers\",\"201\":\"microstructure\\nmicroorganisms\\nmathematical model\\npropulsion\\nreal-time systems\\nstochastic processes\\nmicroscopy\\nmicrorobots\\nmobile robots\\nmobile microrobots\\nmicrostructure transport\\nswimming cells\\nrigid microstructure boundaries\\ndense bacterial bath\\nasymmetrically shaped microstructures\\ntranslational motion control\\nrotational motion control\\nreal-time automated exposure\\nvisual feedback\\nstochastic cell structure collisions\\nmicrorobot trajectories\\nmicrorobot design\\nmicrorobot control\",\"202\":\"focusing\\nreal-time systems\\nfeedback control\\ninstruction sets\\nposition measurement\\nimage processing\\nforce\\nchannel flow\\nfeedback\\nflow control\\nmicrofluidics\\nposition control\\nreal-time extraction\\nequilibrium positions\\ninertial microfluidics\\nlab-on-a-chip devices\\nhigh-throughput biological analysis\\nchannel geometry\\ninertial focusing devices\\nimage processing algorithm\\nreal-time measurement\\nhigh-speed vision system\",\"203\":\"embryo\\ndrag\\nforce\\nmicrochannels\\nmicroscopy\\nperformance evaluation\\nimage resolution\\nflow control\\ngenetics\\nmicrorobots\\nmotion control\\nrobot vision\\nzoology\\nmicrorobotic system\\nthree-dimensional cell rotation\\ncellular injection\\nmaterial extraction\\nin vitro fertilization\\nivf\\nintracytoplasmic sperm injection\\nicsi\\npreimplantation genetic diagnosis\\npgd\\nmanual cell manipulation\\nfluidic flow control\\ncomputer vision\\nzebrafish embryo orientation\",\"204\":\"microchannels\\ncameras\\nloading\\nmicroscopy\\nautomation\\nmonitoring\\nbiomedical monitoring\\nblood\\nflow control\\nmicrochannel flow\\nmicromanipulators\\nviscoelasticity\\nrobotic manipulation\\nmicrofluidic channel\\nhuman red blood cell\\ncell characterization\\ncell recovery response\\nhigh-frequency flow control\\nhigh-speed vision\\nsingle cell viscoelasticity\\non-chip active cell evaluation\\nlaunch operation\\nload operation\\ncatch operation\",\"205\":\"magnetization\\nshape\\ntemperature measurement\\nperpendicular magnetic anisotropy\\nmagnetic fields\\nmagnetic resonance imaging\\nmicrorobots\\nshape control\\nmagnetic microrobots\\naddressable shape control\\nshape shifting soft microrobots\\nself-folding hydrogel bilayer structures\\nuser-defined trajectories\\nrotating magnetic fields\\nmorphing\\nselective near infrared light exposure\",\"206\":\"three-dimensional displays\\nkinematics\\nmagnetosphere\\nangular velocity\\naerospace electronics\\ntrajectory\\ngravity\\nclosed loop systems\\nlinearisation techniques\\nmicrorobots\\nmobile robots\\nmotion control\\npath planning\\nservomechanisms\\ntrajectory control\\nclosed-loop control\\n3d path following\\nmagnetic helical microswimmer\\nerror kinematic model linearization\\nsideslip angle\\nattack angle\\nvisual servo control\\nhelix trajectory\\nsinusoidal trajectory\",\"207\":\"permanent magnets\\nmagnetic resonance imaging\\nmagnetic moments\\nforce\\nactuators\\npermanent magnet motors\\nmagnetic devices\\nelectromagnetic actuators\\nmagnetic fields\\nmagnetic variables control\\nmicrorobots\\nmagnetic control\\nmicrorobotic system\\npermanent magnet\\nelectromagnetic actuation system\\nmagnetic field strength\\ngradient strength\\nheat generation\",\"208\":\"force\\nmagnetic flux\\nprobes\\nmagnetic hysteresis\\nmagnetic levitation\\nmagnetic resonance imaging\\nmagnetic fields\\nbiocontrol\\nbiology computing\\nbiomagnetism\\nbiomechanics\\nbiomems\\ncellular biophysics\\nmagnetic particles\\noptical microscopy\\npermalloy\\nposition control\\nposition measurement\\ntrajectory control\\n3d-magnetic tweezer system\\nmagnetic pole positioning mechanism\\nmechanical properties\\nliving cells\\nmagnetically levitated microprobe\\nspecimen cells\\ninverted optical microscope\\n3d probe position measurement\\ncontrol-interface software\\nassembly error\\ngap distance\\nsextuple magnetic poles\\nsharpen-tipped permalloy rods\\ndriving coil\\npole positioning mechanism\\ntrajectory tracking\\nmagnetic particle\\ntarget trajectories\\nhomogeneity evaluation\\nforce generation\\nsize 4.5 mum\\nfeni\",\"209\":\"legged locomotion\\nfoot\\nmathematical model\\nforce\\ntrajectory\\ndigital signal processing\\nattitude control\\ncompliance control\\ncontrol system synthesis\\ndeformation\\nhumanoid robots\\nstability\\nhumanoid walking\\ncompliant soles\\nfoot sole compliance\\nshock absorption\\ntouchdown\\nground roughness\\nattitude stabilization\\nsole deformation estimator\\ncontroller design\\nstability requirement\\nhrp-4 humanoid robot\\nzmp stabilization\",\"210\":\"trajectory\\ndynamics\\nacceleration\\nlegged locomotion\\nplanning\\nmathematical model\\nhumanoid robots\\ninteger programming\\nmotion control\\npredictive control\\nstability\\ntime-varying systems\\nmodel predictive control\\ndynamic footstep adjustment\\nmotion divergent component\\nmpc schemes\\ntime-varying divergent component stabilization\\nstep positions\\nstep rotations\\nstable walking motions\\nrotation approximations\\nmixed-integer program\\ntime-varying dcm\\nescher humanoid\",\"211\":\"legged locomotion\\ndynamics\\nload modeling\\nanalytical models\\ntrajectory\\nstability analysis\\nmobile robots\\nnonlinear control systems\\nrobot dynamics\\nunified bipedal gait\\nunified bipedal walking\\nvirtual holonomic constraint\\npdac\\nconventional humanoids\\nindependent controllers\\nenergy efficiency\\ninherent controller\\npassive dynamic autonomous control\\ndamping and spring loaded inverted pendulum\\nd-slip model\\nhumanoid natural dynamics\\nvhc\",\"212\":\"legged locomotion\\nmuscles\\nrobot kinematics\\nrobot sensing systems\\nbiomechanics\\nbiological system modeling\\nbiomimetics\\nmotor synergies\\ndarwin-op biped robot\\nsynergistic control verification\\nwalking activity\\nactivation profiles\\nmechanical role\\nrobotic system\\nbioinspired controller\\nrobotic locomotion\\nsignal reconstruction\\nfunctional transformation\\nsensorimotor signals\\nbiomechanical output\\nlocomotor activation pattern\",\"213\":\"planning\\nrobots\\ncollision avoidance\\nimage edge detection\\nthree-dimensional displays\\npath planning\\nreal-time systems\\ncomputational geometry\\ncomputer graphics\\nhumanoid robots\\nimage segmentation\\nimage sensors\\nmicroprocessor chips\\nrobot vision\\nreal-time footstep planning\\ngeometric approach\\nfootstep planning systems\\nexternal sensors\\nexternal mapping\\ntraversability analysis\\ncomputationally expensive algorithms\\nonboard rgbd camera\\nplanar region segmentation\\nsingle cpu core\\ndynamic environments\\nonboard hardware\",\"214\":\"legged locomotion\\nfoot\\nencoding\\nstability analysis\\ntracking\\noutput feedback\\nasymptotic stability\\nrobot dynamics\\nstability\\nbipedal gait recharacterization\\nwalking encoding generalization\\nstable dynamic walking\\nexponentially stable periodic bipedal walking\\nasymmetric walking pattern\\njoint positions\\ngeneral walking encoding method\\nwalking pattern encoding\",\"215\":\"legged locomotion\\nrobot kinematics\\nhumanoid robots\\nhip\\nsprings\\nfoot\\ncompensation\\ndesign engineering\\nfeedback\\nfeedforward\\nrobot dynamics\\nbipedal humanoid robot\\ndynamic locomotion\\nregulator-based feedback control implementation\\nformal gait design\\ncontrol-informed mechanical design\\nphysical system\\nfeedforward control input\\nactive elements\\npassive elements\\nfull body dynamics\\ndynamic walking gait generation\\nformal framework\\nleg morphology\\ncontrol in the loop design\\nelectromechanical components\\ndurus\\nprototype humanoid robotics platform\\ndynamic walking behaviors\",\"216\":\"robot sensing systems\\nfoot\\nlegged locomotion\\nkinematics\\ncomputational modeling\\ncompensation\\ndistance measurement\\ngait analysis\\nhumanoid robots\\nlearning (artificial intelligence)\\nrobot vision\\nodometry learning\\nmobile robot localization\\nintegration errors\\nmechanical complexity\\nsystem uncertainties\\nfoot-ground contacts\\nvisual odometry\\nodometry drifting compensation\\nmachine learning\\nsmall-size low-cost humanoid robot\\nground conditions\\nodometry accuracy improvement\",\"217\":\"actuators\\nhip\\nknee\\ntorque\\nlegged locomotion\\nrobustness\\nhumanoid robots\\noptimisation\\nwalk-man humanoid lower body design optimization\\nenhanced physical performance\\nrobot deployment\\nphysical performance\\nphysical interactions\\nhumanoid walk-man\\ndisaster response scenarios\\ndesign optimization features\\nleg kinematic selection\\nleg structure\\nleg mass distribution\\nphysical robustness\\nelastic transmission\",\"218\":\"accelerometers\\ngyroscopes\\nrobot sensing systems\\nacceleration\\nangular velocity\\nvelocity measurement\\nacceleration measurement\\nangular velocity measurement\\ncalibration\\ndifferentiation\\nfeedback\\nhumanoid robots\\nhydraulic systems\\ninertial systems\\npotentiometers\\nsignal processing\\nstate estimation\\ntime-varying systems\\nsarcos hydraulic humanoid\\njoint feedback control\\njoint potentiometer signals\\nnumerical differentiation\\ntime-varying gyroscope biases\\njoint position measurements\\ngyroscope angular velocities\\ncalibration routines\\nlink imu poses\\nthree-axis accelerometer\\nthree-axis gyroscope\\nimu\\nlink-mounted inertial measurement units\\nhumanoid robot joint acceleration determination\\nhumanoid robot joint velocity determination\\ninertial sensor-based humanoid joint state estimation\",\"219\":\"correlation\\nbayes methods\\nsparse matrices\\ncovariance matrices\\nmagnetic sensors\\nsensor fusion\\ngaussian processes\\nimage representation\\nmarkov processes\\nrobot vision\\nslam (robots)\\nvectors\\ngaussian markov random field\\nsensor data fusion\\nprobabilistic framework\\nstate representation\\nsparse information matrix\\nmean state vector\\npipe wall thickness mapping\\nsimultaneous localization and mapping\\nslam\",\"220\":\"robot sensing systems\\nbayes methods\\nprobabilistic logic\\ncomputational modeling\\nreal-time systems\\nlaser radar\\narithmetic\\nimage fusion\\nmicrocontrollers\\nmobile robots\\nrobot vision\\nmultisensor fusion\\noccupancy grid\\nog\\ninteger arithmetic\\nprobabilistic representation\\nbayesian technique\\nevidence combination\\nmicrocontroller\\nmobile robot\",\"221\":\"robot sensing systems\\ndistance measurement\\ncalibration\\nclocks\\nstate estimation\\nprototypes\\nactuators\\ncables (mechanical)\\nkalman filters\\nmotion control\\nnonlinear filters\\nposition control\\nrobots\\nuncertain systems\\ncompliant robots\\nmass efficient systems\\nuncertain environments\\nfuture control algorithms\\nunscented kalman filter\\nukf\\ninertial measurements\\nultra wideband time-of-flight ranging measurements\\nactuator state information\\nsuperball\\ntensegrity based planetary exploration robotic prototype\\nglobal position\\nrolling maneuvers\\nsmall-amplitude deformations\\ncable actuation\",\"222\":\"market research\\nfeature extraction\\nthree-dimensional displays\\nrobustness\\nsensor phenomena and characterization\\nurban areas\\nimage registration\\nstatistical analysis\\ntown and country planning\\n3d-range registration\\n2d-image\\nscene categorization\\nstatistical similarity measurements\\nnormalized mutual information\\nnmi\\nurban sensing applications\\nrange scans\\nhigh-end range sensors\\n2d-3d attribute-pairs\\nstatistical registration\\nregistration accuracy\",\"223\":\"robot sensing systems\\nprobabilistic logic\\nsurface reconstruction\\nuncertainty\\nnoise measurement\\nsurface treatment\\ncomputational modeling\\ncomputational geometry\\nmeasurement uncertainty\\nprobability\\nsensor fusion\\nprobabilistic multisensor fusion\\nsigned distance functions\\n3d sensor measurements\\ntruncated signed distance functions\\nmeasurement noise\\nsurface modeling\\nrandom variables\\nexplicit estimation\\nspatial uncertainty\\ngpu\\nonline updates\\nnoisy sensor data\\nsurface estimation\\nuncertainty estimation\\nnoise characteristics\",\"224\":\"gyroscopes\\nmicromechanical devices\\nvelocity measurement\\nkalman filters\\nrobot sensing systems\\nactuators\\nestimation theory\\nhumanoid robots\\nvelocity control\\nmicroelectromechanical system\\nmems gyro network\\ndistributed gyroscope\\njoint velocity estimation\\nactuator velocity estimation\\nhydraulic humanoid robot\\nkalman filter\",\"225\":\"cameras\\nthree-dimensional displays\\ntracking\\noptimization\\nrobot vision systems\\nvisualization\\nlighting\\nmotion estimation\\nstereo image processing\\ndirect visual-inertial odometry\\nstereo cameras\\ncamera pose\\nsemidense depth maps\\nstatic stereo\\nfixed-baseline images\\ntemporal stereo\",\"226\":\"shape\\nmicroscopy\\ngravity\\npath planning\\nplanning\\ncollision avoidance\\ngrasping\\ndexterous manipulators\\nimage matching\\nmicromanipulators\\nobject detection\\nrobot vision\\nmicrohand positioning\\nobstacle avoidance\\ngrasping part\\ncell manipulation\\ncell observation\\nconvex hull\\nhu moment invariant\\ntemplate matching\\nautomatic stage system\\nmicroscopic field-of-view\\nxy stage\\nrotate stage\\nmicro hand\\nhu moment\\npotential method\",\"227\":\"fasteners\\ntorque\\nforce\\ndrag\\nthree-dimensional displays\\nmagnetic forces\\nshapes (structures)\\nstructural engineering\\nmagnetically folding multi-material structures\\n3d structures folding method\\nplanar microfabricated components\\nmagnetics\\nimpulsive force\\nscaling analysis\\nplanar-fabricated multimaterial fold-up cubes\\ndynamic simulation model\",\"228\":\"micromagnetics\\nmagnetic separation\\nsaturation magnetization\\ncontrollability\\nmagnetic moments\\nmagnetic tunneling\\nmagnetic resonance imaging\\ndrug delivery systems\\nflow control\\nlaminar flow\\nlinear quadratic control\\nmicrochannel flow\\nobservability\\noptimal control\\nmultiple magnetic microbeads\\nmicrofluidic channels\\ndrug targeting applications\\ntherapeutic magnetic microbeads\\nmagnetic gradient fields\\nobservability conditions\\ncontrollability conditions\\nlinear quadratic with integral action control\\nlqi\\nmillimeter-sized fluidic artery vessels\\ntrajectory constraints\\nvelocity constraints\\nlaminar viscous fluidic environment\",\"229\":\"barium\\nmagnetization\\nmagnetic moments\\nrobots\\nmagnetic resonance\\nmagnetic particles\\nmagnetoelasticity\\nfeedback\\nmagnetic fields\\nmicrorobots\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nindependent control\\nmillimeter-scale soft-bodied magnetic robotic swimmers\\nswimmer speed\\nactuation magnetic field\\nmagnetic moment\\nheading difference\\nactuation field strength\\nfeedback controller\\npositioning\\npath following\\ntype i sequential controller\\ntype ii parallel controller\\nsize 1.5 mm\\nsize 4.9 mm\\nsize 0.06 mm\",\"230\":\"magnetosphere\\nmagnetic moments\\nneedles\\nnumerical models\\nelectromagnetics\\nforce\\nmagnetic resonance imaging\\nelectrospinning\\niron compounds\\nmagnetic fields\\nmicroactuators\\nmicrorobots\\nsperm-shaped magnetic microrobots\\nvoltage\\nsolution concentration\\ndynamic viscosity\\ndistance\\nsyringe needle\\ncollector\\nbeading effect\\nsperm cells\\nultrafine fiber\\niron oxide nanoparticle\\nmagnetic dipole moment\\ndirectional control\\nmagnetic field\\nmagnetic torque\\nplanar flagellar waves\\nflagellated swim\\nelastohydrodynamics approach\\ntimoshenko-rayleigh beam theory\",\"231\":\"conferences\\nautomation\\nbiological techniques\\nbiology computing\\ncomputerised tomography\\ngeology\\nimage resolution\\nmanipulators\\nscanning electron microscopy\\nnanomanipulation integration\\nelectron microscope-computed tomography imaging system\\nsem-ct imaging system\\n3d nanomanipulation\\nnanorobotic manipulation system\\nbiological organism\\nreal-time high resolution sem observation\\ntwo dimensional information\\nsurficial information\\n3d space\\nsem observation\\ncaenorhabditis elegans\\nbiological sample\\nc. elegans\\nbrass materials\\ncopper materials\\nnanoinjector\",\"232\":\"three-dimensional displays\\nsolid modeling\\nrobustness\\nplanning\\nmeasurement\\nforce\\nfriction\\ncloud computing\\ncontrol engineering computing\\nconvolution\\ndata analysis\\nimage classification\\nlearning systems\\nneural nets\\npath planning\\nrobot vision\\ndex-net 1.0\\ndexterity network 1.0\\ncloud-based network\\n3d object models\\ndataset\\nrobust grasp planning\\nmulti-armed bandit model\\ncorrelated rewards\\ncloud robotics\\nparallel-jaw grasps\\nforce closure\\nmultiview convolutional neural networks\\nmv-cnn\\nsimilarity metrics\\ndeep learning method\\nobject classification\",\"233\":\"robots\\nvisualization\\ngrasping\\nthumb\\nwrist\\nrobustness\\ngaze tracking\\ngrippers\\nrobot vision\\nvisual cues\\nfront-orthogonal views\\nside-orthogonal views\\nrobot hand grasping evaluation\\neye-tracking\\nground truth\\ngrasp predictions\\nfalse positives\\nobject center-line\\nobject top\\nrobot finger location\\nrobot wrist location\\nrobot arm location\\nvisual patterns\\nleft images\\nright images\\nobject location\\nobject shape\\ntransition matrices\",\"234\":\"force\\ngrasping\\nrobot sensing systems\\nelectromyography\\nvibrations\\ndexterous manipulators\\nmechanical variables control\\nsensors\\nreflex control\\npisa\\/iit softhand\\nobject slippage\\ngrasp robustness\\ncurrent control mode\\npose control mode\\nimpedance control mode\\nthimblesense fingertip sensors\\nslippage detection\\nslippage control\\nimpedance reflex mode\\nseven degrees-of-freedom robotic arm\\nprosthetic applications\",\"235\":\"topology\\ngrasping\\ncomputational modeling\\nthree-dimensional displays\\nshape\\nanalytical models\\nports (computers)\\ngraph theory\\ngrippers\\nobject recognition\\nrope caging\\nstretching ropes\\n3d object\\nreeb graph\",\"236\":\"handover\\nforce\\nrobot sensing systems\\nacceleration\\nrobot kinematics\\ngrippers\\nmanipulators\\nfail-safe object handover controller\\nrobot-human handover\\nobject acceleration\\nregrasping mechanism\\nacceleration sensing setup\\nrobot gripper\\nbaxter research robot\",\"237\":\"thumb\\ngrasping\\nelectron tubes\\nforce\\ntorque\\nactuators\\ndexterous manipulators\\npipes\\npneumatic actuators\\npolymer foams\\ndoor opening mission\\naerial manipulator\\nforce-weight ratio\\ndistributed grasping force\\ndesign parameter\\none degree of freedom motion\\nnotch\\nsoft polyurethane foam\\nzigzag tube\\nindex finger\\nunderactuated structure\\nshaped object grasping\\nlightweight underactuated pneumatic finger actuator\",\"238\":\"three-dimensional displays\\nrobot sensing systems\\nrobot kinematics\\nrehabilitation robotics\\nshape\\ngrasping\\nassisted living\\nmanipulators\\nservice robots\\ntelerobotics\\nrobotic grasp detection\\nassistive robotic manipulation\\nraw point cloud depth data\\ngeometric approach\\ngeometric shape primitives\\nlocal surface properties\\nshape primitive categories\\nrobot teleoperation\\nassistive robotic arm\",\"239\":\"grasping\\nmanifolds\\nkinematics\\nrobots\\noptimization\\nencoding\\nplanning\\ndexterous manipulators\\ngrippers\\nmanipulator dynamics\\nmanipulator kinematics\\noptimisation\\nreachability analysis\\nkuka-kr5 arm\\nrandom forest learning\\ncanny grasp quality metric\\noffline optimization module\\nonline execution module\\nmultifingered inverse kinematics\\nhand reachability\\ndexterous in-hand manipulation\\nfingertip grasp planning\\nfingertip grasping manifold\",\"240\":\"force\\nrobots\\nresistance\\nkinematics\\njacobian matrices\\ngrasping\\nfriction\\ndexterous manipulators\\ngrippers\\nmanipulator kinematics\\nmechanical contact\\nhand mechanism structure\\nresistant contact forces\\ninternal contact forces\\nactive forces\\npassive forces\\nfinger contact forces\\ngrasp security measures\\nhand mechanism kinematics\\nwrench resistance\\nkinematic properties\\nfingertip bodies\\nrobotic grasping\\nwrench resistant multifinger hand mechanisms\",\"241\":\"robots\\ngrasping\\nvisualization\\ncomputer architecture\\ncomputational modeling\\nneural networks\\nreal-time systems\\ncontrol engineering computing\\nfeedforward neural nets\\ngraphics processing units\\nimage colour analysis\\nmanipulators\\nrobot vision\\nobject discovery\\ngrasp detection\\nshared convolutional neural network\\ngrasp configuration\\ncnn\\ngpu\\nlabeled rgbd dataset\\nrobotic grasping\",\"242\":\"grasping\\nsensors\\nsolid modeling\\ngrippers\\ncameras\\nprobabilistic logic\\nrobots\\nindustrial robots\\nlogistics\\nprecision engineering\\nset theory\\nprobabilistic models\\nunknown objects\\nreliable precision grasping\\nmanufacturing tasks\\nhousehold tasks\\nlogistic tasks\\nobject property estimation\\nviable grasps\\ngrasp execution\\ntwo-stage grasp generation method\\nhard-to-grasp object set\",\"243\":\"robot sensing systems\\ndynamics\\nforce\\nrobot kinematics\\nkinematics\\nforce control\\nforce sensors\\nmedical robotics\\nrobot dynamics\\nsurgery\\nintrinsic wrench sensing model\\nmultibackbone continuum robots\\nactuation load\\nstatics models\\nforce sensing models\\ndynamic forces\\nlagrangian formulation\\nwrench estimation model\\nsingle-segment continuum robot\\ndistal dexterity\",\"244\":\"engines\\nrobots\\nfriction\\nsolid modeling\\ncomputational modeling\\ndynamics\\ncomputer games\\ndata visualisation\\ndigital simulation\\nintegration\\nnumerical analysis\\nrwpe\\ntight itting assembly operations\\ncontact-force resolution problem\\nsecond order integration method\\nfirst order numerical integration\\nrobworkphysicsengine\\nrobotics\\nanimation\\nphysics engines\\ndynamic simulation engine\",\"245\":\"trajectory optimization\\nmathematical model\\nassistive devices\\nlegged locomotion\\ndynamics\\nhandicapped aids\\nhuman-robot interaction\\nmedical robotics\\nmotion control\\noptimisation\\ntrajectory control\\ntrajectory optimization formulation\\nassistive robotic devices\\nsit-to-stand assistive device\\nmotion impairments\\nhumans\\nconstrained lagrangian\\nmotion equations\\njoint reaction wrenches\\nmotion impairment\\nactuation wrenches\\nvertical motion\",\"246\":\"dynamics\\ngravity\\nmathematical model\\nrobot sensing systems\\nsymmetric matrices\\nnumerical models\\nmanipulator dynamics\\nnewton method\\nnonlinear programming\\nrobot parameter extraction\\ndynamic coefficients\\nnonlinear optimization method\\nnumerical parameters\\nrobot dynamics\\nmodel-based control laws\\nrecursive newton-euler algorithms\\nglobal optimization techniques\\nidentification process\\nparameter extraction process\\nkuka lwr iv+ robot\\nlink dynamics\",\"247\":\"legged locomotion\\nsprings\\nforce\\nfoot\\ngeometry\\ntrajectory\\nrobot dynamics\\nsprings (mechanical)\\naiming\\nvaulting\\nspider inspired leaping\\njumping robots\\njumping spiders\\nfront legs\\nrear legs\\nreal spider anatomy\\nleg geometry\\njumping motion\\nsimulations\\ndynamic analysis\\nspider jumping mechanism\\nflatter take-off trajectories\\nspring-release mechanism\",\"248\":\"friction\\nforce\\nmathematical model\\nrobots\\ngears\\ntorque\\ndynamics\\nmanipulator dynamics\\nfast forward dynamics simulation\\nrobot manipulators\\nhighly frictional gears\\njoint friction torques\\nforward dynamics simulation\\nfriction inside mechanical gears\\ninternal friction torques\\nexternal contact forces\\nnet joint actuation torque\",\"249\":\"kinematics\\npulleys\\nmanipulators\\nwires\\ntorque\\npower cables\\ndesign engineering\\nend effectors\\nmanipulator kinematics\\nmechanical strength\\ninverse kinematics\\nrobot arm design\\nsix-dof handheld robot arm\\ndegrees-of-freedom\\ncable driven manipulator\\nhandheld robotic tasks\\ncoupled tendon approach\\nmovement speed\\nconfiguration space\\narm size reduction\\nspace carving approach\\nstructural strength\\njoint limits\\nlink mass minimization\\nend effector pose\\nhandheld robot design\",\"250\":\"kinematics\\nwheels\\nacceleration\\ntrajectory\\nmobile robots\\ndamping\\nacceleration control\\nactuators\\nrobot dynamics\\nrobot kinematics\\nsteering systems\\ntrajectory control\\nvelocity control\\nkinematic modelling\\nsingularity treatment\\nwheeled mobile robot\\nsteer joint configuration\\nnonholonomic omnidirectional mobile robot\\nthree-dimensional trajectory\\nmotion plane\\nvelocity space\\nactuator acceleration limit\\nsteer rate damping behavior\\nneobotix-mpo700 mobile robot\\nwheeled mobile robots\\nsteerable wheels\",\"251\":\"prototypes\\nrobot kinematics\\nducts\\npower cables\\nsprings\\nhardware\\nmaintenance engineering\\nmicrorobots\\nmobile robots\\nmotion control\\nrobust control\\nsynchronisation\\nsecond generation prototype\\nducttv2\\nduct exploration\\nduct maintenance\\nagile robots\\nirregular duct systems\\ncomplex duct systems\\nduct climbing tetrahedral tensegrity robot\\nactuator power\\ncable routing\\ncompliance\\nsynchronized control\\nt-junction navigation\\nsharp corner navigation\\nrobust motion control\",\"252\":\"kinematics\\njacobian matrices\\nredundancy\\nmanipulators\\nactuators\\nnonhomogeneous media\\nredundant manipulators\\npower manipulability analysis\\nredundant actuated parallel kinematic manipulators\\nspatial object manipulation\\npkm\\ninhomogeneous jacobian matrices\\ntranslational 3-rpc manipulator\",\"253\":\"electron tubes\\nspirals\\nrobot kinematics\\ngears\\nservice robots\\nmanipulators\\naircraft\\ncontrol system synthesis\\nmobile robots\\nflying vehicles\\nspherical robot configuration\\n3dof robot arm\\nprismatic joint\\nspiral zipper\\nspherical robot arm\",\"254\":\"robots\\nknowledge representation\\ncognition\\nstochastic processes\\ngrammar\\nplanning\\nfeedback control\\ndata structures\\nfeedback\\ngraph theory\\nintelligent robots\\nlearning systems\\nrobot learning\\nspatial and-or graph\\ntemporal and-or graph\\ncausal and-or graph\\nstochastic graph-based framework\\naction planning\\nhierarchical data structure\\nspatial knowledge\\ntemporal knowledge\\ncausal knowledge\\nhuman demonstrations\\nrobotic platforms\\ncloth-folding task\",\"255\":\"training\\nrobustness\\nrobots\\nneural networks\\nimage recognition\\nthree-dimensional displays\\ndatabases\\nimage matching\\nlearning (artificial intelligence)\\nneural nets\\nobject recognition\\nrobot vision\\nrobust single-view instance recognition\\nrobot interaction\\nobject instance recognition\\nrobot viewing angle\\nneural network training\\nviewpoint changes\\ntraining procedure\\nkeypoint matching\\ntemplate matching\\nsparse coding\",\"256\":\"measurement\\ntime series analysis\\ncorrelation\\nneural networks\\nlinear programming\\nfeature extraction\\nlearning systems\\ndata analysis\\nhuman factors\\nlearning (artificial intelligence)\\nneural nets\\ndeep metric learning autoencoder\\nnonlinear temporal alignment\\nhuman action recognition\\nhuman motion\\nspatiotemporal features\\nnonlinear time alignment method\\nneural network\\nk-nearest neighbor classifier\\nk-nn classifier\\naction 3d datasets\\ndaily activity 3d datasets\\nmsr\",\"257\":\"grasping\\nstandards\\nrobot sensing systems\\nnoise measurement\\ntraining\\nthree-dimensional displays\\ngradient methods\\nlearning systems\\nrobots\\ntop grasp hypothesis\\nrobotic grasping\\nnoisy sensor data\\npartial point cloud\\nsupervised learning method\\ntop-ranked training grasp\\nstandard ranking loss\\nbinary labels\\nstochastic gradient descent\",\"258\":\"three-dimensional displays\\nimage segmentation\\nsolid modeling\\nrobots\\nlayout\\ntraining\\nnavigation\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nneurocontrollers\\nrandom processes\\nrobot vision\\nfinally\\nreal-time 3d scene layout\\nsupervised convolutional neural networks\\nindoor corridor scenes\\nobstacles identification\\nrobot navigation\\nreal-world corridor scenes\\nsingle-image methods\\nmanhattan-world assumptions\\nmachine learning\\ngeometric modelling\\n3d model\\ncnn\\ngeometric class labelling\\nconditional random field\\ncrf\\nline segments\\nwall-ground boundaries\\ngeometric constraints\",\"259\":\"hamming distance\\nimage matching\\ncalibration\\nbenchmark testing\\nvisualization\\nobject recognition\\nlocal binary descriptors matching\\npairwise tests-based local binary descriptors\\nlocality sensitive hashing\\nlsh scheme\\nkendall tau rank distance\\nimage patches\\nfeature-based matching problems\\ncalibration method\\nimage matching scenario\\nmotion benchmarks\\nvisual odometry\\nobject recognition problems\\nwhole descriptor\\nspeeding-up matching\",\"260\":\"proposals\\ntraining\\nlaser radar\\nneural networks\\nfeature extraction\\ncomputer architecture\\nrobot sensing systems\\nimage colour analysis\\nimage fusion\\nimage representation\\nimage sensors\\nneural nets\\nobject detection\\noptical radar\\npedestrians\\ntraffic engineering computing\\nlidar image fusion\\npedestrian detection\\nconvolutional neural networks\\ncnn\\ndepth map\\n3d scene\\nhorizontal disparity\\nheight above ground\\nmicrosoft kinect depth maps\\nup-sampled lidar data\\nfeature representations\\nrgb image channels\\nkitti pedestrian detection dataset\\nregion-cnn\\nr-cnn\\nhha features\\nrgb web data\\nmid-level feature representation\\nobject class decision\",\"261\":\"visualization\\ntracking\\nfeature extraction\\ntraining\\nadaptation models\\ndata models\\nimage color analysis\\nimage sequences\\nlearning (artificial intelligence)\\nobject detection\\nobject tracking\\nvideo signal processing\\nalextrac\\naffinity learning\\ntemporal reinforcement\\nassociation chains\\nself-supervised approach\\nobject detections\\nvideo sequence\\ntracking-by-detection systems\\naffinity model\\ndata association cost estimation\\nvideo data sequence\\nvisual appearance\\ntemporal windows\\nsimilarity measures\\ncosine similarity\\nmot benchmark\\nmultiple object tracking accuracy\\nmota\",\"262\":\"robot sensing systems\\nsurface texture\\ntraining\\nsupport vector machines\\nsurface impedance\\nlearning systems\\ndexterous manipulators\\nhaptic interfaces\\nlearning (artificial intelligence)\\nprior tactile experience\\nrobotic hands\\nin-hand objects discrimination\\nonline tactile transfer learning strategy\\nsurface texture properties\\nrobotic hand\\nartificial robotic skin\\ntactile knowledge transfer\",\"263\":\"estimation\\nthree-dimensional displays\\ntime measurement\\nmathematical model\\nprobabilistic logic\\nrobots\\nobject recognition\\nprobability\\nrecursive estimation\\nprobabilistic surface normal estimator efficiency\\nsurface normal direction\\noccupancy information\\ngrid representation\\nbatch formulations\\nrecursive formulations\\npoint cloud library\\nrobotics\",\"264\":\"three-dimensional displays\\nimage reconstruction\\nsemantics\\nlabeling\\nproposals\\nfeature extraction\\nstreaming media\\nimage colour analysis\\ninference mechanisms\\nobject recognition\\nprobability\\ndense scene reconstruction\\nobject labeling\\nrgb-d sensor\\nspatio-temporally consistent object proposal tracking\\ninference algorithm\\nobject class probability\\nvoxel-based prediction hypothesis\\nrgb-d object\\nlive video stream\\nmicrosoft kinect\\n3d scene reconstruction\",\"265\":\"shape\\nthree-dimensional displays\\nfeature extraction\\ncolor\\nrobots\\ncognitive science\\nreliability\\ngraph theory\\nimage classification\\nimage colour analysis\\nimage segmentation\\nimage sensors\\nimage texture\\nobject detection\\nrobot vision\\nunder-segmentation rate\\nhuman-annotated object instances\\ngeometry domain\\ntexture domain\\nclassifier ensemble\\nconvex-shaped head\\nconvex-shaped arms\\nteddy bear\\nprimitive-shaped objects\\ncomplex-shaped objects\\nprimitive-shaped object candidates\\nover-segmentation\\nunstructured scenes\\nkinect-like sensor\\nrgb-d point clouds\\nunknown object discovery\\nunstructured environments\\nnonprimitive-shaped objects\\nhierarchical graph-based discovery\",\"266\":\"image segmentation\\nthree-dimensional displays\\nimage edge detection\\nshape\\nrobots\\nobject segmentation\\nsurface treatment\\ncurve fitting\\nfeature extraction\\nimage matching\\nobject detection\\nrobot vision\\ntabletop scenes\\n3d pointcloud scans\\nsurface normal edge curve extraction\\nsurface normal edge curve matching\\n3d bilateral symmetries\\npartial 3d pointclouds\\nunknown object segmentation\\nnonconvex object segmentation\\nsymmetry constraint\\ncluttered scene segmentation\",\"267\":\"feature extraction\\ndetectors\\nservice robots\\nrobot sensing systems\\nvisualization\\nproposals\\nneural networks\\nfeedforward neural nets\\nimage sensors\\nobject recognition\\nrobot vision\\npart-based room categorization\\nhousehold service robots\\npreviously-unseen home environment\\nvisual sensor\\nobject-agnostic region proposals\\nconvolutional neural network\\ncnn\\ncategory-specific discriminative parts\\npart-based model\\nplace recognition\\nimage degradation\\nimage scaling\\naspect changes\\nplace categorization\\nannotation ambiguities\",\"268\":\"three-dimensional displays\\nsolid modeling\\ncomputational modeling\\nobject recognition\\nimage reconstruction\\ncameras\\nsimultaneous localization and mapping\\nimage recognition\\nimage segmentation\\nobject detection\\npose estimation\\nrobot vision\\nslam (robots)\\nsimultaneous reconstruction segmentation and recognition\\nslam\\n3d object recognition\\nperforming object recognition\",\"269\":\"robots\\nfeature extraction\\nvisualization\\nimage segmentation\\nestimation\\nerror analysis\\ncontext\\nenvironmental factors\\ngeometry\\nimage sequences\\nlearning (artificial intelligence)\\nobject detection\\nvisual saliency learning\\nenvironment exploration mechanism\\ntask-specific visual saliency\\ngeometrical criterion\\nintrinsic motivation\\nrgb-d images sequences\\nindoor object detection\\nsalient elements\",\"270\":\"image segmentation\\nmotion segmentation\\nimage color analysis\\nthree-dimensional displays\\noptical imaging\\ncameras\\ncomputational modeling\\napproximation theory\\ngraph theory\\nimage colour analysis\\nimage motion analysis\\nimage sequences\\npattern clustering\\nvideo signal processing\\nobject-based segmentation\\nrgbd video\\nspatio-temporal cues\\nstructural scene properties\\nsemantic scene properties\\ncomputer vision\\nspatio-temporal video segmentation\\nsuperpixel segmentation\\nlow-level appearance features\\nhigh-level appearance features\\ndeformable parts model appearance features\\nmotion\\noptical flow\\ngraph laplacian\\ndepth cues\\nsimilarity metric\\nobject boundaries\\ndepth disparities\\nspectral clustering\\nspatio-temporal graphs\\nnystr\\u00f6m approximation strategy\\nspatio-temporal clustering\\nlocal connectivity patterns\",\"271\":\"semantics\\ntraining\\nimage segmentation\\nrobots\\nsun\\nmachine learning\\nneural networks\\nlearning (artificial intelligence)\\nneurocontrollers\\nobject detection\\nrobot vision\\nunderstand scene categories\\nsemantic regularized scene classifier\\nconvolutional neural networks\\nrobotics\\nmachine learning technique\\nrobotics applications\\niconic object images\\nobject knowledge\\nobject level information\\nsemantic segmentation\",\"272\":\"feature extraction\\nconvolution\\ntraining\\nneural networks\\nbenchmark testing\\ndetectors\\ncontext\\nfeedforward neural nets\\nimage sensors\\nlearning (artificial intelligence)\\nobject detection\\npedestrians\\nfar distance\\nsmall scale pedestrian detection\\ncamera\\nconvolutional neural network based method\\nfeature learning\\nend-to-end approach\\ncaltech pedestrian detection benchmark\\nlog-average miss rate\",\"273\":\"calibration\\ncameras\\nrobot vision systems\\nrobot kinematics\\nreal-time systems\\nobject tracking\\nobservability\\npose estimation\\nrobot vision\\nintegrated online robot-camera calibration\\nobject pose estimation\\nhand-eye calibration\\nreal-time model-based tracking approach\\noff-line calibration\\nbundle adjustment techniques\\nobservability criterion\\nframe calibration\",\"274\":\"robot sensing systems\\npayloads\\nobservers\\nservice robots\\nexoskeletons\\nrobot dynamics\\npassivity-based nonlinear disturbance observer\\nunknown payload carrying applications\\npowered upper-limb robot control\\ndob designs\\npassivity property\\nhuman operator\\nenvironmental interactions\\ncontrol loop\\ndisturbance observation property\\nnominal model\\ntuning rule\",\"275\":\"robot kinematics\\nforce\\nfriction\\nactuators\\nlegged locomotion\\ndynamics\\nmechanical contact\\nmotion control\\noptimisation\\nconstrained robot control\\nfriction cone constraints\\nunilateral contacts\\nlinear inequality constraints\\nquadratic inequality constraints\\nphysical constraints\\nactuator saturation limits\\nunactuated joints\\nactuation effort minimization\\nredundant systems\\nminimal-order dynamics model\\ncontact switching\\nforce vector yields\\nprimary control inputs\\nsecondary control inputs\\ninteraction control\\nactuation effort problem minimization\\nactuation limitations\\noptimization programming\",\"276\":\"trajectory\\ncollision avoidance\\nservice robots\\nmanipulator dynamics\\noptimization\\nindustrial manipulators\\noptimisation\\npredictive control\\nindustrial robots\\nminimum-time control\\nhierarchical approach\\nhierarchical optimization problem\\nmodel predictive control\\nreactive system behavior\\ncollision avoidance constraints\\nhigh frequency chattering\\nscara robots\\nindustrial manipulator\",\"277\":\"manipulator dynamics\\ndynamics\\nkinematics\\nforce\\nmobile communication\\ncollision avoidance\\ndecentralised control\\nend effectors\\nforce control\\nmobile robots\\nredundant manipulators\\nvariable structure systems\\nvelocity control\\ncooperative redundant omnidirectional mobile manipulators\\nmodel-free decentralized integral sliding modes\\npassive velocity fields\\nrobotic system\\nomnidirectional mobile base\\ndecentralized cooperative control\\nk-omm\\nhomogeneous holonomic constraints\\nchatterless integral sliding mode\\nforce-position control\\nexponential tracking\\nvelocity deviation\\nobject mobility\\npassive smooth velocity fields\\nobstacles avoidance\\ninternal forces\\nobject grasp\\nend-effector position\\n8-dof kuka youbot omm system\",\"278\":\"torque\\nservomotors\\nmanipulators\\nservice robots\\nconvergence\\nadaptive control\\ncontrol system synthesis\\nindustrial manipulators\\niterative learning control\\nrobust control\\nrobust two-degree-of-freedom iterative learning control\\nindustrial robot manipulators\\nilc\\nservo flexibility compensation\\nrobust synthesis method\",\"279\":\"wheels\\nmobile communication\\nmobile robots\\nvehicle dynamics\\nuncertainty\\ndynamics\\nvehicles\\nadaptive control\\ncollision avoidance\\nfault tolerant control\\nfriction\\nrobot dynamics\\nrobust control\\nuncertain systems\\nomnidirectional mobile platforms\\n4 mecanum wheels\\nfault tolerant control problem\\nconstrained workspace\\nstatic obstacles\\nflat workspace\\ndrive shaft\\nrobust motion control scheme\\noperational workspace\\nworkspace boundaries\\ndynamic model uncertainties\\ndynamic model uncertainty\\nnavigation functions\\nadaptive control techniques\\nrobust motion planning\\nsecond order dynamics\\nparametric uncertainty\",\"280\":\"force\\ntorque\\nrobot sensing systems\\nneural networks\\nmanipulators\\ncontrol engineering computing\\nforce control\\nneural nets\\npath planning\\nposition control\\nsoftware agents\\ntorque control\\nsoftbot\\nsoftware-based lead-through\\nservo robot\\ninteractive control method\\nrigid robotics\\nneural network classifier\\nposition mapping\\ntorque reading\\nforce direction\\nmanipulator\",\"281\":\"robots\\ntorque\\nimpedance\\ntorque control\\nservomotors\\nvalves\\ndynamics\\nfeedback\\nhydraulic actuators\\nrobot dynamics\\nrobust control\\nthree-term control\\nvelocity control\\nmodel-free joint torque control\\nnatural velocity feedback effect\\nmechanical impedance\\ninternal-loop control\\nexternal-loop control\\n2 degrees-of-freedom\\njoint torque tracking\\nproportional-integral-derivative control\\nrobustness\\n2dof control\\ndisturbance observer form\\n1dof rotary hydraulic robot joint\",\"282\":\"torque\\nrotors\\ntorque measurement\\ninduction motors\\nwheels\\nforce\\nrobot sensing systems\\ncontrollability\\nmechatronics\\nmobile robots\\nsensors\\ntorque evaluation method\\nspherical motors\\nsix-axis force\\/torque sensor\\nmultidegrees-of-freedom actuators\\noutput torque\\nmechatronic systems\\nangular velocity\\noff-axis simultaneous measurement\\nrotor driver\\nomnidirectional mobile robots\\nspherical induction motor\",\"283\":\"communication channels\\ncomputer architecture\\nrobots\\nimpedance\\ndelays\\ncouplings\\nforce\\nrobot dynamics\\ntelerobotics\\ntransparency oriented wave-based bilateral teleoperation architecture\\nwave variables\\ncommunication delay\\nwave-based communication channel dynamics\\ncomplex architecture\\nempirical analysis\",\"284\":\"agriculture\\ncameras\\nthree-dimensional displays\\nrobot vision systems\\nmanipulators\\nimage color analysis\\ncrops\\nimage colour analysis\\nimage segmentation\\nleast squares approximations\\noptimisation\\npose estimation\\nautomated crop harvesting\\n6dof pose estimation\\nsweet-pepper crops\\nautonomous harvesting\\nrobotic manipulator\\nkinect fusion algorithm\\nrgb-d data fusion\\neye-in-hand camera\\ncolour segmentation\\nnonlinear least squares optimisation\\n6dof manipulator\\ncrop orientation\\nsweet pepper pose detection\",\"285\":\"cameras\\nrobot vision systems\\ncalibration\\nrobot kinematics\\nservice robots\\nmathematical model\\nend effectors\\nindustrial robots\\npose estimation\\nrobot vision\\nsingle image based camera calibration\\nrobot end-effector\\nsix dimensional pose measurement\\nindustrial robot\\npose determination\\ncamera internal parameters\\nzhang camera calibration algorithm\\nrobot repeatability measurement\\nartificial reality toolkit aruco\\nkinematic parameter identification\\ncircle point analysis method\\nkuka kr5 arc robot identification\",\"286\":\"three-dimensional displays\\nreal-time systems\\nlighting\\nmatrix decomposition\\ngraphics processing units\\nlaplace equations\\ngaussian processes\\nimage matching\\nimage texture\\nmatrix algebra\\nobject detection\\npose estimation\\nprincipal component analysis\\nreal-time scalable 6dof pose estimation\\ntextureless objects\\nrobotics\\nrgb-d image captures\\nrgb object\\ngpu implementation\\nlaplacian of gaussian space\\nreal-time matching\\ntemplate set\\nnormalized cross-correlation operation\\nmatrix-matrix multiplication\\nlarge-scale template matching\\ncandidate elimination method\",\"287\":\"image reconstruction\\ntrajectory\\nthree-dimensional displays\\ncomputational modeling\\ndata models\\nobservers\\ncameras\\nimage sequences\\nmultiple independent captures\\nincremental reconstruction algorithm\\nmobile devices\\n3d-structure\\nmultiple image sequences\",\"288\":\"cameras\\nlaser radar\\nsensor systems\\nrobot vision systems\\nenvironmental factors\\nobject detection\\npose estimation\\nrobot vision\\nvision system\\ndepth processing algorithm\\ndrc-hubo+\\n3d information\\nenvironment conditions\\ndepth-map upsampling method\\nrobotic applications\\nsynthetic datasets\\nreal-world datasets\",\"289\":\"clothing\\ngaussian processes\\nfeature extraction\\nsupport vector machines\\nprobabilistic logic\\ntraining\\nkernel\\napproximation theory\\nclothing industry\\nimage classification\\nindustrial manipulators\\noptimisation\\nprobability\\nproduction engineering computing\\nclothing category recognition\\nfree-configuration\\ninteractive perception\\nhighly-wrinkled cloth recognition\\nclothes sorting pipeline\\nprewashing stage\\nautonomous laundering process\\nclothing manipulation\\nperception confidence\\ngarments\\nclothing perception\\nclassification probabilities\\nprobabilistic classifiers\\npredictive probabilities\\nmulticlass gaussian process classification\\nlaplace approximation\\nposterior inference\\nhyperparameter optimisation\\nmarginal likelihood maximisation\\nunknown garment recognition\\nwrinkled configurations\",\"290\":\"three-dimensional displays\\nshape\\ndata models\\nrobots\\ncomputational modeling\\nsolid modeling\\nnumerical models\\ncomputer graphics\\nmanipulators\\nrobot vision\\nmodel-based approach\\nsubstitute tools\\n3d vision data\\nrobot\\nmanipulation activities\\nhand-coded models\\nsuperquadrics\\npoint clouds\\nflexible matching\\nknowledge transfer\\nreasoning engine\",\"291\":\"cameras\\ndairy products\\nrobot vision systems\\nfeature extraction\\ntrajectory\\ngesture recognition\\nhuman-robot interaction\\nimage colour analysis\\nimage segmentation\\npattern clustering\\nrobot vision\\nunsupervised learning\\nwatch-bot\\nhuman forgotten actions\\nrobotic system\\nkinect v2 rgb-d sensor\\nlaser pointer\\nassistive robot\\nlearning algorithm\\naction-object cooccurrence learning\\naction temporal relation\\nunsupervised action segmentation\\naction cluster assignment performance\\nhuman activity rgb-d video dataset\\nrobotic experiment\",\"292\":\"three-dimensional displays\\nhistograms\\nobject recognition\\nrobustness\\npattern recognition\\nshape\\nmeasurement\\nstatistical analysis\\nlocal surface description\\n3d object recognition\\n2d object recognition\\nlocal reference frame or axis\\nlrf\\/a stability\\nlrf estimation\\nhistogram of distances approach\\nl2-norm metrics\\npoint clouds\\n3d matching\\nlocal features\\npoint cloud\",\"293\":\"cameras\\nrobot vision systems\\nobject detection\\nservers\\nvisualization\\nmobile robots\\npath planning\\nrobot vision\\nsensor fusion\\ndistributed robotic vision service\\nvideo acquisition\\nimage acquisition\\ndrvs\\nrobot path planning\\nrobot line-of-sight\\ndistributed visual object detection service\\nremote camera nodes\\ntask-specific object detection\\ngeographic region of interest\\nobject type\\nvisual processing\\nhigh-level object information\\nsensor discovery\\nmobile robot compute load\\nwireless network utilization\",\"294\":\"fasteners\\nrobot sensing systems\\nfeature extraction\\noptimization\\ndetectors\\nsynchronous motors\\nassembling\\nelectric vehicles\\nimage processing\\nmechanical engineering computing\\nobject detection\\nelectric vehicle motor autonomous disassembly\\nrobot cognition\\nimage processing algorithm\\nmotor screw autonomous detection\\nmotor screws grayscale\\nmotor screws depth\\nmotor screws hsv values\",\"295\":\"agriculture\\nimage segmentation\\nimage color analysis\\nrobustness\\nrobots\\nvisualization\\nlighting\\nagricultural machinery\\ncrops\\nimage colour analysis\\nindustrial robots\\nrobot vision\\noccluded crops\\nvisual detection\\nautomated harvesting\\ncrop detection system\\nsweet pepper\\ncapsicum\\nrobotic systems\\nlocal binary pattern\\nprepixel segmentation\\ncolour imagery\",\"296\":\"feature extraction\\ncameras\\nvisualization\\nprobabilistic logic\\nrobustness\\nimage segmentation\\noptimization\\ncovariance matrices\\ndistance measurement\\ngaussian distribution\\nimage motion analysis\\nimage reconstruction\\nimage sequences\\nimage texture\\nminimisation\\nmobile robots\\nprobability\\nrobot vision\\nstereo image processing\\nstereo visual odometry\\npoints and line segments probabilistic combination\\nmotion reconstruction\\npoint feature tracking\\nimage sequence\\nlow-textured scenes\\nprojection errors nonlinear minimization\\ngaussian distribution errors\\nmobile robotics\",\"297\":\"trajectory\\ndictionaries\\nencoding\\npredictive models\\nsignal processing algorithms\\ncameras\\nprediction algorithms\\nmarkov processes\\nmobile robots\\nmotion control\\npattern clustering\\nsignal representation\\ntrajectory control\\naugmented dictionary learning\\nmotion prediction\\nmultivariate trajectories representation\\nbehavior patterns\\nmobile agents\\npart-based trajectory representation\\nmarkovian-based approaches\\nclustering-based approaches\\naugmented seminonnegative sparse coding\\nasnsc algorithm\\nconstrained dictionary learning problem\\nconvexity condition\\ntrajectory modeling application\\nmotion patterns\\ndictionary atoms\",\"298\":\"cameras\\nmanipulators\\nswitches\\nrobot kinematics\\nrobot sensing systems\\ncontrollers\\nfork lift trucks\\npalletising\\npallet detection\\nmonocular camera\\nunfavorable weather conditions\\nunstructured outdoor environments\\nobject manipulation\\nfiducial markers\\nmobile base\\npose estimation\\nobject-local frame\\nearth-fixed frame\\nmobile manipulator\\nyaw angle estimation\\nstate space\\nsmooth switching logic\\nmacro-micro visual mobile manipulation architecture\\nafs hydraulic machine\\narticulated-frame-steering\\nautonomous pallet picking\\nmultistage controller\",\"299\":\"trajectory\\npredictive models\\nsemantics\\nmarkov processes\\ncollision avoidance\\ncontext\\nfiltering theory\\nimage motion analysis\\noptimal control\\npedestrians\\nroad traffic control\\nintent-aware long-term prediction\\npedestrian motion prediction\\njump-markov process\\nmarkov decision process framework\\nrao-blackwellized filter\\npedestrian state\\nstochastic policy\",\"300\":\"navigation\\ntrajectory\\nrobustness\\nsimultaneous localization and mapping\\nvisualization\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nstatistical testing\\ntwo-step procedure\\nstatistical test\\nrobot navigation\\nstandard navigation system\\nrobot trajectory\\nros\\nautonomous exploration robot\\nunderground environments\\noffice environments\\nunknown environment\\nrobot guidance\\nrobust homing\",\"301\":\"microphones\\nrobot sensing systems\\ntracking\\nreverberation\\nmobile robots\\nprocess control\\nacoustic signal processing\\nclosed loop systems\\nposition control\\nsound-based control\\nmobile robot\\nrobot audition\\nauditory features\\nclosed-loop control\\ntime difference of arrival\\naural perception\\ntdoa measurements\\npositioning task\\nrobot perception\",\"302\":\"monitoring\\nplanning\\nbayes methods\\ndiseases\\nlogistics\\noptimization\\nagriculture\\nautonomous aerial vehicles\\ncontrol engineering computing\\ncrops\\nenvironmental monitoring (geophysics)\\ngaussian processes\\nimage capture\\nimage classification\\ninterpolation\\nlearning (artificial intelligence)\\nminimisation\\nmobile robots\\npath planning\\nplant diseases\\nregression analysis\\nrobot vision\\nroute planning\\nactive classification\\nuavs\\nagricultural crop mapping\\nenvironmental monitoring\\nairborne monitoring\\naerial image classification\\nactive learning method\\nminimization uncertainties\\nbinary logistic regression\\ngaussian process\\npathologies detection\\nmap interpolation\\nbayesian optimization\\ninformative trajectory planning\",\"303\":\"vehicles\\ntrajectory\\nplanning\\nuncertainty\\nrobustness\\ntracking\\nprobabilistic logic\\nestimation theory\\nmobile robots\\noff-road vehicles\\noptimal control\\npath planning\\nsampling methods\\ntracked vehicles\\nautonomous tracked vehicles\\ndeformable high slip terrain\\noptimal global planner\\noff-road terrain\\nslip estimation\\nrobust incremental sampling based motion planning\\ncc-rrt* algorithm\\nlqg-mp algorithm\\nprobabilistically feasible trajectory\\nchance constrained approach\",\"304\":\"planning\\nwind forecasting\\natmospheric modeling\\nlattices\\nspatiotemporal phenomena\\ncolor\\nreal-time systems\\naircraft control\\nautonomous aerial vehicles\\ncollision avoidance\\ndoppler radar\\ngraph theory\\ntime-varying systems\\nwind\\nany-time path-planning\\ntime-varying wind field\\nmoving obstacles\\nspatiotemporally varying wind-field\\nchanging wind prediction\\nobstacle prediction\\n(d+1)-dimensional space-time lattice\\nany-time algorithm\\na*-like cost heuristic\\nmotion modeling error\\nany-time performance\\nconnectivity model\\ndiscrete graph\\ncontinuous cost-field\\nfixed-wing unmanned aircraft system\\natmospheric models\\nonline real-time wind prediction\\nonline real-time wind sensing\",\"305\":\"robot sensing systems\\nnavigation\\ndata collection\\ntime measurement\\nmobile robots\\nwireless sensor networks\\ndata acquisition\\nreliability\\nactive sensing data collection\\nautonomous mobile robots\\nfine-grain sensor measurements\\nstatic sensors\\nhuman crowd-sourcing efforts\",\"306\":\"visualization\\nsimultaneous localization and mapping\\ncameras\\nimage edge detection\\nfeature extraction\\npipelines\\nrocks\\nautonomous underwater vehicles\\nimage matching\\nimage registration\\nmobile robots\\nobject detection\\npath planning\\nrobot vision\\nslam (robots)\\ncluster-based loop closing detection\\nunderwater slam\\nfeature-poor underwater environment\\nvision-based localization systems\\nfeature matching\\nvisual information\\nvisual features\\nvisual keypoint clustering\\nauv navigation\\nautonomous underwater vehicle\\nseagrass\\nsandbanks\\nmarine environments\\nbalearic islands\\nvisual registration\",\"307\":\"electric variables measurement\\nelectrodes\\nprobes\\nanalytical models\\natmospheric measurements\\nparticle measurements\\nnumerical models\\nneural nets\\nsonar\\nunderwater acoustic communication\\nneural-based underwater surface localization\\nelectrolocation\\nneural network\\nforward map\\nplanar wall\\nmotion control loop\",\"308\":\"synthetic aperture sonar\\nsensors\\ncurrent measurement\\nreal-time systems\\nsonar applications\\nvehicles\\nautonomous underwater vehicles\\nmarine control\\nobject recognition\\nsonar imaging\\nunderwater sound\\nautonomous mission\\nlaboratory-based data-processing\\nsas sensor\\nmine countermeasure operation\\nauv\\nautonomous underwater vehicle\\nstrong currents\\nadaptive underwater sonar\",\"309\":\"magnetometers\\naccelerometers\\nestimation\\ncalibration\\nacceleration\\ntime measurement\\ngyroscopes\\nattitude control\\nattitude measurement\\nautonomous underwater vehicles\\nfibre optic gyroscopes\\ninertial navigation\\nmobile robots\\nnonlinear filters\\nposition control\\nunderwater vehicle attitude estimation\\nmagnetic disturbance\\norientation estimation\\nautonomous navigation task\\nmobile robot\\nposition estimation problem\\ndead reckoning navigation system\\nautonomous underwater vehicle\\nattitude estimation system\\nnonlinear complementary filter\\nncf adaptation\\ninertial measurement unit\\nimu\\nmagnetometer\\nsingle-axis fibre optic gyroscope\\nfog\\nattitude estimation filter\\nuniversity of florence typhoon auv\\nexternal unpredictable disturbance\",\"310\":\"vehicles\\nacoustics\\nunderwater vehicles\\nsensors\\nsonar navigation\\ncooperative systems\\nglobal positioning system\\nkalman filters\\nmarine navigation\\nnonlinear filters\\nstate estimation\\nunderwater acoustic communication\\nunderwater vehicles cooperative navigation\\nrange-rate observations\\nunderwater vehicles underwater communication\\nacoustic modem\\nattitude sensors\\ndepth sensors\\nsurface vehicle\\ngps\\nrange observations\\nunderwater vehicle state estimation\\ndelayed-state extended kalman filter\\nrange-rate measurements\",\"311\":\"adaptation models\\ncomputational modeling\\nmathematical model\\nheuristic algorithms\\nvehicles\\nvehicle dynamics\\napproximation algorithms\\napproximation theory\\nautonomous underwater vehicles\\nbayes methods\\nfault diagnosis\\ngaussian processes\\nlearning (artificial intelligence)\\nonline fault detection\\nmodel adaptation\\nthruster failures\\nauv hardware integrity\\nvehicle dynamical model\\nmixture of gaussians representation\\nvariational bayes approximation\\nfiltering equations\\nlearning\\nwave tank\\nheriot-watt university\\nunderwater navigation\\ngaussian mixtures\\nbayesian inference\\nfault detection\",\"312\":\"antenna radiation patterns\\ndipole antennas\\nmanganese\\nthree-dimensional displays\\nrobot sensing systems\\nestimation\\nattenuation\\nautonomous underwater vehicles\\nelectromagnetic wave attenuation\\nmarine communication\\nmobile robots\\nposition measurement\\nradiowave propagation\\nrssi\\nwireless sensor networks\\n3d underwater localization scheme\\nem wave attenuation\\ndepth sensor\\nposition determination\\nremotely operated underwater vehicle\\nsignal strength\\ncommercial radio-frequency sensor\\nposition information\\nstructured environment\\n2d environment\\nantenna radiation power\\nsending antenna\\nreceiving antenna\\nelevation angle\\nelevation scope\\nradiation pattern\\ndipole antenna\\ndistance estimation\\n3d localization experiment\\nunderwater wireless sensor network\\nrov position tracking\",\"313\":\"vehicles\\nscattering\\nacoustics\\narrays\\nreceivers\\nacoustic arrays\\nnose\\nacoustic wave scattering\\nautonomous underwater vehicles\\nimage sensors\\nmobile robots\\nmulti-robot systems\\nauv behavior\\nbistatic acoustic scattering\\nmultistatic acoustic scattering\\nseabed target\\nautonomous underwater vehicle\\nharbor security\\nimaging sensor\\ntarget localization\\ntarget classification\\nmultiple auv\\nacoustic source\\nhydrophone nose array\\nradiation pattern\\nsource frequency\\naspect angle\\nmobile autonomous vehicle\\nbistatic angle\\nvehicle dynamics constraint\",\"314\":\"data models\\nvisualization\\nbayes methods\\ncameras\\nrobot vision systems\\nsemantics\\nspatiotemporal phenomena\\ngeophysical image processing\\noceanographic techniques\\nvideo signal processing\\nanomaly detection\\nbayesian nonparametric scene modeling technique\\nvideo data\\nanomalous flora detection\\nimage data\\nunderwater robot\\nstatic camera\\ndynamic unstructured environment\\nstatic seafloor camera\\ncoral reef\\nunderwater vehicle\\nbenthic flora\\nsurface waves\",\"315\":\"probes\\nsprings\\ntorque\\nbiological tissues\\nsilicon\\nphantoms\\nsurgery\\nhaptic interfaces\\nmedical robotics\\nmemory primitives\\ninformation gain metrics\\nartificial silicon phantom tissue\\ndepth estimation\\ninternal stiffness variables\\nindentation probe\\nlaboratory-made variable stiffness\\nhaptic perception\\nembodied information gain\\ninternal stiffness control\\ninteraction behavior efficacy\",\"316\":\"force\\nforce feedback\\nvehicles\\nmanuals\\nvirtual environments\\ndiscrete cosine transforms\\nautomotive engineering\\nhaptic interfaces\\nroad vehicles\\nstability\\nhaptic simulation\\nautomotive automatic gearshift\\nstability analysis\\nforce profiles\\n2 dof haptic interface\\ncar manufacturers\\nphysical mock-ups\\ngearshift handle\\nforce feedback control law\\nstate machine\\nforce feedback profile\\ntransition function\\nrendered force-position profiles\",\"317\":\"pins\\nstability analysis\\nfluids\\napproximation algorithms\\npower system stability\\nthree-dimensional displays\\nvalves\\ncomputer displays\\ncomputer graphics\\nhaptic interfaces\\nmatrix decomposition\\nlcd screen\\ncolumn dynamics\\nrow dynamics\\nline scanning\\nsnmf system\\nstability\\nrank-one approximation\\nfluid power\\nseminonnegative matrix factorization\\npin array shape displays\\npassivity\",\"318\":\"spectrogram\\nrobot sensing systems\\ndictionaries\\nvibrations\\nencoding\\ndynamics\\ncontrol engineering computing\\ngrippers\\nsignal representation\\nsupport vector machines\\ntactile sensors\\nunsupervised learning\\nunsupervised feature learning\\ndynamic tactile events classification\\nsparse coding\\nrobotic operations\\nobjects displacement\\nrobot-related motion\\nslippage detection\\nobject-gripper slip\\nobject-world slip\\npower spectral density\\npsd\\ntactile dynamic signal\\nautomatic speech recognition\\nasr\\nsparse representation\\nsparse vectors\\nhigh-level features\\nlinear support vector machine\\nsvm\",\"319\":\"performance evaluation\\nhaptic interfaces\\nphantoms\\nmanipulators\\nrobot control\\nsurgery\\nmulti-robot systems\\ntelerobotics\\ntrajectory control\\nuser task performance\\ntwo-handed complementary-motion teleoperation\\nmultiple degree-of-freedom\\nmultiple-dof\\nteleoperated task performance\\ncooperative manipulation\\nmaster haptic device manipulation\\nslave robot\\nbimanual teleoperation system\\n3-dof haptic master interfaces\\n6-dof slave manipulator\\nmaster motion\\nsingle-master\\/single-slave teleoperation system\\n6-dof master manipulator\\ndof decomposition\\ntask completion time\\ntrajectory tracking\",\"320\":\"haptic interfaces\\nnavigation\\nshape\\nvisualization\\nmobile handsets\\nlegged locomotion\\nperformance evaluation\\nhuman-robot interaction\\npedestrians\\nminimalistic handheld haptic interface\\npedestrian navigation assistance\\nunobtrusive stimulus\\nhaptic taco\\nrobotic interfaces\\n2dof shape changing interface\\nminimal 1dof interfaces\\nhaptic lotus\\nforce exertion capability\\nembodied steepest descent method\\ntaco interface\\n2dof shape-changing interface\\nanimotus\",\"321\":\"force\\njacobian matrices\\nhaptic interfaces\\nactuators\\nkinematics\\ntorque\\nmathematical model\\nposition measurement\\nvirtual reality\\nhaptic device\\nhigh-force display capability\\nwide workspace\\nvirtuapower\\ndegrees of freedom position measurement\\ndof position measurement\\nparallel mechanism\\ndouble in-parallel supporting chains\\nmechanical design\\nspace jacobians\\nkinematic analysis\\nlinear force\\ntorque sensor\\nforce sensor\\nvirtual reality environment\",\"322\":\"computational modeling\\ndeformable models\\nadaptation models\\nfinite element analysis\\nskin\\nmathematical model\\nspatial resolution\\ncollision avoidance\\ngraphics processing units\\nhuman-robot interaction\\nunified representation\\nsimulated deformable objects\\nvirtual environments\\nunified approach\\nspecific interaction\\nobject type\\nhuman-robot interactions\\ngraphic processing unit\\ngpu\\ncollision detection\\nfriction models\",\"323\":\"jamming\\nshape\\nhaptic interfaces\\nrobot sensing systems\\nactuators\\nregulators\\nclosed loop systems\\nfeedback\\nmechanical properties\\nshape control\\nsprings (mechanical)\\nclosed-loop shape control\\nhaptic jamming deformable surface\\nhaptic jamming tactile display\\nthin particle jamming cells\\ndepth map\\nrgb-d sensor\\nshape feedback\\nclosed-loop control\\nmass-spring model\",\"324\":\"robots\\nshape\\nvehicles\\nelectron tubes\\npotential energy\\nplanning\\ngravity\\ndifferential equations\\nmotion control\\nbarycentric spherical robot\\ncenter-of-mass\\nfirst-order differential equation\\nsymmetry-breaking potential energy\\nlagrangian reduction\\nbkmm approach\\ngeometric mechanics\\nreduced dynamical equation-of-motion\\nnet vehicle motion\\nmomentum\\ninternal actuation\\nnontrivial correlation\\npropulsion\\ncenter-of-rotation\",\"325\":\"heuristic algorithms\\nmanipulator dynamics\\nmathematical model\\ntransmission line matrix methods\\nangular velocity\\noptimisation\\npath planning\\nreorientation algorithm\\nfree-floating serial manipulator\\ninternal motion\\npath planning algorithm\\nmanipulator dynamic model\\nnonlinear optimization technique\\nactuator velocity trajectories\\nthree-link planar robot\\nopen-loop mode\",\"326\":\"mathematical model\\nplanning\\ndynamics\\ncontrollability\\nlegged locomotion\\nfeedforward neural networks\\nfeedback\\nfeedforward\\nlyapunov methods\\nmobile robots\\npath planning\\npendulums\\nstability\\nmotion planning\\nhoop-pendulum\\nunderactuated systems\\nlocal controllability\\nequilibrium point\\nfeedback controller\\nfeedforward controller\\nlyapunov function\\nsystem stability\\nbeta function\",\"327\":\"planning\\nheuristic algorithms\\napproximation algorithms\\nservice robots\\nsearch problems\\nsystematics\\nmanipulators\\nmotion control\\npath planning\\na*-connect algorithm\\nbounded suboptimal bidirectional heuristic search\\nbidirectional planning\\nunidirectional planning\\nmotion planning\\nhigh-dimensional configuration space\\nsampling-based planners\\nrrt-connect\\nsearch-based algorithms\\nforward search\\nbackward search\\ncomputational complexity\\nmanipulation domain\\nnavigation domain\",\"328\":\"planning\\nmanifolds\\nmobile robots\\ntrajectory\\npneumatic systems\\naerospace electronics\\nmotion control\\npath planning\\ntopology\\nmotion planning\\nplanar robot\\ntether stiffness\\nmobile robot\\ntopological constraint\\ncable model\\ndubins curve theory\\nconfiguration space manifold\\ndiscrete structure\\natlas on-the-fly\\npath location\",\"329\":\"trajectory\\naerospace electronics\\nplanning\\nmobile robots\\nrobot kinematics\\nmotion control\\npath planning\\nprobability\\ntrees (mathematics)\\nrrt-based nonholonomic motion planning\\nany-angle path biasing\\nrrt* planning techniques\\nnonholonomic constraints\\nplanning times\\nhierarchical techniques\\ntheta*-rrt\\nnonholonomic wheeled robots\\ngeometric information\\ndifferential drive system\\nhigh-dimensional truck-and-trailer system\\ntrajectory generation\\nanalytical steer function\",\"330\":\"robot sensing systems\\ninspection\\ncameras\\ntrajectory\\nplanning\\nmobile robots\\npath planning\\nsampling methods\\ntrees (mathematics)\\nlimited sensing\\nsampling-based strategy\\nomnidirectional sensing\\ncoverage path planning\\nasymptotically optimal inspection planning algorithm\\ndifferential constraints\\nrealistic sensing model\\nvisual inspection\\nrandom inspection tree algorithm\",\"331\":\"trajectory\\ntraveling salesman problems\\nrobots\\nvehicle dynamics\\nstochastic processes\\nspace vehicles\\nplanning (artificial intelligence)\\ntravelling salesman problems\\nstochastic traveling salesman problem\\nkinodynamic vehicles\\nrobotics\\nmonitoring\\nsurveillance\\nreconnaissance\\nstochastic tsp\\ndynamical systems\\nstochastic orienteering problems\\ntsp tour\\nplanning algorithms\",\"332\":\"robot kinematics\\nruntime\\ncollaboration\\ncontext\\nxml\\nnavigation\\nhuman-robot interaction\\nrescue robots\\nhuman-robot collaborative high-level control\\nrescue robotics\\noperator assisted semiautonomous robots\\ncomplex tasks solving\\ncomplex robot behaviors\\nhierarchical state machines\\nremote operator\",\"333\":\"position measurement\\nmeter reading\\nmonitoring\\nwires\\nrobot kinematics\\ninductors\\nfield programmable gate arrays\\nnuclear power stations\\npressure vessels\\nrobot assisted automatic measurement system\\nmultistud tensioning machine\\nmstm\\nspecialized equipment\\nreactor pressure vessel\\nrpv\\nnuclear power plant maintenance\\nmeasurement meters\\nworking positions\\nreal-time data monitoring\\nfield programmable gate array\\nfpga\\ndistributed control scheme\\ndata acquisition\\nmstm control plc\\nchina nuclear power technology research institute\",\"334\":\"grippers\\nmanipulators\\nforce\\nmathematical model\\ntrajectory\\nposition control\\nforce control\\ngait analysis\\nmobile robots\\nmotion control\\ngait control verification\\nreaction null-space\\nground-gripping robot\\nminor-body surface\\nspace features\\nharsh terrain\\nmicrogravity\\nmoving mechanism\\nrock climber\\nrobot gait\\nidling arm motion\\nreaction force control\\ntip position control law\\nreactionless control\\nair floating system\\nplanar simulation\\nirregular terrain\",\"335\":\"wheels\\nstress\\nforce measurement\\nforce\\nsoil\\nstress measurement\\nsoil measurements\\nmobile robots\\nstress distributions\\ngrousers\\nloose soil\\nwheeled mobile robot traveling\\nmobility performance\\nwheel surface\\nwheel rotation\\n3d force data\\nshear stress\",\"336\":\"satellites\\nforce\\nposition measurement\\ntelerobotics\\nforce measurement\\ntransmission line matrix methods\\naerospace robotics\\nartificial satellites\\nforce control\\nmotion control\\nposition control\\nexperimental verification\\nuser-specified motions\\nposition measurements\\nregistration errors\\nhybrid position-force controller\\ninsulation patch\\nsatellite access panel\\ntelerobotic satellite servicing\\nlocal task frame\\nremote physical system\\nmodel geometry\\nsensor-based control\\nremote robot\\ncommunication delay\\nsatellite servicing\\nmodel-based teleoperation\\ntask frame estimation\",\"337\":\"workability\\ncrawlers\\nmanipulators\\nrobot kinematics\\nforce\\ncontrol systems\\nhydraulic control equipment\\nrescue robots\\nstability\\ntelerobotics\\nfour-arm four-crawler advanced disaster response robot\\noctopus\\nrobot mobility\\nrobot workability\\nmutual complementary strategy\\nstabilization\\nhydraulic driven robot\\nteleoperated robot\",\"338\":\"wheels\\nstability analysis\\nforce\\nactuators\\nlegged locomotion\\ncollision avoidance\\ncompliant mechanisms\\nmotion control\\nrobot kinematics\\nstability\\nvelocity control\\ncompliant wheel-leg robot\\nrough terrain crossing\\ncompliant elements\\nactuation\\nwheel-legged robot\\nlocomotion properties\\nunknown terrains\\nirregular terrains\\nobstacles detection\\nstructural compliances\\nsteep obstacles\\nchassis\\npostural control flexibility\\nvertical series elastic actuators\\nsea\\npassive horizontal compliant mechanism\\nwheel speed control\\npostural servoing\\nlocal reactive loop\\nvertical forces\\nstability margin\",\"339\":\"buildings\\ntrajectory\\nrobot sensing systems\\nplanning\\ngyroscopes\\nbrick\\nbuilding\\nconstruction industry\\nend effectors\\nindustrial manipulators\\nmobile robots\\nposition control\\nin situ fabricator autonomous repositioning\\nin situ fabricator autonomous localization\\nindustrial manufacturing\\nbuilding construction sites\\non-board sensing\\nmobile robot\\nend-effector positioning\\ndigital fabrication\\nbrick vertical stacking\",\"340\":\"trajectory\\noptimization\\njacobian matrices\\naerospace electronics\\nnull space\\nrobots\\nkinematics\\noptimisation\\npath planning\\nredundant manipulators\\nlow dimensional human preference tracking\\nmotion optimization\\nhigh degree of freedom robots\\ndof\\nsingle cost function\\ntask specific constraints\\nmotion planning system\\ndimensionality reduction\\ndarpa robotics challenge\\ndrc\\nredundant dof manipulators\\njoint trajectory\",\"341\":\"acceleration\\nrobot motion\\nman-machine systems\\nvideos\\ntiming\\naerospace electronics\\nhumanoid robots\\nimage motion analysis\\nmotion control\\nvideo signal processing\\nlaban head-motions\\nrobot state\\nrobot body language\\nfunctional robots\\nhuman-machine environments\\nmotion expressions\\nrobot task motions\\nlaban efforts\\nacting training\\ndance training\\n2-dof nao head\\n4-dof keepon robot\\nlook-for-someone behaviors\\nrobot motion videos\\neffort motion examples\\nlegibility results\\nrobot motion patterns\",\"342\":\"navigation\\ntrajectory\\nmobile robots\\nbayes methods\\nlearning (artificial intelligence)\\nmeasurement\\ngraph theory\\nlearning systems\\npath planning\\nsocially normative robot navigation behaviors\\nbayesian inverse reinforcement learning\\nsocially normative human behavior\\ngraph-based representation\\nlarge-scale pedestrian simulator\",\"343\":\"navigation\\nrobot kinematics\\ntrajectory\\nrobot sensing systems\\nmeasurement\\nmanifolds\\nmobile robots\\nautonomous indoor robot navigation\\nsketch interface\\ndrawing routes\\ndrawing maps\\nhand-drawn sketches\\nindoor environment\\nlocal deformations\\nmetric manifold\\nreal-world scenarios\",\"344\":\"robot sensing systems\\ncoherence\\ninformatics\\nhumanoid robots\\nmeasurement\\nspeech\\nhuman-robot interaction\\nintelligent robots\\nbertsolaritza\\nbasque improvised contest poetry\\nrobot body language\\nrobot communication capabilities\\ntheatrical performances\\nsocial robotics\\nhand robot autonomy\\nsocial behaviors\",\"345\":\"vegetation\\nsensors\\nbiochemistry\\nwheels\\nprobes\\nrobots\\nuser interfaces\\nmobile robots\\nmobile robotics\\ntranshumus\\nartistic work\\nvenice biennale\\nroots\\nmobile platforms\\ntree metabolism\\noperational spaces\",\"346\":\"navigation\\nrobot kinematics\\nrobot sensing systems\\nadaptation models\\ngrammar\\nknowledge based systems\\ngeriatrics\\nhuman-robot interaction\\nservice robots\\nnavigational commands\\nelderly people\\nliving standard\\nvoice instructions\\nlexical symbols\\nuncertain information\\nnavigational user commands\\nrobot experience model\\nrem\\nlexical representations\\nheterogeneous domestic environments\\nstrict grammar model\\nassistive robot platform\\nunderstanding uncertain information\\nhuman-robot interactions\\nhuman friendly robot\\nassistive robots\\nexperience of robots\",\"347\":\"robot sensing systems\\nlighting\\ncameras\\nlaser radar\\nhardware\\ncollision avoidance\\nlaser ranging\\nmobile robots\\noptical radar\\noptical sensors\\nsensors\\nsmart phones\\noutdoor environments\\nlaser distance sensors\\nlidar\\noutdoor robotic vehicles\\nlow-cost depth sensing technologies\\nsmartphone-based planar laser distance sensor design\\nhardware additions\",\"348\":\"mathematical model\\nrobot sensing systems\\nestimation\\ndiscrete wavelet transforms\\nuncertainty\\naerodynamics\\naerospace robotics\\nalgebra\\ndifferentiation\\nestimation theory\\nhaar transforms\\nintelligent control\\nnonlinear control systems\\npitch control (position)\\nstatistical analysis\\nthree-term control\\nnonlinear model-free control\\nflapping wing flying robot\\naerial robots\\nerror elimination\\nflapping dynamics\\nzero statistical knowledge\\nnoise affecting sensor measurements\\nintelligent pid controllers\\nipid controllers\\nindirect estimation\\nnumerical differentiation\\nhaar-wavelet differentiator\\nalgebraic method\\nnonasymptotic method\\npitch control\\naltitude control\",\"349\":\"gradient methods\\ntrajectory\\npredictive control\\nvehicle dynamics\\nupper bound\\nunmanned aerial vehicles\\napproximation theory\\nautonomous aerial vehicles\\ncontinuous time systems\\nlinear quadratic control\\napproximate model predictive control scheme\\nunmanned aerial vehicle\\ncontinuous time parametrization\\noptimal control problem\\nonline optimization\\nmemory requirements\\ndual gradient method\\nstm32 cortex m4 processor\\nlinear quadratic regulator\\naggressive maneuvers\",\"350\":\"trajectory\\noptimal control\\nheuristic algorithms\\nmanipulators\\nload modeling\\noptimization\\naircraft control\\ncontrol system synthesis\\nhelicopters\\nnumerical analysis\\noptimisation\\ntrajectory control\\ntrajectory generation\\nquadrotor based systems\\nnumerical optimal control\\ncontroller design\\nanalytic solutions\\ndirect multiple shooting approach\\ncomplex problems\\nanalytic development\\noff-the-shelf optimization solver\\nonline trajectory generation\\nquadrotor-and-pendulum system\\nmanipulation tasks\\naerial manipulator\",\"351\":\"vehicles\\nrobustness\\ntrajectory\\nmathematical model\\nstandards\\nvehicle dynamics\\nautonomous aerial vehicles\\ncontrol system synthesis\\nhelicopters\\nmicrorobots\\nposition control\\nrobust control\\ntracking\\nvtol aerial robots\\nvertical take-off and landing microaerial vehicles\\nmav\\nvelocity profile\\nrobust maneuver regulation controller\\ntracking scheme\\ntracking controller\\nnanoquadrotor\",\"352\":\"uncertainty\\nadaptation models\\npropellers\\nmathematical model\\nstandards\\nacceleration\\nrobustness\\nadaptive control\\naircraft control\\nattitude control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nfeedforward\\nhelicopters\\nrobust control\\ntrajectory control\\nuncertain systems\\nvariable structure systems\\nvehicle dynamics\\nadaptive super twisting controller\\nquadrotor uav\\nrobust quadrotor controller\\nreference trajectory tracking\\ndisturbances\\ngain adaptation law\\ncontroller design\\ncontrol loops\\nposition control\\nfeedforward dynamic inversion control\\nsliding mode controller\\nhigher order quadrotor dynamic model\\nsimmechanics physical simulation\\nparameter uncertainties\\nnoisy measurements\\nexternal perturbations\\nunmanned aerial vehicle\",\"353\":\"sensor phenomena and characterization\\nestimation\\nsensor fusion\\ndata integration\\nattitude control\\nheuristic algorithms\\nalgebra\\nfiltering theory\\nlyapunov methods\\nstability\\nsensors model\\ndata fusion\\ncomplementary filters\\nattitude estimation\\nattitude stabilization\\nmeasured inertial vectors\\nsensors dynamics\\ninertial vectors\\ngyro measurements\\nalgebraic algorithm\\nglobally asymptotic attitude estimation\\nlyapunov method\",\"354\":\"collision avoidance\\ncameras\\nrobot sensing systems\\ntarget tracking\\nsurveillance\\nnavigation\\nautonomous aerial vehicles\\nfiltering theory\\nhelicopters\\nimage colour analysis\\nimage sensors\\nmobile robots\\nobject detection\\nobject tracking\\npredictive control\\nrobot vision\\nobstacle detection\\nobstacle tracking\\nobstacle avoidance\\nteleoperated uav\\nteleoperated multirotor unmanned aerial vehicles\\ncollision-free indoor navigation algorithm\\nrgb-d camera\\nbin-occupancy filter\\nrobot velocity estimation\\ndirect field-of-view\\nmodel predictive control approach\\ncomputational unit\\nexternal cpu\",\"355\":\"vehicles\\nacceleration\\npropellers\\naircraft\\nengines\\nrobustness\\ncouplings\\naircraft control\\nfeedback\\nfeedforward\\nnonlinear dynamical systems\\nnumerical analysis\\nproportional control\\nprop-hanging control\\nthrust vector vehicle\\nhybrid nonlinear dynamic inversion method\\nfixed-wing aircraft\\ncruise performance\\nhover ability\\ninherent instability property\\ncoupling property\\nnonlinearity property\\nnonminimum phase characteristics\\nndi method\\nincremental ndi\\nangular acceleration feedback control methods\\nfeedforward control\\nlogical integral control\\nnumerical simulation\\nhardware-in-the-loop simulation\",\"356\":\"vehicles\\naerodynamics\\npropellers\\nattitude control\\ncontrol systems\\nrobot sensing systems\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\nrobot dynamics\\nvehicle dynamics\\nwind tunnels\\nfull attitude control\\nconvertible vtol tailsitter uav\\nconvertible vtol tailsitter unmanned aerial vehicle\\nfixed wing systems\\nrotary wing systems\\nwind tunnel measurements\\nrotorcraft\\nhigh performance flight control\",\"357\":\"visualization\\nrobot kinematics\\ntrajectory\\ndynamics\\nshape\\natmospheric measurements\\nforce feedback\\nhuman-robot interaction\\nmanipulators\\nrobot online coaching\\nphysical interaction\\nvisual interaction\\nhuman-robot interaction strategies\\ndynamic motion primitives\\nlow-level robot control approaches\\ncompliant robot\\nkuka lwr-4 robot arm\\nhuman-robot coaching interfaces\\nvisual feedback\\nposition feedback\",\"358\":\"force\\ndamping\\nservice robots\\nstandards\\nrobot sensing systems\\ncollaboration\\nadaptive control\\ncompliance control\\nforce control\\ngrippers\\nhuman-robot interaction\\nindustrial manipulators\\noccupational safety\\ntactile sensors\\niso10218-compliant adaptive damping controller\\nsafe physical human-robot interaction\\nrobot behavior\\nsafety level\\nphysical contact\\nindustrial environment\\ntool velocity\\npower\\ncontact force\\nhand-arm robotic system\\nmock-up collaborative application\\nsafe interaction\\ntactile sensing\\ngrasp force regulation\\nintuitive interface\",\"359\":\"manipulators\\nadmittance\\nkinematics\\nperformance analysis\\nimpedance\\nforce\\nend effectors\\nhuman-robot interaction\\nmanipulator performance constraints\\ncartesian admittance control\\nhuman-robot cooperation\\nhuman-robot physical interaction\\nvirtual constraints\\nlow-performance configurations\\ncartesian frame\\nend effector\\nperformance index\\n7-dof lwr serial manipulator\\nkinematic manipulability index\\nsingularity avoidance\",\"360\":\"trajectory\\nimpedance\\nrobots\\ndamping\\npredictive models\\ntorque\\nbayes methods\\ngaussian processes\\nhuman-robot interaction\\nimpedance-based gaussian processes\\nhuman behavior prediction\\nseamless physical human-robot interaction\\nphri\\nblack box\\nhuman motor control model\\ngp\\nimpedance parameters\",\"361\":\"wheelchairs\\ntorque\\nservomotors\\nimmune system\\nbrakes\\nforce\\nwheels\\nhandicapped aids\\npower transmission (mechanical)\\nservomechanisms\\nvariable speed gear\\nlower-limb-disabled individuals\\nfree locomotion\\ncycling wheelchair\\nupward slope\\ndownward slope\\ncontinuously variable transmission\\ncvt control methods\\ngravity compensation\\nservo brake control\\nhorizontal road\\ncadence control\",\"362\":\"impedance\\nrobots\\nneuromuscular\\ndynamics\\nhuman-robot interaction\\nstability analysis\\nsymmetric matrices\\nstability\\nhuman interactive robots\\nphysical human-robot interaction\\nrobot control\\nenergetic passivity\\nmechanical impedance\\nhuman neuromuscular system\\nhuman mechanical impedance\\ninteractive dynamics\",\"363\":\"robot kinematics\\ncollision avoidance\\nlegged locomotion\\nports (computers)\\ncollaboration\\nservice robots\\nhuman-robot interaction\\nport-based modeling\\nsafety-enhancing energy shaping control\\ncollision detection\\ncontact-related injury reduction\\nphysical human-robot collaboration\\nphrc\\nenergy monitoring control system\\nport-hamiltonian formalism\\nenergy-based compliance controller\",\"364\":\"magnetomechanical effects\\nmagnetic hysteresis\\ntorque\\npermanent magnets\\nfinite element analysis\\nelectromagnetics\\ncomputational modeling\\nclutches\\ncoils\\nmagnetic fields\\nmagnetorheology\\npower consumption\\nrobots\\nhybrid magneto-rheological clutch\\nsafe robotic applications\\nmr clutches\\nmagnetic field generation\\nelectromagnetic coil\\npermanent magnet\\nbias magnetic field density\\noptimum working point\\nenergized coil\\ntorque-to-mass ratio\\nelectrical power consumption\\nfinite element method\\nfem\\ncoil based clutch\",\"365\":\"robot sensing systems\\nforce\\nsensor arrays\\nforce measurement\\nskin\\nneural networks\\ncontrol engineering computing\\nhuman-robot interaction\\nindustrial manipulators\\nlearning (artificial intelligence)\\ntactile sensors\\nsensor configuration\\ntriaxial force measurement\\nphysical human robot interaction\\nphri\\ntactile sensor array\\nartificial neural network training\\nrobotic arm control\\nshear force measurement\\nindustrial robot\",\"366\":\"safety\\nservice robots\\ncomputer architecture\\nprediction algorithms\\nalgorithm design and analysis\\ntrajectory\\nflexible manufacturing systems\\nindustrial manipulators\\noptimal control\\npath planning\\n6 dof robot arm\\nplanar robot arm\\nsafety controller\\nparallel controller structure\\noptimal control problem\\nrobot control problem\\nrobot motion planning\\nhuman workers\\nflexible production lines\\nintelligent industrial co-robots\\nalgorithmic safety measurement\",\"367\":\"payloads\\nrobot sensing systems\\ncollision avoidance\\nrobot kinematics\\ntransportation\\nmobile robots\\nasymptotic stability\\ndistributed control\\nmotion control\\nnavigation\\nrobot dynamics\\nmulticonstrained joint transportation tasks\\nautonomous mobile robots\\ndynamical systems approach\\ndistributed leader-helper architecture\\nwarehouses\\noffice-like environments\\ntime series asymptotic stability states\\nattractor dynamics approach\\nbehavior based robotics\\nattractor state time sequence\",\"368\":\"robot sensing systems\\ncomputer architecture\\ncontext\\nrecurrent neural networks\\nvehicles\\nlogic gates\\nlearning systems\\nneurocontrollers\\nrecurrent neural nets\\nroad traffic control\\nsensor fusion\\ndriver activity anticipation\\nsensory-fusion architecture\\nspatiotemporal reasoning\\ndeep learning approach\\nsensory-rich robotics\\nlong short-term memory units\\nlstm units\\nlong temporal dependencies\\nsequence-to-sequence prediction\\nloss layer\\ndriving maneuver anticipation\\nmultiple sensors\",\"369\":\"trajectory\\nglobal positioning system\\nrobustness\\ncontext\\nkernel\\nestimation\\nurban areas\\ndata compression\\nunsupervised learning\\nunsupervised trajectory compression\\nkernel density estimation\\nkde\\nsmall scale topological artifacts\\ncrowd-sourced gps trajectories\",\"370\":\"testing\\nservice robots\\nrobot sensing systems\\nload modeling\\ncomputational modeling\\nindustrial robots\\nmaterials handling\\nindustrial transport robots\\nmodel-based techniques\\nautonomous robots\\nindustrial setting\\nmodel-driven engineering idea\\nindustrial use-case\\nhand-coded solutions\",\"371\":\"vehicles\\nfuels\\ntrajectory\\nsurveillance\\nsemantics\\nrouting\\nstochastic processes\\nenergy consumption\\nfuel economy\\nrelaxation theory\\ntemporal logic\\nvehicle routing\\ndynamic routing\\nenergy-aware vehicles\\ntemporal logic constraints\\nvehicle routing problem\\nvehicle team\\ntime-window temporal logic\\ntwtl formula\\nfuel consumption\\nstochastic model\\nrefueling\\nminimal relaxation\\ntemporal relaxation\\njoint trajectories\\nquadrotors\",\"372\":\"roads\\ncameras\\nvehicles\\nestimation\\nfeature extraction\\nacceleration\\nheuristic algorithms\\ngeometry\\nmulti-agent systems\\nobject detection\\nobject tracking\\ntraffic engineering computing\\nmultilane detection\\nmultilane tracking\\nreactive multiagent system\\ncamera information\\nroad marking detection\\nroad marking feature extraction\\npoint distribution\\nconfidence map\\ngeometric fitting\\nsivic platform\\nego lane detection\",\"373\":\"games\\nspace exploration\\nnavigation\\nautonomous agents\\nvehicles\\ngame theory\\npath planning\\nmobile robots\\noptimal control\\nvehicle routing\\noptimal navigation policy\\nautonomous agent\\nadversarial environments\\nautonomous vehicle navigation\\noptimal strategies\\nminimal cut-maximal flow\\nrandomized path planning\",\"374\":\"vehicles\\ndoppler effect\\ndoppler radar\\nradar detection\\nazimuth\\njacobian matrices\\ndistance measurement\\nmobile robots\\nobservability\\nremotely operated vehicles\\nrobot vision\\nsensor placement\\nobservability analysis\\noptimal sensor placement\\nautonomous navigation\\nrobust localization\\nautonomous vehicles\\nredundant estimation process\\nstereo radar odometry system\\nredundant system\\ndoppler error\\noptimal vehicle sensor placement\",\"375\":\"vehicles\\ncameras\\nthree-dimensional displays\\nimage color analysis\\nhead\\nroads\\nrobot vision systems\\ncomputer vision\\ndriver information systems\\ngaze tracking\\nhead-up displays\\nman-machine systems\\nstereo image processing\\nuser interfaces\\ntraffic awareness driver assistance\\nstereovision\\neye-tracking\\nhead-up display\\ntraffic lights\\nobstacles\\nassistance system\\ncrucial traffic situations\\ncomputer vision detection\\ntraffic participants\\neye tracking device\\nhead localization\\nhuman machine interface\\nacoustic module\\nfiducial markers\\nvideo frames\\nautonomous car\\nadas platform\",\"376\":\"roads\\ntraining\\nbenchmark testing\\nneural networks\\ncomputer architecture\\nvisualization\\nstandards\\nimage classification\\nmobile robots\\nmotion control\\nneural net architecture\\nneurocontrollers\\nobject detection\\npath planning\\nroad vehicles\\nrobot vision\\nautonomous navigation systems\\nroad area\\nmaneuvers\\nvisual road detection\\nconvolutional neural network architecture\\nnetwork-in-network architecture\\nnin architecture\\ncontextual window sizes\\nautonomous vehicle\",\"377\":\"proposals\\nthree-dimensional displays\\nsemantics\\ndetectors\\nobject tracking\\nimage segmentation\\ncameras\\ncomputer vision\\nimage motion analysis\\nmultiscale object candidates\\ngeneric object tracking\\nstreet scenes\\nvision based systems\\nurban environments\\npractical driving scenarios\\nstatic object\\nmoving object\\nkitti dataset\\nannotated classes\",\"378\":\"estimation theory\\nnatural scenes\\nrobots\\nstereo image processing\\nruntime performance\\nreconstruction quality\\nstereo disparity accuracy\\nsemidense reconstruction\\nstereo imagery\\npiece-wise planar mesh\\nscene depth\\npotentially enable robots\\npeak frame-rate\\nstereo algorithms\\ntunable stereo reconstruction\",\"379\":\"rotors\\naerodynamics\\nsensors\\nreal-time systems\\nvehicle dynamics\\nrobots\\nangular velocity\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\ngeometry\\nhelicopters\\nlinear systems\\nmicrorobots\\nmobile robots\\nscheduling\\ntrajectory control\\nvelocity control\\nhigh-speed controlled maneuver\\nautonomous 19-gram quadrotor\\ngain scheduling control strategy\\nlinear time-invariant controller design\\nlti controller design\\nspeed planning\\ncubic function\\ngeometric generalization\\ntrajectory generation\\ncontroller synthesis\\nmicrounmanned aerial vehicle\\nmuav\",\"380\":\"robot sensing systems\\nvisualization\\ntarget tracking\\ncameras\\nvibrations\\ncalibration\\nautonomous aerial vehicles\\ngaze tracking\\nhelicopters\\nimage motion analysis\\nrobot vision\\nhyperacute gimbal eye\\nprecise hovering\\nbio-inspired artificial eye\\ncustom-made gimbal system\\nmoving target tracking\\nquadrotor hovering\\naiborne eye\\nbio-inspired reflex\\nrobot gaze locking\\nrobot rotations\\nrobot disturbances\\nvisual processing algorithm\",\"381\":\"aerodynamics\\nkinematics\\njoints\\nrobot kinematics\\nmorphology\\nattitude control\\nautonomous aerial vehicles\\nmotion control\\nposition control\\nbat bot\\nbiologically inspired flying machine\\nbat aerial locomotion\\nflight capabilities\\nbiologically inspired bat robot\\nbody attitude control\\nnonlinear forces\\naerodynamic\\nmusculoskeletal mechanism\\nbiologically inspired soft robot\\nflapping machine\\ndegrees of actuation\\ndoa\\nuntethered flights\\nbat flight mechanism design\\nforelimb flapping motion\\nforelimb mediolateral motion\\nhindlimb dorsoventral motion\",\"382\":\"vehicles\\nbatteries\\nrobot sensing systems\\ntesting\\ninsects\\naerospace components\\naerospace control\\nautonomous aerial vehicles\\nbiomimetics\\niterative methods\\nopen loop systems\\nradiocommunication\\nsensors\\nexternal motion capture arena\\nonboard sensors\\ndata collection\\nrobust development environment\\nphysical vehicle\\nflight test iteration\\nopen-loop flight control\\nwireless communication\\nintegrated electronics\\nthrust generation\\nbenchtop testing\\nactive flight\\ngliding flight\\nbiologically inspired untethered vehicle\\nflight energetics experiments\\nflight control experiments\\nuntethered flapping-wing platform\",\"383\":\"fasteners\\ndrag\\nactuators\\nforce\\nresonant frequency\\nvehicles\\nfrequency response\\naerodynamics\\naerospace robotics\\ncontrol system synthesis\\ndamping\\nnonlinear control systems\\ninsect-scale flying robots\\nefficiency improvement\\ncomplex system dynamics\\nlift production\\npower consumption\\nexpected payload\\nvehicle mass\\nmean lift improvement\\nharvard dual-actuator robobee\\nnarrow actuation force window\\noptimal passive wing hinge angle\\nhoney bees\\nwing dynamics\\npassively rotating wing hinges\\nactuation-limited flapping-wing vehicles\\nnonlinear damping model\\nautonomous operation\\nunderactuated flapping-wing vehicles\\nsystem design improvement\\nnonlinear resonance modeling\",\"384\":\"rotors\\nsensors\\nvelocity control\\naerodynamics\\nforce\\nvisualization\\npayloads\\nautonomous aerial vehicles\\nhelicopters\\nvehicle dynamics\\nwakes\\nthrust loss saving design\\noverlapping rotor arrangement\\nsmall multirotor unmanned aerial vehicles\\ninspection operations\\nsurveillance operations\\ndisaster site observations\\nbuilding inspections\\ncivilian applications\\nheavy sensors\\nbatteries\\npayload capacity\\nrotor flow interaction\\naerodynamic perspective\\nwake flow\\noctorotor uav configuration\",\"385\":\"robots\\ngrasping\\nforce\\nplugs\\ngrippers\\nplastics\\nvehicles\\naerospace robotics\\nmobile robots\\npatents\\nvacuum pumps\\nversatile aerial grasping\\non-board vacuum pump\\npatented self-sealing suction cup technology\\nlocal pulling contact forces\\nobject grasping\\nself-sealing nature\\naerial applications\\nmicropump vacuum generator\\ngripper components\",\"386\":\"image edge detection\\nhistograms\\noptical sensors\\noptical imaging\\ndrones\\ncameras\\nestimation\\nimage sequences\\nmobile robots\\nrobot vision\\nvelocity measurement\\nlocal histogram matching\\noptical flow computation efficiency\\nvelocity estimation\\npocket drones\\nautonomous flight\\nstm32f4 microprocessor\\nstereo-camera\\nedge histograms\\nlocal optical flow\\nsubpixel flow determination\\ntime horizon adaptation\\nvelocity measurements\\nvelocity control-loop\",\"387\":\"vehicles\\nactuators\\ntorque\\nforce\\npropellers\\nrotors\\nvehicle dynamics\\nautonomous aerial vehicles\\nrobot dynamics\\nreversible motor-propeller actuators\\nvehicle rotational dynamics\\nvehicle translational dynamics\\nvehicle design\\nvehicle agility\\neight-rotor configuration\\ngeneric actuator configurations\\nstatic force analysis\\ntorque analysis\\nomni-directional aerial vehicle\",\"388\":\"legged locomotion\\nmathematical model\\nacceleration\\nadaptation models\\ngravity\\nactuators\\nhelicopters\\nprototypes\\nrobot dynamics\\nrotors\\ndynamic underactuated flying-walking robot\\nduck robot\\nhigh-mobility flying platform\\npassive-dynamic legs\\nprototype robot design\\naerial platform\\npassive walking\\nlow-energy terrestrial locomotion\\nactive walking\\nquadcopter rotors\\nflat surfaces\\ninclined surfaces\",\"389\":\"vehicles\\npropellers\\nattitude control\\nforce\\nangular velocity\\nvehicle dynamics\\naerodynamics\\naerospace robotics\\ncascade control\\ncontrol system synthesis\\nperturbation techniques\\nposition control\\nrobot dynamics\\nrobust control\\nstochastic processes\\ncontrollable flying vehicle\\nsingle-moving part\\nmonospinner\\nthree-translational-degree-of-freedom\\ntwo-rotational degree-of-freedom\\nscalar control input\\nthrust magnitude\\ncascaded control strategy\\ninner attitude controller\\nouter position controller\\nvehicle design\\nrobustness metrics\\nhover maintenance ability\\nperturbations\\ninput saturation probability\\nstochastic model\\nmechanical design\\ncontrol design\",\"390\":\"torque\\nfasteners\\ndrag\\naerodynamics\\nvehicle dynamics\\nforce\\nsteady-state\\nactuators\\naerospace components\\naerospace control\\nautonomous aerial vehicles\\nhinges\\nlaminates\\nmicrorobots\\nmobile robots\\nstability\\ntorque control\\nyaw torque generation\\nwing pitching dynamics\\nmicroaerial vehicle\\ndual-actuator mav\\nflight stability\\nflight control\\nlaminate based nonlinear hinge stiffness\\nsplit-cycle actuation\",\"391\":\"aircraft\\nrotors\\naircraft propulsion\\nfasteners\\ngravity\\nbatteries\\naerospace components\\naerospace propulsion\\nautonomous aerial vehicles\\nhelicopters\\nsecondary cells\\nsolar cell arrays\\nsolar powered vehicles\\nsmall-scale solar-powered uav\\nsuav:q\\nhybrid approach\\nsolar-powered flight\\naerial platform\\nflight time\\nlong-range capability\\nmaneuverability characteristics\\nstationary characteristics\\nmultirotor platform\\nsolar cell\\nenergy storage\\npropulsion system technology\\nsmall-scale hybrid unmanned aerial vehicle\\nquad-rotor\\nenergy collection\\nenergy supply\\nsolar-powered fixed-wing aircraft\\npower electronics\\nbattery charging\\npower loading\\nsolar array\",\"392\":\"three-dimensional displays\\nrobot kinematics\\ncalibration\\ngrippers\\ntracking\\ntrajectory\\nactuators\\ngaussian processes\\nhuman-robot interaction\\nregression analysis\\ntelerobotics\\n3d gaze cursor\\ncontinuous calibration\\nend-point grasp control\\nrobotic actuators\\nmotor intention\\nparalysed patients\\nimpaired patients\\nhuman-robot control interface\\nspatial cursor signal\\ngaze-based robotic teleoperation\\n3d calibration\\nnonlinear regression\\ngaussian process regressors\\nmultijoint robot arm\\neuclidean space\\nrobot gripper\",\"393\":\"decision making\\ncognition\\ncomputer architecture\\ncomputational modeling\\nmeasurement\\nrobot sensing systems\\ncognitive systems\\ngeneralisation (artificial intelligence)\\ngesture recognition\\nhuman-robot interaction\\nintelligent robots\\nlearning systems\\nself-reflective risk-aware artificial cognitive model\\nrobot response\\nhuman activities\\nhuman-robot teaming\\nhuman action interpretation\\ncooperative robots\\nintelligent interaction\\nhuman peers\\ninterpretability indicator\\npretrained activity recognition model\\ngeneralizability indicator\\nco-robot self-reflection\\nreasoning\\ntopic modeling\\nsrac model\\nrobot action risk\\nhuman behavior response\",\"394\":\"robot kinematics\\ncameras\\nrobot vision systems\\nhead\\nthree-dimensional displays\\nhumanoid robots\\nimage motion analysis\\ninference mechanisms\\nobject recognition\\nrobot vision\\nmarkerless perspective taking\\nunconstrained environments\\nreasoning\\nicub\\nvisuospatial perspective taking\\nsingle depth camera\\nmarker-based motion capture systems\\npoint cloud\\negocentric perspective\",\"395\":\"predictive models\\nprediction algorithms\\ncomputational modeling\\nplanning\\nintegrated circuit modeling\\ncost function\\nrobots\\nhuman-robot interaction\\nmobile robots\\nmotion control\\nobject tracking\\npath planning\\npredictive control\\nrobot vision\\nplanning-based algorithms\\nhuman motion prediction\\nvisual tracking\\nplanning techniques\\nfast marching method\\ngoal estimation\\nspatiotemporal prediction\",\"396\":\"predictive models\\nbiological system modeling\\ntime series analysis\\nwheelchairs\\nmethod of moments\\nrobot sensing systems\\ncollision avoidance\\ngaussian processes\\nlearning (artificial intelligence)\\nleast squares approximations\\nmobile robots\\nmotion estimation\\nobject detection\\npedestrians\\nrobot vision\\ntime series\\ntime series models\\npedestrian motion prediction\\nrobot systems\\nreal-world environments\\ndynamic objects\\npredictive object motion model\\npredictive linear gaussian model\\nplg model\\nleast-squares criteria\\nsynthetic datasets\\nwheelchair robot\",\"397\":\"robots\\nspeech\\ncontext modeling\\ncomputational modeling\\ncontext\\nwrist\\nprobability\\nbayes methods\\nintelligent robots\\nunsupervised learning\\nmultimodal referring expression interpretation\\ncommunicative input processing\\nrobot reaction\\nmultimodal bayes filter\\nperson referential expressions\\nperson spoken words\\nperson gestures\\nsimulated home kitchen domain\\ncontextual knowledge learning\\nincrementally communicative input processing\\nrelative word timing\\nrelative gesture timing\",\"398\":\"shape\\nfasteners\\nalgorithm design and analysis\\nrobots\\ngames\\nprototypes\\nprobabilistic logic\\nhaptic interfaces\\nhuman computer interaction\\ninteractive systems\\nmodular interactive devices\\ntouchscreens\\nhinge-mounted turntable mechanism\\ncubimorph mechanical design\\nprobabilistic roadmap algorithm\",\"399\":\"robot kinematics\\nlearning (artificial intelligence)\\nelectromyography\\ndata models\\nrobot control\\nuncertainty\\nhuman-robot interaction\\nservice robots\\nlearning assistive strategies\\nuser robot interactions\\nreinforcement learning approach\\nmovement assistance\\nmovement rehabilitation\\nmechanical models\\nhuman user\\nrigid-body dynamics\\ncenter of mass\\nzmp\\ninverted pendulum\\npolicy search problem\\nemg measurement system\",\"400\":\"visualization\\nrobot kinematics\\ncomputational modeling\\nmathematical model\\nrobot sensing systems\\ncontext\\nfeature extraction\\nhumanoid robots\\nhuman-robot interaction\\nmaximum likelihood estimation\\nobject detection\\nrobot vision\\ncommunicative behavior modeling\\nrobot verbal behaviors\\nrobot nonverbal behaviors\\nhuman partner\\ncomputer vision\\ncognitive psychology\\nlow-level scene features\\nhigh-level scene features\\nuser attention\\nlikelihood maximization\\nhuman-robot collaboration\\nhuman evaluations\",\"401\":\"safety\\ncomputer architecture\\nservice robots\\nsoftware\\nmedical robotics\\nrobot sensing systems\\ncontrol engineering computing\\npublic domain software\\nsafety-critical software\\nsoftware architecture\\nsurgery\\ncomponent-based robotic systems\\nmedical robot systems\\nsurgical robot systems\\nsafety-critical systems\\nrobot safety\\nrobot hardware\\nrobot software\\nhuman-robot collaboration\\ncomponent-based frameworks\\nstate-based model\\nsafety-oriented software architecture\\nsafecass open source software framework\",\"402\":\"robots\\ninspection\\nwelding\\ncleaning\\nheadphones\\nthree-dimensional displays\\naugmented reality\\ncooperative systems\\nfeedback\\nmanipulators\\nstatistical analysis\\nvisual feedback methods\\nrobot assisted reaching feedback methods\\nnasa task load index survey\\nanova\\npassive handheld wand\\nhandheld robot arm\\nrobot arm gesturing\\n2d screen\\nsee-through ar display\\nstereoscopic vr display\\ndegree of freedom\\nspatial exploration task\\nfeedback information\\ncooperative handheld robot device\\nspatial guidance\",\"403\":\"grounding\\nrobot kinematics\\njoints\\nindexes\\nhumanoid robots\\nsemantics\\ncomputer aided instruction\\ncontrol engineering computing\\nhuman-robot interaction\\ninteractive systems\\nhierarchical action learning\\ninteractive grounding\\nbody parts\\nprotoactions\\nhumans programming\\nspoken language\\nhumanoid robot icub\\nhand coded a-priori knowledge\\nchild development theories\\nhierarchical learning\\nrobot motor babbling\\non-the-fly spoken descriptions\\nlinear model\",\"404\":\"containers\\nbenchmark testing\\nsoftware\\nrobot kinematics\\ngrasping\\ncontrol engineering computing\\nrobots\\nsoftware engineering\\ncitk platform\\nrobot software\\nvirtualization technology\\nsoftware container\\nrobot full-system simulation\\nsustainable robotics system benchmarking\\nrobobench\",\"405\":\"distortion\\ncameras\\nrobot vision systems\\ndistortion measurement\\ntraining\\ndexterous manipulators\\nimage capture\\nimage sensors\\nlearning (artificial intelligence)\\nmeasurement errors\\nrobot vision\\nmultipath distortion removal\\ntime-of-flight range images\\nrobotic arm setup\\nmodulated light signals\\nmodulated light scenes\\nlearning-based approach\\ntof camera\\ndeep learning\\ntof range images\\nhigh precision structured light sensor\\nerror reduction\",\"406\":\"shape\\nrobots\\ndata models\\ncameras\\ngrasping\\ndatabases\\nthree-dimensional displays\\nimage classification\\nimage colour analysis\\nimage representation\\nmanipulators\\nprobability\\nrandom processes\\nrobot vision\\nshape recognition\\nstability\\nglobal object shape exemplar-based prediction\\nlocal shape similarity\\nrobot\\ngraspable object\\nrgb-d camera\\nlocal shape representation\\nlatent global object shape prediction\\npredicted average global shape\\nscene exploration\\nrandom forest classifier\\npositively predicted query data point\",\"407\":\"robots\\ngrasping\\ngrippers\\nthree-dimensional displays\\ndata models\\nlabeling\\ntraining\\nimage classification\\nlearning (artificial intelligence)\\nneurocontrollers\\nregression analysis\\nrobot vision\\nimage patches\\n18-way binary classification\\nregression problem\\ncnn\\nconvolutional neural network\\ntrial-and-error experiments\\nsemantics\\nhuman-labeled datasets\\nmodel free learning-based robot grasping\\nsupersizing self-supervision\",\"408\":\"visualization\\nwavelet transforms\\ncameras\\nvisual servoing\\nimage resolution\\ncontrol system synthesis\\nmanipulators\\nmatrix algebra\\nrobot vision\\nvelocity control\\nwavelets-based visual servoing\\nvisual servoing control law\\nglobal information image\\nwavelet spatial coefficients\\nvisual signal\\nvisual controller design\\nrobot spatial velocity\\ntask function controller\\n3ppsr parallel kinematic manipulator\\neye-to-hand configuration\\nmultiple resolution interaction matrix\",\"409\":\"visualization\\nvisual servoing\\nprobes\\nrobot kinematics\\nconvergence\\nrobustness\\nbiomedical ultrasonics\\nmedical robotics\\nrobot vision\\nvelocity control\\nwavelet transforms\\nultrasound-based visual servoing\\nshearlet decomposition coefficient\\n6 degrees-of-freedom vision-based controller\\nrobot-assisted medical application\\ntime-variation\\nus probe\\nspatial velocity\\ntask-function control law\\nabdominal phantom\",\"410\":\"needles\\nprobes\\nimage segmentation\\nvisual servoing\\ntracking\\nultrasonic imaging\\nbiomedical ultrasonics\\nhealth care\\nmedical robotics\\nmedical ultrasound probe\\npercutaneous needle insertion\\nultrasound imaging\\nhospitals\\nplacement accuracy\\nhealth care personnel\\nneedle orientation\\nneedle position\\nvisual features\\nclinical setting\\nwater tank\",\"411\":\"visual servoing\\ncameras\\nvisualization\\nshape\\nhuman-robot interaction\\nmanipulators\\nrobot vision\\nvita\\nvisual task specification interface\\nhuman robot interface\\nhri\\nsemiautonomous human-in-the-loop control\\nunstructured environments\\nimage editor\\ngeometric overlays\\nvisual task specification\\nuncalibrated image-based visual servoing\\nuvs\\nshape sorter\\ncircular lid\\nbarrett wam arm-and-hand\",\"412\":\"ultrasonic imaging\\nprobes\\nforce\\nimage quality\\nmotion compensation\\nrobot sensing systems\\nacoustic materials\\nend effectors\\nfeature extraction\\noptimisation\\nrobots\\nconfidence-driven control\\nultrasound probe\\ntarget-specific acoustic window optimization\\nrobotic ultrasound imaging\\nconvex probe\\nend-effector\",\"413\":\"support vector machines\\nvisual servoing\\nvisualization\\ncameras\\nhistograms\\nshape\\nfeature extraction\\nend effectors\\nimage classification\\nlearning (artificial intelligence)\\npose estimation\\nposition control\\nobject instance\\nvisual feature\\nobject geometry\\ngeometrical feature\\nappearance variation\\nshape variation\\ndiscriminative learning framework\\nbinary classifier\\nclassification error\\nend-effector control\\nlinear svm\\nkernel svm\\nexemplar support vector machine\\nhistogram of oriented gradients\\nhog feature\\nobject category\\nzero terminal velocity\\ncamera pose error\",\"414\":\"visualization\\nnavigation\\nthree-dimensional displays\\nfeature extraction\\nreal-time systems\\ninsects\\nadaptive optics\\naircraft control\\naircraft navigation\\nautonomous aerial vehicles\\nclosed loop systems\\ndynamic programming\\nhelicopters\\nimage sequences\\npath planning\\nposition control\\nrobot vision\\nvelocity control\\nsparse snapshot-based navigation strategy\\nuas guidance\\nnatural environments\\nvisual navigation\\nreal-time guidance\\nautonomous rotorcraft control\\noutdoor environments\\nsnapshot sequences\\nexploratory journey\\n3d position\\nvelocity\\nmapless algorithm\\nroute learning\\ndrift-free navigation\\nunknown 3d environments\\nunstructured 3d environments\\nsparse visual route description\\nclosed-loop flight experiments\\nsmall-size rotorcraft\\ndynamic path optimisation\\nunmanned aerial systems\",\"415\":\"graphics processing units\\ncameras\\nreal-time systems\\nstreaming media\\nhelicopters\\nsensors\\naircraft control\\nautonomous aerial vehicles\\nembedded systems\\nimage filtering\\nimage resolution\\nobject detection\\npose estimation\\nrobot vision\\ngraphics processing unit\\ncluttered environments\\nmarker detection\\nsmall quadrotor\\nautonomous landing\\nautonomous takeoff\\nvideo stream off-board\\non-board high-frequency pose estimation\\nhigh-performance cpu\\/gpu embedded system\\nparallel image processing\\nuav\\nunmanned aerial vehicle\\nreal-time system\\ngpu-based pose estimation\",\"416\":\"monitoring\\nsemantics\\nrobot kinematics\\nvisualization\\nreal-time systems\\nresource management\\nmulti-robot systems\\nobject detection\\nrobot vision\\nstatistical analysis\\nmulti-robot visual support system\\nadaptive roi selection\\nregion-of-interest selection\\ngestalt perception\\nvisual assistance system\\nautonomous monitoring robot\\ngestalt detection\\nlatent dirichlet allocation\\nprobabilistic topic model\\nrisk degree\",\"417\":\"robot sensing systems\\nthree-dimensional displays\\nentropy\\ncameras\\nimage reconstruction\\nprobabilistic logic\\ncontrol engineering computing\\nimage sensors\\nmobile robots\\nmotion control\\npublic domain software\\nrobot vision\\nsolid modelling\\ninformation gain formulation\\nactive volumetric 3d reconstruction\\nnext-best view selection\\nvolumetric reconstruction\\nmobile robot\\ncamera\\nprobabilistic volumetric map\\ndiscrete candidate views\\nvisibility likelihood\\nrobot movement\\nutility functions\\nmodular software system\\nopen source\",\"418\":\"joints\\nrobots\\nbones\\nbiomechanics\\nthumb\\nligaments\\nartificial limbs\\nmanipulators\\nmedical robotics\\ntelerobotics\\nbiomimetic anthropomorphic robotic hand design\\nartificial limb regeneration\\ntelemanipulation\\nhuman hand biomechanics\\nprosthetic hand\",\"419\":\"potential energy\\nkinematics\\ndynamics\\nkinetic theory\\nexoskeletons\\nhip\\nfoot\\nlegged locomotion\\nmedical robotics\\nnonlinear control systems\\northotics\\npatient rehabilitation\\nrobot kinematics\\ntorque control\\nunderactuated potential energy shaping\\npowered ankle-foot orthosis\\ncontrol methodologies\\nrehabilitation orthoses\\nrehabilitation exoskeletons\\nnormal kinematics\\nkinematic control\\ncontrol category\\nkinetic control\\nvirtual body-weight support\\nstroke gait rehabilitation\\nnonlinear control law\\nhuman-like biped model\\ntorque control strategy testing\\npositive virtual body-weight augmentation\\nnegative virtual body-weight augmentation\",\"420\":\"hip\\nforce\\nlegged locomotion\\nactuators\\nthigh\\ncable shielding\\ntiming\\nforce control\\niterative methods\\nmedical control systems\\npatient rehabilitation\\nsensors\\nimu-based iterative control\\ninertial measurement unit\\nhip extension assistance\\nsoft exosuit\\nmaximum hip flexion angle estimation\\nlab-based multi-joint actuation platform\\nsensor reconfiguration\\ncontroller design\\nsuit-human interface\\ngait detection\\nstep-by-step actuator position profile generation\\nonset timing\\npeak timing\\npeak magnitude\\ndelivered force control\\nmetabolic reduction\\nbiomechanical response\\nphysiological response\",\"421\":\"force\\nelectronic mail\\ntextiles\\nhip\\nsensor systems\\nactuators\\ncables (mechanical)\\nforce control\\ngait analysis\\nmetabolic rate reductions\\nankle joint\\nactive force\\npretension\\nposition control\\nreal-time data measurement\\ngyro sensors\\nload cell\\nbowden cables\\nforce transmission\\nactuation system\\nhip flexion\\nankle plantarflexion\\npositive-power period\\nnegative-power period\\nassistance level\\nbody-worn lower-extremity soft exosuit\\ntextile architecture\\nload paths\\nassistive forces\\npositive power control\\nnegative power control\",\"422\":\"electromyography\\nprosthetics\\nmuscles\\nswitches\\nelectrodes\\nfrequency control\\nencoding\\nartificial limbs\\ndamping\\ngrippers\\nhuman-robot interaction\\nlinear systems\\nmedical robotics\\npower grasp\\npisa\\/iit softhand\\nhand movement\\nlinear descriptor systems\\ndynamic synergies\\npassive damping components\\nunderactuated soft hand\\ncontrol signals\\ndynamic frequency content\\nmovement intentions\\nhi-tech poly-articular hands\\ngripper-like systems\\nhand prosthetics\\nenhanced prosthesis control\\nhand embodiment\\nnatural user commands\\ndynamic content matching\\nsofthand pro-d\",\"423\":\"force\\ngrasping\\nthumb\\nprosthetic hand\\nwrist\\nsensors\\ndexterous manipulators\\nforce control\\nforce sensors\\nobject detection\\nprosthetics\\nresistors\\nrobot vision\\nforce-and-slippage control strategy\\npoliarticulated prosthetic hand\\nreal-time object slippage detection\\ncontrol strategy\\ngrasping force regulation\\nslippage prevention\\nfinger coordination method\\nih2 azzurra\\nprensilia\\nhuman-like behaviour\\ntips force sensing resistors\",\"424\":\"exoskeletons\\nimpedance\\ntorque\\nsensors\\nthigh\\nobservers\\nhip\\nhandicapped aids\\nhuman-robot interaction\\nmedical robotics\\nmotion control\\ntime-varying systems\\ntorque control\\nlower limb exoskeleton\\nsit-to-stand movement assistance\\ndaily living activity\\nsts movement\\nelderly people\\ndependent people\\nintention-based active impedance control\\naic strategy\\nmechanical impedance\\nhuman-exoskeleton system\\nexoskeleton power assistance\\nhuman joint torque observer design\\nhuman joint torque estimation\\njoint angle information\\nelectromyography\\nemg\\nforce sensor\\ntorque sensor\\ntime-varying desired impedance model\\nwearer lower limb motion ability\\nwearer motion intention\",\"425\":\"actuators\\nthumb\\nrobots\\nforce\\nfabrics\\nelectromyography\\nforce control\\nmedical robotics\\npneumatic actuators\\nprosthetics\\nradiofrequency identification\\nrobot kinematics\\nradio-frequency identification technique\\nsurface electromyography\\nkinematic assistance\\nkinetic assistance\\nmanipulation task\\nsoft fabric-regulated pneumatic actuator\\nfunctional grasp pathology\\nhand assistive application\\nrfid\\nemg\\nuser intent detection\\nfabric-regulated soft robotic glove\",\"426\":\"torque\\nprosthetics\\nbrushless dc motors\\nmathematical model\\nrobots\\nbiomechanics\\nerror compensation\\nfeedback\\nfrequency response\\nmedical robotics\\nmotion control\\ntorque control\\nenergy-efficient torque controller\\npassive dynamics\\nrobotic transtibial prosthesis\\nlevel-ground walking\\nankle joint\\nhuman body locomotion\\nhierarchical structure\\nlow-level controller\\nmotor current generation\\nmotor current control\\nhigh-level command\\nhigh-level controller\\nforward estimator\\nfeedback compensator\\nmotor torque estimation\\nankle torque\\nprosthesis model\\ntransmission gain\\nlow-level control error\\ntorque measurement\\nstep response\\nlinear torque tracking\\nsmall rms tracking error\",\"427\":\"strain\\ngrasping\\ninterference\\nprosthetics\\ntactile sensors\\nforce\",\"428\":\"trajectory\\nlegged locomotion\\ncomputational modeling\\nhumanoid robots\\ngenerators\\noptimal control\\nkinematics\\npredictive control\\ntrajectory control\\ngeneralized legged locomotion\\nunder-actuated systems\\nhumanoid\\nquadruped robots\\nwalking pattern generator\\nstable trajectory\\nangular momentum\\nmodel predictive controller\\nacyclic contact planner\\ncontact sequence\\n3d model\\nstable walking pattern\\ndynamically-stable whole-body trajectory\\nsecond-order hierarchical inverse kinematics\\nhandrail support\",\"429\":\"legged locomotion\\nfriction\\nmathematical model\\nforce\\nfoot\\nrobot kinematics\\nforce control\\ngait analysis\\nmotion control\\nstability\\n3dof passive dynamic walking motion\\ncompass-like biped robot\\nsemicircular feet\\nslippery downhill\\npassive compass gait stability\\nsliding friction force\\nholonomic constraint\",\"430\":\"legged locomotion\\nnumerical models\\nmathematical model\\nanalytical models\\ndynamics\\nsprings\\ncompliant mechanisms\\npath planning\\nreduced order systems\\nrobot dynamics\\nmodel-based bounding\\nquadruped robot\\nrobot dynamic bounding locomotion\\nsagittal plane model\\nrobot natural dynamics\\ntwo-rolling-leg model\\ntrl model\\ncompliant legs\\nrolling legs\\nplanar rigid-body model\\nnumerical fixed point analysis\\ntouchdown conditions\\npassive dynamic behaviors\\nleg trajectories\\npreset position control strategy\\ncomplex gait behavior\\nreduced-order model\\nbehavioral guidance\",\"431\":\"legged locomotion\\nimpedance\\nforce\\nsurface impedance\\nfoot\\ntrajectory\\nrobot dynamics\\nstability\\nrobot locomotion\\nsoft ground\\nhard ground\\nground properties\\ndynamic behavior\\nlegged robots\\nground impedance\\nfoot-ground interaction\\nmit super mini cheetah robot\\nself-disturbances\\nground stiffness\\nground friction\\nvariable-terrain control\\nground measurement\",\"432\":\"legged locomotion\\nactuators\\ndc motors\\nwheels\\ntorque\\ncompliant mechanisms\\ndeformation\\ndesign engineering\\npower transmission (mechanical)\\nrobot dynamics\\ncompliant terrains\\nquadruped robot pronking gait\\nground deformation\\nreaction wheel\\nbody pitch\\nleg motor drivetrains\\ndynamic models\\nlegged robot design\\ncommanded apex heights\\ndeformable terrains\\nforward velocities\",\"433\":\"feature extraction\\nrobot sensing systems\\nacoustics\\nlegged locomotion\\nsupport vector machines\\nfrequency-domain analysis\\noperating systems (computers)\\nterrain mapping\\nacoustics based terrain classification\\nlegged robots\\nterrain-robot interactions\\nvaluable sensing modality\\nonline realtime terrain classification system\\n32-dimensional feature vector\\nmulticlass support vector machine\\nrobotic operating system\\nros\",\"434\":\"legged locomotion\\nroads\\nrobot sensing systems\\nservomotors\\nactuators\\nnavigation\\nservomechanisms\\nroad following\\nblind crawling robot\\nmobile robot navigation\\nforward looking camera\\ntactile information estimation\\nrobot servo drives\\nonline classification\\nroad terrain\\noutdoor environment\\nurban park pathways\\nterrain classification\\ncrawling robot\",\"435\":\"mobile communication\\nlegged locomotion\\nmanipulators\\nacceleration\\ntrajectory\\nfoot\\nmultilegged mobile robots\\nmanipulation capability\\nlocomotion\\nbase robot trunk\\nintegrated control framework\\npayload estimation scheme\\nstatic platform\\nwalking mobile platform\",\"436\":\"motion segmentation\\nprinters\\nsolid modeling\\nfield-flow fractionation\\ncomputational modeling\\nprinting\\nfabrication\\nextrusion\\nmanufacturing systems\\nrapid prototyping (industrial)\\n3d-aware toolpath algorithm\\nfused filament fabrication\\nrapid prototyping\\ncustom fabrication\\nthree-dimensional model\\nlocal feature independence\\nfff slicing methods\\nextrusionless travel\",\"437\":\"system recovery\\nrobustness\\nmanufacturing systems\\npetri nets\\ntopology\\nsupervisory control\\nassembling\\nrobust control\\nautomated manufacturing systems\\nassembly operations\\nams\\ndeadlock problem\\nblocking problem\\nrobust supervisory control policy\\nstructure-oriented specification\\nmonolithic specification\",\"438\":\"friction\\ngrippers\\nrobot sensing systems\\nwrapping\\nmathematical model\\nball bearings\\nknow tying\\nsimple knots\\ntying control\\nspecialized gripper\\nrobotic manipulation\\nruyi knot\",\"439\":\"robot sensing systems\\nface\\ngrasping\\nshape\\ncomplexity theory\\ngeometry\\nmanipulators\\narbitrary knots topology\\ncontact locations\\ngrasp points\\nknot diagram\",\"440\":\"three-dimensional displays\\nrobot kinematics\\nshape\\nautomation\\nkinematics\\ncouplings\\nrobots\\nshapes (structures)\\nstructural engineering\\nrobotic folding\\nautomatic folding\\n2d structure\\n3d structure\\ntwo-dimensional structures\\nthree-dimensional structures\\nrobotic ribbon folding concept\\nshape retention\\nmacroscopic scale\\nflexure orientation\\nstructural elements\\nplanar kinematic linkages\\nnoncrossing four-bar mechanism\",\"441\":\"head\\nvisualization\\ncontrol systems\\nalgorithm design and analysis\\nskeleton\\nrobots\\ntracking\\nbiology\\nautomated system\\ninvestigating sperm orientation\\nfluid flow\\nmammalian sperms\\nfemale reproductive tract\\nrheotaxis\\nshort distance guidance\\negg cell\\ntail behavior\\nmanipulate human sperm orientation\\nquantifies sperm tail behavior\\nsperm head angle\\nautomated sperm selection\\nhead angle tracking\\ndynamic sperm turning behavior\",\"442\":\"system recovery\\nmanufacturing systems\\nautomation\\npetri nets\\ntrajectory\\nprocess control\\ncomputers\\nassembling\\ncontrol system synthesis\\ndistributed control\\npredictive control\\nproduction management\\ndistributed supervisor synthesis\\nautomated manufacturing systems\\nflexible routes\\nassembly operations\\nams\\naesm structure\\nmarked graph blocks\\ndeadlock resolution\\nmonolithic methodologies\\nsiphon-based mechanism\\nmodel predictive control techniques\",\"443\":\"markov processes\\nrobot sensing systems\\nautomobiles\\nplanning\\nuncertainty\\nmobile robots\\ntemporal logic\\nvehicles\\nsimultaneous model identification\\ntask satisfaction\\ntemporal logic constraints\\ncyber-physical systems\\nautonomous cars\\nnuclear hazard inspection robots\\nautomated fault detection\\nformal synthesis\\nverification techniques\\ninformation-seeking trajectories\\nanomaly detection\\nprime motivation\\nadversarial markov decision process\\nlinear temporal logic formula\\nrobot-environment interaction\\nadversarial mdp\\nltl formulae\\nrobotic car\\nsurveillance operation\",\"444\":\"planning\\nsemantics\\nuncertainty\\ntrajectory\\nprobabilistic logic\\nsimultaneous localization and mapping\\nmobile robots\\noptimal control\\npath planning\\nprobability\\nslam (robots)\\nstochastic systems\\ntemporal logic\\ntrajectory control\\noptimal temporal logic planning\\nprobabilistic semantic maps\\nrobot motion planning\\ntemporal logic constraints\\nsemantic simultaneous localization and mapping\\nslam\\nlinear temporal logic specification\\nltl specification\\noptimal control problem\\nstochastic logic formula evaluation\\ndeterministic shortest path problem\\nconfidence parameter\\nrobot trajectory\\nadmissible heuristic function\\ntemporal logic specification\\na* algorithm\\nsimulated semantic environment\\ndifferential-drive robot\",\"445\":\"legged locomotion\\nrobot kinematics\\nelectromyography\\nmuscles\\nelectroencephalography\\nkinematics\\nmedical robotics\\npatient rehabilitation\\nunilateral walking surface stiffness perturbations\\nbrain response\\nbilaterally informed robot-assisted gait rehabilitation\\ngait impairment\\nneurological disorders\\nstroke\\nsensorimotor function rehabilitation\\ntreadmill-based therapy\\ninter-leg coordination mechanism\\nvst robotic device\\nvariable stiffness treadmill\\nclinical prospective\\nsupraspinal neural activity\\nhemiparesis\",\"446\":\"mobile communication\\nexoskeletons\\nlegged locomotion\\nhip\\npelvis\\nkinematics\\nend effectors\\nforce control\\nmanipulator kinematics\\nmedical robotics\\nmobile robots\\npatient rehabilitation\\nforce rendering performance\\nkinematically redundant dual layer actuation\\nmotion controlled holonomic mobile platform\\npelvis-hip exoskeleton module\\nassiston-gait overground gait\\nworkspace centering control\\nredundant kinematics\",\"447\":\"legged locomotion\\ngears\\nmuscles\\nfoot\\nknee\\nshafts\\ntraining\\ngait analysis\\ngeriatrics\\nhandicapped aids\\npatient rehabilitation\\nwalking assistance apparatus\\ngait training\\nexercise promotion\\npatient neurorehabilitation\\nankle joint plantarflexion\\nankle joint dorsiflexion\\nequipped person gait\\nelderly patients\",\"448\":\"frequency estimation\\ntime-frequency analysis\\ndamping\\nfrequency modulation\\npathology\\nband-pass filters\\nbioelectric phenomena\\nfast fourier transforms\\ninterpolation\\nmedical disorders\\nmedical signal processing\\ntremor parameter estimation\\ntremor suppression\\npathological tremor\\nfunctional electrical stimulation\\nfes\\namplitude estimation\\ntremor signal frequency\\nsliding fast fourier transform\\nsfft\\ninterpolation procedure\\nactual tremor signals\",\"449\":\"elbow\\nelectromyography\\nprosthetics\\nangular velocity\\nshoulder\\ncontrol systems\\nelectrodes\\nmedical control systems\\nmotion control\\ncoordinating shoulder motion\\nelbow motion\\nmyoelectric transhumeral prosthesis control approach\\nreaching task\\nsurface electromyograms\\nemg\\ninertia measurement unit\\nimu\\njoints coordinated control\\nvirtual environment\\npick-and-place task\\ncoordinated control approach\\nsequential control approach\",\"450\":\"tendons\\nactuators\\nforce\\nprototypes\\ncouplings\\nassistive devices\\nrobots\\nhandicapped aids\\nmedical robotics\\npatient rehabilitation\\nwearable exotendon networks\\nwhole-hand movement patterns\\nstroke patients\\nwearable hand rehabilitation device\\nwearable hand assistive device\\nhand impairments\\nsingle linear actuator\\ntendon network configurations\\nfull finger extension\\nproximal flexion\\ndistal extension\",\"451\":\"medical treatment\\nexoskeletons\\nlegged locomotion\\nrobot sensing systems\\ncontrol systems\\ncognition\\ngait analysis\\nhandicapped aids\\nmedical robotics\\npatient rehabilitation\\ncpwalker\\nrobotic platform\\ngait rehabilitation\\ncerebral palsy\\nposture disorder\\nmovement disorder\\nimmature brain\\ncognition impairments\\ncommunication disabilities\\nmotor disabilities\\nbehaviour issues\\nseizure disorder\\npain musculoskeletal problems\\nsecondary musculoskeletal problems\\nsmart walker\\nexoskeleton\\nrehabilitation robotics\\ntop-down approach\",\"452\":\"thumb\\nwires\\nrobots\\nelectron tubes\\nactuators\\npolymers\\ngrippers\\npressure sensors\\npolymer-based tendon-driven wearable robotic hand\\nexo-glove poly\\nsilicone\\nmultiple-user environments\\nunder-actuation mechanism\\nobject grasping\\nteflon tubes\\nwrap grasp\\nmat-type pressure sensor\",\"453\":\"servers\\ntraining\\ncomputers\\nrobots\\nmobile communication\\nfoot\\nbiomedical measurement\\ncontrol engineering computing\\ngraphical user interfaces\\nhuman-robot interaction\\nmedical computing\\nmedical robotics\\nmobile robots\\northotics\\npatient rehabilitation\\noverground mobile robot based gait rehabilitation system\\nusability testing\\nmodular powered orthosis\\nmopass system\\noverground walking training\\ngait pattern adjustment\\ngraphical user interface\",\"454\":\"vibrations\\ntorque\\nactuators\\nsteady-state\\nrobot kinematics\\nanalytical models\\ndexterous manipulators\\ngrippers\\nmotion control\\nviscoelasticity\\nunderactuated robot finger\\nfinger posture control\\nvariable vibration center effect\\nvvce\\ntwo-joint two-link system\\nactuator\\nviscoelastic joint\\nfinger mechanism design\\nobject grasping\\ngrasping motion\",\"455\":\"stators\\ninduction motors\\nrobot sensing systems\\nmobile robots\\ntorque\\nrotors\\nstability\\nsimbot\\nspherical induction motor based ballbot\\nhuman-sized dynamically stable mobile robot\\nmechanical complexity reduction\",\"456\":\"mathematical model\\nstability criteria\\ndelays\\nestimation\\ndelay effects\\ncontrol system synthesis\\nlinear quadratic control\\nnonlinear systems\\npendulums\\nstability\\ntweaking differentiators\\npr controller\\nfuruta pendulum\\nmechatronics systems\\nfeedback control law\\nunderactuated mechanical systems\\ntime delays\\nproportional-retarded control law\\npr control law\\nstabilization\\nunderactuated rotational inverted pendulum\\nlaboratory platform\\nlinear quadratic regulator\\nlqr\",\"457\":\"legged locomotion\\nforce\\nwinches\\nconnectors\\nfriction\\nforce control\\nmotion control\\nmulti-robot systems\\ntensile strength\\nstep climbing cooperation primitives\\nreversible connection\\nvelociroach hexapedal legged robots\\nremovable connection\\nrobot team\\nbody length\\ncoefficient of friction\\nquasistatic analysis\\nconnected robots\\nwinch module\\ncooperative climbing\\ncontrollable forces\\nconstant bounding frequency\\ndrive motors\\ntether tension\",\"458\":\"legged locomotion\\ndynamics\\ndatabases\\nplanning\\nrobot kinematics\\nheuristic algorithms\\ngaussian processes\\nmotion control\\npath planning\\nregression analysis\\nsearch problems\\nonline motion planning\\nmotion generation methodology\\nunderactuated dynamic planar walking\\nmotion primitives database\\nregression methodology\\nbest first graph search approach\\ngaussian process\",\"459\":\"foot\\nlegged locomotion\\nfriction\\nturning\\nforce\\nrobot sensing systems\\nadhesion\\nmotion control\\nrobot dynamics\\nelectroadhesive feet\\nturning control\\nlegged robots\\nlow voltage electroadhesives\\ndynamic turning\\nturning radii\",\"460\":\"robots\\nmathematical model\\nnavigation\\nsprings\\nprototypes\\ncomputational modeling\\ntrajectory\\ncollision avoidance\\nmicroactuators\\nmicrorobots\\nmobile robots\\nsampling methods\\ndifferential jumping navigation mode\\nmicrorobot navigation\\nlight weight microactuation mechanisms\\nplanning strategies\\nmultihop navigation\\nobstacle avoidance\\nasymmetrical thrust\\ndifferential jump model\\nstate propagation equations\\nsampling based motion planner\\ntrajectory compution\\nplanar surface navigation\\ndifferential jump mechanism\",\"461\":\"joining processes\\nrobot kinematics\\nend effectors\\nstability analysis\\ntrajectory\\nrobot sensing systems\\nasymptotic stability\\ngrippers\\nmanipulators\\nmobile robots\\nmotion control\\npath planning\\npoincare mapping\\npropellers\\nstability\\npoincare map\\ngenerated cyclic motion\\nvirtual connecting manipulation concept\\npropeller motion\\nflower-stick juggling task\\ngeneralized motion planning framework\\nvirtual connecting manipulation\\nnongrasping manipulation systems\\nrobotic motion control\\nvirtual connecting constraint\\nnongrasping manipulation control scheme\",\"462\":\"belts\\nmobile communication\\nheuristic algorithms\\nmathematical model\\nthree-dimensional displays\\nrobot sensing systems\\ncontrol engineering computing\\nconveyors\\nmechanical engineering computing\\nmobile robots\\nreachability analysis\\nautomatic mobile conveyor line configuration\\nmobile conveyor belts\\nassembly lines\\nairports\\nconveyor system\\ndisaster areas\\nreachability\",\"463\":\"inspection\\ntime-frequency analysis\\nrobots\\nestimation\\ntraining\\ndetectors\\nspectrogram\\nestimation theory\\nindustrial robots\\nlearning (artificial intelligence)\\nmaintenance engineering\\ndefect detection\\nmaterial condition estimation\\nensemble learning\\nhammering test\\nrobotic hammering inspection\\nsocial infrastructure maintenance\\nsocial infrastructures\\nmaterial defect conditions\\nautomated diagnosis methodology\\nsuperannuated social infrastructures\\nrobotic inspection\\nmechanical running-noise\",\"464\":\"optimization\\nrobustness\\nrobots\\ngrippers\\ntrajectory\\nbelts\\nforce\\nfood products\\nindustrial manipulators\\ninspection\\nmaterials handling equipment\\nmotion control\\noptimisation\\nmeat properties\\nvisual inspection\\ngripper configuration\\nrobot motion\\ndynamic simulation framework\\nrobotic optimization problems\\npartly stochastic optimization\\nbound optimization\\noptimization schemes\\ndeformable objects\\nrobotic pick-and-place operations\\nrobust optimization\",\"465\":\"robots\\nheating\\nsockets\\nshape\\nfabrication\\nsurface fitting\\ncontrol engineering computing\\nelastic constants\\nmanipulator dynamics\\npolymers\\nrapid prototyping (industrial)\\nredundant manipulators\\nshape memory effects\\nthree-dimensional printing\\nyoung's modulus\\n3d printing process\\nfused deposition modelling\\nfdm\\nvariable stiffness hyperredundant robotic arm\\nshape memory polymer\\nsmp\\nyoung modulus\\nacrylonitrile butadiene styrene\\nabs\\nomnidirectional ball joint design\",\"466\":\"liquids\\nrobots\\nprinters\\nsolid modeling\\nsolids\\ngrippers\\nhydraulic systems\\noptical polymers\\nrobot dynamics\\nprintable hydraulics\\nfunctional robots\\n3d printers\\nphotopolymers\\nnoncuring liquid\\nprefilled fluidic channels\\nlinear bellows actuators\\ngear pumps\\nsoft grippers\\nhexapod robot\\ncommercially-available 3d printer\\nflexible robots\\nadditive manufacturing\\nsoft material robotics\\nprintable robotics\\nhydraulic robots\",\"467\":\"robot sensing systems\\nuncertainty\\nvisualization\\nactuators\\neducation\\ncompensation\\nobject tracking\\npd control\\nrobot vision\\nvariable structure systems\\nhigh-performance robotic contour tracking\\nnonmodel-based dynamic compensation\\nhigh-speed camera\\nhigh-speed compensation actuator\\nprecompensated proportional-derivative sliding mode control\\nprecompensated pd-smc\\nplanar-contour shapes\\nrandom smooth-curvature\",\"468\":\"robot sensing systems\\nforce\\nfeeds\\nneedles\\nforce measurement\\nsensor systems\\ntextile technology\\nvelocity control\\ndifferential feed control\\ncorner matching\\nautomated sewing cell\\nuncertain material characteristics\\nfeed speed\\nsewing force\\nreference points\",\"469\":\"robot sensing systems\\nservice robots\\nrobot kinematics\\nposition measurement\\ntarget tracking\\ncalibration\\naerospace industry\\naircraft manufacture\\ncompliance control\\ndrilling\\nerror compensation\\nindustrial robots\\nposition control\\nhigh accurate robotic drilling\\nexternal sensor\\ncompliance model-based compensation\\nhigh accurate absolute robot positioning\\nrobot model\\ntool calibration\\nproduct uncertainty\\nmodel-based error compensation\\nsensor-based compensation\\nerror source structured analysis\\nrobotic manufacturing\\nerror identification\\nerror source breaking down approach\\nerror reduction\\naircraft manufacturing\\nindustrial application\",\"470\":\"shape\\nmanipulators\\nplanning\\nclamps\\noptimization\\nboundary conditions\\ngravity\\npath planning\\nshape control\\nmanipulation planning\\nmultiple interlinked deformable linear objects\\naerospace assembly\\nautomotive assembly\\ninterlinked dlo\\nclamping points\\nmanipulation actions\\ninterlink constraints\\ndlo shape\\ncable length\\nstiffness\\nshape computations\",\"471\":\"search problems\\nmanipulators\\nmagnetic resonance\\nsorting\\nprobabilistic logic\\ncontext\\ncollision avoidance\\ngraph theory\\nprobability\\nfast extension primitive\\nefficiently solving general rearrangement tasks\\nincremental sampling\\nmanipulating multiple movable obstacles\\nmonotone instances\\nbacktracking search\\nmonotone rearrangement instances\\ngraph dependency\\nminimum constraint removal paths\\nmcr\\ntopological sorting\\nprobabilistic completeness\",\"472\":\"manipulators\\ntrajectory\\nrobot kinematics\\nmathematical model\\ndynamics\\nkinematics\\ndexterous manipulators\\nend effectors\\nmanipulator dynamics\\noptimal control\\nservice robots\\ntrajectory control\\nvelocity control\\ncoordinate-free framework\\nrobotic pizza tossing-catching\\nautonomous pizza tossing-catching\\npizza dough\\ngrasp constraints\\neuler-lagrange dynamic equations of motion\\nrobotic manipulator\\nrigid-body dynamics\\nmathematical models\\nexponentially convergent controller\\noptimal trajectory\\nend-effector\",\"473\":\"planning\\nphysics\\nmanipulators\\naerospace electronics\\ntrajectory\\nclutter\\nmobile robots\\npath planning\\ntrajectory control\\nrearrangement planning\\nobject-centric action spaces\\nrobot-centric action spaces\\nrobot trajectory\\nrearrangement planning problem\\nobject contact\\narm interaction\\nmobile robot\\nhousehold manipulator\",\"474\":\"cost function\\nrobot motion\\ntrajectory\\ncollaboration\\ncollision avoidance\\nplanning\\nhuman-robot interaction\\nmanipulators\\nmotion control\\npath planning\\nmotion planning\\nhuman-robot manipulation\\nshared workspace\\nhuman-robot collaboration\\nmanipulation tasks\\nworkspace avoidance\\nrobot motion consistency\\nhuman-robot workspace sharing study\\nbaseline method\\ntask success rate\\ntask completion time\",\"475\":\"planning\\ntactile sensors\\nforce sensors\\nforce\\nforce measurement\\ncameras\\nmanipulators\\nmotion control\\npath planning\\nsampling methods\\ngroping behavior planning\\ngroping behavior execution\\ncontact sensor based manipulation\\ncontact information\\noccupancy grid map\\ncontact sensor model\\nsampling-based motion planning\",\"476\":\"three-dimensional displays\\nrobot sensing systems\\nuncertainty\\ndatabases\\ntorso\\ngrippers\\noptimisation\\npath planning\\ngrasp selection planning\\npartially occluded objects\\n3d object models\\ndepth sensor\\nadaptive submodular maximization problems\\nnear-optimal sequence\\nrobot poses\\nobservation costs\\ntraverse costs\\npr2\",\"477\":\"robots\\nplanning\\nvocabulary\\nforce\\ntorque\\nmeasurement\\nfriction\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmanipulator dynamics\\ncomplex planning\\ncontinuous space\\nobject-agent-environment framework\\nlearning\\nrobotic task-level planning\\nhigh-level paradigm\\nworkspace affordances\\nmechanical wrenches\\naffordance characterization\\nmanipulator wrench spaces\\naffordance-feasible planning\",\"478\":\"service robots\\nmanipulators\\nbatteries\\ngrasping\\nrobotic assembly\\nkinematics\\nfeedback\\nforce measurement\\nlinearisation techniques\\ntorque measurement\\nfolding assembly\\ndual-arm robotic manipulation\\nhigher level assembly strategy\\nprismatic-revolute joint\\nfeedback linearisation\\nforce torque measurements\",\"479\":\"microscopy\\noptical imaging\\nbiomedical optical imaging\\nrobots\\ncharge carrier processes\\nintegrated optics\\nlinear programming\\nbrownian motion\\nmanipulators\\nmathematical analysis\\nstability\\nstochastic systems\\noptical manipulation\\nmultiple microscopic objects\\nbrownian perturbations\\ntiny objects\\nbrownian effect\\nrobotic control technique\\nautomated manipulation\\nrigorous mathematical formulation\\nstochastic perturbations\",\"480\":\"orthotics\\nactuators\\noptical fiber sensors\\nfingers\\nlight emitting diodes\\nvalves\\nbending\\nbiomechanics\\nbiomedical equipment\\nbiomedical measurement\\nbody sensor networks\\nelastomers\\nfibre optic sensors\\nintelligent sensors\\nmedical control systems\\nmicrocontrollers\\nmotion control\\npneumatic actuators\\nposition control\\npressure control\\npressure measurement\\npressure sensors\\npulse width modulation\\nthree-term control\\ncurvature control\\nsoft orthotics\\nlow-cost solid-state optics\\nembedded optical fiber\\npneumatically powered actuators\\noptical sensors\\northotic actuator-sensor pair\\nhuman finger\\nelastomeric actuator\\nwearer finger\\nsignal intensity measurement\\nlight-emitting diode input traveling\\nled\\nlight intensity\\nstatic states\\ndynamic states\\ngain-scheduled proportional-integral-derivative controller\\npid\\npulse-width-modulation signals\\npwm signals\\ninternal pressure\\nsolenoid valve\",\"481\":\"muscles\\nforce\\nfluids\\nstrain\\nrubber\\nactuators\\nelectron tubes\\nelectroactive polymer actuators\\nfluidic devices\\nmuscle\\npneumatic actuators\\npower-augmentation\\nmoderate-budget robotics classroom\\nwpi\\nmckibben muscle\\nlatex based hydromuscle\\npassive material property\\nbiological muscle\\nsoft fluidic linear actuator\\nefficiency 88 percent\\nefficiency 27 percent\\nartificial muscle\\nsoft actuators\\nhydraulics\\npneumatics\\ncontrol\\nrobots\",\"482\":\"muscles\\nforce\\nactuators\\nstrain\\ncontracts\\nrobots\\nrubber\\ndexterous manipulators\\nelectroactive polymer actuators\\nhysteresis\\nnonlinear control systems\\npneumatic actuators\\nrobust control\\nstrain soft artificial muscle\\ninverse pneumatic artificial muscle\\nipam\\nsoft pneumatic actuators\\nmckibben muscle\\nnonlinear control\\nfriction\\nlinear mapping\\nmckibben muscle boasts\\nspeed tests\\npressure tests\\ndisplacement tests\\nrobotic finger\",\"483\":\"skin\\nhaptic interfaces\\nforce\\ntactile sensors\\nstandards\\nvirtual environments\\nrendering (computer graphics)\\nforce feedback\\nskin deformation feedback\\nmass rendering\\nhuman perception\\nvirtual environment\\nweber fraction\\nvirtual mass\\nkinesthetic force feedback\",\"484\":\"kalman filters\\nforce\\nestimation\\nforce measurement\\nvideos\\nparameter estimation\\nbayes methods\\nbiomedical materials\\ncardiology\\nelastic deformation\\nmedical robotics\\ntissue engineering\\nyoung's modulus\\nbayesian estimation\\nnonrigid mechanical parameters\\ntemporal sequences\\ntissue deformation\\nsurgical planning\\nmechanical-property estimation\\nlandmark-based displacement tracking\\nyoung modulus\\nhuman heart\\nelasticity\",\"485\":\"muscles\\nrobot kinematics\\nradio frequency\\npulleys\\nhip\\nforce\\nmobile robots\\nrobot dynamics\\ntorque control\\nmusculoskeletal quadruped robot\\ntorque-angle relationship control system\\nchangeable mechanical properties\\ndynamic robots\\nchangeable output properties\\ntarcs\\nstatic properties\\njumping ability\\nbiarticular muscle\\nmonoarticular muscle\\njumpable range\\ndirectional property\\nsimulations\",\"486\":\"hysteresis\\nload modeling\\nvibrations\\nloading\\nstrain\\nmathematical model\\ngenetic algorithms\\ncables (mechanical)\\ndamping\\ndynamic response\\nmedical robotics\\nsurgery\\nbouc-wen hysteresis model\\ncable loading\\ncable driven robot\\nparameter identification\\nparameter optimization\\ngenetic algorithm\\nraven ii surgical robotic surgery\\nlinear damping\\nvibration test\\nenergy dissipation\",\"487\":\"manipulators\\nwires\\nshafts\\nelectron tubes\\nlungs\\ninstruments\\ncontrol system synthesis\\ndiseases\\nflexible manipulators\\nlung\\nmedical robotics\\npatient diagnosis\\npatient treatment\\nwire-driven flexible manipulator\\nbronchoscopic interventions\\nlung disease diagnosis\\nlung disease treatment\\nendobronchial devices\\nminiaturized blocks\\nflexible shafts\",\"488\":\"actuators\\nmanipulator dynamics\\nreal-time systems\\nplanning\\nkinematics\\npath planning\\nreal-time path planning algorithm\\nmultisegment continuum manipulator\\ndynamic environments\\nclassic potential field method\\nkinematics model\\nconstant-curvature assumption\\nactuator space\\nmechanical constraints\\ntendon-driven single-segment continuum manipulator prototype\",\"489\":\"muscles\\npneumatic systems\\ntrajectory\\nservice robots\\nrobustness\\nacceleration\\ncontrol system synthesis\\ndexterous manipulators\\nintelligent robots\\npneumatic actuators\\nthree-term control\\ntrajectory control\\nrobot skill learning methods\\nrobot table tennis\\nuncertain task\\njoint space\\ntask space\\npid controller\\ntrajectory tracking control\\npneumatic artificial muscles\\nantagonistic compliant actuation\\nlight-weight robot arm design\\ncomplex system control\\ncontrol algorithms\\nantagonistic muscle-based design\\nenergy release\\nhigh-speed catapult-like motions\\npassive compliance\\noperational lifetimes\\npower-to-weight ratios\\nthrowing motions\\nhitting motions\\nmotor skills\",\"490\":\"manipulators\\nfocusing\\nkinematics\\nshape\\nlayout\\nsoftware\\ncontinuum mechanics\\ninteractive devices\\nmanipulator kinematics\\nredundant manipulators\\nshear modulus\\ntelerobotics\\nteleoperation mappings\\nrigid link robots\\nextensible continuum counterparts\\ncontinuum robots\\nthree-degree-of-freedom joysticks\\nthree-dof joysticks\\nsix-degree-of-freedom rigid-link manipulator\\ninput device\\nrigid-link arm\\ncontinuum robot\\nintuitive operational interface\\ndegree-of-freedom continuum robot\\nplanar tasks\\nspatial tasks\",\"491\":\"heart\\ntracking\\nthree-dimensional displays\\ntrajectory\\nmotion compensation\\nposition measurement\\ncameras\\nmanipulators\\nmedical robotics\\nrobot vision\\nstereo image processing\\nsurgery\\ntelerobotics\\nbimanual teleoperation\\nheart motion compensation system\\nda vinci research kit\\nda vinci surgical system\\nminimally invasive coronary artery bypass surgery\\nbeating heart\\nnovint falcon device\\nheart simulator\\nndi optotrak certus tracker\\nndi measurements\\nda vinci patient side manipulators\\nstereo endoscopic camera\\nvisual stabilization\",\"492\":\"instruments\\nsurgery\\nrobot kinematics\\nestimation\\nendoscopes\\nq measurement\\nmedical robotics\\nrobust trocar detection\\ntrocar localization\\nrobot-assisted endoscopic surgery\\nkinematic constraints\\nendoscopic instruments\\ninstallation procedure\\nregistration procedure\\nfulcrum coordinates\",\"493\":\"instruments\\nthree-dimensional displays\\ncameras\\noptical imaging\\nnoise measurement\\nimage segmentation\\nintegrated optics\\ngradient methods\\nmanipulator kinematics\\nmedical robotics\\nposition control\\nrobot vision\\nrobust control\\nsurgery\\nrobust image-based computation\\nimage-guided manipulation\\n3d position control\\nrcm-constrained instruments\\nmonocular cameras\\ninstrument position\\nsingle 2d image\\ngradient descent algorithm\\nline segment\\nmanipulated tool\\niterative algorithm\\nnoisy image measurements\\nkinematic controller\\nposition estimator\\n3d motion\\nrobotic instrument\\nnumerical simulations\\nremote centre of motion\",\"494\":\"grasping\\nmanipulators\\nmaster-slave\\ngrippers\\nposition control\\nmanipulator kinematics\\nmedical robotics\\nrods (structures)\\nsurgery\\nsurgical robot systems\\nlaparoscopic surgery\\nsurgical manipulation system\\nforceps manipulators\\ngrasping mechanisms\\nrods\\nrobotic forceps holders\\nchopstick\\nfive-dof kinematic model\\nthree-dof translational motion\\nsliding motion\\ngrasping motion\\nmaster-slave system\\ntracking performance\\nblock-transfer task\\nblock-rotation task\\nsurgeon training\",\"495\":\"force\\nestimation\\ninstruments\\ndynamics\\nfriction\\nrobot sensing systems\\ntorque\\nforce control\\nforce sensors\\ngrippers\\nkalman filters\\nmedical robotics\\nnonlinear filters\\nposition control\\nsurgery\\nvelocity control\\nelongated cable-driven elongated surgical instruments\\nhaptic feedback\\nrobotic minimally invasive surgeries\\ndynamic model-based sensorless grip force estimation method\\nhaptic perception problem\\ncable-pulley properties\\ngripper jaw position\\ngripper jaw velocity\\nunscented kalman filter\\nmotor encoder readings\\nmotor output torques\\nbounding filter\\nraven-ii surgical robot\\nground truth data\\nsensorless grasp force estimation\\ndynamic modeling\\nelongated cable driven instrument\\nsurgical robot\\nminimally invasive surgery\",\"496\":\"cameras\\nrobot vision systems\\nkinematics\\nmanipulators\\nstate estimation\\nmathematical model\\ncontrol engineering computing\\nkalman filters\\nmanipulator kinematics\\nmedical image processing\\nmedical robotics\\nnonlinear filters\\nparameter estimation\\nposition control\\nrobot vision\\nstereo image processing\\nsurgery\\nonline state estimation\\noffline ukf parameter estimation\\nraven-ii experimental surgical research platform\\ncable parameter estimation\\ndual ukf\\nlow cost stereo camera\\nmanipulator positioning\\ntransmission kinematics\\nsurgical robots\\ncable driven manipulators\\ncable driven surgical robot joint angle estimation\\n3d vision\\nunscented kalman filter\\ncable driven mechanism\\nflexible manipulators\",\"497\":\"visualization\\nmotion segmentation\\nfeature extraction\\nhidden markov models\\nkinematics\\nmachine learning\\ncontrol engineering computing\\nconvolution\\nimage classification\\nimage segmentation\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nneural nets\\npattern clustering\\nrobot vision\\nsurgery\\ntransforms\\nvideo signal processing\\nunsupervised trajectory segmentation\\nmultimodal surgical demonstrations\\nrobot-assisted minimally invasive surgery\\nfixed-camera video\\nsurgical subtask kinematic recordings\\nlearning from demonstrations\\ntransition state clustering with deep learning\\ntsc-dl unsupervised algorithm\\ntask-level segmentation\\nvisual feature space\\nimage classification deep convolutional neural networks\\ndimensionality reduction\\nvisual encoding\\nscale invariant feature transforms\",\"498\":\"surgery\\nkinematics\\ntraining\\nrobots\\nrobustness\\nhidden markov models\\nmanuals\\nfeature extraction\\nmedical robotics\\npatient care\\nrobot kinematics\\ncanonical activity sequence\\nsurgical training task\\nactivity segment annotation\\ntraining laboratory\\nkernel density estimation\\ndynamic time warping\\nstacked denoising autoencoder\\nnonlinear feature extraction\\nkinematic data annotation\\nunsupervised alignment\\nsurgical recordings\\nactivity annotations\\neducation tool\\nassessment tool\\nsurgical tool motion\\nminimally-invasive surgical techniques\\nrobotic surgery\\nautomatic activity annotation\\nunsupervised surgical data alignment\",\"499\":\"force\\ngeometry\\nrobot sensing systems\\nestimation\\nsurface treatment\\nprobes\\nbiological organs\\nbiological tissues\\nend effectors\\ngaussian processes\\nmedical robotics\\npath planning\\nsurgery\\ntactile sensors\\ntumours\\nconcurrent nonparametric estimation\\ntissue stiffness\\ncontinuous adaptive palpation\\ntumors\\norgan geometry\\nrobotic surgery\\nsurgical guidance\\nforce sensors\\ntactile sensor\\npath optimization technique\\nadaptive high fidelity model reconstruction\",\"500\":\"catheters\\npath planning\\ngeometry\\nthree-dimensional displays\\nrobots\\nheart\\nmuscles\\ncardiology\\ncomputational geometry\\ndiseases\\nmedical robotics\\nsurgery\\ncatheter path planning algorithm\\nrobot-enhanced cardiac radiofrequency catheter ablation\\nrfca\\ncardiac arrhythmia treatment\\nendocardial walls\\nabnormal electrical circuit prevention\\nadult congenital heart disease\\nachd\\nsurgical treatments\\nabnormal cardiac rhythms\\ncardiac ep mapping optimisation\\noptimal mapping position determination\\ncurvature quadric error metric simplification\\ndistance weighted quadric error metric simplification\\nqems\\ncardiac chamber geometry recovery\\nep mapping recovery\\nep data collection\",\"501\":\"needles\\ntrajectory\\nsurgery\\nactuators\\nplanning\\nautomation\\nrobots\\nconvex programming\\nmechanical guides\\nmedical robotics\\npose estimation\\nautomating multithrow multilateral surgical suturing\\nmechanical needle guide\\nsequential convex optimization\\nsuture needle angular positioner\\nsnap\\nerror reduction\\nneedle pose estimation\\nstandard actuator\\ntissue phantoms\\ndvrk\",\"502\":\"cameras\\nvisual servoing\\nwavelet transforms\\nbiomedical optical imaging\\noptical imaging\\noptical interferometry\\nadaptive optics\\nmedical image processing\\nmedical robotics\\nmicrorobots\\noptical tomography\\nrobot vision\\nautomated in-plane oct-probe positioning\\nrepetitive optical biopsies\\nvision-guided control law\\nmicrorobotic-assisted biomedical applications\\noptical coherence tomography\\noct images\\nvision-based controller\\noct b-scan images\",\"503\":\"software\\nhumanoid robots\\nplanning\\ncontext\\nautomata\\nsafety\\nfailure analysis\\nfinite state machines\\nformal specification\\nprogram compilers\\ntemporal logic\\nreactive high-level behavior synthesis\\natlas humanoid robot\\nformal synthesis theory\\nreal-world complex robotic systems\\nautomatic code generation\\nlinear temporal logic\\nltl\\nlow-level failures\\nreactive mission plan synthesis\\nstate machine generation\\nteam vigir software\\nformal synthesis techniques\\nrobot operating system\\nros packages\",\"504\":\"trajectory\\nsafety\\ncollision avoidance\\nreal-time systems\\nhumanoid robots\\ncomputational modeling\\nhuman-robot interaction\\nmobile robots\\ntrajectory control\\nreal-time evasive motion planning\\nreal-time evasive motion execution\\nhumanoid robot\\nsafety area\\nevasion maneuver\\ncenter of mass\\nmotion commands\\nclosed-form expressions\\nnao humanoid\",\"505\":\"convergence\\nplanning\\nheuristic algorithms\\nsearch problems\\nacceleration\\ncost function\\noptimisation\\npath planning\\nrobots\\nsampling methods\\ntrees (mathematics)\\nregionally accelerated batch informed trees\\nrabit\\noptimal path planning\\nsampling-based optimal planner\\noptimization technique\\nchomp\\nnonconvex cost function\\nglobal search\\ndifficult-to-sample homotopy classes\\nalmost-sure asymptotic convergence\",\"506\":\"robot sensing systems\\nprobabilistic logic\\nspace exploration\\nentropy\\nuncertainty\\ncollision avoidance\\nmobile robots\\noptimal control\\npath planning\\nprobability\\nrobotic exploration planner\\nprobabilistic guarantees\\npath planning algorithms\\npath completion\\nmission success\\nintegrated exploration algorithms\\nlocalization\\nautonomous robotic system\\nprobabilistic framework\\nhamiltonian path problem\\nguaranteed probabilistic information explorer\\ng-pie\\nasymptotic optimality\",\"507\":\"robot sensing systems\\ngaussian processes\\nrobot kinematics\\ndata models\\nposition measurement\\ncollision avoidance\\ndecentralised control\\nhelicopters\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\ndecentralized multiagent exploration\\nonline-learning\\ngaussian process\\ndata model\\nspatial correlations\\nexploration algorithm\\ninformation transmission\\nmultiagent coordination\\nlight-weight collision avoidance\\nmagnetic field intensity\\nground-based robot\\nquadcopters\\nultrasound sensor\\nterrain profile\",\"508\":\"visualization\\nfeature extraction\\npredictive models\\nlogic gates\\nsea measurements\\nplanning\\noceans\\nautonomous underwater vehicles\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nmultimodal information-theoretic measures\\nautonomous exploration\\ninformation gathering missions\\nocean environment\\nauv time constraint\\nauv energy constraint\\nmultimodal learning approach\\nin-situ visual observations\\nshipborne bathymetry\\nvisual information gain\",\"509\":\"aerospace electronics\\nplanning\\nuncertainty\\ntrajectory\\nmathematical model\\ncost function\\nbelief maintenance\\nconcave programming\\nconvex programming\\nmarkov processes\\nmotion control\\npath planning\\nfeedback motion planning\\nnon-gaussian uncertainty\\nnon-convex state constraints\\npartially observed markov decision process\\npomdp problem\\nconvex program\\nfeedback policy\\nnon-gaussian belief space\\nnonlinear observation models\\nreceding horizon control strategy\\nparticle filters\\nnongaussian belief representation\",\"510\":\"trajectory\\nquaternions\\ninterpolation\\nrobot sensing systems\\nservice robots\\ndexterous manipulators\\nhuman-robot interaction\\nlow-pass filters\\noptimisation\\npath planning\\nrobot dynamics\\nrobot kinematics\\nvelocity-limited online trajectory generator\\nrobot dynamic\\nrobot kinematic\\nhuman arm motion\\nfrequency visual tracking\\nhuman robot collaboration applications\\nspherical linear interpolation\\nlow-pass filter\\nnonlinear programming methods\\nmotion planning\\nonline motion generation\",\"511\":\"robot sensing systems\\ngames\\nimage edge detection\\nrobot kinematics\\nplanning\\ngraph theory\\nmobile robots\\nmotion control\\npath planning\\nsensor placement\\nvelocity control\\npursuit-evasion problem\\ntwo-dimensional environment\\nfixed beam sensors\\nsensor directions\\nline-of-sight detection\\nfixed direction\\npursuer motion strategy\\nevader detection\\nunbounded speed\\nconvex conservative regions\\npursuit-evasion graph\\npeg\\nmobile robot\\nplanning motions\",\"512\":\"collision avoidance\\nrobot sensing systems\\noptimization\\nmobile communication\\ncomputational geometry\\nconvergence\\nmobile robots\\noptimisation\\nvoronoi-based coverage control\\nheterogeneous disk-shaped robots\\ndistributed mobile sensing applications\\nsensory footprint\\nbody footprint\\ngeneralized voronoi diagrams\\nvector field planner\\ncombined sensory coverage\\ncollision avoidance problem\\nassociated constrained optimization problem\",\"513\":\"three-dimensional displays\\nrobot sensing systems\\noctrees\\npath planning\\nmemory management\\nspace heating\\ncomputer graphics\\nsuper ray based updates\\noccupancy maps\\nmap representations\\npoint clouds\\nrepresentative ray\\nfrustum\\nmap update process\\nmapping lines\\naxis-aligned space subdivisions\\noctomap\\ngrid maps\",\"514\":\"robot sensing systems\\ngeometry\\ntomography\\nplanning\\nmobile robots\\nchemioception\\ngas sensors\\nrobots\\nhuman expert\\ntemplate matching technique\\noptimization algorithm\\nreconstruction errors\\nsensing geometries\\nsmell\\nrobot assisted gas tomography\\nefficient sensor planning strategies\",\"515\":\"robot sensing systems\\ngrasping\\nsensor arrays\\ncentralized control\\nshape\\ndexterous manipulators\\ngrippers\\nmotion control\\nposition control\\nsensors\\nmultifingered hand control\\nrobotic arm control\\nproximity sensor\\nfingertip motion\\ngrasping posture control\",\"516\":\"global positioning system\\ncurrent measurement\\nswitches\\ncalibration\\nnoise measurement\\nrobot sensing systems\\nposition measurement\\naerospace control\\nautonomous aerial vehicles\\nmobile robots\\npose estimation\\nprobability\\nsensor fusion\\ntelerobotics\\nself-calibrating multisensor fusion\\nprobabilistic measurement validation\\nseamless sensor switching\\nuav\\ndata fusion\\nmultiple sensors\\nmobile platform\\nstate estimation\\naccurate calibration\\naccurate initialization\\ndelays\\nagile aerial vehicles\\nmeasurement validation\\nstatistical signal quality analysis\\nextrinsic sensor states\\nultrawideband range measurements\\nuwb range measurements\\nvisual pose estimation\",\"517\":\"rotors\\naerodynamics\\nsolid modeling\\nvehicle dynamics\\nforce\\ndrag\\nautonomous aerial vehicles\\nmatrix algebra\\nmaximum likelihood estimation\\nmobile robots\\nstate estimation\\nmaximum likelihood parameter identification\\nmicro aerial vehicles\\ndynamic model\\nrealistic simulation\\nrecorded flight data\\nmaximum likelihood estimation scheme\\ndominant parameters\\ninertia matrix\\ncenter of gravity\\ncog\\nimu\\nmav model\",\"518\":\"accelerometers\\ncalibration\\ngyroscopes\\nrobot sensing systems\\nacceleration\\ncameras\\nestimation\\nmotion control\\nmotion estimation\\nrobot vision\\nkalibr\\ninertial measurement unit\\nimu calibration toolbox\\nopen-source camera\\nrobotic system\\negomotion estimation\\nrotational motion\\naccelerometer axis\",\"519\":\"conferences\\nautomation\\ncalibration\\nclosed loop systems\\nflanges\\nindustrial robots\\ninstruments\\nleast squares approximations\\nmotion control\\nrobot kinematics\\nlocal calibration\\nclosed-loop calibration\\nindustrial serial robot\\nlow-cost 3d measuring device\\nautomated calibration\\nmitutoyo digital indicators\\nmastering fixture\\nkinematic coupling\\nrobot movements\\nabsolute cartesian coordinates\\nprecision balls\\nrobot joint encoders\\nabb irb 120 robot flange\\nrenishaw telescoping ballbar\\nposition errors minimization\\nleast squares method\",\"520\":\"service robots\\ncalibration\\nrobot kinematics\\nrobot sensing systems\\nkinematics\\nload modeling\\nelastic constants\\nend effectors\\nerror analysis\\nindustrial manipulators\\nnoise measurement\\nobservability\\noptical tracking\\ntorque\\nelasto-geometrical calibration method\\nindustrial robot\\nmultidirectional external loads\\nlaser tracker\\ngeometric parameter errors\\njoint stiffness parameters\\nrobot end-effector\\nrobot configurations\\nrobot position errors\\nexternal forces\\nexternal torques\\n6-dof cable-driven parallel robot\\nabb robot\\ncalibration process\\nunmodeled error measurement\\nobservability analysis\\nrobot elasto-geometrical parameters\\ncalibration efficiency evaluation\\nkinematic calibration\",\"521\":\"robot sensing systems\\ncalibration\\nmobile robots\\nkinematics\\nmobile communication\\ntrajectory\\nwheeled mobile platform unsupervised calibration\\nwheeled mobile robot\\nelementary motion\\nrobot steering\",\"522\":\"calibration\\ncameras\\ntraining\\ntraining data\\nrobot vision systems\\nkernel\\nmathematical model\\nparameter estimation\\nunsupervised learning\\ninstance selection\\ncamera calibration\\ntraining data distribution\\nlarge-scale physical experiments\\nnonuniform training data\\nreprojection error\\nparameter variance\",\"523\":\"cameras\\nthree-dimensional displays\\noptical wavelength conversion\\ncomputational modeling\\nsolid modeling\\nimage segmentation\\nacceleration\\ngeometry\\nimage registration\\ngeometry based exhaustive line correspondence determination\\n3d line segments\\n2d line segments\\ncamera axis direction\\ngeometric constraints\\ncamera position finding\\nsensor information\",\"524\":\"calibration\\nrobot sensing systems\\nlaser radar\\ncameras\\nthree-dimensional displays\\ntransforms\\ngraph theory\\nimage sensors\\nmobile robots\\nnatural scenes\\noptical radar\\nrobot vision\\nlidar-camera system calibration\\nsensor pose-graph\\ndiligent scene selection scheme\",\"525\":\"convolution\\nprobabilistic logic\\ncalibration\\ncovariance matrices\\nrobot sensing systems\\ndexterous manipulators\\nfiltering theory\\nmanipulator kinematics\\nmatrix algebra\\nprobability\\nax=xb hand-eye calibration problem\\nrigid body transformations\\nasynchronous sensors\\nmissing data\\nprobabilistic approach\\ndata streams\\nill-conditioned data pair filtering\\nbatch method\\ncalibration accuracy improvement\",\"526\":\"computational modeling\\ncalibration\\nservice robots\\nmeasurement uncertainty\\nkinematics\\ntraining data\\nend effectors\\nerror compensation\\ngaussian processes\\nindustrial robots\\nregression analysis\\nindustry robots calibration\\nloading effects\\nproduct-of-exponential\\ngaussian process\\ngp\\ngp regression\\ngeometric error compensation\\nnongeometric error compensation\\nrobot manipulator\\nend effector\\nrobot positioning accuracy\\nbase-tool calibration\\npoe calibration\",\"527\":\"needles\\nprobes\\nvisualization\\nrobots\\nthree-dimensional displays\\noptimization\\nkalman filters\\nmedical control systems\\nmedical image processing\\nmedical supplies\\noptimisation\\ntracking\\nultrasonic imaging\\nvisual needle tip tracking method\\nneedle localization algorithm\\nneedle template images\\ngauss-newton optimization\\n2d us images\\nimage based visual tracking techniques\\nneedle insertions\\npercutaneous needle procedures\\nmedical imaging techniques\\n2d ultrasound images\\nbiopsy needles\",\"528\":\"needles\\nmagnetomechanical effects\\nelectrodes\\nsuperconducting magnets\\nmagnetic resonance imaging\\nshafts\\ntrajectory\\nbiomedical electrodes\\nbrain\\nmanipulators\\nmedical robotics\\npermanent magnets\\nsurgery\\ntorque control\\ntrajectory control\\nmagnetic needle guidance\\nneurosurgery\\nmagnetic-tip steerable needle\\ndeep brain stimulation electrode placement\\ntip orientation control\\nneodymium-iron-boron permanent magnet\\nclinically-sized magnetic-manipulation system\\nhuman-in-the-loop control\\ntissue damage\\ndevice retraction\\ndevice redirection\\ninsertion point\\nsize 1.3 mm\\nsize 0.7 mm\",\"529\":\"endoscopes\\nforce\\ntumors\\nelectrostatic discharges\\nrobot sensing systems\\ncontrol engineering computing\\nmedical image processing\\nmedical robotics\\nrobot vision\\nsurgery\\ndisposable actuator\\ndistally-mounted actuator\\nactive endcap\\nendoscopist distal dexterity\\nflexible articulating exoskeleton\\nprinted-circuit mems\\nelectrosurgical tools\\nembedded proprioceptive sensing\\ndistributed led-phototransistor pairs\\nlight intensity modulation\\ndistal degree-of-freedom\\nshape memory alloy technology\\nactuation transmission system\\ndistal articulation\\ncircumferential incisions\\nmodular actuator\\nin vivo dexterity\\ngastrointestinal tract\\nlesions\\nearly-stage gastric neoplasia treatment\\nendoscopic submucosal dissection\\nburgeoning transendoscopic procedures\\nendoscopic surgery\\nsnap-on robotic wrist module\",\"530\":\"conferences\\nautomation\\nflexible manipulators\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmechanical engineering computing\\nmedical robotics\\nregression analysis\\nsupport vector machines\\nsurgery\\nautomate surgical tasks\\nflexible serpentine manipulator\\nlearning actuation space trajectory from demonstration\\nminiaturized flexible tendon-driven serpentine manipulators\\ntsm\\nsurgical robotic systems\\ntorturous human lumen\\noral cavity\\nupper gi tract\\nsufficient nonlinearity\\nmodel uncertainty\\nmodel based control\\nsurgeon fatigue\\ntask efficiency\\ndata-driven approach\\nlfd\\nstatistical machine learning models\\nsmooth motor trajectory generalization\\ndirect actuation space learning\\nmotion segmentation\\nsoft margin support vector machine\\nsoft-svm\\nlocally weighted regression\\nlwr\\nendoscopic submucosal dissection\\nesd\\ncompliant insertion\",\"531\":\"catheters\\ncameras\\nmagnetic resonance imaging\\nmirrors\\ngeometry\\nrobots\\nbiomedical mri\\nmedical image processing\\nshape measurement\\nstereo image processing\\ncatadioptric stereo tracking\\nthree dimensional shape measurement\\nmri guided catheters\\nactuated steerable catheters\\ncatadioptric stereo camera system\",\"532\":\"shape\\ntendons\\nsurgery\\ncavity resonators\\nbones\\nlesions\\nrobots\\ndexterous manipulators\\nmanipulator kinematics\\nmedical robotics\\nbrain\\ndura\\nlower cranial nerves\\nfacial nerve\\njugular bulb\\ncarotid artery\\ninner ear\\nplanar phantom\\ntypical cyst cavity\\ndcm kinematics\\ncystic lesion\\ninstrument lumen\\nflexible ring curette\\nborescope channel\\nrcm\\nremote center-of-motion\\ncysts surgical devices\\ndexterous continuum manipulator\\nrobotic surgery\",\"533\":\"catheters\\nimaging\\nmanipulators\\nshafts\\nrobot sensing systems\\nrobustness\\nbiomedical ultrasonics\\ncardiology\\nclosed loop systems\\nmedical robotics\\nphantoms\\nultrasonic imaging\\nporcine animal model\\nvasculature phantom\\nphysiological disturbances\\nbacklash\\nfriction\\nclosed loop control\\nrobotic catheter steering\\nheart\\nultrasound imaging catheters\\ncardiac catheters\\nunconstrained catheter shaft motion\",\"534\":\"needles\\nultrasonic imaging\\nshape\\nbiological tissues\\nfiber gratings\\nsensors\\nbiomedical equipment\\nbiomedical ultrasonics\\nbragg gratings\\nfibre optic sensors\\nimage reconstruction\\nkalman filters\\nmedical image processing\\nnonlinear filters\\nobject tracking\\nshape recognition\\nstrain measurement\\nbiological tissue\\nfbg-sensor data\\nultrasound images\\nneedle insertion procedures\\nclinical practice\\ndiagnostic purposes\\ntherapeutic purposes\\nneedle steering system\\nactuated-tip flexible needle\\nstrain measurements\\nfiber bragg grating sensors\\nneedle shape online reconstruction\\n3d-space\\nautomated breast volume scanner\\nabvs\\nunscented kalman filter\\nultrasound-based tracking algorithm\\nrobust tracking\\nfbg-sensor measurements\",\"535\":\"needles\\nsprings\\ntrajectory\\nforce\\nthree-dimensional displays\\nsolid modeling\\nshafts\\nbending\\nbiological tissues\\ngelatin\\nmedical robotics\\nphantoms\\nsprings (mechanical)\\nsteering systems\\nsurgery\\nvelocity control\\nneedle steering fusing direct base manipulation\\ntip-based control\\nrobotic needle steering\\nflexible beveled tip needles\\nsymmetric tip needle bending control\\ntip deflection\\nbeveled tip needle\\n3d model\\nneedle-tissue interaction model\\nlocal virtual springs\\nneedle shaft\\nonline estimation\\nvisual feedback\\nlow level controller\\ntask function framework\\nduty cycling method\\nnatural deflection\\ngelatin phantom\\nsubmillimeter accuracy\",\"536\":\"robot sensing systems\\npneumatic systems\\nuncertainty\\nobservability\\nrobot kinematics\\ncontinuum mechanics\\nmagnetic sensors\\nsensor placement\\nstate estimation\\nstatistical analysis\\nconcentric-tube robots\\nmagnetic sensing system\\noptimization problem\\nsensing-system design\\nstatistical state estimation\\ncontinuum robot kinematic equations\\ndifferential representation\\ncontinuum robots\\nsensor selection\",\"537\":\"laser radar\\niterative closest point algorithm\\nmobile robots\\nthree-dimensional displays\\nrobustness\\nconvergence\\ncovariance analysis\\ndistance measurement\\nminimisation\\nmotion estimation\\nmotion measurement\\noptical radar\\noptical scanners\\noptical sensors\\nvelocity measurement\\nrobotic application\\nsingle cpu core\\nwheel odometry\\ncoarse-to-fine scheme\\nminimization problem\\n3d visual odometry\\nsensor velocity\\nflow constraint equation\\nlidar\\nplanar motion estimation\\nrange flow-based approach\\nradial laser scanner\\nplanar odometry\\ntime 0.9 ms\",\"538\":\"three-dimensional displays\\nrobot sensing systems\\niterative closest point algorithm\\nlaser radar\\ncameras\\nimage segmentation\\nestimation\\ncomputer graphics\\ndistance measurement\\nestimation theory\\noptical radar\\nrandom processes\\nsampling methods\\ncollar line segments\\nodometry estimation\\nvelodyne lidar point cloud scans\\ndata point sparsity\\ndata point quantity\\nrandom sampling\\nkitti dataset\\npoint cloud registration\",\"539\":\"dynamics\\ntracking\\nlaser radar\\nbayes methods\\nthree-dimensional displays\\nrobot sensing systems\\nmobile robots\\nnavigation\\noptical radar\\nroad vehicles\\nmotion-based detection\\n3d lidar scans\\nrobots\\nheavy traffic situations\\nsmart navigation\\nautonomous driving\\nbayesian\",\"540\":\"dictionaries\\nsimultaneous localization and mapping\\nhistograms\\ndata models\\nbuildings\\nlaser ranging\\nmobile robots\\nhierarchical spatial model\\n2d range data\\nservice robots\\nmobile robot\\n2d ground-plan-like laser-range-data-based room categorization\\ncompositional hierarchical representation\\nabstraction layer\\nmulticategory set\\naffinity measure\\npart selection\\nlayer construction\",\"541\":\"cameras\\nrendering (computer graphics)\\nrobustness\\ngraphics processing units\\nlighting\\nthree-dimensional displays\\nindoor environments\\nautonomous aerial vehicles\\nimage sensors\\nmesh generation\\nmicrorobots\\nmobile robots\\nobject tracking\\npath planning\\npose estimation\\nrobot vision\\ntrajectory control\\nsimultaneous tracking-and-rendering approach\\nreal-time monocular camera-based localization method\\nmav\\nhigh-speed micro air vehicle control\\nmesh map\\ntrajectory planning\\nstar\\ncamera image tracking\\nvirtual image rendering\\nrobust semidirect image alignment technique\\ngps-denied indoor environments\",\"542\":\"speech\\nnoise measurement\\nmicrophones\\nmobile robots\\nrobustness\\ndelays\\nbroadband communication\\nnoise\\nspeech synthesis\\ntime-of-arrival estimation\\nnoise mask\\ntdoa speech sound source localization\\nnoisy environments\\nbroadband coherent noise sound source\\nsteady-state segments\\ninvalid sound sources localization\\ntime difference of arrival\\ntdoa discrimination\\nspeech source\\nnoise transition\",\"543\":\"pipelines\\nadaptive optics\\nimage edge detection\\nmobile robots\\noptical devices\\ndecoding\\ncontrol engineering computing\\nrobot vision\\nslam (robots)\\nreal-time high-accuracy 2d localization\\nstructured patterns\\ndigital pens\\nprinted patterns\",\"544\":\"robot sensing systems\\nnoise measurement\\nforce\\ncovariance matrices\\ntracking\\nobject tracking\\nprobability\\nrobot vision\\nprobabilistic sensor data processing\\nrobot localization\\nload-sensing floors\\nmotion capture system\",\"545\":\"transducers\\nreceivers\\nacoustics\\nrobots\\nhardware\\nkalman filters\\nunderwater vehicles\\nacoustic transducers\\nautonomous underwater vehicles\\ncontrol engineering computing\\nmulti-robot systems\\nnonlinear filters\\nposition control\\nhyperbolic acoustic one-way localization system\\nunderwater swarm robotics\\nunderwater robot self-localization\\nanchored transducers\\nacoustic signals\\none-way signal transmission\\ntime differences of arrival\\ntdoa\\nextended kalman filter\\nglobal position\\nsignal processing chain\\nhardware design\\nsoftware design\\noff-the-shelf hardware\",\"546\":\"robot sensing systems\\nradiofrequency identification\\nantenna measurements\\nprobabilistic logic\\nradio frequency\\nantennas\\natmospheric measurements\\nfingerprint identification\\nmobile robots\\nparticle filtering (numerical methods)\\nprobability\\nrobot vision\\nrssi\\nslam (robots)\\nvectors\\nrfid\\nlocation fingerprinting\\nfingerprint similarity sensor model\\nprobabilistic similarity measure\\nmobile robot localization\\nreceived signal strength\\nrss\\nvector similarity measure\\nparticle filter\",\"547\":\"cameras\\nvoltage control\\nthree-dimensional displays\\nrobot sensing systems\\nimage sensors\\nbrightness\\nedge detection\\nimage representation\\npose estimation\\nspatiotemporal phenomena\\nevent sensors\\nfast localization-and-tracking algorithm\\nasynchronous events\\nluminance\\ngradient pixels\\nimage edges\\nspatio-temporal binning scheme\\nline detection\\n3d model representation\\nvertical lines\\nsensor pose estimation\\n2d event lines\\n3d world lines\",\"548\":\"actuators\\ndynamic programming\\ntrajectory\\ntorque\\nrobots\\ngears\\ncost function\\ncontrollers\\noptimal control\\ntwo-speed actuators\\nclosed loop control\\ncontinuous input variable\\nmotor torque\\ndiscrete input variable\\nmode selection\\nmultiple gear ratios\\ndiscrete modes\\ncomplexity\\nhigh-level control\\nplanning\\ncontroller\\nlow-dimensional model\\nregression analysis\\nglobal feedback laws\\nnearly-optimal feedback laws\\nquadratic cost function\\ntwo-speed actuator prototype\\nglobal stability\",\"549\":\"wheels\\nvehicles\\nrobots\\ndata models\\npredictive models\\nprediction algorithms\\ntrajectory\\ngeometry\\nmobile robots\\noptimal control\\npath planning\\npredictive control\\nslip-aware model predictive optimal control\\npath following\\nwheeled mobile robots\\nwmr\\nwheel slip\\nreceding horizon model predictive path follower\\nrhmppf\\nvehicle mobility\\ngeometric pure-pursuit path follower\\n6-wheel skid-steered robot\",\"550\":\"vehicles\\npredictive models\\nend effectors\\nmathematical model\\npredictive control\\nkinematics\\nautonomous underwater vehicles\\nautoregressive processes\\nmanipulator kinematics\\nmobile robots\\nmotion control\\nposition control\\nkinematic control\\nautonomous underwater vehicle-manipulator system\\nauvms\\nautoregressive vehicle motion prediction\\nmodel predictive control\\nwave disturbances\\nundesired end effector motion\\nmanipulator joint maneuvering\\nsteady end-effector position\\nar model\\nvehicle motion prediction\\nmpc\\njoint motion optimization\",\"551\":\"stochastic processes\\ncollision avoidance\\nrobots\\ntrajectory\\nuncertainty\\noptimization\\nprobabilistic logic\\nbayes methods\\nlearning (artificial intelligence)\\noptimisation\\npredictive control\\ntrajectory control\\nstochastic collision avoidance\\nbayesian policy optimization\\ncluttered environment\\nstochastic trajectory optimization\\ndynamic obstacles\\nmachine learning\\npolicy search\\nconstrained model-predictive control solver\",\"552\":\"optimal control\\ntrajectory\\nestimation\\naerospace electronics\\nheuristic algorithms\\nmonte carlo methods\\nprocess control\\ncollision avoidance\\ncontinuous time systems\\nimportance sampling\\nintegral equations\\nmotion control\\nrobots\\nstate-space methods\\nstochastic systems\\ntopology\\ntrajectory control\\ntopology-guided path integral approach\\ncontinuous-time continuous-space stochastic optimal control problem\\nrobot motion\\ncluttered environment\\npath integral representation\\nsampling process\\nestimation process\\nhighly nonconvex state space\\nobstacle field\\ntopological motion planning algorithm\\nreceding-horizon scheme\\ndynamically feasible collision-free trajectory\\nfeyman-kac path integral formula\",\"553\":\"hip\\ntorque\\nexoskeletons\\nlegged locomotion\\nthigh\\noscillators\\ntrajectory\\ngait analysis\\ngeriatrics\\nrobots\\nautonomous hip exoskeleton\\nmetabolic cost\\nmuscle weakness\\ncontroller\\nuser gait phase\\nwalking speed\\nground inclinations\\nassistance torque\\ngait assistance\\nmetabolic energy consumption\\ntreadmill\\nankle exoskeletons\\nmuscle-tendon unit\\nhip joint\\nankle joint\",\"554\":\"dynamics\\nrobots\\ntrajectory optimization\\nplanning\\nmathematical model\\nlegged locomotion\\nmathematical analysis\\nmotion control\\noptimisation\\npath planning\\nrobot dynamics\\nscheduling\\ntrajectory control\\nhierarchical planning\\ndynamic movements\\nscheduled contact sequences\\nhuman locomotion behaviors\\nanimal locomotion behaviors\\ndynamic motions\\nhierarchical trajectory optimization approach\\nplanning dynamic movements\\nunscheduled contact sequences\\nkinematic fashion\\nnonsliding active contacts\\nenvironment geometry\\nmathematical program with complementarity constraints\\noptimization phases\\nmpcc\\nlegged robots\",\"555\":\"topology\\nrobot kinematics\\nsearch problems\\ntuning\\noptimization\\nsymmetric matrices\\nmatrix algebra\\noptimisation\\nrobots\\noptimal isomorphic goal adjacency\\noptimal isomorphic adjacency matrix\\noptimal orthogonal matrix\\noptimal permutation matrix\\nrobot location\",\"556\":\"magnetic resonance\\nmagnetic moments\\nactuators\\nsprings\\ntorque\\nrotors\\nrobots\\nautonomous aerial vehicles\\nbifurcation\\nbiomimetics\\nelectromagnetic actuators\\nmobile robots\\nrobot kinematics\\nsynchronisation\\nscalable high frequency flapping wing robotic platform mechanics\\nlift-off\\nlift-to-weight ratio\\nelectromagnetic actuator design\\ncoil cross-section profile\\nvirtual spring magnets\\nmagnetic interaction torque\\nwing-to-wing synchronization effect\\nsystem equilibria bifurcation\\nseparation distance\\nbiomechanical linkage system\\ndipteran flies\\nasymmetric wing synchronization\",\"557\":\"grippers\\nlegged locomotion\\nlaminates\\nfoot\\ncrawlers\\nkinematics\\naircraft control\\nhelicopters\\naerial robot\\nlegged robot\\nsma-actuated gripper\\nsingle-degree-of-freedom walking mechanism\\nrobotic platform\\nflight capability\\ngrasping capability\\nwalking capability\\nsustained flight\\nswarm control\\nswarm modeling\\nswarm behavior\\nquadrotor aircraft\\nmesoscale robot\\nflying monkey\",\"558\":\"robots\\ndc motors\\nforce\\ngears\\nload modeling\\nenergy storage\\nsprings\\ncollision avoidance\\nmobile robots\\nrubber\\ntrajectory control\\ntrajectory-adjustable integrated milliscale jumping-crawling robot\\nheight-adjustable jumping module\\nobstacle avoidance\\nenergy-storing capacity\\nlatex rubber\\nknee-like joint\\nelastic material displacement\\ndash crawler\",\"559\":\"force\\nlegged locomotion\\nsurface tension\\nbuoyancy\\nhydrodynamics\\nsprings\\nelastic constants\\nimpact (mechanical)\\nmobile robots\\nmotion control\\nrobot dynamics\\ncontinuous jumping robot\\njumping locomotion\\nwater striders\\nhorizontal rowing motion\\nimpact force\\njumping mechanism\\nsupporting legs\\nrobot center-of-gravity\\nspring stiffness\\njumping angle\",\"560\":\"robot sensing systems\\nshape\\nelectrodes\\ncurrent measurement\\nanalytical models\\nellipsoids\\nmobile robots\\nobject recognition\\ntensors\\nobject shape recognition\\nelectric sense\\nellipsoid polarization tensor\\nellipsoidal object\\nunderwater sensor\\nelectric fish\\nmusic algorithm\\nmultiple signal classification algorithm\\nobject electrical response model\",\"561\":\"kinematics\\nrobot sensing systems\\nmobile robots\\nmotion segmentation\\nchains\\npath planning\\nsensors\\nthree-dimensional navigation\\nintegrated sensors\\nguiding mechanisms\\nactuated chain segments\\ncontinuous tracks\\nrugged terrain\\nself-propelled continuous-track-robot\\nourobot\",\"562\":\"neck\\nrobots\\ntendons\\nrobustness\\nprototypes\\nmuscles\\nyoung's modulus\\ncontinuum mechanics\\nelasticity\\nhumanoid robots\\nstructurally flexible humanoid spine\\ntendon-driven elastic continuum\\nrobustness property\\nelastic backbone approach\\ncontinuum mechanism\\nplanar setup\",\"563\":\"soil\\ngravity\\nrobot kinematics\\nactuators\\nsprings\\ndna\\nmobile robots\\nsprings (mechanical)\\ncircumnutations\\npenetration strategy\\nplant-root-inspired robot\\npeculiar plant root movements\\nrobotic roots\\nexploration tasks\\nsoft actuation mechanism\\nhelical compression springs\\nstimulus-oriented behavior\\nartificial soil\\nsawdust\",\"564\":\"robot sensing systems\\nforce\\npredictive models\\nforce measurement\\nclosed loop systems\\ndistributed sensors\\nforce control\\nmobile robots\\nnonlinear systems\\nopen loop systems\\nrobot dynamics\\nrobot kinematics\\nvolterra series\\ndistributed sensor\\nnonlinear miso volterra series model\\nnonlinear multiple-input-single-output volterra series model\\n3d propulsive force prediction\\nflexible multiple degree of freedom robotic fin\\nopen-loop control\\nclosed-loop control\\nnonlinear system identification\",\"565\":\"robot sensing systems\\noscillators\\npressure sensors\\nangular velocity\\nmonitoring\\npressure measurement\\nrobot kinematics\\nunderwater vehicles\\nspeed evaluation\\nfreely swimming robotic fish\\nartificial lateral line\\nbio inspired robot\\nsensing platform\\nlinear motion\\nlinear velocity evaluation\\npressure profile measurement\\nonboard imu\\ninertial measurement unit\\nrobot motion kinematics\\nswimming speed\\nrobot angular velocity oscillation amplitude\\nnonlinear prediction model\\ndistributed pressure\",\"566\":\"legged locomotion\\nvibrations\\nactuators\\nresonant frequency\\naluminum\\nsolids\\nbeams (structures)\\nbiomimetics\\nmotion control\\npiezoelectric devices\\npiezoelectric materials\\ntether-less legged piezoelectric miniature robot\\nbounding gait locomotion\\nbidirectional motion\\nlpmr\\nstanding wave vibrations\\npiezoelectric patch\\nmetal beam\\ncontact joints\\nvibration bending modes\\nunderactuated system\\nbackward motion\\nforward motion\\nembedded mass\\nblocking force\\napplied voltage\",\"567\":\"vehicles\\nvehicle dynamics\\ndynamics\\nrobot sensing systems\\npropulsion\\nunderwater vehicles\\nautonomous underwater vehicles\\nbiomimetics\\nfeedback\\nnonlinear control systems\\nrobust control\\nthree-term control\\nparameter uncertainties\\nexternal disturbances\\nopen water environment\\nnonlinear rise feedback controller\\npid controller\\nbioinspired u-cat underwater auv depth control\\nvehicle nonlinear behaviour\\nunderwater vehicle control\\nreal operating conditions\\nbiomimetic u-cat turtle-like auv\",\"568\":\"cameras\\nsensors\\noptimization\\ncovariance matrices\\nnoise measurement\\nrobustness\\nimage colour analysis\\nimage matching\\nimage sensors\\nleast mean squares methods\\npose estimation\\nabsolute pose estimation\\nrgb-d frames\\noutlier contaminated matching point sets\\nrgb-d sensors\\nsurface normals\\nsparse matching sets\\nransac algorithm\\ndirect least-square approach\",\"569\":\"uncertainty\\nbayes methods\\nneural networks\\ncameras\\nsimultaneous localization and mapping\\ncomputational modeling\\nmeasurement uncertainty\\ngraph theory\\nlearning (artificial intelligence)\\nneural nets\\noptimisation\\nrobot vision\\nslam (robots)\\nmetric relocalization error\\ngraph optimisation\\nsingle rgb image\\n6-dof camera pose\\nbayesian convolutional neural network\\nrobust monocular six degree of freedom visual relocalization system\\nreal-time monocular six degree-of-freedom visual relocalization system\\ncamera relocalization\\ndeep learning\\nmodelling uncertainty\",\"570\":\"cameras\\nthree-dimensional displays\\nsolid modeling\\nsimultaneous localization and mapping\\nobject detection\\nimage resolution\\nestimation\\nrobot vision\\nslam (robots)\\nobject-aware bundle adjustment\\nmonocular scale drift correction\\nsingle-camera simultaneous localization and mapping system\\ncalamitous drift\\nobject detections\\nbundle adjustment\\nvisual odometry methods\\nkitti dataset\\nlong-range outdoor sequences\\nhand-held camera\",\"571\":\"stochastic processes\\nuncertainty\\nkalman filters\\nrobot sensing systems\\nfeature extraction\\nmobile robots\\nmonte carlo methods\\npath planning\\npose estimation\\nrobot vision\\nmonte carlo simulation\\nkalman filter equation\\npose tracking\\nlocalization uncertainty\\nstochastic geometry\\nglobal localization\\nminimum detection rate\\ndetectable landmark type\\nmodel-based derivation\\nbehavior planning\\npose estimation uncertainty\\nmobile robot\\ndynamic environment\\nlandmark-based localization\\nperception design\\nlocalization accuracy estimation\",\"572\":\"global positioning system\\nsimultaneous localization and mapping\\nreal-time systems\\ncameras\\nreceivers\\nautonomous aerial vehicles\\nmobile robots\\npose estimation\\nonline pose estimation\\nmapping\\nunknown environments\\nautonomous unmanned aerial vehicles\\nsimultaneous map estimation\\nlight-weight uav\\nreal-time pose estimation\\nrtk-gps\\nslam solution\\nimage data\\nomnidirectional multi-fisheye-camera system\\nslam procedure\\nspatial resection\\nbundle adjustment\\nraw gps observations\\nimu data\\nreliable pose estimation\\ngeoreferenced terrestrial laser scanner\",\"573\":\"trajectory\\nestimation\\nsimultaneous localization and mapping\\ncurrent measurement\\nimage representation\\nimage resolution\\nsampling methods\\nslam (robots)\\ntrajectory control\\nnonuniform sampling strategies\\ncontinuous correction based trajectory estimation\\nsliding window estimation\\nonline simultaneous localization and mapping\\nsliding window size\\ncomputational cost\\ncost reduction\\ntrajectory representation\\nnonuniform temporal resolution\\ncontinuous-time representations\\nknots location\\nlaser-odometry slam problem\",\"574\":\"atmospheric measurements\\nparticle measurements\\ncurrent measurement\\nmobile robots\\nlength measurement\\ntrajectory\\ncollision avoidance\\nestimation theory\\nrobot vision\\nslam (robots)\\ntrajectory control\\nnonvisual localization\\nnonvisual mapping\\ntethered mobile robot\\nelectromechanical tether\\nrugged terrain\\nmaneuverability\\ncluttered environment\\ntether entanglement\\ntether measurement\\nwheel odometry\\nvehicle trajectory estimation\\ntether-to-obstacle contact point\\nfastslam\\ntether length\\nbearing measurement\\ntethered robotic explorer\",\"575\":\"semantics\\nsatellites\\nlaser radar\\ncameras\\nglobal positioning system\\nland vehicles\\nrobot sensing systems\\nimage matching\\nmobile robots\\nnavigation\\nremotely operated vehicles\\nrobot vision\\nvision-based robot localization\\nremote locations\\ngps-denied unmanned ground vehicle localization\\nugv localization\\nground image matching\\nremote areas\\nugv sensor horizon\\nsemantic information\\nsemantics-based matching\\nparticle filter framework\\nsatellite maps\",\"576\":\"cameras\\nheating\\nservice robots\\nlighting\\nrobot sensing systems\\nimage matching\\nimage resolution\\nimage sensors\\nimage sequences\\ninterpolation\\npath planning\\nrobot vision\\n2d visual place recognition\\ndomestic service robot\\nlawn mowing robot\\nvacuum cleaning robot\\nconsumer robot\\nrange-based sensor\\nvisual sensor\\nuser-placed beacon\\nactive range\\nvisual sensing solution\\nvision-based solution\\n2d localization\\nday-time maps\\nlow resolution image matching\\ncontrast-normalized image matching\\nimage sequence-based matching\\nplace match interpolation\\nodometry noise level\\nlow light camera technology\\nrobust place recognition\",\"577\":\"three-dimensional displays\\nvisualization\\ndatabases\\npipelines\\nsimultaneous localization and mapping\\nhistograms\\nfeature extraction\\nimage matching\\nobject recognition\\npath planning\\nrobot vision\\nslam (robots)\\nvisual databases\\nvisual place recognition\\npublicly available datasets\\nsparse visual descriptors\\nviewpoint changes\\nseasonal changes\\n3d-features\\nslam\\nsparse triangulated landmarks\\nstructural descriptor\\nsparse-visual feature maps\\n3d-scene information\\n3d-geometry information\\ninformation retrieval\\nvision-based approach\\nvisual information\\nrobot navigation\\npositional drift\\nsparse visual information\\npoint cloud descriptors\",\"578\":\"feature extraction\\nurban areas\\ndatabases\\ncameras\\nrobot sensing systems\\ngeography\\nprobability\\ntext detection\\ngeo-referenced texts\\npublic maps\\ntextual features\\nrgb-camera\\noff-the-shelf text extraction methods\\nprobabilistic localization approach\\ngps-based localization\",\"579\":\"grippers\\ngrasping\\nplanning\\nthree-dimensional displays\\npredictive models\\nrobot sensing systems\\ncollision avoidance\\nlearning (artificial intelligence)\\nmanipulators\\nregression analysis\\npush-and-pull manipulation\\nobject grasping\\ngripper-arm collision avoidance\\nobject reconfiguration\\npredefined object-directed tool actions\\nmultimodal regressor\\npr2 robot\",\"580\":\"exoskeletons\\ntrajectory\\nrobot sensing systems\\nforce\\nrobot kinematics\\ngrasping\\nactuators\\ndexterous manipulators\\nforce feedback\\nhumanoid robots\\nsensors\\ntelerobotics\\nsynergy-based interface\\nmaster-slave system\\nsynergy-based bilateral tele-manipulation strategy\\nremotely controlled pisa\\/iit softhand\\n3-finger hand exoskeleton\\nmaster device\\nsensory system\\nposition encoder\\ncurrent sensor\\nteleoperation strategies\\ncartesian-based hand synergies\\nprojection tool\\nfingertip cartesian space\\ngrasp principal components\\nunconstrained operator hand motion\\nsh motor position reference extraction\\ninteraction force estimation\\nrobotic hand\\n1-dimensional force\\n9d fingertip cartesian space\\ninverse projection\\nfinger-individualized forces\\nsynergy based weighted representation\\noperator fingertips\\nforce feedback hand exoskeleton\\nenvironment impedance\\nball squeezing experiment\\nmanipulation interface\\ncoman humanoid robot\\nvision-based tracking system\\noperator wrist trajectory monitoring\\nbody-machine bilateral interface\",\"581\":\"visualization\\nrobot sensing systems\\nnoise reduction\\nencoding\\nplanning\\nfuses\\nbelief networks\\nlearning (artificial intelligence)\\nrobots\\nsignal denoising\\nsoftware agents\\nauto-encoders\\ncontinuous space\\nda\\nmultimodal data\\ndata discretization\\nbayesian networks\\nonline learning scenarios\\nsimulated robotic experiments\",\"582\":\"grasping\\ndatabases\\nphase change materials\\nend effectors\\nrehabilitation robotics\\ngrippers\\nmanipulators\\nflexible semiautonomous grasping\\nassistive robotic manipulator\\nfive-finger robotic hand\\nrobotic arm\\nrotational behavior\\nsemiautonomy scheme\\ngrasping process\\nvelocity commands\\nvirtual fixtures\\npsycho-physical user study\\ngrasp objects\\nsimulation environment\\nspacemouse interface\\nreal robotic system\\nbio-signals\\npure teleoperation modes\\nsemiautonomous mode\",\"583\":\"grippers\\nfriction\\nforce\\ntorque\\nrobots\\nacceleration\\ndynamics\\nbars\\ndexterous manipulators\\nmotion control\\npower control\\nswing-up regrasping algorithm\\nenergy control based algorithm\\nrobotic arm\\narm gripper\\npinching point function\\ndissipative frictional torques\\nthree degrees of freedom manipulator\\nbar regrasping\",\"584\":\"haptic interfaces\\nrobot sensing systems\\nskin\\nvisualization\\ngrippers\\ndexterous manipulators\\nforce feedback\\ntactile sensors\\nupper-limb amputees\\nvisual attention\\npressure feedback\\nvibrotactile feedback\\nthree-finger robotic gripper\\nur5 universal robot\\ngrasp control\\nslippage detection\\nroutine object-manipulation tasks\\nvisual feedback\\ncontact detection tasks\\nrobotic arm control\\nhaptic feedback\",\"585\":\"grippers\\nshape\\ngrasping\\ngears\\nservice robots\\ndexterous manipulators\\nfood processing industry\\nindustrial manipulators\\nfood handling gripper\\nappetizing presentation\\nindustrial robot\\nfood production\\nlunch box setout\\nmultifingered gripper\\nsliding push part\",\"586\":\"grippers\\ngrasping\\nspace vehicles\\nspinning\\nangular velocity\\nwrist\\nforce\\naerospace robotics\\npath planning\\nfree-flyer acquisition\\nspinning objects\\ngecko-inspired adhesives\\nspacecraft docking\\ntumbling objects acquisition\\nmicrogravity\\nautonomous object manipulation\\nplanar free-floating platforms\\nincoming linear velocity\\nincoming angular velocity\\ngripper designs\\ngrasping strategies\\nmotion planning\",\"587\":\"visualization\\nrobot sensing systems\\nmechanical factors\\noptical imaging\\ndexterous manipulators\\nimage filtering\\nimage matching\\nimage sequences\\npose estimation\\nrobot vision\\nvideo signal processing\\ninteractive computational imaging\\ndeformable object analysis\\nvisual object analysis\\nobject mechanical properties\\nperiodic stimulus\\nmatched video filtering\\nmatched video analysis\\nstiff objects\\nfragile objects\\nlow-texture objects\\nmechanical behaviours\\nlinear filter\\nnoise reduction\\ncontrast enhancement\\nmotion amplification\\noptical flow\\nfiltered video\\nillumination variation\\npose variation\\nfluid level estimation\",\"588\":\"acceleration\\nrobustness\\nrobots\\ngrasping\\ncouplings\\nforce\\nenergy conservation\\ncompensation\\ndexterous manipulators\\nforce control\\ninteraction force decomposition\\ncompensating force maximization\\nphysical work constraints\\nrobustness-reflective force\\naccelerating force\\nrobotic grasping\\nphysically-motivated bounding constraint\\nenergy conservation law\\ninteraction force analysis\\nfinger grasp\\ninteraction points\",\"589\":\"robots\\npredictive models\\nforce\\nactuators\\nrobustness\\nfriction\\nstability criteria\\ncompliance control\\npath planning\\nstability\\nunderactuated compliant hands\\ncompliant gripper grasping irregular objects\\nmechanism design\\ngrasp planning\\nboundary layer expanded mesh technique\\nblem technique\\nfidelity criteria\\ncontact force variation\\ncontact position variation\\ncontact normal variation\",\"590\":\"actuators\\nrobots\\nfabrication\\nazimuth\\ndamping\\nbuildings\\ncomplexity theory\\nbuildings (structures)\\nphotovoltaic power systems\\npower system control\\nsolar cells\\nsolar power\\nsoro-track\\ntwo-axis soft robotic platform\\nsolar tracking\\nbuilding-integrated photovoltaic applications\\ntwo-axis soft robotic actuator\\nsra\\nmorphological complexity\\npower-to-weight ratio\\nenvironmental conditions\\ndynamic building facades\",\"591\":\"auxetic materials\\nmetamaterials\\nrobots\\nactuators\\nforce\\nstrain\\nbellows\\nelasticity\\nrobot dynamics\\nsoft robot design\\nauxetic metamaterial\\nsoft materials\\nmechanical metamaterials\\nintrinsic synchronization\\n3d-printed metamaterials\\nsynchronization mechanism\\nnormal elastic properties\\nauxetic properties\",\"592\":\"manipulators\\nkinematics\\ngeometry\\nactuators\\ncompounds\\nanalytical models\\ndeformation\\nmanipulator kinematics\\npneumatic actuators\\ncompound continuum manipulators\\nsoft continuum manipulators\\nhybrid structures\\ntuneable structures\\ninverse kinematics model\\nvirtual work principle\\ngeometry deformation approach\\nstiff-flop arm\\npneumatically actuated continuum manipulator\\nmanipulator static behaviour\\nbody force\\nplanar movement\\nbraid-surface relative slip constraint\\ncross section deformation\\nregional tunable stiffness structure\",\"593\":\"robot sensing systems\\nactuators\\nmagnetic flux\\nmagnetic separation\\nmagnetic circuits\\nbending\\nclosed loop systems\\ndesign engineering\\niterative methods\\nmagnetic sensors\\nmechanoception\\npneumatic actuators\\nvariable structure systems\\ncomposite soft bending actuation module\\nsoft robotics\\nbiological creatures\\npneumatically-actuated soft bending actuation module\\nintegrated curvature sensing\\nresistive flex sensor\\nmagnetic curvature sensor\\nexternal motion capture system\\niterative sliding mode controller\\nstep curvature reference\\nmodule controllability\\novershoot phenomenon\\nactive feedback control\\npositioning errors\\nproprioception\\nclosed-loop control\",\"594\":\"shape\\nactuators\\ndistortion\\ntransforms\\nrobots\\nthree-dimensional displays\\nforce\\nrobot programming\\nlightweight shape-shifting system\\nthree-dimensional morphing pixel\\nmorphing 3d map\\nmultifunctional self-folding origami design\\nmodified kresling pattern\\nfolding motions\\nunfolding motions\\nlow-profile torsional sma wire actuator\\nmodular approaches\",\"595\":\"strain\\nskin\\nresistance\\nrobot sensing systems\\npolymers\\nfabrics\\ncalibration\\nbiomedical measurement\\ncomposite materials\\ngait analysis\\nmulti-wall carbon nanotubes\\nnanofabrication\\npatient monitoring\\nsensors\\nprintable skin adhesive stretch sensor\\nmultiaxis human joint angle measurement\\nbiomedical engineering application\\ngesture recognition\\nmotion monitoring\\nsilicone rubber\\nmultiwall carbon nanotube composite\\nstrain sensor\\ncalibration method\",\"596\":\"actuators\\nmanipulators\\nkinematics\\nfabrication\\ntrajectory\\npneumatic systems\\nend effectors\\niterative methods\\nmanipulator kinematics\\nmotion control\\npneumatic actuators\\nvariable structure systems\\nend-effector geometric jacobian controller\\nik controller\\ndirect inverse kinematic controller\\ngeometric jacobian-enhanced iterative sliding mode controller\\nrpam\\nreverse pneumatic artificial muscles\\nuniversal joint module\\nrobotic manipulators\\nsoft-actuated modular manipulator\",\"597\":\"actuators\\nforce\\ngrasping\\nmicrochannels\\nstrain\\nmechanical sensors\\ndexterous manipulators\\nfeedforward\\nforce control\\npneumatic control equipment\\nposition control\\nsensors\\nthree-term control\\nsoft pneumatic actuator finger improvement\\nspa\\nsoft egain sensors integration\\nposition control integration\\nforce control integration\\nrigid fingernails integration\\nrobotic hands\\nobject grasp\\nobject manipulation\\nfeedforward models\\npid controller\",\"598\":\"detectors\\nadaptation models\\ntraining\\ntraining data\\nrobots\\nobject detection\\nproposals\\nimage colour analysis\\nimage fusion\\nneural nets\\ncross-modal adaptation\\nrgb-d detection\\nconvolutional neural network\\ncnn based object detectors\\nrgb images\\nlabeled depth images\\nrgb object detector\\nmid-level fusion\",\"599\":\"iterative closest point algorithm\\nuncertainty\\ndata models\\nmeasurement\\nthree-dimensional displays\\nrobustness\\nrobots\\ngaussian processes\\nimage registration\\niterative methods\\nlearning (artificial intelligence)\\nobject detection\\nlearning anisotropic icp\\nla-icp\\nrobust registration\\nonline learning approach\\n3d object registration\\niterative closest point method\\ngeneralized distance function\\nanisotropic gaussian\\ncovariance parameter\\nlikelihood function\\nrgb-d object datasets\\nconvergence\\npose accuracy\",\"600\":\"three-dimensional displays\\nface\\nimage segmentation\\nclustering algorithms\\nsensors\\nrobots\\ntopology\\nsampling methods\\n3d point cloud segmentation\\ntopological persistence\\n3d point cloud data\\npersistent homology theory\\nzeroth homology group\\nsampling conditions\\nfixed distance metric\",\"601\":\"three-dimensional displays\\nsearch problems\\nsolid modeling\\nobject recognition\\ncomputational modeling\\nrobot sensing systems\\ncombinatorial mathematics\\nrobot vision\\nperception via search\\nmultiobject recognition and localization\\nflexible automated manufacturing\\npersonal assistance\\ndiscriminative methods\\nfeature descriptors\\nobserved sensor data\\noptimization\\ncombinatorial search problem\\nmonotone scene generation tree\\ncombinatorial search tractable\\nrobotic domains\",\"602\":\"semantics\\nthree-dimensional displays\\nrobustness\\nobject recognition\\nlighting\\nimage segmentation\\nimage registration\\npose estimation\\nrandom processes\\nhierarchical semantic parsing\\nobject pose estimation\\ndensely cluttered scene\\n3d object recognition\\ntextureless surface\\nobject instance segmentation\\nhierarchical semantic segmentation algorithm\\nransac-based registration method\\n6-dof object\\ngeneralized pooling scheme\\nconvolutional architecture\\nrgb-d dataset\",\"603\":\"sensors\\ncalibration\\ncameras\\nthree-dimensional displays\\ndistortion\\nimage color analysis\\nimage reconstruction\\nimage motion analysis\\nimage sensors\\nrobot vision\\nstructure based autocalibration\\nrgb-d sensors\\ncomputer vision\\nimage measurements\\nartificial landmarks\\ncaptured datasets\\ncalibration patterns\\nsparse environment model\\nslam\\nstructure from motion\\nintrinsic calibration\",\"604\":\"motion segmentation\\ntracking\\nshape\\nimage reconstruction\\nkinematics\\nimage segmentation\\nestimation\\nestimation theory\\npose estimation\\nrobot vision\\nshape recognition\\nvisual perception\\nunknown articulated objects\\npose tracking\\nshape reconstruction\\nkinematic structure\\nrobot perception\",\"605\":\"cameras\\ntrajectory\\ndistortion\\nrobot vision systems\\nimage edge detection\\nthree-dimensional displays\\ncomputer vision\\nimage colour analysis\\nimage restoration\\nmotion estimation\\npose estimation\\nmotion blur removal\\ndepth camera\\nstructured light range sensors\\nslrs\\nmicrosoft kinect\\nelectronic rolling shutters\\ners\\nrolling shutter distortion\\ncamera motion estimation\\nrgbd camera\\ndepth image\\ncolor image\\nmathematical model\\ndistortion rectification\\nscene flow\\ncamera pose estimation\",\"606\":\"yield estimation\\nglobal positioning system\\nmonitoring\\ncameras\\ntraining\\nsimultaneous localization and mapping\\nagriculture\\nautonomous aerial vehicles\\ncollision avoidance\\ncontrol engineering computing\\nhelicopters\\noperating systems (computers)\\nrobot vision\\nvideo surveillance\\nplantation monitoring\\nautonomous quadcopter\\nprecision agriculture\\nsupervised learning approach\\ninterrow plantation path\\nnavigation framework\\ncollision-free gps\\nros\\nrobot operating system\\ntrajectory planning\\ncontrol module\\nconvex programming techniques\\nminimum time trajectory\\npomegranate dataset\\nplantation surveillance video\",\"607\":\"agriculture\\nhyperspectral imaging\\ntraining data\\nrobots\\ntraining\\ncameras\\ncrops\\nimage classification\\nimage colour analysis\\nlearning (artificial intelligence)\\nobject detection\\nself-supervised weed detection\\nvegetable crops\\nground based hyperspectral imaging\\nweed infestation treatment\\nweed infestation eradication\\nhigh spatial resolution hyperspectral imaging data\\nground based platform\\nspectral vegetation signature\\nweed species\\nper-pixel classification\\nstatic training data\\ntemporal variability\\nphysiological change\\nenvironmental change\\nself-supervised training method\\nseeding pattern\\nvegetable field\\nrgb imaging\\nupdate weed appearance model\\nhyperspectral crop-weed discrimination\\nautonomous mobile ground vehicle\\ncorn crop rows\",\"608\":\"three-dimensional displays\\nimage reconstruction\\nvegetation\\nsensors\\ndistortion\\nskeleton\\nimage sensors\\nagriculture\\ncrops\\nimage filtering\\nrobot vision\\nsolid modelling\\ndormant apple tree modeling\\nsingle depth image\\nrobotic pruning application\\ndormant pruning\\nlabor-intensive operations\\nspecialty crop production\\nbranch removal\\nautomatic pruning\\npruning rules\\nbranch pruning point estimation\\n3d reconstruction scheme\\ndiameter-error estimation\\ndepth image filtering\\nsemicircle-based modeling\\nindoor environment\\noutdoor environment\\nbranch diameter estimation\\n3d model of apple trees\\ndepth sensing\\nsemicircle estimation\",\"609\":\"robot sensing systems\\ncameras\\nsupport vector machines\\ndiseases\\ngreen products\\nlight emitting diodes\\nstandards\\nautonomous aerial vehicles\\nbotany\\nimage sensors\\nlearning (artificial intelligence)\\nobject detection\\nplant diseases\\nremote sensing\\nautonomous phytopathology\\ncitrus greening disease detection\\nclose-range remote sensing\\nuav\\nunmanned aerial vehicles\\nplant disease monitoring\\nuav-mounted sensor suite\\nflorida citrus production\\ndepth-invariant sensing methodology\\npolarized amber light\\nreflectance measurement\\nstarch accumulation measurement\\ngreening-infected leaves\\ndepth information\\nmachine learning models\\ncalifornia\\nplant pathology studies\",\"610\":\"agriculture\\nvegetation mapping\\nrobots\\nfeature extraction\\nsugar industry\\nspraying\\ncameras\\nagrochemicals\\ncrops\\nfarming\\nimage classification\\nmarkov processes\\nmobile robots\\nrandom processes\\nrobot vision\\nsmoothing methods\\nsugar beets\\nweeds\\nprecision farming\\nherbicides\\npesticides\\nselective spraying\\nmanual weed removal\\nvalue crops\\nrobot perception system\\nmobile robot\\nvegetation detection\\nrandom forest classification\\nsmoothing\\nmarkov random field\\nfarm robot\",\"611\":\"metadata\\ntraining\\nlighting\\nsun\\nagriculture\\nimage color analysis\\nvegetation\\nimage classification\\nmeta data\\nmultilayer perceptrons\\norchard metadata\\nmonocular vision systems\\nprecision agriculture applications\\nimage parsing\\nintra-class variations\\nmultiscale multilayered perceptron architecture\\nmlp architecture\\npixel segmentation\\napple orchard\\nfruit detection\\nyield estimation\",\"612\":\"agriculture\\npipelines\\nimage color analysis\\nlighting\\nimage resolution\\ndetectors\\nshape\\ncrops\\nimage classification\\nimage filtering\\nimage texture\\nlearning (artificial intelligence)\\nobject detection\\ntexture-based fruit detection\\nfruit smooth patterns\\nkeypoint detection algorithm\\nround fruit detection\\nhigh resolution imagery\\ngrapes\\napples\\ncontrasting features\\nfruit contour\\nocclusion\\nfruit color\\nbackground foliage\\ngradual intensity variation\\ngradient orientation\\nfruit location\\nseed points\\nimage filter\\nclassification\\nmodified histogram of oriented gradients\\npairwise intensity comparison texture descriptor\\nrandom forest classifier\",\"613\":\"actuators\\ncameras\\nswitches\\nvibrations\\nquantization (signal)\\nrobot vision systems\\nrobot vision\\nvibration control\\nopen-loop manner\\ndynamic vision\\npiezoelectric actuator\\nrobotic system\\nsmooth-pursuit like motion\\npulse-like switching\\ncomplaint mechanism\\nhigh-frequency switching\\nunit actuator\\npwm quantization\\nclassical feedback controller\\nmotion profile\\npoint-to-point movements\\ncamera positioning devices\\nvibration suppression techniques\\nsmooth-pursuit like movements\\ncompliant camera orientation system\\ntracking\\ndiscrete switching commands\",\"614\":\"robots\\nbatteries\\nmathematical model\\nreceivers\\nheuristic algorithms\\nalgorithm design and analysis\\nsoftware algorithms\\nnear-optimal dynamic power sharing scheme\\nself-reconfigurable modular robots\\nnear-optimal power sharing mechanism\\nconnected modules\\npower status\\nrobotic system\\nphysics-based simulation environments\\nreal robotic hardware\\npolymorphic robotics laboratory\\nuniversity of southern california\",\"615\":\"robot sensing systems\\npropulsion\\nrobot kinematics\\nrouting\\nforce\\nlight sources\\ndecentralised control\\nflow control\\nhydraulic control equipment\\nmarine control\\nmarine propulsion\\nmobile robots\\nmotion control\\nmodular hydraulic propulsion\\nfluid routing\\nmodular robot\\nfluid environment\\nrobot modules\\nhydraulics network\\ndecentralized motion controller\\n3d simulations\\n2d prototype modules\\nwater tank\",\"616\":\"robots\\ncontracts\\nrobustness\\ntunneling\\nlattices\\ncolor\\nalgorithm design and analysis\\nmobile robots\\nrobust control\\nrobust meta-module\\ncompact meta-module\\nedge-hinged modular robot units\\nm-tran\\nsuperbot\\nsmores\\nubot\\npolybot\\nckbot\\nmolecubes\\nroombots\\nrotational degrees of freedom\\nscrunch\\/relax operations\\ntransfer operations\\ntunneling-based reconfiguration strategy\\ncrystalline robots\\ntelecube robots\\ngeometric reconfiguration algorithms\",\"617\":\"kinematics\\nend effectors\\npath planning\\nservice robots\\ncollision avoidance\\nmanipulators\\ntask-driven algorithm\\ntask-based configuration synthesis algorithm\\nmodular robot manipulators\\nobstacles\\nconfiguration synthesis method\\ncollision detection\\ntask space\",\"618\":\"manipulators\\nshoulder\\nelbow\\nwrist\\ngears\\nsafety\\ndesign\\nmanipulator dynamics\\nmanipulator kinematics\\nmodular manipulator design\\nrevolute joint\\nadjacent link\\nanthropomorphic arm manipulator\\nserial-chain manipulator\\nkinematic representation\\nd-h parameter\\ndenavit-hartenberg parameter\",\"619\":\"robots\\nplanning\\nresource management\\nentropy\\nshape\\npath planning\\nnavigation\\nmobile robots\\nmulti-robot systems\\noptimisation\\nsimultaneous configuration formation\\ninformation collection\\nmodular robotic systems\\nsingleton modules\\nuser-specified target configuration\\nrobot navigation\\nbudget-limited heuristic search-based algorithm\\nentropy maximization\",\"620\":\"robot kinematics\\nalgorithm design and analysis\\nintegrated circuits\\nmobile communication\\nshape\\ntransforms\\nmotion control\\npath planning\\nrobots\\nfull-resolution reconfiguration planning\\nheterogeneous cube-shaped modular robots\\nsliding motion primitive\\nfull-resolution reconfiguration algorithm\\nmotion primitive\\ncube module\\nmodular robot design\\ncompressed configuration\\nthree-dimensional connected structures\",\"621\":\"robots\\nmagnetic domains\\nmagnetoacoustic effects\\ndynamics\\nboundary conditions\\nlaplace equations\\nmicroorganisms\\nmicrorobots\\nmobile robots\\nmotion control\\nmulti-robot systems\\nposition control\\ntime series\\ndynamic actuating fields\\nmicrorobotic swarm steering\\nmicrorobot motion control\\ntime-series sequence\\nactuating field direction\\ntarget position\\noptimal moving direction\\nsteering algorithm\\nstatistically optimal actuating direction\\nmicrorobot transport\\nmagnetic field generation\\naggregated magnetic microparticles\",\"622\":\"charge carrier processes\\noptical feedback\\noptical saturation\\nbiomedical optical imaging\\nintegrated optics\\nlaser beams\\noptical imaging\\nbiology\\nbrownian motion\\nclosed loop systems\\nfeedback\\nmanipulators\\nradiation pressure\\nstability\\nstochastic processes\\nrobot-assisted optical trapping\\nbiological cell manipulation\\nstochastic perturbations\\nrobotic control technique\\nrandom brownian perturbations\\ncell optical manipulation\\nposition feedback saturation\\nclosed-loop system stability\",\"623\":\"robots\\ncollision avoidance\\nmulti-robot systems\\ngrammar\\ncommunication networks\\nlabeling\\ncontinuous systems\\ndiscrete systems\\ndistributed control\\ngraph grammars\\nmobile robots\\nmotion control\\nrobot dynamics\\nswitching systems (control)\\ntemporal logic\\nhybrid control\\nmultirobot systems\\nembedded graph grammars\\ncooperative motion control\\ncooperative task control\\ndynamic constraints\\nmodel-checking-based task planning module\\nswitching control modes\\nlocal interaction rules\\nlinear temporal logic\\ncommunication network connectivity maintenance\\ninterrobot collision avoidance\",\"624\":\"robot kinematics\\nmirrors\\nrobot sensing systems\\nangular velocity\\nconvergence\\ndistributed control\\nmulti-robot systems\\nposition control\\nmultirobot system\\npiecewise-smooth function\\nrelabeling strategy\\nmirror configuration\\norientation control\\nreference frame\\nnonholonomic robot team\\ndistributed formation control\",\"625\":\"topology\\nmulti-robot systems\\nrobot kinematics\\nlaplace equations\\nmatrix decomposition\\ntrajectory\\nmotion control\\nposition control\\ntime-varying systems\\ncoordinated motion\\nmultirobot systems\\ntime varying communication topologies\\ncontrol strategy\\ncooperative tracking\\nperiodic setpoint trajectories\\nheterogeneous robot group\\ncontrol input\\nrobot position control\\nfinite range communication devices\",\"626\":\"robot sensing systems\\ntrajectory\\nrobot kinematics\\nmaximum likelihood estimation\\nthree-dimensional displays\\nconvergence\\ndistributed algorithms\\niterative methods\\nmobile robots\\nmulti-robot systems\\npath planning\\ntrajectory control\\ndistributed trajectory estimation\\ncommunication constraints\\nprivacy constraints\\ntwo-stage distributed gauss-seidel approach\\ndistributed algorithm\\n3d trajectory estimation\\ncooperative robots\\nrelative pose measurements\\nmaximum likelihood trajectory\\nquadratic subproblems\\ndgs algorithm\\ninformation exchange\",\"627\":\"robot kinematics\\nrouting\\ntrajectory\\ncomputational modeling\\nmobile robots\\nprobabilistic logic\\nfeedback\\nmulti-robot systems\\npath planning\\ncommunication-aware multi-robot systems\\nhybrid architecture\\nmobile robot team\\nmultihop ad-hoc network\\nmobility problem\\ncommunication problem\\nouter global planning loop\\ninner local loop\\nmotion routing\\nnetwork routing\\ntwo-stage feedback system\",\"628\":\"servers\\ncloud computing\\nreal-time systems\\nbandwidth\\nlegged locomotion\\ncomputer architecture\\ncontrol engineering computing\\ngeographic information systems\\nhumanoid robots\\nlow latency bounty hunting\\ngeographically adjacent server configuration\\nreal-time cloud control\\nfrequency real-time controllers\\nphysical robot hardware\\nbounty hunting cloud server architecture\\nrobot weight\\nstability algorithms\\nphysical robot\\nhumanoid robot\",\"629\":\"forward error correction\\nrobot kinematics\\nencoding\\nrouting protocols\\nrobustness\\nreed-solomon codes\\nmobile ad hoc networks\\nmobile robots\\nquality of service\\nadaptive forward error correction\\nadjustable-latency qos\\nrobotic networks\\nfec technique\\nlatency tolerance\\nnetwork traffic\\npacket delivery performance\\npacket reception history\\nfec encoding strength\\nquality of service mechanism\\nmobile robotic teams\\nmanet\",\"630\":\"shape\\nplanning\\ntrajectory\\nrobot kinematics\\ndynamics\\nsurveillance\\ndifferential equations\\nmulti-robot systems\\npath planning\\ntime-varying systems\\ndynamic perimeter surveillance\\nmotion planning method\\ndifferentiable boundary function\\ndifferential equation\\ntime-varying perimeter\",\"631\":\"robot sensing systems\\nmobile communication\\nestimation\\ntemperature measurement\\ngaussian processes\\nenvironmental factors\\nmobile robots\\nregression analysis\\nthermometers\\nwireless sensor networks\\nenvironmental field estimation\\nhybrid-mobility sensor networks\\nflying robots\\nenvironmental sensing\\nsmall infrared thermometer\\nsurface temperature\\nspatial statistics\\ngaussian process regression\\nindoor testbed\\nquadrotor\\nsimulated static sensing nodes\\nspatial distribution\\ncontrolled thermal gradient\\nreal-world testbed environment\\ntemperature 25 degc to 65 degc\",\"632\":\"boats\\nrobots\\nocean temperature\\nvehicles\\nsea surface\\nbiological system modeling\\nmobile robots\\nmotion control\\nmulti-robot systems\\nrobotic swarms\\nocean surface mapping\\nrobotic boats collective motion\\nlocal neighbor-neighbor interaction\\nsize-tunable mapping\\nenvironment-responsive migration\\nleader-responsive migration\\ncatalina harbor\\nsanta catalina island\\ncalifornia\",\"633\":\"trajectory\\nspirals\\nvehicles\\nrobot kinematics\\ndecentralized control\\nsynchronization\\ndistributed control\\nmanipulators\\nmarine control\\nmobile robots\\nmotion control\\nmulti-robot systems\\nposition control\\ndistributed planar manipulation\\nfluidic environments\\nautonomous surface vehicles\\nanalytical motion trajectories\",\"634\":\"interpolation\\nrobots\\ntransient analysis\\nposition measurement\\nbiology\\ndynamics\\nestimation\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmotion control\\nmulti-robot systems\\nposition control\\nswarm behavior classification\\ncompressive subspace learning\\nbio-inspired robot swarms\\ncollective behaviors\\nagent measurements\\nbehavior recognition\\nswarm behavior prediction\\nswarm collective motion\\nlow-dimensional linear subspace\\ntransient agent behavior\\nbehavior subspaces\\nsubspace estimation\\ncompressive measurements\\nagent positions\",\"635\":\"robots\\nmathematical model\\ntorque\\nobservers\\ntrajectory\\ngrasping\\ncontrol system synthesis\\nmanipulator dynamics\\nmulti-robot systems\\nrobot swarm control\\nparent system\\nobject manipulation\\nmean position\\nswarm distribution variance\\ncontroller design\\nstep input tracking\\nstep disturbance rejection\",\"636\":\"observers\\nrobot kinematics\\ncontrol systems\\nestimation error\\nvehicle dynamics\\nmulti-robot systems\\ndiscrete time systems\\ndistributed control\\nestimation theory\\nfeedforward\\nlinear systems\\noptimal control\\nposition control\\nstate feedback\\nmultiple robots\\nformation control\\nlocal observer\\nglobal control input\\nfeedforward term\\nlinear static feedback law\\ndiscrete-time linear dynamics\\nmultirobot systems\\ndistributed state feedback control\\ndiscrete-time control\",\"637\":\"collision avoidance\\nnavigation\\noptimization\\nrobot kinematics\\nprogramming\\nplanning\\nconvex programming\\ndistributed control\\ngeometry\\nmobile robots\\nmulti-robot systems\\ndistributed multirobot formation control\\noptimization approach\\ngeometric approach\\n2d environments\\n3d environments\\nstatic obstacles\\ndynamic obstacles\\nvisibility radius\\ndistributed consensus\\nsequential convex programming\\nconvex neighborhood\\ncollision-free formation\\nquadrotors\",\"638\":\"robot sensing systems\\noptimization\\nswitches\\nmeasurement\\nlarge-scale systems\\ncontinuous systems\\ndistributed control\\nmulti-robot systems\\noptimisation\\nheterogeneous robots swarm\\nheterogeneous large-scale distributed robotic systems\\ndiversity impact\\ncontinuous model\\noptimization problem\\noptimal transition rate set\\ntrait distribution\\neigenspecies\",\"639\":\"robot sensing systems\\nknowledge based systems\\nknowledge engineering\\nsemantics\\ndata visualization\\nbig data\\ncloud computing\\ncontrol engineering computing\\nknowledge representation\\nmanipulators\\nmobile robots\\nweb services\\nopen robotics research\\nweb-based knowledge services\\nbig data storage\\nbig data management\\nknowledge processing\\ncloud-based computation\\nweb technology\\nrobotics community\\nopen research discipline\\neu project\\npizza preparation experiment\\nautonomous robot manipulation\\nopen knowledge base\\ncloud-based robot knowledge service\\nweb-based robot knowledge service\\nopenease\",\"640\":\"robot kinematics\\nmathematical model\\nsimultaneous localization and mapping\\nprobabilistic logic\\nrobustness\\nmobile robots\\nmonte carlo methods\\nprobability\\nrobots\\nprobabilistic qualitative relational mapping\\npqrm algorithm\\nnoisy sensor measurements\\nqualitative state representations\\nrelative map information\\nprobabilistic distributions\\nmonte carlo simulations\\nnew college dataset\",\"641\":\"decision making\\nhistory\\nprobabilistic logic\\nuncertainty\\nrobot sensing systems\\nsynchronization\\ndecision theory\\nentropy\\ngraph theory\\nmarkov processes\\nmulti-robot systems\\noptimisation\\ngraph-based cross entropy method\\nprobabilistic algorithm\\ndecentralized partially observable markov decision process\\nsynchronous decision-making frameworks\\nreal-world robotics applications\\nmultirobot dec-pomdp\\nhigh-level macro-actions\\nasynchronous decision-making\\ncombinatorial optimization literature\\nconstrained package delivery domain\",\"642\":\"containers\\nsearch problems\\nrobot sensing systems\\ncognition\\nplanning\\nmobile communication\\nmanipulators\\nmarkov processes\\nmobile robots\\nmonte carlo methods\\npath planning\\nmobile manipulation robot\\nobject search\\ncluttered domain\\noccluding objects\\nobject collocation\\nmarkov chain monte carlo method\\nmcmc method\\nspatial constraints\\nreceding-horizon forward search\\nbaseline systematic search strategy\",\"643\":\"vehicles\\nrobot sensing systems\\nmoon\\ncollision avoidance\\nnoise reduction\\nnasa\\nmobile robots\\nplanetary rovers\\nremotely operated vehicles\\nstatistical testing\\nlunar rover\\nvirtual bumper\\nsafeguarding mechanism\\nsequential probability ratio test\\nremotely operated robots\",\"644\":\"robots\\nbayes methods\\nuncertainty\\nmarkov processes\\ngames\\nplanning\\nobservability\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nuncertainty handling\\npartially observable markov decision process\\npomdp-lite\\nrobot planning\\nsystem uncertainty\\nbayesian reinforcement learning algorithm\",\"645\":\"neural networks\\nrobots\\ncomputational modeling\\nstochastic processes\\ndynamic programming\\nlearning (artificial intelligence)\\npredictive models\\nneural nets\\nregression analysis\\nstatistical distributions\\ntransfer functions\\nnonlinear activation function\\noutput probability distribution\\nlwr\\nlocally weighted regression\\nreinforcement learning\\nddp\\ndifferential dynamic programming\\nneural network\",\"646\":\"computational modeling\\nmathematical model\\noptimal control\\nrobots\\ndata models\\nheuristic algorithms\\napproximation algorithms\\nlearning (artificial intelligence)\\nautonomous drifting\\nsimulation-aided reinforcement learning\\ncontinuous state-action simulators\\nreal-world robot\\ncontrol policies\\ngradient-based learner\\ninformation transfer\\nrobotic car learning\\ncontrolled drifting maneuvers\",\"647\":\"planning\\nprobabilistic logic\\nsemantics\\nrobot sensing systems\\ncognition\\nnatural languages\\nhumanoid robots\\nhuman-robot interaction\\nprobability\\nhuman instructions\\nrobot actions\\ngoal formulation\\nprobabilistic planning\\nspoken language\\nverbal instructions\\ncognitive architecture\\nlanguage-based semantic reasoning\\nrobot symbols\\naction execution\\naffordance concept\\ntask execution\\nicub humanoid robot\\nevent failure\\naction failure\",\"648\":\"planning\\nrobots\\nmonitoring\\ncollision avoidance\\nuncertainty\\nrobustness\\napproximation algorithms\\nimage representation\\nmanipulators\\nmobile robots\\npath planning\\nrobot vision\\nimplicit belief-space pre-images\\nhierarchical execution\\nfactored approximation\\npre-image representation\\nmobile-manipulation domain\\npick-and-place manipulation\\nhierarchical planning method\\ncontinual replanning\",\"649\":\"robot kinematics\\ntrajectory\\nschedules\\nvehicles\\nstability analysis\\nautonomous aerial vehicles\\ncollision avoidance\\nhelicopters\\nmanipulators\\nmobile robots\\nsampling methods\\ntrajectory control\\npioneer 3dx mobile robot\\n3d robotics quadcopter\\nobstacle-free trajectory generation\\nsampling-based planner\\nmotion planning\\nhierarchical task decomposition\\nrobot coordination\\ntask planning\\nautonomous packet transportation\\nugv\\nunmanned ground vehicle\\nmanipulator arm\\nmmuav\\nunmanned aerial vehicle\\nautonomous delivery task\",\"650\":\"robots\\nplanning\\ncollaboration\\ntraining\\nmarkov processes\\nstate estimation\\ncognition\\nhuman-robot interaction\\ninference mechanisms\\nlearning (artificial intelligence)\\nhierarchical task network autonomous construction\\nhuman-robot collaboration\\nclique-chain htn\\ngraphical task representations\\nmotor primitives\\nenvironmental representation\\nmultiresolution goal inference task\\ntransfer learning application\",\"651\":\"robots\\nplanning\\nsequential analysis\\nface\\nhardware\\nsoftware algorithms\\nsoftware\\nmobile robots\\nproduction planning\\nrobotic assembly\\nassembly sequence planning\\nplanar structures\\nrectangular modules\\nbrick wall pattern\\nmobile modular robots\\ntarget structures\\nsymmetric patterns\",\"652\":\"automata\\nlinear programming\\nplanning\\nmarkov processes\\nautomation\\ndecision making\\nlinear systems\\npath planning\\nrobots\\nstochastic systems\\ntemporal logic\\nmultiobjective planning\\nmultiple high level task specifications\\nsequential stochastic decision making problem\\nmultiple objective functions\\nlinear temporal logic\\nsatisfaction probability\\nconstrained markov decision processes\\nformal verification\\nproduct operation\\nlinear program\",\"653\":\"robot kinematics\\nrobot sensing systems\\nbase stations\\nplanning\\nrelays\\nmulti-robot systems\\npath planning\\nasynchronous multirobot exploration\\nrecurrent connectivity constraints\\ncentralized control\\nteam exploration strategy\\ncommunication constraints\\ncentralized asynchronous planning framework\",\"654\":\"robots\\ncollision avoidance\\nplanning\\ncomputational modeling\\npins\\nkinematics\\nsearch problems\\nmanipulator kinematics\\nposition control\\nstability\\nsupport pin utility\\nsequential robotic manipulation\\npick-and-place regrasp reorientation capability\\nvertical pin\\nstable object placements\\nforce-closure grasps\",\"655\":\"planning\\ncomputer aided manufacturing\\nrobot kinematics\\nstandards\\nstrips\\nreal-time systems\\nassembling\\nconcurrent engineering\\nend effectors\\nhuman-robot interaction\\nindustrial manipulators\\nlearning systems\\nmonte carlo methods\\nmulti-agent systems\\nhuman-robot assembly task\\nlearning from demonstration\\nmonte-carlo planning\\nstandard relational mdps\\nrelational concurrent activity processes\\nmultiple end-effectors\\nsingle-robot manipulation\\nmultiagent domains\\nhuman-robot collaboration\\nconcurrent cooperation\",\"656\":\"radar tracking\\ntracking\\nrobot sensing systems\\ndetectors\\nmobile communication\\nfiltering theory\\nintelligent control\\nmobile robots\\nrobot vision\\nmultimodal people tracking\\nmobile platforms\\ndynamic environments\\nintelligent systems\\nrobots\\nintelligent human environments\\nperson detectors\\nfiltering methods\\ndata association algorithms\\ncomputer vision communities\\nreal-world scenarios\\nsensory modalities\\nperformance metrics\\nintegrated real-time multimodal laser\\nrgb-d people tracking framework\\nfirst-person perspective\",\"657\":\"detectors\\ntraining\\ncameras\\nthree-dimensional displays\\nrobot vision systems\\nfeature extraction\\nimage colour analysis\\nimage matching\\nimage sensors\\nobject detection\\npedestrians\\nreal-time rgb-d based template matching pedestrian detection\\ncomputer vision\\nreal-time depth-based template matching people detector\\nupper-body orientations\\nsliding window\\nweighted template approach\\nappearance based detector\\nappearance cues\\neth dataset sequence\",\"658\":\"trajectory\\ntracking\\nprediction algorithms\\nhidden markov models\\ncomputational modeling\\npredictive models\\nreal-time systems\\nbayes methods\\nmotion control\\npedestrians\\nroad traffic control\\ntrajectory control\\nvelocity control\\nglmp\\nrealtime pedestrian path prediction\\nglobal and local movement patterns\\nreal-time algorithm\\ncluttered environments\\npedestrian motion\\ncrowd density\\nshort-term prediction\\nlong-term prediction\\n2d trajectories\\nbayesian inference\\nvelocities\\nentry points\\nmovement features\\nsparse trajectory\\nnoisy trajectory\\nindoor crowd videos\\noutdoor crowd videos\",\"659\":\"adaptation models\\ntrajectory\\ntraining\\ndetectors\\nobject tracking\\nreliability\\nproposals\\nimage classification\\nlearning (artificial intelligence)\\npedestrians\\ntrees (mathematics)\\nhierarchical online domain adaptation\\ndeformable part-based model\\ndpm\\ntwo-level hierarchical adaptation tree\\ninstance model\\nleaf nodes\\ncategory model\\nroot node\\nmultiple object tracking procedure\\nmot\\ntarget-domain annotated data\\nsource-domain data\\nsource-to-target domain adaptation\\nobject annotation\\npedestrian detection\\nexemplar classifier online training\\npedestrian instances\\nhierarchical model\\npedestrian trajectory\",\"660\":\"legged locomotion\\ngyroscopes\\naccelerometers\\ntracking\\nmotion segmentation\\nrobot sensing systems\\nacceleration\\ninertial navigation\\nkalman filters\\nnonlinear filters\\nwearable computers\\nwalking compass\\nhead-mounted imu sensor\\nhead-mounted orientation system\\nhos\\ninertial sensor\\nstable normalized coordinate system\\nwalking patterns\\nrotating motion\\ninterfering head motion elimination\\nextended kalman filter\\ngoogle glass platform\\npath tracking\\npdr\\nactivity recognition algorithms\",\"661\":\"bones\\ntracking\\nanimals\\noptimization\\nmuscles\\nmanuals\\noptimisation\\nparameter estimation\\nrobots\\nautomatic bone parameter estimation\\nanimation\\nbiomechanics\\nrobotics\\nanimal behavior\\ntracking accuracy\\npassive optical motion capture systems\\nbone length relations\\noptimal skeleton model\\nmanual process\\nprobabilistic skeleton tracking performance\\nsequential model-based bayesian optimization\\nsmbo methods\\nreal-world datasets\",\"662\":\"lenses\\ncolor\\nfinite impulse response filters\\ncameras\\nsilicon\\nthermal lensing\\noptical imaging\\nimage colour analysis\\ninfrared imaging\\nobject detection\\ncompact coaxial thermal imaging system\\ncolor imaging system\\nsilicon-glass hybrid lens\\ncompact optical system\\nfocused light\\nsilicon plate\\nfocal plane\",\"663\":\"force\\nrobot sensing systems\\ngrasping\\ntracking\\nfeature extraction\\npattern recognition\\ndexterous manipulators\\nelectromyography\\nforce measurement\\ngrippers\\nhuman-robot interaction\\ninertial systems\\nmedical signal processing\\nregression analysis\\nsensors\\ntime-varying systems\\nteleoperation system\\ngrasping force estimation\\ntime-varying semg feature compensation\\ntele-manipulation\\nsemg signals\\nhuman motion\\nforce intention\\ndiscrete force level detection\\nobject holding\\nobject manipulation\\nsurface electromyogram signals\\nregression strategy\\nproportional measurements\\ncontinuous measurements\\ngrasping force transmission\\n7-dof robot arm\\ninertia measurement unit sensor interface\\nforce capturing system\\nmotion capturing system\\ninteraction tasks\\ngrasping motions\\nholding motions\\nreleasing motions\",\"664\":\"robot sensing systems\\noptical sensors\\nvisualization\\nbiomedical optical imaging\\nlighting\\nvelocity measurement\\nautomobiles\\nclosed loop systems\\nestimation theory\\nkalman filters\\nmobile robots\\nnonlinear filters\\npi control\\nrobot vision\\nsteering systems\\nvelocity control\\noptic-flow based car-like robot\\nlight level range\\nbiocarbot\\nvelocity estimation\\nsteering angle estimation\\nextended kalman filter\\nekf\\nclosed-loop mode\\nproportional integral controller\\npi controller\",\"665\":\"solid modeling\\nvisualization\\nthree-dimensional displays\\ncameras\\nrobot vision systems\\nimage reconstruction\\nautonomous underwater vehicles\\nestimation theory\\nexpectation-maximisation algorithm\\nfeature extraction\\ngaussian processes\\nslam (robots)\\nlarge-scale model-assisted bundle adjustment\\ngaussian max-mixtures\\nvisually-derived features\\nthree-dimensional mesh\\nexpectation-maximization\\nem\\nfeature locations estimation\\nsimultaneous localization and mapping solvers\\nslam solvers\\nbluefin robotics hovering autonomous underwater vehicle\\nhauv\\nss curtiss\",\"666\":\"legged locomotion\\nwheels\\nmobile communication\\nrobot sensing systems\\nactuators\\nmanipulators\\nrescue robots\\n3d laser data\\n2d heightmap\\nnimbro rescue\\ndarpa robotics challenge\\nmobile manipulation robot\\nsearch & rescue operations\\nmomaro\\nwheeled-legged robot\\nhybrid driving-stepping locomotion\",\"667\":\"kinematics\\njoints\\nwheels\\njacobian matrices\\nsuspensions\\nmobile robots\\nflexible structures\\nlegged locomotion\\nmars\\npath planning\\nplanetary rovers\\nrobot kinematics\\nsuspensions (mechanical components)\\nactively articulated suspension\\nmartian analog surface\\nactively articulated wheel-on-leg rovers\\ntraversability\\nkinematic model\\nrecursive kinematic propagation\\nonline terrain map generation\\nlimb articulation angles\\nrover body trajectories\\nmartian analog terrain\",\"668\":\"logic gates\\ntraining\\nrobot sensing systems\\ncomputer architecture\\nrecurrent neural networks\\nvibrations\\nneurons\\nfrequency-domain analysis\\nmobile robots\\nneurocontrollers\\nrecurrent neural nets\\ntime-domain analysis\\nrnn\\nrobust vibration-based ground classification\\ndynamic cortex memories\\ndcm\\nlong short term memories\\nlstm\\nregularization technique\\nsequence boundary dropout\\nsbd\\ntime domain\\nfrequency domain\\nclassification accuracy\\nrandom activation preservation\\nrap\",\"669\":\"boats\\nmathematical model\\nacceleration\\ndynamics\\nactuators\\ndata models\\ncameras\\nimage processing\\nlearning (artificial intelligence)\\nmobile robots\\nfad learning\\nmotor babbling\\nboat dynamics\\nfishing\\nboat modeling method\\nfree dynamics acceleration\\nboat actuator acceleration\\ndynamics learning trees\\ndlt\\nimage processing method\\nunderwater camera\\nwater pool\",\"670\":\"cameras\\nlighting\\nrobot vision systems\\nharmonic analysis\\nrobustness\\nimage matching\\nmobile robots\\nnavigation\\nslam (robots)\\nskyline-based localisation\\nmanoeuvring robots\\nuv sensors\\nspherical harmonics\\nplace recognition\\nrobot navigation\\nrough terrain\\nsensory modality\\nimage processing\\nsequence-based image matching\\nuv-sensitive fisheye lens camera\\npitch and roll invariance\\nappearance-invariant technique\\nseqslam\\nviewpoint-invariant technique\\nfab-map 2.0\",\"671\":\"image edge detection\\nsolid modeling\\nthree-dimensional displays\\ncomputational modeling\\nrobustness\\ntraining\\nedge detection\\nimage sequences\\nmatrix algebra\\npose estimation\\nregression analysis\\nfast 6d pose estimation\\ntexture-less objects\\nsingle rgb image\\n3d pose estimation\\ncoarse initialization\\nedge correspondences\\nsimilarity measure\\nlinear regression matrix\\nobject clutter\\nbackground clutter\",\"672\":\"robot sensing systems\\nlaser radar\\nroads\\ncameras\\nvehicles\\nprobabilistic logic\\nmobile robots\\noptical radar\\nradar imaging\\nprobabilistic traversability map generation\\n3d-lidar\\ncamera\\nrough terrain\\noutdoor mobile robot\\nlane markings\\n2d probabilistic grid map online\\ntraversability estimation\\nvision sensor\\nunmanned ground vehicle\",\"673\":\"videos\\npediatrics\\ntracking\\nstatistics\\nmathematical model\\nsociology\\ncomputer vision\\nbehavioural sciences computing\\ndata analysis\\nimage colour analysis\\nmedical disorders\\npsychology\\nvideo coding\\nautomated activity video coding\\nocd study\\nbehavior analysis\\nrisk marker identification\\ncomputer vision techniques\\nautomatic video data analysis\\nenvironmental factors\\nobsessive-compulsive disorder\\nautomatic overhead video annotation\\nrinsing\\napplying soap\\nturning on-off-the-water faucet\\ncolor-based background subtraction method\\nforeground probability score\\nlabeled regions-of-interest\\nroi\\nactivation signal characterization\\nhandwashing procedure\",\"674\":\"three-dimensional displays\\nobject tracking\\nreal-time systems\\nrobustness\\nvegetation\\ncameras\\ncloud computing\\nimage denoising\\nimage restoration\\nlearning (artificial intelligence)\\npose estimation\\nmultiple rigid symmetric object tracking\\nmultiple rigid nonsymmetric objects tracking\\nreal-time object tracking approach\\nreal-time requirement setting\\nrandom-forest based learning\\nobject motion\\n3d point cloud data\\nobject pose tracking\\n3d space\\nmotion blur\\nnoisy depth data\\ncamera motion\",\"675\":\"welding\\ncameras\\nrobot vision systems\\nlighting\\nimage edge detection\\narc welding\\ncmos image sensors\\ncontrol engineering computing\\nfield programmable gate arrays\\nimage denoising\\nobject detection\\nproduction engineering computing\\nrobot vision\\nrobotic welding\\nvision-based measurement system\\nwelding groove measurements\\nrobotic welding applications\\nelectric arc welding\\nfusion welding\\nvbm system\\nbeveled edge measurement\\nwelding plates\\nsingle cmos camera\\nvisual sensor\\ncomputer vision algorithms\\nfpga board\\nfield programmable gate array board\\nrobot movement control\\nweld pattern adjustment\\nwelder equipment parameter adjustment\\ngroove mapping equipment\\nnoise removal technique\\nline segment detection technique\",\"676\":\"robot sensing systems\\nvisualization\\nuncertainty\\nbayes methods\\nrobotic assembly\\ninspection\\nindustrial manipulators\\nmobile robots\\nrobot vision\\nstatistical distributions\\nbayesian formulation\\nprobability distributions\\ndepth sensor\\nshape models\\nrobotic assembly manipulation\\nprobabilistic visual verification\",\"677\":\"robots\\nlesions\\nsurgery\\nblades\\nimplants\\nvelocity control\\nelectron tubes\\nbone\\ndesign engineering\\ndexterous manipulators\\nmanipulator kinematics\\nmedical robotics\\nprosthetics\\ndebriding tool design\\ndebriding tool quantitative characterization\\nrobot-assisted osteolysis treatment\\nrobotic system\\nbone degradation\\npolyethylene liner\\nacetabular implant\\ntotal hip replacement surgery\\ncontinuum dexterous manipulator\\ncdm integration\\nrobotic arm\\nrotational speed\\naspiration pressure\\nirrigation flow\\nsweeping velocity\",\"678\":\"clothing\\niron\\nrobot sensing systems\\nlighting\\nshape\\npipelines\\nfeature extraction\\nimage colour analysis\\nimage fusion\\nmanipulators\\nrobot vision\\nservice robots\\nmultisensor surface analysis\\nrobotic ironing\\nrobotic manipulation\\nsurface scan techniques\\ncurvature scan\\ndiscontinuity scan\\nshape features\\nwrinkle analysis algorithm\",\"679\":\"trajectory\\nvehicles\\nsafety\\nrobustness\\nrobots\\nadaptive control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nfeedback\\nrobust control\\nstate-space methods\\nreach control approach\\nquadrocopter\\npolytopic state space\\nhigh-level control specification\\nrobust controller design\\npiecewise affine feedback controller synthesis\\nsafe robust robot maneuvers\",\"680\":\"dynamics\\nestimation\\nsolid modeling\\nforce\\nhumanoid robots\\naerospace electronics\\nadaptive control\\nrobot dynamics\\nrobot kinematics\\nrobust control\\nvariable structure systems\\nrobust whole-body controller\\nadaptive whole-body controller\\n23-dof humanoid robot\\nmultiple tasks\\nuncertain disturbances\\ndynamic model-free whole-body controller development\\nhigh kinematic redundancy\\nforce-level operational-space control framework\\njoint torques\\nrobot dynamics model\\nadaptive sliding-mode\\nonline dynamics estimation schemes\",\"681\":\"force\\nadaptation models\\nrobot kinematics\\nmobile robots\\ndynamics\\ntrajectory\\nadaptive control\\nhuman-robot interaction\\nmotion control\\npath planning\\nrobot dynamics\\nrobot navigation\\nhuman environments\\nsocial force model\\nsocial proxemics potential field\\nproxemics theory\\nsocial interaction force\\nrobot motion control design\\ndynamic control\\nkinematic control\\nkinematic velocity constraints\\nsocial convention\\nv-rep platform\",\"682\":\"pd control\\nmanipulators\\ndesign methodology\\nstability analysis\\ncontrol system synthesis\\nmotion control\\noptimal control\\nstability\\ntrajectory control\\nl1 optimal pd controller\\njoint motion control\\nrobot manipulator\\nproportional-derivative controller synthesis\\nl\\u221e norm\\ninput-to-state stability\\niss\\nvector-valued signal\\nbounded persistent disturbances\\nl\\u221e space\\ntrajectory tracking control\",\"683\":\"target tracking\\nrobot sensing systems\\nprediction algorithms\\nmobile robots\\nuncertainty\\ntime measurement\\ngaussian processes\\nmixture models\\nmotion control\\nposition control\\nrobust control\\nsensors\\nmultiple-hypothesis chance-constrained target tracking\\nidentity uncertainty\\nmicrosoft kinect sensor\\npioneer robot\\ntracking success probability\\ngaussian mixture model\\ntarget appearance model\\ntarget motion model\\nmultiple-hypothesis prediction algorithm\\nmoving target position\\nfinite sensing region\\nfan-shaped field of view\\ncrowded environments\\nmobile robot\\nrobust target tracking algorithm\",\"684\":\"exoskeletons\\nadaptive control\\ntorque\\ntrajectory\\nmuscles\\nreal-time systems\\nbone\\ncontrol system synthesis\\nmuscle\\npatient rehabilitation\\ntrajectory control\\nlissi-lab\\neicosi\\nexoskeleton intelligently communicating and sensitive to intention\\nnonlinear proportional control\\ntrajectory tracking\\ntherapeutic doctor\\nresistive rehabilitation\\nassistance-as-needed rehabilitation\\nknee joint level\\nlower limb exoskeleton control\\nreal-time experiments\\nactuated knee joint exoskeleton\\naugmented l1 adaptive control\",\"685\":\"uncertainty\\nswitches\\nstability analysis\\ndelays\\nrobust control\\nadaptive systems\\nadaptive control\\napproximation theory\\nerror analysis\\nmobile robots\\ntrajectory control\\nuncertain euler-lagrange systems\\nadaptive time-delayed robust control\\natrc\\ntrajectory tracking control\\nswitching control logic\\ntime-delayed logic\\napproximation error\\nstability criterion\\nwmr\\nnonholonomic wheeled mobile robot\",\"686\":\"semantics\\ntraining\\ncomputer vision\\nbenchmark testing\\nvisualization\\nbayes methods\\nimage classification\\nimage filtering\\nlearning (artificial intelligence)\\nmobile robots\\nnavigation\\nneural nets\\nobject detection\\nslam (robots)\\nplace categorization\\nsemantic mapping\\nmobile robot\\nvisual recognition\\nconvolutional network\\nclosed-set limitation\\nlearning\\nsemantic class recognition\\ndomain knowledge\\nclassification system\\nbayesian filter framework\\ntemporal coherence\\nsemantic information\\nrobotic object detection performance\\nrobot behaviour\\nnavigation task\\nros module\",\"687\":\"three-dimensional displays\\nsolid modeling\\ntraining\\nsimulated annealing\\ndatabases\\nsimultaneous localization and mapping\\nlayout\\nimage reconstruction\\nrendering (computer graphics)\\nsolid modelling\\nstatistical analysis\\nscenenet framework\\nindoor scene understanding\\n3d scene annotation\\nhierarchical simulated annealing optimisation\\nstatistics learning\\n3d object database\\nopensurfaces\\narchivetextures\\nmodelnet database\\n3d reconstruction benchmarking\\nsupervised training\\nrendered annotated sequences\",\"688\":\"computational modeling\\nfeature extraction\\ndata models\\nthree-dimensional displays\\ncontext\\ntraining data\\nrobots\\ngeometry\\nobject recognition\\npattern clustering\\npose estimation\\n3d spatial relationships\\nunsupervised fashion\\n3d object poses\\nrelative geometry\\nrelevance-weighted distance\\nransem\\nrobust sample-based clustering\\nvalidation data\\nnyuv2\\nreal-world kinect dataset\\nk-means cluster\\nunsupervised cluster extraction\",\"689\":\"three-dimensional displays\\nshape\\nsolid modeling\\nvehicles\\ndeformable models\\nimage reconstruction\\npipelines\\ncomputer vision\\nimage motion analysis\\nimage representation\\nobject detection\\nroad traffic\\nroad vehicles\\nslam (robots)\\nsolid modelling\\ntraffic engineering computing\\nvideo signal processing\\nmonocular reconstruction\\nslam\\nshape priors\\n3d representation\\nscene understanding\\nroads\\n3d vehicle detection\\nvehicle tracking\\nmonocular video\\ndiverse vehicle skeletal structure\\nmultiple view setting\\nimage detection\\nstructure-from-motion algorithm\",\"690\":\"image color analysis\\nvisualization\\nrobot sensing systems\\ntraining\\nservice robots\\nobject detection\\nimage representation\\nimage sensors\\nindustrial robots\\nlifts\\nrobot vision\\nslam (robots)\\nwarehousing\\nvisual mapping\\nindustrial robot\\ntask-specific approach\\nsaliency driven approach\\nmapping system\\nwarehouse robot\\nvisual representation\\nrgb-d sensor\\nfork-lift robot\",\"691\":\"robots\\nimage color analysis\\nshape\\ncognition\\nknowledge based systems\\nvisualization\\npipelines\\ncontrol engineering computing\\ninference mechanisms\\nmobile robots\\nobject recognition\\nquery processing\\nrobot vision\\nself-adaptive robotic perception\\nautonomous object manipulation\\nmobile robot\\nrobotic agent\\ntask aware robot manipulation\\nrobosherlock\\nperception pipeline\\nquerying\\nknowledge-based reasoning\",\"692\":\"robots\\nnavigation\\nsemantics\\nlayout\\nnatural languages\\ncontext\\nmeasurement\\nimage representation\\nmobile robots\\npath planning\\nrobot vision\\nreal space navigation\\nsemantic descriptions\\nsymbolic language phrases\\nmobile robot\\nsymbolic language description\\nrepresentative map\\nabstract map\\ntopological structure representation\\nspatial layout representation\\nsymbolically defined locations\\ngoal-directed exploration\\nhigh-level semantic plan\\nmetric guidance\\ndoor labels\"},\"Benchmark Setup\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1},\"Experimental Results\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1},\"Code Link\":{\"0\":null,\"1\":null,\"2\":\"'http:\\/\\/www.csc.kth.se\\/~pokorny'\\n\\n'http:\\/\\/www.csc.kth.se\\/icra2016topology\\/'\",\"3\":\"'Planar 6DOF robot arm from our family of 2\\u201310 DOF arms and 30.000 sampled joint-configurations displayed by color coded endpoint positions of the resulting link placements (top left). Bottom left: first persistence diagram (computed in 0.52s) for these samples reveals three holes in the\\\\n(\\\\n\\u03b8\\\\n1\\\\n, \\\\n\\u03b8\\\\n2\\\\n)\\\\nprojection\\\\n\\u03a0\\\\n. The projected samples\\\\nX\\\\ncorrespond to vertices of\\\\nD\\\\nC\\\\n0.1\\\\n(X)\\\\nshown in the top right figure. The three large colored points in the persistence diagram correspond to the colored winding centers in the top right. Bottom right: projections of WA-RRT* trajectories in\\\\n(\\\\n\\u03b8\\\\n1\\\\n, \\\\n\\u03b8\\\\n2\\\\n)\\\\n-coordinates for a 2 DOF arm, illustrating different found homotopy classes. Please see the supplementary video for animations.'\",\"4\":null,\"5\":\"'The main steps for computing the ground truth are pseudo-coded in Algorithm 2. Algorithm 2 needs to maintain a vector of length\\\\n|G|\\\\nfor each state\\\\ns\\u2208S\\\\nto store goal-oriented MDP values for all goals, requiring to run VI\\\\n|G|\\\\nbatches for every state. The\\\\n|R|\\\\nMAVs collectively construct a\\\\n|R|\\u00d7|G|\\\\nmatrix, which can be used for task allocation with time complexity of only\\\\nO(|R\\u2225G\\\\n|\\\\n2\\\\n)\\\\n[13]. The task allocation result then determines a goal-constrained optimal action for each MAV, which drives the MAV to move to future states leading to different goals. Algorithm 2 is repeated after MAV enters a new state, with VI being re-executed every time to dynamically update obstacle information, as each MAV acts as a dynamic obstacle.'\",\"6\":null,\"7\":null,\"8\":null,\"9\":\"'We present several modifications to the previously proposed MSPP algorithm that can speed-up its execution considerably. The MSPP algorithm leverages a multi-scale representation of the environment in n dimensions encoded in tree structure constructed by recursive dyadic partitioning of the search space. We first present a new method to compute the graph neighbors in order to reduce the complexity of each iteration, from O(|V|2) to O(|V| log |V|). We then show how to delay expensive intermediate computations until we know that new information will be required, hence saving time by not operating on information that is never used during the search. Finally, we present a way to remove the very expensive need to calculate a full multi-scale map with the use of sampling and derive an upper bound on the probability of failure as a function of the number of samples.'\\n\\n'The pseudo-code for the vertex selection is given in Function 4 and the edges can be computed as described in Section III-E.'\",\"10\":null,\"11\":null,\"12\":null,\"13\":null,\"14\":\"'http:\\/\\/opencv.org\\/'\\n\\n'http:\\/\\/opencv.org\\/'\",\"15\":null,\"16\":\"'http:\\/\\/www.orin.jp\\/e\\/'\\n\\n'In our system, all of the sensor information is obtained through image processing, and we can introduce computer vision technology such as open source library for extracting various information with sophisticated algorithm, this can be one of the advantages of our system.'\\n\\n'In the proposed sensing, major specifications, such as spatial and temporal resolution, are determined by the specifications of the image sensor and optics. We used SXVGA, 26 fps camera in this study, and these specifications can be chose in accordance with the task. For example, higher speed and higher resolution cameras are available. Sensor information such as contact and proximity is obtained through image processing, and we can introduce computer vision technology such as open source library for extracting various information.'\",\"17\":null,\"18\":\"\\\"We implemented FISTA in Matlab 2015a, utilizing Mat-lab's GPU capabilities to speed up the algorithm's execution. The resulting code required approximately 1ms per iteration (see Table IV). We ran the algorithm for a constant number of iterations for each compressed sensing problem as it standardized the reconstruction time, and checking for other possible exit criteria greatly reduced performance. We performed simulations for several values of\\\\n\\u03bb\\\\n(see (4)) and determined that\\\\n\\u03bb=0.1\\\\nworked well for all scenarios. We used this parameter value in all simulations.\\\"\",\"19\":\"'Sensor signals. A: clear success case. B: marginal success case. C&d: corresponding end-of-trial evaluation photographs. E: top: color legend for the tactile sensors in A & B; bottom: encoder and spool legends.'\",\"20\":null,\"21\":null,\"22\":\"'Fault diagnosis method study in roller bearing based on wavelet transform and stacked auto-encoder'\",\"23\":\"'We present a probabilistic model for joint representation of several sensory modalities and action parameters in a robotic grasping scenario. Our non-linear probabilistic latent variable model encodes relationships between grasp-related parameters, learns the importance of features, and expresses confidence in estimates. The model learns associations between stable and unstable grasps that it experiences during an exploration phase. We demonstrate the applicability of the model for estimating grasp stability, correcting grasps, identifying objects based on tactile imprints and predicting tactile imprints from object-relative gripper poses. We performed experiments on a real platform with both known and novel objects, i.e., objects the robot trained with, and previously unseen objects. Grasp correction had a 75% success rate on known objects, and 73% on new objects. We compared our model to a traditional regression model that succeeded in correcting grasps in only 38% of cases.'\\n\\n'Here, we focus on grasping, in a scenario with multiple sensory modalities, and we investigate a learning approach to encode grasping knowledge acquired from experience. We aim to provide robots with means of reasoning about object grasps and their probability of success, taking into account the information provided by complementary sensory channels. We consider the integration of both visual and haptic cues, as they contribute substantially to grasp control [9].'\\n\\n'Results in this section show that our model can be used to encode grasping knowledge and to solve different grasping-related problems. We focused on grasp correction, i.e., suggesting better grasping poses in the neighborhood of a failed grasp. Another approach to achieve that is to use a data-base and simply select a stable grasp that has previously been executed. Given repeatable conditions such an approach is likely to have a very high success rate, however it cannot generalize beyond the database as it does not naturally take the failed grasp into consideration. Our approach, on the other hand, providing a probabilistic mapping can generate new stable grasps that are not present in the training data through inference conditioned on the failed grasp. In Figure 9 we compare the predictions of our model with examples from the database and we exemplify that our model is capable of generalizing and through its conditional approach generate better corrections compared to a data-base approach. To evaluate generalization capabilities we have also conducted experiments with test objects, which resulted in similar success rates. The resulting grasps indicate that the model has learned what to change, i.e., how to move the hand, from the associated positive and negative examples in the training data, in order to improve an unstable grasp and can apply it on previously unseen objects.'\",\"24\":null,\"25\":null,\"26\":null,\"27\":null,\"28\":null,\"29\":\"'In programming by demonstration, generalization is necessary to apply demonstrated motions in novel situations. Many existing trajectory representations have poor generalization capabilities since they are built on trajectory coordinates that depend on the context in which the motion is recorded. In order to generalize, the user is typically required to perform a large number of varied demonstrations. This paper instead emphasizes the usefulness of an invariant trajectory representation to separate essential motion information from context-specific information of the recorded demonstrations. The invariants are interpreted as the control inputs of a dynamical system describing the evolution of the trajectory. New trajectories are generated for novel situations as the solution of a constrained optimal control problem in which context-specific information of the novel situation is encoded in the constraints. Results indicate how, starting from only a single demonstration, new trajectories can be generated in novel situations while maintaining similarity with the original demonstration. Invariance in trajectory representations therefore proves useful to reduce the number of necessary demonstrations to learn and apply new motions.'\\n\\n'The code for generating trajectories using the invariant representation has been made publicly available [23].'\",\"30\":\"'In a second experiment, a policy was trained to interactively follow a desired hand position in workspace. In contrast to the first experiment, the controlled quantity is not directly observable. Implicitly the policy learns the forward kinematics and estimates the hand position from joint encoders without the need to provide motion capture information for the hand location. The policy learns the best representation automatically given the task and available sensory information. Examples of executing the learned reaching policy in simulation and on the physical system are shown in Fig. 4'\",\"31\":null,\"32\":\"'In order to ensure the real-time performance of control algorithm, node controllers communicate with the main controllers via Controller Area Network (CAN). HUALEX has a total of three kinds of sensors for its current state monitoring. Encoders are integrated in joint actuators, which measures the current state of each joint. An accelerometer is set at the backpack to record the walking velocity of the pilot. Plantar sensors are placed in the sole for measuring the foot pressure.'\",\"33\":null,\"34\":\"'https:\\/\\/am.is.tuebingen.mpg.de\\/publications\\/marco_icra_2016'\",\"35\":null,\"36\":null,\"37\":null,\"38\":null,\"39\":null,\"40\":null,\"41\":null,\"42\":null,\"43\":null,\"44\":null,\"45\":null,\"46\":\"'We demonstrated learning-based control of a complex, high-dimensional, pneumatically-driven hand. Our results show simple tasks, such as reaching a target pose, as well as dynamic manipulation behaviors that involve repositioning a freely-moving cylindrical object. Aside from the high-level objective encoded in the cost function, the learning algorithm does not use domain knowledge about the task or the hardware, learning a low-level valve control strategy for the pneumatic actuators entirely from scratch. The experiments show that effective manipulation strategies can be automatically discovered in this way.'\",\"47\":null,\"48\":\"'http:\\/\\/www.labex-action.fr\\/'\",\"49\":null,\"50\":\"'http:\\/\\/www.ghmm.org\\/'\",\"51\":null,\"52\":\"'http:\\/\\/goo.gl\\/cY19sq'\\n\\n'http:\\/\\/goo.gl\\/cY19sq'\",\"53\":\"'https:\\/\\/youtu.be\\/uitfri52emi'\\n\\n'https:\\/\\/youtu.be\\/UitFri52EMI'\\n\\n'https:\\/\\/youtu.be\\/UitFri52EMI'\\n\\n'https:\\/\\/youtu.be\\/uitfri52emi'\\n\\n'https:\\/\\/youtu.be\\/uitfri52emi'\",\"54\":null,\"55\":\"'However, this algorithm has two main limitations: 1) it is not guaranteed to find a solution when there exists one, and 2) instantiations are sampled from object-specific, hand-coded distributions. Since the algorithm never reduces the set of possible sampled values, its efficiency degrades as the number of symbolic references increases. In the next subsection we address the first limitation; in the following sections we address the second.'\\n\\n'We encode geometric features of a current plan refinement in a feature vector\\\\nf(u)\\\\n. We train separate weights to estimate the utility of each mode. We do this by stacking two copies of the features and using an indicator to zero out the top or bottom half:\\\\nf((u,m))=\\\\n[\\\\nf(u)\\\\nf(u)\\\\n]\\\\n\\u22a4\\\\n[\\\\n1\\u2212m\\\\nm\\\\n]\\\\n.'\\n\\n'The results demonstrate comparable performance to the baseline systems. A simple hand-coded discretization works well in this domain, and the lack of long-term dependencies in the plan means that backtracking search succeeds quickly. This shows that our system recovers the performance of well-chosen discretizations, when they exist. Figure 1 shows learned tray pickup poses after all 20 iterations.'\\n\\n'A major limitation of our system is the hand-coded features. In future work, we plan to move toward learned features, which offer more complex policy classes. Along the same lines, we hope to consider features of the logical structure of the high-level plan, perhaps using a kernelized method that applies to strings. We also plan to experiment with more sample-efficient RL algorithms.'\",\"56\":\"'The code is written in C++, and runs in Ubuntu 14.04 with ROS Indigo. The simulator has a scene with a youBot, equipped with an arm and a kinect camera, and an object to be examined. From V-REP the object location, expressed in the world frame, is obtained. Odometry data and joint states are provided as normal ROS topics. For repeatability, input is generated by given functions of time, but could easily be provided by user commands from a gamepad. Gurobi is used for solving the optimization problem.'\",\"57\":\"'http:\\/\\/berkeleyautomation.github.io\\/shiv'\\n\\n'We look forward to performing experiments with SHIV in other domains and sharing code and ideas with potential collaborators. One challenge is setting the risk sensitivity parameter as illustrated in Table I. When\\\\n\\u03c3=1\\\\n, SHIV is equivalent to DAgger with high burden on the supervisor. In the driving domain we observe a range of values where the performance \\/ burden tradeoff is nearly constant, suggesting that this parameter could be varied during learning to increase as performance improves with an outer loop designed to tune this parameter.'\\n\\n'http:\\/\\/berkeleyautomation.github.io\\/shiv'\",\"58\":\"'A kernel function is undoubtedly the most crucial ingredient in Gaussian process regression, as it encodes our assumptions about the function which we wish to learn. Intuitively speaking, a kernel function can be interpreted as a non-negative measure defined in the input space which estimates how correlated two function values at different inputs are.'\",\"59\":\"'Preference appraisal reinforcement learning (pearl) framework for learning and executing PBT. The user-provided preferences are encoded into polymorphic features. The learning agent appraises preference priorities on a low-dimensional training problem. The planner takes an initial state of a high-dimensional problem and produces actions in a closed feedback loop.'\",\"60\":\"'http:\\/\\/tiny.cc\\/icra16_video'\\n\\n'http:\\/\\/tiny.cc\\/icra16_video'\\n\\n'In this paper, we encode the safety criterion as a performance threshold,\\\\nJ\\\\nmin\\\\n, below which we do not want to fall; that is,\\\\nJ(\\\\na\\\\nn\\\\n)\\u2265\\\\nJ\\\\nmin\\\\nmust hold with high probability for all\\\\na\\\\nn\\\\nat which\\\\nJ\\\\nis evaluated. With this definition of safety, the resulting controllers are likely to be stable, since unstable systems typically have a significantly lower performance when considering a sufficiently long time horizon.'\",\"61\":\"'Autonomous representations model a movement as a dynamical system; encoding a time-independent relation between the dynamical features of the movement (e.g. position, velocity and acceleration). Such systems form an attractor landscape in which the goal state is a global minimum. Reproduction of the encoded skill is achieved by following the steepest descent of the attractor landscape [2], [3].'\\n\\n'Non-autonomous movement representations encode an explicit dependency between a temporal signal and the dynamical features of the movement. The retrieval of the movement from the model is driven by this temporal signal that can represent time directly, or indirectly using a decay term (see e.g. [4]\\u2013[6]). We call these systems non-autonomous since the system evolution depends on a variable that is not part of the system state.'\\n\\n'Reproduction of the encoded skill is achieved online. At each time-step a control command is obtained by solving an optimal control problem (II-B). The required objective function is constructed based on the information encoded in the HSMM as desecribed in Sections II-C and II-D.'\\n\\n'The demonstrated data are encoded in an HSMM [10], [11], an extension of the Hidden Markov Model (HMM) in which the state1. duration is explicitly modeled as a probability distribution. An HMM models a double stochastic process of which the observations are assumed to be generated by an underlying, unobservable, finite-state Markov chain. We represent each state by a single multivariate Gaussian\\\\nN(\\\\n\\u03bc\\\\ni\\\\n, \\\\n\\u03a3\\\\ni\\\\n)\\\\n, with'\\n\\n'In this work, however, we do not encode the joint probability density function\\\\nP(\\u03be, t)\\\\n. Instead, we use an HSMM encoding\\\\nP(\\u03be)\\\\nin combination with a duration model that replaces the explicit temporal signal. The reference\\\\n\\u03b6\\\\n^\\\\n, previously obtained through Gaussian Mixture Regression (GMR), is replaced by a trajectory based on the state sequence\\\\ns={\\\\ns\\\\n1\\\\n, \\u2026, \\\\ns\\\\nN\\\\np\\\\n}\\\\nthat is synthesized as described in Section II-C. The result is a piecewise reference consisting of the centers and covariance of the Gaussian kernels, i.e.'\\n\\n'There is a clear difference between the online and offline HSMM. The online HSMM is able to spatially reproduce the encoded motion, but requires more time for the reproduction. The offline method reaches the final state within the demonstrated time window, but is not able to match the demonstration spatially. Offline and online state sequence synthesis is the same when the duration model has a variance that approaches zero for all states. The difference between these two reproductions demonstrates the potential of HSMM to encode different responses to temporal perturbation.'\\n\\n'The time-based GMM is able to reproduce the encoded motion with spatial and temporal accuracy. There is, however, a clear difference in magnitude of the control commands (shown in the bottom right plot of Figure 3). The time-based GMM shows much higher interaction forces compared to HSMM-based reproductions.'\\n\\n'Visualization of the encoded TP-hsmm. (a) The experimental setup and the provided demonstrations. (b) States transition model with their corresponding duration. (c) And (d) display the demonstration data (in gray), and the location of the gaussian kernels in the two frames of reference. The colored ellipsoids represent the gaussian kernels. Their colors correspond to the states of the hsmm. The arrow that originates from the center of a gaussian indicates the mean of the encoded velocity.'\",\"62\":null,\"63\":\"'Deep spatial autoencoders for visuomotor learning'\\n\\n\\\"Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.\\\"\\n\\n'Our main contribution is a method for learning vision-based manipulation skills by combining unsupervised learning using deep spatial autoencoders with simple, sample-efficient trajectory-centric reinforcement learning. We demonstrate that this approach can be used to learn a variety of manipulation skills that require \\u201chand-eye\\u201d coordination, including pushing a free-standing toy block, scooping objects into a bowl, using a spatula to lift a bag of rice from the table (shown in Figure 1), and hanging a loop of rope on hooks at various positions. Each of these tasks are learned using the same algorithm, with no prior knowledge about the objects in the scene, and using only the onboard sensors of the PR2 robot, which consist of joint encoders and an RGB camera. We also compare our approach to a number of baselines that are representative of previously proposed visual state space learning methods.'\\n\\n'Linear-Gaussian controllers are easy to train, but they cannot express all possible control strategies, since they essentially encode a trajectory-following controller. To extend the applicability of linear-Gaussian controllers to more complex tasks, prior work has proposed to combine it with guided policy search [7], [8], [23], which is an algorithm that trains more complex policies, such as deep neural networks, by using supervised learning. The supervision is generated by running a simpler algorithm, such as the one in the previous section, from multiple initial states, generating multiple solutions. In our experimental evaluation, we combine linear-Gaussian controllers with guided policy search to learn a policy for hanging a loop of rope on a hook at different positions. A different linear-Gaussian controller is trained for each position, and then a single neural network policy is trained to unify these controllers into a single policy that can generalize to new hook positions. A full description of guided policy search is outside the scope of this paper, and we refer the reader to previous work for details [7], [8], [23].'\\n\\n'RL with deep spatial autoencoders. We begin by training a controller without vision to collect images (left) that are used to learn a visual representation (center), which is then used to learn a controller with vision (right). The learned state representation corresponds to spatial feature points (bottom).'\\n\\n'In the second stage, the initial controller is used to collect a dataset of images, and these images are then used to train our deep spatial autoencoder with unsupervised learning, as described in Section V. Once this autoencoder is trained, the encoder portion of the network can be used to produce a vector of feature points for each image that concisely describes the configuration of objects in the scene. The final state space is formed by concatenating the joint angles, end-effector positions, and feature points, and also including their velocities to allow for dynamic tasks to be learned. We must also define a cost function using this new state space, which we do by presenting an image of the target state to the feature encoder and determining the corresponding state.'\\n\\n\\\"In the third stage, a vision-based controller is trained using the new state space that now contains visual features from the encoder, with the new cost function defined in terms of the visual features and the robot's configuration. This controller is trained using the same trajectory-centric reinforcement learning algorithm as in the first stage, and is able to perform tasks that require controlling objects in the world that are identified and localized using vision.\\\"\\n\\n'A. Deep Spatial Autoencoders'\\n\\n\\\"Autoencoders acquire features by learning to map their input to itself, with some mechanism to prevent trivial solutions, such as the identity function. These mechanisms might include sparsity or a low-dimensional bottleneck. Our autoencoder architecture, shown in Figure 3, maps from full-resolution RGB images to a down-sampled, grayscale version of the input image, and we force all information in the image to pass through a bottleneck of spatial features, which we describe below. Since low-dimensional, dense vectors are generally well-suited for control, a low-dimensional bottleneck is a natural choice for our learned feature representation. A critical distinction in our approach is to modify this bottleneck so that it is forced to encode spatial feature locations rather than feature values \\u2014 the \\u201cwhere\\u201d rather than the \\u201cwhat.\\u201d This architecture makes sense for a robot state representation, as it forces the network to learn object positions; we show in Section VII, that it outperforms more standard architectures that focus on the \\u201cwhat,\\u201d both for image reconstruction, and for robotic control. The final state space for RL is then formed by concatenating this learned encoding, as well as its time derivatives (the feature \\u201cvelocities\\u201d), with the robot's configuration.\\\"\\n\\n'The first part of the encoder architecture is a standard 3-layer convolutional neural network with rectified linear units of the form\\\\na\\\\ncij\\\\n=max(0, \\\\nz\\\\ncij\\\\n)\\\\nfor each channel\\\\nc\\\\nand pixel\\\\n(i,j)\\\\n. We compute the spatial features from the last convolutional by performing a \\u201cspatial soft arg-max\\u201d operation that determines the image-space point of maximal activation in each channel of conv3. This set of maximal activation points forms our spatial feature representation and forces the autoencoder to focus on object positions. The spatial soft arg-max consists of two operations. The response maps of the third convolutional layer (conv3) are first passed through a spatial softmax\\\\ns\\\\ncij\\\\n=\\\\ne\\\\na\\\\ncij\\\\n\\/\\u03b1\\\\n\\/\\\\n\\u2211\\\\ni\\\\n\\u2032\\\\nj\\\\n\\u2032\\\\ne\\\\na\\\\nc\\\\ni\\\\n\\u2032\\\\nj\\\\n\\u2032\\\\n\\/\\u03b1\\\\n, where the temperature\\\\n\\u03b1\\\\nis a learned parameter. Then, the expected 2D position of each softmax probability distribution\\\\ns\\\\nc\\\\nis computed according to\\\\nf\\\\nc\\\\n=(\\\\n\\u2211\\\\ni\\\\ni\\u2217\\\\ns\\\\ncij\\\\n, \\\\n\\u2211\\\\nj\\\\nj\\u2217\\\\ns\\\\ncij\\\\n)\\\\n, which forms the bottleneck of the autoencoder. This pair of operations typically outputs the image-space positions of the points of maximal activation for each channel of conv3. Finally, the decoder\\\\nh\\\\ndec\\\\nis simply a single linear (fully connected) mapping from the feature points\\\\nf\\\\nto the down-sampled image. We found that this simple decoder was sufficient to acquire a suitable feature representation.'\\n\\n'The architecture for our deep spatial autoencoder. The last convolutional layer is passed through a spatial softmax, followed by an expectation operator that extracts the positions of the points of maximal activation in each channel. A downsampled version of the image is then reconstructed from these feature points.'\\n\\n'We optimize the auto encoder using stochastic gradient descent (SGD), and with batch normalization [36] following each of the convolutional operations. The filters of the first convolutional layer are initialized with a network trained on ImageNet [37], [29].'\\n\\n'Not all of the learned feature points will encode useful information that can be adequately modeled with a time-varying linear dynamical system by our RL algorithm. Lighting and camera exposure can cause sudden changes in the image that do not reflect motion of relevant objects. However, the autoencoder will assign some subset of its features to model these phenomena to improve its reconstruction, producing noisy feature points that make it difficult to train the controller. Additionally, even useful features can periodically become occluded, resulting in low activations in the last convolutional layer and unpredictable peak locations.'\\n\\n'RL with Deep Spatial Autoencoders'\\n\\n'We evaluated our method on a range of robotic manipulation tasks, ranging from pushing a lego block to scooping a bag of rice into a bowl. The aim of these experiments was to determine whether our method could learn behaviors that required tracking objects in the world that could only be perceived with vision. To that end, we compared controllers learned by our approach to controllers that did not use vision, instead optimizing for a goal end-effector position. We also compared representations learned with our spatial autoen-coder to hidden state representations learned by alternative architectures, including ones proposed in previous work.'\\n\\n'We also evaluate two alternative autoencoder architectures that are representative of prior work, trained with the same training and validation datasets as our model. The first comparison closely mirrors the method of Lange et al. [1], but with a bottleneck dimensionality of 10 to account for the greater complexity of our system. The second comparison reflects a more recent architectures, using max-pooling to reduce the dimensionality of the image maps and batch normalization after the convolutional layers. The bottleneck for this architecture is 32, matching that of our architecture. Details of both baseline architectures are provided in Appendix B of the supplementary materials2. We evaluated both architectures with and without a smoothness penalty. The results, shown in Table IV, show that these methods struggle with our high-dimensional, real-world tasks, despite the larger model achieving a lower reconstruction loss than our autoencoder. For several of the models, we could not obtain a stable controller, while the others did not effectively integrate visual input into the control strategy. Table IV also shows the performance of our method without the smoothness penalty, and without feature pruning, showing that both of these components are critical for obtaining good results.'\\n\\n'Table IV: Comparisons to prior autoencoder architectures and variants of our method'\\n\\n'Another advantage of our approach is its sample efficiency, which is enabled both by the use of simple linear-Gaussian controllers and a data-efficient neural network architecture. The autoencoders used around 50 trials for each task, with each trial consisting of 100 image frames and 5 second of interaction time, for a total of 5000 frames per task. Training the final vision-based controller required another 50\\u201375 trials, which means that each controller was trained with around 10\\u201315 minutes of total robot interaction time.'\\n\\n'We presented a method for learning state representations using deep spatial autoencoders, and we showed how this method could be used to learn a range of manipulation skills that require close coordination between perception and action. Our method uses a spatial feature representation of the environment, which is learned as a bottleneck layer in an autoencoder. This allows us to learn a compact state from high-dimensional real-world images. Furthermore, since this representation corresponds to image-space coordinates of objects in the scene, it is particularly well suited for continuous control. The trajectory-centric RL algorithm we employ can learn a variety of manipulation skills with these spatial representations using only tens of trials on the real robot.2'\",\"64\":null,\"65\":null,\"66\":\"'We release our code in order to make our results easily replicable4. Additionally, by releasing our models, we hope our haptic classifiers can be integrated into current robotic pipelines that could benefit from better tactile understanding of real objects and surfaces.'\",\"67\":\"'For implementing the proposed algorithm we used two existing open source libraries. For the RRLS learning part we used GURLS [30], a regression and classification library based on the Regularized Least Squares (RLS) algorithm, available for Matlab and C++. For the computations of the regressors\\\\n\\u03a6(q,\\\\nq\\\\n\\u02d9\\\\n,\\\\nq\\\\n\\u00a8\\\\n)\\\\nwe used iDynTree2 (see [31]), a C++ dynamics library designed for free floating robots. Using SWIG [32], iDynTree supports calling its algorithms in several programming languages, such as Python, Lua and Matlab. For producing the presented results, we used the Matlab interfaces of iDynTree and GURLS.'\",\"68\":null,\"69\":\"'The proposed method is part of a larger ROS-based open source package that will be released under the BSD-license. The package allows to perform 2D tracking of unknown object with monocular cameras [23] and 3D tracking with RGBD camera. We also provide code to use different cameras with the tracker and detailed instructions how to use it.'\",\"70\":null,\"71\":null,\"72\":null,\"73\":null,\"74\":\"'https:\\/\\/www.youtube.com\\/watch?v=mh9EIz-Tdxg'\\n\\n'For low-level sensors such as joint encoders, inertial measurement units and torque sensors, estimation is typically performed using Gaussian filters (GF) [10], [20]. The most well known members of the family of GFs are the Extended Kalman Filter (EKF) [22] and the Unscented Kalman Filter (UKF) [11]'\",\"75\":\"'To achieve accurate vision-based control with a robotic arm, a good hand-eye coordination is required. However, knowing the current configuration of the arm can be very difficult due to noisy readings from joint encoders or an inaccurate hand-eye calibration. We propose an approach for robot arm pose estimation that uses depth images of the arm as input to directly estimate angular joint positions. This is a frame-by-frame method which does not rely on good initialisation of the solution from the previous frames or knowledge from the joint encoders. For estimation, we employ a random regression forest which is trained on synthetically generated data. We compare different training objectives of the forest and also analyse the influence of prior segmentation of the arms on accuracy. We show that this approach improves previous work both in terms of computational complexity and accuracy. Despite being trained on synthetic data only, we demonstrate that the estimation also works on real depth images.'\\n\\n\\\"For autonomous, robotic grasping and manipulation, knowing the current pose of the robot's manipulator is important to achieve a good hand-eye coordination. Given the kinematics of the arm and the current joint angles, the pose of each link relative to the camera can be computed. However, getting good estimates for these angles can be difficult as position encoders may have considerable inaccuracies depending on the robot. An example for this is the ARM robot [1], which has encoders in the motors which are located in the shoulders but not directly at the joints. Estimation of the joint positions is thus prone to inaccuracies due to variable cable stretch (see Fig. 1). Therefore, additional techniques for improving the estimated arm pose are necessary.\\\"\\n\\n'This paper presents an approach of frame-by-frame joint angle estimation using depth images as input. It does not require initialization of the solution from e.g. previous frames or joint encoder readings. It is therefore also not prone to an bad initial guess and can immediately recover when losing track of the arm in some frames. Estimation is done with a Random Forest (RF) which is trained on synthetically rendered depth images from which large annotated training sets can be easily built. We analyse the influence of different training objectives on the accuracy of the RF. Apart from the standard mean squared error (MSE) training objective, we also use the specialized DISP distance [2]. This is a model-dependent metric for rigid and articulated body displacement that measures distance in configuration space. Moreover, we show that further improvement can be gained by using a second RF for prior foreground segmentation of the images.'\\n\\n'Visualization of the encoder error. The blue outline shows the estimated pose of the arm based on the encoder readings. The considerable error between estimated and real arm pose makes any fine manipulation task very challenging. The error increases if there is an additional load applied at the end-effector.'\\n\\n'Estimations on real depth images. The upper row shows the depth images, the lower one visualizes in grey the robot pose estimated from encoder readings, in orange the point cloud obtained from the depth images and in blue the estimate of our method for the right arm.'\\n\\n'Pose estimates based on motor encoders of our robot are quite inaccurate. This can be observed by the gap between the real position of the hand as perceived by the camera (orange point cloud) and the estimated pose based on the encoders (grey model). In the first two examples the estimates of our method (blue model) look better than the ones based on the encoders. In the third one it is far off from the real arm pose. This can be explained by the pose which is far away from the poses contained in the training set. This can be mitigated by using a larger training set that better covers the configuration space of the arm.'\",\"76\":null,\"77\":\"'The simulation can also track changes in string lengths similar to how the encoders in the physical prototype measure change in string length. This common feature means that control policies developed in simulation are more easily portable to the physical prototype.'\\n\\n\\\"To simulate our tensegrity joint, we used NASA's Tenseg-rity Robotics Toolkit (NTRT). NTRT is an open-source simulator for the design and control of tensegrity structures and robots which has been built ontop of the Bullet Physics Engine (version 2.82)1 Real-time video can also be recorded with NTRT.\\\"\",\"78\":null,\"79\":null,\"80\":null,\"81\":\"'CAD model of the hopping mechanism. A prismatic joint is realized using two shaft-bearing pairs. Compression springs coil around each shaft and act in parallel to the voice coil. An incremental encoder measures the relative displacement (\\u201cstroke\\u201d) of the coil and body.'\\n\\n'Since we aim to use our hopping mechanism on an untethered robot, we implemented our controller using embedded electronics, and used lightweight (approx. 210 g total) Lithium-polymer batteries to power our logic circuit and voice coil driver. Our control circuit consists of a microcontroller (Parallax Propeller P8\\u00d732A), a voice coil voltage driver (Moticont 800 series), a current sensor (Allegro ACS712) to estimate voice coil force, and an ADC chip (Texas Instruments ADS1015) to read the current sensor. A linear incremental optical encoder (US Digital EM1-0-120-N) and a rotary incremental optical encoder (US Digital E2-32-250-NE-H-D-B) give us full state estimation, and are read directly from the microcontroller. The sensor and control loop run at 1 kHz, while data is output to a desktop computer at approximately 850 Hz (i.e. as fast as possible over serial connection).'\\n\\n'CAD model of experimental setup. An encoder measures the height of the hopping mechanism, which is constrained to a vertical rail. Mass can be added to the system in measured quantities.'\",\"82\":null,\"83\":\"'The +SPEA presented in this paper consists of 4 identical motor units in parallel. Each motor unit consists of: an encoder, a brake, a motor, a gearbox, a last gear stage (i.e. a bevel gear), and a spiral spring which is then finally connected to the output shaft. In this Section III the component selection will be first discussed, followed by the actuator overview and specifications. Finally, the test set-up used in this paper is described.'\\n\\n'Encoder HEDL (ref. 110512): 500 CPT.'\\n\\n'The signals from all the sensors and those provided by the motor drives are captured by a real-time data-acquisition (DAQ) system implemented with multifunction input\\/output boards (PCI-6602 for the encoder readings and PCI-6229 for the analog input-outputs and digital outputs, National Instruments). The DAQ boards are installed on a PC\\\\n(\\\\nCore\\\\nTM\\\\n2\\\\nCPU 6600 at 2.40 GHz, Intel) running Real-Time Windows\\\\nTarget\\\\n\\u00ae\\\\nand\\\\nSimulink\\\\n\\u00ae\\\\n. This system also allows the implementation of conventional proportional controllers for the angular position of the motors and off-line data processing. The motors were controlled by the commercial drives using external reference inputs also generated by the DAQ system. The full system was powered by regulated industrial power supplies (CP-E 24.0 V\\/20 A, CP-E 48.0 V\\/10 A, ABB).'\",\"84\":null,\"85\":null,\"86\":null,\"87\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/33614192'\\n\\n'Plenoptic images. (a) Raw image of micro lens array (mla) showing a surgical tool and suture pad, (b) a magnified view showing the details of MLA images, (c) processed extended depth-of-field image, and (d) virtual depth image color-coded according to depth, with red areas being closer to the camera.'\",\"88\":\"'Hubot is a framework allowing a surgeon to work in collaboration with a robot. During a surgical procedure, some motions are more critical than others as they have more potential to cause harm. A tool-tissue or tool-tool interaction, for instance, is critical and therefore should be performed by the surgeon while other types of motions could be automated. Hubot runs in real time and allows for seamless and automatic transitions between three different modes, depending on the current step of the surgical procedure: automated, manual with haptic guidance and finally fully manual. Hubot was implemented on a Raven II surgical robot (Applied Dexterity, Seattle, WA) augmented with a custom absolute controller (available as an open source project) and teleoperated with two Geomagic Touch haptic devices (Geomagic, Morrisville, NC). Figure 1 presents the general workflow of the Hubot collaborative framework, arranged in two main phases: the learning phase and the execution phase.'\\n\\n'During the study, the participants used two Geomagic Touch devices to control the Raven II robot. A custom handle was designed to replace the standard Geomagic stylus. This handle, called the HIG-Touch (Handheld Intuitive Grasper for Geomagic Touch) mimics the grasper movements resulting in a more intuitive control of the robot (available as an open source project). A set of three foot pedals was also provided. The users had no direct view of the Raven and the task scene. Instead, a 3D endoscope was installed (Olympus, Tokyo, Japan), and the participants were provided with 3D glasses and a 3D screen showing the robot arms and the surgical scene.'\",\"89\":null,\"90\":null,\"91\":null,\"92\":null,\"93\":null,\"94\":null,\"95\":\"'This paper proposes a simple yet effective approach to learn visual features online for improving loop-closure detection and place recognition, based on bag-of-words frameworks. The approach learns a codeword in the bag-of-words model from a pair of matched features from two consecutive frames, such that the codeword has temporally-derived perspective invariance to camera motion. The learning algorithm is efficient: the binary descriptor is generated from the mean image patch, and the mask is learned based on discriminative projection by minimizing the intra-class distances among the learned feature and the two original features. A codeword is generated by packaging the learned descriptor and mask, with a masked Hamming distance defined to measure the distance between two codewords. The geometric properties of the learned codewords are then mathematically justified. In addition, hypothesis constraints are imposed through temporal consistency in matched codewords, which improves precision. The approach, integrated in an incremental bag-of-words system, is validated on multiple benchmark data sets and compared to state-of-the-art methods. Experiments demonstrate improved precision\\/recall outperforming state of the art with little loss in runtime.'\\n\\n'To compensate for perspective effects, visual loop-closure systems often require a similar trajectory profile to trigger a loop-closure. The loop-closing image sequence views the revisited scene from a similar perspective. The key idea in this paper is to learn the codewords by learning feature descriptors invariant to the perspective transformations induced by robot motion. With such codewords, visual features of the same object subject to perspective distortions are more likely to trigger loop-closure hypotheses and improve recall. We use binary features due to their overall advantages demonstrated in loop-closure applications, efficient computation with high precision-recall (PR) [11], [12].'\\n\\n'an efficient algorithm based on LDA for learning codewords invariant to perspective transformations from robot motion, involving only matrix additions on image patches and bit-wise operations on binary vectors;'\\n\\n'theoretical justification for the geometric properties in the learned codeword, demonstrating that the learned codewords can be interpreted to be \\u201ccentroids\\u201d in the space induced by the modified Hamming distance;'\\n\\n'The algorithm defining the mean codeword is summarized in Algorithm 1. Fig. 2 depicts two binary tests with\\\\n{\\\\nI\\\\nm\\\\n, \\\\nI\\\\n1\\\\n, \\\\nI\\\\n2\\\\n}\\\\n. For non-zero variance in the same test across three patches, the corresponding coordinate in\\\\nx\\\\nm\\\\nwill be masked out through\\\\ny\\\\nm\\\\n, as illustrated in Fig. 3.'\\n\\n'Relative to the source codewords\\\\nD\\\\n1\\\\n={\\\\nx\\\\n1\\\\n, \\\\ny\\\\n1\\\\n}\\\\nand\\\\nD\\\\n2\\\\n={\\\\nx\\\\n2\\\\n, \\\\ny\\\\n2\\\\n}\\\\nboth in\\\\nD\\\\nMH\\\\n, the learned mean codewords have the following two nice geometric properties in the space\\\\nD\\\\nMH\\\\ninduced by\\\\nd\\\\nMH\\\\n(\\\\nnote that\\\\n\\u2200\\\\nD\\\\nk\\\\n\\u2208\\\\nD\\\\nMH\\\\n,|\\\\ny\\\\nk\\\\n|\\u22600)\\\\n:'\\n\\n'First consider codewords with only one bit. There are two cases:'\\n\\n'Table I Distances of codewords th one bit (case #1). For simplicity we use to denote\\\\nd\\\\nMH\\\\n(i,j)\\\\nd\\\\nMH\\\\n(\\\\nD\\\\ni\\\\n, \\\\nD\\\\nj\\\\n)'\\n\\n'Table II Distances of codewords with one bit (case #2).'\\n\\n'Here we discuss a technique for improving the detection precision, which comes naturally from the codeword learning process. Fig. 5 illustrates a loop-closure happening around frame\\\\np\\\\n, which revisits the same place captured previously around frame\\\\nk\\\\nin the sequence. A loop-closure hypothesis closing frame\\\\np+1\\\\nwith frame\\\\nk+1\\\\nis triggered in the bag-of-words framework, because these two frames share plenty of matched codewords. Assume two codewords\\\\nD\\\\n(k+1)\\\\nm\\\\nand\\\\nD\\\\n(p+1)\\\\nm\\\\nare matched due to the fact that (at least) features\\\\nf\\\\nk\\\\nand\\\\nf\\\\np\\\\nare strongly matched with each other.'\\n\\n'The initial feature matching is based on raw binary descriptors extracted from visual keypoints (FAST is used). The raw binary descriptors are from the same binary tests which are used from the codeword learning.'\\n\\n'The codeword generation is based on Algorithm 1.'\\n\\n'A codeword merging step is performed when the learned codewords are used to update the vocabulary. This step will iterate through all the codewords generated for the current frame and find the codeword pairs within matching thresholds. These pairs will then be merged according to Algorithm 1 but treating the two codewords to merge as\\\\nD\\\\n1\\\\nand\\\\nD\\\\n2\\\\n. Contrast this merge to [12] which takes the \\u201cnumerical centroid\\u201d. For two binary vectors, the \\u201cnumerical centroid\\u201d is equivalent to the bit-wise OR operation.'\\n\\n'We collected the timing statistics for learning a codeword as in Algorithm 1 from CityCentre experiments. The timing result under normal system process priority is listed in Table VI, and indicates a high efficiency and stability. In our implementation, the bit-wise operations are handled using C++ std::transform function with bit operation structure (e.g. std:: bit_xor) for uchar. The number of 1s in a binary vector is counted directly using a look-up table indexed by uchar values.'\\n\\n'This work describes a method to learn binary codewords online for loop-closure detection. The codewords are learned efficiently in an LDA fashion from matched feature pairs in two consecutive frames, such that the learned codewords encode temporal perspective invariance from the observed motion dynamics. The geometric properties of the learned codewords are mathematically justified. The temporal consistency from the nature of learned codewords is further exploited to cull loop-closure hypotheses. The incremental system is evaluated with precision\\/recall and timing results to quantify the effectiveness and efficiency of the approach.'\\n\\n'Table VI Time (in 10\\u22126 sec.) used for learning one codeword using algorithm 1.'\\n\\n'Learning Binary Codeword Invariant to Frame-By-Frame Motion Dynamics'\\n\\n'Algorithm 1: Learning Codewords from Motion Dynamics'\\n\\n'2) Learning Codewords Invariant to Cross-Frame Motion'\\n\\n'Codeword learning from the perspective of bit-wise operations.'\\n\\n'C. Geometric Properties of Learned Codewords'\",\"96\":\"'https:\\/\\/github.com\\/stephenphillips42\\/erl_egomotion'\\n\\n'Our code is publicly available at https:\\/\\/github.com\\/stephenphillips42\\/erl_egomotion.'\\n\\n'https:\\/\\/github.com\\/stephenphillips42\\/erl_egomotion'\",\"97\":null,\"98\":null,\"99\":null,\"100\":\"'http:\\/\\/rpg.ifi.uzh.ch\\/fov.html'\",\"101\":null,\"102\":\"'We use the scale matrix to encode our prior estimate of the covariance, and the degrees of freedom to encode our confidence in that estimate. Specifically, if we estimate the covariance\\\\nR\\\\nassociated with predictor\\\\n\\u03d5\\\\nto be\\\\nR\\\\n^\\\\nwith a confidence equivalent to seeing\\\\nn\\\\nindependent samples of the error from\\\\nN(0,\\\\nR\\\\n^\\\\n)\\\\n, we would choose\\\\n\\u03bd(\\u03d5)=n\\\\nand\\\\n\\u03a8(\\u03d5)=n\\\\nR\\\\n^\\\\n.'\\n\\n'We implemented PROBE-GK using a combination of MATLAB and C++. We used the open-source library LIBVIS02 [10] for feature extraction and matching. We implemented our own Levenberg-Marquardt optimization routine, and used a custom C++ library to maintain the covariance model and perform inference.'\",\"103\":null,\"104\":null,\"105\":null,\"106\":null,\"107\":null,\"108\":null,\"109\":null,\"110\":null,\"111\":null,\"112\":null,\"113\":\"'Example sequence (from the top left to the bottom right) of follow-the-leader motion of the extensible continuum robot (color coded sections) along a general 3D path composed of sections with constant curvature. Projections of the 3D paths are shown in grey on the coordinate planes.'\",\"114\":null,\"115\":null,\"116\":null,\"117\":\"'This paper introduces a novel asynchronous adaptive brain machine interface (BMI), based on a dry-wireless headset, to trigger the movement of a lower limb exoskeleton robot by foot motor imagery. Specifically, it addresses two issues that are critical for the development of a plug-and-play brain robot interface (BRI): setup-time and the nonstationarity of the electroencephalogram (EEG). The former is solved by a dry-wireless headset that reduces setup-time compared to gel-based systems, and removes the nuisance of cables. The latter has been extensively studied in the literature, leading to effective adaptive algorithms in synchronous BMI. However, asynchronous BMI has received little attention. We propose an extension of state-of-the-art adaptive methods by defining the forgetting factors according to the time constant of the exponential moving average. In addition, we propose feature adaptation as opposed to the standard bias adaptation of a linear classifier. After calibrating the decoder, the subject with a reliable classification of sensorimotor rhythms was asked to trigger robot squatting. The motion was successfully initialized by foot motor imagery; with an essential contribution of the proposed adaptive BMI, which makes features less prone to nonstationarities and improves classification performance compared to standard adaptive methods. The ultimate goal of our research is to develop a plug-and-play co-adaptive BRI for neuromotor rehabilitation.'\\n\\n'We apply a novel adaptation approach for ERD-based BMI, in order to decode foot motor imagery from the highly nonstationary EEG signal, during asynchronous operation. Asynchronous systems continuously analyze the EEG signal, during both rest and motor imagery, allowing the user to perform self-paced decisions. Such design is more complex [20], since it has to handle an additional highly variable non-control state (i.e. rest condition). The proposed adaptation technique is inspired by Wojcikiewicz et al. [21], where they combine adaptive spatial filters [22] with an adaptive Linear Discriminant Analysis (LDA) classifier (i.e. PMean [13]). These studies proposed effective methods for adaptation in synchronous BMI, where the model is adapted trial by trial. However, the adaptation parameters (i.e. forgetting factors) used in synchronous BMI are not suitable for asynchronous BMI, where adaptation should be done frame by frame, with a sliding window approach. Therefore, in our study we propose a quantitative way to define adaptation parameters for asynchronous BMI. Moreover, we propose an alternative approach to the PMean algorithm. In PMean, the bias term of a Linear Discriminant Analysis (LDA) classifier is adapted based on new incoming data. On the other hand, our algorithm leaves the classifier bias unchanged, and directly adapts the mean of incoming features by subtracting their moving average. Additionally, the adapted features are low-pass filtered in order to reduce the influence of artifacts. Feature mean adaptation, as opposed to classifier bias adaptation, allows us to use alternative classifiers adaptively without additional effort. Moreover, it is useful for consistent feature visualization regardless of nonstationarities, or for directly providing the adapted features as feedback to the user.'\\n\\n'Separately for each subject, Leave-One-Run-Out Cross-Validation is used to evaluate the pseudo-online performance of the decoder, avoiding information leakage from run to run:\\\\nn\\u22121\\\\nruns are used as training set and 1 run as test set. This is done to properly evaluate the adaptation ability of the decoder across different runs.'\\n\\n'as already mentioned, the decoder is to be used in asynchronous mode in order to detect foot motor imagery, therefore the two classes of interest are: non-control and foot imagery. In order to find the spatial filters that maximize the difference in variance between the two classes, we use a robust version of the Common Spatial Patterns (CSP) algorithm [28]. For this purpose, from the bandpass filtered EEG signal (7\\u201330 Hz) of the training set, we cut epochs of 3 s length from 0.5 to 3.5 s with respect to the motor imagery onset, for the foot imagery class. For the longer non-control class the epochs start at 2 s and end at 7 s after the motor imagery offset.'\\n\\n'Threshold tuning and all-or-nothing approach. The top row shows how the threshold is tuned based on the training set. In the bottom row the same threshold is applied to classify the test set. The two columns represent the two classes. Each subplot displays the mean (green line) and standard deviation (gray shadow) of the decoder probability output for each time bin. A blue dot represents the sliding window with the highest probability output within a trial (i.e. A set of consecutive sliding windows belonging to the same class). We observe that the mean of motor imagery is higher than the mean of non-control. However, many blue dots of the non-control class are above 0.5, which is the typical threshold. By finding the ROC best operating point of the blue dots, we tune the threshold (red horizontal line). We observe that most of the blue dots can be separated by the tuned threshold both in the training and test set.'\\n\\n'It should be noted that visual cues were only used to train the decoder, and that testing was always done with a sliding window (i.e. asynchronously) both in the pseudo-online analysis and in the online experiment. Therefore, during operation the user can make self-paced decisions irrespectively of any external cue. The visual instructions shown in Fig. 5 and in the accompanying video are purely for demonstration, and their timing is never used by the online decoder.'\\n\\n'Sliding window output. One run of s1 is displayed. The top panel shows the adaptive feature (i.e. Log-power). In this case, there is only one feature because only one CSP filter was automatically selected. The bottom panel shows the relative decoder probability output. The red horizontal threshold is obtained by finding the ROC best operating point of the all-or-nothing probabilities computed on the training set (for more details see fig. 4 and the last paragraph of Section II-C.4). When the threshold is reached, motor imagery is detected. We observe that high probability output is reached concurrently with a strong erd.'\\n\\n\\\"A co-adaptive BRI would aim at reinforcing a subject's kinesthetic imagination ability by proprioceptive feedback. With the current implementation, the robot starts squatting only if the decoder output reaches the threshold reliably, which may add some delay between motor imagery and the feedback. In future studies, we will investigate the effectiveness of alternative feedback approaches: by directly modulating either the position or the speed of the robot's center of mass, proportionally to the decoder probability output. This would provide immediate proprioceptive feedback to the subject, allowing for studies about the reinforcement of motor imagery, and about the influence of the afferent input on the sensorimotor rhythms or on the decoder itself [37]. In a condition where the robot moves while the EEG signal is being analyzed, it will be necessary to quantitatively evaluate the effect of motion artifacts on the dry electrodes, and possibly implement strict artifact correction algorithms.\\\"\",\"118\":\"'Interface between map of head space and the adaptive filter. A) A positive whisker contact generates a gaussian activity in the topographic map centred at the estimated point of contact. B) the map is coarse coded into discrete 2D cells. C) each cell has an associated weight vector\\\\nx,y\\\\n, represented by the\\\\nm\\\\nth\\\\nparallel fibre entering the adaptive filter. Each weight vector is proportionaly modified by the filter in response to orient errors to calibrate the overall map.'\",\"119\":null,\"120\":null,\"121\":null,\"122\":null,\"123\":null,\"124\":\"'a publicly available open source implementation of different room segmentation algorithms,'\",\"125\":\"'A possible way to circumvent these problems is modelling the data using a method that is able to infer spatial correlations among data points. This creates an opportunity to use Gaussian processes (GPs), a Bayesian inference method that is very apt at nonlinear interpolation. To fit a nonparametric function through the data, GPs use a kernel which encodes a prior belief over the correlation between data points [2]. GP-based continuous occupancy mapping methods (GPOMs) have been proposed in the past [3], [4] and provide the basis for this work.'\",\"126\":null,\"127\":\"'Survival time belief shaping using the hazard function. (a)\\u2014(d) show the corresponding hazard, cumulative hazard, survival, and probability density functions, respectively, for three survival time distributions (colored red, blue, and cyan). Note that even visually quite similar probability density functions may encode very different rates of feature disappearance as functions of time (compare the red and cyan functions in (a) and (d)), thus illustrating the utility of analyzing and constructing survival time priors using the hazard function.'\\n\\n'This example also serves to illustrate the utility of the belief-shaping approach. It may not always be clear how to directly construct a survival-time prior that encodes a given set of environmental dynamics; for example, comparing the red and cyan curves in Figs. 2(a) and 2(d) reveals that even visually-similar probability density functions may encode radically different environmental properties. In contrast, the dynamical description provided by the hazard function is easily interpreted, and therefore easily designed.'\\n\\n'From the perspective of belief shaping using the hazard function, this principle suggests the use of a constant hazard rate (since the use of any non-constant function would encode the belief that there are certain distinguished times at which features are more likely to vanish). And indeed, setting\\\\n\\u03bb\\\\nT\\\\n(t)\\u2261\\u03bb\\u2208(0,\\u221e)\\\\ncorresponds (by means of (20)) to selecting the exponential distribution:'\",\"128\":null,\"129\":null,\"130\":null,\"131\":null,\"132\":null,\"133\":null,\"134\":null,\"135\":\"'Therapeutic robots are, in essence, haptic interfaces that would maintain a desired contact impedance as the therapy is applied. A fast dynamic response is required to reproduce the desired transient forces of contact events. Therefore, accurate force and position feedback obtained at the contact point are very crucial for a satisfactory performance. Commercially available optical encoders have been in use for displacement feedback in MRI. In the recent years, several MRI compatible optical force sensors have been developed [8]. However, pneumatic actuation has been exploited in a limited number of rehabilitation-related studies so far [9]. When coupled with MRI-safe sensory units; pneumatically driven tele-operated platforms can be utilized to provide a controlled impedance on haptic interactions.'\",\"136\":null,\"137\":\"'In this paper, we have described a novel approach for generating mosaics from images. Our scheme, named BIMOS, is based on a multi-threaded architecture which allows us to decouple the different parts of the algorithm, speeding up the mosaicing process. The topology of the environment is modelled by means of an undirected graph. To find the overlapping pairs in an efficient way, this graph is created using a visual index of binary features, which is built online. We have validated our approach under different operating conditions, obtaining coherent mosaics in all cases. As a secondary contribution, the code of BIMOS has been made public to the community as a ROS node.'\\n\\n'As a future work, as said previously, we are interested in improving the blending component of the algorithm, since it is an external code and is the main bottleneck of our current solution. To further speed up the process, we plan to adopt a local optimization strategy instead of adjusting the whole graph during the global alignment process. We are also considering to extend the approach to use other motion models and\\/or multiple camera configurations. Finally, BIMOS can be further improved by merging image information with navigation data obtained from other sensors.'\",\"138\":null,\"139\":null,\"140\":\"'The 802.11 protocol multicasts data packets using the broadcast address. A packet with a broadcast\\/multicast address shall be decoded by all recipients of a multicast group. If all members acknowledge (ACK) the receipt of the packet, the ACK packets will collide and so the source will keep on re-transmitting the same packet for several times. The reliability problem stems from the lack of a mechanism that acknowledges reception of multicast packets [23] or retransmissions of lost packets. The challenge is how to make multicasting reliable such that lost packets are retransmitted to the desired recipients.'\\n\\n'The application layer video multicast gateway (ALVM-GW) takes in input N high-quality video streams from the mavs and transcodes them to N low-quality and N high-quality videos. The ALVM-GW manages multiple groups, and adapts transmission and video encoding rates based on the feedback received from the multicast group members.'\\n\\n'This behavior is observed since the video is encoded at 2500 Kbps (Fig. 5(b)) at the start and the transmission rate is set to the lowest\\\\nR\\\\ni\\\\n, i.e. 6 Mbps. As the ALVM-GW receives consecutive AL-ACKs the video encoding rate increases and so the goodput also increases. However, since the receiver node is moving away the signal strength decreases, leading to the increased packet loss (Fig. 5(c)). As the ALVM-GW receives AL-NACKs, the video encoding rate is reduced until the receiver node moves out of communication range and the signal is completely lost. The corresponding delays remain under the 250 ms bound (Fig. 5(d)). Although with this scenario the constant bit rate (CBR) traffic with fixed transmission rate at 6 Mbps performs better, if the input HD video stream is encoded at a higher bit rate, e.g. 25 Mbps, higher packet losses will be observed that will prohibit the reception of the video stream. However, with the rate adaptive scheme the video playback remains smooth.'\\n\\n'Our future work will focus on developing application layer packet-correction codes to reduce the number of retransmissions, enabling higher-quality video stream. Moreover, we will test the performance of the proposed framework using a real testbed.'\",\"141\":null,\"142\":null,\"143\":\"'http:\\/\\/gazebosim.org\\/'\",\"144\":null,\"145\":null,\"146\":\"'http:\\/\\/sites.bu.edu\\/msl\\/vrb-obstacles\\/'\",\"147\":\"'https:\\/\\/youtu.be\\/2T_tt5j1rpA'\\n\\n'In this paper, we present extensive advances in live-fly field experimentation capabilities of large numbers of fixed-wing aerial robots, and highlight both the enabling technologies as well as the challenges addressed in such largescale flight operations. We showcase results from recent field tests, including the autonomous launch, flight, and landing of 50 UAVs, which illuminate numerous operational lessons learned and generate rich multi-UAV datasets. We detail the design and open architecture of the testbed, which intentionally leverages low-cost and open-source components, aimed at promoting continued advances and alignment of multi-robot systems research and practice.'\\n\\n'Robotics and unmanned systems are increasingly a critical element of current and future civilian and military applications, and their capabilities and associated technologies necessarily must continue to rapidly evolve to keep pace with increasingly expanding mission sets. The maturation of unmanned systems technology, most notably in unmanned aerial vehicles (UAVs), has benefited substantially from the confluence of relatively lower cost, increasingly easier access, and wider active engagement by open-source communities, leading to significant advances in the development and deployment of large numbers of autonomous systems.'\\n\\n'Picture of NPS zephyrii UAV, a low-cost yet capable system leveraging open-source and commercially available components.'\\n\\n'Both stacks have their respective ingress, egress, and failsafe locations, as well as a standard \\u201cracetrack\\u201d pattern and the Swarm Ready waypoint, where aircraft awaiting tasking for collective behavior maintain an orbit. Two landing patterns are included in the mission, with selection of one occurring just prior to landing depending on the prevailing wind direction, with a targeted landing location common to all UAVs. Furthermore, a software-enabled failsafe for containment is provided by the geo-fence feature in the open-source autopilot software.'\",\"148\":null,\"149\":null,\"150\":null,\"151\":\"'Map representation. The map is represented as a dense TSDF voxel grid in KinectFusion, whereas in LSD-SLAM it is represented by a set of keyframes and depth estimates for the high-gradient pixels in those frames. The voxel grid is wasteful, requiring empty regions of the space to be represented and processed, whereas keyframes encoded at the level of semi-dense edge pixels are much more compressed.'\\n\\n'Yet another observation is that despite the same code being run on the two platforms (for both KinectFusion and LSD-SLAM), we do not get the same ATE. This is due to different compilers, operating systems, and hardware being used, e.g. different floating point approximation used.'\",\"152\":null,\"153\":\"'For synthetic data generation, we first rendered photo-realistic GS images of a 3D model scene by using POV-Ray from ICL-NUIM RGBD-benchmark dataset generation code [7], then built a RS sequence by gathering all scan-lines from the images corresponding to the ground truth pose of RS camera. Two image sequences were created for the same trajectory but different speeds of camera motion. Sequence154 contains 154 RS images which are extracted and accumulated from 73,920 GS images of\\\\n640\\u00d7480\\\\nresolution. Sequence77 comprises 77 RS images generated from 36,960 rendered images as shown in Figure 6. The motion in this synthetic data is translational only, with no rotational component. First the camera translates in the xy-plane (perpendicular to the viewing direction) in a small loop (the so-called \\u201cSLAM-waggle\\u201d), then proceeds forward\\\\n(+z)\\\\n, to the right\\\\n(+x)\\\\n, backward\\\\n(\\u2212z)\\\\nand left\\\\n(\\u2212x)\\\\n, followed by another forward motion\\\\n(+z)\\\\nto close a full loop. This motion is within a\\\\n2m\\u00d70.4m\\u00d72.25m\\\\nvolume.'\\n\\n'We are extremely grateful to the Australian Research Council for funding this research through project DP130104413, the ARC Centre for Robotic Vision CE140100016, and through a Laureate Fellowship FL130100102 to IDR. We thank Ankur Handa for his POV-Ray code [7], and Computer Vision Group in Technische Universit\\u00e4t M\\u00fcnchen for their LSD-SLAM code and RGBD-benchmark dataset. We also highly appreciate all discussion with and comments from Tom Drummond, Alireza Khosravian, Yasir Latif, Zygmunt Szpak and others in the Australian Centre for Robotic Vision and Australian Centre for Visual Technologies.'\",\"154\":null,\"155\":null,\"156\":null,\"157\":null,\"158\":null,\"159\":null,\"160\":null,\"161\":\"'The three consecutive quadratures are considered in the error analysis to validate the code generated by Maple and to eliminate error propagation as possible cause of bad convergence. Validation of the generated code is performed via comparison of an estimate for the experimental order of convergence (EOC) to the theoretical order. Deviations indicate errors due to cancellation in the evaluated code. The theoretical model error of the composite trapezoidal rule\\\\nT\\\\nN\\\\nis of second order [32]. The approximation error of the ith\\\\n(i\\u2208{1,2,3})\\\\nintegral\\\\nI\\\\ni\\\\nis estimated via\\\\nE\\\\ni\\\\n(\\\\nT\\\\nN\\\\ni\\\\n)=|\\\\nT\\\\nN\\\\ni\\\\n\\u2212\\\\nI\\\\ni\\\\n|\\u2248|\\\\nT\\\\nN\\\\ni\\\\n\\u2212\\\\nT\\\\n5e3\\\\ni\\\\n|\\\\n. The EOC is then computed via'\",\"162\":\"'Before defining the system states, we first define the \\u201ccontrol\\u201d variables of the AMoD system. A control decision is made at each time step for each vehicle parked at a station. The two possible actions each vehicle can take are 1) transport a customer from one station to another, and 2) rebalance the system by driving itself from one station to another (this is a key advantage of robotic vehicles). We can encode these actions using binary variables. Let\\\\nv\\\\nk\\\\nij\\\\n(t)=1\\\\nif vehicle\\\\nk\\\\nis transporting a customer from station\\\\ni\\\\nto station\\\\nj\\\\nbeginning at time\\\\nt\\\\n, and arriving at station\\\\nj\\\\nat time\\\\nt+\\\\nt\\\\nij\\\\n. The travel time\\\\nt\\\\nij\\\\nisassumed to be deterministic and known. Similarly, let\\\\nw\\\\nk\\\\nij\\\\n(t)=1\\\\nif vehicle\\\\nk\\\\nis rebalancing from station\\\\ni\\\\nto station\\\\nj\\\\nbeginning at time\\\\nt\\\\nand arriving at time\\\\nt+\\\\nt\\\\nij\\\\n.'\\n\\n'A few comments are in order. First, one may wonder why information about whether a vehicle is rebalancing or ferrying a passenger is not encoded in the state vector. This is because we have assumed that as soon as a customer boards a vehicle, he\\/she has been serviced and the vehicle is identical to one that is rebalancing (traveling without a customer). The only thing that matters is the time at which the vehicle arrives at its destination. Second, we have assumed that each station has sufficient parking space for as many vehicles as needed. This may be indeed true if the stations are geographical regions and vehicles are loitering within the region while waiting for customers. However, limited parking spaces are a real concern especially if parking spaces serve as charging stations for electric vehicles. In Section VI we discuss how this AMoD framework can easily be extended to include limited parking capacity. In the next section we outline additional considerations associated with electric vehicles, in particular charging constraints (additional operational constraints are discussed in Section VI).'\",\"163\":null,\"164\":\"'Fixed Go-To Task Under Disturbance: In the first experiment, Rezero is given the task to stay at its initial state which is encoded in the final term of the cost function. During the test we perturb the robot manually to various distances to the initial state. These disturbances would eventually exceed the stability margin of most static feedback controllers (see comparison with LQR in the video 1 attachment), unless they are low-gain and hence show bad tracking performance. In Fig. 3 an overhead position plot of one return to the initial position after a perturbation is illustrated. The black circles indicate the starting point (after the perturbation has finished) and the final position. The optimized, predicted trajectories are indicated using a gradient that starts with blue (beginning of the trajectory) and ends in red (end of trajectory). These trajectories are obtained by forward simulating a noiseless model of the system dynamics. In green, the actually executed trajectory is shown. As seen in the plot, the MPC controller gradually adjusts the plan to a circular \\u201cswing-in\\u201d motion. This behavior seems to be favourable in terms of the cost function as it allows a more graceful approach. Due to the limited time horizon, the initial optimization does not converge to this solution and prefers a straight goal approach, but ultimately, the controller converges to a circular balancing trajectory. This trajectory can also be observed in tests with the LQR, which suggests that this behavior emerges from the system dynamics.'\",\"165\":null,\"166\":\"'https:\\/\\/www.youtube.com\\/watch?v=n6AWOLpbNzs'\",\"167\":null,\"168\":null,\"169\":null,\"170\":\"'https:\\/\\/github.com\\/StanfordASL\\/UASEncounter'\\n\\n'The Julia code used for these experiments is available at https:\\/\\/github.com\\/StanfordASL\\/UASEncounter.'\\n\\n'https:\\/\\/github.com\\/StanfordASL\\/UASEncounter'\",\"171\":\"'Within this work, an exploration path planner was proposed that is capable of exploring a previously unknown area, constructing an occupancy map of the perceived environment. While collisions are avoided, good exploration paths are computed online, considering the updated model of the environment. Compared to a frontier-based planner, the proposed method offers improved scaling properties with respect to the size of the scenario, enabling the exploration of large areas. These properties have been demonstrated in simulation test cases, as well as in a challenging real world experiment, using a hexacopter rotorcraft, equipped with a stereo camera system. The implementation of the proposed planner is released as an open source code package for further development by the community [14].'\",\"172\":null,\"173\":\"'http:\\/\\/www.ece.ust.hk\\/-eeshaojie\\/icra2016jing.mp4'\\n\\n'The challenge in trajectory planning for UAVs operating in cluttered environments lies in the difficulty of modeling a large number of possibly non-convex obstacles or determining the traversable space. Earlier studies considered the simplified convex case and proposeed mixed-integer linear programming (MILP) approaches to encode obstacle information [3]\\u2013[6]. In [7], building on the pioneering work on minimum-snap trajectory generation in [1], the authors proposed to model faces of non-convex obstacles using mixed-integer quadratic programming. Instead of modeling obstacles directly, [8] proposed finding large convex segments to cover the free-space [9] and then performing simultaneous segment assignment and safe trajectory generation using mixed-integer programming. However, the computational complexity of all these mixed-integer approaches can quickly become intractable with an increasing number obstacles or free-space regions.'\\n\\n'Two simulation and two real-world flight experiments are presented to validate our approach. Our trajectory generation method is implemented in C++11 using a g++ compiler with the -O3 optimization option. Additionally, we use the sparse linear algebra library in Eigen and an open source quadratic programming solver, OOQP (Object Oriented software for Quadratic Programming) [18]. The boundary margins\\\\n\\u03b4\\\\np\\\\n,\\\\n\\u03b4\\\\nv\\\\nand\\\\n\\u03b4\\\\na\\\\nare set as 0.005 m, 0.05 m\\/s, and 0.05 m\\/s2 respectively.'\",\"174\":null,\"175\":null,\"176\":\"'https:\\/\\/github.com\\/ygling2008\\/dense_new'\\n\\n'In this work, we address the problem of aggressive flight of a quadrotor aerial vehicle using cameras and IMUs as the only sensing modalities. We present a fully integrated quadrotor system and demonstrate through online experiment the capability of autonomous flight with linear velocities up to 4.2 m\\/s, linear accelerations up to 9.6 m\\/s2, and angular velocities up to 245.1 degree\\/s. Central to our approach is a dense visual-inertial state estimator for reliable tracking of aggressive motions. An uncertainty-aware direct dense visual tracking module provides camera pose tracking that takes inverse depth uncertainty into account and is resistant to motion blur. Measurements from IMU pre-integration and multi-constrained dense visual tracking are fused probabilistically using an optimization-based sensor fusion framework. Extensive statistical analysis and comparison are presented to verify the performance of the proposed approach. We also release our code as open-source ROS packages.'\\n\\n'https:\\/\\/github.com\\/ygling2008\\/dense_new'\",\"177\":\"\\\"Panel (a) and (b) show the naive and wind planner's trajectories, respectively, for an example start and goal location. The trajectories are color coded by ground speed. The green and red circles denote the start and goal locations, respectively.\\\"\",\"178\":null,\"179\":null,\"180\":\"'Flight experiments to evaluate the proposed state estimator were conducted using the Iris+ quadrotor by 3D Robotics Inc. The Iris+ features the open-source Pixhawk auto-pilot containing the Invensense MPU 6000 IMU. An Odroid U3 by Hardkernel was employed as the on-board computer for the Iris+. It features a 1.7GHz Exynos4412 Prime Cortex-A9 Quad-core processor with 2Gbyte main memory. The U3 weighs 48g including heat sink making it an ideal on-board computer for light-weight MAVs. The on-board camera was a Point Grey Firefly which connected to the U3 via a USB interface (see Fig. 1). This camera supports both global shutter and external triggering, eliminating the need for explicit time synchronization between the camera and IMU.'\",\"181\":null,\"182\":null,\"183\":\"\\\"As a test of the framework's ability to produce stable control, we use only DRC-HUBO\\u2018s encoders for position control feedback, and no inertial measurement. We fed the same open-loop IK-solved position trajectory from Gait B to DRC-HUBO. We noticed that the robot had a significant propensity to lean forward and fall during the beginning static position, which we determined was likely due to modeling errors. As such, we used the IK solver to adjust the gait intuitively, asking it to move the feet forward by 4cm to tune its balance. Because of the HZD-IK framework, this adjustment was computationally trivial. With this adjustment made, DRC-HUBO would start from rest, execute the startup procedure, and walk nine dynamic steps, open-loop, before falling forward. Fig. 8 shows tiled images of the experimental gait.\\\"\",\"184\":\"'We use articulated rigid multibody models to formulate the dynamics of the full-body human model. For the dynamics modeling we created RBDL - the Rigid Body Dynamics Library which uses a reduced coordinate approach to model kinematics and dynamics of multibody systems. It is an open-source implementation [31] of various state-of-the-art reduced coordinate algorithms for multibody dynamics described in [32].'\",\"185\":\"'Ranger has a variety of sensors. Foot sensors measure the force between the heel of the foot and the ankle joint, which we then threshold to determine if a foot is in contact with the ground or not. Each joint has absolute angle encoders for both the motor and the end-effector. Finally, there is an IMU (gyro and accelerometers) located on the outer legs that we use for estimating the absolute orientation and angular rate of the outer legs.'\\n\\n'The motor controllers are at the bottom level of the controller. They run a simple proportional-integral control loop at 2 kHz on each of the three joint motors (outer ankle, inner ankle, and hip), tracking a desired joint torque. These motor controllers are coded at a low-level in the robot, and we did not change these.'\\n\\n'We implemented the entire design process for the balance controller in Matlab, with most of the simulation code being compiled to MEX for faster run-time. The code takes about 10 minutes to compute the optimal control parameters running on a laptop (Intel Quad-Core i7 CPU Q720, 1.60 GHz), where each walking step of the robot takes about 0.034 seconds to compute, for a total of about 18,000 steps per optimization.'\",\"186\":null,\"187\":null,\"188\":null,\"189\":null,\"190\":null,\"191\":\"'The underlying principle of the proposed planner, whose pseudocode3 is given in Algorithm 1, is simple.'\\n\\n'The constrained motion planner, whose pseudocode is shown in Algorithm 2, is essentially the same of [15]. The task trajectory to be realized is denoted by\\\\ny\\\\nd\\\\n; in the\\\\ni\\u2212th\\\\niteration of Algorithm 1, the planner is invoked with\\\\ny\\\\nd\\\\nset to\\\\ny\\\\n[i]\\\\n. The goal is to find a whole-body motion that realizes\\\\ny\\\\nd\\\\nand is feasible, i.e., satisfies requirements R2-R4.'\\n\\n'Procedure 1 contains a pseudocode for motion generation. This starts by randomly choosing a CoM movement primitive\\\\nu\\\\nk\\\\nCoM\\\\n(\\u22c5)\\\\nof duration\\\\nT\\\\nk\\\\n, simply denoted by\\\\nu\\\\nk\\\\nCoM\\\\nso forth, from the set4'\\n\\n'The pseudocode of the task deformation procedure is given in Algorithm 3. In addition to the latest task trajectory\\\\ny\\\\n[i]\\\\n, the procedure receives in input the limit task sample\\\\ny\\\\n~\\\\nreached on\\\\ny\\\\n[i]\\\\nby the constrained motion planner, as well as the associated exploration tree\\\\nT\\\\n.'\",\"192\":\"'http:\\/\\/rse-lab.cs.washington.edu\\/projects\\/neol'\",\"193\":null,\"194\":\"'In the context of semantic segmentation, there are several approaches that encode segmentation relations using Conditional Random Fields (CRFs) [1], [18]\\u2013[20]. Plath et al. [20] present an approach that couples local image features with a CRF and an image classification approach to combine global image classification with local segmentation. Another branch of CRFs called Hierarchical Conditional Random Fields (HCRF) has been introduced by Boix et al. [1]. They propose a technique called harmony potential to overcome the problem of classical HCRFs, that they do not allow multiple classes to be assigned to a single region. Maire et al. [19] use an alternative people detection and segmentation approach, in which they merge the outputs of a top-down part detector in a generalized eigen problem, producing pixel groupings. Lucchi et al. [18] present an analysis of the importance of spatial and global constraints in CRFs when such features have already extracted information from the whole image.'\",\"195\":\"'Fine-grained action recognition is important for many applications of human-robot interaction, automated skill assessment, and surveillance. The goal is to segment and classify all actions occurring in a time series sequence. While recent recognition methods have shown strong performance in robotics applications, they often require hand-crafted features, use large amounts of domain knowledge, or employ overly simplistic representations of how objects change throughout an action. In this paper we present the Latent Convolutional Skip Chain Conditional Random Field (LC-SC-CRF). This time series model learns a set of interpretable and composable action primitives from sensor data. We apply our model to cooking tasks using accelerometer data from the University of Dundee 50 Salads dataset and to robotic surgery training tasks using robot kinematic data from the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our performance on 50 Salads and JIGSAWS are 18.0% and 5.3% higher than the state of the art, respectively. This model performs well without requiring hand-crafted features or intricate domain knowledge. The code and features have been made public.'\\n\\n'We publicly released our model code, features, and scripts used for 50 Salads and JIGSAWS1 .'\",\"196\":null,\"197\":null,\"198\":null,\"199\":null,\"200\":\"'We propose a Hypercube Pyramid descriptor as a discriminative feature representation that encodes multiscale, spatially-relevant information for RGB-D object and instance categorization (Section III-B).'\\n\\n'To convert the feature maps into the Hypercube representation that encodes multi-scale information, first each convolutional feature map is sub-sampled into three pyramid levels. Specifically, we sub-sample the spatial dimension (\\\\ni,j\\\\n) of each feature map in all convolutional layers into\\\\np\\\\n(1)\\\\n=m\\u00d7m,\\\\np\\\\n(2)\\\\n=2m\\u00d72m\\\\nand\\\\np\\\\n(3)\\\\n=0.5m\\u00d70.5m\\\\nrespectively using bilinear interpolation in order to capture distinctive features of the convolutional layers at multiple scales [15]. Then, we concatenate them together along the depth dimension separately at each pyramid level to produce a pyramid of Hypercube descriptors (see Fig. 1 for illustration). Concretely, our Hypercube at each pyramid level\\\\nP\\\\nis given by'\",\"201\":null,\"202\":null,\"203\":null,\"204\":null,\"205\":null,\"206\":null,\"207\":\"'The permanent actuator magnets used for the prototype system are transversely magnetized, grade N42 cubic NdFeB magnets with side length equal to 2.54 cm. Stepper motors of size NEMA 23 and capable of 0.39 Nm of stall torque were used to rotate the magnets at speeds up to 120 RPM. These motors have average capabilities and a future version of this system could be improved through the use of motors with higher torque and rotational speeds. The stepper motors are controlled using motor driver boards (Quadstepper Motor Driver Board, SparkFun). Motor position feedback is obtained using magnetic rotary encoders (AS5040, ams AG). The driver boards and encoders were interfaced using a digital I\\/O board (USBDIO-48, Accessio) to a PC running Ubuntu Linux with custom control code.'\\n\\n'The structural pieces of the prototype were assembled using laser-cut pieces of high-density fiberboard. Two stationary cameras (FO134TC, Foculus) provide feedback from the top and side of the prototype. For feedback control, a microrobot detection algorithm was implemented using a threshold function and Hough Transform using the openCV library, capable of detection at up to 60 fps. The total cost of the prototype (magnets motors, motor drivers, encoders and structural elements) is approximately 1000 USD. Additional components such as PC, data acquisition card and cameras cost about 2000 USD.'\",\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":null,\"213\":null,\"214\":\"'The desired walking pattern is encoded as\\\\ng\\\\ni\\\\n(\\\\ns\\\\n\\u00af\\\\n,\\\\nq\\\\n\\u00af\\\\n)=0 (i\\u2208{L,R})'\",\"215\":\"'From the high-level, desired joint trajectories travel through two stages before reaching the low-level embedded control. First, desired motor positions are sent from the high-level process to the Simulink-generated process responsible for communicating with the joint microcontrollers. At this stage, the trajectory is up-sampled from 250 Hz to 1 kHz with a first-order hold and sent to the embedded level. An IMU mounted in the torso of DURUS is used to determine the global orientation of the torso for the roll regulator. Finally, incremental encoders and absolute encoders are used at each joint, though incremental encoders are the only sensors used actively in the joint-level feedback control. The joint microcontrollers implement the modulated joint trajectories (4) and the joint velocities from (12) via position control at 10 kHz. The discrete modes of the regulator structure are triggered by the foot interactions with the ground; specifically, strike detection is triggered when the spring deflection in the ankles passes a certain threshold. The results of applying the regulators to DURUS while walking, including the discrete structure of the blending factor together with actual and desired values in (15) and the resulting applied delta,\\\\n\\u0394\\\\nq\\\\nd\\\\n, is shown in Fig. 7.'\",\"216\":\"'Servo-motors control errors. Comparing left knee and left hip pitch reference positions to the read positions from encoders. In addition to the time lag, a max error of 3 to 5 degrees can be observed.'\\n\\n\\\"When using sensors, all the degrees of freedom positions are set to the measured joint encoders. Then, the support foot is determined by the pressure sensors to be the foot measuring the more weight. The robot's yaw orientation is computed by integrating the raw yaw gyroscope which has proven to be more accurate than the integration of the kinematic model rotations. In addition to the mechanical backlash, when walking on soft surface such as artificial grass, the foot may not lie flat. Filtered measured pitch and roll from the IMU placed on the trunk are assumed to be correct. A rotation is then applied between the support foot and the ground in order for the trunk to match the measured orientation.\\\"\\n\\n'In a lesser way, the sensor-based odometry is also underestimating distances even when reading the motor position encoders. This is the result of some non measurable mechanical backlash increasing the real amplitude of the steps.'\\n\\n'Then, switching from a carpet surface to a soft artificial grass increases simulation-based odometry errors but does not affect sensor-based odometry. This is a clue that due to grass friction on flying foot, the actual footstep displacements may be altered. Thus, not using the environment feedback increases the odometry errors. On contrary, sensor-based odometry, taking into account motor encoders and sensors is not significantly affected by the grass.'\",\"217\":\"'The design of the structural parts of both exoskeleton and actuators have been evaluated through finite element analysis (FEM). Load conditions have been defined through simulation of a set of high strength\\/power tasks. These tasks were extracted from the real world challenges that the robot is expected to perform, such as manipulating heavy objects or climbing stairs, in order to encapsulate most of the power\\/strength requirements in a conservative manner. In detail we considered stepping on elevated surfaces (50 cm), large stepping (65 cm), squatting and swaying with 50 kg weight, landing on a single foot from 0.35 m, reactive stepping (forward and lateral) and falling (forward and lateral). The simulations were performed with an open source compliant joint simulator based on Robotran multi-body modeling tool [16]. More details on these tasks and specification results can be found in a related work [17].'\",\"218\":null,\"219\":\"'2.5D maps are preferable for representing the environment owing to their compactness. When noisy observations from multiple diverse sensors at different resolutions are available, the problem of 2.5D mapping turns to how to compound the information in an effective and efficient manner. This paper proposes a generic probabilistic framework for fusing efficiently multiple sources of sensor data to generate amendable, high-resolution 2.5D maps. The key idea is to exploit the sparse structure of the information matrix. Gaussian Markov Random Fields are employed to learn a prior map, which uses the conditional independence property between spatial location to obtain a representation of the state with a sparse information matrix. This prior map encoded in information form can then be updated with other sources of sensor data in constant time. Later, mean state vector and variances can be also efficiently recovered using sparse matrices techniques. The proposed approach allows accurate estimation of 2.5D maps at arbitrary resolution, while incorporating sensor noise and spatial dependency in a statistically sound way. We apply the proposed framework to pipe wall thickness mapping and fuse data from two diverse sensors that have different resolutions. Experimental results are compared with three other methods, showing that, while greatly reducing computation time, the proposed framework is able to capture in large extend the spatial correlation to generate equivalent results to the computationally expensive optimal fusion method in covariance form with a Gaussian Process prior.'\",\"220\":null,\"221\":\"'We focus on the use of low-cost ranging modules and inertial measurement units mounted to the rods of a tensegrity robot as the sensor inputs to an unscented Kalman filter (UKF). These ranging sensors can be purchased off-the-shelf and do not rely on any user-designed mechanical infrastructure to operate. We also use motor encoders to sense change in cable rest length as control inputs into the dynamic model utilized by the UKF. For testing our approach, we use the SUPERball prototype, a six strut tensegrity robot designed to explore tensegrity systems for planetary exploration [5] [6]. SUPERball is shown in Fig. 1.'\\n\\n'We equipped all end caps of SUPERball with a DWM1000 ranging module from DecaWave Ltd. By employing ultra wideband technology, the low-cost DWM1000 modules provide wireless data transfer and highly accurate timestamps of transmitted and received packets. This allows the distance between two DWM1000 modules to be estimated by computing the time-of- flight of exchanged messages without the need for synchronized clocks. We opted for this technology because it allows proprioceptive state estimation (distances between end caps), which cannot be easily tracked directly via motor encoders. [7] Furthermore, we placed eight more DWM1000 modules as \\u201cfixed anchors\\u201d around our testing area to provide a world reference frame for ground truth and generation of a reward signal for the machine learning algorithms that we will use to develop locomotion controllers for the robot. Our intention is that the fixed anchors will not be required in the final deployed version of the robot, and are primarily for use during algorithm development.'\",\"222\":null,\"223\":\"'Reconstructed stanford bunny with color encoded surface variance, with raycasted images from the microsoft kinect and the creative senz3D camera viewpoint.'\",\"224\":\"'We could have tried linear tachometers on the actuators, or put very high resolution linear encoders on the actuators or rotary encoders on the joints to accurately measure velocities. However, we were limited in how we could modify our government-owned Atlas robot. We pursued this MEMS gyro-based approach instead. It is cheap and easy to apply to other human-size humanoid robots.'\\n\\n'The MEMS gyro-based approach is good for soft systems with poorly defined joints (inflatable robots and soft exoskeletons for example) since encoders are not very useful in those robots.'\",\"225\":\"'Images from the euroc (upper row: Motion blur, middle row: Textureless) and malaga datasets (bottom row) with semi-dense depth estimates. Semi-dense depth maps, with color-coded depth estimates are shown on the left.'\",\"226\":null,\"227\":\"'http:\\/\\/ter.ps\\/foldingcube'\",\"228\":null,\"229\":null,\"230\":null,\"231\":null,\"232\":\"https:\\/\\/github.com\\/BerkeleyAutomation\\/dex-net\\/raw\\/gh-pages\\/docs\\/dexnet_icra2016.bib\",\"233\":\"'http:\\/\\/www.saturdayacademy.org\\/ase'\\n\\n'We employed a two-step process to determine where (semantically) in each image the fixation occurred. In the first step we partitioned the full monitor image into the left and right images, the question area (below the images) and other (browser bars, clock, etc). We then further divided the individual grasp images into semantically meaningful regions (which may overlap). These regions were stored as color-coded \\u201cmask\\u201d images. Because the object was placed in the same location in the image for all grasps for that object, we were able to create a single object-only mask. The hand and arm, however, required manually creating a mask for each grasp for each object for each camera view (see Figure 1). The object and grasp masks were merged during analysis.'\",\"234\":null,\"235\":\"'The current implementation using C++ takes about 62 sec. to compute all the minimal rings for a model with 5k faces on a PC with i7-4700MQ 2.4GHz CPU. The long computation time is mainly caused by the unoptimized code for extracting initial rings. One of our future works is to speedup the computation in this step of initial ring generation. Another planned work is to test this rope caging and grasping work on an integrated platform with robotic arm and grippers so that both motion planning and grasping are considered and tested.'\",\"236\":null,\"237\":null,\"238\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/30101035'\",\"239\":\"'http:\\/\\/www.csc.kth.se\\/~kaiyuh\\/videos\\/graspManifold.mp4'\",\"240\":null,\"241\":null,\"242\":null,\"243\":null,\"244\":\"'MuJoCo is a new open source engine that targets physically correct simulation in robotics and biomechanics. The developers of the MuJoCo engine performed a comparison study between Bullet, Havok, MuJoCo, ODE and PhysX[12]. These tests involve larger structures with many constrained bodies, which will not be the focus of this paper.'\\n\\n'In this paper, we have presented a new engine (RWPE) for performing accurate dynamic simulations. We have shown that the engine is superior to the widely used engines ODE and Bullet with respect to precision in movements of free bodies, distribution of forces between redundant contacts, restitution modeling, and in particular avoiding penetration. In the near future, we plan to implement a faster method for solving the convex optimization problem to further increase the computational speed. Moreover, RWPE will be further benchmarked in tests. During spring 2016, we plan to release RWPE as an open source software package under the Apache 2.0 license.'\",\"245\":null,\"246\":null,\"247\":null,\"248\":null,\"249\":\"'We present a novel 6-DoF cable driven manipulator for handheld robotic tasks. Based on a coupled tendon approach, the arm is optimized to maximize movement speed and configuration space while reducing the total mass of the arm. We propose a space carving approach to design optimal link geometry maximizing structural strength and joint limits while minimizing link mass. The design improves on similar non-handheld tendon-driven manipulators and reduces the required number of actuators to one per DoF. As the manipulator has one redundant joint, we present a 5-DoF inverse kinematics solution for the end effector pose. The inverse kinematics is solved by splitting the 6-DoF problem into two coupled 3-DoF problems and merging their results. A method for gracefully degrading the output of the inverse kinematics is described for cases where the desired end effector pose is outside the configuration space. This is useful for settings where the user is in the control loop and can help the robot to get closer to the desired location. The design of the handheld robot is offered as open source. While our results and tools are aimed at handheld robotics, the design and approach is useful to non-handheld applications.'\\n\\n'The prototype arm links were 3D printed in ABS and the idler pulleys machined out of aluminum. The CAD files for all the components are open source and made available at [9]. The robot is powered by six Dynamixel MX-64T servos mounted on cable tensioning mechanisms. After the links and pulleys have been assembled, the driving cables are attached directly to each motor. The motors are mounted on a 3D printed saddle that fits into a corresponding bed shown disassembled in figure 6a. The motor is moved forward and backward in the bed by a bolt that acts as a lead screw. Figure 6b illustrates how the motors are arranged in a sideways \\u201cV\\u201d formation so that the motors can be individually tensioned without the cables crossing. When the arm is first turned on, it is calibrated by controller firmware recording the offset between the known zero angle of the arm, and the current angle recorded by the motor encoders. This calibration must be repeated every time the cables are de-tensioned or re-tensioned during maintenance.'\\n\\n\\\"We present a novel design optimized for the nascent area of handheld robotic applications through the use of link to link space carving. The inverse kinematics are solved analytically by splitting the 6-DoF problem into two coupled 3-DoF problems and merging the results. A method for gracefully degrading the number of DoF that the arm reaches when the desired end effector pose is outside the arm's configuration space is also proposed. The arm design and CAD files are released as open source hardware at [9].\\\"\",\"250\":null,\"251\":\"'Cable-length control is achieved through the integration of four BLDC motors, four Texas Instruments DRV8332 MOSFET triple half-bridge modules, and a 16 bit dsPIC33EP256MU810 running at 70 MIPS. In lieu of hardware quadrature encoders, hall effect sensors trigger interrupt requests on the microcontroller which are handled by a software odometry and block commutation routine. Additionally, ADC channels are used to sample the bulk current through each motor to prevent overheating and to infer rough tension estimates.'\\n\\n\\\"At the time of writing, DuCTTv2's highest level control code is run open loop in MATLAB which sends rest length commands to the motor boards through the 802.15.4 layer at a rate of 50 Hz. Future work will see more advanced embedded processors integrated into the physical prototype to reduce offboard computation requirements.\\\"\\n\\n\\\"The NASA Tensegrity Robotics Toolkit (NTRT) is an open-source library developed by the Intelligent Robotics group at NASA Ames to facilitate simulation of robots based on tensegrity principles [13]. It is built to run on top of the open-source Bullet Physics Engine [14]. The developers have added many tensegrity specific features to the engine, with the goal of creating a reliable engineering tool for the design and control of tensegrity robots. As the native Bullet supplied soft body models were not physically realistic, NTRT has incorporated a custom designed linear cable model using Hooke's law forces to ensure analytic accuracy [15]. Other features include builder tools, motor models, controllers, and machine learning modules. Prior work has validated NTRT against other tensegrity robot hardware implementations [15]. We aim to utilize these features to realistically simulate the robots interaction with itself and the environment, so we constructed a model of DuCTT within NTRT (Fig. 5).\\\"\",\"252\":null,\"253\":null,\"254\":\"'Unifying the three sub-graphs produces a closed-loop framework for robots learning from demonstrations. Moreover, graphs can store relationships in an intuitive and highly regular structure, allowing for algorithms that rely on simple graph manipulations. The real world is encoded through perception into the S-AoG to form a physical belief state of the world. The learning algorithm constructs a C-AoG to understand actions from human demonstrations. And lastly, inference combines the reasoning from the C-AoG and the actuators from the T-AoG to physically perform the task. The energy of the joint parse graph [12] combines the energy terms of each:'\",\"255\":null,\"256\":\"'Deep metric learning autoencoder for nonlinear temporal alignment of human motion'\\n\\n'Temporal alignment is an important preprocessing procedure for human action recognition. The challenge of temporal alignment problem is the temporal scale difference between human actions as well as the variability of each subject. Metric learning is the central problem of temporal alignment. This paper presents a nonlinear time alignment method with deep autoencoder. The spatio-temporal features obtained from the neural network contain the metric information for feature comparison. The effectiveness of our method is verified with k-nearest neighbor (k-NN) classifier on MSR-Action 3D and MSR-Daily Activity 3D datasets. Experimental results illustrate that the proposed method achieves superior performance to other metric based techniques.'\\n\\n'In this paper, we propose a parametric nonlinear time warping method based on deep autoencoder. The main contributions of this paper include the following aspects. First, the nonlinear time series alignment method is introduced. Second, the nonlinear metric function is learned from interclass and intra-class variations. Spatio-temporal features are projected to the common space for comparison. There is no need to learn the projection for each time series. Last, the temporal alignment is optimized with the weakly labelled spatio-temporal features at each time step.'\\n\\n'In this section, we briefly review the previous work on the Mahalanobis distance, deep autoencoder neural network and canonical time warping (CTW) method.'\\n\\n'B. Deep Autoencoder'\\n\\n'Autoencoder can reconstruct the original signal with joint distribution of visible units and hidden units. It is a nonlinear feature learning method to extract the features on nonlinear manifold [12].'\\n\\n'Our method can be considered as a nonlinear extension of canonical time warping method. Spatio-temporal features of the time series are obtained from the deep autoencoder. The generated features contain metric information at the same time.'\\n\\n'The autoencoder framework works as a nonlinear feature learning method to capture the manifold. The\\\\nJ\\\\nAE\\\\nterm aims to map the data from the high dimension to the lower one. The objective function of autoencoder is'\\n\\n'Normally, deep autoencoder is optimized using backpropagation to minimize squared reconstruction error [5] after pre-training. The deep network can also be trained with Hessian-free optimization method [17] without pretraining in advance. The pre-training procedure of deep neural network makes limited improvement for ReLU activation function. Another advantage of ReLU activation function is that neural network can achieve the global minimization through optimization [18]. Those are the reason that ReLu activation function is chosen in our method.'\\n\\n'The deep autoencoder network applied in our method. The input of the network\\\\nX(t)=[x(t\\u2212n\\\\n)\\\\nT\\\\n,\\u2026,x(t\\u22121\\\\n)\\\\nT\\\\n,x(t\\\\n)\\\\nT\\\\n]\\\\nT\\\\nis the concatenation of several successive joint positions.'\\n\\n'Algorithm 1 Nonlinear Time Series Alignment with Deep Metric Learning Autoencoder'\\n\\n'We also compare our method with canonical correlation analysis based methods after explicitly temporal aligning the time series to the same length in Euclidean space. Linear canonical correlation analysis (Linear CCA), kernel canonical correlation analysis (KCCA, Gaussian RBF kernels), deep canonical correlation analysis (DCCA) and deep canonical correlation analysis autoencoder (DCCAE) are compared with our method. These methods project the training samples with linear or nonlinear transformation. Frobenius norm between pairs of data in new space is the correlation of each other [28]. 1-Nearest Neighbor (1-NN) method is adopted for human action classification in our experiments.'\\n\\n'In this paper, we propose a parametric nonlinear metric learning method with deep autoencoder for human action recognition. The metric is learned through the temporal alignment of labelled time series. The spatio-temporal features of time series are obtained with metric information for comparison. From the experimental results, our method outperforms other metric based comparison methods on MSR-Action 3D and MSR-Daily Activity 3D datasets.'\",\"257\":\"'In addition to the grasps, the dataset also contains simulated point clouds that are reconstructed from multiple viewing angles distributed on a sphere around the object centroid. From each point cloud, a set of local shape templates is extracted that essentially encode object shape as seen from the hand (Fig. 4). Apart from object surface information, it also contains information about free and occluded space. Thus a template can be interpreted as an image with 3 color channels. The first channel represents the surface points of the object projected onto the plane spanned by the surface normal. The second channel represents the occluded space which is computed based on the viewpoint and the surface points. Points are again projected onto the same surface plane. Cells in the grid on the surface plane which are neither filled by surface points nor by occlusion points are marked as free space. Each template is linked to exactly two grasp poses that only differ in the initial distance between the palm of the hand and the object surface (the stand-off). The surface normal of a template is equal to the approach vector of the hand. One grasp can however be linked to multiple templates as its associated object surface normal may be visible from multiple viewpoints. An example template representation is shown in Fig. 4. This figure also visualizes different 3D versions of grasp templates for one grasp.'\",\"258\":null,\"259\":\"'The goal of binary descriptors extracted from image patches is to obtain distinctive binary codes that can be used efficiently in image matching applications. Similar image patches should have similar binary codes with a small Hamming distance; dissimilar image patches should lead to binary codes with larger Hamming distances. This goal is the same as in binary LSH schemes [24], [25] that approximate distance computation by first applying hash functions to larger data vectors and then computing the Hamming distance between the resulting binary vectors. The goal in LSH is to map similar objects to similar hash codes with high probability. In contrast, local binary descriptors directly build short binary descriptors by comparing the intensities of pairs of points without ever creating a long descriptor.'\\n\\n\\\"That is, the expectation of the Hamming distance between two binary hash codes of two image patches\\\\np\\\\n1\\\\nand\\\\np\\\\n2\\\\nis an unbiased estimate of Kendall's tau distance between them up to a constant scale factor\\\\nK\\\\n. Then the variance of the normalized Hamming distance can be shown to satisfy\\\"\",\"260\":\"'In this paper, we explore various aspects of fusing LIDAR and color imagery for pedestrian detection in the context of convolutional neural networks (CNNs), which have recently become state-of-art for many vision problems. We incorporate LIDAR by up-sampling the point cloud to a dense depth map and then extracting three features representing different aspects of the 3D scene. We then use those features as extra image channels. Specifically, we leverage recent work on HHA [9] (horizontal disparity, height above ground, and angle) representations, adapting the code to work on up-sampled LIDAR rather than Microsoft Kinect depth maps. We show, for the first time, that such a representation is applicable to up-sampled LIDAR data, despite its sparsity. Since CNNs learn a deep hierarchy of feature representations, we then explore the question: At what level of representation should we fuse this additional information with the original RGB image channels? We use the KITTI pedestrian detection dataset for our exploration. We first replicate the finding that region-CNNs (R-CNNs) [8] can outperform the original proposal mechanism using only RGB images, but only if fine-tuning is employed. Then, we show that: 1) using HHA features and RGB images performs better than RGB-only, even without any fine-tuning using large RGB web data, 2) fusing RGB and HHA achieves the strongest results if done late, but, under a parameter or computational budget, is best done at the early to middle layers of the hierarchical representation, which tend to represent midlevel features rather than low (e.g. edges) or high (e.g. object class decision) level features, 3) some of the less successful methods have the most parameters, indicating that increased classification accuracy is not simply a function of increased capacity in the neural network.'\",\"261\":null,\"262\":null,\"263\":null,\"264\":null,\"265\":null,\"266\":\"'To evaluate our method, we have collected a novel dataset of tabletop scenes. Unlike most such datasets which contain single RGB-D frames, we reconstruct high-quality pointclouds of the scenes observed from multiple points using an open source implementation of the KinectFusion algorithm [4]. Additionally, in order to keep track of occlusions, we construct a volumetric occupancy map for each of the scenes [5]. Evaluation on this dataset shows that the use of the symmetry constraint allows our approach to correctly and accurately segment complex scenes containing non-convex objects as well as multiple objects in stacked or touching configurations while approaches that rely on convexity as the primary grouping principle struggle in such scenarios.'\",\"267\":\"'http:\\/\\/vision.fe.uni-lj.si\\/~rokm\\/icra2016\\/household_room_dataset'\\n\\n'A service robot that operates in a previously-unseen home environment should be able to recognize the functionality of the rooms it visits, such as a living room, a bathroom, etc. We present a novel part-based model and an approach for room categorization using data obtained from a visual sensor. Images are represented with sets of unordered parts that are obtained by object-agnostic region proposals, and encoded using state-of-the-art image descriptor extractor - a convolutional neural network (CNN). An approach is proposed that learns category-specific discriminative parts for the part-based model. The proposed approach was compared to the state-of-the-art CNN trained specifically for place recognition. Experimental results show that the proposed approach outperforms the holistic CNN by being robust to image degradation, such as occlusions, modifications of image scaling, and aspect changes. In addition, we report non-negligible annotation errors and image duplicates in a popular dataset for place categorization and discuss annotation ambiguities.'\",\"268\":\"'Intuitively, by comparing the 3D scene reconstructions against the fully-3D model representations, the robustness of 3D object recognition as well as the accuracy of the estimated 3D object pose can be improved. Based on this, our work proposes a framework that deploys simultaneously a SLAM algorithm to reconstruct the environment, an incremental segmentation algorithm to obtain 3D segments of such reconstruction in real-time, and an incremental 3D object recognition algorithm that carries out descriptor matching out of such segments. The goal is to demonstrate that we can exploit multiple viewpoints around the same scene to achieve robust recognition and extremely stable 3D poses in presence of heavy clutter and occlusion at a high efficiency. At the same time, by matching segments that encode the fully-3D shape of our object directly against the fully 3D model, we can be robust towards noise and artifacts that affect the current scene frame. Moreover, being the number of global descriptors that need to be computed equal to the number of such scene segments, the overall process is remarkably efficient, yielding near real-time performance. We validate our proposal by evaluating qualitatively its performance on a benchmark dataset for 3D object recognition in clutter. In addition, we also test our framework within an augmented reality application, so as to demonstrate its usefulness and effectiveness in real scenarios.'\",\"269\":\"'A feature extraction applied to the RGB image that encodes the color of each pixel and its neighborhood at different scales, averaged using superpixels. The method is more extensively explained in [6]. The feature extractor is applied on the whole input frame, and returns a 39 dimensions feature vector for each pixel. The feature extraction is much faster to compute than the geometric segmentation, and is available everywhere.'\",\"270\":null,\"271\":\"'In this paper, we encode the depth representation as a combination of depth image and normal vector image, and then the RGB-D input has 7 channels for each image as shown in Figure 2.'\\n\\n'Object occurrence + SVM. Assuming the ground truth of object occurrences is known in every image, each image can be represented by a binary encoded vector with length\\\\nM\\\\no\\\\n, with 1 denotes the object is contained in the image and 0 otherwise, and\\\\nM\\\\no\\\\nis the number of object classes in the whole dataset. A linear SVM is employed for classification.'\",\"272\":null,\"273\":\"'https:\\/\\/youtu.be\\/te_bmik2tps'\\n\\n\\\"Snapshots from the on-line calibration procedure applied to real-world sequences involving an actuated camera (A-D), a static camera on the robot (E-H), and a static camera external to the robot (I\\u2013J). The robot's right arm is not shown in panel (J) Since it was broken at the time of the experiment and the joint encoder offsets were unknown. The full sequences are shown in the supplemental material video.\\\"\",\"274\":null,\"275\":null,\"276\":null,\"277\":null,\"278\":null,\"279\":null,\"280\":null,\"281\":null,\"282\":\"'The 6-axis force\\/torque sensor is WEF-6A200-4-UG5 type of WACOH-TECH Inc. that can measure up to 200N force and 4 Nm torque with sampling rate of 2 ksamples\\/s. The readings can be obtained via USB (as a virtual COM port), that is recorded at 100 samples\\/s by a python code on a PC.'\",\"283\":\"'Master and slave sides receive as an input a power variable (i.e. force or velocity) and, therefore, at each time step it is necessary to decode the information contained in the incoming power wave for computing the desired input and the outgoing power wave. Thus, for the master port with an admittance causality and the slave port with impedance causality we have that:'\",\"284\":null,\"285\":null,\"286\":null,\"287\":null,\"288\":\"'Object Detection And Autoencoder-Based 6d Pose Estimation For Highly Cluttered Bin Picking'\",\"289\":\"'https:\\/\\/youtu.be\\/zsmrcqsTPGQ'\",\"290\":\"\\\"A robot can feasibly be given knowledge of a set of tools for manipulation activities (e.g. hammer, knife, spatula). If the robot then operates outside a closed environment it is likely to face situations where the tool it knows is not available, but alternative unknown tools are present. We tackle the problem of finding the best substitute tool based solely on 3D vision data. Our approach has simple hand-coded models of known tools in terms of superquadrics and relationships among them. Our system attempts to fit these models to point clouds of unknown tools, producing a numeric value for how good a fit is. This value can be used to rate candidate substitutes. We explicitly control how closely each part of a tool must match our model, under direction from parameters of a target task. We allow bottom-up information from segmentation to dictate the sizes that should be considered for various parts of the tool. These ideas allow for a flexible matching so that tools may be superficially quite different, but similar in the way that matters. We evaluate our system's ratings relative to other approaches and relative to human performance in the same task. This is an approach to knowledge transfer, via a suitable representation and reasoning engine, and we discuss how this could be extended to transfer in planning.\\\"\\n\\n'Our system has three essential components: (i) Hand-coded models of ideal objects for tasks; this is similar to several works in this area defining how objects are composed from parts [6]\\u2013[8]. (ii) Task models that specify the relative importance of various parameters of the model for a particular task. (iii) A matching algorithm that tries multiple fits in the point cloud, converging toward that preferred by the task model. We evaluated our approach on 3D scans from everyday household objects, comparing our system with ground truth from performing the tasks, human ratings and also with two variants of an approach based on histogram features of shapes of parts. Our results show high accuracy. Our main limitation is that we did not discover the relative importance of parameters of the models from data by machine learning; instead we have crude hand-coded rules in the task model. A data driven approach would be superior at finding optimal parameters.'\\n\\n'Note that while the set of objects is not huge they are highly varied. Note also that these objects are the \\u2018test set\\u2019 there is no training set, as no learning was used, instead the source model of tool and task are handcoded.'\\n\\n'We asked humans to rate the usefulness of the 20 test objects for each of the tasks on a 1 to 10 scale as follows: 1\\u20135 Does not work; 6\\u20138 Could work, but not the best; 9\\u201310 Works best. We decided to have ten numeric values in order to capture such distinctions as a tool that almost works, versus one that is utterly useless. Humans only got to see 2D rendered point cloud images. For some objects we included two images where one perspective did not make the shape clear. Images of a human performing the task on a target object were also provided. We also explained the action for example specifying the swinging type of action is important or else the human may consider alternate actions like thumping downwards while gripping the screwdriver with base pointing downwards, which do not correspond to the task relationships coded in our system. In order to focus on shape and not materials we told humans to assume all objects were made of solid metal. We averaged the ratings across the humans.'\\n\\n'We now consider some future improvements that could be made. Our task model rules are crude in that they apply a quadratic, or linear or flat function to parameters. A better approach would be to use machine learning from a good dataset to do fine-grained tweaking of weights on function parameters. This means learning in what ways a component of the source model can be allowed to morph its shape (or change scale) while still being effective. Learning could also be used for a more ambitious extension. Currently our source models are hand-coded. We see this as an important first step. It allows us to test our hypothesis about what is a good representation for a source model. Having completed this we can tackle learning of a source model, given a set of geometric primitives to be used in the modelling, and a training set of objects for each function.'\",\"291\":null,\"292\":\"'Histogram of distances (HoD) concept. (a) A spherical area of radius r (red) centred on a keypoint is extracted. (b) One random border point from the local area is selected as reference point (yellow) and the reference point to vertices L2-norm distances are calculated (in red as example). (c) The coarse and fine normalised distances were encoded into a histogram of distances'\",\"293\":\"'Example code for the robot controller sending a request to the DRVS Server API for object detection with a yellow HSV colour model.'\\n\\n'The robot and the camera nodes need a shared definition of the object type and parameter descriptions. In this experiment only two object description types were used, coded directly in the robot and the camera nodes. Standardised representations of object types and detection algorithms [15] may provide a well-defined but still flexible specification language. DRVS would use the ontology terms as labels for the services and algorithms in service requests.'\",\"294\":null,\"295\":\"'1) Sparse Auto-Encoder (SAE) Feature'\\n\\n'An autoencoder [11] is an unsupervised feature learning approach based on neural networks. The objective is learn a\\\\nD\\\\n-dimensional representation which can well represent the input data\\\\nx\\\\nof dimension\\\\nD\\\\nx\\\\n, where\\\\nD\\u226a\\\\nD\\\\nx\\\\n. This is achieved by learning a symmetric neural network which, in the first instance, progressively reduces the number of neurons until the middle layer conists of\\\\nD\\\\nneurons; this projects the input into a non-linear low-dimensional space. Layers after this middle layer then increase in size until it is the same size of the input data; this backprojects the low-dimensional representation. Thus, the objective is that the output of the network\\\\ny\\\\nmatches the input of the network\\\\nx\\\\n. This allows it to be trained in an unsupervised manner.'\\n\\n'An overview of local binary patterns are encoded. Using\\\\nP=8\\\\nsampling points on the circle of radius\\\\nR\\\\nthe local binary pattern values are evaluated and represented as a binary string. This binary string can then be converted to an integer value.'\\n\\n'To evaluate our systems we divided the annotated data into training, validation and test sets. The training set was used to train the models such as the CRF and auto-encoder. The validation set was used to choose the optimal system as well its parameters and a threshold\\\\n\\u03c4\\\\nwhich was then applied to the test set. The test set was then used to present the final performance of the system, neither the parameters nor the threshold\\\\n\\u03c4\\\\nwere tuned on this set. Using these three splits allows us to separate the optimisation of system parameters from the final evaluation of system performance. This differs to much of the previous work which optimised their system, parameters and thresholds on their test set and not on a separate validation set.'\\n\\n'For comparison, we use auto-encoder features similar to those used by Hung et al. [2], however, when using these features we use just the NIR imagery 4. This is because we incorporated colour as a separate feature using the HSV colour space for each pixel. It was not possible to compare to the prior work on sweet pepper segmentation conducted by Bac et al. [7] due to their use of a specialised six band multi-spectral camera.'\",\"296\":\"'http:\\/\\/mapir.isa.uma.es\\/'\",\"297\":\"'https:\\/\\/goo.gl\\/dSMCom'\\n\\n'https:\\/\\/goo.gl\\/dSMCom'\\n\\n'Effect of augmenting the activeness variables. We represent a data sample (black) on the left as a linear combination of the two dictionary atoms (green and red) on the right. Top row shows summing dictionary atoms with opposite velocities can numerically fit the data. This is undesirable because it makes the sparse codes less interpretable. Bottom row shows that augmenting the positive activeness variables addresses this problem, because when velocities of opposite direction add to reduce error, the activeness variables would add to increase error.'\\n\\n'For the pedestrian dataset, the ASNSC algorithm converged in 59 iterations within 4.98 seconds, and learned a 9 column dictionary and the associated sparse code. The learned dictionary correspond well to human intuition \\u2013 each dictionary atom specifies a motion pattern that either enters or exits the intersection. Visualization of all nine dictionary atoms and the corresponding projection are shown in fig. 6.'\",\"298\":\"'Pseudo-code for the segment of a path with a VS flag:'\",\"299\":null,\"300\":null,\"301\":null,\"302\":null,\"303\":null,\"304\":null,\"305\":null,\"306\":null,\"307\":null,\"308\":null,\"309\":null,\"310\":\"\\\"Range-only one-way travel-time (OWTT) cooperative underwater navigation uses ranges estimated from the acoustic time-of-flight between subsea nodes, e.g., between two vehicles or between a client vehicle and a server reference beacon of known (fixed or moving) location. When all vehicles and beacons (nodes) are equipped with precision clocks, each node's acoustic data transmission can be received by multiple receiving nodes\\u2014enabling all nodes within acoustic range to simultaneously (a) measure range to the transmitting node from the measured time-of-flight and (b) decode the data encoded in the acoustic data packet. This method provides both bounded-error position estimates and long range capabilities with reduced need for multiple costly fixed beacons, as is the case with most LBL systems. OWTT navigation also provides scalability by allowing all vehicles within acoustic range to simultaneously use the same acoustic data packet broadcast independent of the number of vehicles.\\\"\",\"311\":null,\"312\":null,\"313\":null,\"314\":null,\"315\":null,\"316\":\"'The dynamics characteristic of the device are evaluated to define the control strategy and to set the control parameters. The Bode diagram of both the joints of the device is obtained applying a chirp signal of torque and acquiring the displacement by motor encoder, as shown in Figure 3.'\",\"317\":null,\"318\":null,\"319\":null,\"320\":null,\"321\":null,\"322\":null,\"323\":null,\"324\":null,\"325\":null,\"326\":null,\"327\":null,\"328\":null,\"329\":null,\"330\":null,\"331\":null,\"332\":\"'Given the scenario of an operator wanting to modify running code during a rescue mission or similar critical tasks, there are several risks that have to be considered. Therefore, this section lists the main challenges which come along with identified risks and discusses provisions to be taken in order to address them.'\\n\\n'In order to address this challenge, several mechanisms need to be added for ensuring both, early avoidance and detection of errors, as well as robustness in case a behavior fails during runtime. A first approach for avoiding errors is relying on model-based behavior specification along with automatic code generation instead of manually written code.'\\n\\n'Finally, code generation addresses the challenge of operator distraction and reduces the likelihood of runtime failures. When specifying changes to a behavior, the operator necessarily takes the role of a software developer, equivalent to designing a behavior in advance. In order to reduce the cognitive load, keep the distraction imposed by making these changes low, and enabling operators with no expert knowledge to make adjustments, state machines are intuitively modeled based on available state implementations. From this model, the executable source code defining a behavior is then automatically generated. If more flexibility is required in certain situations, an expert operator is still able to make arbitrary adjustments in the source code afterwards.'\\n\\n\\\"Behavior to solve the DRC task of turning the valve, visualized by flexbe's statemachine editor. Even during execution, the structure can easily be re-arranged without manually writing code.\\\"\\n\\n'The presented approach has been implemented using ROS and is available as the open-source behavior framework FlexBE 3. It provides an extensive user interface for designing and executing behaviors as well as a runtime-safe onboard executive. As part of the evaluation of the capability to react to unforeseen situations, FlexBE has not only been evaluated using a carefully designed scenario, but also integrated into multiple systems and practically applied in two different robot competitions.'\\n\\n'In this paper, a novel high-level control approach incorporating operator collaboration has been presented and its main challenges have been addressed. Using our approach, the operator is able to influence mission execution of the robot when required and can even completely re-design the applied high-level strategy. These concepts have been implemented by the open-source behavior framework FlexBE.'\\n\\n'C. Code Generation'\",\"333\":\"'The major data packages in the information flow incorporates meter readings, robot motion commands and robot states. According to the system configuration, the controllers on site and the PC in distance have functions on different levels. For a meter reading, the controller examines the format of the raw data immediately after data acquisition. Generally, format errors indicate connection instability, battery problem or EMC issues. The controller only sends converted readings with smaller data package size to the PC, or gives error code as soon as format problem occurs. The PC has higher computation capacity thus its power is used in massive data processing, for instance, data threshold matching, maximum\\/minimum\\/average value computation and level evaluation. (Level evaluation means distribution analysis of the stud tension values in order to prevent MSTM platform tilt issues.) For robot control, the DMController provides real time motion control of the robot connected to it. There are 9 motion states while a robot travels from the storage position to the working position, vice versa. The controller receives command of moving a certain robot to a target position, then plans and exerts its own robot movement. If an error occurs during the procedure, the controller stops robot motion, sends a corresponding error code to the PC and waits for the PC respond to the abnormal situation.'\",\"334\":null,\"335\":null,\"336\":null,\"337\":\"'OCTOPUS can be teleoperated from remote locations. To enhance the perception ability of the operator, OCTOPUS is equipped with sensors that enable an operator to monitor the status updates of the robot. The joint angle is measured using potentiometers and a rotary encoder. OCTOPUS has inertial measurement units (IMUs) installed on the body, flippers, manipulators, and crawlers, and the 10-inch monitor on the console provides three-dimensional computer graphics to display the posture of OCTOPUS, as shown in Fig. 6(c). The posture-display system uses rviz, which works on the framework of robot operating system (ROS). Communication between the sensor and computer is based on socket communication using the UDP.'\",\"338\":\"'http:\\/\\/www.isir.upmc.fr\\/vid\\/compliant_crossing.mp4'\",\"339\":null,\"340\":null,\"341\":\"\\\"B. Laban Space Effort\\\\nThe purpose of the Space Effort is to indicate an agent's attitude toward its goal; it could have one clear goal, it could have several possible goals (as in a multi-person conversation) or it might be avoiding its goal or have no particular goal at all.\\\\nSpace Effort={\\\\nstarting position\\\\ntarget Gaussian(s)\\\\nView Source For simplicity, we have chosen to implement the robot's current goal as a distribution of one (Space=direct) or more (Space=indirect) Gaussians. The robot can sample this when selecting a new target orientation. We want to control people's first impression of the robots' Space setting, so we hardcode initial yaw orientation values.\\\"\",\"342\":\"'These results demonstrate the generalization ability of the learning approach and the ability of its features to encode the relevant influences for learning complex navigation behaviors of varying social normativeness. They also show that the robot is able to generate behavior that involves temporal credit assignment (deciding to take a longer path) in order to achieve a goal that was not specified in its demonstrated objective (mission completion metric).'\\n\\n'For simulation, we use an open-source 1 pedestrian simulator [22] in which agents are guided by potentials governed by the social force model [23]. The simulator provides realistic walking pedestrians as well as social groupings using one unified model. For both simulation and the experiments with the real robot we use the learned behaviors to generate costmaps, on which we plan global paths using weighted\\\\nA\\\\n\\u2217\\\\nthen drive the robot locally using an elastic band local planner. We use the move_base2 framework from ROS [24] to ensure we have the same configurations in simulation and on the robot, with only differences in low-level driving control. As such we implement new costmap layers akin to [12] in the move_base framework which then represent the new behaviors. This way the behaviors can be intepreted as a policies for motion planning. Using the standard\\\\nA\\\\n\\u2217\\\\nplanner from ROS [24] eliminates experimental variation, but advanced planning algorithms such as RRT can also be used in practice.'\",\"343\":\"'The sketched map is encoded in the robot a s a grid map, while the path is stored as a set of waypoints obtained by listening to touch events on the tablet. We interpret the initial position of the robot as the starting point of the path and set the initial orientation by estimating the direction of vector from the starting point to the next consecutive point beyond a preset threshold distance. This was done to avoid small squiggles in the beginning that affect the direction computation.'\",\"344\":\"'http:\\/\\/www.ros.org\\/'\\n\\n'github.com\\/rsait\\/rsait_public_packages'\",\"345\":null,\"346\":null,\"347\":null,\"348\":null,\"349\":\"'http:\\/\\/flyingmachinearena.org\\/people\\/'\",\"350\":\"'https:\\/\\/www.youtube.com\\/watch?v=hcFK32C-bxs'\\n\\n'In the case where the task is simple enough, we can encode it using only constraints. Starting with initial guess which does not respect the system dynamic but respects the initial and final constraints (quasi-static trajectory), the algorithm is able to find a valid trajectory after few iterations (convergence criteria: KKT conditions under 10\\u221212). The computation times for different systems\\/tasks can be found in Tab. I (trajectories of 20 shooting nodes over 8 seconds).'\\n\\n'In this part, system starts and ends at chosen positions and, on its way, has to pick an object at a certain position then place it to an other one. No obstacles are added to the problem so the object is considered as flying in the air. The task is encoded as follow: the initial and final states are set using initial and final constraints and picking\\/placing tasks are encoded using the cost function'\\n\\n'The multiple shooting algorithm has been implemented by Moritz Diehl et al. in an open source optimal control software, the ACADO toolkit [21]. ACADO solves multiple shooting problems thanks to a Sequential Quadratic Programming (SQP) algorithm, together with state-of-the-art techniques to condense, relax, integrate and differentiate the problem. This tool has already been shown to be useful to generate complex quadcopter trajectories as throwing and catching an inverted pendulum [8]. Trajectories presented in the following parts have been generated using this toolkit.'\",\"351\":\"'https:\\/\\/www.bitcraze.io\\/crazyfiie\\/'\",\"352\":null,\"353\":null,\"354\":null,\"355\":null,\"356\":\"'In order to estimate the pose of the vehicle and enable its automatic control, the following sensors were used: An Inertial Measurement Unit (IMU) with integrated 3 axes accelerometer and gyroscope to measure the accelerations and angular velocities that are acting on the system, a magnetometer to measure the magnetic flux, a GPS module for position updates, a barometer to assess the atmospheric pressure and a differential pressure sensor to determine the velocity of the airstream. All the information gathered from the sensors is then fused to estimate the pose using the Extended Kalman Filter (EKF) of the Pixhawk open source software stack [13].'\",\"357\":\"'We designed a randomized cross-over study that compared the effectiveness and feasibility of three underlying methods of robotic coaching. All three methods are based on on-line adaptation of the learned periodic motion, encoded with a dynamic movement primitive.'\",\"358\":\"'The implementation has been realized on a computer with an i7-4600U processor running Linux. All the code was written in C++ using the Knowbotics framework, currently developed at LIRMM 4. The FRI was used to communicate with the Kuka arm, and a ROS interface 5 to control the Shadow Hand and get the BioTacs measures. Kinematic computations were performed using the Robotics Library 6. The controller sample time was\\\\nT\\\\ns\\\\n=5\\\\nms and the average computation time required for each loop was 0.3 ms. To generate the arm reference trajectory\\\\nx\\\\n\\u02d9\\\\nr\\\\n, and for the hand OTG, we use the Reflexxes library 7.'\",\"359\":null,\"360\":\"\\\"Matlab's Simulink Coder\\\"\",\"361\":\"'The continuously variable transmission (CVT) which is commercially available produced by Fallbrook Technologies Inc. for bicycle, is attached to the rear wheel. The servo brake is also attached to the real wheel. The angular velocities of three wheels are measured by the encoders, and the slope angle is measured by the inclination sensor. The torque sensor is also attached to the pedal which is used for only the evaluation of the experiments. The system has a controller and it can change the gear ratio of the CVT and the braking torque of the servo brake in real time.'\",\"362\":null,\"363\":null,\"364\":null,\"365\":null,\"366\":null,\"367\":null,\"368\":\"'http:\\/\\/www.brain4cars.com\\/'\",\"369\":null,\"370\":\"'When autonomous robots are deployed in an industrial setting they are expected to work 24 hours a day, 7 days a week. Therefore, dependability of the robots is crucial. In this paper we present an approach following the model-driven engineering idea that supports dependability in different stages of the live cycle of robots. In particular we present how model-based testing and diagnosis can be used for this goal and how suitable models for these approaches can be obtained. The proposed approach was evaluated in a real industrial use-case showing superior performance compared to the hand-coded solutions used before.'\\n\\n\\\"The work presented in this paper is mainly concerned with the second and fourth stage respectively implementation and operation. Currently, during implementation the robot systems are manually tested using ad-hoc test cases. Using formal models of the robot's behavior obtained from requirements and other design information and methods from model-based testing we were able to automate the testing while automatically achieving more fine-grained tests revealing a number of previously hidden problems. During operation so far a number of hand-coded plausibility checks are used to monitor the proper function of the system. By reusing the above models of the nominal system behavior, information from fault mode and effect analysis and model-based techniques a more systematic monitoring and diagnosis of the systems has been achieved.\\\"\",\"371\":null,\"372\":\"'https:\\/\\/youtu.be\\/ImIdvApWLgQ'\",\"373\":null,\"374\":\"'The implemented solution was coded in C++, using the Ceres library [2] as the optimization engine to solve equation 3. A straightforward outlier rejection was implemented based on the residuals obtained after a first optimization call, as well as using a Cauchy loss function. The stereo radar odometry estimate ouput was computed at 5 Hz, so each iteration considered all detections read during the last 200 ms interval, and used the previous solution as an initial guess.'\\n\\n'Such qualitative results illustrate that stereo radar odometry approach is a promising technique, providing accuracy at the level of a wheel encoder system, but able to observe the full 2D twist (lateral velocity also). Moreover, these results also demonstrates the relevance of sensor placement to achieve an accurate odometry estimate.'\",\"375\":null,\"376\":\"'http:\\/\\/youtu.be\\/3FER84XD17w'\\n\\n\\\"CNNs can be viewed as an enhancement of the standard Multi-Layer Perceptrons (MLPs), where the main difference is the addition of convolutional layers. Such layers perform a similar operation as a standard image convolution but instead of using hard-coded kernels or filter banks, these parameters are learned through backpropagation jointly with the rest of the network. Such layers allow the training of much larger (deep) networks and are able to learn directly from images (or raw input in general) avoiding, therefore, the need for task-specific hand-crafted features. A convolutional layer can be defined by the number of filters\\\\n(\\\\nN\\\\nf\\\\n)\\\\n, the filters' size\\\\n(\\\\nF\\\\nx\\\\n\\u00d7\\\\nF\\\\ny\\\\n)\\\\nand their stride\\\\n(\\\\nS\\\\nx\\\\n\\u00d7\\\\nS\\\\ny\\\\n)\\\\n. The number of learnable parameters of a layer is\\\\nI\\\\nc\\\\n\\u00d7\\\\nN\\\\nf\\\\n\\u00d7\\\\nF\\\\nx\\\\n\\u00d7\\\\nF\\\\ny\\\\n+\\\\nN\\\\nb\\\\n, where\\\\nI\\\\nc\\\\nis the number of channels of the previous layer and the final\\\\nN\\\\nb\\\\nrefers to the additional bias parameter for each filter. Given an input with size\\\\nI\\\\nc\\\\n\\u00d7\\\\nI\\\\nx\\\\n\\u00d7\\\\nI\\\\ny\\\\n, the output size can be calculated according to:\\\"\",\"377\":null,\"378\":null,\"379\":null,\"380\":null,\"381\":null,\"382\":null,\"383\":null,\"384\":null,\"385\":null,\"386\":null,\"387\":\"'http:\\/\\/flyingmachinearena.org\\/people\\/'\",\"388\":null,\"389\":\"'http:\\/\\/flyingmachinearena.org\\/'\",\"390\":null,\"391\":null,\"392\":\"'Our subjects reported natural control of the system through the use of detecting long natural fixations to encode movement commands, avoiding the cumbersome procedure of learning gaze gestures. Moreover, user visual attention was always on the scene of their interactions, and not distracted by a screen-based user interface. Binocular eye tracking allowed us not only to perform 3D gaze measurement, but also to add a further dimension. We used voluntary \\u2018winks\\u2019 rather than natural occurring blinks and to include extra action selection features to our system: here, opening and closing of the robot gripper. New commands could be easily implemented including the detection of left vs right \\u2018winks\\u2019 independently or sequences of them, that may potentially add complexity and applicability towards prosthetic control.'\",\"393\":\"\\\"Probabilistic reasoning: Topic models are applied to reason about human activities, which are trained offline and used online. The training set is provided as a prior that encodes a history of sensory information. This module uses the proposed indicators to select topic models that better match human's perspective, and to discover new activities in an online fashion.\\\"\\n\\n\\\"The gap's width mainly depends on the observation's novelty, in terms of the novel activity's similarity to the activities in the training dataset. This similarity is encoded by the portion of overlapping features. A more novel activity is generally represented by a set of more distinct visual features with less overlapping with the features existing during training, which generally results in a larger gap. For example, activities in the Weizamann dataset share fewer motions and thus contain a less number of overlapping features, which leads to a larger gap. Third, when the dictionary size increases, the model's\\\\nPvwp\\\\nvalues decrease at a similar rate. This is because in this case, the probability of a specific codeword appearing in an instance decreases, resulting in a decreasing Pvwp value.\\\"\",\"394\":null,\"395\":\"'We are currently cleaning up our code and it will be made publicly available at the time of the final version of this paper, if it is accepted.'\",\"396\":null,\"397\":\"\\\"Algorithm 1 shows pseudocode for our approach, generating a belief distribution over the possible current states\\\\nbel(\\\\nx\\\\nt\\\\n)\\\\n, while Figure 3 shows an example of the system's execution. The person's speech is ambiguous, and the system initially infers an approximately bimodal distribution between the two bowls. The robot does not hand over any object, which elicits a disambiguating response from the person, who points at the appropriate object. The model incorporates information from language and infers the person is referring to the blue bowl.\\\"\",\"398\":\"'As with the original algorithm, we check for an edge between two nodes only if those nodes are nearby. This is computed using the number of rotations needed to go from one node to another (as a maximum threshold parameter of the algorithm). To compute this difference, we encode our chains as a vector of position (1 straight, 2 top, 3 bottom, 4 back, 5 front) that represents the relative position of each module to its previous neighbor. For example a straight chain of 5 modules with the last module placed on the bottom of the previous one is {1,1,1,3}. The difference between two vectors tells how many rotations are needed to go from the first chain to the other one and thus decide if the chains are nearby.'\\n\\n'We used handwritten chains to test our code. For a designer of an application, the Motein algorithm [7] can help in automating the chains corresponding to multiple final and initial shapes (Fig. 11).'\",\"399\":null,\"400\":null,\"401\":\"'The commercial ROBODOC system (THINK Surgical, Inc., Fremont, CA, USA) has a solid set of safety features that have obtained FDA approval and EU CE marking, and has been in clinical use since 1992. As part of a collaborative research project with the manufacturer, we have one research ROBODOC system and have full access to the source code of the original product software. We created the research system by adapting the commercial system software to the cisst component-based framework, which required an architectural style change from the monolithic architecture to the framework-based architecture. We then introduced the SAFECASS layer to the system by (1) decomposing safety features into reusable mechanisms (i.e., filters) and JSON specifications, and (2) performing code-level refactoring to benefit from SAFECASS services. With these changes, the system achieved the SAFECASS-based architecture.'\\n\\n'The next step is to update the structure of the CONTROL component to take advantage of the state-based semantics of the GCM. Below is a highly simplified code of the original control loop of the component (Run() method) with focus on the force-based safety feature:'\\n\\n'Medical and surgical robot systems are examples of safety-critical systems due to the potential hazards that can lead to severe injury or the loss of human life. Furthermore, robot safety is becoming increasingly important in other domains as robots begin to share their workspace with humans. At the same time, the complexity of robot hardware and software has been increasing to endow them with capabilities for safe human-robot collaboration and other demanding tasks. While component-based frameworks have been widely adopted in robotics to manage the increasing scale and complexity of these systems, it is still challenging for both academic researchers and industrial developers to develop robot systems with safety in a reusable and structured manner. To address these issues, we reformulate safety as a visible, reusable, and verifiable property, rather than an embedded, hard-to-reuse, and hard-to-test property that is tightly coupled with the system. Specifically, we present a state-based model and a safety-oriented software architecture that allow us to reuse the design of safety features across different systems and different applications without relying on a particular component model. To demonstrate and evaluate the proposed methods, we built SAFECASS, an open source software framework that enables reuse of experience and knowledge on safety, and applied it to two medical robot systems, one of which is presented in this paper.'\\n\\n'This work presented an open source software framework, Safety Architecture for Engineering Computer-Assisted Surgical Systems (SAFECASS), that supports the development and implementation of safety mechanisms by enabling reuse of knowledge and experience on safety. Essentially, SAFECASS aims to improve reusability of the design of safety features. The two key design concepts of SAFECASS are framework independence and safety design decomposition. SAFECASS relies on the generic component model (GCM) to represent the design of safety features. The GCM defines the internal processing pipeline that is comprised of inputs, filters, events, and states. All components and their structural elements (i.e., provided and required interfaces) in the application are represented by instances of the GCM within SAFECASS. These \\u201cproxy\\u201d objects maintain the safety states of the application components, thus avoiding the need to modify the component-based framework used within the application. Safety design decomposition facilitates reuse of safety features by providing generic safety mechanisms in the SAFECASS layer that can be configured from the application and framework layers via JSON format configuration files. SAFECASS also provides tools to access the internal processing pipeline, thereby enabling services for the system designers, such as system introspection, fault injection, event generation, and run-time system status visualization. To the best of our knowledge, no existing framework in robotics provide a comparable architecture and tools to facilitate the design and development process for safety features.'\",\"402\":null,\"403\":\"'Learning by instruction allows humans programming a robot to achieve a task using spoken language, without the requirement of being able to do the task themselves, which can be problematic for users with motor impairments. We provide a developmental framework to program the humanoid robot iCub without any hand-coded a-priori knowledge about any motor skills. Inspired by child development theories, the system involves hierarchical learning, starting with the human verbally labelling robot body parts. The robot can then focus its attention on a precise body part during robot motor babbling, and link the on-the-fly spoken descriptions of proto-actions to angle values of a specific joint. The direct grounding of proto-actions is possible through the use of a linear model which calculates the effects on the joint of the proto-action and the body part used, allowing a generalisation of the proto-action if the joint has never been used before. Eventually, transferring the grounding is allowed via learning by instructions where humans can combine the newly acquired proto-actions to build primitives and more complex actions by scaffolding them. The framework has been validated using a humanoid robot iCub, which is able to learn without any prior knowledge: 1) the name of its fingers and the corresponding joint number, 2) how to fold and unfold them and 3) how to close or open its hand and how to show numbers with its fingers.'\\n\\n\\\"We first present the humanoid robot iCub used for the experiment and describe the different components of the architecture. Then we explain the semantic grounding of body parts using motor babbling from the robot and spoken descriptions from the human. We present the motor babbling procedure, and how a human describes proto-actions done by the iCub on-the-fly. Using the relation between a named bodypart and the joint, the robot can analyze the encoder values to extract the meaning of the proto-action (e.g. the velocity to apply and\\/or the angle value to reach). These proto-actions (e.g. fold thumb, unfold index) are generalized to other body parts or combined to create more complex actions, like closing or opening the iCub's hand.\\\"\\n\\n'Now that the robot knows the name of its body parts, it investigates what kind of actions can be done with them. Knowing the name of the body part is needed to lead the attention of the human to the corresponding limb. It is also important for the robot in order to reduce the complexity of the following reasoning method: instead of extracting all the available encoder values, the iCub is able to create a subset of only the named joints to check for the effects of the proto-action. Thus the complexity is reduced from 53 DOF (all the degrees of freedom of the iCub) to potentially 1 DOF.'\",\"404\":\"'http:\\/\\/robobench.net\\/'\\n\\n'As the name suggests, continuous integration (CI) testing is a software engineering practice where code committed by developers is frequently tested to ensure they produce correct results. CI testing can encompass testing of multiple programs working together as a robotic system. By requirement, a body of code and dependencies that is able to pass CI testing is able to compile and run without human invention, and therefore ready to be captured in a container.'\\n\\n\\\"Sampling based software profiling is not well supported inside virtual environments. In our experience OProfile most reliably identifies the source code for function call samples. The competing Linux perf tool supports sampling within KVM based virtual machines, but not in container-based machines such as Docker containers. When using perf within containers, perf delays resolving source code information to after sampling is complete, looking in file system locations which may have been removed by Docker's union file system; therefore, perf does not reliably generate source-level information.\\\"\\n\\n'A large amount of computing energy is spent in infrastructure code'\",\"405\":null,\"406\":null,\"407\":null,\"408\":null,\"409\":\"'The pose error obtained at the convergence using high resolution robot encoders supplied by the simulation software, was measured to be\\\\n\\u0394\\\\ne\\\\nfinal\\\\n(mm,deg)=(0.1,\\u00d7,0.2, 0.02,0.18,0.08)\\\\n. Figure 6 (left) shows the pose error decay for each DOF when Figure 6(right) illustrates the velocities sent to the robot DOFs. After about thirty seconds, we can see that the system has converged smoothly.'\\n\\n'Before the proposed approach can be validated on the experimental work-flow, it was tested using a developed C++ simulator which was very appropriate to validate the theoretical developments in different conditions of use without any risk of damaging the US probe. The simulator, mimicking a 2D US probe on an abdomen phantom, was implemented using the: Open-source Visual Servoing Platform (ViSP)1, Visualization ToolKit library (VTK)2 and Shearlab library 3 (available in MatLab version and implemented by ourselves in C++). An example of a US volume as well as a 2D US slice are shown in Fig. 4. This US volume is composed of 335 parallel 2D images of size\\\\n250\\u00d7250\\\\nwith a pixel size of\\\\n0.6mm\\u00d70.6mm\\\\nand interval of 0.3mm between each image (captured with a real 3D US Probe).'\",\"410\":null,\"411\":null,\"412\":null,\"413\":\"'SVM based visual servoing vs direct visual servoing like [8]. The proposed task function of using SVM classification error results in precise alignment of the desired pose while the traditional feature error based positioning results in higher error when the instances are different as reflected in the final error images (c-d). The error image represents the difference in intensities between the desired and the current image with an offset of 128 such that gray color encodes zero error.'\",\"414\":null,\"415\":\"'In order to evaluate the run-time performance of the GPU-code, a series of specific experiments have been carried out. Those experiments consisted of in increasing artificially the number of the detected ellipses and evaluating the total computation time required by the GPU code. While the CPU-based functions such as findContours require a computation time that is proportional to the amount of information to process, the GPU-based function show a computation time much less dependent to the amount of information to process. However the main drawback of the GPU computation concerns the time necessary to upload and download data between the CPU and GPU memory. In Figure 7, different tests varying artificially the number of ellipses detected by the algorithm are shown. Figure 8 shows the computation time required to elaborate the current frame comparing it with the number of ellipses found in that frame. The GPU implementation allows to have a limited computation time regardless the complexity of the image. In Figure 8, the dark curve describes the computation time in ms, while the light line shows the corresponding number of found ellipses. The total computation time of the GPU-based Find Marker Routine is around 3 ms for every frame, regardless the number of detected ellipses.'\",\"416\":null,\"417\":\"'http:\\/\\/rpg.ifi.uzh.ch\\/'\\n\\n'We consider the problem of next-best view selection for volumetric reconstruction of an object by a mobile robot equipped with a camera. Based on a probabilistic volumetric map that is built in real time, the robot can quantify the expected information gain from a set of discrete candidate views. We propose and evaluate several formulations to quantify this information gain for the volumetric reconstruction task, including visibility likelihood and the likelihood of seeing new parts of the object. These metrics are combined with the cost of robot movement in utility functions. The next best view is selected by optimizing these functions, aiming to maximize the likelihood of discovering new parts of the object. We evaluate the functions with simulated and real world experiments within a modular software system that is adaptable to other robotic platforms and reconstruction problems. We release our implementation open source.'\",\"418\":null,\"419\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/27390625'\\n\\n\\\"Given the requirements of the proposed algorithm, the user's gait phase, ankle angle, and absolute shank angle were measured by the following sensors. Two force sensors (FlexiForce A301, Tekscan, Inc.) were embedded into an insole which is placed beneath the user's foot for detecting the phase of gait, e.g., stance vs. swing. These two force sensors were placed within the normal COP trajectory to provide precise readings, where one was placed under the ball of the foot, while the other one under the heel. The insole was produced on a Connex 350 3D printer and made from a rubber-like polyjet photopolymer. An optical incremental encoder (2048 CPR, E6-2048-250-IE-S-H-D-3, US Digital, Inc.) was used to obtain the ankle angle, and an Inertial Measurement Unit (3DM-GX4-25-RS232-SK1, LORD MicroStrain, Inc.) was installed on the main structure to obtain the absolute shank angle. The system was designed with a safety button that must be held continuously by the user to power the PAFO, i.e., an enable signal. The user could release the button to disable the PAFO at any point in the experiment, e.g., if balance was lost.\\\"\",\"420\":null,\"421\":null,\"422\":\"'In this paper we propose an approach that tries to exploit the frequency content of EMG in an innovative and natural way. A prototype of a dynamic synergies hand was designed and it was tested by using commercial EMGs electrodes. Rather than using the sort of frequency modulation that commercial EMG decoders adopt, we aim at shaping the posture of a PAP by using the velocity reference itself, associating different speeds with different movements. A slow muscle contraction was associated with a slow synergy while the fast synergy came from a fast muscle contraction.'\\n\\n'decode'\\n\\n'encode'\",\"423\":null,\"424\":null,\"425\":\"'The trajectories of the markers were recorded by the cameras at a sampling rate of 100Hz. The kinematics data were averaged across three trials and analyzed using VICON BodyBuilder and VICON BodyLanguage (Vicon Motion System Ltd., UK) with customized code to calculate the range of motion of each joint (Table I).'\",\"426\":null,\"427\":null,\"428\":\"'https:\\/\\/github.com\\/humanoid-path-planner'\\n\\n'All the computations were performed offline on a Intel Xeon(R) CPU E3-1240 v3 @ 3.40GHz. The contact planner is open-source and available at https:\\/\\/github.com\\/humanoid-path-planner. The OCP is solved using the proprietary software MUSCOD-II provided by the Interdisciplinary Center for Scientific Computing (IWR) of Heidelberg University. This software offers an OCP toolkit (e.g. integration and numerical-differentiation routines) along with an efficient sparse sequential-quadratic-program solver. The whole-body trajectory is obtained from the contact sequence and the COM and angular-momentum trajectory using a second-order inverse kinematics. The typical tasks where the tracking of the contact placements, the tracking of the COM position and an additional posture task to keep the configuration close to the planned postures. The computed trajectories are then executed by the real robot. For the walking experiments, we used a closed-loop control provided with the robot to stabilise the movements of the rubber bush inside each foot [26]. The stabiliser was not used for the climbing scenario as it is not able to handle hand contacts.'\\n\\n'https:\\/\\/github.com\\/humanoid-path-planner'\",\"429\":null,\"430\":null,\"431\":null,\"432\":\"'The controller uses data from force sensors yielding the ground reaction forces, from two encoders at each leg that measure the relative angle\\\\n\\u03c6\\\\nand leg length\\\\nl\\\\n, and estimates of the body position and attitude\\\\n(x,y,\\u03b8)\\\\nusing the robot model fused with data from an inertial sensor, [4]. No additional sensor for the terrain properties is required.'\",\"433\":\"\\\"To capture the audio data, we used the open source software HARK-ROS and Hark designer [12] in Ubuntu 12.04. A ROS node was configured to publish audio data messages which could either be saved as a bag file for offline processing or used directly for online real-time classification. During recording, each of the bag files were labelled with the terrain type and the location. Labelling the location was imperative in validating the classifier's performance with new locations, this is explained further in Section V-C. The labelled bag files were then converted to MAT-files for generating the training set in Matlab.\\\"\",\"434\":null,\"435\":\"'https:\\/\\/youtu.be\\/RKwWxEc-ric'\",\"436\":\"'Since the generation of tool path segments in each layer of the input model is straightforward, for simplicity we describe our algorithm by assuming we are given a set of motion segments (i.e., G-code instructions) and the associated layer of each segment. In our current implementation, we make use of the open-source Slic3r [13] path generator, which follows the iso-planar method described in Section I-A. In the sections that follow we describe each phase of our algorithm. First, we parse the input segments (Section III-A). Next, the layers are divided into islands, or portions of the path that are convenient to print together in (Section III-B). Then, each island is assigned a set of dependencies that must be met before it can be put onto the final path (Section III-C). Finally, the islands are combined into a final path in Section III-D. Algorithm 1 (OPTIMIZE) gives a top-down overview of the complete algorithm.'\\n\\n'To test such a large set of models, we made use of a simulator. The simulator was configured to read a tool path represented in the NIST RS274NGC G-code standard [10], compatible with the Marlin firmware [11]. The abstracted tool path could then be operated on and rendered back into a G-code representation and analyzed. We ran the simulator for each of the 409 models, using parameters appropriate for our test printer. As mentioned previously, the height of the bounding box of the print head was set to 7mm as was the radius.'\",\"437\":null,\"438\":\"'Gauss code'\",\"439\":\"'Shoelace knot diagrams, with Gauss code 1+, 2\\u2212, 3\\u2212, 4+, 5\\u2212, 6\\u2212, 7+, 3+, 2+, 1\\u2212, 4\\u2212, 5+, 6+, 7\\u2212.'\\n\\n'Gauss code'\\n\\n'Each crossing appears twice on a Gauss code for an arbitrary knot. Let us define a knot unit as a connected subsection of Gauss code where all the labels in the subsection has appeared twice. For example, let us consider three overhand knots in a row, with Gauss code\\\\nG={\\\\n1\\\\n+\\\\n,\\\\n2\\\\n\\u2212\\\\n,\\\\n3\\\\n+\\\\n,\\\\n1\\\\n\\u2212\\\\n,\\\\n2\\\\n+\\\\n,\\\\n3\\\\n\\u2212\\\\n,\\\\n4\\\\n+\\\\n,\\\\n5\\\\n\\u2212\\\\n,\\\\n6\\\\n+\\\\n,\\\\n4\\\\n\\u2212\\\\n,\\\\n5\\\\n+\\\\n,\\\\n6\\\\n\\u2212\\\\n,\\\\n7\\\\n+\\\\n,\\\\n8\\\\n\\u2212\\\\n,\\\\n9\\\\n+\\\\n,\\\\n7\\\\n\\u2212\\\\n,\\\\n8\\\\n+\\\\n,\\\\n9\\\\n\\u2212\\\\n}\\\\n. This structure contains three knot units,\\\\nG\\\\n1\\\\n={\\\\n1\\\\n+\\\\n,\\\\n2\\\\n\\u2212\\\\n,\\\\n3\\\\n+\\\\n, \\\\n1\\\\n\\u2212\\\\n,\\\\n2\\\\n+\\\\n,\\\\n3\\\\n\\u2212\\\\n},\\\\nG\\\\n2\\\\n={\\\\n4\\\\n+\\\\n,\\\\n5\\\\n\\u2212\\\\n,\\\\n6\\\\n+\\\\n,\\\\n4\\\\n\\u2212\\\\n,\\\\n5\\\\n+\\\\n,\\\\n6\\\\n\\u2212\\\\n}\\\\nand\\\\nG\\\\n3\\\\n={\\\\n7\\\\n+\\\\n,\\\\n8\\\\n\\u2212\\\\n,\\\\n9\\\\n+\\\\n,\\\\n7\\\\n\\u2212\\\\n,\\\\n8\\\\n+\\\\n,\\\\n9\\\\n\\u2212\\\\n}\\\\n. We call a c-path a bridge if it connects two knot units.'\\n\\n'For an arbitrary knot with Gauss code\\\\nG\\\\nwhere\\\\n|G|=2k\\\\n, let there be\\\\nn\\\\nic\\u2212p\\\\ninterior c-paths,\\\\nn\\\\nec\\u2212p\\\\nexterior c-paths,\\\\nn\\\\nic\\\\ninterior crossings and\\\\nn\\\\nec\\\\nexterior crossing. Then, the following equations and inequalities hold:'\\n\\n'Given a Gauss code\\\\nG\\\\n, we would like to generate an efficient polygonal knot diagram, corresponding to a grasp of the knot. The number of grasp points (fingers) needed is related to the number of crossings, which can be found from the Gauss code:'\\n\\n'For example, let us consider a double coin knot, with Gauss code\\\\nG={\\\\n1\\\\n+\\\\n,\\\\n2\\\\n\\u2212\\\\n,\\\\n3\\\\n+\\\\n,\\\\n4\\\\n\\u2212\\\\n,\\\\n5\\\\n+\\\\n,\\\\n6\\\\n\\u2212\\\\n,\\\\n2\\\\n+\\\\n,\\\\n7\\\\n\\u2212\\\\n,\\\\n4\\\\n+\\\\n,\\\\n8\\\\n\\u2212\\\\n,\\\\n6\\\\n+\\\\n,\\\\n1\\\\n\\u2212\\\\n,\\\\n7\\\\n+\\\\n,\\\\n3\\\\n\\u2212\\\\n,\\\\n8\\\\n+\\\\n,\\\\n5\\\\n\\u2212\\\\n.'\\n\\n'Now we will show that chords formed by connecting exterior crossings that are adjacent in the Gauss code do not intersect except at common endpoints. Consider four exterior crossings\\\\na,b,c,d\\\\n, such that\\\\na\\\\nand\\\\nc\\\\nare adjacent in the Gauss code, and\\\\nb\\\\nand\\\\nd\\\\nare adjacent in the Gauss code. Line segment\\\\nac\\\\ncuts\\\\nP\\\\ninto two pieces. If\\\\nb\\\\nand\\\\nd\\\\nbelong to different pieces of\\\\nP\\\\nw.r.t.\\\\nac\\\\n, then the only way to avoid intersection between\\\\nac\\\\nand\\\\nbd\\\\nusing any arbitrary curve is if\\\\nb\\\\nand\\\\nd\\\\nare connected outside\\\\nP\\\\n. But c-path\\\\n(b, d)\\\\nis not an exterior c-path; therefore to be consistent with the Gauss code,\\\\nb\\\\nand\\\\nd\\\\nmust be in the same piece. \\u25a0'\\n\\n'So far, we have placed all the exterior crossings and connected them if they are adjacent on the Gauss code. So far, no undesired intersection have been introduced. An example of placing all exterior crossings and connecting them is shown in Figure 3. Now we will work inwards, starting by placing all the crossings that are directly connected to the exterior crossings.'\\n\\n'For an arbitrary knot unit with Gauss code\\\\nG\\\\n, where\\\\n|G|=2k<\\u221e\\\\n, there always exist a polygonal knot diagram of the knot with no more than\\\\n3k\\u22122\\\\nline segments.'\\n\\n'For an arbitrary knot with Gauss code\\\\nG\\\\nwhere\\\\n|G|=2k\\\\n, to draw a polygonal knot diagram of this knot, at least\\\\n\\u2308\\\\n3+\\\\n8k+1\\\\n\\u221a\\\\n2\\\\n\\u2309\\\\nand no more than\\\\n3k\\u22122\\\\nsequential line segments are required.'\\n\\n'An arbitrary knot with Gauss code\\\\nG\\\\nwhere\\\\n|G|=2k\\\\ncan always be folded from a polygonal arc of no more than\\\\n9k\\u22126\\\\nlinks, starting from a straight configuration.'\",\"440\":null,\"441\":null,\"442\":null,\"443\":null,\"444\":null,\"445\":null,\"446\":null,\"447\":null,\"448\":null,\"449\":null,\"450\":null,\"451\":\"'We have installed two encoders, one at each traction motor, whose data is transformed to angular speed. This information is used into the speed control of the walker wheels.'\",\"452\":null,\"453\":\"'The server waits for incoming connections from the therapist PC and the controller PC. After a successful authentication, all connected computers may communicate with each other. Communication consists of special messages that contain a header and a body part. The header holds information like sender, receiver, time stamp and command code, while the body has the actual payload, which can be interpreted according to the command code. The server inside the patient display acts as a message dispatcher that forwards all messages to the respective receiver. The server logs all communication into a MySQL database, so that the complete training session, including all parameters and measurement data is recorded and may be replayed or evaluated later on.'\\n\\n'Two different types of drives were used in the system: Schunk PDU 70 modules for the pelvis motors and Elmo whistle Solo G-SOLWHI20\\/100SE for the hip, knee and wheels. While the platform motors are standard DC motors for electric wheel chairs, the hip and knee drives use Maxon EC45 Flat motors that are equipped with MILE Encoders that have a resolution of 2048 counts per turn.'\",\"454\":null,\"455\":\"'SIMbot has three legs which are conceptually identical to those used on the IMB-drive ballbot. The legs are used only for stability when SIMbot is powered down. Each leg is driven by a linear drive screw powered by a dc motor with an attached quadrature encoder. The end of each leg has a spring-loaded limit switch with a ball caster. The limit switch detects contact with the floor and with the body when the legs are going down and up, respectively. Each leg is run by a separate motor driver with an integrated PID controller for velocity control of each individual leg.'\",\"456\":\"'https:\\/\\/www.youtube.com\\/channe1\\/UCL17qX3nXT84w-TTqlrldHA'\\n\\n'The real-time controller is on a dedicated target PC that communicates with the host PC; it has one PCI slot for the encoder card and four interfaces for RS232 signals; this PC has Intel Core i5 processor with real-time SW (workshop), the host-target environment is a PC with MATLAB real-time workshop compiler installed on Intel Core i5 processor with Window 7. Both communicate with the Conversion Unit via a Matlab-compatible Quatech RS232-card. The control system is developed in MATLAB2015a\\/Simulink and downloaded to the target PC. In Fig. 12 the performance of angular position\\\\n0(t)\\\\nof experiment platform PendCon Advanced closed-loop control (11) is presented. The rotational angle of the horizontal rod\\\\n\\u03d5(t)\\\\nof PendCon Advanced is shown in Fig. 13. Whereas in Fig. 14 the control signal of (11) applied to PendCon Advanced is displayed. Here, the gains of the control law are the same as the gains used in the simulations above. Simultaneously, in this experimental results a comparative of behavior of PR controller and LQR is presented. The LQR parameters are computed using MATLAB lqr function with the weighting matrices\\\\nQ=10\\u00d7\\\\nI\\\\n4\\\\n,R=1\\\\n, where\\\\nI\\\\n4\\\\n\\u2208\\\\nR\\\\n4\\u00d74\\\\nis identity matrix.'\\n\\n'Due to presence of errors of the quantization of encoders and of the wireless communication, which is used for the position measurement of the unactuated variable of the Furuta pendulum under study, noise as well as delays in measurements make difficult to find parameters of various algorithms that can be used for its velocity estimation. To recover an acceptable performance without relying on accurate estimates of the derivative for feedback, we design of a PR controller for stabilization of the Furuta pendulum. With the presented approach, the use of an estimation of velocity for the unactuated variable is avoided resulted in a visibly better performance. Since a retarded term is included in the control law, the closed-loop dynamics are represented by a characteristic quasi-polynomial and the stability analysis was successfully done with Mikhailov criterion and a root locus of the corresponding quasi-polynomial. A two-step procedure for tuning the 4 parameters of a controller was proposed. Experimental results and a comparison with an LQR controller show the advantages of the proposed methodology. Extensions for a wider class of underactuated mechanical systems is considered for future work as well as taking into account transmission delays and nonlinearities.'\",\"457\":null,\"458\":null,\"459\":null,\"460\":null,\"461\":null,\"462\":null,\"463\":null,\"464\":null,\"465\":null,\"466\":\"'3D-printed bellows produced via inkjet in a single print. Codeposition of liquids and solids allows fine internal channels to be fabricated and pre-filled. The part is ready to use when it is removed from the printer.'\",\"467\":null,\"468\":null,\"469\":null,\"470\":\"'https:\\/\\/youtu.be\\/MkUllcfIwDQ'\",\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":null,\"476\":\"'Utility function: Adaptive monotonous submodular function that encodes the amount of confidence (uncertainty reduction) in the state estimation. A lower value represents higher uncertainty.'\\n\\n'Conceptually, the overall algorithm is as follows: 1) run 3D point cloud processes for a new observation, 2) update the belief state according to fitness to each hypothesis, 3) compute expected marginal utility for each action, 4) greedily select and take the action with maximum expected marginal utility per cost, and 5) iterate above processes until sufficient information is gathered. In Appendix, the pseudo-code is presented to show how belief state update and action selection are conducted iteratively.'\",\"477\":null,\"478\":null,\"479\":null,\"480\":null,\"481\":null,\"482\":null,\"483\":null,\"484\":null,\"485\":null,\"486\":null,\"487\":null,\"488\":\"\\\"We use three Maxon DC Motors as actuators. Each motor, equipped with encoders to ensure precise velocity control, is connected to the manipulator's tendons via a gearbox with a 128:1 reduction ratio. The rotational speed of the motors governs the tendons' linear velocities and changes the tendons' lengths.\\\"\",\"489\":\"'http:\\/\\/ei.is.tuebingen.mpg.de\\/person\\/dbuechler'\\n\\n'http:\\/\\/ei.is.tuebingen.mpg.de\\/person\\/dbuechler'\\n\\n'Schematic description of the position control loop for one PAM muscle pair. The absolute value of the output signal of the position control PID receives one of both muscle dependent on the sign of this signal. The other PAM is set to its individual minimum pressure. The pressure within each PAM is governed by separate pids that set the input voltage to the proportional air flow valves. The sensor values are provided by festo pressure sensors and angle encoders.'\\n\\n'The complete system comprises eight pressure sensors and proportional valves as well as four incremental angular encoders to govern and describe the movement. Control is eased by delegating low level task, such as extraction of the angular value from the A and B digital signals given by the encoder or regulating the pressure within each PAM. The National Instruments PCIe 7842R FPGA card has been used to take over these tasks and govern the communication with the hardware. The FPGA was programmed in Labview. To assure fast implementation, we used the FPGA Interface C API to generate a bitfile along with header files which can be incorporated in any C project. Thus, the control algorithm can be implemented in C on top of the basic functionalities supplied by the FPGA. The sensor values are read with 10 kHz and new desired pressure values are adjusted at 1 kHz. Fig. 4(a) shows the pressure response to a step in desired value from minimum (0 bar) to maximum air pressure (3 bar). The resulting pressure regulation reaches the desired value within a maximum of 0.25 seconds.'\",\"490\":null,\"491\":\"\\\"the Novint's positioning accuracy (using its optical encoders and its proprietary kinematic model)\\\"\",\"492\":\"'From these distances, the pseudo code of building a data list\\\\nL\\\\nq\\\\ncontaining\\\\nq\\\\ndata pairs is detailed in Algorithm 1 where:'\\n\\n'This corresponds to the pseudo-code of Algorithm 2, where Length(L) returns the number of elements already included in a list\\\\nL\\\\nand Least_square_estimation(\\\\nL\\\\n) performs a LS estimation from\\\\nL\\\\nand returns the estimated position and the associated error. Algorithm 2 is run at each sampling period, in parallel to Algorithm 1 that builds the lists.'\",\"493\":null,\"494\":null,\"495\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34290900'\\n\\n'Haptic feedback plays a key role in surgeries, but it is still a missing component in robotic Minimally Invasive Surgeries. This paper proposes a dynamic model-based sensorless grip force estimation method to address the haptic perception problem for commonly used elongated cable-driven surgical instruments. Cable and cable-pulley properties are studied for dynamic modeling; grip forces, along with driven motor and gripper jaw positions and velocities are jointly estimated with Unscented Kalman Filter and only motor encoder readings and motor output torques are assumed to be known. A bounding filter is used to compensate for model inaccuracy and to improve method robustness. The proposed method was validated on a 10mm gripper which is driven by a Raven-II surgical robot. The gripper was equipped with 1-dimensional force sensors which served as ground truth data. The experimental results showed that the proposed method provides sufficiently good grip force estimation, while only motor encoder and the motor torques are used as observations.'\\n\\n'A dynamic model-based grip force estimator was proposed to replace force sensing in elongated cable-driven surgical instruments. In the proposed method, only motor output torques and motor encoder readings are needed for force perception and no extra hardware is required, in order to address the problem that force measurement schemes which require hardware modifications face significant adoption barriers due to sterilization and cost constraints.'\",\"496\":\"'Cable driven manipulators are popular in surgical robots due to compact design, low inertia, and remote actuation. In these manipulators, encoders are usually mounted on ...'\\n\\n'Cable driven manipulators are popular in surgical robots due to compact design, low inertia, and remote actuation. In these manipulators, encoders are usually mounted on the motor, and joint angles are estimated based on transmission kinematics. However, due to non-linear properties of cables such as cable stretch, lower stiffness, and uncertainties in kinematic model parameters, the precision of joint angle estimation is limited with transmission kinematics approach. To improve the positioning of these manipulators, we use a pair of low cost stereo camera as the observation for joint angles and we input these noisy measurements into an Unscented Kalman Filter (UKF) for state estimation. We use the dual UKF to estimate cable parameters and states offline. We evaluated the effectiveness of the proposed method on a Raven-II experimental surgical research platform. Additional encoders at the joint output were employed as a reference system. From the experiments, the UKF improved the accuracy of joint angle estimation by 33- 72%. Also, we tested the reliability of state estimation under camera occlusion. We found that when the system dynamics is tuned with offline UKF parameter estimation, the camera occlusion has no effect on the online state estimation.'\\n\\n'Raven-II experimental research platform. Joint encoders are mounted on each joint for validation only and their values were not used in the controller or the ukf. Stereo cameras, fixed perpendicular to the robot base, were used for stereo 3D vision.'\\n\\n'In Raven, encoders are initialized at the hard-stop position (the physical joint limit position of each joint). Thus at the hard-stop position, there is no kinematic error. Furthermore, at this position from kinematics, the translation offset between the origin of frame zero and the origin of the end-effector (marker)\\\\n0\\\\nP\\\\n0,M\\\\nis known. Since in this work, we are only tracking translation, we can assume there is no rotation between frame zero and the origin of the marker. Hence, they are coincident\\\\n(\\\\n0\\\\nM\\\\nR=Identity)\\\\n. Thus, the homogeneous transformation from the camera to the marker\\\\nC\\\\nM\\\\nT\\\\nis known at this point with translation vector\\\\nC\\\\nP\\\\nC,M\\\\nand rotation matrix\\\\nC\\\\nM\\\\nR\\\\n=\\\\nC\\\\n0\\\\nR\\\\n. Also, the homogeneous transformation between frame zero to marker\\\\n(\\\\n0\\\\nM\\\\nT)\\\\nis known with translation vector\\\\n0\\\\nP\\\\n0,M\\\\nand rotation matrix\\\\n0\\\\nM\\\\nR\\\\n. Hence, the transformation from camera frame to frame zero can be calculated by:'\\n\\n'Raven has four states for each link. The states are motor angle, motor velocity, joint angle and joint velocity. However, an optical encoder with 4000 counts per revolution is mounted directly on the shaft of each motor. Therefore, the position and velocity of the motor is well known and equations 2\\u20134 need not be solved. Hence, the dynamics can be simplified and the system states can be reduced to two states for each link (joint angle, and joint velocity). With this simplification the state space form of the Raven can be expressed as:'\\n\\n'In this paper, the main objective is to improve the state estimation of the joint angles by using the UKF and low cost stereo vision. Therefore, to measure the performance of the UKF, additional optical encoders (Avago Technologies, model number AEDA-3300, 80000 counts per revolution, for the first two rotary joints and linear optical encoder MicroE Systems, model number Mercury II 1600, resolution 5 \\u03bc\\\\nm\\\\n, for the third prismatic joint) were installed on the joints for comparison. Moreover, since we are only investigating state estimation accuracy of the UKF and not the controller performance, the feedback input of the controller is based on the motor angles and the UKF estimated states were not used in the control. Fig. 4 shows a block diagram representation of the joint angle computations and measurements.\\\\nq\\\\nrepresents the true joint angle.\\\\nq\\\\n^\\\\nKin\\\\nis the estimated joint angle based on transmission kinematics.\\\\nq\\\\n^\\\\nC\\\\nis the estimated joint angle based on camera raw data, and\\\\nq\\\\n^\\\\nUKF\\\\nis the estimated joint angle based on UKF method proposed in this paper.'\\n\\n\\\"In this work, we used motor encoders and low cost cameras to estimate joint transmission parameters of rigid link, cable parameters for all the links, and joint states of a serial cable driven robots on the Raven experimental surgical robot platform utilizing the Unscented Kalman Filter. With this method, we were able to fine tune the system dynamics by estimating parameters offline. Once the dynamics were tuned with UKF parameter estimation, the UKF was able to improve the joint angle estimation. The occlusion experiment suggests that this method is reliable and robust even when the camera is occluded during state estimation. From the experiments' once the system parameters are tuned with stereo vision and the UKF offline, the camera measurements did not provide much improvement online during state estimation. Hence, only motor measurements and UKF will be sufficient. This is beneficial because in practice the camera will get occluded during surgery. However, running both the camera and motor encoder at the same time improves safety due to redundancy that is provided to estimate joint angles.\\\"\\n\\n'In surgery, in addition to accuracy, redundancy is also important. In case of sensor failures, it is essential to have backup sensors or methods. The future work will include investigating whether the UKF can estimate joint angles using only camera measurements in case of motor encoder failure. Also, we will study for how long and what frequency and speed range the filter can maintain convergence under camera occlusion when a motor encoder fails. Furthermore, we will use the results of ongoing research on improving cable driven mechanism dynamics in [26], which may improve the UKF performance when motor encoder fails.'\",\"497\":\"https:\\/\\/github.com\\/BerkeleyAutomation\\/tsc-dl\\/zipball\\/master\",\"498\":\"'Robotic surgery and other minimally-invasive surgical techniques are an integral part of patient care, and readily yield large amounts of data. Surgical tool motion (kinematic data) contains information that is useful for assessment and education. Typically, assessment and education tools that rely upon the kinematic data require substantial manual processing such as activity annotations. The goal of this paper was to develop an automated method to align surgical recordings and assign activity annotations. We developed an approach based on unsupervised alignment to efficient annotate kinematic data for its constituent activity segments. Our method includes extracting non-linear features from the kinematic data using a stacked de-noising autoencoder, and using modified dynamic time warping to align the kinematic data from different trials of the study task. We combined alignment between a test and one or a small set of template trials (with prior manual annotations) with voting based on kernel density estimation to transfer labels from the template to the test trial. Our experiments on performance of this method using two datasets captured in the training laboratory demonstrate an accuracy of 72% to 94% for annotating activity segments within a surgical training task. Our findings are robust to data captured from several surgeons, and to deviations in activity from a canonical activity sequence.'\\n\\n'Our objective was to develop and evaluate an unsupervised learning method to recognize surgical activities. We first learn a non-linear representation of the kinematic data that extracts useful information using a stacked denoising autoen-coder (SDAE) [12]. We then align the surgical trials using dynamic time warping with a customized bounding technique and progression strategy. We develop a voting technique to combine the alignment of SDAE features with manual activity labels in a small set of trials for automated transfer of gesture or maneuver labels. Finally, we demonstrate that our method enables universal pre-trained resources for surgical activity recognition across data sets. The proposed method does not impose model-based assumptions on the data, and therefore it can be easily generalized to data from various resources and modalities.'\\n\\n'An autoencoder (AE) is an artificial neural network to reconstruct an output from an input with minimal error. An AE may be designed with non-linear activation functions to decompose non-linear dependencies among features and generate a reconstruction that reduces such dependencies. Furthermore, AEs can be used to reduce dimensions of the input by specifying the last layer to have fewer nodes than the input layer. A layer-wise pre-training strategy provides an efficient way to train the AE [14].'\\n\\n'We hypothesized that SDAE features from the MISTIC-SL and JIGSAWS datasets contain shared information to the extent determined by similarity in tool motion while performing common activity segments. To verify this hypothesis, we trained the autoencoder on the MISTIC-SL dataset and used it to extract features and evaluate alignment with the JIGSAWS dataset. We conducted this analysis separately using pairs of trials within and across subjects, denoted by i-subject and x-subjects, respectively.'\\n\\n'In this paper we developed a technique for aligning and annotating surgical activities using kinematic data from an unsupervised perspective. Our method involves learning nonlinear features from the kinematic data via stacked denoising autoencoders, and using a modified dynamic time warping approach to align surgical trials. We also developed a voting method based on kernel density estimation using the alignment results to robustly transfer labels from a small template set to large numbers of unlabelled trials. Our method provides a way to catalog or index data on surgical tool motion that are now ubiquitously available but lack in annotations.'\",\"499\":null,\"500\":\"'http:\\/\\/hamlyn.doc.ic.ac.uk\\/cardiacEP'\\n\\n'Code and further information can be found at our website http:\\/\\/hamlyn.doc.ic.ac.uk\\/cardiacEP.'\",\"501\":\"'For supervised automation of multi-throw suturing in Robot-Assisted Minimally Invasive Surgery, we present a novel mechanical needle guide and a framework for optimizing needle size, trajectory, and control parameters using sequential convex programming. The Suture Needle Angular Positioner (SNAP) results in a 3x error reduction in the needle pose estimate in comparison with the standard actuator. We evaluate the algorithm and SNAP on a da Vinci Research Kit using tissue phantoms and compare completion time with that of humans from the JIGSAWS dataset [5]. Initial results suggest that the dVRK can perform suturing at 30% of human speed while completing 86% suture throws attempted. Videos and data are available at: berkeleyautomation.github.io\\/amts.'\",\"502\":null,\"503\":\"\\\"In this work, we take a step towards bridging the gap between the theory of formal synthesis and its application to real-world, complex, robotic systems. In particular, we present an end-to-end approach for the automatic generation of code that implements high-level robot behaviors in a verifiably correct manner, including reaction to the possible failures of low-level actions. We start with a description of the system defined a priori. Thus, a non-expert user need only specify a high-level task. We automatically construct a formal specification, in a fragment of Linear Temporal Logic (LTL), that encodes the system's capabilities and constraints, the task, and the desired reaction to low-level failures. We then synthesize a reactive mission plan that is guaranteed to satisfy the formal specification, i.e., achieve the task's goals or correctly react to failures. Lastly, we automatically generate a state machine that instantiates the synthesized symbolic plan in software. We showcase our approach using Team ViGIR's software and Atlas humanoid robot and present lab experiments, thus demonstrating the application of formal synthesis techniques to complex robotic systems. The proposed approach has been implemented and open-sourced as a collection of Robot Operating System (ROS) packages, which are adaptable to other systems.\\\"\\n\\n\\\"We now encode the connection between the activation and the possible outcomes of the system's actions,\\\\na\\u2208A\\\\n. The environment safety assumption (6) dictates that the value of an outcome should not change if the corresponding action has not been activated again. In other words, outcomes persist through time.\\\"\\n\\n'Motivated by the DRC tasks, we present formulas that encode the accomplishment of each goal once. However, LTL can naturally handle repeating tasks (e.g. patrolling). We can even combine the two paradigms, e.g., \\u201cAccomplish the goals\\\\nG\\\\ninfinitely often, but if anything fails, abort\\u201d.'\\n\\n'It is important to point out the trade-off between expressivity and automation. On the one hand, an expert user can manually write a very expressive and customized formal specification. On the other hand, the generation of the formal specification can be automated, as we do here, but possibly at the expense of expressivity (e.g., due to hard-coded design choices). However, there is no research barrier to recovering expressivity; the formal language supports it. Thus, we plan on extending our user interface in immediate future work.'\",\"504\":\"'http:\\/\\/www.comanoid.eu\\/'\",\"505\":null,\"506\":\"'Information collection tasks are encoded as a function of Random Variables (RVs). While there are many metrics which capture information gathering, such as Fisher Information [11], and Kullback-Leibler divergence [12], this paper uses Differential Entropy (DE). This is because the entropy of a RV gives an absolute measure of uncertainty and monotonically decreases as one becomes more certain about that variable. Entropy is also easily generalized to collections of i.d. random variables and is agnostic to their true underlying distribution.'\",\"507\":null,\"508\":\"'Thus, for each bathymetric patch, we extract the mean depth and encode it as a \\u2018one-hot\\u2019 feature. The observed depth range is divided into 1m bins, the corresponding depth bin is encoded as a 1, and values of adjacent bins follow a Gaussian-like falloff. The remaining zero-meaned patch is encoded using an autoencoder [15] trained on a large dataset of 500000 bathymetry patches. This learns first-order features for the bathymetry [6], such as gradient and edge filters.'\\n\\n'Each patch is encoded according to the learned dictionary, using the Orthogonal Matching Pursuit (OMP) algorithm.'\",\"509\":\"'A holonomic system in a complex scenario. Solid lines show the optimal trajectories, dotted show the initial, for two different scenarios. The longer trajectory includes obstacles, and the other, no obstacles. The dots around the start points show the initial particles. Landmarks are marked as stars and information is coded with color (lighter means more information). Lookahead time horizon for the longer trajectory is 100 and 30 for the other. The axes units are in meters.'\",\"510\":null,\"511\":null,\"512\":null,\"513\":\"'To generate super rays using three mapping lines, we project input points into each mapping line. When points are assigned to the same segment in all of three mapping lines, those points have the same access patterns. Similar to the 2-D case, we generate a super ray for those points. The pseudo code of generating super rays for a cell in the 3-D case is shown in Alg. 2.'\",\"514\":\"\\\"The resulting gas distributions maps from the three strategies are shown with the same color code as in fig. 7. Magnified sections of interesting areas are shown on the right, where the position of gas sources is marked with small red circles. For both areas of interest, the resulting reconstruction using the xvt-spp generated measurement positions highlights the gas sources better than the gas maps built using the measurements based on the human expert's strategy or the template matching.\\\"\",\"515\":null,\"516\":null,\"517\":null,\"518\":\"'For both, orientation and translation, a sixth-order B-spline is employed, which encodes linear and angular acceleration as a cubic polynomial.'\\n\\n'An increasing number of robotic systems feature multiple inertial measurement units (IMUs). Due to competing objectives-either desired vicinity to the center of gravity when used in controls, or an unobstructed field of view when integrated in a sensor setup with an exteroceptive sensor for ego-motion estimation-individual IMUs are often mounted at considerable distance. As a result, they sense different accelerations when the platform is subjected to rotational motions. In this work, we derive a method for spatially calibrating multiple IMUs in a single estimator based on the open-source camera\\/IMU calibration toolbox kalibr. We further extend the toolbox to determine IMU intrinsics, enabling accurate calibration of low-cost IMUs. The results suggest that the extended estimator is capable of precisely determining these intrinsics and even of localizing individual accelerometer axes inside a commercial grade IMU to millimeter precision.'\\n\\n'The approach was implemented as an extension to the open-source camera\\/IMU calibration toolbox kalibr2 [7] and will be released as an update to it.'\\n\\n'In this work, we presented an extension to the open-source calibration toolbox kalibr that allows for determining the extrinsics and intrinsics of multiple IMUs in a single estimator. We further demonstrated that it is feasible to infer the location of individual accelerometer axes to millimeter precision.'\",\"519\":\"'https:\\/\\/youtu.be\\/2zQzQcX64vw'\\n\\n\\\"We propose an automated, closed-loop, and local calibration method for serial robots that uses a new, low-cost, 3D measuring device. The device consists of three Mitutoyo digital indicators, arranged in an orthogonal manner, and a mastering fixture based on kinematic coupling. The indicators communicate, via wireless connection, with a PC that controls the movements of the robot. To measure absolute Cartesian coordinates, the device is positioned incrementally over each of several 0.5-inch precision balls until all indicators are at zero, at which time the robot joint encoders are read. The balls are fixed with respect to the robot's base. The precise relative positions of the centers of these balls must be known in advance. In this study, the measuring device is mounted on the flange of an ABB IRB 120 robot. Only three precision balls are used, spaced 300 mm apart, and the distances between these balls are measured with a Renishaw telescoping ballbar. The absolute accuracy of the robot was enhanced by minimizing its position errors, using the least squares method. The feasibility of the calibration approach was demonstrated through a simulation study. Finally, an experimental validation showed that our calibration method caused the maximum position error of the robot, inside a sphere of 400 mm in diameter, to be reduced to 0.491 mm.\\\"\",\"520\":\"'In order to automatically perform the measurement procedure, the laser tracker, the ABB robot and the control hardware of the cable-driven robot are connected to a host computer through an Ethernet connection. The control hardware of the cable-driven robot consists of a real-time target PC which runs real-time codes provided by the xPC target toolbox of MATLAB. The real-time control codes are created by Simulink Coder and a C\\/C++ compiler in the host computer. The target PC is connected to the motor drivers via a Quanser Q8 data acquisition card which sends the control signals to the motors and reads the angular positions of the motors. This card also reads the analog data provided by the ATI six-channel force\\/torque sensor. The block diagram of the whole process is illustrated in Fig. 6.'\\n\\n'While the ABB robot moves to a new position, the host computer reads the robot joint values and calculates the required cable lengths for the cable-driven robot at each instance. Therefore, the host computer is able to synchronize the movements of the cable-driven robot with the ABB robot. In order to calculate the required lengths of the cables, inverse kinematics equations of the cable-driven robot are solved at each step time. A force controller is implemented on the target PC in order to apply the desired wrench on the moving platform. Based on the feedback signals of the servo motor encoders and the force\\/torque sensor, the controller sends the required command to the servo motors to regulate the applied force on the robot end-effector. Using this experimental setup, a fully automated procedure is designed for the calibration experiment.'\",\"521\":\"'The task of calibrating a mobile robot has been addressed since many years. In this paper we focus on the so called kinematic calibration. In a wheeled mobile robot this consists of estimating the odometry parameters, that are required to convert wheel encoder ticks in a relative motion of the mobile base on a local plane, and of estimating the position of one or more sensors on the mobile base.'\\n\\n'Several approaches to address this task have been proposed in the literature. All of them are regarded as passive processes, where the user moves the platform in an environment, recording the data from the encoder and from the exteroceptive sensors (like laser scanners or cameras) mounted on the robot. The calibration procedure consumes these data and seeks for the parameters that better explain the sensor perceptions, given a calibration pattern.'\\n\\n'Most of the calibration procedures in the literature rely on least squares minimization. The general procedure consists in moving the robot along a predefined trajectory, while recording its encoder ticks. During the acquisition, a ground truth of the position of the mobile base or its sensors is obtained through some external observer or by algorithms that process only the exteroceptive sensor input.'\\n\\n'By knowing the kinematic model is then possible to estimate the motion of the mobile base or its sensors from the measured encoder ticks.'\\n\\n'When started, the system computes the effect that each motion has on the observability of the parameters. This is done by executing each motion while recording the encoder ticks and the estimated motion from the scan-matcher. After one motion\\\\nmotion\\\\nj\\\\nis executed, our procedure estimates an optimal parameter\\\\nk\\\\nj\\\\nset for that motion, together with its inverse covariance\\\\nH\\\\nj\\\\n. This is done by solving the least squares problem in Eq. (6), by considering only the measurements gathered during the execution of the motion. We call this stage \\u201cexploration\\u201d, as the system estimates the effect that each particular motion has to the parameters.'\\n\\n'In this section we shortly report some recurrent behavior we observed during the calibration of the differential drive robot, to better understand the behavior of the approach. Whereas we did not hardcode any heuristic for selecting the motions, we observed that typically the system follows a specific pattern, especially at the beginning. Usually the first choice is to execute the Forward motion, since the\\\\nH\\\\nforward\\\\nmatrix has a higher determinant compared to the others. This results in quickly reducing the uncertainty of the encoder ratios\\\\nk\\\\nl\\\\n,\\\\nk\\\\nr\\\\nand of the laser heading\\\\n\\u03b8\\\\ns\\\\n.'\",\"522\":\"'https:\\/\\/github.com\\/Humhu\\/calotypes'\\n\\n'We present physical experimental results on calibration for nine Sony Playstation Eyes, a low-cost USB camera. Our calibration target is a 7 by 6 checkerboard pattern with 35 mm squares rasterized and painted onto a sheet of white acrylic plastic with a laser cutter. The experiment software was implemented in C++ and uses the OpenCV camera calibration tools. All experiments use a pinhole model with a non-fixed aspect ratio, and two radial and two tangential distortion coefficients. Source code is available at https:\\/\\/github.com\\/Humhu\\/calotypes.'\\n\\n'https:\\/\\/github.com\\/Humhu\\/calotypes'\",\"523\":null,\"524\":null,\"525\":\"'Missing-Code-Occurrence Probability Calibration Technique for DAC Nonlinearity With Supply and Reference Circuit Analysis in a SAR ADC'\",\"526\":null,\"527\":null,\"528\":null,\"529\":null,\"530\":\"'Surgical robotic systems with miniaturized flexible Tendon-driven Serpentine Manipulators (TSM) have enjoyed increasing popularities among surgeons and researchers for their advantages of working in constrained and torturous human lumen such as oral cavity and upper GI tract. However, they suffer from sufficient nonlinearities and model uncertainties due to friction, tension varying, tendon slacking, etc. Model based control is insufficient to overcome such uncertainties and automate challenging surgical related tasks. The objective of this work is to automate certain clinical tasks to alleviate surgeon fatigue and promote task efficiency in kinematics free and sensor free circumstances. We present a data-driven approach based on Learning from Demonstration (LfD), which utilizes statistical machine learning models to encode system underlying dynamics and generalize smooth motor trajectories by direct actuation space learning. Motion segmentation is enabled with soft margin Support Vector Machine (soft-SVM) in complicated tasks. We also make attempts to retrieve task-specific properties by Locally Weighted Regression (LWR). We evaluated the approach on two surgical related tasks: compliant insertion and simplified Endoscopic Submucosal Dissection (ESD). The flexible TSM successfully reproduced both tasks and demonstrated superior trajectory performance. A video is available at: https:\\/\\/youtu.be\\/rLQo6xKtyMI.'\\n\\n'To encode the observed actuation space data, GMM is expressed by the following probability density function:'\\n\\n'Generalized actuation commands versus those recorded from human demonstrations for the compliant tube insertion task. Upper: demonstation motor trajectory (black) encoded by 7 gaussian components for 60 deg tube (left) and by 9 gaussian components for 90 deg tube (right). Lower: smooth actuation trajectory (dark blue) generalized from GMR (left: 60 deg tube; right: 90 deg tube). Both commands are normzlized to [-1 1].'\\n\\n'We have designed two clinically related tasks: compliant tube insertion and simplified ESD cutting to evaluate the proposed method. From only 3\\u20135 demonstrations, our statistical model could encode the underlying dynamics of the tasks and generalize a smooth actuation trajectory. Moreover, for complicated tasks incorporating multiple motion primitives, soft-SVM made automatic motion segmentation possible, spline interpolation allows the smooth transition between two adjacent motion primitives. For both tasks, the flexible TSM acquired the skills needed and reproduced the task faithfully. Trajectory performance is superior to human demonstration in terms of smoothness and RMS error.'\",\"531\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/28392968'\",\"532\":null,\"533\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/27525170'\",\"534\":\"'The ABVS scans the phantom at a constant speed of 1.55 mm\\/s. The needle insertion speed is synchronized with the ABVS speed to keep the needle tip within the ultrasound plane. This is achieved by defining two different insertion speeds. The needle is inserted at a speed of 1.4 mm\\/s if the needle is ahead of the transducer, and is therefore visible in ultrasound images. It is inserted at a speed of 1.7 mm\\/s if the needle is not visible in ultrasound images. This ensures that the needle tip is being tracked, not the needle shaft. The position of the transducer, and therefore the needle tip, is computed using linear stage motor encoder values. The needle velocity out of ultrasound plane is considered and compensated in the controller.'\",\"535\":null,\"536\":null,\"537\":\"'http:\\/\\/mapir.isa.uma.es\\/work\\/rf20'\\n\\n'http:\\/\\/mapir.isa.uma.es\\/work\\/rf2o'\\n\\n'In this paper we present a fast and precise method to estimate the planar motion of a lidar from consecutive range scans. For every scanned point we formulate the range flow constraint equation in terms of the sensor velocity, and minimize a robust function of the resulting geometric constraints to obtain the motion estimate. Conversely to traditional approaches, this method does not search for correspondences but performs dense scan alignment based on the scan gradients, in the fashion of dense 3D visual odometry. The minimization problem is solved in a coarse-to-fine scheme to cope with large displacements, and a smooth filter based on the covariance of the estimate is employed to handle uncertainty in unconstraint scenarios (e.g. corridors). Simulated and real experiments have been performed to compare our approach with two prominent scan matchers and with wheel odometry. Quantitative and qualitative results demonstrate the superior performance of our approach which, along with its very low computational cost (0.9 milliseconds on a single CPU core), makes it suitable for those robotic applications that require planar odometry. For this purpose, we also provide the code so that the robotics community can benefit from it.'\\n\\n'Odometry is an essential component for robot localization. It is commonly solved through three major techniques that are based on inertial devices, wheel encoders or visual odometry (either by feature tracking or by dense image alignment). Inertial measurement units (IMUs) are ideal to estimate spatial orientation but accumulate too much translational error over time due to their inability to cancel the gravitational component of the measurement [1]. Odometry based on enconders has extensively been used to provide fast motion estimates for wheeled or legged robots, though this approach is prone to being inaccurate due to wheel\\/leg slippage and the impreciseness of the kinematic robot models. Last, vision-based methods are arguably the most flexible and powerful solution to the motion estimation problem because they can be adapted to work with different types of robots (wheeled, legged, aerial) and configurations (2D-3D motion).'\\n\\n'To validate the results obtained in the simulated experiments, we employ a real mobile robot equipped with a Hokuyo laser scanner which navigates in an office-like environment. Making use of a mobile robot allows us to include the odometry estimates from the onboard encoders (a pair of low-cost AMT102-v incremental encoders from CUI Inc.), but prevents us from performing a quantitative comparison given the lack of a precise ground truth. Therefore, in this section the different methods are compared just qualitatively by plotting 2D maps built purely from the odometry pose estimates. In other words, for each method we present maps built as a concatenation of 2D point clouds along their estimated trajectories, without resorting to global consistency or any other mapping strategy.'\",\"538\":null,\"539\":\"'We would like to thank Pratik Agarwal for the fruitful discussions and Frank Moosmann for providing the dataset and the source code for Moving Object Mapping.'\",\"540\":\"'http:\\/\\/go.vicos.si\\/drdataset'\",\"541\":\"'https:\\/\\/groups.csail.mit.edu\\/rrg\\/starforthefullsequence'\\n\\n'We generated a colored mesh of each test environment using a Kinect RGBD sensor. We used the open-source FastFusion algorithm [25] for the public RGBD dataset and Kintinuous [15] for other environments. The drift in the odometry during mesh building often caused the mesh to be deformed, or created flaws in the texture when closing the loop. While STAR could track camera poses using a deformed mesh map, for quantitative analysis, we used ground-truth Vicon poses to isolate tracking errors from errors in the map. A mesh map of the Vicon room, built with the ground-truth, is shown in Figure 5.'\",\"542\":null,\"543\":\"'Each dot encodes two bits\\\\n(x, y)\\\\n. In order to find out whether the\\\\nx\\\\nbit is 1 or \\u22121, it is sufficient to know on which side of the diagonal (red dashes on the figure) the dot lies. Thus, in order to assign probabilities for\\\\nx\\\\nwe project the dot onto the diagonal by adding the components of the offset from the grid intersection:\\\\n\\u03b4\\\\nv\\\\n+\\\\n\\u03b4\\\\nu\\\\n(and same for\\\\ny\\\\nwith the other diagonal, i.e. By\\\\n\\u03b4\\\\nv\\\\n\\u2212\\\\n\\u03b4\\\\nu\\\\n).'\\n\\n'decoded'\\n\\n'Accuracy of x coordinate measurements when device is stationary, 20 samples each. Position marked with the cross was consistently misdecoded. Best, mean and worst accuracies are calculated with absolute values.'\\n\\n'Accuracy of orientation measurements when device is stationary, 20 samples each. Positive and negative biases are coded with red and blue respectively. Best, mean and worst accuracies are calculated over absolute values.'\\n\\n'Building over algorithms previously developed for digital pens, this article introduces a novel 2D localization technique for mobile robots, based on simple printed patterns. This method combines high absolute accuracy (below 0.3mm), unlimited scalability, low computational requirements (the presented open-source implementation runs at above 45Hz on a low-cost microcontroller) and low cost (below \\u20ac30 per device at prototype stage). The article first presents the underlying algorithms and localization pipeline. It then describes our reference hardware and software implementations, and finally evaluates the performance of this technique for mobile robots.'\\n\\n'Our reference software implementation is available under an open-source license from chili. epfl. ch\\/libdots. It can be built as a standalone library and has been successfully cross-compiled for low-end targets such as the PIC32MZ microcontroller. The repository also provides a sample test application that works with a standard desktop webcam (as long as it permits to focus on close objects, so that printed dots are visible). Tools to generate dot patterns and overlay them on any PDF file are provided as well.'\\n\\n'In this article, we introduced to the robotics community a real-time, pattern-based 2D localization method well suited for mobile robots. Furthermore, our contribution includes an open-source, high-performance software implementation, alongside a reference hardware setup whose accuracy has been validated. The main benefits of our method include:'\",\"544\":null,\"545\":\"'http:\\/\\/tinyurl.com\\/tuhh-hippocampus'\",\"546\":null,\"547\":\"'(a) An image of a corridor scene. (b) The generated events while walking in the corridor. The red points show events with polarity +1 and green ones denote events with polarity \\u22121. (c) The color-coded timestamps of the first events in different pixel locations. (d) The color-coded time difference between two events at individual pixels. When the intensity change is large, the same point will provoke multiple events, and the time difference between the two events on the same point indicates the strength of the contrast.'\",\"548\":null,\"549\":\"'A modified LandTamer 6\\u00d76 all-wheel drive skid-steered vehicle, manufactured by PFM Manufacturing, retrofitted with forward looking lidar and camera units, wheel encoders and a NovAtel SPAN INS solution (with real time RTK corrections) served as the platform of choice for data collection. 3D Terrain and mobility models for the simulation were generated from the data that was collected with this platform from a real world site of approximately 1 square kilometer in area. A small portion of the site, approximately 100 square meters in area was then chosen for simulation (Fig. 4a). The data for building the terrain geometry was collected using the SICK Lidars mounted on a nodding mechanism. A 2.5D height map was generated with the geo-registered lidar data. This map served as the \\u201cground truth\\u201d for the vehicle mobility simulator. Fig. 4c is an overhead view of a colorized model of the real world test site.'\\n\\n'In this section, we describe our efforts to validate the performance of the RHMPPF in the real world. A 4\\u00d74, all-wheel drive, skid-steered vehicle, manufactured by Clear Path Robotics called Husky retrofitted with wheel encoders and a pose system similar to the one on the Land Tamer served as the platform of choice for our field trials. The Husky measures 39 \\u00d7 14 \\u00d7 19.6 inches in its exterior dimensions and had a wheel diameter of 13 inches and 5 inches of ground clearance. We used the ROS based command and control API that was shipped with the platform. The RHMPPF was implemented as a ROS node and produced control commands at 20 Hz. The joystick controller was re-programmed to execute the commands generated by the RHMPPF on demand. Similar to the data collection efforts mentioned above, slip parameters of the terrain and the power train dynamics were learned by driving the Husky on the representative terrains. We evaluated the performance of the RHMPPF against a pure pursuit path tracker using cross track error as the guiding metric. In all our trials, we set the mission speed to 1.0 m\\/s, the top speed of the platform. We chose two terrains:'\",\"550\":null,\"551\":null,\"552\":null,\"553\":\"'Each torque pattern is encoded by a set of knot-point vectors\\\\n{x, y, z}\\\\n, similar to knot points in splines, where x represents the gait phase, y the torque value, and z the multiplicity, respectively. The size\\\\nN\\\\nof the vectors should be the same for all the torque patterns. For 3 speeds (slow, normal, fast) and 3 inclinations (downward, level, upward), we have defined 9 torque patterns and corresponding sets of vectors, some of which are illustrated in Fig. 5. Given current speed and inclination while walking, a set of knot-point vectors is obtained by interpolating the predefined sets of knot-point vectors. A torque pattern is then obtained as a function of gait phase from the set of knot-point vectors corresponding to current speed and inclination as'\",\"554\":\"'We use the hydraulically-actuated robot leg, HyL, in our experiments. HyL weighs approximately 11 kg, is fully-torque controlled and equipped with precision joint encoders, and load cells. HyL is a 1D floating-base system with 2 actuated joints as is shown in Fig. 2. Controller computations are done on-board in an i7\\/2.8 GHz PC with a realtime-time kernel. Motion plans are computed off-line using a predefined terrain model.'\",\"555\":null,\"556\":null,\"557\":\"'http:\\/\\/mrsl.grasp.upenn.edu\\/yashm\\/icra2016.mov'\\n\\n'http:\\/\\/mrsl.grasp.upenn.edu\\/yashm\\/ICRA2016.mov'\\n\\n'Pattern design of the crawler (a, b) color-coded diagrams of the kinematic structure of the robot correspond to linkages in the 2-D layout of the top and bottom laminates (c) the bottom laminate is overlaid on the top laminate and the structure is folded into a robot.'\",\"558\":null,\"559\":null,\"560\":null,\"561\":\"'Sequence of postures from simulation during locomotion on flat terrain. The postures are color-coded from light gray (first iteration) to black (latest iteration).'\",\"562\":null,\"563\":null,\"564\":null,\"565\":null,\"566\":null,\"567\":null,\"568\":\"'github.com\\/Mavericklsd\\/rgbd_pose_estimation'\",\"569\":null,\"570\":\"'http:\\/\\/www.cvlibs.net\\/datasetslkitti\\/eval_odometry.php'\",\"571\":null,\"572\":\"'Whenever UAVs operate outdoors, they typically make use of GPS observations for global positioning without a direct need of performing the mapping task simultaneously. Usually, GPS-based state estimation on light-weight UAVs is based on a L1 C\\/A-code GPS receiver, MEMS inertial sensors and a magnetometer [1]\\u2013[3]. Such a sensor combination only leads to global position accuracies of approx. 2-10m and attitude accuracies of approx. 1\\u20135 deg. This is often good enough to autonomously follow waypoints, but it is typically insufficient for UAV control or for geodetic-grade surveying and mapping applications.'\\n\\n'code ranges'\\n\\n'DD code ranges on the L1 and the L2 frequency, measured in the e-frame, with a rate of 10Hz,'\\n\\n'As new measurements often have only a local effect and fill-in may become expensive, iSAM2 encodes the conditional density of cliques in a so-called Bayes tree, which allows for an efficient incremental reordering, just-in-time relinearization and partial solving, when parameters change only locally. For more details, see [12].'\",\"573\":\"'As high uncertainty is associated to encoder-based odometry of such skid-steering vehicles, the encoders and IMU measurements are pre-fused with the technique presented in Kubelka et al. [10]. The resulting odometry estimate is the one to be processed by Algorithm 1.'\\n\\n'Field robot nifti equipped with an IMU, a rotating laser-scanner and encoders.'\",\"574\":\"'Length error: An encoder mounted to a pulley on the tether arm is used to measure length. This measurement is subject to unbounded drift since the encoder is not absolute, and the tether can slip on the pulley.'\",\"575\":\"'In order to solve the localization problem, one needs to identify a representation of images that is robust to seasonal changes. To solve this problem, the images are represented using semantic categories rather than appearance (as encoded by, e.g., the SIFT vector). The key idea is that even though the appearance of the map might change, the underlying semantics remain the same.'\\n\\n'Edges, such as those between a dirt-road and grass, implicitly encode a change in semantic category. Image semantics can be compared using a measure of edge-overlap between images. An alternate formulation to measure semantic similarity would be to compute the overlap area between the same semantic categories. We present both edge-based and area-based semantic localization in this section.'\\n\\n'The first column corresponds to the warped panorama (top). Remaining columns (left to right) correspond to ground truth for the spring, summer, and winter maps. The binary images for the satellite are obtained via kmeans clustering. The area-based or edge-based representation encodes the ground\\/non-ground semantics.'\",\"576\":\"'https:\\/\\/wiki.qut.edu.au\\/display\\/cyphy\\/Datasets'\\n\\n\\\"To generate the sequential heat map, the previous interpolated heat map is taken and translated the same distance as the shift in the query image location from the previous query location and then summed with the current query image heat map. For these experiments we used simulated odometry with varying noise models; for a live robot implementation this data would come directly from either the robot's wheel encoders or a visual odometry system, or both. The best match position and closest reference image match were then found using the same process as for the single frame matching method. All code and datasets are freely available at the following link: https:\\/\\/wiki.qut.edu.au\\/display\\/cyphy\\/Datasets\\\"\\n\\n'The current algorithms are implemented as unoptimized Matlab code. For the datasets presented here, the primary computational overhead is the image comparison process. When comparing a query image to a dataset of 50 reference images, at a resolution of 48 \\u00d7 24 pixels at every rotation (48 rotations), just under 3 million pixel comparisons are performed for every query image. A single core CPU can perform approximately 1 billion single byte pixel comparisons per second, while a GPU can do approximately 80 billion per second using optimized C code. Hence the techniques presented here could likely be performed in realtime on a robotic platform when optimized, even on lightweight computation hardware.'\",\"577\":null,\"578\":null,\"579\":null,\"580\":\"\\\"In this work a novel synergy-based bilateral tele-manipulation strategy is introduced. The proposed algorithm has been primarily developed to remotely control the Pisa\\/IIT SoftHand (SH) using a 3-finger hand exoskeleton as master device. With a single actuator and a sensory system limited to a position encoder and a current sensor, the SH minimalist design promotes robustness but challenges traditional teleoperation strategies. To tackle this challenge, the concept of Cartesian-based hand synergies is introduced as a projection tool which maps the fingertip Cartesian space to the directions oriented along the grasp principal components. The unconstrained motion of the operator's hand is projected on this space to extract the SH's motor position reference. Conversely, the interaction force estimated at the robotic hand as a 1-dimensional force along the first synergy is projected to the 9D fingertip Cartesian space through an inverse projection. The resultant finger-individualized forces form a synergy based weighted representation of the grasping effort applied by the SH and are displayed to the operators fingertips using the force feedback hand exoskeleton. The system's ability to reflect the environment's impedance has been experimentally validated during a ball squeezing experiment. To assess the overall effectiveness of the proposed system as a manipulation interface, the SoftHand was mounted on the humanoid robot COMAN and the setup was subsequently enriched with a vision-based tracking system monitoring the operators wrist trajectory. Experimental results indicate that the proposed body-machine bilateral interface allows for the intuitive performance of stable grasps and transport of a large range of diversely shaped objects.\\\"\\n\\n\\\"The configuration of the exoskeleton links is monitored using the position encoders mounted on all joints. The 6-DoFs Cartesian trajectory of the operator's fingertip\\\\nx(t)\\\\ncan be computed straightforwardly using the forward kinematics of the exoskeleton fingers as shown in Eq. 1.\\\"\",\"581\":\"'https:\\/\\/github.com\\/atabakd\\/Continuousaffordances'\\n\\n'Denoising auto-encoders for learning of objects and tools affordances in continuous space'\\n\\n'The concept of affordances facilitates the encoding of relations between actions and effects in an environment centered around the agent. Such an interpretation has important impacts on several cognitive capabilities and manifestations of intelligence, such as prediction and planning. In this paper, a new framework based on denoising Auto-encoders (dA) is proposed which allows an agent to explore its environment and actively learn the affordances of objects and tools by observing the consequences of acting on them. The dA serves as a unified framework to fuse multi-modal data and retrieve an entire missing modality or a feature within a modality given information about other modalities. This work has two major contributions. First, since training the dA is done in continuous space, there will be no need to discretize the dataset and higher accuracies in inference can be achieved with respect to approaches in which data discretization is required (e.g. Bayesian networks). Second, by fixing the structure of the dA, knowledge can be added incrementally making the architecture particularly useful in online learning scenarios. Evaluation scores of real and simulated robotic experiments show improvements over previous approaches while the new model can be applied in a wider range of domains.'\\n\\n\\\"One way to encode the aforementioned dynamic relationships is through the concept of affordances. Affordances can be defined as sensorimotor contingency patterns in the perceptual stimulus [4] which arise from action possibilities that an environment can offer an agent. The term was first introduced in J. J. Gibson's influential work [5] and has gathered the attention of many researchers from diverse fields such as developmental psychology, neuroscience and robotics.\\\"\\n\\n\\\"This is also the scenario which will be presented in this work. More specifically, we will explain how a denoising Auto-encoder (dA) [7], which are inspired from Multi-Layer Perceptrons (MLPs), can be trained to find relations between objects' and tools' shape descriptors with actions and the effects of executing those actions - the relative displacement of object. dA has been selected to encode the knowledge of affordances for several reasons. First, since they are a variant of neural networks, they can be applied in online learning scenarios. Second, they impose no constrain on the type of inputs they can use (continuous or discrete) and third, they were applied successfully in fusing multi-modal information and retrieve one modality given the others [8].\\\"\\n\\n'The particular architecture of dA has another desired property to model object affordances. In order to better explain this property, we contrast dA to its predecessor MLPs. If an MLP was used to encode the knowledge of affordances, one would need to train a network which uses objects and tools features and actions, and outputs the effect. Then it was necessary to train another network with objects and tools features and effects, to infer the action, and so on. In addition, different combinations would all require a dedicated network, e.g. given object features and effects, infer tool features and actions. On the other hand, a dA architecture fuses all the modalities together and thus, one network can be used to infer all arbitrary combinations of missing and available features and modalities (Of course not all queries will lead to meaningful results. For example one can provide tool and object features to the network and the network will provide answers for actions and effects, which in this case it would be hard to interpret these outputs).'\\n\\n\\\"The effects of action execution are considered as the 2D displacement of the object's center of mass along the lateral and longitudinal directions on the table's plane. To calculate this value, the position of the object on the image plane before the action execution and 5 seconds after action execution is recorded. The difference between these two values serves as the effect. In longitudinal movement, the positive direction means closer to the robot (down in image space) and the negative direction means farther away from the robot (up in image space). In lateral movement, positive and negative values are used to encode right and left movements, respectively.\\\"\\n\\n'C. Denoising Auto-Encoders for Learning and Using Object Affordances'\\n\\n'We propose to use an over-complete denoising Auto-encoder (dA) to capture the structure of the affordance dataset. The dA for this work consists of one encoding layer\\\\ng\\\\n\\u22121\\\\nand one decoding layer\\\\ng\\\\n(Fig. 2). A simple auto-encoder takes an input vector\\\\nx\\u2208[0,1\\\\n]\\\\nm\\\\nand maps it to a hidden representation\\\\nz\\u2208[0,1\\\\n]\\\\nn\\\\nthrough a deterministic mapping\\\\nz=\\\\ng\\\\n\\u22121\\\\n(x)\\\\nwhere:'\\n\\n'Schematics of an auto-encoder where x is the input, z is the hidden representation and\\\\nx\\\\n^\\\\nis the reconstruction.\\\\nb\\\\ni\\\\ns\\\\nare the bias terms associated with hidden layer units and\\\\nb\\\\n\\u2032\\\\ni\\\\ns\\\\nare bias terms associated with the output layer.'\\n\\n'One approach to make an over-complete auto-encoder to learn useful features is by adding random noise to input, but force the network to reconstruct the denoised version of the input [7]. The network cannot reconstruct its input from noisy observations, unless it could capture the structure of the dataset. In this case, the learning algorithm tries to converge z to a representation which is useful to reconstruct the input when some features are not available.'\\n\\n'In this section we will discuss the training process of the proposed over-complete denoising auto-encoder and validate its performance on two datasets. One of these datasets was collected using iCub simulator and the other was collected from the real robot.'\\n\\n'A. Training Process of Denoising Auto-Encoder'\\n\\n'In this work we have proposed the use of denoising auto-encoders as a computational framework to learn and reason about object affordances. This model is able to find structures which can relate object and tool features with actions and effects. These representations can be later used to 1) predict the effects of action executions on different objects with different tools and 2) imitate effects by selecting the correct action and tool. The comparison of dA performance with previous approaches shows its superior generalization capabilities while maintaining desirable expressiveness power. Infering the action and tool directly from object features and observed effect is one of the main contributions of this work. Our experiments indicates that going from simulation datasets to real datasets does not impose any significant degradation of performance and implies effectiveness of this approach on real robot scenarios.'\\n\\n'Denoising auto-encoders are mostly used in the deep learning literature as a pre-training step for multi-layer neural networks. To the knowledge of authors, this work shows the first use of dAs in relatively small datasets with low amount of features. To take this work to where dAs shine the brightest, new scenarios must be defined where the features are not predefined but are learned from camera images and actions are not just labels but the dynamics of joint angles movements. On the other hand, as the results of the current work suggest, dA can be trained to find interesting relations even with small datasets. One possible approach to further investigate the capabilities of the proposed architecture is to use it as an online learning mechanism in real robot scenarios. The dA can benefit from more tool and object features which better covers the space because of its relatively large expressive power.'\\n\\n'https:\\/\\/github.com\\/atabakd\\/Continuousaffordances'\",\"582\":null,\"583\":null,\"584\":null,\"585\":null,\"586\":null,\"587\":\"'coded'\\n\\n'Other extensions include dynamic gain and code selection and their incorporation into the resulting feature values, and an exploration of features based on spatial flow distributions and dynamics. We anticipate automating the collection of large datasets allowing the application of modern, highperformance learning algorithms and, ultimately, using these to drive manipulation decisions. Finally, we anticipate installing a force sensor to allow a robot to learn the visual estimation of physically grounded quantities.'\",\"588\":null,\"589\":null,\"590\":\"'https:\\/\\/vimeo.com\\/152272960'\",\"591\":null,\"592\":null,\"593\":\"'Observed magnetic flux density is shown for different curvature values. Magnetic flux densities are obtained from finite element analysis as a look-up table. Position calculations are coded in matlab. The distance between the magnet and the hall element is 8 mm.'\",\"594\":null,\"595\":null,\"596\":null,\"597\":null,\"598\":null,\"599\":null,\"600\":\"'Homology is an algebraic means to identify the holes in a topological space. It is based on the concept of a boundary homomorphism where simplicial homology encodes how simplices are attached to their lower dimensional faces.'\\n\\n'To aid our understanding of the results we present the barcode diagrams for each scene in Figs. 6(a)-6(f). In these diagrams, the x-axis represents the filtration values and the y-axis represents the 0-dimensional generators of homology. The length of the blue lines corresponds to the lifespan of the generators of homology. Shorter lines are points that die early in the filtration while longer lines are points that persist for a greater number of steps. A line with a blue triangle equates to a point with infinite persistence; the corresponding 0-dimensional generating cycle persisted past the end of the filtration and did not get filled in as a boundary after all simplices of the underlying complex had been added.'\\n\\n'The barcode diagrams for the scenes in fig. 5. Lines with blue triangles show points of infinite persistence.'\\n\\n'For future work, we would like to combine the global information provided by topology with local information, such as color and surface normals, to perform region-based segmentation of objects in point clouds. Exploring the use of higher dimensional homology groups for segmenting groupings of voxels (supervoxels) is on our agenda. Lastly, we intend to make available an open source release of our software to the point cloud processing community for use in 3D perception research.'\",\"601\":null,\"602\":null,\"603\":\"'Evaluation of the computed relative pose between color and depth camera. The depth map is rendered into the viewpoint of the color camera, such that image and geometry boundaries will align well under a correct calibration. The full resolution images and the different cutouts show the image, overlaid with the warped, color coded depth map.'\\n\\n'We would like to thank Chris Sweeney and Sameer Agarwal for their support for the optimization with Ceres, Christian H\\u00e4ne for providing code for the volumetric depth map fusion, and the anonymous reviewers for their helpful comments.'\",\"604\":null,\"605\":\"'Please zoom in. The edge-fattening effect of camera motion on the depth image. First row: (a) And (b) are the color and depth images of a meshed plate taken by a stationary camera. In it is marked the row within a green rectangle. In (c) is plotted the depth values at this particular row, as a function of column number. This is for the row marked in green in (a). Second row: (d) And (e) are the color and depth images of the same scene taken by a camera rotating at 40 rpm, with the same row marked (f) is the corresponding depth histogram plot. (g) Gives the color coding for the maps (b) and (e). Each pixel in the depth maps (b) and (e) encodes the depth recorded by the camera at that location. Missing information is given zero depth. As can be seen in the (b) and (c) for the stationary scene, the gaps in the viewed grill are present. As they are occluded in the depth maps, the values of the depth map at those locations are in-fact marked zero. The dips in (c) also show this. In comparison, in the second row, edge fattening causes the filling up of those gaps and causes a fiat-lining of the histogram (f).'\",\"606\":\"'http:\\/\\/tinyurl.com\\/oc4uwlm'\",\"607\":null,\"608\":null,\"609\":null,\"610\":null,\"611\":null,\"612\":null,\"613\":null,\"614\":null,\"615\":\"'The 3-D simulator used here is based on the open-source Open Dynamics Engine (ODE) library [24]. The robots operate in an unbounded continuous underwater environment, and are assumed to be neutrally buoyant.'\",\"616\":null,\"617\":null,\"618\":\"'Two joint modules as the proof of concept (POC) are developed in order for actuator, encoder and controller to be mounted inside a single body for the modularization of the joint. The structure of each joint module is as shown in Figure 4. The specification of joint modules is as shown in Table I.'\",\"619\":null,\"620\":null,\"621\":null,\"622\":null,\"623\":null,\"624\":\"'The sensing between pairs of robots is limited to a subset of the whole team. This limitation is encoded in the interaction graph\\\\nG=(V, E)\\\\n. The nodes in the graph,\\\\nV\\\\n, represent the different robots of the team, whereas the edges,\\\\n(i,j)\\u2208E\\\\n, represent the availability of the relative position and orientation of robot\\\\nj,[\\\\np\\\\nij\\\\n,\\\\n\\u03b8\\\\nij\\\\n]\\\\n, to robot\\\\ni\\\\n. This way, the set of robots observed by robot\\\\ni\\\\nis denoted by\\\\nN\\\\ni\\\\n.'\",\"625\":null,\"626\":null,\"627\":null,\"628\":null,\"629\":\"'In addition to these standard measures, much work has been done to apply error correcting codes to wireless communications through FEC. The general schema of utilizing FEC in this domain is that by including additional redundancy information in transmissions, the system can recover lost data. This has seen numerous applications, from more reliable video streaming [2] to a more stable platform for TCP in environments with high packet loss [3].'\\n\\n'Reed-Solomon codes are a class of error-correcting codes first presented by Reed and Solomon in the early 1960s [15]. While Reed-Solomon codes are widely used for correcting both errors and erasures, our use case only requires the latter, since we assume the integrity of received packets is verified by a cyclic redundancy check. An\\\\n(n, k)\\\\nReed-Solomon code is used to produce\\\\nm=n\\u2212k\\\\nparity symbols from\\\\nk\\\\ndata symbols. When correcting erasures, an\\\\n(n, k)\\\\nReed-Solomon code has the property that if any\\\\nk\\\\nof the\\\\nn\\\\ntransmitted symbols are received successfully, then all of the\\\\nn\\\\nsymbols can be fully recovered. This property is the foundation on which our system operates.'\\n\\n'The batch encoding process, using a (4, 3) reed-solomon code. The payloads of the three original packets in the batch are encoded to produce a single parity packet.'\\n\\n'The batch decoding process for the batch encoded in fig. 3. The second original packet is not received, but the decoder is able to recover its contents using the other received packets.'\\n\\n'Algorithm 1 details the encoding calculation. Beginning with 0, the Calcencodestrength procedure iteratively proposes parity packet amount\\\\nm\\\\nto transmit along with the data packets. For each\\\\nm\\\\n, an expected distribution of received packets from the\\\\nn=k+m\\\\ntransmitted packets is obtained through Monte Carlo simulation. The algorithm then draws from this distribution to compute the expected value of the effective PRR for that parity amount. If this expected value is greater than or equal to the target, Calcencodestrength returns\\\\nm\\\\n. Otherwise, the process continues until\\\\nm\\\\nreaches its maximum value, which can be set based on bandwidth or power consumption considerations.'\\n\\n\\\"Immediately after it is received from the physical channel, a packet is passed into the FEC module. If a packet's payload is not encoded (i.e. it is a data packet), the packet is immediately passed to the routing module. Copies of the contents of all received packets are stored temporarily to be used in the batch decoding process. Once any\\\\nk\\\\nof the batch's\\\\nn\\\\npackets have been received, the entire batch can be decoded. If any of the data packets were not received, the entire batch is passed through a decoder, as illustrated in Fig. 4. The decoding procedure is symmetric to the encoding process previously described, except that all of the data can be reconstructed from only\\\\nk\\\\nof the\\\\nn\\\\ntransmitted packets. For\\\\ni=1,2,\\u2026,\\\\nl\\\\nmax\\\\n, byte\\\\ni\\\\nof each of the received packets is inserted into a buffer. The index of each byte in the buffer is determined by the position of the corresponding packet in the batch. By passing this buffer into an\\\\n(n, k)\\\\nReed-Solomon decoder, the\\\\nm\\\\nerasures can be corrected. Through this erasure-correction process, the contents of any missing packets can be recovered. Once the decoding process is complete, recovered packets are passed to the routing module.\\\"\\n\\n'The second result of increasing latency tolerance is reduced overhead in terms of bandwidth usage. Due to the uncertainty present in the estimation of link quality, encoding strength decisions are necessarily conservative. As latency tolerance increases, so do the sizes of the batches being encoded. This allows for a finer granularity in encoding strength decisions. Even with an estimated link quality of 100%, the system will still elect to send parity packets due to the uncertainty of the estimate. In the extreme case of a single-packet batch, adding a parity packet results in twice the bandwidth usage.'\",\"630\":\"'https:\\/\\/www.youtube.com\\/watch?v=Ug5Ne6v34iQ'\",\"631\":null,\"632\":null,\"633\":null,\"634\":null,\"635\":\"'To acquire the abstract state and the parent state, as well as collect data on the effectiveness of the controller, the setup uses ARToolKit, an open source fiducial tracking software package designed for augmented reality applications [12], and a wide-angle camera to achieve real time tracking of the robots as well as the plane. This system can track the robots and plane at about 30 frames per second. The controller operates at 10Hz, so this framerate is sufficient; every third frame is pulled for analysis. The position reported by the ARToolkit software has a standard deviation of about 2mm with the hardware used.'\",\"636\":null,\"637\":null,\"638\":null,\"639\":\"'https:\\/\\/data.open-ease.org\\/'\",\"640\":null,\"641\":null,\"642\":\"'Four illustrative simulated domains, and execution costs for different strategies on the domains. The top row shows the configurations of three containers; object types are color-coded, and the distances between containers are to-scale. The bottom row shows box plots of total execution costs (including motion, removing, and observation actions) over 25 trials for 6 different planning strategies. Each box and whisker visualizes the distribution of executions costs: the red line is the median, the box shows the first and third quartiles (i.e., is the inter-quartile range), the whiskers extend to the range of the costs, and the \\u201c+\\u201d markers are significantly beyond this range. See text for a comparison between the different strategies on each domain.'\",\"643\":null,\"644\":null,\"645\":null,\"646\":\"'https:\\/\\/youtu.beopsmd5yubf0\\/'\",\"647\":\"'https:\\/\\/github.com\\/robotology\\/poeticon'\\n\\n'The outcome of the grounding is a list of possible actions, given robot knowledge and what surrounds it. For each rule, the grounding module will find the possible objects match, creating several possible grounded actions, e.g., grasp_obj_with_hand becomes grasp_spoon_with_left, grasp_spoon_with_right, grasp_tomato_with_left, etc. Then, for each of these actions, we predict the effects by querying the Bayesian Network: the effects are encoded as probability distributions, and therefore the actions become probabilistic rules that can be used by the planner. The list of possible outcomes is now associated to their respective probability:'\\n\\n'https:\\/\\/github.com\\/robotology\\/poeticon'\",\"648\":null,\"649\":\"'The Pioneer P3DX robot is equipped only with its basic sensory set, namely encoders and ultrasound sensors (sonars). As we use a-priori known static map, sonars are not used. On the software level, the Pioneer computer runs Ubuntu 14.04 with ROS Indigo. There are several ROS packages simultaneously running on-board Pioneer to support mission execution - p2os_purdue driver (communication with low-level controller), move-base package (navigation and position control), action server (interface between navigation stack and high level planner) and an instance of developed high level mission planner, described in Sec II.'\",\"650\":\"'For generality, we encode tasks with the traditional SMDP representation\\\\n{S, A, R, P}\\\\n. Due to the lack of assumptions regarding symbolic skill knowledge, we represent environment states as compositions of functions\\\\nf\\\\na\\\\nn\\\\n\\u2218\\\\nf\\\\na\\\\nn\\u22121\\\\n\\u2218\\u2026\\u2218\\\\nf\\\\na\\\\n1\\\\n(x)\\\\n, where x is the initiation state at the time of execution and\\\\nf\\\\na\\\\ni\\\\n:S\\u2192S\\\\nrepresents the effect that\\\\na\\\\ni\\\\nhas on the environment. This function composition serves to map environment states satisfying preconditions of\\\\na\\\\n1\\\\nto environment states satisfying postconditions of\\\\na\\\\nn\\\\n. Each of these\\\\nf\\\\na\\\\ni\\\\nis taken directly from the action sequence followed by the agent through the state space to reach the current state. We assume the capability to identify commutativity between\\\\nf\\\\na\\\\ni\\\\n, as these compositions will result in equivalent final states.'\\n\\n'The tasks used in this evaluation were generated using encoded IKEA furniture assembly tasks, a domain previously explored in multi-agent collaborative scenarios [32], as a statistical basis for the task structure. These tasks had subtask commutativity\\/invariance, per-skill observation set overlap, and action set sizes sampled from distributions modeled after real assembly tasks. These scenarios serve as a convenient method of providing arbitrarily complex tasks within which to demonstrate the scaling capabilities of our work. Our dataset included 76 tasks, each with 4 randomly determined execution paths. These tasks ranged in size from 10 to 100 subgoals, consisting of parameterized pick, place, and fasten actions. Within the CC-HTNs, the average depth for a motor primitive ranged from 1.99 to 3.61, with a mean across all tasks of 2.84.'\",\"651\":\"'The pseudocode is presented in Algorithm 1. The input to Algorithm 1 is an array containing a sequence of the coordinates of the sites of a target structure; the coordinate of a site actually refers to the coordinate of its centroid. See Fig. 6a for the example target structure we will use here. The algorithm returns a feasible assembly sequence, a reordering of the input sequence. Algorithm 1 actually features disassembly sequencing; however, in the pseudocode, we just say that a \\u201csite\\u201d is disassembled from a structure, instead of a physical \\u201crobot\\u201d, because we do not need physical robots to describe a target structure. Algorithm 1 can be made run in\\\\nO(m)\\\\ntime, where\\\\nm\\\\nis the number of the member sites, as will be explained in the following paragraphs.'\",\"652\":null,\"653\":\"'First, in order to encode the fact that some robots may already have a goal assigned, and hence could be able to behave as relays for the currently replanning robots, we modify\\\\nG\\\\nt\\\\nby substituting the vertices assigned to non-ready robots with ficticious communication edges directly connecting the neighbors of the removed vertices with the BS. Moreover, we double each undirected communication edge, except for the set of edges incident to the BS, for which we keep only the outgoing arcs. With a slight abuse of notation, let us denote again as\\\\nG\\\\nt\\\\n=(\\\\nV\\\\nt\\\\n, \\\\nC\\\\nt\\\\n)\\\\nthe modified graph derived as described above. We define the following binary variables:'\",\"654\":null,\"655\":\"'https:\\/\\/vimeo.com\\/139342248'\\n\\n'https:\\/\\/vimeo.com\\/139342248'\\n\\n'https:\\/\\/vimeo.com\\/139342248'\\n\\n'For each predicate, a detector has been coded, e.g.:'\",\"656\":\"'Large parts of our framework, including our new multimodal annotation tool and the parameters used to obtain our results, are publicly available as open source, to allow researchers to quickly reproduce our results on their own datasets and to evaluate their own algorithms using our framework.'\\n\\n'(a) All components of our framework are implemented as separate, reusable ROS modules, most of them open source. So far, we have integrated four existing tracking methods [7]\\u2013[10] into our framework for a fair comparison under identical conditions. (b) Our new multi-modal track annotation tool, based upon rviz. The 3D visualization encompassing RGB-D and laser point clouds as well as annotated trajectories (red line with yellow waypoint markers), along with 2D camera views with projected annotations (small images), significantly speeds up the annotation process. (c) Our service robot platform, equipped with front-and rear-facing 2D laser and RGB-D sensors. (d) Our mobile sensor platform, in a similar sensor configuration.'\\n\\n'Fig. 2a gives an overview of the main components of our modular people tracking framework. All of these components are fully integrated into ROS and publicly available and documented on GitHub1 . In the following, we will briefly describe the most important components, starting from the left at the detection layer.'\",\"657\":\"'Table I shows the run-time for each component of our pipeline. The evaluations are done on a machine with Intel i7-4790K CPU (using only single core of CPU). The verifying phase is done as a post processing step. The publicly available MATLAB code of the jointDeep detector [15] is used as the verifier. The verifier can be optimized to reduce the run-time. The frame rate of the whole pipeline (including verifier) is 22 fps and without verifier it is 45 fps.'\",\"658\":null,\"659\":null,\"660\":null,\"661\":null,\"662\":null,\"663\":null,\"664\":\"\\\"Although we focused here only on the information provided by the LMSs, data provided by other sensors (such as wheel encoders, IMUs and GPSs) could be integrated in order to achieve greater robustness and eventually obtain a precise robot's positioning.\\\"\",\"665\":null,\"666\":\"'Point cloud of the egocentric multiresolutional surfel map as viewed by the robot operator during the debris task of the first DRC competition run. The color encodes height.'\",\"667\":null,\"668\":null,\"669\":null,\"670\":\"'Place recognition is a key capability for navigating robots. While significant advances have been achieved on large, stable platforms such as robot cars, achieving robust performance on rapidly manoeuvring platforms in outdoor natural conditions remains a challenge, with few systems able to deal with both variable conditions and large tilt variations caused by rough terrain. Taking inspiration from biology, we propose a novel combination of sensory modality and image processing to obtain a significant improvement in the robustness of sequence-based image matching for place recognition. We use a UV-sensitive fisheye lens camera to segment sky from ground, providing illumination invariance, and encode the resulting binary images using spherical harmonics to enable rotation-invariant image matching. In combination, these methods also produce substantial pitch and roll invariance, as the spherical harmonics for the sky shape are minimally affected, providing the sky remains visible. We evaluate the performance of our method against a leading appearance-invariant technique (SeqSLAM) and a leading viewpoint-invariant technique (FAB-MAP 2.0) on three new outdoor datasets encompassing variable robot heading, tilt, and lighting conditions in both forested and urban environments. The system demonstrates improved condition- and tilt-invariance, enabling robust place recognition during aggressive zigzag manoeuvring along bumpy trails and at tilt angles of up to 60 degrees.'\",\"671\":\"'http:\\/\\/www.povray.org\\/'\\n\\n'A fundamental step to solve bin-picking and grasping problems is the accurate estimation of an object 3D pose. Such visual task usually rely on profusely textured objects: standard procedures such as detection of interest points or computation of appearance-based descriptors are favoured by using a highly informative surface. However, texture-less objects or their parts (i.e., those whose surface texture is poorly conditioned) are common in any environment but still challenging to deal with. This is due the fact that the distribution of surface brightness makes difficult to compute interest points or appearance-based descriptors. In this paper, we propose a method to estimate the 3D pose for texture-less objects given a coarse initialization: the pose is estimated using using edge correspondences, where the similarity measure is encoded using a pre-computed linear regression matrix. Furthermore, we also propose a method to increase the robustness of the estimated pose against background and object clutter. We validate both methods by using synthetic and real image sequences with objects with known ground truth.'\",\"672\":null,\"673\":\"'Analysis of behavior using video is a promising approach for identifying risk markers for psychopathology that can be applied in a wide range of populations. Computer vision techniques are needed to automatically code behavior in order to reduce time and effort in these analyses. This paper discusses algorithms developed for the automatic analysis of video data from a study regarding the impact of environmental factors on youths with obsessive-compulsive disorder. Overhead videos of subjects washing hands were automatically annotated for activities such as rinsing, applying soap, and turning on\\/off the water faucet. These automated annotations were created by using a color-based background subtraction method to create a foreground probability score which is then used to determine if various labeled regions of interest (ROIs) are activated. These activation signals are then characterized and used to determine when different substeps of the handwashing procedure are performed. Automated annotations were validated by comparisons with hand-labeled ground truth.'\\n\\n'In order to compare against the ground truth, the start\\/end times from the characterized activation signals must be coded into incident times for the four activities of interest: turnsOnWater, turnsOffWater, appliesSoap, and rinsesSoap. This was done by creating a set of rules for which start time indicates the beginning of an activity. For instance, the water cannot be turned off before it is turned on and water cannot be turned off before soap is rinsed, so turnsOffWateris characterized as the first faucet activation that occurs after rinsesSoap. The appliesSoap activity is characterized as the first soap activation that occurs. The rinsesSoap activity is characterized as the first sink bowl activation that occurs after turnsOnWater and appliesSoap. The turnsOnWater activity is characterized as the first faucet activation that occurs.'\\n\\n'As part of the larger study, the videos were coded to indicate the time at which each activity occurs. This coding was performed manually by 3 individuals, and those codes were combined to create a consensus score. These manual annotations were used as a ground truth for comparing against the automated coding methods. Accuracy was calculated by comparing the difference between algorithm-computed and ground truth times for each activity. Algorithm-computed times were deemed correct if they fell within a time window centered on the ground truth time. Accuracy for an activity is defined as the ratio of correctly computed times versus the number of videos containing the activity. The size of the time window was varied in order to characterize the sensitivity of accuracy to the time window. Accuracy was chosen over the more common precision\\/recall since it is not possible to have a false positive; the number of activities is fixed so there will always be a coded start time for each activity. This can lead to a false negative if the algorithmically-generated code is not within the time window. However, no false positives can be generated, so precision will remain static as the time window is varied in size.'\\n\\n'Additionally, we would like to consider more handwashing activities, particularly activities related to drying (such as picking up a towel and dropping the towel). There are other aspects as well that were coded manually but do not yet have an automated coding, such as how many towels were used (for disposable towels), how much soap was used (in terms of number of pumps on the dispenser), and other miscellaneous behaviors like if the subject wiped down the sink top with a towel. The dataset also includes videos from a frontal view point and the overhead videos that were used. Incorporating these frontal views could provide another avenue for automated coding.'\\n\\n'These techniques could be applied to the other activities included in the study, especially the free arrange and arrangement in contrasting environments. These activities, as well as the handwashing activity, have already been coded by hand and data generated in the manual coding is currently undergoing statistical analysis.'\",\"674\":null,\"675\":\"'Imaging a metallic surface, however, is a challenge due to the shiny and reflective characteristics. This paper proposes a VBM system to recognize the groove geometry for a robotic welding process and its integration to a welding setup. Hardware and software are combined to map the welding groove geometry providing the necessary data to adjust the control settings of the welding equipment. A number of alternatives is tested to evaluate the capabilities and impacts of each algorithm choice. The aim of our proposal is to be used on steel plates from 13 to 20mm thickness, with V-shaped bevels that vary from 3 to 9mm cut at 45 to 55 angle. The system is implemented on top of a Bug-O Matic Weaver welding robot, manufactured by Bug-O Systems International, using an Altera DEO-Nano FPGA board and a Terasic D5M 5 megapixel camera. Groove detection is synchronized with encoder based position.'\\n\\n'Once we know the possible groove edges, a greedy Non-Maximum Suppression algorithm is used to separate false positives and negatives that still remain after the image processing and determine the point that better represents the groove edges on the image. It is known that the steel plates thickness can vary from 13mm up to 20mm and that the groove angle can vary within 45\\u201355 degrees. Also, the gap between the plates that are being welded has to be something between 3mm and 9mm. Using the application domain information we can establish some threshold values to filter the outputs of the vision algorithms. Pixel to millimeter conversion and world position association achieved via encoder based dead-reckoning complete the mapping, supporting the decision making and welding equipment actuation.'\\n\\n'The system is integrated with a reliable and widely accepted industrial robot. Encoders are used as odometer sensors (dead reckoning) of the tractor and weaver arm. The data is decoded using an 8-bit quadrature decoder module designed in VHDL. Both tractor and weaver speed setpoints are calculated using a\\\\n\\u0394\\u03a3\\\\nmodule with 12 bit resolution.'\",\"676\":null,\"677\":null,\"678\":\"'http:\\/\\/www.cs.columbia.edu\\/~yli\\/ICRA2016'\",\"679\":\"'http:\\/\\/tiny.cc\\/quadrotorRCPx'\\n\\n'This condition encodes that the velocity vector at each vertex points in the right direction so that trajectories leave the simplex through an exit facet while avoiding crossing the restricted facets. The feasibility of the inequalities in (3) can be easily checked numerically via a linear program. If they are not feasible, then RCP is not solvable and the choice of restricted and exit facets must be modified. If they are feasible, then for a feasible choice of\\\\nu\\\\ni\\\\n,i\\u2208\\\\nI\\\\nn\\\\n, it can be shown that one can construct an affine feedback to be used over the entire simplex, see [10]. The affine feedback controller has the form'\\n\\n\\\"Next, to define the sequence of simplices to be visited, for each simplex we choose its restricted and exit facets. This information is also encoded in Figure 5 via the red dashed lines. For the L2R mode, the facets were chosen so as to ensure that the resulting closed-loop vector field causes trajectories to reach the set\\\\nB\\\\nright\\\\n. Due to symmetry, the R2L can be implemented trivially using the L2R mode's design.\\\"\\n\\n'Our experimental platform is the Parrot AR. Drone 2.0 running firmware version 2.3.3. We interface with the AR. Drone through ROS, an open-source robot operating system [21]. More precisely, we used ROS Hydro, installed on a 64-bit 12.04 Ubuntu version. In addition, we used the ROS ardrone autonomy package [21], version 1.3.1. All experiments were conducted with the indoor hull shown in Figure 7, which protects the vehicle propellers.'\",\"680\":\"'http:\\/\\/gitlab.robotran.be\\/walkman\\/walkman_robotran'\",\"681\":null,\"682\":null,\"683\":null,\"684\":null,\"685\":null,\"686\":\"'http:\\/\\/tinyurl.com\\/semantic-mapping-QUT'\\n\\n'http:\\/\\/tinyurl.com\\/semantic-mapping-QUT'\\n\\n'A semantic map generated by the system described in this paper. The colors encode the semantic categories of different places encountered in the environment. The figure shows a map of an office environment (orange) with a kitchenette (dark green) and a long corridor (light green).'\\n\\n'An illustration of the semantic map structure. Level zero of the map is an regular occupancy grid but the higher levels encode the probabilities that a grid cell belongs to a certain semantic category. Each layer in the map represents one semantic category. The cells are updated according to equation (6).'\\n\\n'Fig. 5 shows the output of our semantic mapping system using the RGB images in the nine different tested environments on campus. The map are color coded where the only the winning labels are rendered. The percentage values given in the figure correspond to the fraction of correctly classified images listed in Table I, but do not necessarily reflect the correctly classified map area.'\",\"687\":\"'http:\\/\\/robotvault.bitbucket.org\\/'\\n\\n'http:\\/\\/robotvault.bitbucket.org\\/'\\n\\n'https:\\/\\/github.com\\/ankurhanda\\/blender_scripts'\\n\\n'http:\\/\\/bit.ly\\/lgi642J'\\n\\n'Note that all the distances used in our constraints are actually the projections on the plane corresponding to the ground floor (XY) of the actual 3D distances. The pseudocode of the optimisation is outlined in Algorithm 1.'\\n\\n'We build an open-source repository of annotated synthetic indoor scenes - the SceneNet Basis Scenes (SN-BS) - containing a significant number of scenes downloaded from various online repositories and manually labelled. Given an annotated 3D model, it becomes readily possible to render as many high-quality annotated 2D views as desired, at any resolution and frame-rate. In comparison, existing real world datasets are fairly limited in their size, e.g. NYUv2 [7] provides only 795 training images for 894 classes and SUN RGB-D [26] provides 5, 825 RGB-D training images with 800 object classes. Considering the range of variability that exists in real world scenes, these datasets are clearly limited by their sample sizes. To add variety in the shapes and categories of objects, we augment our basis scenes by generating new scenes from models sampled from various online 3D object repositories.'\\n\\n'https:\\/\\/github.com\\/ankurhanda\\/blender_scripts'\",\"688\":null,\"689\":null,\"690\":\"'The image encoded in opponent color space is used in global and local saliency computations. An example of opponent color space is depicted in Figure 2.'\",\"691\":null,\"692\":\"'a novel method for parametrically defining the topological structure and spatial layout information encoded in spatial language phrases. This allows the robot to decode the spatial meaning communicated through symbolic language phrases.'\"},\"Stars\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":13,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":3,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":0,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":5,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":6,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":4,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1},\"Forks\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":26,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":3,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":20,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":4,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":11,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1},\"Citations\":{\"0\":28,\"1\":29,\"2\":26,\"3\":15,\"4\":4,\"5\":10,\"6\":39,\"7\":14,\"8\":5,\"9\":7,\"10\":17,\"11\":12,\"12\":1,\"13\":11,\"14\":29,\"15\":15,\"16\":14,\"17\":11,\"18\":6,\"19\":18,\"20\":28,\"21\":24,\"22\":7,\"23\":11,\"24\":6,\"25\":6,\"26\":6,\"27\":33,\"28\":16,\"29\":6,\"30\":47,\"31\":1,\"32\":43,\"33\":25,\"34\":105,\"35\":22,\"36\":9,\"37\":24,\"38\":45,\"39\":10,\"40\":1,\"41\":20,\"42\":21,\"43\":27,\"44\":9,\"45\":85,\"46\":151,\"47\":4,\"48\":4,\"49\":49,\"50\":68,\"51\":33,\"52\":30,\"53\":64,\"54\":53,\"55\":44,\"56\":4,\"57\":52,\"58\":27,\"59\":16,\"60\":195,\"61\":16,\"62\":38,\"63\":426,\"64\":71,\"65\":336,\"66\":180,\"67\":42,\"68\":9,\"69\":1,\"70\":18,\"71\":42,\"72\":3,\"73\":19,\"74\":43,\"75\":25,\"76\":8,\"77\":23,\"78\":20,\"79\":2,\"80\":3,\"81\":17,\"82\":8,\"83\":23,\"84\":65,\"85\":51,\"86\":10,\"87\":21,\"88\":16,\"89\":16,\"90\":6,\"91\":19,\"92\":4,\"93\":26,\"94\":33,\"95\":22,\"96\":30,\"97\":5,\"98\":80,\"99\":21,\"100\":126,\"101\":74,\"102\":13,\"103\":44,\"104\":15,\"105\":6,\"106\":3,\"107\":16,\"108\":14,\"109\":5,\"110\":6,\"111\":5,\"112\":94,\"113\":45,\"114\":29,\"115\":33,\"116\":21,\"117\":16,\"118\":11,\"119\":5,\"120\":7,\"121\":2,\"122\":46,\"123\":25,\"124\":94,\"125\":9,\"126\":0,\"127\":56,\"128\":23,\"129\":22,\"130\":52,\"131\":3,\"132\":15,\"133\":13,\"134\":5,\"135\":5,\"136\":56,\"137\":16,\"138\":20,\"139\":13,\"140\":7,\"141\":29,\"142\":26,\"143\":2,\"144\":28,\"145\":14,\"146\":19,\"147\":57,\"148\":857,\"149\":19,\"150\":95,\"151\":26,\"152\":16,\"153\":32,\"154\":12,\"155\":19,\"156\":79,\"157\":1,\"158\":17,\"159\":15,\"160\":130,\"161\":7,\"162\":91,\"163\":14,\"164\":144,\"165\":0,\"166\":5,\"167\":190,\"168\":2,\"169\":166,\"170\":14,\"171\":273,\"172\":61,\"173\":124,\"174\":84,\"175\":96,\"176\":31,\"177\":38,\"178\":18,\"179\":1,\"180\":10,\"181\":16,\"182\":-1,\"183\":8,\"184\":26,\"185\":3,\"186\":27,\"187\":3,\"188\":4,\"189\":45,\"190\":4,\"191\":4,\"192\":15,\"193\":8,\"194\":92,\"195\":73,\"196\":3,\"197\":15,\"198\":14,\"199\":8,\"200\":33,\"201\":6,\"202\":0,\"203\":5,\"204\":11,\"205\":13,\"206\":18,\"207\":27,\"208\":6,\"209\":10,\"210\":32,\"211\":4,\"212\":4,\"213\":20,\"214\":13,\"215\":85,\"216\":10,\"217\":47,\"218\":21,\"219\":3,\"220\":21,\"221\":29,\"222\":8,\"223\":11,\"224\":16,\"225\":179,\"226\":1,\"227\":5,\"228\":11,\"229\":8,\"230\":20,\"231\":3,\"232\":261,\"233\":5,\"234\":-1,\"235\":14,\"236\":15,\"237\":15,\"238\":37,\"239\":7,\"240\":2,\"241\":59,\"242\":5,\"243\":5,\"244\":12,\"245\":2,\"246\":25,\"247\":8,\"248\":4,\"249\":7,\"250\":14,\"251\":37,\"252\":2,\"253\":34,\"254\":38,\"255\":34,\"256\":17,\"257\":7,\"258\":30,\"259\":1,\"260\":85,\"261\":34,\"262\":24,\"263\":1,\"264\":12,\"265\":10,\"266\":23,\"267\":17,\"268\":75,\"269\":46,\"270\":7,\"271\":84,\"272\":14,\"273\":6,\"274\":3,\"275\":16,\"276\":13,\"277\":19,\"278\":27,\"279\":22,\"280\":0,\"281\":0,\"282\":9,\"283\":9,\"284\":44,\"285\":20,\"286\":48,\"287\":9,\"288\":11,\"289\":27,\"290\":27,\"291\":13,\"292\":18,\"293\":6,\"294\":23,\"295\":57,\"296\":67,\"297\":23,\"298\":6,\"299\":128,\"300\":7,\"301\":6,\"302\":7,\"303\":17,\"304\":19,\"305\":15,\"306\":38,\"307\":2,\"308\":24,\"309\":3,\"310\":14,\"311\":19,\"312\":15,\"313\":3,\"314\":15,\"315\":10,\"316\":3,\"317\":1,\"318\":24,\"319\":1,\"320\":7,\"321\":5,\"322\":5,\"323\":22,\"324\":4,\"325\":6,\"326\":7,\"327\":13,\"328\":5,\"329\":79,\"330\":6,\"331\":3,\"332\":58,\"333\":2,\"334\":1,\"335\":8,\"336\":5,\"337\":14,\"338\":14,\"339\":38,\"340\":2,\"341\":40,\"342\":91,\"343\":30,\"344\":10,\"345\":0,\"346\":26,\"347\":10,\"348\":12,\"349\":29,\"350\":46,\"351\":6,\"352\":25,\"353\":2,\"354\":46,\"355\":0,\"356\":68,\"357\":7,\"358\":25,\"359\":21,\"360\":12,\"361\":4,\"362\":20,\"363\":11,\"364\":19,\"365\":9,\"366\":38,\"367\":26,\"368\":183,\"369\":5,\"370\":11,\"371\":26,\"372\":23,\"373\":1,\"374\":7,\"375\":22,\"376\":99,\"377\":38,\"378\":32,\"379\":12,\"380\":4,\"381\":49,\"382\":19,\"383\":30,\"384\":22,\"385\":46,\"386\":18,\"387\":169,\"388\":15,\"389\":23,\"390\":12,\"391\":-1,\"392\":20,\"393\":2,\"394\":24,\"395\":29,\"396\":5,\"397\":54,\"398\":23,\"399\":18,\"400\":20,\"401\":3,\"402\":9,\"403\":7,\"404\":21,\"405\":21,\"406\":1,\"407\":886,\"408\":18,\"409\":12,\"410\":4,\"411\":12,\"412\":17,\"413\":4,\"414\":7,\"415\":29,\"416\":6,\"417\":94,\"418\":148,\"419\":11,\"420\":61,\"421\":59,\"422\":24,\"423\":20,\"424\":37,\"425\":79,\"426\":2,\"427\":15,\"428\":122,\"429\":6,\"430\":5,\"431\":26,\"432\":4,\"433\":41,\"434\":11,\"435\":34,\"436\":18,\"437\":2,\"438\":10,\"439\":4,\"440\":5,\"441\":4,\"442\":2,\"443\":4,\"444\":19,\"445\":11,\"446\":5,\"447\":9,\"448\":1,\"449\":5,\"450\":19,\"451\":20,\"452\":112,\"453\":8,\"454\":0,\"455\":9,\"456\":7,\"457\":16,\"458\":7,\"459\":4,\"460\":5,\"461\":1,\"462\":4,\"463\":19,\"464\":10,\"465\":22,\"466\":161,\"467\":14,\"468\":1,\"469\":27,\"470\":19,\"471\":50,\"472\":12,\"473\":46,\"474\":22,\"475\":5,\"476\":18,\"477\":3,\"478\":15,\"479\":7,\"480\":37,\"481\":29,\"482\":64,\"483\":8,\"484\":2,\"485\":2,\"486\":36,\"487\":11,\"488\":25,\"489\":29,\"490\":8,\"491\":8,\"492\":24,\"493\":2,\"494\":7,\"495\":35,\"496\":27,\"497\":57,\"498\":13,\"499\":24,\"500\":10,\"501\":105,\"502\":5,\"503\":34,\"504\":7,\"505\":76,\"506\":6,\"507\":42,\"508\":3,\"509\":11,\"510\":6,\"511\":13,\"512\":51,\"513\":1,\"514\":6,\"515\":21,\"516\":33,\"517\":18,\"518\":136,\"519\":21,\"520\":37,\"521\":6,\"522\":4,\"523\":2,\"524\":23,\"525\":22,\"526\":21,\"527\":15,\"528\":27,\"529\":17,\"530\":13,\"531\":8,\"532\":10,\"533\":11,\"534\":11,\"535\":9,\"536\":19,\"537\":30,\"538\":50,\"539\":118,\"540\":2,\"541\":11,\"542\":13,\"543\":23,\"544\":13,\"545\":8,\"546\":12,\"547\":11,\"548\":6,\"549\":14,\"550\":15,\"551\":28,\"552\":9,\"553\":85,\"554\":27,\"555\":1,\"556\":21,\"557\":36,\"558\":51,\"559\":4,\"560\":11,\"561\":3,\"562\":19,\"563\":27,\"564\":0,\"565\":16,\"566\":17,\"567\":14,\"568\":6,\"569\":400,\"570\":52,\"571\":10,\"572\":30,\"573\":14,\"574\":5,\"575\":16,\"576\":15,\"577\":42,\"578\":18,\"579\":17,\"580\":8,\"581\":38,\"582\":9,\"583\":15,\"584\":13,\"585\":16,\"586\":26,\"587\":5,\"588\":2,\"589\":21,\"590\":17,\"591\":39,\"592\":21,\"593\":43,\"594\":14,\"595\":19,\"596\":12,\"597\":85,\"598\":70,\"599\":9,\"600\":9,\"601\":37,\"602\":17,\"603\":7,\"604\":42,\"605\":9,\"606\":30,\"607\":50,\"608\":14,\"609\":19,\"610\":63,\"611\":13,\"612\":28,\"613\":4,\"614\":3,\"615\":12,\"616\":4,\"617\":10,\"618\":10,\"619\":8,\"620\":9,\"621\":4,\"622\":4,\"623\":5,\"624\":8,\"625\":4,\"626\":18,\"627\":8,\"628\":3,\"629\":11,\"630\":13,\"631\":5,\"632\":14,\"633\":4,\"634\":17,\"635\":3,\"636\":0,\"637\":67,\"638\":23,\"639\":20,\"640\":3,\"641\":19,\"642\":8,\"643\":1,\"644\":41,\"645\":48,\"646\":36,\"647\":34,\"648\":10,\"649\":29,\"650\":59,\"651\":15,\"652\":13,\"653\":29,\"654\":16,\"655\":24,\"656\":90,\"657\":10,\"658\":49,\"659\":15,\"660\":22,\"661\":13,\"662\":1,\"663\":8,\"664\":3,\"665\":3,\"666\":38,\"667\":44,\"668\":21,\"669\":7,\"670\":21,\"671\":27,\"672\":28,\"673\":9,\"674\":27,\"675\":14,\"676\":4,\"677\":27,\"678\":30,\"679\":8,\"680\":16,\"681\":11,\"682\":4,\"683\":2,\"684\":16,\"685\":23,\"686\":99,\"687\":71,\"688\":9,\"689\":39,\"690\":3,\"691\":9,\"692\":16}}"