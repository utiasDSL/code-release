"{\"Conference\":{\"0\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"2\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"3\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"4\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"5\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"6\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"7\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"8\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"9\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"10\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"11\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"12\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"13\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"14\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"15\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"16\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"17\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"18\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"19\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"20\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"21\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"22\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"23\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"24\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"25\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"26\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"27\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"28\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"29\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"30\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"31\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"32\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"33\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"34\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"35\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"36\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"37\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"38\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"39\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"40\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"41\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"42\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"43\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"44\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"45\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"46\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"47\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"48\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"49\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"50\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"51\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"52\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"53\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"54\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"55\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"56\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"57\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"58\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"59\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"60\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"61\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"62\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"63\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"64\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"65\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"66\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"67\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"68\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"69\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"70\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"71\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"72\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"73\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"74\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"75\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"76\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"77\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"78\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"79\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"80\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"81\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"82\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"83\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"84\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"85\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"86\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"87\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"88\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"89\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"90\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"91\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"92\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"93\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"94\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"95\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"96\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"97\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"98\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"99\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"100\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"101\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"102\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"103\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"104\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"105\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"106\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"107\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"108\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"109\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"110\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"111\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"112\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"113\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"114\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"115\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"116\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"117\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"118\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"119\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"120\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"121\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"122\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"123\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"124\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"125\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"126\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"127\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"128\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"129\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"130\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"131\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"132\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"133\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"134\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"135\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"136\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"137\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"138\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"139\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"140\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"141\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"142\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"143\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"144\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"145\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"146\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"147\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"148\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"149\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"150\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"151\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"152\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"153\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"154\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"155\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"156\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"157\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"158\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"159\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"160\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"161\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"162\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"163\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"164\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"165\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"166\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"167\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"168\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"169\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"170\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"171\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"172\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"173\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"174\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"175\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"176\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"177\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"178\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"179\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"180\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"181\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"182\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"183\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"184\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"185\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"186\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"187\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"188\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"189\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"190\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"191\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"192\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"193\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"194\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"195\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"196\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"197\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"198\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"199\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"200\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"201\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"202\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"203\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"204\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"205\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"206\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"207\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"208\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"209\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"210\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"211\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"212\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"213\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"214\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"215\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"216\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"217\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"218\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"219\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"220\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"221\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"222\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"223\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"224\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"225\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"226\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"227\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"228\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"229\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"230\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"231\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"232\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"233\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"234\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"235\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"236\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"237\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"238\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"239\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"240\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"241\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"242\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"243\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"244\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"245\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"246\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"247\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"248\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"249\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"250\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"251\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"252\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"253\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"254\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"255\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"256\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"257\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"258\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"259\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"260\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"261\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"262\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"263\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"264\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"265\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"266\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"267\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"268\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"269\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"270\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"271\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"272\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"273\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"274\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"275\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"276\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"277\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"278\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"279\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"280\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"281\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"282\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"283\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"284\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"285\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"286\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"287\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"288\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"289\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"290\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"291\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"292\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"293\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"294\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"295\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"296\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"297\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"298\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"299\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"300\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"301\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"302\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"303\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"304\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"305\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"306\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"307\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"308\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"309\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"310\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"311\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"312\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"313\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"314\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"315\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"316\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"317\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"318\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"319\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"320\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"321\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"322\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"323\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"324\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"325\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"326\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"327\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"328\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"329\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"330\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"331\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"332\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"333\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"334\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"335\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"336\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"337\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"338\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"339\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"340\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"341\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"342\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"343\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"344\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"345\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"346\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"347\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"348\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"349\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"350\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"351\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"352\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"353\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"354\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"355\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"356\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"357\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"358\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"359\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"360\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"361\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"362\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"363\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"364\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"365\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"366\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"367\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"368\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"369\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"370\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"371\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"372\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"373\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"374\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"375\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"376\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"377\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"378\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"379\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"380\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"381\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"382\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"383\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"384\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"385\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"386\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"387\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"388\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"389\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"390\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"391\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"392\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"393\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"394\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"395\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"396\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"397\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"398\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"399\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"400\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"401\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"402\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"403\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"404\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"405\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"406\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"407\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"408\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"409\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"410\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"411\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"412\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"413\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"414\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"415\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"416\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"417\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"418\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"419\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"420\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"421\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"422\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"423\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"424\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"425\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"426\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"427\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"428\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"429\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"430\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"431\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"432\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"433\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"434\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"435\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"436\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"437\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"438\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"439\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"440\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"441\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"442\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"443\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"444\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"445\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"446\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"447\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"448\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"449\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"450\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"451\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"452\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"453\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"454\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"455\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"456\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"457\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"458\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"459\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"460\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"461\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"462\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"463\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"464\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"465\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"466\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"467\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"468\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"469\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"470\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"471\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"472\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"473\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"474\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"475\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"476\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"477\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"478\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"479\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"480\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"481\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"482\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"483\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"484\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"485\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"486\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"487\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"488\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"489\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"490\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"491\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"492\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"493\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"494\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"495\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"496\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"497\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"498\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"499\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"500\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"501\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"502\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"503\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"504\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"505\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"506\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"507\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"508\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"509\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"510\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"511\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"512\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"513\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"514\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"515\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"516\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"517\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"518\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"519\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"520\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"521\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"522\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"523\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"524\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"525\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"526\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"527\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"528\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"529\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"530\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"531\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"532\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"533\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"534\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"535\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"536\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"537\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"538\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"539\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"540\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"541\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"542\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"543\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"544\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"545\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"546\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"547\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"548\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"549\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"550\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"551\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"552\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"553\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"554\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"555\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"556\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"557\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"558\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"559\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"560\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"561\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"562\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"563\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"564\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"565\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"566\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"567\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"568\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"569\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"570\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"571\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"572\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"573\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"574\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"575\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"576\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"577\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"578\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"579\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"580\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"581\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"582\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"583\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"584\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"585\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"586\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"587\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"588\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"589\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"590\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"591\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"592\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"593\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"594\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"595\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"596\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"597\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"598\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"599\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"600\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"601\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"602\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"603\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"604\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"605\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"606\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"607\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"608\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"609\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"610\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"611\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"612\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"613\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"614\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"615\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"616\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"617\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"618\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"619\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"620\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"621\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"622\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"623\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"624\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"625\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"626\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"627\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"628\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"629\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"630\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"631\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"632\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"633\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"634\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"635\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"636\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"637\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"638\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"639\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"640\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"641\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"642\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"643\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"644\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"645\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"646\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"647\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"648\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"649\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"650\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"651\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"652\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"653\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"654\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"655\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"656\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"657\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"658\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"659\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"660\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"661\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"662\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"663\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"664\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"665\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"666\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"667\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"668\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"669\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"670\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"671\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"672\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"673\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"674\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"675\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"676\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"677\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"678\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"679\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"680\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"681\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"682\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"683\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"684\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"685\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"686\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"687\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"688\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"689\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"690\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"691\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"692\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"693\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"694\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"695\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"696\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"697\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"698\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"699\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"700\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"701\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"702\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"703\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"704\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"705\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"706\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"707\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"708\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"709\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"710\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"711\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"712\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"713\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"714\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"715\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"716\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"717\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"718\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"719\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"720\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"721\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"722\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"723\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"724\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"725\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"726\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"727\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"728\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"729\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"730\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"731\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"732\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"733\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"734\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"735\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"736\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"737\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"738\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"739\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"740\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"741\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"742\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"743\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"744\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"745\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"746\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"747\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"748\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"749\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"750\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"751\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"752\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"753\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"754\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"755\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"756\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"757\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"758\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"759\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"760\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"761\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"762\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"763\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"764\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"765\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"766\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"767\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"768\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"769\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"770\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"771\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"772\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"773\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"774\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"775\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"776\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"777\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"778\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"779\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"780\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"781\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"782\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"783\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"784\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"785\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"786\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"787\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"788\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"789\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"790\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"791\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"792\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"793\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"794\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"795\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"796\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"797\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"798\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"799\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"800\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"801\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"802\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"803\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"804\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"805\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"806\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"807\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"808\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"809\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"810\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"811\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"812\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"813\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"814\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"815\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"816\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"817\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"818\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"819\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"820\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"821\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"822\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"823\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"824\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"825\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"826\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"827\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"828\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"829\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"830\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"831\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"832\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"833\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"834\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"835\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"836\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"837\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"838\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"839\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"840\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"841\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"842\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"843\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"844\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"845\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"846\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"847\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"848\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"849\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"850\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"851\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"852\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"853\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"854\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"855\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"856\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"857\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"858\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"859\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"860\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"861\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"862\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"863\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"864\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"865\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"866\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"867\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"868\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"869\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"870\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"871\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"872\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"873\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"874\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"875\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"876\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"877\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"878\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"879\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"880\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"881\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"882\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"883\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"884\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"885\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"886\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"887\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"888\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"889\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"890\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"891\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"892\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"893\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"894\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"895\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"896\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"897\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"898\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"899\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"900\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"901\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"902\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"903\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"904\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"905\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"906\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"907\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"908\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"909\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"910\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"911\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"912\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"913\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"914\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"915\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"916\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"917\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"918\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"919\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"920\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"921\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"922\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"923\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"924\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"925\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"926\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"927\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"928\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"929\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"930\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"931\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"932\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"933\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"934\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"935\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"936\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"937\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"938\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"939\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"940\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"941\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"942\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"943\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"944\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"945\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"946\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"947\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"948\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"949\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"950\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"951\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"952\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"953\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"954\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"955\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"956\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"957\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"958\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"959\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"960\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"961\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"962\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"963\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"964\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"965\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"966\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"967\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"968\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"969\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"970\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"971\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"972\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"973\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"974\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"975\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"976\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"977\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"978\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"979\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"980\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"981\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"982\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"983\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"984\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"985\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"986\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"987\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"988\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"989\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"990\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"991\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"992\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"993\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"994\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"995\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"996\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"997\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"998\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"999\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1000\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1001\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1002\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1003\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1004\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1005\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1006\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1007\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1008\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1009\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1010\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1011\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1012\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1013\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1014\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1015\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1016\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1017\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1018\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1019\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1020\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1021\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1022\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1023\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1024\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1025\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1026\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1027\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1028\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1029\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1030\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1031\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1032\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1033\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1034\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1035\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1036\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1037\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1038\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1039\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1040\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1041\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1042\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1043\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1044\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1045\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1046\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1047\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1048\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1049\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1050\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1051\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1052\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1053\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1054\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1055\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1056\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1057\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1058\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1059\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1060\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1061\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1062\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1063\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1064\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1065\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1066\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1067\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1068\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1069\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1070\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1071\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1072\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1073\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1074\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1075\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1076\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1077\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1078\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1079\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1080\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1081\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1082\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1083\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1084\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1085\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1086\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1087\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1088\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1089\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1090\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1091\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1092\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1093\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1094\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1095\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1096\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1097\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1098\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1099\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1100\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1101\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1102\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1103\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1104\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1105\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1106\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1107\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1108\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1109\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1110\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1111\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1112\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1113\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1114\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1115\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1116\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1117\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1118\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1119\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1120\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1121\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1122\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1123\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1124\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1125\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1126\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1127\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1128\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1129\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1130\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1131\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1132\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1133\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1134\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1135\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1136\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1137\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1138\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1139\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1140\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1141\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1142\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1143\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1144\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1145\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1146\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1147\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1148\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1149\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1150\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1151\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1152\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1153\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1154\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1155\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1156\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1157\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1158\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1159\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1160\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1161\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1162\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1163\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1164\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1165\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1166\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1167\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1168\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1169\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1170\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1171\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1172\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1173\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1174\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1175\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1176\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1177\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1178\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1179\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1180\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1181\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1182\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1183\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1184\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1185\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1186\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1187\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1188\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1189\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1190\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1191\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1192\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1193\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1194\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1195\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1196\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1197\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1198\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1199\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1200\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1201\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1202\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1203\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1204\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1205\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1206\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1207\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1208\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1209\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1210\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1211\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1212\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1213\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1214\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1215\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1216\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1217\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1218\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1219\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1220\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1221\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1222\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1223\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1224\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1225\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1226\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1227\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1228\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1229\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1230\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1231\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1232\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1233\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1234\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1235\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1236\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1237\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1238\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1239\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1240\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1241\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1242\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1243\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1244\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1245\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1246\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1247\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1248\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1249\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1250\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1251\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1252\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1253\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1254\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1255\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1256\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1257\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1258\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1259\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1260\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1261\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1262\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1263\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1264\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1265\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1266\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1267\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1268\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1269\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1270\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1271\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1272\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1273\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1274\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1275\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1276\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1277\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1278\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1279\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1280\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1281\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1282\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1283\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1284\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1285\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1286\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1287\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1288\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1289\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1290\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1291\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1292\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1293\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1294\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1295\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1296\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1297\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1298\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1299\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1300\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1301\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1302\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1303\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1304\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1305\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1306\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1307\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1308\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1309\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1310\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1311\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1312\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1313\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1314\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1315\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1316\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1317\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1318\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1319\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1320\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1321\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1322\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1323\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1324\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1325\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1326\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1327\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1328\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1329\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1330\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1331\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1332\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1333\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1334\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1335\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1336\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1337\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1338\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1339\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1340\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1341\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1342\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1343\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1344\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1345\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1346\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1347\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1348\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1349\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1350\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1351\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1352\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1353\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1354\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1355\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1356\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1357\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1358\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1359\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1360\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1361\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1362\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1363\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1364\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1365\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1366\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1367\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1368\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1369\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1370\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1371\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1372\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1373\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1374\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1375\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1376\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1377\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1378\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1379\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1380\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1381\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1382\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1383\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1384\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\",\"1385\":\"2021 IEEE International Conference on Robotics and Automation (ICRA)\"},\"Year\":{\"0\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"2\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"3\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"4\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"5\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"6\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"7\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"8\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"9\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"10\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"11\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"12\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"13\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"14\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"15\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"16\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"17\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"18\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"19\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"20\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"21\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"22\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"23\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"24\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"25\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"26\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"27\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"28\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"29\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"30\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"31\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"32\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"33\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"34\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"35\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"36\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"37\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"38\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"39\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"40\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"41\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"42\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"43\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"44\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"45\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"46\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"47\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"48\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"49\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"50\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"51\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"52\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"53\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"54\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"55\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"56\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"57\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"58\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"59\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"60\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"61\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"62\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"63\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"64\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"65\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"66\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"67\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"68\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"69\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"70\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"71\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"72\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"73\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"74\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"75\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"76\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"77\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"78\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"79\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"80\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"81\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"82\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"83\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"84\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"85\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"86\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"87\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"88\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"89\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"90\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"91\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"92\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"93\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"94\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"95\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"96\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"97\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"98\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"99\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"100\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"101\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"102\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"103\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"104\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"105\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"106\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"107\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"108\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"109\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"110\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"111\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"112\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"113\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"114\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"115\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"116\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"117\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"118\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"119\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"120\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"121\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"122\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"123\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"124\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"125\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"126\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"127\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"128\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"129\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"130\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"131\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"132\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"133\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"134\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"135\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"136\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"137\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"138\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"139\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"140\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"141\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"142\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"143\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"144\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"145\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"146\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"147\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"148\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"149\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"150\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"151\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"152\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"153\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"154\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"155\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"156\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"157\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"158\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"159\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"160\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"161\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"162\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"163\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"164\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"165\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"166\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"167\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"168\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"169\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"170\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"171\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"172\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"173\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"174\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"175\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"176\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"177\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"178\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"179\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"180\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"181\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"182\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"183\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"184\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"185\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"186\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"187\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"188\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"189\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"190\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"191\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"192\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"193\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"194\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"195\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"196\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"197\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"198\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"199\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"200\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"201\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"202\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"203\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"204\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"205\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"206\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"207\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"208\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"209\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"210\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"211\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"212\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"213\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"214\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"215\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"216\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"217\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"218\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"219\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"220\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"221\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"222\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"223\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"224\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"225\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"226\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"227\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"228\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"229\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"230\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"231\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"232\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"233\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"234\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"235\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"236\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"237\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"238\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"239\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"240\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"241\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"242\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"243\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"244\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"245\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"246\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"247\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"248\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"249\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"250\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"251\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"252\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"253\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"254\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"255\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"256\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"257\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"258\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"259\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"260\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"261\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"262\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"263\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"264\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"265\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"266\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"267\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"268\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"269\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"270\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"271\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"272\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"273\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"274\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"275\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"276\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"277\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"278\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"279\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"280\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"281\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"282\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"283\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"284\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"285\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"286\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"287\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"288\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"289\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"290\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"291\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"292\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"293\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"294\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"295\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"296\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"297\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"298\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"299\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"300\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"301\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"302\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"303\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"304\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"305\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"306\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"307\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"308\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"309\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"310\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"311\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"312\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"313\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"314\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"315\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"316\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"317\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"318\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"319\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"320\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"321\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"322\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"323\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"324\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"325\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"326\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"327\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"328\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"329\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"330\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"331\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"332\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"333\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"334\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"335\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"336\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"337\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"338\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"339\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"340\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"341\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"342\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"343\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"344\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"345\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"346\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"347\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"348\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"349\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"350\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"351\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"352\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"353\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"354\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"355\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"356\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"357\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"358\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"359\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"360\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"361\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"362\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"363\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"364\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"365\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"366\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"367\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"368\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"369\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"370\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"371\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"372\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"373\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"374\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"375\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"376\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"377\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"378\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"379\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"380\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"381\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"382\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"383\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"384\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"385\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"386\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"387\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"388\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"389\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"390\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"391\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"392\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"393\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"394\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"395\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"396\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"397\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"398\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"399\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"400\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"401\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"402\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"403\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"404\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"405\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"406\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"407\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"408\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"409\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"410\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"411\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"412\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"413\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"414\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"415\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"416\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"417\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"418\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"419\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"420\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"421\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"422\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"423\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"424\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"425\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"426\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"427\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"428\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"429\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"430\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"431\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"432\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"433\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"434\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"435\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"436\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"437\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"438\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"439\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"440\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"441\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"442\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"443\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"444\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"445\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"446\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"447\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"448\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"449\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"450\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"451\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"452\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"453\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"454\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"455\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"456\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"457\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"458\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"459\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"460\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"461\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"462\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"463\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"464\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"465\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"466\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"467\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"468\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"469\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"470\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"471\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"472\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"473\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"474\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"475\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"476\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"477\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"478\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"479\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"480\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"481\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"482\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"483\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"484\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"485\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"486\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"487\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"488\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"489\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"490\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"491\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"492\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"493\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"494\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"495\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"496\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"497\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"498\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"499\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"500\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"501\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"502\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"503\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"504\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"505\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"506\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"507\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"508\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"509\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"510\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"511\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"512\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"513\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"514\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"515\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"516\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"517\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"518\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"519\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"520\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"521\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"522\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"523\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"524\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"525\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"526\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"527\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"528\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"529\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"530\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"531\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"532\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"533\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"534\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"535\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"536\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"537\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"538\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"539\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"540\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"541\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"542\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"543\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"544\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"545\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"546\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"547\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"548\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"549\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"550\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"551\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"552\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"553\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"554\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"555\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"556\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"557\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"558\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"559\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"560\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"561\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"562\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"563\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"564\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"565\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"566\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"567\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"568\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"569\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"570\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"571\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"572\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"573\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"574\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"575\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"576\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"577\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"578\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"579\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"580\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"581\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"582\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"583\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"584\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"585\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"586\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"587\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"588\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"589\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"590\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"591\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"592\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"593\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"594\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"595\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"596\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"597\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"598\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"599\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"600\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"601\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"602\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"603\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"604\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"605\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"606\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"607\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"608\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"609\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"610\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"611\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"612\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"613\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"614\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"615\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"616\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"617\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"618\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"619\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"620\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"621\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"622\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"623\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"624\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"625\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"626\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"627\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"628\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"629\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"630\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"631\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"632\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"633\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"634\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"635\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"636\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"637\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"638\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"639\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"640\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"641\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"642\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"643\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"644\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"645\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"646\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"647\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"648\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"649\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"650\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"651\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"652\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"653\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"654\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"655\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"656\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"657\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"658\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"659\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"660\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"661\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"662\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"663\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"664\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"665\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"666\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"667\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"668\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"669\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"670\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"671\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"672\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"673\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"674\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"675\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"676\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"677\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"678\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"679\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"680\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"681\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"682\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"683\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"684\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"685\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"686\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"687\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"688\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"689\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"690\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"691\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"692\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"693\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"694\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"695\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"696\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"697\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"698\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"699\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"700\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"701\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"702\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"703\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"704\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"705\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"706\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"707\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"708\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"709\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"710\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"711\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"712\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"713\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"714\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"715\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"716\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"717\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"718\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"719\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"720\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"721\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"722\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"723\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"724\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"725\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"726\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"727\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"728\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"729\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"730\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"731\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"732\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"733\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"734\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"735\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"736\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"737\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"738\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"739\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"740\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"741\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"742\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"743\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"744\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"745\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"746\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"747\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"748\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"749\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"750\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"751\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"752\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"753\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"754\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"755\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"756\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"757\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"758\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"759\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"760\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"761\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"762\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"763\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"764\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"765\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"766\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"767\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"768\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"769\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"770\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"771\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"772\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"773\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"774\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"775\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"776\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"777\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"778\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"779\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"780\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"781\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"782\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"783\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"784\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"785\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"786\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"787\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"788\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"789\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"790\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"791\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"792\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"793\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"794\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"795\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"796\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"797\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"798\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"799\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"800\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"801\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"802\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"803\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"804\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"805\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"806\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"807\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"808\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"809\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"810\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"811\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"812\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"813\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"814\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"815\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"816\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"817\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"818\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"819\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"820\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"821\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"822\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"823\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"824\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"825\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"826\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"827\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"828\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"829\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"830\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"831\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"832\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"833\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"834\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"835\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"836\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"837\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"838\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"839\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"840\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"841\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"842\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"843\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"844\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"845\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"846\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"847\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"848\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"849\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"850\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"851\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"852\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"853\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"854\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"855\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"856\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"857\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"858\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"859\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"860\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"861\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"862\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"863\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"864\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"865\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"866\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"867\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"868\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"869\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"870\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"871\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"872\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"873\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"874\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"875\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"876\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"877\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"878\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"879\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"880\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"881\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"882\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"883\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"884\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"885\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"886\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"887\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"888\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"889\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"890\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"891\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"892\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"893\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"894\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"895\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"896\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"897\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"898\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"899\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"900\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"901\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"902\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"903\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"904\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"905\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"906\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"907\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"908\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"909\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"910\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"911\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"912\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"913\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"914\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"915\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"916\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"917\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"918\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"919\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"920\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"921\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"922\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"923\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"924\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"925\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"926\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"927\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"928\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"929\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"930\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"931\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"932\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"933\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"934\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"935\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"936\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"937\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"938\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"939\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"940\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"941\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"942\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"943\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"944\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"945\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"946\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"947\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"948\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"949\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"950\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"951\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"952\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"953\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"954\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"955\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"956\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"957\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"958\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"959\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"960\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"961\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"962\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"963\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"964\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"965\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"966\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"967\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"968\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"969\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"970\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"971\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"972\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"973\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"974\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"975\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"976\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"977\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"978\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"979\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"980\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"981\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"982\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"983\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"984\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"985\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"986\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"987\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"988\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"989\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"990\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"991\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"992\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"993\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"994\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"995\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"996\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"997\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"998\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"999\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1000\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1001\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1002\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1003\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1004\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1005\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1006\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1007\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1008\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1009\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1010\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1011\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1012\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1013\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1014\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1015\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1016\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1017\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1018\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1019\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1020\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1021\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1022\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1023\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1024\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1025\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1026\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1027\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1028\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1029\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1030\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1031\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1032\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1033\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1034\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1035\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1036\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1037\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1038\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1039\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1040\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1041\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1042\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1043\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1044\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1045\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1046\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1047\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1048\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1049\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1050\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1051\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1052\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1053\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1054\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1055\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1056\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1057\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1058\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1059\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1060\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1061\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1062\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1063\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1064\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1065\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1066\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1067\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1068\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1069\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1070\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1071\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1072\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1073\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1074\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1075\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1076\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1077\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1078\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1079\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1080\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1081\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1082\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1083\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1084\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1085\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1086\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1087\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1088\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1089\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1090\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1091\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1092\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1093\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1094\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1095\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1096\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1097\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1098\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1099\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1100\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1101\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1102\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1103\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1104\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1105\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1106\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1107\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1108\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1109\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1110\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1111\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1112\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1113\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1114\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1115\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1116\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1117\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1118\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1119\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1120\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1121\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1122\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1123\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1124\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1125\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1126\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1127\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1128\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1129\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1130\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1131\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1132\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1133\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1134\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1135\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1136\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1137\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1138\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1139\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1140\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1141\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1142\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1143\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1144\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1145\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1146\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1147\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1148\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1149\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1150\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1151\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1152\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1153\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1154\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1155\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1156\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1157\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1158\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1159\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1160\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1161\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1162\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1163\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1164\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1165\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1166\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1167\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1168\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1169\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1170\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1171\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1172\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1173\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1174\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1175\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1176\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1177\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1178\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1179\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1180\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1181\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1182\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1183\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1184\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1185\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1186\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1187\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1188\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1189\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1190\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1191\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1192\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1193\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1194\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1195\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1196\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1197\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1198\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1199\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1200\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1201\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1202\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1203\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1204\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1205\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1206\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1207\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1208\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1209\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1210\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1211\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1212\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1213\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1214\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1215\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1216\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1217\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1218\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1219\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1220\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1221\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1222\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1223\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1224\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1225\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1226\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1227\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1228\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1229\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1230\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1231\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1232\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1233\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1234\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1235\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1236\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1237\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1238\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1239\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1240\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1241\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1242\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1243\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1244\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1245\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1246\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1247\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1248\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1249\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1250\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1251\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1252\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1253\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1254\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1255\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1256\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1257\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1258\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1259\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1260\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1261\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1262\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1263\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1264\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1265\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1266\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1267\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1268\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1269\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1270\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1271\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1272\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1273\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1274\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1275\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1276\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1277\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1278\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1279\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1280\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1281\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1282\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1283\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1284\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1285\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1286\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1287\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1288\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1289\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1290\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1291\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1292\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1293\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1294\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1295\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1296\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1297\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1298\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1299\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1300\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1301\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1302\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1303\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1304\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1305\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1306\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1307\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1308\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1309\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1310\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1311\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1312\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1313\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1314\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1315\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1316\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1317\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1318\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1319\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1320\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1321\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1322\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1323\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1324\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1325\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1326\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1327\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1328\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1329\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1330\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1331\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1332\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1333\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1334\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1335\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1336\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1337\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1338\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1339\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1340\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1341\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1342\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1343\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1344\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1345\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1346\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1347\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1348\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1349\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1350\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1351\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1352\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1353\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1354\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1355\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1356\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1357\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1358\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1359\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1360\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1361\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1362\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1363\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1364\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1365\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1366\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1367\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1368\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1369\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1370\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1371\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1372\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1373\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1374\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1375\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1376\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1377\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1378\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1379\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1380\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1381\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1382\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1383\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1384\":\"Date of Conference: 30 May 2021 - 05 June 2021\",\"1385\":\"Date of Conference: 30 May 2021 - 05 June 2021\"},\"Paper Title\":{\"0\":\"Uncertainty-aware Non-linear Model Predictive Control for Human-following Companion Robot\",\"1\":\"Path Planning in Uncertain Ocean Currents using Ensemble Forecasts\",\"2\":\"Distributed Motion Coordination Using Convex Feasible Set Based Model Predictive Control\",\"3\":\"Risk-Conditioned Distributional Soft Actor-Critic for Risk-Sensitive Navigation\",\"4\":\"Optimized Method for Planning and Controlling the Somersault Motion of Quadruped Robot\",\"5\":\"Motion Coupling Analysis for the Decoupled Design of a Two-segment Notched Continuum Robot\",\"6\":\"VINS-Motion: Tightly-coupled Fusion of VINS and Motion Constraint\",\"7\":\"Robot-to-image Registration with Geometric Marker for CT-guided Robotic Needle Insertion\",\"8\":\"Shape Sensor Using Magnetic Induction with Frequency Sweeping for Medical Catheters\",\"9\":\"Temperature Compensated 3D Printed Strain Sensor for Advanced Manufacturing Applications\",\"10\":\"Design of a deployable underwater robot for the recovery of autonomous underwater vehicles based on origami technique\",\"11\":\"Modelling and optimisation of a mechanism-based metamaterial for a wrist flexion-extension assistive device\",\"12\":\"Mechatronic Design of A Low-Noise Active Knee Prosthesis with High Backdrivability\",\"13\":\"Introspective Visuomotor Control: Exploiting Uncertainty in Deep Visuomotor Control for Failure Recovery\",\"14\":\"Sim-to-Real Visual Grasping via State Representation Learning Based on Combining Pixel-Level and Feature-Level Domain Adaptation\",\"15\":\"Dexterous Manoeuvre through Touch in a Cluttered Scene\",\"16\":\"Mapless-Planner: A Robust and Fast Planning Framework for Aggressive Autonomous Flight without Map Fusion\",\"17\":\"Robotic Indoor Scene Captioning from Streaming Video\",\"18\":\"Geometry-Aware Unsupervised Domain Adaptation for Stereo Matching\",\"19\":\"Reasoning Operational Decisions for Robots via Time Series Causal Inference\",\"20\":\"HueCode: A Meta-marker Exposing Relative Pose and Additional Information in Different Colored Layers\",\"21\":\"B-splines for Purely Vision-based Localization and Mapping on Non-holonomic Ground Vehicles\",\"22\":\"Robust SRIF-based LiDAR-IMU Localization for Autonomous Vehicles\",\"23\":\"Structure Reconstruction Using Ray-Point-Ray Features: Representation and Camera Pose Estimation\",\"24\":\"Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture\",\"25\":\"Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation\",\"26\":\"Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction\",\"27\":\"A Variational Infinite Mixture for Probabilistic Inverse Dynamics Learning\",\"28\":\"Model-based Domain Randomization of Dynamics System with Deep Bayesian Locally Linear Embedding\",\"29\":\"Deep Imitation Learning for Autonomous Navigation in Dynamic Pedestrian Environments\",\"30\":\"Learning from Demonstration without Demonstrations\",\"31\":\"Reachability-based Push Recovery for Humanoid Robots with Variable-Height Inverted Pendulum\",\"32\":\"Meaningful Centroidal Frame Orientation of Multi-body Floating Locomotion Systems\",\"33\":\"Soft-Jig-Driven Assembly Operations\",\"34\":\"Prediction-Error Negativity to Assess Singularity Avoidance Strategies in Physical Human-Robot Collaboration\",\"35\":\"A Large Area Robotic Skin with Sparsely Embedded Microphones for Human-Robot Tactile Communication\",\"36\":\"Star Topology based Interaction for Robust Trajectory Forecasting in Dynamic Scene\",\"37\":\"A Peg-in-hole Task Strategy for Holes in Concrete\",\"38\":\"TaskNet: A Neural Task Planner for Autonomous Excavator\",\"39\":\"Steering Induced Roll Quantification During Ship Turning Circle Manoeuvre\",\"40\":\"Uncertainty-aware Self-supervised Target-mass Grasping of Granular Foods\",\"41\":\"SCT-CNN: A Spatio-Channel-Temporal Attention CNN for Grasp Stability Prediction\",\"42\":\"Spherical Magnetic Joint for Inverted Locomotion of Multi-Legged Robot\",\"43\":\"An Open-Source Mechanical Design of ALARIS Hand: A 6-DOF Anthropomorphic Robotic Hand\",\"44\":\"Biomimetic Operational Space Control for Musculoskeletal Humanoid Optimizing Across Muscle Activation and Joint Nullspace\",\"45\":\"Parallel Actuation of Nanorod Swarm and Nanoparticle Swarm to Different Targets\",\"46\":\"Robotic Micromanipulation for Active Pin Alignment in Electronic Soldering Industry\",\"47\":\"Observation Space Matters: Benchmark and Optimization Algorithm\",\"48\":\"Interleaving Fast and Slow Decision Making\",\"49\":\"Multi-output Infinite Horizon Gaussian Processes\",\"50\":\"Estimation and Adaption of Indoor Ego Airflow Disturbance with Application to Quadrotor Trajectory Planning\",\"51\":\"Real-time active detection of targets and path planning using UAVs\",\"52\":\"EVA-Planner: Environmental Adaptive Quadrotor Planning\",\"53\":\"Differential Information Aided 3-D Registration for Accurate Navigation and Scene Reconstruction\",\"54\":\"Autonomous Navigation in Dynamic Environments with Multi-Modal Perception Uncertainties\",\"55\":\"Learning World Transition Model for Socially Aware Robot Navigation\",\"56\":\"Probabilistic Dynamic Crowd Prediction for Social Navigation\",\"57\":\"Consensus-Based Control Barrier Function for Swarm\",\"58\":\"Bayesian Disturbance Injection: Robust Imitation Learning of Flexible Policies\",\"59\":\"Active Modular Environment for Robot Navigation\",\"60\":\"Deep reinforcement learning of event-triggered communication and control for multi-agent cooperative transport\",\"61\":\"Multi-Robot Task Allocation Games in Dynamically Changing Environments\",\"62\":\"An Upper Confidence Bound for Simultaneous Exploration and Exploitation in Heterogeneous Multi-Robot Systems\",\"63\":\"Priority Patrolling using Multiple Agents\",\"64\":\"Anticipatory Navigation in Crowds by Probabilistic Prediction of Pedestrian Future Movements\",\"65\":\"Real-Time Human Lower Limbs Motion Estimation and Feedback for Potential Applications in Robotic Gait Aid and Training\",\"66\":\"Cost-to-Go Function Generating Networks for High Dimensional Motion Planning\",\"67\":\"Smooth-RRT: Asymptotically Optimal Motion Planning for Mobile Robots under Kinodynamic Constraints\",\"68\":\"Continuous Optimization-Based Task and Motion Planning with Signal Temporal Logic Specifications for Sequential Manipulation\",\"69\":\"Proximal Policy Optimization with Relative Pearson Divergence\",\"70\":\"Optimal Object Placement for Minimum Discontinuity Non-revisiting Coverage Task\",\"71\":\"Search-Based Online Trajectory Planning for Car-like Robots in Highly Dynamic Environments\",\"72\":\"Task-Space Decomposed Motion Planning Framework for Multi-Robot Loco-Manipulation\",\"73\":\"SMT-Based Optimal Deployment of Mobile Rechargers\",\"74\":\"Fast Replanning Multi-Heuristic A\",\"75\":\"Generating Large-Scale Trajectories Efficiently using Double Descriptions of Polynomials\",\"76\":\"Restoring Force Design of Active Self-healing Tension Transmission System and Application to Tendon-driven Legged Robot\",\"77\":\"A Translational Parallel Continuum Robot Reinforced by Origami and Cross-Routing Tendons\",\"78\":\"Design of a 3-DOF Coupled Tendon-Driven Waist Joint\",\"79\":\"Design and Modeling of a Variable-Stiffness Spring Mechanism for Impedance Modulation in Physical Human\\u2013Robot Interaction\",\"80\":\"Mecanum Crank: A Novel Omni-Directional Vehicle Using Crank Leg\",\"81\":\"Position and Orientation Control of Polygonal Objects by Sensorless In-hand Caging Manipulation\",\"82\":\"3D biped locomotion control including seamless transition between walking and running via 3D ZMP manipulation\",\"83\":\"GCC-PHAT with Speech-oriented Attention for Robotic Sound Source Localization\",\"84\":\"Towards Robust GNSS Positioning and Real-time Kinematic Using Factor Graph Optimization\",\"85\":\"Camera Relocalization using Deep Point Cloud Generation and Hand-crafted Feature Refinement\",\"86\":\"Accelerating Probabilistic Volumetric Mapping using Ray-Tracing Graphics Hardware\",\"87\":\"UVIP: Robust UWB aided Visual-Inertial Positioning System for Complex Indoor Environments\",\"88\":\"LiDAR-Based Initial Global Localization Using Two-Dimensional (2D) Submap Projection Image (SPI)\",\"89\":\"Automatic Hyper-Parameter Tuning for Black-box LiDAR Odometry\",\"90\":\"Locus: LiDAR-based Place Recognition using Spatiotemporal Higher-Order Pooling\",\"91\":\"Automated Extrinsic Calibration for 3D LiDARs with Range Offset Correction using an Arbitrary Planar Board\",\"92\":\"Machine Learning-based Human-Following System: Following the Predicted Position of a Walking Human\",\"93\":\"Anytime Game-Theoretic Planning with Active Reasoning About Humans\\u2019 Latent States for Human-Centered Robots\",\"94\":\"Momentum Observer-Based Collision Detection Using LSTM for Model Uncertainty Learning\",\"95\":\"Deep Learning and Mixed Reality to Autocomplete Teleoperation\",\"96\":\"Learning Spatial Context with Graph Neural Network for Multi-Person Pose Grouping\",\"97\":\"Automatic Hanging Point Learning from Random Shape Generation and Physical Function Validation\",\"98\":\"Graph Convolutional Network based Configuration Detection for Freeform Modular Robot Using Magnetic Sensor Array\",\"99\":\"An analytical diabolo model for robotic learning and control\",\"100\":\"Peer-Assisted Robotic Learning: A Data-Driven Collaborative Learning Approach for Cloud Robotic Systems\",\"101\":\"Lywal: a Leg-Wheel Transformable Quadruped Robot with Picking up and Transport Functions\",\"102\":\"Stair Climbing Capability-Based Dimensional Synthesis for the Multi-legged Robot\",\"103\":\"Versatile Locomotion by Integrating Ankle, Hip, Stepping, and Height Variation Strategies\",\"104\":\"A Self-Training Approach-Based Traversability Analysis for Mobile Robots in Urban Environments\",\"105\":\"Proactive Interaction Framework for Intelligent Social Receptionist Robots\",\"106\":\"Estimation of Spatially-Correlated Ocean Currents from Ensemble Forecasts and Online Measurements\",\"107\":\"Circus ANYmal: A Quadruped Learning Dexterous Manipulation with Its Limbs\",\"108\":\"Long-Range Hand Gesture Recognition via Attention-based SSD Network\",\"109\":\"Spectral Temporal Graph Neural Network for Trajectory Prediction\",\"110\":\"Dark Reciprocal-Rank: Teacher-to-student Knowledge Transfer from Self-localization Model to Graph-convolutional Neural Network\",\"111\":\"Efficient SE(3) Reachability Map Generation via Interplanar Integration of Intra-planar Convolutions\",\"112\":\"Orientation Control of an Electromagnetically Actuated Soft-Tethered Colonoscope Based on 2OR Pseudo-Rigid-Body Model\",\"113\":\"An Integrated High-dexterity Cooperative Robotic Assistant for Intraocular Micromanipulation\",\"114\":\"Design of Soft Sensor for Feedback Control of Bio-actuator Powered by Skeletal Muscle\",\"115\":\"A portable acoustofluidic device for multifunctional cell manipulation and reconstruction\",\"116\":\"Design and soft-landing control of a six-legged mobile repetitive lander for lunar exploration\",\"117\":\"LEAF: Latent Exploration Along the Frontier\",\"118\":\"LAFFNet: A Lightweight Adaptive Feature Fusion Network for Underwater Image Enhancement\",\"119\":\"Ultrasound Doppler Imaging and Navigation of Collective Magnetic Cell Microrobots in Blood\",\"120\":\"Fixed-root Aerial Manipulator: Design, Modeling, and Control of Multilink Aerial Arm to Adhere Foot Module to Ceilings using Rotor Thrust\",\"121\":\"Autonomous Decentralized Shape-Based Navigation for Snake Robots in Dense Environments\",\"122\":\"Real-time Optimal Navigation Planning Using Learned Motion Costs\",\"123\":\"Autonomous Navigation for Adaptive Unmanned Underwater Vehicles Using Fiducial Markers\",\"124\":\"Command Filtered Tracking Control for High-order Systems with Limited Transmission Bandwidth\",\"125\":\"Multi-Scale Cost Volumes Cascade Network for Stereo Matching\",\"126\":\"Hierarchical MCTS for Scalable Multi-Vessel Multi-Float Systems\",\"127\":\"Distributed Heuristic Multi-Agent Path Finding with Communication\",\"128\":\"A Geometric Folding Pattern for Robot Coverage Path Planning\",\"129\":\"Tree Search-based Task and Motion Planning with Prehensile and Non-prehensile Manipulation for Obstacle Rearrangement in Clutter\",\"130\":\"Active Information Acquisition under Arbitrary Unknown Disturbances\",\"131\":\"Real-time Obstacle Avoidance with a Virtual Torque Approach for a Robotic Tool in the End Effector\",\"132\":\"Approximating Constraint Manifolds Using Generative Models for Sampling-Based Constrained Motion Planning\",\"133\":\"Maintaining a Reliable World Model using Action-aware Perceptual Anchoring\",\"134\":\"Dynamic Window Approach with Human Imitating Collision Avoidance\",\"135\":\"Dynamic Movement Primitive based Motion Retargeting for Dual-Arm Sign Language Motions\",\"136\":\"SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure\",\"137\":\"An On-Line POMDP Solver for Continuous Observation Spaces\",\"138\":\"Modeling and Simulation of Running Expansion with Trunk and Pelvic Rotation Assist Suit\",\"139\":\"Pneumatic actuation-based bidirectional modules with variable stiffness and closed-loop position control\",\"140\":\"A Capturability-based Control Framework for the Underactuated Bipedal Walking\",\"141\":\"Appearance-based Loop Closure Detection via Bidirectional Manifold Representation Consensus\",\"142\":\"Synergetic Effect between Limbs and Spine Dynamics in Quadruped Walking Robots\",\"143\":\"A Locally-Adaptive, Parallel-Jaw Gripper with Clamping and Rolling Capable, Soft Fingertips for Fine Manipulation of Flexible Flat Cables\",\"144\":\"Stable, Sensor-less and Compliance-less Module Connection for Automated Construction System of a Modularized Rail Structure\",\"145\":\"Numerical Simulations of A Novel Force Controller Serially Combining The Admittance and Impedance Controllers\",\"146\":\"Kinematic Stability based AFG-RRT Path Planning for Cable-Driven Parallel Robots\",\"147\":\"Simultaneous Precision Assembly of Multiple Objects through Coordinated Micro-robot Manipulation\",\"148\":\"Dynamic Compensation in Throwing Motion with High-Speed Robot Hand-Arm\",\"149\":\"Signal Temporal Logic Synthesis as Probabilistic Inference\",\"150\":\"Bias Compensated UWB Anchor Initialization using Information-Theoretic Supported Triangulation Points\",\"151\":\"Multiresolution Representations for Large-Scale Terrain with Local Gaussian Process Regression\",\"152\":\"MSTSL: Multi-Sensor Based Two-Step Localization in Geometrically Symmetric Environments\",\"153\":\"Interactive Planning for Autonomous Urban Driving in Adversarial Scenarios\",\"154\":\"Kernel-Based 3-D Dynamic Occupancy Mapping with Particle Tracking\",\"155\":\"AdaGrasp: Learning an Adaptive Gripper-Aware Grasping Policy\",\"156\":\"TRANS-AM: Transfer Learning by Aggregating Dynamics Models for Soft Robotic Assembly\",\"157\":\"Embedding Symbolic Temporal Knowledge into Deep Sequential Models\",\"158\":\"Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning\",\"159\":\"Linguistic Descriptions of Human Motion with Generative Adversarial Seq2Seq Learning\",\"160\":\"Evolvable Motion-planning Method using Deep Reinforcement Learning\",\"161\":\"Learning Sequences of Manipulation Primitives for Robotic Assembly\",\"162\":\"EGO-Swarm: A Fully Autonomous and Decentralized Quadrotor Swarm System in Cluttered Environments\",\"163\":\"Robust Landing Stabilization of Humanoid Robot on Uneven Terrain via Admittance Control and Heel Strike Motion\",\"164\":\"State Estimation for Hybrid Wheeled-Legged Robots Performing Mobile Manipulation Tasks\",\"165\":\"Adversarial Skill Learning for Robust Manipulation\",\"166\":\"Learning Visual Affordances with Target-Orientated Deep Q-Network to Grasp Objects by Harnessing Environmental Fixtures\",\"167\":\"Enhancing Robot Perception in Grasping and Dexterous Manipulation through Crowdsourcing and Gamification\",\"168\":\"Teaching Robotic and Biomechatronic Concepts with a Gripper Design Project and a Grasping and Manipulation Competition\",\"169\":\"Model based evaluation of human and lower-limb exoskeleton interaction during sit to stand motion\",\"170\":\"Efficient solution method based on inverse dynamics for optimal control problems of rigid body systems\",\"171\":\"Reduction of Ground Impact of a Powered Exoskeleton by Shock Absorption Mechanism on the Shank\",\"172\":\"FlowDriveNet: An End-to-End Network for Learning Driving Policies from Image Optical Flow and LiDAR Point Flow\",\"173\":\"PocoNet: SLAM-oriented 3D LiDAR Point Cloud Online Compression Network\",\"174\":\"3D Reconstruction of Deformable Colon Structures based on Preoperative Model and Deep Neural Network\",\"175\":\"An Encoder-Free Joint Velocity Estimation Method for Serial Manipulators Using Inertial Sensors\",\"176\":\"D-ACC: Dynamic Adaptive Cruise Control for Highways with Ramps Based on Deep Q-Learning\",\"177\":\"Precise Multi-Modal In-Hand Pose Estimation using Low-Precision Sensors for Robotic Assembly\",\"178\":\"Assembly Sequences Based on Multiple Criteria Against Products with Deformable Parts\",\"179\":\"Distributed Dynamic Map Fusion via Federated Learning for Intelligent Networked Vehicles\",\"180\":\"Underwater Stability of a Morphable Aerial-Aquatic Quadrotor With Variable Thruster Angles\",\"181\":\"Development of Flapping Robot with Self-Takeoff from The Ground Capability\",\"182\":\"Fast-Tracker: A Robust Aerial System for Tracking Agile Target in Cluttered Environments\",\"183\":\"NavRep: Unsupervised Representations for Reinforcement Learning of Robot Navigation in Dynamic Human Environments\",\"184\":\"High-Speed Planning in Unknown Environments for Multirotors Considering Drag\",\"185\":\"Visual Servoing of Cable-Driven Parallel Robots with Tension Management\",\"186\":\"Automated design of underactuated monolithic soft robotics structures with multiple predefined end poses\",\"187\":\"Vision Based Adaptation to Kernelized Synergies for Human Inspired Robotic Manipulation\",\"188\":\"Vision-Based Robotic Pushing and Grasping for Stone Sample Collection under Computing Resource Constraints\",\"189\":\"Friction Estimation for Tendon-Driven Robotic Hands\",\"190\":\"Representation Matters: Improving Perception and Exploration for Robotics\",\"191\":\"Global Aerial Localisation Using Image and Map Embeddings\",\"192\":\"UWB Indoor Global Localisation for Nonholonomic Robots with Unknown Offset Compensation\",\"193\":\"Range Image-based LiDAR Localization for Autonomous Vehicles\",\"194\":\"RadarLoc: Learning to Relocalize in FMCW Radar\",\"195\":\"MonStereo: When Monocular and Stereo Meet at the Tail of 3D Human Localization\",\"196\":\"Enabling spatio-temporal aggregation in Birds-Eye-View Vehicle Estimation\",\"197\":\"Multimodal Scale Consistency and Awareness for Monocular Self-Supervised Depth Estimation\",\"198\":\"There and Back Again: Self-supervised Multispectral Correspondence Estimation\",\"199\":\"Learning Conditional Postural Synergies for Dexterous Hands: A Generative Approach Based on Variational Auto-Encoders and Conditioned on Object Size and Category\",\"200\":\"ReForm: A Robot Learning Sandbox for Deformable Linear Object Manipulation\",\"201\":\"Adversarial Imitation Learning with Trajectorial Augmentation and Correction\",\"202\":\"Learning Reachable Manifold and Inverse Mapping for a Redundant Robot manipulator\",\"203\":\"Leveraging Forward Model Prediction Error for Learning Control\",\"204\":\"GoSafe: Globally Optimal Safe Robot Learning\",\"205\":\"Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning\",\"206\":\"Active Model Learning using Informative Trajectories for Improved Closed-Loop Control on Real Robots\",\"207\":\"Differentiable Physics Models for Real-world Offline Model-based Reinforcement Learning\",\"208\":\"Adversarial Training is Not Ready for Robot Learning\",\"209\":\"Deep Learning on 3D Object Detection for Automatic Plug-in Charging Using a Mobile Manipulator\",\"210\":\"Rate Mode Bilateral Teleoperation Based on Passivity Tanks and Variable Admittance Control\",\"211\":\"Task Autocorrection for Immersive Teleoperation\",\"212\":\"Manipulability optimization for multi-arm teleoperation\",\"213\":\"Human-robot collaborative object transfer using human motion prediction based on Cartesian pose Dynamic Movement Primitives\",\"214\":\"Dynamic Projection of Human Motion for Safe and Efficient Human-Robot Collaboration\",\"215\":\"Achieving Hard Real-Time Capability for 3D Human Pose Estimation Systems\",\"216\":\"Augmented Hierarchical Quadratic Programming for Adaptive Compliance Robot Control\",\"217\":\"An Optimization Approach for a Robust and Flexible Control in Collaborative Applications\",\"218\":\"Predicting the Post-Impact Velocity of a Robotic Arm via Rigid Multibody Models: an Experimental Study\",\"219\":\"Conv1D Energy-Aware Path Planner for Mobile Robots in Unstructured Environments\",\"220\":\"Resilient Collision-tolerant Navigation in Confined Environments\",\"221\":\"Experimental Validation of Unsteady Wave Induced Loads on a Stationary Remotely Operated Vehicle\",\"222\":\"ASVLite: a high-performance simulator for autonomous surface vehicles\",\"223\":\"Continuous Shortest Path Vector Field Navigation on 3D Triangular Meshes for Mobile Robots\",\"224\":\"Learning to Propagate Interaction Effects for Modeling Deformable Linear Objects Dynamics\",\"225\":\"Modal Dynamic Modelling and Experimental Validation of a Curved Extensible Continuum Manipulator\",\"226\":\"Online Informative Path Planning for Active Information Gathering of a 3D Surface\",\"227\":\"Mathematical Modeling of a Highly Underactuated Tool for Draping Fiber Plies on Double Curved Molds\",\"228\":\"On-line force capability evaluation based on efficient polytope vertex search\",\"229\":\"Shared Autonomy for Teleoperated Driving: A Real-Time Interactive Path Planning Approach\",\"230\":\"Comfortable and Safe Decelerations for a Self-Driving Transit Bus\",\"231\":\"A Multi-UAV System for Detection and Elimination of Multiple Targets\",\"232\":\"Autonomous Aerial Swarming in GNSS-denied Environments with High Obstacle Density\",\"233\":\"Adaptive stiffness estimation impedance control for achieving sustained contact in aerial manipulation\",\"234\":\"Model Predictive Control for Dynamic Quadrotor Bearing Formations\",\"235\":\"Direct Force and Pose NMPC with Multiple Interaction Modes for Aerial Push-and-Slide Operations\",\"236\":\"A Tethered Quadrotor UAV\\u2212Buoy System for Marine Locomotion\",\"237\":\"Distributed Variable-Baseline Stereo SLAM from two UAVs\",\"238\":\"Embodied Reasoning for Discovering Object Properties via Manipulation\",\"239\":\"Robust High-Transparency Haptic Exploration for Dexterous Telemanipulation\",\"240\":\"Unsupervised Feature Learning for Manipulation with Contrastive Domain Randomization\",\"241\":\"\\u201cWhat\\u2019s This?\\u201d - Learning to Segment Unknown Objects from Manipulation Sequences\",\"242\":\"Robust Distributed Estimation of the Algebraic Connectivity for Networked Multi-robot Systems\",\"243\":\"ModGNN: Expert Policy Approximation in Multi-Agent Systems with a Modular Graph Neural Network Architecture\",\"244\":\"Optimization-Inspired Controller Design for Transient Legged Locomotion\",\"245\":\"Multi-Layered Safety for Legged Robots via Control Barrier Functions and Model Predictive Control\",\"246\":\"Agile Actions with a Centaur-Type Humanoid: A Decoupled Approach\",\"247\":\"Combined Sampling and Optimization Based Planning for Legged-Wheeled Robots\",\"248\":\"Image Representation of a City and Its Taxi Fleet for End-To-End Learning of Rebalancing Policies\",\"249\":\"COLREGs-Informed RRT* for Collision Avoidance of Marine Crafts\",\"250\":\"Learning to Robustly Negotiate Bi-Directional Lane Usage in High-Conflict Driving Scenarios\",\"251\":\"Self-Supervised Motion Retargeting with Safety Guarantee\",\"252\":\"Sparse Multilevel Roadmaps for High-Dimensional Robotic Motion Planning\",\"253\":\"Saliency Features for 3D CAD-Data in the Context of Sampling-Based Motion Planning\",\"254\":\"Search-based Planning of Dynamic MAV Trajectories Using Local Multiresolution State Lattices\",\"255\":\"Multifunctional Arm for Telerobotic Wind Turbine Blade Repair\",\"256\":\"Automated Behavior Tree Error Recovery Framework for Robotic Systems\",\"257\":\"Design of a magnetic actuation system for a microbiota-collection ingestible capsule\",\"258\":\"Reactive Cooperative Manipulation based on Set Primitives and Circular Fields\",\"259\":\"Efficient Multi-scale POMDPs for Robotic Object Search and Delivery\",\"260\":\"Co-Optimizing Robot, Environment, and Tool Design via Joint Manipulation Planning\",\"261\":\"End-to-End Semi-supervised Learning for Differentiable Particle Filters\",\"262\":\"Learning to Localize in New Environments from Synthetic Training Data\",\"263\":\"Exploration of Large Outdoor Environments Using Multi-Criteria Decision Making\",\"264\":\"SD-DefSLAM: Semi-Direct Monocular SLAM for Deformable and Intracorporeal Scenes\",\"265\":\"Learning Robot Trajectories subject to Kinematic Joint Constraints\",\"266\":\"Self-Imitation Learning by Planning\",\"267\":\"Robot Learning of 6 DoF Grasping using Model-based Adaptive Primitives\",\"268\":\"Conditional StyleGAN for Grasp Generation\",\"269\":\"Go Fetch! - Dynamic Grasps using Boston Dynamics Spot with External Robotic Arm\",\"270\":\"Multi-FinGAN: Generative Coarse-To-Fine Sampling of Multi-Finger Grasps\",\"271\":\"Sample-efficient Reinforcement Learning in Robotic Table Tennis\",\"272\":\"No Face-Touch: Exploiting Wearable Devices and Machine Learning for Gesture Detection\",\"273\":\"DILIGENT-KIO: A Proprioceptive Base Estimator for Humanoid Robots using Extended Kalman Filtering on Matrix Lie Groups\",\"274\":\"Improving Safety and Accuracy of Impedance Controlled Robot Manipulators with Proximity Perception and Proactive Impact Reactions\",\"275\":\"Optimal scaling of dynamic safety zones for collaborative robotics\",\"276\":\"3D Collision-Force-Map for Safe Human-Robot Collaboration\",\"277\":\"Safe, Passive Control for Mechanical Systems with Application to Physical Human-Robot Interactions\",\"278\":\"Collision Detection, Identification, and Localization on the DLR SARA Robot with Sensing Redundancy\",\"279\":\"CSM: Contact Sensitivity Maps for Benchmarking Robot Collision Handling Systems\",\"280\":\"A Data-Driven Approach for Contact Detection, Classification and Reaction in Physical Human-Robot Collaboration\",\"281\":\"Pointing at Moving Robots: Detecting Events from Wrist IMU Data\",\"282\":\"PATHoBot: A Robot for Glasshouse Crop Phenotyping and Intervention\",\"283\":\"Using depth information and colour space variations for improving outdoor robustness for instance segmentation of cabbage\",\"284\":\"MP-STSP: A Multi-Platform Steiner Traveling Salesman Problem Formulation for Precision Agriculture in Orchards\",\"285\":\"Behavior-Tree-Based Person Search for Symbiotic Autonomous Mobile Robot Tasks\",\"286\":\"Integration of a Human-aware Risk-based Braking System into an Open-Field Mobile Robot\",\"287\":\"Online velocity fluctuation of off-road wheeled mobile robots: A reinforcement learning approach\",\"288\":\"Switching Control in Two-Wheeled Self-Balancing Robots\",\"289\":\"A Finite-Gain Stable Multi-Agent Robot Control Framework with Adaptive Authority Allocation\",\"290\":\"Decentralized Connectivity Maintenance with Time Delays using Control Barrier Functions\",\"291\":\"Multi-robot Implicit Control of Herds\",\"292\":\"Robust Frequency-Based Structure Extraction\",\"293\":\"A Hybrid Collision Model for Safety Collision Control\",\"294\":\"Efficient Recovery of Multi-Camera Motion from Two Affine Correspondences\",\"295\":\"Dynamic-Aware Autonomous Exploration in Populated Environments\",\"296\":\"Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill Primitives\",\"297\":\"Improving Dynamics of an Aerial Manipulator with Elastic Suspension Using Nonlinear Model Predictive Control\",\"298\":\"Pneumatic-Mechanical Systems in UAVs: Autonomous Power Line Sensor Unit Deployment\",\"299\":\"Combined System Identification and State Estimation for a Quadrotor UAV\",\"300\":\"Geometry-aware Compensation Scheme for Morphing Drones\",\"301\":\"SplatPlanner: Efficient Autonomous Exploration via Permutohedral Frontier Filtering\",\"302\":\"Fast Sampling-based Next-Best-View Exploration Algorithm for a MAV\",\"303\":\"Neuromorphic control for optic-flow-based landing of MAVs using the Loihi processor\",\"304\":\"Event-driven Vision and Control for UAVs on a Neuromorphic Chip\",\"305\":\"Deep Neuromorphic Controller with Dynamic Topology for Aerial Robots\",\"306\":\"A Variable Soft Finger Exoskeleton for Quantifying Fatigue-induced Mechanical Impedance\",\"307\":\"Computing the positioning error of an upper-arm robotic prosthesis from the observation of its wearer\\u2019s posture\",\"308\":\"Intent-aware control in kinematically redundant systems: Towards collaborative wearable robots\",\"309\":\"Design, Development and Validation of a Dynamic Fall Prediction System for Excavators\",\"310\":\"Feasible and Adaptive Multimodal Trajectory Prediction with Semantic Maneuver Fusion\",\"311\":\"Exploiting latent representation of sparse semantic layers for improved short-term motion prediction with Capsule Networks\",\"312\":\"Movement recognition and prediction using DMPs\",\"313\":\"Whole Body Model Predictive Control with a Memory of Motion: Experiments on a Torque-Controlled Talos\",\"314\":\"Constraint Handling in Continuous-Time DDP-Based Model Predictive Control\",\"315\":\"Sparsity-Inducing Optimal Control via Differential Dynamic Programming\",\"316\":\"A Passive Navigation Planning Algorithm for Collision-free Control of Mobile Robots\",\"317\":\"Expansive Voronoi Tree: A Motion Planner for Assembly Sequence Planning\",\"318\":\"MS2MP: A Min-Sum Message Passing Algorithm for Motion Planning\",\"319\":\"Cubic B\\u00e9zier Local Path Planner for Non-holonomic Feasible and Comfortable Path Generation\",\"320\":\"Voxplan: A 3D Global Planner using Signed Distance Function Submaps\",\"321\":\"Globally Optimal Online Redundancy Resolution for Serial 7-DOF Kinematics Along SE(3) Trajectories\",\"322\":\"Robot Arm Motion Planning Based on Geodesics\",\"323\":\"FlexDMP \\u2013 Extending Dynamic Movement Primitives towards Flexible Joint Robots\",\"324\":\"ManhattanSLAM: Robust Planar Tracking and Mapping Leveraging Mixture of Manhattan Frames\",\"325\":\"Weighted Node Mapping and Localisation on a Pixel Processor Array\",\"326\":\"SoftMP: Attentive feature pooling for joint local feature detection and description for place recognition in changing environments\",\"327\":\"Beyond ANN: Exploiting Structural Knowledge for Efficient Place Recognition\",\"328\":\"Simultaneous Multi-Level Descriptor Learning and Semantic Segmentation for Domain-Specific Relocalization\",\"329\":\"RADIATE: A Radar Dataset for Automotive Perception in Bad Weather\",\"330\":\"Poisson Surface Reconstruction for LiDAR Odometry and Mapping\",\"331\":\"Lidar-Monocular Surface Reconstruction Using Line Segments\",\"332\":\"Balancing on a Springy Leg\",\"333\":\"Gyrubot: nonanthropomorphic stabilization for a biped\",\"334\":\"A novel method for computing the 3D friction cone using complimentary constraints\",\"335\":\"Learning Behavior Trees with Genetic Programming in Unpredictable Environments\",\"336\":\"Learning Efficient Constraint Graph Sampling for Robotic Sequential Manipulation\",\"337\":\"Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration\",\"338\":\"Predicting Disparity Distributions\",\"339\":\"Scoring Graspability based on Grasp Regression for Better Grasp Prediction\",\"340\":\"MonoSOD: Monocular Salient Object Detection based on Predicted Depth\",\"341\":\"Fast Footstep Planning with Aborting A\",\"342\":\"Exploiting visual servoing and centroidal momentum for whole-body motion control of humanoid robots in absence of contacts and gravity\",\"343\":\"Virtual Adversarial Humans finding Hazards in Robot Workplaces\",\"344\":\"Crowd against the machine: A simulation-based benchmark tool to evaluate and compare robot capabilities to navigate a human crowd\",\"345\":\"A Unified Perception Benchmark for Capacitive Proximity Sensing Towards Safe Human-Robot Collaboration (HRC)\",\"346\":\"Learning Human-like Hand Reaching for Human-Robot Handshaking\",\"347\":\"Simultaneous haptic guidance and learning of task parameters during robotic teleoperation \\u2013 a geometrical approach\",\"348\":\"Human-Like Artificial Skin Sensor for Physical Human-Robot Interaction\",\"349\":\"A Reversible Dynamic Movement Primitive formulation\",\"350\":\"Towards efficient human-robot cooperation for socially-aware robot navigation in human-populated environments: the SNAPE framework\",\"351\":\"Analysis of Open-Loop Grasping From Piles\",\"352\":\"Human Initiated Grasp Space Exploration Algorithm for an Underactuated Robot Gripper Using Variational Autoencoder\",\"353\":\"An Underactuated Gripper based on Car Differentials for Self-Adaptive Grasping with Passive Disturbance Rejection\",\"354\":\"Data-driven sea state estimation for vessels using multi-domain features from motion responses\",\"355\":\"A Fault Tolerant Control Architecture Based on Fault Trees for an Underwater Robot Executing Transect Missions\",\"356\":\"Robust Underwater Visual SLAM Fusing Acoustic Sensing\",\"357\":\"Real-time Friction Estimation for Grip Force Control\",\"358\":\"Uncertainty-aware deep learning for robot touch: Application to Bayesian tactile servo control\",\"359\":\"Towards integrated tactile sensorimotor control in anthropomorphic soft robotic hands\",\"360\":\"An efficient approach to closed-loop shape control of deformable objects using finite element models\",\"361\":\"Learning Stable Normalizing-Flow Control for Robotic Manipulation\",\"362\":\"Model Predictive Robot-Environment Interaction Control for Mobile Manipulation Tasks\",\"363\":\"Surgical Gesture Recognition Based on Bidirectional Multi-Layer Independently RNN with Explainable Spatial Feature Extraction\",\"364\":\"What data do we need for training an AV motion planner?\",\"365\":\"Learn to Path: Using neural networks to predict Dubins path characteristics for aerial vehicles in wind\",\"366\":\"Automated Generation of Robot Trajectories for Assembly Processes Requiring Only Sparse Manual Input\",\"367\":\"Benchmarking Real-Time Capabilities of ROS 2 and OROCOS for Robotics Applications\",\"368\":\"The KIT Gripper: A Multi-Functional Gripper for Disassembly Tasks\",\"369\":\"In-Process Workpiece Geometry Estimation for Robotic Arc Welding based on Supervised Learning for Multi-Sensor Inputs\",\"370\":\"Context-Dependent Anomaly Detection for Low Altitude Traffic Surveillance\",\"371\":\"Autonomous Flying into Buildings in a Firefighting Scenario\",\"372\":\"Polyhedral Friction Cone Estimator for Object Manipulation\",\"373\":\"Interpretability in Contact-Rich Manipulation via Kinodynamic Images\",\"374\":\"Model-Free Reinforcement Learning for Stochastic Games with Linear Temporal Logic Objectives\",\"375\":\"Secure Planning Against Stealthy Attacks via Model-Free Reinforcement Learning\",\"376\":\"Hierarchies of Planning and Reinforcement Learning for Robot Navigation\",\"377\":\"A Laser-based Dual-arm System for Precise Control of Collaborative Robots\",\"378\":\"Near-Optimal Multi-Robot Motion Planning with Finite Sampling\",\"379\":\"Whole-Body Real-Time Motion Planning for Multicopters\",\"380\":\"Collision-Free MPC for Legged Robots in Static and Dynamic Scenes\",\"381\":\"Obstacle Avoidance with Kinetic Energy Buffer\",\"382\":\"Learning from Simulation, Racing in Reality\",\"383\":\"Equality Constrained Differential Dynamic Programming\",\"384\":\"Unsupervised Motion Estimation of Vehicles Using ICP\",\"385\":\"CNN-based Ego-Motion Estimation for Fast MAV Maneuvers\",\"386\":\"Mid-Air Range-Visual-Inertial Estimator Initialization for Micro Air Vehicles\",\"387\":\"Pose Estimation for Vehicle-mounted Cameras via Horizontal and Vertical Planes\",\"388\":\"Dynamic Occupancy Grid Mapping with Recurrent Neural Networks\",\"389\":\"Automatic Mapping of Tailored Landmark Representations for Automated Driving and Map Learning\",\"390\":\"Lightweight Semantic Mesh Mapping for Autonomous Vehicles\",\"391\":\"LatentSLAM: unsupervised multi-sensor representation learning for localization and mapping\",\"392\":\"Robot in a China Shop: Using Reinforcement Learning for Location-Specific Navigation Behaviour\",\"393\":\"Model Identification of a Small Fully-Actuated Aquatic Surface Vehicle Using a Long Short-Term Memory Neural Network\",\"394\":\"Real-Time Trajectory Adaptation for Quadrupedal Locomotion using Deep Reinforcement Learning\",\"395\":\"Robust Iterative Learning Control for Pneumatic Muscle with State Constraint and Model Uncertainty\",\"396\":\"NDT-Transformer: Large-Scale 3D Point Cloud Localisation using the Normal Distribution Transform Representation\",\"397\":\"Implementation of a Reactive Walking Controller for the New Open-Hardware Quadruped Solo-12\",\"398\":\"Imitation Learning from MPC for Quadrupedal Multi-Gait Control\",\"399\":\"Comparison of predictive controllers for locomotion and balance recovery of quadruped robots\",\"400\":\"Locomotion Adaptation in Heavy Payload Transportation Tasks with the Quadruped Robot CENTAURO\",\"401\":\"Efficient Self-Supervised Data Collection for Offline Robot Learning\",\"402\":\"Active Inference for Integrated State-Estimation, Control, and Learning\",\"403\":\"Robot Program Parameter Inference via Differentiable Shadow Program Inversion\",\"404\":\"A Fully Spiking Neural Control System Based on Cerebellar Predictive Learning for Sensor-Guided Robots\",\"405\":\"Learning to steer a locomotion contact planner\",\"406\":\"Learning Shape Control of Elastoplastic Deformable Linear Objects\",\"407\":\"Precise Jump Planning using Centroidal Dynamics based Bilevel Optimization\",\"408\":\"DeepWalk: Omnidirectional Bipedal Gait by Deep Reinforcement Learning\",\"409\":\"ULT-model: Towards a one-legged unified locomotion template model for forward hopping with an upright trunk\",\"410\":\"Nonlinear stiffness allows passive dynamic hopping for one-legged robots with an upright trunk\",\"411\":\"Task Planning with a Weighted Functional Object-Oriented Network\",\"412\":\"Towards providing explanations for robot motion planning\",\"413\":\"Engagement Estimation During Child Robot Interaction Using Deep Convolutional Networks Focusing on ASD Children\",\"414\":\"Ergodic imitation: Learning from what to do and what not to do\",\"415\":\"Imitation Learning with Inconsistent Demonstrations through Uncertainty-based Data Manipulation\",\"416\":\"Learning Motor Resonance in Human-Human and Human-Robot Interaction with Coupled Dynamical Systems\",\"417\":\"Interpreting Contact Interactions to Overcome Failure in Robot Assembly Tasks\",\"418\":\"Real-time Surgical Environment Enhancement for Robot-Assisted Minimally Invasive Surgery Based on Super-Resolution\",\"419\":\"Composing HARMONI: An Open-source Tool for Human and Robot Modular OpeN Interaction\",\"420\":\"Robot Interaction Studio: A Platform for Unsupervised HRI\",\"421\":\"Which gesture generator performs better?\",\"422\":\"Robot-supervised Learning of Crop Row Segmentation\",\"423\":\"Neural Network Controller for Autonomous Pile Loading Revised\",\"424\":\"Deep Reinforcement Learning for Concentric Tube Robot Control with a Goal-Based Curriculum\",\"425\":\"Optimized 3D path planner for steerable catheters with deductive reasoning\",\"426\":\"Robotic Electrospinning Actuated by Non-Circular Joint Continuum Manipulator for Endoluminal Therapy\",\"427\":\"Distributed Full-Consensus Control of Multi-Robot Systems with Range and Field-of-View Constraints\",\"428\":\"Scalable Recursive Distributed Collaborative State Estimation for Aided Inertial Navigation\",\"429\":\"Distributed Multi-Target Tracking in Camera Networks\",\"430\":\"GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics\",\"431\":\"Robot-Safe Impacts with Soft Contacts Based on Learned Deformations\",\"432\":\"Contact Forces Preintegration for Estimation in Legged Robotics using Factor Graphs\",\"433\":\"Amortized Q-learning with Model-based Action Proposals for Autonomous Driving on Highways\",\"434\":\"Decision Making for Autonomous Driving via Augmented Adversarial Inverse Reinforcement Learning\",\"435\":\"Interpretable Goal-based Prediction and Planning for Autonomous Driving\",\"436\":\"Encoding Human Driving Styles in Motion Planning for Autonomous Vehicles\",\"437\":\"Spherical Multi-Modal Place Recognition for Heterogeneous Sensor Systems\",\"438\":\"A Direct Collocation method for optimization of EMG-driven wrist muscle musculoskeletal model\",\"439\":\"Practical and Accurate Generation of Energy-Optimal Trajectories for a Planar Quadrotor\",\"440\":\"Optimization-based Trajectory Planning for Tethered Aerial Robots\",\"441\":\"Grasp Analysis and Manipulation Kinematics for Isoperimetric Truss Robots\",\"442\":\"Improving Grasp Classification through Spatial Metrics Available from Sensors\",\"443\":\"Assistive supernumerary grasping with the back of the hand\",\"444\":\"MCMC Occupancy Grid Mapping with a Data-Driven Patch Prior\",\"445\":\"Shape-Based Transfer of Generic Skills\",\"446\":\"Safety Uncertainty in Control Barrier Functions using Gaussian Processes\",\"447\":\"Object Rearrangement Using Learned Implicit Collision Functions\",\"448\":\"Visual-Laser-Inertial SLAM Using a Compact 3D Scanner for Confined Space\",\"449\":\"Efficient Multi-sensor Aided Inertial Navigation with Online Calibration\",\"450\":\"Robust Monocular Visual-Inertial Depth Completion for Embedded Systems\",\"451\":\"Simultaneous Estimation and Modeling of Robotic Systems with Non-Gaussian State Belief\",\"452\":\"Efficient Online Calibration for Autonomous Vehicle\\u2019s Longitudinal Dynamical System: A Gaussian Model Approach\",\"453\":\"Fuzzing Mobile Robot Environments for Fast Automated Crash Detection\",\"454\":\"Optimal Estimation of the Centroidal Dynamics of Legged Robots\",\"455\":\"A Unified Optimization Framework and New Set of Performance Metrics for Robot Leg Design\",\"456\":\"A Novel Model Predictive Control Framework Using Dynamic Model Decomposition Applied to Dynamic Legged Locomotion\",\"457\":\"Generating Continuous Motion and Force Plans in Real-Time for Legged Mobile Manipulation\",\"458\":\"Robots of the Lost Arc: Self-Supervised Learning to Dynamically Manipulate Fixed-Endpoint Cables\",\"459\":\"Learning to Rearrange Deformable Cables, Fabrics, and Bags with Goal-Conditioned Transporter Networks\",\"460\":\"A Joint Network for Grasp Detection Conditioned on Natural Language Commands\",\"461\":\"ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation\",\"462\":\"Efficient Reachability Analysis of Closed-Loop Systems with Neural Network Controllers\",\"463\":\"Motion Planning and Feedback Control for Bipedal Robots Riding a Snakeboard\",\"464\":\"Global Position Control on Underactuated Bipedal Robots: Step-to-step Dynamics Approximation for Step Planning\",\"465\":\"One-Step Ahead Prediction of Angular Momentum about the Contact Point for Control of Bipedal Locomotion: Validation in a LIP-inspired Controller\",\"466\":\"Hybrid Sampling\\/Optimization-based Planning for Agile Jumping Robots on Challenging Terrains\",\"467\":\"Investigation of Unmanned Aerial Vehicle Gesture Perceptibility and Impact of Viewpoint Variance\",\"468\":\"Watch Where You\\u2019re Going! Gaze and Head Orientation as Predictors for Social Robot Navigation\",\"469\":\"Can a Robot Trust You? : A DRL-Based Approach to Trust-Driven Human-Guided Navigation\",\"470\":\"Mesh Based Analysis of Low Fractal Dimension Reinforcement Learning Policies\",\"471\":\"Design and Validation of a Novel Exoskeleton Hand Interface: The Eminence Grip\",\"472\":\"Active Telepresence Assistance for Supervisory Control: A User Study with a Multi-Camera Tele-Nursing Robot\",\"473\":\"Force-Sensing Tensegrity for Investigating Physical Human-Robot Interaction in Compliant Robotic Systems\",\"474\":\"Risk-Aware Decision Making for Service Robots to Minimize Risk of Patient Falls in Hospitals\",\"475\":\"Haptic Feedback Improves Human-Robot Agreement and User Satisfaction in Shared-Autonomy Teleoperation\",\"476\":\"Environment Reconfiguration Planning for Autonomous Robotic Manipulation to overcome Mobility Constraints\",\"477\":\"Adaptive Sampling using POMDPs with Domain-Specific Considerations\",\"478\":\"Human Arm Stability in Relation to Damping-Defined Mechanical Environments in Physical Interaction with a Robotic Arm\",\"479\":\"Conditioning Style on Substance: Plans for Narrative Observation\",\"480\":\"DeepQ Stepper: A framework for reactive dynamic walking on uneven terrain\",\"481\":\"Wetland Soil Strength Tester and Core Sampler Using a Drone\",\"482\":\"Backstepping and Sliding Mode Control for AUVs Aided with Bioinspired Neurodynamics\",\"483\":\"Learning-based Inverse Kinematics from Shape as Input for Concentric Tube Continuum Robots\",\"484\":\"Using Euler Curves to Model Continuum Robots\",\"485\":\"Data-driven Actuator Selection for Artificial Muscle-Powered Robots\",\"486\":\"EMG-Based Neural Network Model of Human Arm Dynamics in a Haptic Training Simulator of Sinus Endoscopy\",\"487\":\"Multimodal dynamics modeling for off-road autonomous vehicles\",\"488\":\"An Anytime Algorithm for Chance Constrained Stochastic Shortest Path Problems and Its Application to Aircraft Routing\",\"489\":\"An Intention Guided Hierarchical Framework for Trajectory-based Teleoperation of Mobile Robots\",\"490\":\"Accelerating combinatorial filter reduction through constraints\",\"491\":\"MorphEyes: Variable Baseline Stereo For Quadrotor Navigation\",\"492\":\"A Drive-through Recharging Strategy for a Quadrotor\",\"493\":\"Continuous-time State & Dynamics Estimation using a Pseudo-Spectral Parameterization\",\"494\":\"Active Bayesian Multi-class Mapping from Range and Semantic Segmentation Observations\",\"495\":\"Attention-Based Probabilistic Planning with Active Perception\",\"496\":\"Search-based Planning for Active Sensing in Goal-Directed Coverage Tasks\",\"497\":\"Docking and Undocking a Modular Underactuated Oscillating Swimming Robot\",\"498\":\"Predictive 3D Sonar Mapping of Underwater Environments via Object-specific Bayesian Inference\",\"499\":\"Self-Organized Evasive Fountain Maneuvers with a Bioinspired Underwater Robot Collective\",\"500\":\"Mass Estimation of a Moving Object Through Minimal Manipulation Interaction\",\"501\":\"GelSight Wedge: Measuring High-Resolution 3D Contact Geometry with a Compact Robot Finger\",\"502\":\"Identifying External Contacts from Joint Torque Measurements on Serial Robotic Arms and Its Limitations\",\"503\":\"Contact Mode Guided Sampling-Based Planning for Quasistatic Dexterous Manipulation in 2D\",\"504\":\"kPAM-SC: Generalizable Manipulation Planning using KeyPoint Affordance and Shape Completion\",\"505\":\"Alternative Paths Planner (APP) for Provably Fixed-time Manipulation Planning in Semi-structured Environments\",\"506\":\"Hierarchical Planning for Long-Horizon Manipulation with Geometric and Symbolic Scene Graphs\",\"507\":\"Contact Localization for Robot Arms in Motion without Torque Sensing\",\"508\":\"Semi-Infinite Programming with Complementarity Constraints for Pose Optimization with Pervasive Contact\",\"509\":\"Finite-Horizon Synthesis for Probabilistic Manipulation Domains\",\"510\":\"IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks\",\"511\":\"Learning Dexterous Grasping with Object-Centric Visual Affordances\",\"512\":\"Learning Collaborative Pushing and Grasping Policies in Dense Clutter\",\"513\":\"Grasping with Chopsticks: Combating Covariate Shift in Model-free Imitation Learning for Fine Manipulation\",\"514\":\"Learning Task-Oriented Dexterous Grasping from Human Knowledge\",\"515\":\"Feedback Linearization for Quadrotors with a Learned Acceleration Error Model\",\"516\":\"Cirrus: A Long-range Bi-pattern LiDAR Dataset\",\"517\":\"Airflow-Inertial Odometry for Resilient State Estimation on Multirotors\",\"518\":\"\\u03c0-LSAM: LiDAR Smoothing and Mapping With Planes\",\"519\":\"Robust Place Recognition using an Imaging Lidar\",\"520\":\"High-Speed Robot Navigation using Predicted Occupancy Maps\",\"521\":\"The Fluid Field SLIP Model: Terrestrial-Aquatic Dynamic Legged Locomotion\",\"522\":\"Dynamics Randomization Revisited: A Case Study for Quadrupedal Locomotion\",\"523\":\"Learning Multimodal Contact-Rich Skills from Demonstrations Without Reward Engineering\",\"524\":\"DIPN: Deep Interaction Prediction Network with Application to Clutter Removal\",\"525\":\"Learning Sampling Distributions Using Local 3D Workspace Decompositions for Motion Planning in High Dimensions\",\"526\":\"Learning and Planning for Temporally Extended Tasks in Unknown Environments\",\"527\":\"Behavior Tree Learning for Robotic Task Planning through Monte Carlo DAG Search over a Formal Grammar\",\"528\":\"Improving Off-road Planning Techniques with Learned Costs from Physical Interactions\",\"529\":\"A Comparison Between Joint Space and Task Space Mappings for Dynamic Teleoperation of an Anthropomorphic Robotic Arm in Reaction Tests\",\"530\":\"Identifying Driver Interactions via Conditional Behavior Prediction\",\"531\":\"Autonomous Robotic Escort Incorporating Motion Prediction and Human Intention\",\"532\":\"Two-Stage Clustering of Human Preferences for Action Prediction in Assembly Tasks\",\"533\":\"Dynamically Switching Human Prediction Models for Efficient Planning\",\"534\":\"Temporal Anticipation and Adaptation Methods for Fluent Human-Robot Teaming\",\"535\":\"Robust Planning with Emergent Human-like Behavior for Agents Traveling in Groups\",\"536\":\"Order Matters: Generating Progressive Explanations for Planning Tasks in Human-Robot Teaming\",\"537\":\"Learning from Demonstration for Real-Time User Goal Prediction and Shared Assistive Control\",\"538\":\"Reaching Pruning Locations in a Vine Using a Deep Reinforcement Learning Policy\",\"539\":\"A Generative Model-Based Predictive Display for Robotic Teleoperation\",\"540\":\"A Robot Walks into a Bar: Automatic Robot Joke Success Assessment\",\"541\":\"Detecting and Counting Oysters\",\"542\":\"Autonomous Distributed 3D Radiation Field Estimation for Nuclear Environment Characterization\",\"543\":\"Locomotion and Control of a Friction-Driven Tripedal Robot\",\"544\":\"Design Considerations for a Steerable Needle Robot to Maximize Reachable Lung Volume\",\"545\":\"Nth Order Analytical Time Derivatives of Inverse Dynamics in Recursive and Closed Forms\",\"546\":\"Efficient Configuration Exploration in Inverse Dynamics Acquisition of Robotic Manipulators\",\"547\":\"SelfDeco: Self-Supervised Monocular Depth Completion in Challenging Indoor Environments\",\"548\":\"Detect, Reject, Correct: Crossmodal Compensation of Corrupted Sensors\",\"549\":\"Advanced Sensing Development to Support Robot Accuracy Assessment and Improvement\",\"550\":\"Robotic Grasping of Fully-Occluded Objects using RF Perception\",\"551\":\"A Simulation-Based Grasp Planner for Enabling Robotic Grasping during Composite Sheet Layup\",\"552\":\"Collision-free vector field guidance and MPC for a fixed-wing UAV\",\"553\":\"Toward Impact-resilient Quadrotor Design, Collision Characterization and Recovery Control to Sustain Flight after Collisions\",\"554\":\"Soft Hybrid Aerial Vehicle via Bistable Mechanism\",\"555\":\"H-ModQuad: Modular Multi-Rotors with 4, 5, and 6 Controllable DOF\",\"556\":\"Robust Adaptive Synchronization of Interconnected Heterogeneous Quadrotors Transporting a Cable-Suspended Load\",\"557\":\"Adaptive Failure Search Using Critical States from Domain Experts\",\"558\":\"Policy Transfer via Kinematic Domain Randomization and Adaptation\",\"559\":\"Uniform Complete Observability of Mass and Rotational Inertial Parameters in Adaptive Identification of Rigid-Body Plant Dynamics\",\"560\":\"Assumption Monitoring Using Runtime Verification for UAV Temporal Task Plan Executions\",\"561\":\"Scalable POMDP Decision-Making Using Circulant Controllers\",\"562\":\"Implicit Integration for Articulated Bodies with Contact via the Nonconvex Maximal Dissipation Principle\",\"563\":\"Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models\",\"564\":\"DisCo RL: Distribution-Conditioned Reinforcement Learning for General-Purpose Policies\",\"565\":\"LASER: Learning a Latent Action Space for Efficient Reinforcement Learning\",\"566\":\"Region-Based Planning for 3D Within-Hand-Manipulation via Variable Friction Robot Fingers and Extrinsic Contacts\",\"567\":\"Planning for Multi-stage Forceful Manipulation\",\"568\":\"Towards Robust Planar Translations using Delta-manipulator Arrays\",\"569\":\"Manipulation Planning Among Movable Obstacles Using Physics-Based Adaptive Motion Primitives\",\"570\":\"Robotic Grasping through Combined Image-Based Grasp Proposal and 3D Reconstruction\",\"571\":\"Attribute-Based Robotic Grasping with One-Grasp Adaptation\",\"572\":\"Collision-Aware Target-Driven Object Grasping in Constrained Environments\",\"573\":\"6-DoF Contrastive Grasp Proposal Network\",\"574\":\"Decision Making in Joint Push-Grasp Action Space for Large-Scale Object Sorting\",\"575\":\"Deep Affordance Foresight: Planning Through What Can Be Done in the Future\",\"576\":\"Learning Dense Rewards for Contact-Rich Manipulation Tasks\",\"577\":\"ACRONYM: A Large-Scale Grasp Dataset Based on Simulation\",\"578\":\"DWA-RL: Dynamically Feasible Deep Reinforcement Learning Policy for Robot Navigation among Mobile Obstacles\",\"579\":\"Reinforcement Learning for Autonomous Driving with Latent State Inference and Spatial-Temporal Relationships\",\"580\":\"Improving Ranging-Based Location Estimation with Rigidity-Constrained CRLB-Based Motion Planning\",\"581\":\"Invariant Extended Kalman Filtering Using Two Position Receivers for Extended Pose Estimation\",\"582\":\"Compartmentalized Covariance Intersection: A Novel Filter Architecture for Distributed Localization\",\"583\":\"3D Motion Capture of an Unmodified Drone with Single-chip Millimeter Wave Radar\",\"584\":\"Zero-Shot Reinforcement Learning on Graphs for Autonomous Exploration Under Uncertainty\",\"585\":\"Fast Uncertainty Quantification for Deep Object Pose Estimation\",\"586\":\"Mesh Reconstruction from Aerial Images for Outdoor Terrain Mapping Using Joint 2D-3D Learning\",\"587\":\"ECNNs: Ensemble Learning Methods for Improving Planar Grasp Quality Estimation\",\"588\":\"Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies\",\"589\":\"SuPer Deep: A Surgical Perception Framework for Robotic Tissue Manipulation using Deep Learning for Feature Extraction\",\"590\":\"Perceive, Attend, and Drive: Learning Spatial Attention for Safe Self-Driving\",\"591\":\"Learning Human Objectives from Sequences of Physical Corrections\",\"592\":\"SimGAN: Hybrid Simulator Identification for Domain Adaptation via Adversarial Reinforcement Learning\",\"593\":\"Look at my new blue force-sensing shoes!\",\"594\":\"UAV Target-Selection: 3D Pointing Interface System for Large-Scale Environment\",\"595\":\"SQRP: Sensing Quality-aware Robot Programming System for Non-expert Programmers\",\"596\":\"Automated Environment Reduction for Debugging Robotic Systems\",\"597\":\"ARROCH: Augmented Reality for Robots Collaborating with a Human\",\"598\":\"ARC-LfD: Using Augmented Reality for Interactive Long-Term Robot Skill Maintenance via Constrained Learning from Demonstration\",\"599\":\"Bringing WALL-E out of the Silver Screen: Understanding How Transformative Robot Sound Affects Human Perception\",\"600\":\"How People Use Active Telepresence Cameras in Tele-manipulation\",\"601\":\"Social Navigation for Mobile Robots in the Emergency Department\",\"602\":\"Decentralized Structural-RNN for Robot Crowd Navigation with Deep Reinforcement Learning\",\"603\":\"Range Limited Coverage Control using Air-Ground Multi-Robot Teams\",\"604\":\"Communication Strategy for Efficient Guidance Providing : Domain-structure Awareness, Performance Trade-offs, and Value of Future Observations\",\"605\":\"LBGP: Learning Based Goal Planning for Autonomous Following in Front\",\"606\":\"Reactive Human-to-Robot Handovers of Arbitrary Objects\",\"607\":\"Smile Like You Mean It: Driving Animatronic Robotic Face with Learned Models\",\"608\":\"I Know What You Meant: Learning Human Objectives by (Under)estimating Their Choice Set\",\"609\":\"Analyzing Human Models that Adapt Online\",\"610\":\"When Shall I Be Empathetic? The Utility of Empathetic Parameter Estimation in Multi-Agent Interactions\",\"611\":\"Three-dimensional Terrain Aware Autonomous Exploration for Subterranean and Confined Spaces\",\"612\":\"Semantically-Aware Strategies for Stereo-Visual Robotic Obstacle Avoidance\",\"613\":\"LiDARNet: A Boundary-Aware Domain Adaptation Model for Point Cloud Semantic Segmentation\",\"614\":\"Transition Motion Planning for Multi-Limbed Vertical Climbing Robots Using Complementarity Constraints\",\"615\":\"Inverse Dynamics Control of Compliant Hybrid Zero Dynamic Walking\",\"616\":\"The dynamic effect of mechanical losses of transmissions on the equation of motion of legged robots\",\"617\":\"Stabilizing Neural Control Using Self-Learned Almost Lyapunov Critics\",\"618\":\"Regularizing Action Policies for Smooth Control with Reinforcement Learning\",\"619\":\"DeepReach: A Deep Learning Approach to High-Dimensional Reachability\",\"620\":\"Deep Reinforcement Learning for Active Target Tracking\",\"621\":\"Deep Reinforcement Learning for Mapless Navigation of a Hybrid Aerial Underwater Vehicle with Medium Transition\",\"622\":\"NF-iSAM: Incremental Smoothing and Mapping via Normalizing Flows\",\"623\":\"UPSLAM: Union of Panoramas SLAM\",\"624\":\"RELLIS-3D Dataset: Data, Benchmarks and Analysis\",\"625\":\"Model-based Reinforcement Learning with Provable Safety Guarantees via Control Barrier Functions\",\"626\":\"Continual Model-Based Reinforcement Learning with Hypernetworks\",\"627\":\"Reinforcement Learning Based Temporal Logic Control with Maximum Probabilistic Satisfaction\",\"628\":\"Solving Markov Decision Processes with Partial State Abstractions\",\"629\":\"CVaR-based Flight Energy Risk Assessment for Multirotor UAVs using a Deep Energy Model\",\"630\":\"Hypergame-based Adaptive Behavior Path Planning for Combined Exploration and Visual Search\",\"631\":\"Morphologically Adapatative Quad-Rotor Towards Acquiring High-Performance Flight: A Comparative Study and Validation\",\"632\":\"Beelines: Motion Prediction Metrics for Self-Driving Safety and Comfort\",\"633\":\"Performance Metrics Calculation for Assembly Systems with Exponential Reliability Machines\",\"634\":\"Learning Seed Placements and Automation Policies for Polyculture Farming with Companion Plants\",\"635\":\"A General-Purpose Anomalous Scenario Synthesizer for Rotary Equipment\",\"636\":\"An Autonomous Vault-Building Robot System for Creating Spanning Structures\",\"637\":\"Towards the Unification of System Design and Motion Synthesis for High-Performance Hopping Robots\",\"638\":\"Multi-Step Recurrent Q-Learning for Robotic Velcro Peeling\",\"639\":\"Reset-Free Reinforcement Learning via Multi-Task Learning: Learning Dexterous Manipulation Behaviors without Human Intervention\",\"640\":\"Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning\",\"641\":\"Robotic Slicing of Fruits and Vegetables: Modeling the Effects of Fracture Toughness and Knife Geometry\",\"642\":\"Auto-Tuned Sim-to-Real Transfer\",\"643\":\"A Convex Quasistatic Time-stepping Scheme for Rigid Multibody Systems with Contact and Friction\",\"644\":\"Uniform Object Rearrangement: From Complete Monotone Primitives to Efficient Non-Monotone Informed Search\",\"645\":\"RASCAL: Robotic Arm for Sherds and Ceramics Automated Locomotion\",\"646\":\"Reactive Planning for Mobile Manipulation Tasks in Unexplored Semantic Environments\",\"647\":\"Arm-Hand Systems As Hybrid Parallel-Serial Systems: A Novel Inverse Kinematics Solution\",\"648\":\"Tactile-RL for Insertion: Generalization to Objects of Unknown Geometry\",\"649\":\"Sim-to-Real for Robotic Tactile Sensing via Physics-Based Simulation and Learned Latent Projections\",\"650\":\"Tactile SLAM: Real-time inference of shape and pose from planar pushing\",\"651\":\"APPLI: Adaptive Planner Parameter Learning From Interventions\",\"652\":\"APPLR: Adaptive Planner Parameter Learning from Reinforcement\",\"653\":\"Reinforced iLQR: A Sample-Efficient Robot Locomotion Learning\",\"654\":\"Learning Multi-Arm Manipulation Through Collaborative Teleoperation\",\"655\":\"Scalable Learning of Safety Guarantees for Autonomous Systems using Hamilton-Jacobi Reachability\",\"656\":\"OmniHang: Learning to Hang Arbitrary Objects using Contact Point Correspondences and Neural Collision Estimation\",\"657\":\"Asynchronous Multi-View SLAM\",\"658\":\"Fusion-DHL: WiFi, IMU, and Floorplan Fusion for Dense History of Locations in Indoor Environments\",\"659\":\"LVI-SAM: Tightly-coupled Lidar-Visual-Inertial Odometry via Smoothing and Mapping\",\"660\":\"Learned Uncertainty Calibration for Visual Inertial Localization\",\"661\":\"Distributed Client-Server Optimization for SLAM with Limited On-Device Resources\",\"662\":\"Model Predictive Control for Cooperative Hunting in Obstacle Rich and Dynamic Environments\",\"663\":\"Instance-Aware Predictive Navigation in Multi-Agent Environments\",\"664\":\"SimNet: Learning Reactive Self-driving Simulations from Real-world Observations\",\"665\":\"Robotic Information Gathering using Semantic Language Instructions\",\"666\":\"Deep Structured Reactive Planning\",\"667\":\"Learning a Centroidal Motion Planner for Legged Locomotion\",\"668\":\"Optimizing Cellular Networks via Continuously Moving Base Stations on Road Networks\",\"669\":\"The Resh Programming Language for Multirobot Orchestration\",\"670\":\"Sensing via Collisions: a Smart Cage for Quadrotors with Applications to Self-Localization\",\"671\":\"Generative Design of NU\\u2019s Husky Carbon, A Morpho-Functional, Legged Robot\",\"672\":\"Learning Bipedal Robot Locomotion from Human Movement\",\"673\":\"Learning Task Space Actions for Bipedal Locomotion\",\"674\":\"Preference-Based Learning for User-Guided HZD Gait Generation on Bipedal Walking Robots\",\"675\":\"Reinforcement Learning for Robust Parameterized Locomotion Control of Bipedal Robots\",\"676\":\"Online Dynamic Time Warping Algorithm for Human-Robot Imitation\",\"677\":\"Investigation of Multiple Resource Theory Design Principles on Robot Teleoperation and Workload Management\",\"678\":\"Time-Domain Passivity-based Controller with an Optimal Two-channel Lawrence Telerobotic Architecture\",\"679\":\"Can Therapists Design Robot-Mediated Interventions and Teleoperate Robots Using VR to Deliver Interventions for ASD?\",\"680\":\"A Low-cost Intrinsically Safe Mechanism for Physical Distancing Between Clinicians and Patients\",\"681\":\"Collaborative Fall Detection using a Wearable Device and a Companion Robot\",\"682\":\"Intermittent Visual Servoing: Efficiently Learning Policies Robust to Instrument Changes for High-precision Surgical Manipulation\",\"683\":\"Crawling Support Using Wearable SuperLimbs: Human-Robot Synchronization and Metabolic Cost Assessment\",\"684\":\"ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton Gait Preference Landscapes\",\"685\":\"Control of a Transfemoral Prosthesis on Sloped Terrain using Continuous and Nonlinear Impedance Parameters\",\"686\":\"Model-Dependent Prosthesis Control with Interaction Force Estimation\",\"687\":\"End-to-end grasping policies for human-in-the-loop robots via deep reinforcement learning\",\"688\":\"Situational Confidence Assistance for Lifelong Shared Autonomy\",\"689\":\"Recognizing Orientation Slip in Human Demonstrations\",\"690\":\"Aggregating Long-Term Context for Learning Laparoscopic and Robot-Assisted Surgical Workflows\",\"691\":\"A Safe Hierarchical Planning Framework for Complex Driving Scenarios based on Reinforcement Learning\",\"692\":\"Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning\",\"693\":\"Collision Avoidance in Tightly-Constrained Environments without Coordination: a Hierarchical Control Approach\",\"694\":\"Zero-Potential-Energy Motions due to Stiffness in Impedance Control of Robotic Tasks: an Innovative Theory and Experimental Study\",\"695\":\"No-frills Dynamic Planning using Static Planners\",\"696\":\"PCMPC: Perception-Constrained Model Predictive Control for Quadrotors with Suspended Loads using a Single Camera and IMU\",\"697\":\"Learning Agile Locomotion Skills with a Mentor\",\"698\":\"Automating Behavior Selection for Affective Telepresence Robot\",\"699\":\"Fast Path Computation using Lattices in the Sensor-Space for Forest Navigation\",\"700\":\"Hierarchical Object Map Estimation for Efficient and Robust Navigation\",\"701\":\"Robot Navigation in Constrained Pedestrian Environments using Reinforcement Learning\",\"702\":\"Team Assignment for Heterogeneous Multi-Robot Sensor Coverage through Graph Representation Learning\",\"703\":\"GPR-based Model Reconstruction System for Underground Utilities Using GPRNet\",\"704\":\"Replay Overshooting: Learning Stochastic Latent Dynamics with the Extended Kalman Filter\",\"705\":\"Freyja: A Full Multirotor System for Agile & Precise Outdoor Flights\",\"706\":\"Optimizing Part Placement for Improving Accuracy of Robot-Based Additive Manufacturing\",\"707\":\"Automated Mosquito Salivary Gland Extractor for PfSPZ-based Malaria Vaccine Production\",\"708\":\"An Artin Braid Group Representation of Knitting Machine State with Applications to Validation and Optimization of Fabrication Plans\",\"709\":\"Discriminative Asymmetric Learning for Efficient Surgical Instrument Parsing\",\"710\":\"One to Many: Adaptive Instrument Segmentation via Meta Learning and Dynamic Online Adaptation in Robotic Surgical Video\",\"711\":\"Target-targeted Domain Adaptation for Unsupervised Semantic Segmentation\",\"712\":\"Point Cloud Segmentation via Edge-fused Local Graph Learning\",\"713\":\"Tightly-Coupled Multi-Sensor Fusion for Localization with LiDAR Feature Maps\",\"714\":\"Greedy-Based Feature Selection for Efficient LiDAR SLAM\",\"715\":\"Retrieval and Localization with Observation Constraints\",\"716\":\"An integrated approach for determining objects to be relocated and their goal positions inside clutter for object retrieval\",\"717\":\"A Hybrid Position\\/Force Controller for Joint Robots\",\"718\":\"DIMSAN: Fast Exploration with the Synergy between Density-based Intrinsic Motivation and Self-adaptive Action Noise\",\"719\":\"Design and Testing of a Damped Piezo-Driven Decoupled XYZ Stage\",\"720\":\"Innovative Design and Simulation of a Transformable Robot with Flexibility and Versatility, RHex-T3\",\"721\":\"A Variable Stiffness Actuator Based on Second-order Lever Mechanism and Its Manipulator Integration\",\"722\":\"Robot Motion Planning with Human-Like Motion Patterns based on Human Arm Movement Primitive Chains\",\"723\":\"A Model-Free Synchronous Control of Humanoid Robot Finger\",\"724\":\"Generalized Point Set Registration with the Kent Distribution\",\"725\":\"Self-Supervised Learning for Monocular Depth Estimation on Minimally Invasive Surgery Scenes\",\"726\":\"Reciprocally Rotating Magnetic Actuation and Automatic Trajectory Following for Wireless Capsule Endoscopy\",\"727\":\"Reduced Dynamics and Control for an Autonomous Bicycle\",\"728\":\"Balance Control of a Novel Wheel-legged Robot: Design and Experiments\",\"729\":\"RGB Matters: Learning 7-DoF Grasp Poses on Monocular RGBD Images\",\"730\":\"Hybrid Vision\\/Force Control for Interaction with the Bottle-like Object\",\"731\":\"REGNet: REgion-based Grasp Network for End-to-end Grasp Detection in Point Clouds\",\"732\":\"Elevation control of a soft jumping robot\",\"733\":\"A Soft-Rigid Air-Propelled Pipe-Climbing Robot\",\"734\":\"Mechanical Intelligence for Adaptive Precision Grasp\",\"735\":\"Hierarchical Learning from Demonstrations for Long-Horizon Tasks\",\"736\":\"Pylot: A Modular Platform for Exploring Latency-Accuracy Tradeoffs in Autonomous Vehicles\",\"737\":\"Decentralized Circle Formation Control for Fish-like Robots in the Real-world via Reinforcement Learning\",\"738\":\"Distributed Rendezvous Control of Networked Uncertain Robotic Systems with Bearing Measurements\",\"739\":\"Multi-Parameter Optimization for a Robust RGB-D SLAM System\",\"740\":\"Invariant EKF based 2D Active SLAM with Exploration Task\",\"741\":\"MRPB 1.0: A Unified Benchmark for the Evaluation of Mobile Robot Local Planning Approaches\",\"742\":\"Belief Space Partitioning for Symbolic Motion Planning\",\"743\":\"Anticipatory Planning and Dynamic Lost Person Models for Human-Robot Search and Rescue\",\"744\":\"IMU Data Processing For Inertial Aided Navigation: A Recurrent Neural Network Based Approach\",\"745\":\"Highly Efficient Line Segment Tracking with an IMU-KLT Prediction and a Convex Geometric Distance Minimization\",\"746\":\"Robust localization for planar moving robot in changing environment: A perspective on density of correspondence and depth\",\"747\":\"IMU\\/Vehicle Calibration and Integrated Localization for Autonomous Driving\",\"748\":\"Developing of A Rigid-Compliant Finger Joint Exoskeleton Using Topology Optimization Method\",\"749\":\"SpringExo, a spring-based exoskeleton for providing knee assistance: Design, Characterization and Feasibility Study\",\"750\":\"Robust Motion Averaging under Maximum Correntropy Criterion\",\"751\":\"Robust Semantic Map Matching Algorithm Based on Probabilistic Registration Model\",\"752\":\"Accurate and Robust Scale Recovery for Monocular Visual Odometry Based on Plane Geometry\",\"753\":\"Viko: An Adaptive Gecko Gripper with Vision-based Tactile Sensor\",\"754\":\"POIS: Policy-Oriented Instance Segmentation for Ambidextrous Robot Picking\",\"755\":\"Thrust Enhancement of Wave-driven Unmanned Surface Vehicle by using Asymmetric Foil\",\"756\":\"Conquering Textureless with RF-referenced Monocular Vision for MAV State Estimation\",\"757\":\"Control of an Aerial Manipulator Using a Quadrotor with a Replaceable Robotic Arm\",\"758\":\"A Real-Time Multi-Task Framework for Guidewire Segmentation and Endpoint Localization in Endovascular Interventions\",\"759\":\"Towards Adjoint Sensing and Acting Schemes and Interleaving Task Planning for Robust Robot Plan\",\"760\":\"Autonomous Multi-View Navigation via Deep Reinforcement Learning\",\"761\":\"A Multi-spectral Dataset for Evaluating Motion Estimation Systems\",\"762\":\"PicoVO: A Lightweight RGB-D Visual Odometry Targeting Resource-Constrained IoT Devices\",\"763\":\"3D Surfel Map-Aided Visual Relocalization with Learned Descriptors\",\"764\":\"Vision-based Path Following of Snake-like Robots\",\"765\":\"Configuration Transformation of the Wheel-Legged Robot Using Inverse Dynamics Control\",\"766\":\"A Passive Hydraulic Auxiliary System Designed for Increasing Legged Robot Payload and Efficiency\",\"767\":\"Legged Robot State Estimation in Slippery Environments Using Invariant Extended Kalman Filter with Velocity Update\",\"768\":\"Multi-target Coverage with Connectivity Maintenance using Knowledge-incorporated Policy Framework\",\"769\":\"SMMR-Explore: SubMap-based Multi-Robot Exploration System with Multi-robot Multi-target Potential Field Exploration Method\",\"770\":\"Multi-objective Conflict-based Search for Multi-agent Path Finding\",\"771\":\"Remote-Center-of-Motion Recommendation toward Brain Needle Intervention Using Deep Reinforcement Learning\",\"772\":\"Autonomous Navigation of an Ultrasound Probe Towards Standard Scan Planes with Deep Reinforcement Learning\",\"773\":\"A Knowledge-Based Fast Motion Planning Method Through Online Environmental Feature Learning\",\"774\":\"Modeling and Control of an Untethered Magnetic Gripper\",\"775\":\"A Flexible Magnetic Field Mapping Model For Calibration of Magnetic Manipulation System\",\"776\":\"Dynamic tracking for microrobot with active magnetic sensor array\",\"777\":\"Design and Experimental Validation of a Robotic System for Reactor Core Detector Removal\",\"778\":\"Accurate and Robust Stereo Direct Visual Odometry for Agricultural Environment\",\"779\":\"A Cascaded LiDAR-Camera Fusion Network for Road Detection\",\"780\":\"Design and Analysis of a Novel Lightweight, Versatile Soft-rigid Robot\",\"781\":\"Kinetostatics for variable cross-section continuum manipulators\",\"782\":\"Numerical Simulation of an Untethered Omni-Directional Star-Shaped Swimming Robot\",\"783\":\"NeuralSim: Augmenting Differentiable Simulators with Neural Networks\",\"784\":\"Continuous Transition: Improving Sample Efficiency for Continuous Control Problems via MixUp\",\"785\":\"Effective Crash Recovery of Robot Software Programs in ROS\",\"786\":\"Point Set Registration With Semantic Region Association Using Cascaded Expectation Maximization\",\"787\":\"A Flexible and Efficient Loop Closure Detection Based on Motion Knowledge\",\"788\":\"A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving\",\"789\":\"Visual Semantic Localization based on HD Map for Autonomous Vehicles in Urban Scenarios\",\"790\":\"Hybrid Bird\\u2019s-Eye Edge Based Semantic Visual SLAM for Automated Valet Parking\",\"791\":\"Collaborative Visual Inertial SLAM for Multiple Smart Phones\",\"792\":\"MS: A New Exact Algorithm for Multi-agent Simultaneous Multi-goal Sequencing and Path Finding\",\"793\":\"Inertial Aided 3D LiDAR SLAM with Hybrid Geometric Primitives in Large-scale Environments\",\"794\":\"Prediction-Based Reachability for Collision Avoidance in Autonomous Driving\",\"795\":\"Lane-free Autonomous Intersection Management: A Batch-processing Framework Integrating Reservation-based and Planning-based Methods\",\"796\":\"Pheromone-Diffusion-based Conscientious Reactive Path Planning for Road Network Persistent Surveillance\",\"797\":\"MDANet: Multi-Modal Deep Aggregation Network for Depth Completion\",\"798\":\"GPR: Grasp Pose Refinement Network for Cluttered Scenes\",\"799\":\"Contour Primitive of Interest Extraction Network Based on One-Shot Learning for Object-Agnostic Vision Measurement\",\"800\":\"Magnetically-Connected Modular Reconfigurable Mini-robotic System with Bilateral Isokinematic Mapping and Fast On-site Assembly towards Minimally Invasive Procedures\",\"801\":\"Reinforcement Learning Control of A Novel Magnetic Actuated Flexible-joint Robotic Camera System for Single Incision Laparoscopic Surgery\",\"802\":\"Muscular stimulation based biological actuator from locust\\u2019s hindleg\",\"803\":\"An Efficient Parallel Self-assembly Planning Algorithm for Modular Robots in Environments with Obstacles\",\"804\":\"Multi-robot Informative Path Planning using a Leader-Follower Architecture\",\"805\":\"Tightly-Coupled Perception and Navigation of Heterogeneous Land-Air Robots in Complex Scenarios\",\"806\":\"Proactive Action Visual Residual Reinforcement Learning for Contact-Rich Tasks Using a Torque-Controlled Robot\",\"807\":\"ParametricNet: 6DoF Pose Estimation Network for Parametric Shapes in Stacked Scenarios\",\"808\":\"Optimal Online Dispatch for High-Capacity Shared Autonomous Mobility-on-Demand Systems\",\"809\":\"An Improved Magnetic Spot Navigation for Replacing the Barcode Navigation in Automated Guided Vehicles\",\"810\":\"ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking\",\"811\":\"Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label\",\"812\":\"Siamese Anchor Proposal Network for High-Speed Aerial Tracking\",\"813\":\"Modeling Affect-based Intrinsic Rewards for Exploration and Learning\",\"814\":\"A Multi-Level Network for Human Pose Estimation\",\"815\":\"Open-set Intersection Intention Prediction for Autonomous Driving\",\"816\":\"A general elimination strategy for camera motion estimation\",\"817\":\"Group Feature Learning and Domain Adversarial Neural Network for aMCI Diagnosis System Based on EEG\",\"818\":\"Line-based Automatic Extrinsic Calibration of LiDAR and Camera\",\"819\":\"RIL: Riemannian Incremental Learning of the Inertial Properties of the Robot Body Schema\",\"820\":\"Two-stream 2D\\/3D Residual Networks for Learning Robot Manipulations from Human Demonstration Videos\",\"821\":\"Waypoints updating based on Adam and ILC for path learning in physical human-robot interaction\",\"822\":\"A Graph Attention Spatio-temporal Convolutional Network for 3D Human Pose Estimation in Video\",\"823\":\"Micro Robotic Manipulation System for the Force Stimulation of Muscle Fiber-like Cell Structure\",\"824\":\"A Versatile Vision-Pheromone-Communication Platform for Swarm Robotics\",\"825\":\"3D Periodic Magnetic Servoing System for Microrobot Actuation Using Decoupled Asynchronous Repetitive Control Approach\",\"826\":\"Efficient Heuristic Generation for Robot Path Planning with Recurrent Generative Model\",\"827\":\"Scalable Coverage Path Planning of Multi-Robot Teams for Monitoring Non-Convex Areas\",\"828\":\"Time and Energy Optimized Trajectory Generation for Multi-Agent Constellation Changes\",\"829\":\"Towards an Online RRT-based Path Planning Algorithm for Ackermann-steering Vehicles\",\"830\":\"Three-dimensional Positioning of the Micropipette for Intracytoplasmic Sperm Injection\",\"831\":\"Robotic Cardinal Vein Microinjection of Zebrafish Larvae Based on 3D Positioning\",\"832\":\"A Bipolar Myoelectric Sensor-Enabled Human-Machine Interface Based On Spinal Module Activations\",\"833\":\"Enhancement for Robustness of Koopman Operator-based Data-driven Mobile Robotic Systems\",\"834\":\"Collision Risk Assessment and Obstacle Avoidance Control for Autonomous Sailing Robots\",\"835\":\"MSTC\\u2217:Multi-robot Coverage Path Planning under Physical Constrain\",\"836\":\"Impact Mitigation for Dynamic Legged Robots with Steel Wire Transmission Using Nonlinear Active Compliance Control\",\"837\":\"Robust Improvement in 3D Object Landmark Inference for Semantic Mapping\",\"838\":\"YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection\",\"839\":\"Sliding Mode Control of the Semi-active Hover Backpack Based on the Bioinspired Skyhook Damper Model\",\"840\":\"Fast Light Show Design Platform for K-12 Children\",\"841\":\"Autonomous Overtaking in Gran Turismo Sport Using Curriculum Reinforcement Learning\",\"842\":\"An MR Safe Rotary Encoder Based on Eccentric Sheave and FBG Sensors\",\"843\":\"Elastic and Efficient LiDAR Reconstruction for Large-Scale Exploration Tasks\",\"844\":\"KFS-LIO: Key-Feature Selection for Lightweight Lidar Inertial Odometry\",\"845\":\"CamVox: A Low-cost and Accurate Lidar-assisted Visual SLAM System\",\"846\":\"PSF-LO: Parameterized Semantic Features Based Lidar Odometry\",\"847\":\"Relational Navigation Learning in Continuous Action Space among Crowds\",\"848\":\"Limits of Probabilistic Safety Guarantees when Considering Human Uncertainty\",\"849\":\"Probabilistic Human Motion Prediction via A Bayesian Neural Network\",\"850\":\"Directed Acyclic Graph Neural Network for Human Motion Prediction\",\"851\":\"Tracking Partially-Occluded Deformable Objects while Enforcing Geometric Constraints\",\"852\":\"Online Recommendation-based Convolutional Features for Scale-Aware Visual Tracking\",\"853\":\"Exploiting Probabilistic Siamese Visual Tracking with a Conditional Variational Autoencoder\",\"854\":\"Toward intraoperative endomicroscopy with a GPU-accelerated deformable video mosaicking algorithm\",\"855\":\"Cutting Depth Compensation Based on Milling Acoustic Signal for Robotic-Assisted Laminectomy\",\"856\":\"UMLE: Unsupervised Multi-discriminator Network for Low Light Enhancement\",\"857\":\"Unsupervised Learning of 3D Scene Flow from Monocular Camera\",\"858\":\"Deep3DRanker: A Novel Framework for Learning to Rank 3D Models with Self-Attention in Robotic Vision\",\"859\":\"FGR: Frustum-Aware Geometric Reasoning for Weakly Supervised 3D Vehicle Detection\",\"860\":\"Towards Collision Detection, Localization and Force Estimation for a Soft Cable-driven Robot Manipulator\",\"861\":\"Kinematic analysis of a flexible surgical instrument for robot-assisted minimally invasive surgery\",\"862\":\"ENCODE: a dEep poiNt Cloud ODometry nEtwork\",\"863\":\"CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth\",\"864\":\"Lifelong Localization in Semi-Dynamic Environment\",\"865\":\"Deep Online Correction for Monocular Visual Odometry\",\"866\":\"Direct Sparse Stereo Visual-Inertial Global Odometry\",\"867\":\"Adversarially-trained Hierarchical Feature Extractor for Vehicle Re-identification\",\"868\":\"VIC-Net: Voxelization Information Compensation Network for Point Cloud 3D Object Detection\",\"869\":\"Semantic Reinforced Attention Learning for Visual Place Recognition\",\"870\":\"Towards Efficient Multiview Object Detection with Adaptive Action Prediction\",\"871\":\"Learning a Geometric Representation for Data-Efficient Depth Estimation via Gradient Field and Contrastive Loss\",\"872\":\"Stereo-augmented Depth Completion from a Single RGB-LiDAR image\",\"873\":\"PENet: Towards Precise and Efficient Image Guided Depth Completion\",\"874\":\"Probabilistic 3D Multi-Modal, Multi-Object Tracking for Autonomous Driving\",\"875\":\"AVGCN: Trajectory Prediction using Graph Convolutional Networks Guided by Human Attention\",\"876\":\"Attentional-GCNN: Adaptive Pedestrian Trajectory Prediction towards Generic Autonomous Vehicle Use Cases\",\"877\":\"Spatial Graph Regularized Multi-kernel Subtask Cross-correlation Tracker\",\"878\":\"A Large-Scale Dataset for Benchmarking Elevator Button Segmentation and Character Recognition\",\"879\":\"Neighborhood Spatial Aggregation based Efficient Uncertainty Estimation for Point Cloud Semantic Segmentation\",\"880\":\"S3Net: 3D LiDAR Sparse Semantic Segmentation Network\",\"881\":\"VID-Fusion: Robust Visual-Inertial-Dynamics Odometry for Accurate External Force Estimation\",\"882\":\"LIRO: Tightly Coupled Lidar-Inertia-Ranging Odometry\",\"883\":\"Verbal Focus-of-Attention System for Learning-from-Observation\",\"884\":\"Hybrid Model Control of WalkON Suit for Precise and Robust Gait Assistance of Paraplegics\",\"885\":\"A Novel Gait Phase Detection Algorithm for Foot Drop Correction through Optimal Hybrid FES-Orthosis Assistance\",\"886\":\"Rapid Pose Label Generation through Sparse Representation of Unknown Objects\",\"887\":\"Learning to Predict Repeatability of Interest Points\",\"888\":\"DRACO: Weakly Supervised Dense Reconstruction And Canonicalization of Objects\",\"889\":\"FastFlowNet: A Lightweight Network for Fast Optical Flow Estimation\",\"890\":\"Joint Representation of Temporal Image Sequences and Object Motion for Video Object Detection\",\"891\":\"Targetless Multiple Camera-LiDAR Extrinsic Calibration using Object Pose Estimation\",\"892\":\"Toward a Unified Framework for Point Set Registration\",\"893\":\"Robot Motion Control with Compressive Feedback\",\"894\":\"MFPN-6D : Real-time One-stage Pose Estimation of Objects on RGB Images\",\"895\":\"A Novel Tactile Feedback System with On-Line Texture Decoding and Direct-Texture-Feedback\",\"896\":\"PLG-IN: Pluggable Geometric Consistency Loss with Wasserstein Distance in Monocular Depth Estimation\",\"897\":\"Real-Time Mesh Extraction from Implicit Functions via Direct Reconstruction of Decision Boundary\",\"898\":\"Uncertainty-Aware Fast Curb Detection Using Convolutional Networks in Point Clouds\",\"899\":\"OCR-based Inventory Management Algorithms Robust to Damaged Images\",\"900\":\"Learning Domain Adaptation with Model Calibration for Surgical Report Generation in Robotic Surgery\",\"901\":\"Data-driven Holistic Framework for Automated Laparoscope Optimal View Control with Learning-based Depth Perception\",\"902\":\"Deep Reinforcement Learning Framework for Underwater Locomotion of Soft Robot\",\"903\":\"A Parallelized Iterative Algorithm for Real-Time Simulation of Long Flexible Cable Manipulation\",\"904\":\"An Autonomous Robotic Flexible Endoscope System with a DNA-inspired Continuum Mechanism\",\"905\":\"Amplifying Laminar Jamming for Soft Robots by Geometry-Induced Rigidity\",\"906\":\"Towards a Multi-imager Compatible Continuum Robot with Improved Dynamics Driven by Modular SMA\",\"907\":\"Optimization-Based Visual-Inertial SLAM Tightly Coupled with Raw GNSS Measurements\",\"908\":\"LiTAMIN2: Ultra Light LiDAR-based SLAM using Geometric Approximation applied with KL-Divergence\",\"909\":\"Compositional and Scalable Object SLAM\",\"910\":\"A Robotic Defect Inspection System for Free-form Specular Surfaces\",\"911\":\"Serverless Architecture for Service Robot Management System\",\"912\":\"Can Non-Humanoid Social Robots Reduce Workload of Special Educators : An Online and In-Premises Field Study\",\"913\":\"Heart Position Estimation based on Bone Distribution toward Autonomous Robotic Fetal Ultrasonography\",\"914\":\"Chip-Less Wireless Sensing of Kirigami Structural Morphing Under Various Mechanical Stimuli Using Home-Based Ink-Jet Printable Materials\",\"915\":\"A 2-Dimensional Branch-and-Bound Algorithm for Hand-Eye Self-Calibration of SCARA Robots\",\"916\":\"Long-term Multiple Time-Constant Model of a Spring Roll Dielectric Elastomer Actuator under Dynamic Loading\",\"917\":\"A Wheeled V-shaped In-Pipe Robot with Clutched Underactuated Joints\",\"918\":\"Walking Trajectory Design of Hydraulic Legged Robot with Limited Powered Pump\",\"919\":\"Robotic Guide Dog: Leading a Human with Leash-Guided Hybrid Physical Interaction\",\"920\":\"An Overconstrained Robotic Leg with Coaxial Quasi-direct Drives for Omni-directional Ground Mobility\",\"921\":\"Towards Real-time Semantic RGB-D SLAM in Dynamic Environments\",\"922\":\"Real-time Robot Path Planning using Rapid Visible Tree\",\"923\":\"Semantically Guided Multi-View Stereo for Dense 3D Road Mapping\",\"924\":\"Spatial Reasoning from Natural Language Instructions for Robot Manipulation\",\"925\":\"Robust 360-8PA: Redesigning The Normalized 8-point Algorithm for 360-FoV Images\",\"926\":\"Initialisation of Autonomous Aircraft Visual Inspection Systems via CNN-Based Camera Pose Estimation\",\"927\":\"Voxelized GICP for Fast and Accurate 3D Point Cloud Registration\",\"928\":\"Robust Navigation for Racing Drones based on Imitation Learning and Modularization\",\"929\":\"Learning Interpretable End-to-End Vision-Based Motion Planning for Autonomous Driving with Optical Flow Distillation\",\"930\":\"Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad Weather\",\"931\":\"Referring Image Segmentation via Language-Driven Attention\",\"932\":\"GPU-Efficient Dense Convolutional Network for Real-time Semantic Segmentation\",\"933\":\"Feature Enhanced Projection Network for Zero-shot Semantic Segmentation\",\"934\":\"Learning Optical Flow with R-CNN for Visual Odometry\",\"935\":\"A Normal Distribution Transform-Based Radar Odometry Designed For Scanning and Automotive Radars\",\"936\":\"An Equivariant Filter for Visual Inertial Odometry\",\"937\":\"A Tactile Sensing Foot for Single Robot Leg Stabilization\",\"938\":\"Looking Farther in Parametric Scene Parsing with Ground and Aerial Imagery\",\"939\":\"Fast Motion Understanding with Spatiotemporal Neural Networks and Dynamic Vision Sensors\",\"940\":\"Design and Modeling of a Biomimetic Gastropod-like Soft Robot with Wet Adhesive Locomotion\",\"941\":\"Nonlinear Disturbance Observer-based Robust Motion Control for Multi-joint Series Elastic Actuator-driven Robots\",\"942\":\"Biomimetic Control of Myoelectric Prosthetic Hand Based on a Lambda-type Muscle Model\",\"943\":\"Context-Aware Safe Reinforcement Learning for Non-Stationary Environments\",\"944\":\"Quantification of Joint Redundancy considering Dynamic Feasibility using Deep Reinforcement Learning\",\"945\":\"Deep Balanced Learning for Long-tailed Facial Expressions Recognition\",\"946\":\"AU-Expression Knowledge Constrained Representation Learning for Facial Expression Recognition\",\"947\":\"Covariance Self-Attention Dual Path UNet for Rectal Tumor Segmentation\",\"948\":\"Fabric defect detection using tactile information\",\"949\":\"Real-time 3D-Lidar, MMW Radar and GPS\\/IMU fusion based vehicle detection and tracking in unstructured environment\",\"950\":\"Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery\",\"951\":\"City-scale Scene Change Detection using Point Clouds\",\"952\":\"Visual Place Recognition via Local Affine Preserving Matching\",\"953\":\"View-expansive Microscope System with Real-time High-resolution Imaging for Simplified Microinjection Experiments\",\"954\":\"Leveraging Enhanced Virtual Reality Methods and Environments for Efficient, Intuitive, and Immersive Teleoperation of Robots\",\"955\":\"End-to-end Multi-Instance Robotic Reaching from Monocular Vision\",\"956\":\"FG-Conv: Large-Scale LiDAR Point Clouds Understanding Leveraging Feature Correlation Mining and Geometric-Aware Modeling\",\"957\":\"Exploiting Local Geometry for Feature and Graph Construction for Better 3D Point Cloud Processing with Graph Neural Networks\",\"958\":\"Stereo Object Matching Network\",\"959\":\"Operational Space Control for Planar PAN\\u20131 Underactuated Manipulators Using Orthogonal Projection and Quadratic Programming\",\"960\":\"A Stable Control Strategy for Industrial Robots with External Feedback Loop\",\"961\":\"Operational Space Control Under Actuator Bandwidth Limitation\",\"962\":\"Design and Implementation of a Novel, Intrinsically Safe Rigid-Flexible Coupling Manipulator for COVID-19 Oropharyngeal Swab Sampling\",\"963\":\"Design and Control of Fully Handheld Microsurgical Robot for Active Tremor Cancellation\",\"964\":\"Design and Experiment of a Pneumatic Soft Climbing Robot\",\"965\":\"Surface Robots based on S-Isothermic Surfaces\",\"966\":\"Origami-Inspired Snap-through Bistability in Parallel and Curved Mechanisms Through the Inflection of Degree Four Vertexes\",\"967\":\"Markov Parallel Tracking and Mapping for Probabilistic SLAM\",\"968\":\"Multi-session Underwater Pose-graph SLAM using Inter-session Opti-acoustic Two-view Factor\",\"969\":\"Avoiding Degeneracy for Monocular Visual SLAM with Point and Line Features\",\"970\":\"Not your grandmother\\u2019s toolbox \\u2013 the Robotics Toolbox reinvented for Python\",\"971\":\"Multiple-Place Swarm Foraging with Dynamic Robot Chains\",\"972\":\"Collaborative Learning of Multiple-Discontinuous-Image Saliency Prediction for Drone Exploration\",\"973\":\"Positioning Control for Underactuated Unmanned Surface Vehicles to Resist Environmental Disturbances\",\"974\":\"S2P2: Self-Supervised Goal-Directed Path Planning Using RGB-D Data for Robotic Wheelchairs\",\"975\":\"Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots\",\"976\":\"Route Coverage Testing for Autonomous Vehicles via Map Modeling\",\"977\":\"A data-set and a method for pointing direction estimation from depth images for human-robot interaction and VR applications\",\"978\":\"Extendable Navigation Network based Reinforcement Learning for Indoor Robot Exploration\",\"979\":\"Extrinsic Contact Sensing with Relative-Motion Tracking from Distributed Tactile Measurements\",\"980\":\"Robotic Imitation of Human Assembly Skills Using Hybrid Trajectory and Force Learning\",\"981\":\"Applications: Twisted String Actuation-based Compact Automatic Transmission\",\"982\":\"Power Transmission Design of Fast and Energy-Efficient Stiffness Modulation for Human Power Assistance\",\"983\":\"A Novel Variable Resolution Torque Sensor Based on Variable Stiffness Principle\",\"984\":\"VINSEval: Evaluation Framework for Unified Testing of Consistency and Robustness of Visual-Inertial Navigation System Algorithms\",\"985\":\"An Event-based Vision Dataset for Visual Navigation Tasks in Agricultural Environments\",\"986\":\"Towards In-Field Phenotyping Exploiting Differentiable Rendering with Self-Consistency Loss\",\"987\":\"Efficient Haptic Rendering of Regolith\",\"988\":\"Parameterizable and Jerk-Limited Trajectories with Blending for Robot Motion Planning and Spherical Cartesian Waypoints\",\"989\":\"Learning Camera Performance Models for Active Multi-Camera Visual Teach and Repeat\",\"990\":\"MS-RANAS: Multi-Scale Resource-Aware Neural Architecture Search\",\"991\":\"Vision-Based Mobile Robotics Obstacle Avoidance With Deep Reinforcement Learning\",\"992\":\"OpenBot: Turning Smartphones into Robots\",\"993\":\"Quasi-LPV Unknown Input Observer with Nonlinear Outputs: Application to Motorcycles\",\"994\":\"A Novel Torsional Actuator Augmenting Twisting Skeleton and Artificial Muscle for Robots in Extreme Environments\",\"995\":\"Simple But Effective Redundant Odometry for Autonomous Vehicles\",\"996\":\"Markov Localisation using Heatmap Regression and Deep Convolutional Odometry\",\"997\":\"Autonomous Cooperative Visual Navigation for Planetary Exploration Robots\",\"998\":\"Minimum-Effort Task-based Design Optimization of Modular Reconfigurable Robots\",\"999\":\"Computational design of energy-efficient legged robots: Optimizing for size and actuators\",\"1000\":\"On the Effect of Robotic Leg Design on Energy Efficiency\",\"1001\":\"Shared Control of Robot-Robot Collaborative Lifting with Agent Postural and Force Ergonomic Optimization\",\"1002\":\"Rapidly adapting robot swarms with Swarm Map-based Bayesian Optimisation\",\"1003\":\"Probabilistic Scan Matching: Bayesian Pose Estimation from Point Clouds\",\"1004\":\"Reinforcement Learning for Orientation Estimation Using Inertial Sensors with Performance Guarantee\",\"1005\":\"Consistent State Estimation on Manifolds for Autonomous Metal Structure Inspection\",\"1006\":\"Binary-LoRAX: Low-Latency Runtime Adaptable XNOR Classifier for Semi-Autonomous Grasping with Prosthetic Hands\",\"1007\":\"Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes\",\"1008\":\"Residual Squeeze-and-Excitation Network with Multi-scale Spatial Pyramid Module for Fast Robotic Grasping Detection\",\"1009\":\"End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB\",\"1010\":\"Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer\",\"1011\":\"There and Back Again: Learning to Simulate Radar Data for Real-World Applications\",\"1012\":\"Productive Multitasking for Industrial Robots\",\"1013\":\"Automated Planning of Workcell Layouts Considering Task Sequences\",\"1014\":\"Motion-Aware Robotic 3D Ultrasound\",\"1015\":\"A 3D Printed Mechanical Model of the Knee to Detect and Avoid Total Knee Replacement Surgery Errors\",\"1016\":\"Detecting blindspots in colonoscopy by modelling curvature\",\"1017\":\"Out-of-Plane Corrections for Autonomous Robotic Breast Ultrasound Acquisitions\",\"1018\":\"Highly Manoeuvrable Eversion Robot Based on Fusion of Function with Structure\",\"1019\":\"A Novel Hybrid Approach for Fault-Tolerant Control of UAVs based on Robust Reinforcement Learning\",\"1020\":\"Using Reinforcement Learning to Create Control Barrier Functions for Explicit Risk Mitigation in Adversarial Environments\",\"1021\":\"EagerMOT: 3D Multi-Object Tracking via Sensor Fusion\",\"1022\":\"Faster R-CNN-based Decision Making in a Novel Adaptive Dual-Mode Robotic Anchoring System\",\"1023\":\"Robot Action Diagnosis and Experience Correction by Falsifying Parameterised Execution Models\",\"1024\":\"TT-SLAM: Dense Monocular SLAM for Planar Environments\",\"1025\":\"DOT: Dynamic Object Tracking for Visual SLAM\",\"1026\":\"Optimizing Keypoint-based Single-Shot Camera-to-Robot Pose Estimation through Shape Segmentation\",\"1027\":\"Handling Object Symmetries in CNN-based Pose Estimation\",\"1028\":\"Efficient and Robust Orientation Estimation of Strawberries for Fruit Picking Applications\",\"1029\":\"Probabilistic Terrain Estimation for Autonomous Off-Road Driving\",\"1030\":\"Tracking 6-DoF Object Motion from Events and Frames\",\"1031\":\"Visual Tracking of Deforming Objects Using Physics-based Models\",\"1032\":\"Deep 6-DoF Tracking of Unknown Objects for Reactive Grasping\",\"1033\":\"TSDF++: A Multi-Object Formulation for Dynamic Object Tracking and Reconstruction\",\"1034\":\"Subsequent Keyframe Generation for Visual Servoing\",\"1035\":\"Siame-se(3): regression in se(3) for end-to-end visual servoing\",\"1036\":\"Formal Verification of ROS Based Systems Using a Linear Logic Theorem Prover\",\"1037\":\"Fuzz Testing in Behavior-Based Robotics\",\"1038\":\"The Robot Household Marathon Experiment\",\"1039\":\"ProbRobScene: A Probabilistic Specification Language for 3D Robotic Manipulation Environments\",\"1040\":\"Towards Real-Time Interaction with Industrial Robots in the Creative Industries\",\"1041\":\"Automated acquisition of structured, semantic models of manipulation activities from human VR demonstration\",\"1042\":\"A Shared Control Framework for Robotic Telemanipulation Combining Electromyography Based Motion Estimation and Compliance Control\",\"1043\":\"Optimal TCP and Robot Base Placement for a Set of Complex Continuous Paths\",\"1044\":\"Control-Tree Optimization: an approach to MPC under discrete Partial Observability\",\"1045\":\"Leveraging Neural Network Gradients within Trajectory Optimization for Proactive Human-Robot Interactions\",\"1046\":\"Gramian-based optimal active sensing control under intermittent measurements\",\"1047\":\"Receding-Horizon Perceptive Trajectory Optimization for Dynamic Legged Locomotion with Learned Initialization\",\"1048\":\"Friction-driven Three-foot Robot Inspired by Snail Movement\",\"1049\":\"Modeling and Optimal Control for Rope-Assisted Rappelling Maneuvers\",\"1050\":\"Path Planning for a Reconfigurable Robot in Extreme Environments\",\"1051\":\"AM-RRT: Informed Sampling-based Planning with Assisting Metric\",\"1052\":\"Complete Path Planning That Simultaneously Optimizes Length and Clearance\",\"1053\":\"CABiNet: Efficient Context Aggregation Network for Low-Latency Semantic Segmentation\",\"1054\":\"Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis\",\"1055\":\"Plane Segmentation in Organized Point Clouds using Flood Fill\",\"1056\":\"Semantic Feature Mining for 3D Object Classification and Segmentation\",\"1057\":\"VelocityNet: Motion-Driven Feature Aggregation for 3D Object Detection in Point Cloud Sequences\",\"1058\":\"Don\\u2019t Blindly Trust Your CNN: Towards Competency-Aware Object Detection by Evaluating Novelty in Open-Ended Environments\",\"1059\":\"What My Motion tells me about Your Pose: A Self-Supervised Monocular 3D Vehicle Detector\",\"1060\":\"Self-Supervised Person Detection in 2D Range Data using a Calibrated Camera\",\"1061\":\"Trajectory Optimisation in Learned Multimodal Dynamical Systems via Latent-ODE Collocation\",\"1062\":\"Inverse Dynamics vs. Forward Dynamics in Direct Transcription Formulations for Trajectory Optimization\",\"1063\":\"Coupled Mobile Manipulation via Trajectory Optimization with Free Space Decomposition\",\"1064\":\"Grasp Detection for Robot to Human Handovers Using Capacitive Sensors\",\"1065\":\"Design and Development of a Robotic Bioreactor for In Vitro Tissue Engineering\",\"1066\":\"Screw theory-based stiffness analysis for a fluidic-driven soft robotic manipulator\",\"1067\":\"Minimum directed information: A design principle for compliant robots\",\"1068\":\"Model and Validation of a Highly Extensible and Tough Actuator based on a Ballooning Membrane\",\"1069\":\"RGB-D SLAM with Structural Regularities\",\"1070\":\"Generic Hand\\u2013Eye Calibration of Uncertain Robots\",\"1071\":\"In Situ Translational Hand-Eye Calibration of Laser Profile Sensors using Arbitrary Objects\",\"1072\":\"Autonomous UAV Safety by Visual Human Crowd Detection Using Multi-Task Deep Neural Networks\",\"1073\":\"CloudAAE: Learning 6D Object Pose Regression with On-line Data Synthesis on Point Clouds\",\"1074\":\"Reward Conditioned Neural Movement Primitives for Population-Based Variational Policy Optimization\",\"1075\":\"Contextual Latent-Movements Off-Policy Optimization for Robotic Manipulation Skills\",\"1076\":\"AcinoSet: A 3D Pose Estimation Dataset and Baseline Models for Cheetahs in the Wild\",\"1077\":\"PyraPose: Feature Pyramids for Fast and Accurate Object Pose Estimation under Domain Shift\",\"1078\":\"Investigations on Output Parameterizations of Neural Networks for Single Shot 6D Object Pose Estimation\",\"1079\":\"Self-supervised Learning of LiDAR Odometry for Robotic Applications\",\"1080\":\"Tight Integration of Feature-based Relocalization in Monocular Direct Visual Odometry\",\"1081\":\"Fast Few-Shot Classification by Few-Iteration Meta-Learning\",\"1082\":\"Deep Hierarchical Rotation Invariance Learning with Exact Geometry Feature Representation for Point Cloud Classification\",\"1083\":\"Fool Me Once: Robust Selective Segmentation via Out-of-Distribution Detection with Contrastive Learning\",\"1084\":\"Rapid Solution of Cosserat Rod Equations via a Nonlinear Partial Observer\",\"1085\":\"The Effects of Robot Cognitive Reliability and Social Positioning on Child-Robot Team Dynamics\",\"1086\":\"Linear-Quadratic Optimal Control in Maximal Coordinates\",\"1087\":\"Safe and Efficient Model-free Adaptive Control via Bayesian Optimization\",\"1088\":\"Hierarchical and Flexible Traffic Management of Multi-AGV Systems Applied to Industrial Environments\",\"1089\":\"Combining Multi-Robot Motion Planning and Goal Allocation using Roadmaps\",\"1090\":\"Asynchronous Reliability-Aware Multi-UAV Coverage Path Planning\",\"1091\":\"Distributed coordinated path following using guiding vector fields\",\"1092\":\"On Smooth Time-Optimal Trajectory Planning in Twisted String Actuators\",\"1093\":\"Fast Swing-Up Trajectory Optimization for a Spherical Pendulum on a 7-DoF Collaborative Robot\",\"1094\":\"Human-Robot Collaborative Multi-Agent Path Planning using Monte Carlo Tree Search and Social Reward Sources\",\"1095\":\"PlaneSegNet: Fast and Robust Plane Estimation Using a Single-stage Instance Segmentation CNN\",\"1096\":\"Fast Object Segmentation Learning with Kernel-based Methods for Robotics\",\"1097\":\"Diffuser: Multi-View 2D-to-3D Label Diffusion for Semantic Scene Segmentation\",\"1098\":\"A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI\",\"1099\":\"Attentional Learn-able Pooling for Human Activity Recognition\",\"1100\":\"Real-time Instance Detection with Fast Incremental Learning\",\"1101\":\"3D3L: Deep Learned 3D Keypoint Detection and Description for LiDARs\",\"1102\":\"Online Dynamic Trajectory Optimization and Control for a Quadruped Robot\",\"1103\":\"Online DCM Trajectory Adaptation for Push and Stumble Recovery during Humanoid Locomotion\",\"1104\":\"Exploring Dynamic Context for Multi-path Trajectory Prediction\",\"1105\":\"Foot Control of a Surgical Laparoscopic Gripper via 5DoF Haptic Robotic Platform: Design, Dynamics and Haptic Shared Control\",\"1106\":\"Providing Automatic Feedback to Trainees after Automatic Evaluation\",\"1107\":\"A Haptic Mouse Design with Stiffening Muscle Layer for Simulating Guarding in Abdominal Palpation Training\",\"1108\":\"Model-based Design and Digital Implementation to Improve Control of the da Vinci Research Kit Telerobotic Surgical System\",\"1109\":\"Shared control strategy for needle insertion into deformable tissue using inverse Finite Element simulation\",\"1110\":\"An Optimized Two-Layer Approach for Efficient and Robustly Stable Bilateral Teleoperation\",\"1111\":\"Soft Robot Optimal Control Via Reduced Order Finite Element Models\",\"1112\":\"MULLS: Versatile LiDAR SLAM via Multi-metric Linear Least Square\",\"1113\":\"Dynamic Object Aware LiDAR SLAM based on Automatic Generation of Training Data\",\"1114\":\"Connecting Semantic Building Information Models and Robotics: An application to 2D LiDAR-based localization\",\"1115\":\"No Need for Interactions: Robust Model-Based Imitation Learning using Neural ODE\",\"1116\":\"Robust Trajectory Planning with Parametric Uncertainties\",\"1117\":\"LiDAR few-shot domain adaptation via integrated CycleGAN and 3D object detector with joint learning delay\",\"1118\":\"Device Design and System Integration of a Two-Axis Water-immersible Micro Scanning Mirror (WIMSM) to Enable Dual-modal Optical and Acoustic Communication and Ranging for Underwater Vehicles\",\"1119\":\"Vanishing Point Aided LiDAR-Visual-Inertial Estimator\",\"1120\":\"Learning robust driving policies without online exploration\",\"1121\":\"SSCNav: Confidence-Aware Semantic Scene Completion for Visual Semantic Navigation\",\"1122\":\"Ego-centric Stereo Navigation Using Stixel World\",\"1123\":\"PyTouch: A Machine Learning Library for Touch Processing\",\"1124\":\"Contingencies from Observations: Tractable Contingency Planning with Learned Behavior Models\",\"1125\":\"ScrewNet: Category-Independent Articulation Model Estimation From Depth Images Using Screw Theory\",\"1126\":\"Visual Perspective Taking for Opponent Behavior Modeling\",\"1127\":\"Learning Tactile Models for Factor Graph-based Estimation\",\"1128\":\"Double-Prong ConvLSTM for Spatiotemporal Occupancy Prediction in Dynamic Environments\",\"1129\":\"Social-STAGE: Spatio-Temporal Multi-Modal Future Trajectory Forecast\",\"1130\":\"Efficient Map Prediction via Low-Rank Matrix Completion\",\"1131\":\"Constrained Image-Based Visual Servoing using Barrier Functions\",\"1132\":\"Deep Learning-Based Photoacoustic Visual Servoing: Using Outputs from Raw Sensor Data as Inputs to a Robot Controller\",\"1133\":\"Analyzing Neural Jacobian Methods in Applications of Visual Servoing and Kinematic Control\",\"1134\":\"ALTRO-C: A Fast Solver for Conic Model-Predictive Control\",\"1135\":\"Model Predictive Control of Nonlinear Latent Force Models: A Scenario-Based Approach\",\"1136\":\"The Value of Planning for Infinite-Horizon Model Predictive Control\",\"1137\":\"Automatic Tuning for Data-driven Model Predictive Control\",\"1138\":\"DESERTS: DElay-tolerant SEmi-autonomous Robot Teleoperation for Surgery\",\"1139\":\"Discrete Time Delay Feedback Control of Stewart Platform with Intelligent Optimizer Weight Tuner\",\"1140\":\"The Effect of Input Signals Time-Delay on Stabilizing Traffic with Autonomous Vehicles\",\"1141\":\"A Confidence-Based Supervised-Autonomous Control Strategy for Robotic Vaginal Cuff Closure\",\"1142\":\"Design and Control of 5-DoF Robotically Steerable Catheter for the Delivery of the Mitral Valve Implant\",\"1143\":\"A Robotic System for Implant Modification in Single-stage Cranioplasty\",\"1144\":\"Dispersion-Minimizing Motion Primitives for Search-Based Motion Planning\",\"1145\":\"Meta-Adversarial Inverse Reinforcement Learning for Decision-making Tasks\",\"1146\":\"Min-Max Entropy Inverse RL of Multiple Tasks\",\"1147\":\"CAROM - Vehicle Localization and Traffic Scene Reconstruction from Monocular Cameras on Road Infrastructures\",\"1148\":\"A Front-End for Dense Monocular SLAM using a Learned Outlier Mask Prior\",\"1149\":\"HyperMap: Compressed 3D Map for Monocular Camera Registration\",\"1150\":\"Bidirectional Attention Network for Monocular Depth Estimation\",\"1151\":\"Self-Guided Instance-Aware Network for Depth Completion and Enhancement\",\"1152\":\"World-in-the-Loop Simulation for Autonomous Systems Validation\",\"1153\":\"RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer\",\"1154\":\"A Framework for Multisensory Foresight for Embodied Agents\",\"1155\":\"Protective Policy Transfer\",\"1156\":\"A Data-Driven Reinforcement Learning Solution Framework for Optimal and Adaptive Personalization of a Hip Exoskeleton\",\"1157\":\"Drumming Arm: an Upper-limb Prosthetic System to Restore Grip Control for a Transradial Amputee Drummer\",\"1158\":\"FLEXotendon Glove-III: Soft Robotic Hand Rehabilitation Exoskeleton for Spinal Cord Injury\",\"1159\":\"Development of a Series Elastic Elbow Neurological Exam Training Simulator for Lead-pipe Rigidity\",\"1160\":\"Multi-Modal Motion Planning Using Composite Pose Graph Optimization\",\"1161\":\"Asymptotically Optimal Kinodynamic Planning Using Bundles of Edges\",\"1162\":\"CollisionIK: A Per-Instant Pose Optimization Method for Generating Robot Motions with Environment Collision Avoidance\",\"1163\":\"Trajectory Optimization for Manipulation of Deformable Objects: Assembly of Belt Drive Units\",\"1164\":\"Equality Constrained Linear Optimal Control With Factor Graphs\",\"1165\":\"Robust Optimization-based Motion Planning for high-DOF Robots under Sensing Uncertainty\",\"1166\":\"Optimized Coverage Planning for UV Surface Disinfection\",\"1167\":\"Constrained Differential Dynamic Programming Revisited\",\"1168\":\"Multi-Robot Gaussian Process Estimation and Coverage: Deterministic Sequencing Algorithm and Regret Analysis\",\"1169\":\"Multi-agent Receding Horizon Search with Terminal Cost\",\"1170\":\"Shaped Policy Search for Evolutionary Strategies using Waypoints\",\"1171\":\"Multi-Agent Active Search using Realistic Depth-Aware Noise Model\",\"1172\":\"Communication-Aware Multi-robot Coordination with Submodular Maximization\",\"1173\":\"Online Flocking Control of UAVs with Mean-Field Approximation\",\"1174\":\"Proportional and Reachable Cluster Teleoperation of a Distributed Multi-Robot System\",\"1175\":\"Multiplexing Robot Experiments: Theoretical Underpinnings, Conditions for Existence, and Demonstrations\",\"1176\":\"Efficient Multi-Robot Inspection of Row Crops via Kernel Estimation and Region-Based Task Allocation\",\"1177\":\"High-Frequency Nonlinear Model Predictive Control of a Manipulator\",\"1178\":\"Adaptive Nonlinear Model Predictive Control for Autonomous Surface Vessels With Largely Varying Payload\",\"1179\":\"Time-Varying Model Predictive Control for Highly Dynamic Motions of Quadrupedal Robots\",\"1180\":\"Koopman NMPC: Koopman-based Learning and Nonlinear Model Predictive Control of Control-affine Systems\",\"1181\":\"A Visibility Roadmap Sampling Approach for a Multi-Robot Visibility-Based Pursuit-Evasion Problem\",\"1182\":\"Time-Optimal Multi-Quadrotor Trajectory Planning for Pesticide Spraying\",\"1183\":\"Do You See What I See? Coordinating Multiple Aerial Cameras for Robot Cinematography\",\"1184\":\"MIDAS: Multi-agent Interaction-aware Decision-making with Adaptive Strategies for Urban Autonomous Navigation\",\"1185\":\"Bimanual Regrasping for Suture Needles using Reinforcement Learning for Rapid Motion Planning\",\"1186\":\"Dual-Arm Needle Manipulation with the da Vinci\\u00ae Surgical Robot Under Uncertainty\",\"1187\":\"Learning Surgical Motion Pattern from Small Data in Endoscopic Sinus and Skull Base Surgeries\",\"1188\":\"Chance Constrained Simultaneous Path Planning and Task Assignment with Bottleneck Objective\",\"1189\":\"Constrained Path Planning and Guidance in General Wind Fields\",\"1190\":\"LTO: Lazy Trajectory Optimization with Graph-Search Planning for High DOF Robots in Cluttered Environments\",\"1191\":\"Path Optimization for Ground Vehicles in Off-Road Terrain\",\"1192\":\"Robust & Asymptotically Locally Optimal UAV-Trajectory Generation Based on Spline Subdivision\",\"1193\":\"Autonomous Vehicle Motion Planning via Recurrent Spline Optimization\",\"1194\":\"Cooperative Visual-Inertial Odometry\",\"1195\":\"Evaluation of a drone-based camera calibration approach for hard-to-reach cameras\",\"1196\":\"ViNG: Learning Open-World Navigation with Visual Goals\",\"1197\":\"MaAST: Map Attention with Semantic Transformers for Efficient Visual Navigation\",\"1198\":\"A Few Shot Adaptation of Visual Navigation Skills to New Observations using Meta-Learning\",\"1199\":\"Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation\",\"1200\":\"Congestion-aware Multi-agent Trajectory Prediction for Collision Avoidance\",\"1201\":\"3D Multi-Object Tracking using Random Finite Set-based Multiple Measurement Models Filtering (RFS-M3) for Autonomous Vehicles\",\"1202\":\"Joint Object Detection and Multi-Object Tracking with Graph Neural Networks\",\"1203\":\"droidlet: modular, heterogenous, multi-modal agents\",\"1204\":\"Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation\",\"1205\":\"A Self-Supervised Near-to-Far Approach for Terrain-Adaptive Off-Road Autonomous Driving\",\"1206\":\"A Self-supervised Learning System for Object Detection in Videos Using Random Walks on Graphs\",\"1207\":\"Adversarial Differentiable Data Augmentation for Autonomous Systems\",\"1208\":\"Reward Machines for Vision-Based Robotic Manipulation\",\"1209\":\"What Can I Do Here? Learning New Skills by Imagining Visual Affordances\",\"1210\":\"Learning Geometric Reasoning and Control for Long-Horizon Tasks from Visual Input\",\"1211\":\"Simulation of Vision-based Tactile Sensors using Physics based Rendering\",\"1212\":\"AXLE: Computationally-efficient trajectory smoothing using factor graph chains\",\"1213\":\"Computationally-Efficient Roadmap-based Inspection Planning via Incremental Lazy Search\",\"1214\":\"Serverless Multi-Query Motion Planning for Fog Robotics\",\"1215\":\"Composable Geometric Motion Policies using Multi-Task Pullback Bundle Dynamical Systems\",\"1216\":\"Homotopy-Driven Exploration of Human-made Spaces Using Signs\",\"1217\":\"Graph-based Topological Exploration Planning in Large-scale 3D Environments\",\"1218\":\"Recovering Stress Distribution on Deformable Tissue for a Magnetic Actuated Insertable Laparoscopic Surgical Camera\",\"1219\":\"A Novel Robotic System for Ultrasound-guided Peripheral Vascular Localization\",\"1220\":\"Real-to-Sim Registration of Deformable Soft Tissue with Position-Based Dynamics for Surgical Robot Autonomy\",\"1221\":\"Toward Force Estimation in Robot-Assisted Surgery using Deep Learning with Vision and Robot State\",\"1222\":\"Collective Transport of Unconstrained Objects via Implicit Coordination and Adaptive Compliance\",\"1223\":\"Approximate Solutions to a Class of Reachability Games\",\"1224\":\"Reactive Task and Motion Planning under Temporal Logic Specifications\",\"1225\":\"Vision-Based Shape Reconstruction of Soft Continuum Arms Using a Geometric Strain Parametrization\",\"1226\":\"A Dynamics Simulator for Soft Growing Robots\",\"1227\":\"6D Object Pose Estimation with Pairwise Compatible Geometric Features\",\"1228\":\"Robust Skin-Feature Tracking in Free-Hand Video from Smartphone or Robot-Held Camera, to Enable Clinical-Tool Localization and Guidance\",\"1229\":\"From Multi-Target Sensory Coverage to Complete Sensory Coverage: An Optimization-Based Robotic Sensory Coverage Approach\",\"1230\":\"FISAR: Forward Invariant Safe Reinforcement Learning with a Deep Neural Network-Based Optimizer\",\"1231\":\"Coding for Distributed Multi-Agent Reinforcement Learning\",\"1232\":\"Customized Handling of Unintended Interface Operation In Assistive Robots\",\"1233\":\"Multi-Objective Graph Heuristic Search for Terrestrial Robot Design\",\"1234\":\"Factor Graph-Based Trajectory Optimization for a Pneumatically-Actuated Jumping Robot\",\"1235\":\"MO-BBO: Multi-Objective Bilevel Bayesian Optimization for Robot and Behavior Co-Design\",\"1236\":\"Manipulator Task Space Trajectory Tracking with Kinematics and Dynamics Uncertainties\",\"1237\":\"Computing All Solutions to a Discretization-Invariant Formulation for Optimal Mechanism Design\",\"1238\":\"Optimal Multi-Manipulator Arm Placement for Maximal Dexterity during Robotics Surgery\",\"1239\":\"Fast Near-Optimal Heterogeneous Task Allocation via Flow Decomposition\",\"1240\":\"Data-Driven Adaptive Task Allocation for Heterogeneous Multi-Robot Teams Using Robust Control Barrier Functions\",\"1241\":\"Multi-agent Aerial Monitoring of Moving Convoys using Elliptical Orbits\",\"1242\":\"PuzzleBots: Physical Coupling of Robot Swarms\",\"1243\":\"Spatial Intention Maps for Multi-Agent Mobile Manipulation\",\"1244\":\"Flocking-Segregative Swarming Behaviors using Gibbs Random Fields\",\"1245\":\"Multi-Agent Ergodic Coverage in Urban Environments\",\"1246\":\"Multi-view Sensor Fusion by Integrating Model-based Estimation and Graph Learning for Collaborative Object Localization\",\"1247\":\"Deep Multi-view Depth Estimation with Predicted Uncertainty\",\"1248\":\"MultiViewStereoNet: Fast Multi-View Stereo Depth Estimation using Incremental Viewpoint-Compensated Feature Extraction\",\"1249\":\"Scalable Active Information Acquisition for Multi-Robot Systems\",\"1250\":\"MAPS-X: Explainable Multi-Robot Motion Planning via Segmentation\",\"1251\":\"Spatial and Temporal Splitting Heuristics for Multi-Robot Motion Planning\",\"1252\":\"A Primitive-Based Approach to Good Seamanship Path Planning for Autonomous Surface Vessels\",\"1253\":\"A Scavenger Hunt for Service Robots\",\"1254\":\"Exploring Large and Complex Environments Fast and Efficiently\",\"1255\":\"Planning Laser-Forming Folding Motion with Thermal Simulation\",\"1256\":\"Emergent Hand Morphology and Control from Optimizing Robust Grasps of Diverse Objects\",\"1257\":\"Generalizing Object-Centric Task-Axes Controllers using Keypoints\",\"1258\":\"Dynamic Primitives and Optimal Feedback Control for the Manipulation of Complex Objects\",\"1259\":\"Learning Reactive and Predictive Differentiable Controllers for Switching Linear Dynamical Models\",\"1260\":\"The Reachable Set of a Drone: Exploring the Position Isochrones for a Quadcopter\",\"1261\":\"Two-Stage Trajectory Optimization for Flapping Flight with Data-Driven Models\",\"1262\":\"Online Trajectory Optimization for Dynamic Aerial Motions of a Quadruped Robot\",\"1263\":\"A Continuous-Time Approach for 3D Radar-to-Camera Extrinsic Calibration\",\"1264\":\"Auto-calibration Method Using Stop Signs for Urban Autonomous Driving Applications\",\"1265\":\"Efficient Real-Time Inference in Temporal Convolution Networks\",\"1266\":\"F-SIOL-310: A Robotic Dataset and Benchmark for Few-Shot Incremental Object Learning\",\"1267\":\"Deformable Linear Object Prediction Using Locally Linear Latent Dynamics\",\"1268\":\"Visual-Inertial Filtering for Human Walking Quantification\",\"1269\":\"VOLDOR+SLAM: For the times when feature-based or direct methods are not good enough\",\"1270\":\"ROBIN: a Graph-Theoretic Approach to Reject Outliers in Robust Estimation using Invariants\",\"1271\":\"CLIPPER: A Graph-Theoretic Framework for Robust Data Association\",\"1272\":\"Monitoring Fatigue-Induced Changes in Performance during Robot-Mediated Dynamic Movement\",\"1273\":\"Fingertip Pulse-Echo Ultrasound and Optoacoustic Dual-Modal and Dual Sensing Mechanisms Near-Distance Sensor for Ranging and Material Sensing in Robotic Grasping\",\"1274\":\"Development of a Perception System for an Autonomous Surface Vehicle using Monocular Camera, LIDAR, and Marine RADAR\",\"1275\":\"Detecting and Mapping Trees in Unstructured Environments with a Stereo Camera and Pseudo-Lidar\",\"1276\":\"Linear Inverse Problem for Depth Completion with RGB Image and Sparse LIDAR Fusion\",\"1277\":\"Macro-Mini Actuation of Pneumatic Pouches for Soft Wearable Haptic Displays\",\"1278\":\"Artificial Neural Networks to Solve Forward Kinematics of a Wearable Parallel Robot with Semi-rigid Links\",\"1279\":\"Batteries, camera, action! Learning a semantic control space for expressive robot cinematography\",\"1280\":\"Sim-to-Real Learning of All Common Bipedal Gaits via Periodic Reward Composition\",\"1281\":\"Agile Robot Navigation through Hallucinated Learning and Sober Deployment\",\"1282\":\"Neural fidelity warping for efficient robot morphology design\",\"1283\":\"Computational Design and Fabrication of Corrugated Mechanisms from Behavioral Specifications\",\"1284\":\"Human Driven Compliant Transmission Mechanism\",\"1285\":\"Design Paradigms Based on Spring Agonists for Underactuated Robot Hands: Concepts and Application\",\"1286\":\"Multibranch Learning for Angiodysplasia Segmentation with Attention-Guided Networks and Domain Adaptation\",\"1287\":\"Model-Predictive Control of Blood Suction for Surgical Hemostasis using Differentiable Fluid Simulations\",\"1288\":\"Learning Dense Visual Correspondences in Simulation to Smooth and Fold Real Fabrics\",\"1289\":\"StRETcH: a Soft to Resistive Elastic Tactile Hand\",\"1290\":\"An Active Palm Enhances Dexterity of Soft Robotic In-Hand Manipulation\",\"1291\":\"Compensating for Unmodeled Forces using Neural Networks in Soft Manipulator Planning\",\"1292\":\"Towards Robust One-shot Task Execution using Knowledge Graph Embeddings\",\"1293\":\"An Efficient Closed-Form Method for Optimal Hybrid Force-Velocity Control\",\"1294\":\"Sliding on Manifolds: Geometric Attitude Control with Quaternions\",\"1295\":\"Encoding Defensive Driving as a Dynamic Nash Game\",\"1296\":\"Enhancing Safety of Students with Mobile Air Filtration during School Reopening from COVID-19\",\"1297\":\"Probabilistic Safety-Assured Adaptive Merging Control for Autonomous Vehicles\",\"1298\":\"Leveraging Post Hoc Context for Faster Learning in Bandit Settings with Applications in Robot-Assisted Feeding\",\"1299\":\"Task-Invariant Learning of Continuous Joint Kinematics during Steady-State and Transient Ambulation Using Ultrasound Sensing\",\"1300\":\"Kinesthetic feedback improves grasp performance in cable-driven prostheses\",\"1301\":\"Generalized Nonlinear and Finsler Geometry for Robotics\",\"1302\":\"A Fast and Approximate Medial Axis Sampling Technique\",\"1303\":\"Contact-Implicit Trajectory Optimization With Learned Deformable Contacts Using Bilevel Optimization\",\"1304\":\"Energy-optimal Path Planning with Active Flow Perception for Autonomous Underwater Vehicles\",\"1305\":\"Double Meta-Learning for Data Efficient Policy Optimization in Non-Stationary Environments\",\"1306\":\"Adversarial Attacks on Optimization based Planners\",\"1307\":\"TORNADO-Net: mulTiview tOtal vaRiatioN semAntic segmentation with Diamond inceptiOn module\",\"1308\":\"Lite-HDSeg: LiDAR Semantic Segmentation Using Lite Harmonic Dense Convolutions\",\"1309\":\"A Graph-Based Method for Joint Instance Segmentation of Point Clouds and Image Sequences\",\"1310\":\"Affordable Autonomy through Cooperative Sensing and Planning\",\"1311\":\"Reachable Polyhedral Marching (RPM): A Safety Verification Algorithm for Robotic Systems with Deep Neural Network Components\",\"1312\":\"Multi-Robot Dynamical Source Seeking in Unknown Environments\",\"1313\":\"Volumetric Objectives for Multi-Robot Exploration of Three-Dimensional Environments\",\"1314\":\"Flow-FL: Data-Driven Federated Learning for Spatio-Temporal Predictions in Multi-Robot Systems\",\"1315\":\"Non-Monotone Energy-Aware Information Gathering for Heterogeneous Robot Teams\",\"1316\":\"Multi-Robot Distributed Semantic Mapping in Unfamiliar Environments through Online Matching of Learned Representations\",\"1317\":\"Sensor Placement for Globally Optimal Coverage of 3D-Embedded Surfaces\",\"1318\":\"Reachability Analysis for FollowerStopper: Safety Analysis and Experimental Results\",\"1319\":\"Multi-Hypothesis Interactions in Game-Theoretic Motion Planning\",\"1320\":\"An Approximation Algorithm for an Assisted Shortest Path Problem\",\"1321\":\"Projector-Guided Non-Holonomic Mobile 3D Printing\",\"1322\":\"Robot Development and Path Planning for Indoor Ultraviolet Light Disinfection\",\"1323\":\"Piecewise-Linear Motion Planning amidst Static, Moving, or Morphing Obstacles\",\"1324\":\"Smooth Path Planning for Continuum Arms\",\"1325\":\"Anticipatory Path Planning for Continuum Arms in Dynamic Environments\",\"1326\":\"Occupancy Map Inpainting for Online Robot Navigation\",\"1327\":\"Ellipse Loss for Scene-Compliant Motion Prediction\",\"1328\":\"Predictive Runtime Monitoring for Mobile Robots using Logic-Based Bayesian Intent Inference\",\"1329\":\"Graph-SIM: A Graph-based Spatiotemporal Interaction Modelling for Pedestrian Action Prediction\",\"1330\":\"Efficient and Robust LiDAR-Based End-to-End Navigation\",\"1331\":\"Environmental Hotspot Identification in Limited Time with a UAV Equipped with a Downward-Facing Camera\",\"1332\":\"Object-centric Video Prediction without Annotation\",\"1333\":\"Generalization in Reinforcement Learning by Soft Data Augmentation\",\"1334\":\"Test-Time Training for Deformable Multi-Scale Image Registration\",\"1335\":\"Evaluating Initialization Methods for Discriminative and Fast-Converging HGMM Point Clouds\",\"1336\":\"Out-of-Distribution Robustness with Deep Recursive Filters\",\"1337\":\"ZePHyR: Zero-shot Pose Hypothesis Rating\",\"1338\":\"A Generalized A Algorithm for Finding Globally Optimal Paths in Weighted Colored Graphs\",\"1339\":\"Tailored Magnetic Torsion Springs for Miniature Magnetic Robots\",\"1340\":\"Automated End-Effector Alignment for Robotic Cell Manipulation\",\"1341\":\"A high-voltage power electronics unit for flying insect robots that can modulate wing thrust\",\"1342\":\"Residual Model Learning for Microrobot Control\",\"1343\":\"Small Autonomous Robot Actuator (SARA): A Solar-Powered Wireless MEMS Gripper\",\"1344\":\"Tiny Robot Learning (tinyRL) for Source Seeking on a Nano Quadcopter\",\"1345\":\"A Soft Robotic Gripper with Anti-Freezing Ionic Hydrogel-Based Sensors for Learning-Based Object Recognition\",\"1346\":\"Adaptive Tracking Control of Soft Robots Using Integrated Sensing Skins and Recurrent Neural Networks\",\"1347\":\"Embedded Neuromorphic Architecture for Form + Function 4-D Printing of Robotic Materials: Emulation of Optimized Neurons\",\"1348\":\"Toward Robust and Efficient Online Adaptation for Deep Stereo Depth Estimation\",\"1349\":\"Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model Alignments\",\"1350\":\"Learning the Next Best View for 3D Point Clouds via Topological Features\",\"1351\":\"A New Framework for Registration of Semantic Point Clouds from Stereo and RGB-D Cameras\",\"1352\":\"A Legged Soft Robot Platform for Dynamic Locomotion\",\"1353\":\"States and Contact Forces Estimation for a Fabric-Reinforced Inflatable Soft Robot\",\"1354\":\"Acoustic Communication and Sensing for Inflatable Modular Soft Robots\",\"1355\":\"Semantic SLAM with Autonomous Object-Level Data Association\",\"1356\":\"Kimera-Multi: a System for Distributed Multi-Robot Metric-Semantic Simultaneous Localization and Mapping\",\"1357\":\"Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search\",\"1358\":\"Visionary: Vision architecture discovery for robot learning\",\"1359\":\"Zero-shot Policy Learning with Spatial Temporal Reward Decomposition on Contingency-aware Observation\",\"1360\":\"Approximate Inverse Reinforcement Learning from Vision-based Imitation Learning\",\"1361\":\"Validation of a Novel Parallel-Actuated Shoulder Exoskeleton Robot for the Characterization of Human Shoulder Impedance\",\"1362\":\"Planning on a (Risk) Budget: Safe Non-Conservative Planning in Probabilistic Dynamic Environments\",\"1363\":\"Avoidance Critical Probabilistic Roadmaps for Motion Planning in Dynamic Environments\",\"1364\":\"Strobe: An Acceleration Meta-algorithm for Optimizing Robot Paths using Concurrent Interleaved Sub-Epoch Pods\",\"1365\":\"Designing Multi-Stage Coupled Convex Programming with Data-Driven McCormick Envelope Relaxations for Motion Planning\",\"1366\":\"Fusing RGBD Tracking and Segmentation Tree Sampling for Multi-Hypothesis Volumetric Segmentation\",\"1367\":\"YolactEdge: Real-time Instance Segmentation on the Edge\",\"1368\":\"Learning Panoptic Segmentation from Instance Contours\",\"1369\":\"0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera\",\"1370\":\"Adaptation to Team Composition Changes for Heterogeneous Multi-Robot Sensor Coverage\",\"1371\":\"Distributed Multi-Target Tracking for Heterogeneous Mobile Sensing Networks with Limited Field of Views\",\"1372\":\"Safety With Limited Range Sensing Constraints For Fixed Wing Aircraft\",\"1373\":\"An Adaptive Fuzzy Reinforcement Learning Cooperative Approach for the Autonomous Control of Flock Systems\",\"1374\":\"Optimal Sequential Stochastic Deployment of Multiple Passenger Robots\",\"1375\":\"Online Connectivity-aware Dynamic Deployment for Heterogeneous Multi-Robot Systems\",\"1376\":\"Achieving Multitasking Robots in Multi-Robot Tasks\",\"1377\":\"Distributed Topology Correction for Flexible Connectivity Maintenance in Multi-Robot Systems\",\"1378\":\"Decentralized Nested Gaussian Processes for Multi-Robot Systems\",\"1379\":\"Data-based Control of Partially-Observed Robotic Systems\",\"1380\":\"Partial Information Target Defense Game\",\"1381\":\"Long-Horizon Motion Planning for Autonomous Vehicle Parking Incorporating Incomplete Map Information\",\"1382\":\"Towards Safe Motion Planning in Human Workspaces: A Robust Multi-agent Approach\",\"1383\":\"Anytime Fault-tolerant Adaptive Routing for Multi-Robot Teams\",\"1384\":\"Exploiting collisions for sampling-based multicopter motion planning\",\"1385\":\"Multi-Robot Motion Planning with Unlabeled Goals for Mobile Robots with Differential Constraints\"},\"First and Last Author Affiliations\":{\"0\":\"School of Science for Open and Environmental Systems, Graduate School of Science and Technology, Keio University, Yokohama, Japan\\nDepartment of System Design Engineering, Faculty of Science and Technology, Keio University, Yokohama, Japan\",\"1\":\"University of Technology, Sydney, Australia\\nUniversity of Technology, Sydney, Australia\",\"2\":\"Department of Marine Technology, Noweigian University of Science and Technology, Trondheim, Norway\\nThe Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA\",\"3\":\"Naver Labs, Gyeonggi-do, South Korea\\nNaver Labs, Gyeonggi-do, South Korea\",\"4\":\"Robotics, School of Control Science and Engineering, Shandong University, Jinan, China\\nRobotics, School of Control Science and Engineering, Shandong University, Jinan, China\",\"5\":\"Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong\\nDepartment of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong\",\"6\":\"National Key Lab of Science and Technology on Communications, University of Electronic Science and Technology of China\\nChester F. Carlson Center for Imaging Science, Rochester Institute of Technology, USA\",\"7\":\"Graduate School of Creative Science and Engineering, Waseda University, Tokyo, Japan\\nFaculty of Science and Engineering, Waseda University, Tokyo, Japan\",\"8\":\"Center for Healthcare Robotics, Korea Institute Science and Technology (KIST), Seoul, Korea\\nCenter for Healthcare Robotics, Korea Institute Science and Technology (KIST), Seoul, Korea\",\"9\":\"Centre for Autonomous Systems, University of Technology Sydney (UTS), Sydney, Australia\\nCentre for Autonomous Systems, University of Technology Sydney (UTS), Sydney, Australia\",\"10\":\"IRIM, Chinese University of Hong (Shenzhen), Shenzhen, China\\nIRIM, Chinese University of Hong (Shenzhen), Shenzhen, China\",\"11\":\"Robotics Research Centre, School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore\\nRobotics Research Centre, School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore\",\"12\":\"Department of Advanced Manufacturing and Robotics, College of Engineering, Peking University, Beijing, China\\nDepartment of Advanced Manufacturing and Robotics, College of Engineering, Peking University, Beijing, China\",\"13\":\"Applied AI Lab (A2I) Oxford Robotics Institute (ORI), University of Oxford\\nApplied AI Lab (A2I) Oxford Robotics Institute (ORI), University of Oxford\",\"14\":\"Department of Electronics and Computer Engineering, Hanyang University, Korea\\nCogAplex Co., Ltd\",\"15\":\"Institute for Infocomm Research (I2R), A*STAR, Singapore\\nInstitute for Infocomm Research (I2R), A*STAR, Singapore\",\"16\":\"College of Control Science and Engineering, The State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China\\nCollege of Control Science and Engineering, The State Key Laboratory of Industrial Control Technology, Zhejiang University, Hangzhou, China\",\"17\":\"Institute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology\\nInstitute for Artificial Intelligence, Beijing National Research Center for Information Science and Technology\",\"18\":\"SenseTime Japan Ltd\\nSenseTime Japan Ltd\",\"19\":\"School of Engineering, The University of Edinburgh, Edinburgh, United Kingdom\\nSchool of Engineering, The University of Edinburgh, Edinburgh, United Kingdom\",\"20\":\"RIKEN Center for Advanced Intelligence Project, Tokyo, Japan\\nTohoku University, Sendai, Japan\",\"21\":\"Shanghai Engineering Research Center of Intelligent Vision and Imaging, ShanghaiTech University\\nShanghai Engineering Research Center of Intelligent Vision and Imaging, ShanghaiTech University\",\"22\":\"Alibaba Group, Damo Academy\\nMobile Perception Lab, SIST, ShanghaiTech\",\"23\":\"School of Software, Beihang University, Beijing, China\\nSchool of Software, Beihang University, Beijing, China\",\"24\":\"Department of Computer Science, Elsa Lab, National Tsing Hua University, Hsinchu, Taiwan\\nDepartment of Computer Science, Elsa Lab, National Tsing Hua University, Hsinchu, Taiwan\",\"25\":\"TBSI, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\\nTencent AI Lab, Shenzhen, China\",\"26\":\"Digitan & AI Technology Center, Technology Division, Panasonic Corporation, Japan\\nDigitan & AI Technology Center, Technology Division, Panasonic Corporation, Japan\",\"27\":\"Intelligent Autonomous Systems, Technische Universit\\u00e4t Darmstadt\\nIntelligent Autonomous Systems, Technische Universit\\u00e4t Darmstadt\",\"28\":\"Department of Mechanical and Aerospace Engineering, Seoul National University, Seoul, Korea\\nDepartment of Mechanical and Aerospace Engineering, Seoul National University, Seoul, Korea\",\"29\":\"Singapore-MIT Alliance for Research and Technology, Singapore\\nComputer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA\",\"30\":\"School of Computer Science, The University of Sydney, Australia\\nSchool of Computer Science, The University of Sydney, Australia\",\"31\":\"Department of Mechanical and Energy Engineering, Southern University of Science and Technology, China\\nDepartment of Mechanical and Energy Engineering, Southern University of Science and Technology, China\",\"32\":\"CNRS, Institut des Syst\\u00e8mes Intelligents et de Robotique: ISIR, Sorbonne University, Paris, France\\nCNRS, Institut des Syst\\u00e8mes Intelligents et de Robotique: ISIR, Sorbonne University, Paris, France\",\"33\":\"Division of Information Science, Robotics Laboratory, Nara Institute of Science and Technology (NAIST), Japan\\nDivision of Information Science, Robotics Laboratory, Nara Institute of Science and Technology (NAIST), Japan\",\"34\":\"Robotics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia\\nAustralian Artificial Intelligence Institute, School of Software, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia\",\"35\":\"Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea\\nDepartment of Mechanical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea\",\"36\":\"Center for Autonomous Vehicles, Meituan, Beijing, China\\nCenter for Autonomous Vehicles, Meituan, Beijing, China\",\"37\":\"Robotics Research Department, Center for Technology Innovation, R&D Group, Hitachi, Ltd.\\nArtificial Intelligence Research Center, AIST\",\"38\":\"Baidu Research Institute, Sunnyvale, CA, USA\\nBaidu Research Institute, Sunnyvale, CA, USA\",\"39\":\"Faculty of Engineering, Electrical and Computer Engineering, The University of Auckland, Auckland, New Zealand\\nFaculty of Science, River Science, The University of Auckland, New Zealand\",\"40\":\"Associated with Preferred Networks, Inc\\nAssociated with Preferred Networks, Inc\",\"41\":\"Dept. of Modern Mechanical Engineering, Faculty of Science and Engineering, Waseda University, Tokyo, Japan\\nDept. of Modern Mechanical Engineering, Faculty of Science and Engineering, Waseda University, Tokyo, Japan\",\"42\":\"Cyber Media Center, Osaka University, Japan\\nCyber Media Center, Osaka University, Japan\",\"43\":\"School of Engineering and Digital Sciences, Nazarbayev University, Nur-Sultan (Astana), Kazakhstan\\nSchool of Engineering and Digital Sciences, Nazarbayev University, Nur-Sultan (Astana), Kazakhstan\",\"44\":\"Department of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan\\nDepartment of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan\",\"45\":\"Department of Biomedical Engineering, The Chinese University of Hong Kong, Hong Kong, China\\nDepartment of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong, China\",\"46\":\"Chinese Academy of Sciences, Institute of Advanced Technology, Shenzhen, China\\nCAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Chinese Academy of Sciences, Shenzhen, China\",\"47\":\"Lawrence Livermore National Laboratory, Livermore, CA, USA\\nGeorgia Institute of Technology, Atlanta, GA, USA\",\"48\":\"International Institute of Information Technology, Bangalore, India\\nInternational Institute of Information Technology, Bangalore, India\",\"49\":\"School of Mechanical Engineering, Yonsei University, Seoul, Republic of Korea\\nSchool of Mechanical Engineering and the Department of Artificial Intelligence, Yonsei University, Seoul, Republic of Korea\",\"50\":\"Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China\\nDepartment of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China\",\"51\":\"School of Information Science and Technology, Peking University, Beijing, China\",\"52\":\"State Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China\\nState Key Laboratory of Industrial Control Technology, Institute of Cyber-Systems and Control, Zhejiang University, Hangzhou, China\",\"53\":\"Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong SAR, China\\nDepartment of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong SAR, China\",\"54\":\"Singapore-MIT Alliance for Research and Technology, Singapore\\nComputer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA\",\"55\":\"State Key Laboratory of Industrial Control and Technology, Zhejiang University, Hangzhou, P.R. China\\nState Key Laboratory of Industrial Control and Technology, Zhejiang University, Hangzhou, P.R. China\",\"56\":\"University of Technology, Sydney, NSW, Australia\\nUniversity of Technology, Sydney, NSW, Australia\",\"57\":\"Data Science Research Laboratories, NEC Corporation, Kawasaki, Kanagawa, Japan\\nData Science Research Laboratories, NEC Corporation, Kawasaki, Kanagawa, Japan\",\"58\":\"Division of Information Science, Graduate School of Science and Technology, Nara Institute of Science and Technology (NAIST), Japan\\nDivision of Information Science, Graduate School of Science and Technology, Nara Institute of Science and Technology (NAIST), Japan\",\"59\":null,\"60\":null,\"61\":null,\"62\":null,\"63\":null,\"64\":null,\"65\":null,\"66\":null,\"67\":null,\"68\":null,\"69\":null,\"70\":null,\"71\":null,\"72\":null,\"73\":null,\"74\":null,\"75\":null,\"76\":null,\"77\":null,\"78\":null,\"79\":null,\"80\":null,\"81\":null,\"82\":null,\"83\":null,\"84\":null,\"85\":null,\"86\":null,\"87\":null,\"88\":null,\"89\":null,\"90\":null,\"91\":null,\"92\":null,\"93\":null,\"94\":null,\"95\":null,\"96\":null,\"97\":null,\"98\":null,\"99\":null,\"100\":null,\"101\":null,\"102\":null,\"103\":null,\"104\":null,\"105\":null,\"106\":null,\"107\":null,\"108\":null,\"109\":null,\"110\":null,\"111\":null,\"112\":null,\"113\":null,\"114\":null,\"115\":null,\"116\":null,\"117\":null,\"118\":null,\"119\":null,\"120\":null,\"121\":null,\"122\":null,\"123\":null,\"124\":null,\"125\":null,\"126\":null,\"127\":null,\"128\":null,\"129\":null,\"130\":null,\"131\":null,\"132\":null,\"133\":null,\"134\":null,\"135\":null,\"136\":null,\"137\":null,\"138\":null,\"139\":null,\"140\":null,\"141\":null,\"142\":null,\"143\":null,\"144\":null,\"145\":null,\"146\":null,\"147\":null,\"148\":null,\"149\":null,\"150\":null,\"151\":null,\"152\":null,\"153\":null,\"154\":null,\"155\":null,\"156\":null,\"157\":null,\"158\":null,\"159\":null,\"160\":null,\"161\":null,\"162\":null,\"163\":null,\"164\":null,\"165\":null,\"166\":null,\"167\":null,\"168\":null,\"169\":null,\"170\":null,\"171\":null,\"172\":null,\"173\":null,\"174\":null,\"175\":null,\"176\":null,\"177\":null,\"178\":null,\"179\":null,\"180\":null,\"181\":null,\"182\":null,\"183\":null,\"184\":null,\"185\":null,\"186\":null,\"187\":null,\"188\":null,\"189\":null,\"190\":null,\"191\":null,\"192\":null,\"193\":null,\"194\":null,\"195\":null,\"196\":null,\"197\":null,\"198\":null,\"199\":null,\"200\":null,\"201\":null,\"202\":null,\"203\":null,\"204\":null,\"205\":null,\"206\":null,\"207\":null,\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":null,\"213\":null,\"214\":null,\"215\":null,\"216\":null,\"217\":null,\"218\":null,\"219\":null,\"220\":null,\"221\":null,\"222\":null,\"223\":null,\"224\":null,\"225\":null,\"226\":null,\"227\":null,\"228\":null,\"229\":null,\"230\":null,\"231\":null,\"232\":null,\"233\":null,\"234\":null,\"235\":null,\"236\":null,\"237\":null,\"238\":null,\"239\":null,\"240\":null,\"241\":null,\"242\":null,\"243\":null,\"244\":null,\"245\":null,\"246\":null,\"247\":null,\"248\":null,\"249\":null,\"250\":null,\"251\":null,\"252\":null,\"253\":null,\"254\":null,\"255\":null,\"256\":null,\"257\":null,\"258\":null,\"259\":null,\"260\":null,\"261\":null,\"262\":null,\"263\":null,\"264\":null,\"265\":null,\"266\":null,\"267\":null,\"268\":null,\"269\":null,\"270\":null,\"271\":null,\"272\":null,\"273\":null,\"274\":null,\"275\":null,\"276\":null,\"277\":null,\"278\":null,\"279\":null,\"280\":null,\"281\":null,\"282\":null,\"283\":null,\"284\":null,\"285\":null,\"286\":null,\"287\":null,\"288\":null,\"289\":null,\"290\":null,\"291\":null,\"292\":null,\"293\":null,\"294\":null,\"295\":null,\"296\":null,\"297\":null,\"298\":null,\"299\":null,\"300\":null,\"301\":null,\"302\":null,\"303\":null,\"304\":null,\"305\":null,\"306\":null,\"307\":null,\"308\":null,\"309\":null,\"310\":null,\"311\":null,\"312\":null,\"313\":null,\"314\":null,\"315\":null,\"316\":null,\"317\":null,\"318\":null,\"319\":null,\"320\":null,\"321\":null,\"322\":null,\"323\":null,\"324\":null,\"325\":null,\"326\":null,\"327\":null,\"328\":null,\"329\":null,\"330\":null,\"331\":null,\"332\":null,\"333\":null,\"334\":null,\"335\":null,\"336\":null,\"337\":null,\"338\":null,\"339\":null,\"340\":null,\"341\":null,\"342\":null,\"343\":null,\"344\":null,\"345\":null,\"346\":null,\"347\":null,\"348\":null,\"349\":null,\"350\":null,\"351\":null,\"352\":null,\"353\":null,\"354\":null,\"355\":null,\"356\":null,\"357\":null,\"358\":null,\"359\":null,\"360\":null,\"361\":null,\"362\":null,\"363\":null,\"364\":null,\"365\":null,\"366\":null,\"367\":null,\"368\":null,\"369\":null,\"370\":null,\"371\":null,\"372\":null,\"373\":null,\"374\":null,\"375\":null,\"376\":null,\"377\":null,\"378\":null,\"379\":null,\"380\":null,\"381\":null,\"382\":null,\"383\":null,\"384\":null,\"385\":null,\"386\":null,\"387\":null,\"388\":null,\"389\":null,\"390\":null,\"391\":null,\"392\":null,\"393\":null,\"394\":null,\"395\":null,\"396\":null,\"397\":null,\"398\":null,\"399\":null,\"400\":null,\"401\":null,\"402\":null,\"403\":null,\"404\":null,\"405\":null,\"406\":null,\"407\":null,\"408\":null,\"409\":null,\"410\":null,\"411\":null,\"412\":null,\"413\":null,\"414\":null,\"415\":null,\"416\":null,\"417\":null,\"418\":null,\"419\":null,\"420\":null,\"421\":null,\"422\":null,\"423\":null,\"424\":null,\"425\":null,\"426\":null,\"427\":null,\"428\":null,\"429\":null,\"430\":null,\"431\":null,\"432\":null,\"433\":null,\"434\":null,\"435\":null,\"436\":null,\"437\":null,\"438\":null,\"439\":null,\"440\":null,\"441\":null,\"442\":null,\"443\":null,\"444\":null,\"445\":null,\"446\":null,\"447\":null,\"448\":null,\"449\":null,\"450\":null,\"451\":null,\"452\":null,\"453\":null,\"454\":null,\"455\":null,\"456\":null,\"457\":null,\"458\":null,\"459\":null,\"460\":null,\"461\":null,\"462\":null,\"463\":null,\"464\":null,\"465\":null,\"466\":null,\"467\":null,\"468\":null,\"469\":null,\"470\":null,\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":null,\"476\":null,\"477\":null,\"478\":null,\"479\":null,\"480\":null,\"481\":null,\"482\":null,\"483\":null,\"484\":null,\"485\":null,\"486\":null,\"487\":null,\"488\":null,\"489\":null,\"490\":null,\"491\":null,\"492\":null,\"493\":null,\"494\":null,\"495\":null,\"496\":null,\"497\":null,\"498\":null,\"499\":null,\"500\":null,\"501\":null,\"502\":null,\"503\":null,\"504\":null,\"505\":null,\"506\":null,\"507\":null,\"508\":null,\"509\":null,\"510\":null,\"511\":null,\"512\":null,\"513\":null,\"514\":null,\"515\":null,\"516\":null,\"517\":null,\"518\":null,\"519\":null,\"520\":null,\"521\":null,\"522\":null,\"523\":null,\"524\":null,\"525\":null,\"526\":null,\"527\":null,\"528\":null,\"529\":null,\"530\":null,\"531\":null,\"532\":null,\"533\":null,\"534\":null,\"535\":null,\"536\":null,\"537\":null,\"538\":null,\"539\":null,\"540\":null,\"541\":null,\"542\":null,\"543\":null,\"544\":null,\"545\":null,\"546\":null,\"547\":null,\"548\":null,\"549\":null,\"550\":null,\"551\":null,\"552\":null,\"553\":null,\"554\":null,\"555\":null,\"556\":null,\"557\":null,\"558\":null,\"559\":null,\"560\":null,\"561\":null,\"562\":null,\"563\":null,\"564\":null,\"565\":null,\"566\":null,\"567\":null,\"568\":null,\"569\":null,\"570\":null,\"571\":null,\"572\":null,\"573\":null,\"574\":null,\"575\":null,\"576\":null,\"577\":null,\"578\":null,\"579\":null,\"580\":null,\"581\":null,\"582\":null,\"583\":null,\"584\":null,\"585\":null,\"586\":null,\"587\":null,\"588\":null,\"589\":null,\"590\":null,\"591\":null,\"592\":null,\"593\":null,\"594\":null,\"595\":null,\"596\":null,\"597\":null,\"598\":null,\"599\":null,\"600\":null,\"601\":null,\"602\":null,\"603\":null,\"604\":null,\"605\":null,\"606\":null,\"607\":null,\"608\":null,\"609\":null,\"610\":null,\"611\":null,\"612\":null,\"613\":null,\"614\":null,\"615\":null,\"616\":null,\"617\":null,\"618\":null,\"619\":null,\"620\":null,\"621\":null,\"622\":null,\"623\":null,\"624\":null,\"625\":null,\"626\":null,\"627\":null,\"628\":null,\"629\":null,\"630\":null,\"631\":null,\"632\":null,\"633\":null,\"634\":null,\"635\":null,\"636\":null,\"637\":null,\"638\":null,\"639\":null,\"640\":null,\"641\":null,\"642\":null,\"643\":null,\"644\":null,\"645\":null,\"646\":null,\"647\":null,\"648\":null,\"649\":null,\"650\":null,\"651\":null,\"652\":null,\"653\":null,\"654\":null,\"655\":null,\"656\":null,\"657\":null,\"658\":null,\"659\":null,\"660\":null,\"661\":null,\"662\":null,\"663\":null,\"664\":null,\"665\":null,\"666\":null,\"667\":null,\"668\":null,\"669\":null,\"670\":null,\"671\":null,\"672\":null,\"673\":null,\"674\":null,\"675\":null,\"676\":null,\"677\":null,\"678\":null,\"679\":null,\"680\":null,\"681\":null,\"682\":null,\"683\":null,\"684\":null,\"685\":null,\"686\":null,\"687\":null,\"688\":null,\"689\":null,\"690\":null,\"691\":null,\"692\":null,\"693\":null,\"694\":null,\"695\":null,\"696\":null,\"697\":null,\"698\":null,\"699\":null,\"700\":null,\"701\":null,\"702\":null,\"703\":null,\"704\":null,\"705\":null,\"706\":null,\"707\":null,\"708\":null,\"709\":null,\"710\":null,\"711\":null,\"712\":null,\"713\":null,\"714\":null,\"715\":null,\"716\":null,\"717\":null,\"718\":null,\"719\":null,\"720\":null,\"721\":null,\"722\":null,\"723\":null,\"724\":null,\"725\":null,\"726\":null,\"727\":null,\"728\":null,\"729\":null,\"730\":null,\"731\":null,\"732\":null,\"733\":null,\"734\":null,\"735\":null,\"736\":null,\"737\":null,\"738\":null,\"739\":null,\"740\":null,\"741\":null,\"742\":null,\"743\":null,\"744\":null,\"745\":null,\"746\":null,\"747\":null,\"748\":null,\"749\":null,\"750\":null,\"751\":null,\"752\":null,\"753\":null,\"754\":null,\"755\":null,\"756\":null,\"757\":null,\"758\":null,\"759\":null,\"760\":null,\"761\":null,\"762\":null,\"763\":null,\"764\":null,\"765\":null,\"766\":null,\"767\":null,\"768\":null,\"769\":null,\"770\":null,\"771\":null,\"772\":null,\"773\":null,\"774\":null,\"775\":null,\"776\":null,\"777\":null,\"778\":null,\"779\":null,\"780\":null,\"781\":null,\"782\":null,\"783\":null,\"784\":null,\"785\":null,\"786\":null,\"787\":null,\"788\":null,\"789\":null,\"790\":null,\"791\":null,\"792\":null,\"793\":null,\"794\":null,\"795\":null,\"796\":null,\"797\":null,\"798\":null,\"799\":null,\"800\":null,\"801\":null,\"802\":null,\"803\":null,\"804\":null,\"805\":null,\"806\":null,\"807\":null,\"808\":null,\"809\":null,\"810\":null,\"811\":null,\"812\":null,\"813\":null,\"814\":null,\"815\":null,\"816\":null,\"817\":null,\"818\":null,\"819\":null,\"820\":null,\"821\":null,\"822\":null,\"823\":null,\"824\":null,\"825\":null,\"826\":null,\"827\":null,\"828\":null,\"829\":null,\"830\":null,\"831\":null,\"832\":null,\"833\":null,\"834\":null,\"835\":null,\"836\":null,\"837\":null,\"838\":null,\"839\":null,\"840\":null,\"841\":null,\"842\":null,\"843\":null,\"844\":null,\"845\":null,\"846\":null,\"847\":null,\"848\":null,\"849\":null,\"850\":null,\"851\":null,\"852\":null,\"853\":null,\"854\":null,\"855\":null,\"856\":null,\"857\":null,\"858\":null,\"859\":null,\"860\":null,\"861\":null,\"862\":null,\"863\":null,\"864\":null,\"865\":null,\"866\":null,\"867\":null,\"868\":null,\"869\":null,\"870\":null,\"871\":null,\"872\":null,\"873\":null,\"874\":null,\"875\":null,\"876\":null,\"877\":null,\"878\":null,\"879\":null,\"880\":null,\"881\":null,\"882\":null,\"883\":null,\"884\":null,\"885\":null,\"886\":null,\"887\":null,\"888\":null,\"889\":null,\"890\":null,\"891\":null,\"892\":null,\"893\":null,\"894\":null,\"895\":null,\"896\":null,\"897\":null,\"898\":null,\"899\":null,\"900\":null,\"901\":null,\"902\":null,\"903\":null,\"904\":null,\"905\":null,\"906\":null,\"907\":null,\"908\":null,\"909\":null,\"910\":null,\"911\":null,\"912\":null,\"913\":null,\"914\":null,\"915\":null,\"916\":null,\"917\":null,\"918\":null,\"919\":null,\"920\":null,\"921\":null,\"922\":null,\"923\":null,\"924\":null,\"925\":null,\"926\":null,\"927\":null,\"928\":null,\"929\":null,\"930\":null,\"931\":null,\"932\":null,\"933\":null,\"934\":null,\"935\":null,\"936\":null,\"937\":null,\"938\":null,\"939\":null,\"940\":null,\"941\":null,\"942\":null,\"943\":null,\"944\":null,\"945\":null,\"946\":null,\"947\":null,\"948\":null,\"949\":null,\"950\":null,\"951\":null,\"952\":null,\"953\":null,\"954\":null,\"955\":null,\"956\":null,\"957\":null,\"958\":null,\"959\":null,\"960\":null,\"961\":null,\"962\":null,\"963\":null,\"964\":null,\"965\":null,\"966\":null,\"967\":null,\"968\":null,\"969\":null,\"970\":null,\"971\":null,\"972\":null,\"973\":null,\"974\":null,\"975\":null,\"976\":null,\"977\":null,\"978\":null,\"979\":null,\"980\":null,\"981\":null,\"982\":null,\"983\":null,\"984\":null,\"985\":null,\"986\":null,\"987\":null,\"988\":null,\"989\":null,\"990\":null,\"991\":null,\"992\":null,\"993\":null,\"994\":null,\"995\":null,\"996\":null,\"997\":null,\"998\":null,\"999\":null,\"1000\":null,\"1001\":null,\"1002\":null,\"1003\":null,\"1004\":null,\"1005\":null,\"1006\":null,\"1007\":null,\"1008\":null,\"1009\":null,\"1010\":null,\"1011\":null,\"1012\":null,\"1013\":null,\"1014\":null,\"1015\":null,\"1016\":null,\"1017\":null,\"1018\":null,\"1019\":null,\"1020\":null,\"1021\":null,\"1022\":null,\"1023\":null,\"1024\":null,\"1025\":null,\"1026\":null,\"1027\":null,\"1028\":null,\"1029\":null,\"1030\":null,\"1031\":null,\"1032\":null,\"1033\":null,\"1034\":null,\"1035\":null,\"1036\":null,\"1037\":null,\"1038\":null,\"1039\":null,\"1040\":null,\"1041\":null,\"1042\":null,\"1043\":null,\"1044\":null,\"1045\":null,\"1046\":null,\"1047\":null,\"1048\":null,\"1049\":null,\"1050\":null,\"1051\":null,\"1052\":null,\"1053\":null,\"1054\":null,\"1055\":null,\"1056\":null,\"1057\":null,\"1058\":null,\"1059\":null,\"1060\":null,\"1061\":null,\"1062\":null,\"1063\":null,\"1064\":null,\"1065\":null,\"1066\":null,\"1067\":null,\"1068\":null,\"1069\":null,\"1070\":null,\"1071\":null,\"1072\":null,\"1073\":null,\"1074\":null,\"1075\":null,\"1076\":null,\"1077\":null,\"1078\":null,\"1079\":null,\"1080\":null,\"1081\":null,\"1082\":null,\"1083\":null,\"1084\":null,\"1085\":null,\"1086\":null,\"1087\":null,\"1088\":null,\"1089\":null,\"1090\":null,\"1091\":null,\"1092\":null,\"1093\":null,\"1094\":null,\"1095\":null,\"1096\":null,\"1097\":null,\"1098\":null,\"1099\":null,\"1100\":null,\"1101\":null,\"1102\":null,\"1103\":null,\"1104\":null,\"1105\":null,\"1106\":null,\"1107\":null,\"1108\":null,\"1109\":null,\"1110\":null,\"1111\":null,\"1112\":null,\"1113\":null,\"1114\":null,\"1115\":null,\"1116\":null,\"1117\":null,\"1118\":null,\"1119\":null,\"1120\":null,\"1121\":null,\"1122\":null,\"1123\":null,\"1124\":null,\"1125\":null,\"1126\":null,\"1127\":null,\"1128\":null,\"1129\":null,\"1130\":null,\"1131\":null,\"1132\":null,\"1133\":null,\"1134\":null,\"1135\":null,\"1136\":null,\"1137\":null,\"1138\":null,\"1139\":null,\"1140\":null,\"1141\":null,\"1142\":null,\"1143\":null,\"1144\":null,\"1145\":null,\"1146\":null,\"1147\":null,\"1148\":null,\"1149\":null,\"1150\":null,\"1151\":null,\"1152\":null,\"1153\":null,\"1154\":null,\"1155\":null,\"1156\":null,\"1157\":null,\"1158\":null,\"1159\":null,\"1160\":null,\"1161\":null,\"1162\":null,\"1163\":null,\"1164\":null,\"1165\":null,\"1166\":null,\"1167\":null,\"1168\":null,\"1169\":null,\"1170\":null,\"1171\":null,\"1172\":null,\"1173\":null,\"1174\":null,\"1175\":null,\"1176\":null,\"1177\":null,\"1178\":null,\"1179\":null,\"1180\":null,\"1181\":null,\"1182\":null,\"1183\":null,\"1184\":null,\"1185\":null,\"1186\":null,\"1187\":null,\"1188\":null,\"1189\":null,\"1190\":null,\"1191\":null,\"1192\":null,\"1193\":null,\"1194\":null,\"1195\":null,\"1196\":null,\"1197\":null,\"1198\":null,\"1199\":null,\"1200\":null,\"1201\":null,\"1202\":null,\"1203\":null,\"1204\":null,\"1205\":null,\"1206\":null,\"1207\":null,\"1208\":null,\"1209\":null,\"1210\":null,\"1211\":null,\"1212\":null,\"1213\":null,\"1214\":null,\"1215\":null,\"1216\":null,\"1217\":null,\"1218\":null,\"1219\":null,\"1220\":null,\"1221\":null,\"1222\":null,\"1223\":null,\"1224\":null,\"1225\":null,\"1226\":null,\"1227\":null,\"1228\":null,\"1229\":null,\"1230\":null,\"1231\":null,\"1232\":null,\"1233\":null,\"1234\":null,\"1235\":null,\"1236\":null,\"1237\":null,\"1238\":null,\"1239\":null,\"1240\":null,\"1241\":null,\"1242\":null,\"1243\":null,\"1244\":null,\"1245\":null,\"1246\":null,\"1247\":null,\"1248\":null,\"1249\":null,\"1250\":null,\"1251\":null,\"1252\":null,\"1253\":null,\"1254\":null,\"1255\":null,\"1256\":null,\"1257\":null,\"1258\":null,\"1259\":null,\"1260\":null,\"1261\":null,\"1262\":null,\"1263\":null,\"1264\":null,\"1265\":null,\"1266\":null,\"1267\":null,\"1268\":null,\"1269\":null,\"1270\":null,\"1271\":null,\"1272\":null,\"1273\":null,\"1274\":null,\"1275\":null,\"1276\":null,\"1277\":null,\"1278\":null,\"1279\":null,\"1280\":null,\"1281\":null,\"1282\":null,\"1283\":null,\"1284\":null,\"1285\":null,\"1286\":null,\"1287\":null,\"1288\":null,\"1289\":null,\"1290\":null,\"1291\":null,\"1292\":null,\"1293\":null,\"1294\":null,\"1295\":null,\"1296\":null,\"1297\":null,\"1298\":null,\"1299\":null,\"1300\":null,\"1301\":null,\"1302\":null,\"1303\":null,\"1304\":null,\"1305\":null,\"1306\":null,\"1307\":null,\"1308\":null,\"1309\":null,\"1310\":null,\"1311\":null,\"1312\":null,\"1313\":null,\"1314\":null,\"1315\":null,\"1316\":null,\"1317\":null,\"1318\":null,\"1319\":null,\"1320\":null,\"1321\":null,\"1322\":null,\"1323\":null,\"1324\":null,\"1325\":null,\"1326\":null,\"1327\":null,\"1328\":null,\"1329\":null,\"1330\":null,\"1331\":null,\"1332\":null,\"1333\":null,\"1334\":null,\"1335\":null,\"1336\":null,\"1337\":null,\"1338\":null,\"1339\":null,\"1340\":null,\"1341\":null,\"1342\":null,\"1343\":null,\"1344\":null,\"1345\":null,\"1346\":null,\"1347\":null,\"1348\":null,\"1349\":null,\"1350\":null,\"1351\":null,\"1352\":null,\"1353\":null,\"1354\":null,\"1355\":null,\"1356\":null,\"1357\":null,\"1358\":null,\"1359\":null,\"1360\":null,\"1361\":null,\"1362\":null,\"1363\":null,\"1364\":null,\"1365\":null,\"1366\":null,\"1367\":null,\"1368\":null,\"1369\":null,\"1370\":null,\"1371\":null,\"1372\":null,\"1373\":null,\"1374\":null,\"1375\":null,\"1376\":null,\"1377\":null,\"1378\":null,\"1379\":null,\"1380\":null,\"1381\":null,\"1382\":null,\"1383\":null,\"1384\":null,\"1385\":null},\"Keywords or Approach\":{\"0\":\"legged locomotion\\nuncertainty\\nprediction methods\\ncontrol systems\\nentropy\\ntrajectory\\ndelays\\ngait analysis\\nhuman-robot interaction\\nintelligent robots\\nmobile robots\\nnonlinear control systems\\npredictive control\\nrobots\\nuncertainty-aware nonlinear model\\ncompanion robot\\nproactive movement\\nappropriate area\\nhuman personal space\\nuncertainty-aware robot controller\\nnonlinear model predictive control\\nnatural human-followings\\nuncertainty-aware control system\\nappropriate robot movement\\navoiding delay\\nactual human walking data\\nreal-robot experiments\",\"1\":\"monte carlo methods\\noceans\\nweather forecasting\\nestimation\\nprobabilistic logic\\nforecast uncertainty\\nrobustness\\napproximation theory\\nautonomous underwater vehicles\\ncomputational complexity\\nmean square error methods\\nmobile robots\\npath planning\\nsampling methods\\ntree searching\\nuncertain ocean currents\\nensemble forecasts\\npath planning framework\\nensemble forecasting\\ncurrent prediction\\npredicted currents\\nflow field\\nensemble-induced trajectories\\ndeterministic mean-based approach\\nmonte carlo tree search\\nmcts\\nroot-mean-square error distance\\nrmse\\naustralian bureau of meteorology\\nmarine robotics\",\"2\":\"roads\\nsystem recovery\\nprediction algorithms\\nstability analysis\\nrobustness\\nreal-time systems\\ntrajectory\\ncollision avoidance\\ndistributed control\\nmobile robots\\nmotion control\\nmulti-agent systems\\nmulti-robot systems\\noptimisation\\npredictive control\\nroad vehicles\\ndistributed motion coordination\\noptimization-based motion coordination approaches\\nworld multiagent systems\\nhigh computational complexity\\npotential deadlocks\\ndistributed model predictive control approach\\nconvex feasible set algorithm\\nmultivehicle motion coordination\\nautonomous driving\\ncollision avoidance constraints\\ncollision-free trajectories\\ndeadlock\\nchanging vehicles\\nmpc structure\\nlow-level tracking errors\\nmultiple challenging multivehicle environments\\nreciprocal velocity obstacles\",\"3\":\"training\\nnavigation\\nconferences\\nmeasurement uncertainty\\nreinforcement learning\\nrobustness\\nsafety\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\nrisk management\\nrisk-conditioned distributional soft actor-critic\\nrisk-sensitive navigation\\ndeep rl algorithms\\nrisk-neutral manner\\ncost-of-collision\\ndomain randomization\\ndistributional rl algorithm\\nuncertainty-aware policy\\nrisk measure\\ndeep reinforcement learning\\npartially-observed navigation task\\nagents\",\"4\":\"torso\\nlegged locomotion\\ntorque\\nfriction\\nforce\\ndynamics\\ntrajectory\\nlinear programming\\nmotion control\\noptimal control\\npath planning\\nrobot dynamics\\ntrajectory control\\nquadruped robot\\nsimplified dynamic model\\nmaximum ground reaction force\\nfriction cone\\noptimal leg thrusting trajectory\\nwhole body controller\\noptimal joint torque\\nfoot motion\\nyobogo robot platform\\nsomersault motion control\\nsomersault motion planning\\ntorso position\\ntorso posture\\nwebots dynamics simulation software\",\"5\":\"couplings\\nanalytical models\\nmotion segmentation\\ndesign methodology\\ncomputational modeling\\nphantoms\\nnose\\nbending\\nmanipulators\\nmotion control\\nsurgical robots\\nomnidirectional steering\\nmaxillary sinus phantom\\ncoupled bending angle\\ntwo-segment notched continuum robot\\nminimally invasive surgical procedures\\ndistal dexterity\\nmultisegment continuum robots\\nmotion coupling analysis\\ndecoupled design methodology\\ndistal cable force\\nproximal segment\\ncoupled deflection\\ncontinuum surgical robots\",\"6\":\"location awareness\\nvisualization\\nnavigation\\npose estimation\\nwheels\\nrobustness\\ntrajectory\\nimage fusion\\ninertial navigation\\nmaximum likelihood estimation\\nvehicles\\nvins-motion\\nmotion constraint\\nvisual-inertial navigation system\\nvehicle motion constraints\\nautonomous vehicles localization accuracy\\nimu measurement residual\\nvisual measurement residual\\nsystem consistency\\ntightly-coupled fusion\",\"7\":\"automation\\nshape\\nrobot kinematics\\ncomputed tomography\\nconferences\\nneedles\\nend effectors\\ncomputerised tomography\\nimage registration\\nmedical image processing\\nmedical robotics\\nrobot-to-image registration\\ngeometric marker\\nct-guided robotic needle insertion\\ncomputed tomography-guided robotic needle\\nregistration error\\nlower abdominal insertion\\ncross-sectional shapes\\nrotational error\\npositional error\",\"8\":\"band-pass filters\\nmagnetic field measurement\\nvoltage measurement\\nshape\\nmagnetic sensors\\nmagnetic separation\\nsensor phenomena and characterization\\nbending\\ncatheters\\ncoils\\ndexterous manipulators\\nelectromagnetic induction\\nfibre optic sensors\\nmedical image processing\\nmedical robotics\\nposition control\\nsensors\\nshape sensor\\nmedical catheters\\nelectromagnetic based shape sensors\\nvoice coil\\nexternal magnetic field generator outside patient body\\nsensor system\",\"9\":\"temperature sensors\\ntemperature measurement\\nthree-dimensional displays\\nbending\\nstrain measurement\\nsensors\\nmanufacturing\\nbrittleness\\ncracks\\nextrusion\\nmining equipment\\nrapid prototyping (industrial)\\nstrain sensors\\nthree-dimensional printing\\n3d printed strain sensor\\nadvanced manufacturing applications\\nadditive manufacturing\\nmanufacturing end-products\\nlarge-scale extrusion-based 3d printer\\nprint mining equipment\\ngravity separation spiral\\noperational conditions\\ntemperature-compensated strain sensor\\n3d printed inline\\nlarge-scale 3d printed equipment\\nconductive carbon filament\\npolylactic acid base\\nhalf-bridge setup\\ntemperature variations\\ntemperature-controlled tests\\nnontemperature compensated quarter-bridge setup\\nhalf-bridge configuration\\ntemperature impact\\nprinted sensor\\ninternal temperature\\ntemperature sensor\\nprinted traces\\ntemperature 25.0 degc to 40.0 degc\\nadvanced manufacturing\\n3d printing\\nindustry 4.0\\ntemperature compensated strain gauge\",\"10\":\"autonomous underwater vehicles\\nvisualization\\ntorque\\ndrag\\nforce\\nprototypes\\ntransforms\\ncomputational fluid dynamics\\nhydrodynamics\\nmobile robots\\nremotely operated vehicles\\nunderwater vehicles\\ndur\\nrecovery mission\\norigami structure\\nopen states\\nclosed states\\ndifferent recovery stages\\napproaching stage\\nclosed state\\ndrag force\\ncapturing state\\nlarger opening\\nrobot body\\nhigh driven force\\nopen state\\nbalanced force\\ntorque maneuverability\\ndeployable underwater robot\\norigami technique\\nauvs\\nchallenging mission\\nlocalization accuracy\\nmovement capability\",\"11\":\"wrist\\nautomation\\nconferences\\ntools\\nmetamaterials\\nskin\\ndesign tools\\nactuators\\nbiomechanics\\nmedical robotics\\northotics\\npatient rehabilitation\\nmechanism-based metamaterial\\nwrist flexion-extension assistive device\\nmetamaterial structure\\ndegree-of-freedom\\nwrist joint\\nplanar curve\\nactuated mechanisms\\nrehabilitation\\n2d curve\\nwearable robotics\\nexoskeletons\\nassistance\",\"12\":\"knee\\ndamping\\nmechatronics\\ntrajectory tracking\\nsynchronous motors\\ncontrol systems\\nbelts\\nartificial limbs\\ngait analysis\\nmedical control systems\\nprosthetics\\nmechatronic design\\nlow-noise active knee prosthesis\\nhigh backdrivability\\nlow-damping active knee prosthesis\",\"13\":\"uncertainty\\nmonte carlo methods\\nconferences\\nneural networks\\ntactile sensors\\nend effectors\\ntrajectory\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nrobot vision\\nstate-space methods\\nuncertain systems\\nimitation learning-based visuomotor control\\nrobot manipulation tasks\\nend-to-end visuomotor control\\nfailure recovery\\ndeep visuomotor control\\nintrospective visuomotor control\\ntask success rate\\nstate-action space\\nuncertainty cases\\nminimum uncertainty\\nrobot states\\npolicy uncertainties\\npolicy neural network\\nout-of-distribution state\",\"14\":\"adaptation models\\nvisualization\\nautomation\\nconferences\\ngrasping\\nreinforcement learning\\ndata models\\ngrippers\\nrobot vision\\nvisual reality gap\\nreal-world data\\nadapted images\\ngrasping skills\\nfeature-level domain adaptation\\nsim-to-real visual grasping\\nstate representation learning\\npixel-level domain adaptation\\nreal-world objects\\noff-policy actor-critic deep reinforcement learning\\nactor-critic deep rl\\nraw image\\nvisual grasping tasks\\nsimulated image mapping\\nsrl model\",\"15\":\"decision making\\ntactile sensors\\nreinforcement learning\\nfeature extraction\\nend effectors\\nrobustness\\nplanning\\ndexterous manipulators\\nlearning (artificial intelligence)\\nmotion control\\npath planning\\ncontrol loop\\nsophisticated physical interaction\\nsensory-motor control\\ntactile sensing\\naction selection\\ntactile-based motion planning\\nrobot manipulator\\ntactile sensory readings\\noptimal motion sequence\\nkuka lbr iiwa robot\\nsyntouch biotac tactile sensor\\ndexterous manoeuvre\\ncluttered scene\\ndensely cluttered environment\",\"16\":\"automation\\nnavigation\\nconferences\\nrobot sensing systems\\ndata structures\\nskeleton\\ntrajectory\\ncomputational geometry\\nmobile robots\\nobject detection\\npath planning\\nrobot vision\\nflight corridors\\nenvironment information\\nonline replan consistency\\nmapless-planner\\naggressive autonomous flight\\nrobust navigation system\\nenvironment abstraction\\nmapless local planner\\nunfused sensor data\\nlimited-memory data structure\\nhistorical information\\nsampling-based scheme\\nfree-space skeleton\\nsmart waypoint selection strategy\\nhigh-quality trajectories\\nproximity query algorithm\",\"17\":\"visualization\\nautomation\\nconferences\\nrobot vision systems\\nnatural languages\\nstreaming media\\ncameras\\nmobile robots\\nobject detection\\nrobot vision\\nvideo signal processing\\nvideo streaming\\nindoor scene video\\nstreaming video\\naccurate caption\\ninformative caption\\nvisual information\\nscene graph\\nvideo frames as input\\nreal-world robotic platform\\nrobotic indoor scene captioning\",\"18\":\"automation\\nstereo image processing\\nconferences\\naggregates\\nimage matching\\nlearning (artificial intelligence)\\nneural nets\\ngeometry-aware unsupervised domain adaptation\\ndnn-based stereo matching methods\\nunsupervised domain adaptation methods\\nstereo image pair\\nimage-to-image translation network\",\"19\":\"conferences\\ntime series analysis\\ndecision making\\npredictive models\\nunmanned underwater vehicles\\ncognition\\ndata models\\ncausality\\nrobots\\ntime series\\nreasoning operational decisions\\ntime series causal inference\\nunderlying physical interaction\\noperational performance\\ncausal relationship\\nexplainable decision-making\\nnovel causal inference framework\\ndomain knowledge integration\\ndata-driven causal knowledge\\ntime series data\\nunderwater robot\\ncausal structure\\ninference model\\nmodel-free causal inference\\ncomplex environmental interactions\",\"20\":\"image color analysis\\nannotations\\nbrightness\\nswarm robotics\\nink\\ncameras\\nsensors\\nbar codes\\nimage colour analysis\\nimage recognition\\nlearning (artificial intelligence)\\nmobile robots\\npose estimation\\nposition control\\nrobot programming\\nrobot vision\\nsatellite navigation\\nhuecode\\ndifferent colored layers\\nperspective information\\naruco marker\\nqr code\\nmeta-marker\\npose information\\ngnss-denied environment\\nmachine learning\",\"21\":\"location awareness\\nvisualization\\nconferences\\ncomputational modeling\\nkinematics\\nrobustness\\nland vehicles\\nmobile robots\\npath planning\\nrobot dynamics\\nrobot kinematics\\nrobot vision\\nslam (robots)\\nsplines (mathematics)\\nnonholonomic kinematic constraints\\nvehicle motion\\nnonholonomic ground vehicles\\nvision-based localization\\nsmart ground vehicles\\nvision-based mapping\\n6dof bundle adjustment\",\"22\":\"location awareness\\nlaser radar\\nsystematics\\nfeature extraction\\nrobot sensing systems\\nprediction algorithms\\nrobustness\\ninertial navigation\\nmobile robots\\noptical radar\\nsensor fusion\\nrobust srif-based lidar-imu localization\\nmultisensor fusion architecture\\nautonomous vehicle applications\\ncentimetre-level accuracy\\nrobust point-cloud feature matching\\naccurate point-cloud feature matching\\nhighly discriminative features\\nlidar point clouds\\nhigh frequency motion prediction\\nnoise propagation\\non-manifold imu pre-integration\\nmultiframe sliding window square root inverse filter\\nfusion algorithm\\nlidar-imu system\",\"23\":\"visualization\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nautomation\\nconferences\\npose estimation\\nrobot vision systems\\ncameras\\ndistance measurement\\nfeature extraction\\nimage reconstruction\\nmobile robots\\nrobot vision\\nslam (robots)\\nstereo image processing\\nstraight lines\\nstructure reconstruction\\nminimum parameterized representation\\nray-point-ray structures\\nrpr features\\nintersection constraints\\nreconstruction system\\nray-point-ray features\\ncamera pose estimation\\nstraight line features\\nvisual slam\\n3d reconstruction systems\\nparallel constraint\\ncoplanar constraint\\nnovel intersection constraint\",\"24\":\"deep learning\\ncosts\\nconferences\\ndecision making\\ntraining data\\ncomputer architecture\\nreinforcement learning\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nrobotic control tasks\\nagent\\ndeployment-time inference control costs\\ndeep reinforcement learning agents\\nasymmetric architecture\\nchallenging decision making\\nrequired inference costs\\ndeep neural networks\\nhigh energy-consuming computations\\ndrl methods\\nenergy-limited platforms\\ncomputationally expensive policy\",\"25\":\"training\\nuncertainty\\nconferences\\nestimation\\nreinforcement learning\\npredictive models\\nbenchmark testing\\ndeep learning (artificial intelligence)\\nmulti-agent systems\\nmodel-ensemble exploration\\nmodel-based deep reinforcement learning\\nmodel-ensemble method\\noptimistic exploration\\nweighted exploitation\\nexpected accumulative return\\nexpected return\\nmodel uncertainty\\nmodel predictive error propagation\\nsample complexity\\nmodel-ensemble exploitation\",\"26\":\"training\\nservice robots\\ncrops\\nreinforcement learning\\nrobot learning\\ndecoding\\ntrajectory\\nimage segmentation\\nneural nets\\nrobot programming\\ndecoder-free extension\\nlatent state-space models\\nvariational autoencoder\\nlatent trajectory imagination\\nobject vanishing\\ndreamer\\nmodel free reinforcement learning method\\ninfomax\\ndreaming\\nimage region\",\"27\":\"adaptation models\\nuncertainty\\ntrajectory tracking\\nscalability\\nprocess control\\nprobabilistic logic\\ncomplexity theory\\nbayes methods\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmixture models\\nnonparametric statistics\\npolynomials\\nregression analysis\\ntrajectory control\\nvariational techniques\\nvariational infinite mixture\\nprobabilistic inverse dynamics learning\\nprobabilistic regression\\nrobotics applications\\ndata-driven adaptability\\ncomputational efficiency\\nregressors\\nbayesian nonparametric mixtures\\nvariational bayes inference\\nprobabilistic local polynomial models\\ndata-driven complexity adaptation\\ndiscontinuous functions\\nheteroscedastic noise\\nreal-world inverse dynamics datasets\\ninfinite mixture formulation\\nlocal learning\\nmodel complexity\\nonline inverse dynamics control\\ncertainty quantification\\nbarrett-wam manipulator\\ntrajectory tracking performance\\nhierarchical local regression\\ninverse dynamics control\\nfully generative models\\ndirichlet process mixtures\",\"28\":\"training\\nadaptation models\\nuncertainty\\nstochastic processes\\noptimal control\\nreinforcement learning\\ntools\\nbayes methods\\ncontrol engineering computing\\nlearning (artificial intelligence)\\noptimisation\\nrandom processes\\nrobot dynamics\\nrobot kinematics\\ndomain randomization\\ndynamics system\\ndr\\nmodel-based dynamics\\npolicy learning\\nstochastic dynamics\\nlocally linear dynamics\\nmodel-based optimal control\\npolicy optimization\\ndeep bayesian locally linear embedding\\nrobot applications\",\"29\":\"navigation\\nwheelchairs\\nrobot sensing systems\\nmathematical models\\ntrajectory\\ndata mining\\nconvolutional neural networks\\ncollision avoidance\\nhandicapped aids\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\npath planning\\ndynamic environments\\nfully convolutional neural network\\npath extraction\\nunnecessary re-plannings\\nglobal path\\nimitation learning based path planner\\nautonomous wheelchair\\nreal-world dynamic pedestrian environment\\nstatic obstacles avoidance\\ndynamic obstacles avoidance\\ndeep imitation\\nautonomous navigation\\ndynamic pedestrian environments\\nsocially compliant manner\\nautonomous vehicles\\nunnatural vehicle\\npedestrian navigation\\nsocial conventions\\nend-to-end path\",\"30\":\"planing\\nreinforcement learning\\ngames\\nprobabilistic logic\\nsearch problems\\nplanning\\ncomplexity theory\\nlearning (artificial intelligence)\\npath planning\\ntrees (mathematics)\\nstate-of-the-art reinforcement\\nhigh sample complexity\\nsparse reward case\\ncontrol policies\\nexpert demonstrations\\ndemonstration discovery\\ndiscovering demonstrations\\nrapidly-exploring random tree\\ndemonstration trajectories\\ngeneric rl algorithm\\np2d2\\nsampling complexity\",\"31\":\"legged locomotion\\nlevel set\\nconferences\\nhumanoid robots\\noptimal control\\nkinematics\\nrobustness\\nmotion control\\nnonlinear control systems\\npath planning\\npendulums\\nposition control\\nreachability analysis\\nrobot dynamics\\nreachability-based push recovery\\nvariable-height\\npendulum model\\nzero-step capturability\\nhamilton-jacobi reachability analysis\\nsub-zero level set\\nvalue function\\nhj variational inequality offline\\nvhip state\\nzero-step capturable\\nhj reachability analysis\\noptimal control law\\npush recovery online\\nposition-controlled humanoid robot\\nubtech walker robot\\npush robustness\",\"32\":\"fluctuations\\ntorque\\ntracking\\ntorque control\\ndynamics\\nkinematics\\nangular velocity\\nacceleration control\\nangular velocity control\\nlegged locomotion\\nmotion control\\nrobot dynamics\\nrobot kinematics\\nrotational centroidal orientation\\ncentroidal momentum\\ndynamics theory\\nlocomotion robots\\nfloating base\\ncentroidal instantaneous orientation\\ntotal system angular inertia\\ncentroidal angular inertia\\nwhole-robot rotational motion\\ncentroidal frame orientation parameters\\nadjacent control loops\\ncentroidal instantaneous frame\\ncentroidal angle rate\\ncentroidal angular velocity\\nwhole-body torque control\\ncentroidal angular motion\\nsystem design\\nrobotics communities\\nmultibody floating locomotion systems\\nprincipal axes\\nacceleration level\\neuler angle\\nfirst-order kinematics level\\nmotion generation\",\"33\":\"robotic assembly\\nshape\\nfixtures\\nmetals\\nglass\\nsoft robotics\\nusability\\nassembling\\ncad\\ngrippers\\nsoft jig\\nsoft-jig-driven assembly operations\\ngeneral-purpose assembly robot system\\nassembly parts\",\"34\":\"damping\\nsymbiosis\\nperformance evaluation\\nelectric potential\\ncollaboration\\nkinematics\\nelectroencephalography\\nhuman-robot interaction\\nmedical robotics\\nmedical signal processing\\nprediction-error negativity\\nphysical human-robot collaboration\\nstable interaction dynamics\\nevent-related potentials\\npen\\nhuman preference\\ncomplex phrc singularity\\nelectroencephalogram\\neeg\\nexponentially damped least-squared method\\nedls\",\"35\":\"automation\\nconferences\\ntactile sensors\\nskin\\nfabrics\\nclassification algorithms\\nconvolutional neural networks\\ncontrol engineering computing\\nconvolutional neural nets\\nhaptic interfaces\\nhuman-robot interaction\\nmicrophones\\nnonverbal manner\\ntactile stimulus\\npassive body parts\\ndynamic tactile sensor\\nsocial interaction\\nsparsely distributed microphones\\nsparsely embedded microphones\\nhuman-robot tactile communication\\nlarge area robotic skin\\ntdoa source localisation\\nconvolutional neural network\",\"36\":\"training\\nfeature extraction\\ntrajectory\\ntopology\\nspatiotemporal phenomena\\nplanning\\nvehicle dynamics\\ncomputational complexity\\ncomputer vision\\nimage motion analysis\\nimage sequences\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nobject detection\\nobject tracking\\npedestrians\\nposition control\\nroad vehicles\\ntraffic engineering computing\\nstar topology\\nrobust trajectory forecasting\\ndynamic scene\\nmotion prediction\\nmultiple agents\\nintelligent monitoring\\nautonomous driving\\ncomplex interactions\\nsurrounding scene\\naccurate trajectory prediction\\nrobust trajectory prediction\\nmultiple intelligent agents\\nego-agent\\nsurrounding high definition map\\ngiven observed trajectories\\nstar computational topology\\nspatiotemporal interaction features\\ncurrent interaction features\\ntime complexity scales\\nprediction method\\nsurrounding agents\",\"37\":\"training\\ntorque\\nservice robots\\nshape\\nfasteners\\nconcrete\\ntask analysis\\nbrittleness\\nconstruction industry\\nfriction\\nindustrial robots\\nlearning (artificial intelligence)\\nneural nets\\nsurface finishing\\nwalls\\nhigh friction coefficient\\ndeep neural network\\nsurface finish\\nwall surface\\npeg-in-hole task strategy\\nindustrial robot\\nreinforcement learning\\nbrittle nature\\ncontrol parameter tuning\",\"38\":\"learning systems\\nthree-dimensional displays\\nconferences\\nloading\\nexcavation\\nsoftware systems\\nhardware\\nexcavators\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nneural task planner\\ntask planner - tasknet\\ndata-driven method\\nfeasible task-level sequence\\ndemonstration data\\nhigh-level excavation objective\\ntasknet planner\\ntask primitives\\nexcavation trace generator\\nterrain\\nexcavator simulator\\nimitation learning-based methods\\ntask decomposition strategies\\nresulting sequences\\nexcavator motion planner\\nfeasible joint-level trajectories\\nstate-of-the-art autonomous excavator hardware\\nsoftware system\\n49-ton autonomous excavator\\nmaterial loading tasks\",\"39\":\"automation\\nnavigation\\nconferences\\nboats\\npropulsion\\npredictive models\\nturning\\nhydrodynamics\\nmarine propulsion\\nmarine safety\\nmechanical stability\\nposition control\\npropellers\\nships\\nsteering systems\\nvehicle dynamics\\nship turning circle manoeuvre\\nrudder roll stabilization\\nrrs\\nship stabilization\\ncommercial vessels\\nsingle propeller-rudder system\\nsteering mechanisms\\nturning circle test manoeuvre\\nboats dynamic\\nsteering induced roll quantification\\ndifferential jet pump system\\ndjps\\nsprs\\nsteering induced roll\\nthrust asymmetry\\nturning circle test\\nrudder roll stabilisation (rrs)\\ndifferential jet pump system (djps)\\nsingle propeller-rudder system (sprs)\",\"40\":\"training\\nindustries\\nuncertainty\\nshape\\nservice robots\\ntraining data\\ngrasping\\ndeep learning (artificial intelligence)\\nsupervised learning\\nuser interfaces\\nuser-specified target masses\\ngranular foods-coffee beans\\ngrasp point candidates\\ndata collection system\\ndeep neural network\\nshort time-span\\nfood packing industry workers\\nuncertainty-aware self-supervised target-mass grasping\",\"41\":\"conferences\\nneural networks\\ngrasping\\npredictive models\\nrobot sensing systems\\nstability analysis\\nsensors\\nconvolutional neural nets\\ngrippers\\nmanipulator dynamics\\nmechanical stability\\ntactile sensors\\nsct-cnn\\nspatio-channel-temporal attention cnn\\ngrasp stability prediction\\ntactile sensing\\nrobotic manipulation\\nrobust robotic grasping\\ntactile data matrix\\ntemporal attention mechanism\\nsct attention cnn\\nspatio-channel-temporal attention convolutional neural networks\\nspatial-channel mechanism\",\"42\":\"legged locomotion\\nenergy consumption\\nshape\\nconferences\\nforce\\nkinematics\\nsteel\\nmotion control\\npermanent magnets\\nrobot kinematics\\nshapes (structures)\\ninverted locomotion\\nmultilegged robot\\npermanent magnet\\nsteel surface\\nspherical magnetic joint mechanism\\nfoot flexibility\\nrobot gait\\nrobot feet\\nleg-pulling kinematics\",\"43\":\"costs\\nthumb\\nprototypes\\ngrasping\\nthree-dimensional printing\\nmanufacturing\\ntask analysis\\ncontrol engineering computing\\ndesign engineering\\ndexterous manipulators\\nend effectors\\ngrippers\\npublic domain software\\nopen-source mechanical design\\nanthropomorphic alaris robotic hand\\nlow-cost design platform\\nhand design\\ntwo-phalange adaptive thumb designs\\nfunctional robotic hand prototype\\nhand low manufacturing cost\\n6-dof anthropomorphic robotic hand\\n6-dof anthropomorphic alaris robotic hand\\nlinkage-based three-phalange finger\\nnonbackdrivable worm-and-rack transmission mechanisms\\n3d printing technology\",\"44\":\"adaptation models\\ntorque\\nbiological system modeling\\nhumanoid robots\\nprocess control\\nmuscles\\naerospace electronics\\nbiomechanics\\nbiomimetics\\nmanipulator dynamics\\nmuscle\\noptimisation\\nrobot dynamics\\noperational space task\\nbiomimetic operational space control\\nmusculoskeletal humanoid optimizing\\nmuscle activation\\njoint nullspace\\nforce-based operational space controller\\nphysical musculoskeletal humanoid robot arm\\nbiomimetic hill-type muscle model\\njoint torque nullspace\\noptimization process\",\"45\":\"automation\\nconferences\\nnickel\\nmagnetic fields\\ntask analysis\\nperpendicular magnetic anisotropy\\nsubstrates\\nactuators\\niron compounds\\nmicrorobots\\nmobile robots\\nmotion control\\nmulti-robot systems\\nnanoparticles\\nnanorods\\nrobot dynamics\\nswarm intelligence\\nnanorod swarm\\nnanoparticle swarm\\nfe3o4 nanoparticle swarms\\nmacroscopic swarm behaviour\\nparallel actuation\\nsmall-scale agents\\nnickel nanorods\\nmagnetic anisotropy\\nmagnetic field oscillation\\nparallel locomotion\",\"46\":\"industries\\nservice robots\\nmicrofabrication\\nelastic recovery\\nmetals\\nfailure analysis\\npins\\ndesign engineering\\nelasticity\\nmanipulators\\nmicromanipulators\\nplastic deformation\\nsensors\\nsheet metal processing\\nsoldering\\nsolders\\ntin alloys\\nelectronic soldering industry\\nhigh-precision soldering\\nimage-based pin alignment control method\\nactive plastic deformation\\nmetal pin\\nsolder joint\\nimage-based pin alignment controller\\npractical modified robotic manipulation system\\nalignment error\\nsoft object\\nrobotics\\nrobotic micromanipulation\\nactive pin alignment\\nsize 20.0 mum\",\"47\":\"costs\\nautomation\\nconferences\\nreinforcement learning\\nbenchmark testing\\naerospace electronics\\nrobot sensing systems\\ndeep learning (artificial intelligence)\\nrobots\\nsearch problems\\ncandidate observation spaces\\nunnecessary observation channels\\nmanually designed observation spaces\\nobservation space matters\\noptimization algorithm\\ndeep reinforcement learning\\nchallenging control problems\\nreal-world robotic tasks\\ndeep rl algorithms\\ncommon design choices\\nsearch algorithm\\noptimal observation spaces\",\"48\":\"monte carlo methods\\nautomation\\nconferences\\ndecision making\\ngames\\nswitches\\nhistory\\ncomputer games\\nlearning (artificial intelligence)\\ntree searching\\nfast system 1\\nintuitive system 1\\nslower but more analytical system 2\\nsystem 0\",\"49\":\"correlation\\nmeasurement uncertainty\\ngaussian processes\\nbenchmark testing\\nrobot sensing systems\\nbayes methods\\nrandom processes\\ncontrol engineering computing\\ninfinite horizon\\nlearning (artificial intelligence)\\nmean square error methods\\nmobile robots\\nrobot dynamics\\nuncertain systems\\nsingle-output ihgp\\nmultioutput infinite horizon gaussian processes\\nuncertain dynamical environments\\nonline learning\\nnoisy sensory measurement streams\\nnonstationary dynamical random processes\\nrobotics\\nhyperparameters\\ncomputational cost reduction\\nmoihgp\\nrmse\\nbayesian way\",\"50\":\"adaptation models\\ntrajectory planning\\natmospheric modeling\\nestimation\\nunmanned aerial vehicles\\nsafety\\nindoor environment\\naerospace control\\nautonomous aerial vehicles\\ncollision avoidance\\nhelicopters\\nmobile robots\\nnavigation\\npath planning\\nreachability analysis\\nremotely operated vehicles\\nsplines (mathematics)\\ntrajectory control\\nquadrotor trajectory planning\\nwidely adopted unmanned aerial vehicles\\nsignificant adverse factor\\npotential safety issues\\nnarrow confined indoor environments\\nindoor ego airflow disturbance\\ndifferent quadrotors\\ndisturbance model\\ndifferent reconstructed complex environments\\nestimated disturbances\\nsafe trajectory planning\\nmultiple quadrotor platforms\\ndifferent indoor environments\",\"51\":\"surface reconstruction\\nthree-dimensional displays\\nsemantics\\ninspection\\nreal-time systems\\nunmanned aerial vehicles\\npath planning\\nautonomous aerial vehicles\\ncloud computing\\ncollision avoidance\\ndistance measurement\\nimage segmentation\\nmobile robots\\nobject recognition\\noptimisation\\nremotely operated vehicles\\nreal-time active target detection\\nreal-time semantic segmentation\\npoint cloud data\\nsemantic attributes\\nsurrounding obstacles\\nuavs power inspections\\ntime active detection\\nsegmentation method\\neuclidean signed distance fields\\nesdfs\",\"52\":\"adaptation models\\ntrajectory planning\\nheuristic algorithms\\npredictive models\\nprediction algorithms\\nrobustness\\nplanning\\naerospace computing\\nautonomous aerial vehicles\\ncollision avoidance\\ncontrol engineering computing\\nhelicopters\\nmobile robots\\npath planning\\npredictive control\\ninstantaneous motion tendency\\nadaptive trajectories\\ndynamical feasible local trajectories\\nplanning framework\\nadvanced environmental adaptive planning algorithm\\neva-planner\\nenvironmental adaptive quadrotor planning\\nsuperior agility\\nsafe motions\\nflight smoothness\\ndenser environments\\nenvironmental adaptive planner\\nflight aggressiveness\\nobstacle distribution\\nenvironmental adaptive safety aware method\\nsurrounding obstacles\\nenvironmental risk level\\nsafe trajectories\\nmulti-mpcc\",\"53\":\"automation\\nnavigation\\nconferences\\nextraterrestrial measurements\\noptimization\\nimage reconstruction\\nimage registration\\noptimisation\\ntrees (mathematics)\\n3-d registration\\naccurate navigation\\nscene reconstruction\\nalignment method\\npoint-cloud registration\\ntime-differential information\\nmeasured points\\nmultidimensional optimization\\nanalytical solution\\nregistration accuracy\\nk-d trees\",\"54\":\"uncertainty\\nheuristic algorithms\\nconferences\\ngaussian processes\\nrobot sensing systems\\npath planning\\ninference algorithms\\nbayes methods\\nmobile robots\\nrobot dynamics\\nmultimodal perception uncertainties\\nsafe path planning problem\\nmultimodal gp-regulated uncertainties\\nunified uncertainty\\nsafe path planning\\nmulti-modal perception uncertainties\\nautonomous mobility\",\"55\":\"training\\nnavigation\\ndynamics\\nreinforcement learning\\npredictive models\\nlaser modes\\ndata models\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\npath planning\\nrobot vision\\nsocially compliant manner\\nworld transition model\\nsocially aware robot navigation\\ndynamic pedestrian environments\\nautonomous mobile robots\\nmodel-based reinforcement learning approach\\ncrowded environments\\nnavigation policy\\ninteraction data\\nmultiagent simulation\\nvirtual data\\ndeep transition model\\nsocial conventions\\npolicy model\\nlaser scan sequence\\nlaser sequence\\nstacked local obstacle maps\\nstatic obstacles\\ndynamic obstacles\\nmodel training\\nsocial navigation tasks\\nmultiple social scenarios\\nlearned policy\",\"56\":\"measurement\\nuncertainty\\nminimally invasive surgery\\nnavigation\\npredictive models\\nprobabilistic logic\\nsearch problems\\nestimation theory\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nobject tracking\\npedestrians\\nprobability\\nspatiotemporal phenomena\\nvideo signal processing\\nprobabilistic dynamic crowd prediction\\ncrowd behaviour\\nrobotic social navigation\\nhuman society\\nfundamental problem\\ndynamic environment\\npedestrian crowd\\nmacroscopic properties\\nspatial representation\\nconvolutional recurrent model\\nprobabilistic loss function\\nspatio-temporal crowd prediction\\nsocial invasiveness metric\\nglobally-optimal plans\\nrealistic pedestrian simulator\\nsocial navigation approach\",\"57\":\"base stations\\nautomation\\nrobot kinematics\\nconferences\\nsafety\\nmonitoring\\ndistributed control\\nmobile robots\\nmulti-robot systems\\nconsensus-based control barrier function\\nswarm control\\nccbf\\ndistributed system\\nconsensus filter\\ncontrol inputs\\nactual robots\\nmultiple robots\",\"58\":\"deep learning\\ntraining data\\nprobabilistic logic\\nmanipulators\\nrobustness\\nbayes methods\\nsafety\\nlearning (artificial intelligence)\\nmobile robots\\nbayesian disturbance injection\\nrobust imitation learning\\nflexible policies\\nmultiple seemingly optimal actions\\nstandard imitation learning\\nexpert actions\\npoor generalizability\\nimitation learning framework\\nbayesian variational inference\\nflexible nonparametric multiaction policies\\nintroducing optimizing disturbances\\nricher demonstration dataset\\ncombinatorial approach forces\\nstable multiaction policies\\nur3 6-dof robotic arm\\nimproved flexibility\\nlearning performance\\ncontrol safety\",\"59\":\"protocols\\nnavigation\\nheuristic algorithms\\nconferences\\nprototypes\\nrouting\\nplanning\\nmobile robots\\nmulti-robot systems\\npath planning\\nrespective physical region\\ncells interact\\nafada\\nmodular cells\\ndynamic environment\\nmultirobot navigation\\nactive modular environment\\nrobot-environment interaction\\nnavigation tasks\\nactive environment\\nlifelong planning\\nreservation protocol\",\"60\":\"deep learning\\nautomation\\nconferences\\nreinforcement learning\\ncomputer architecture\\nnumerical simulation\\ntiming\\ndeep learning (artificial intelligence)\\nmulti-agent systems\\nevent-triggered architecture\\nfeedback controller\\ncommunication input\\ntriggering mechanism\\nevent-triggered control policies\\nmultiagent deep deterministic policy gradient\\ntransport performance\\ncommunication savings\\ndeep reinforcement learning\\nevent-triggered communication\\nmultiagent reinforcement\\ncontrol strategies\\ntypical end-to-end deep neural network policies\\nfixed-rate communications\",\"61\":\"adaptation models\\nheuristic algorithms\\nperturbation methods\\ndecision making\\nrobustness\\nresource management\\ntask analysis\\nconvergence\\ngame theory\\nmulti-agent systems\\nmulti-robot systems\\noptimal control\\nmultirobot task allocation games\\ndynamically changing environments\\ndecision-making algorithm\\noptimal stationary task allocation\\nmultirobot trash collection application\\nconvergence analysis\",\"62\":\"monte carlo methods\\nautomation\\nrobot kinematics\\nsurveillance\\nconferences\\nrobot sensing systems\\nsensors\\nmobile robots\\nmulti-robot systems\\npath planning\\ntree searching\\nupper confidence bound\\nsimultaneous exploration\\nheterogeneous multirobot systems\\nfunctionally specialised robots\\nenvironmental information\\nscout-task robot architecture\\nscout-task coordination\\nmultidrone surveillance scenario\\nscout robots\\ntask robots capture detailed information\",\"63\":\"automation\\nsurveillance\\nconferences\\ntask analysis\\nmulti-agent systems\\nroad traffic control\\ntraffic management\\nsumo\\ntime period based patrolling\\ntpbp\\ntime period requirement\\npatrolling algorithm\\nagent online\\nprioritized patrolling problem\\npre-specified time period\\nprioritized locations\\nidleness value\\nliterature concentrate\\nsurveillance task\\ncrucial feature\\nmultiple agents\",\"64\":\"automation\\nnavigation\\nconferences\\ndynamics\\nstochastic processes\\npredictive models\\nprobabilistic logic\\ncollision avoidance\\nmobile robots\\nprobability\\nprobabilistic prediction\\npedestrian future movements\\ndynamic environments\\nstochastic process anticipatory navigation\\nnonholonomic robots\\npredictive model\\ncontinuous-time stochastic processes\\nanticipated pedestrian positions\\nchance constrained collision-checking\\ntime-to-collision control problem\\nprobabilistic collision-checking\\ncrowded simulation environments\\nreal-world pedestrian dataset\",\"65\":\"legged locomotion\\nknee\\ntraining\\nmeasurement units\\nthigh\\nestimation\\npredictive models\\nbiomedical measurement\\ngait analysis\\nhuman-robot interaction\\nkalman filters\\nmean square error methods\\nmedical computing\\nmedical robotics\\nmotion estimation\\nmotion sensors\\npatient monitoring\\npatient rehabilitation\\nregression analysis\\nrobotic gait training\\ngait measurement\\nhuman-robotic interaction\\nrobotic walkers\\nrehabilitation devices\\nhorizontal motion\\nhuman walking\\ninertial measurement units\\nhuman lower limbs motion estimation\\nshanks motion\\nshank-mounted imus\\nthighs motion\\npose prediction model\\nmultiple linear regression\\nkalman filter\\nroot-mean-square error\\nknee joint angle estimation\\nhip position estimation\\nankle position estimation\\nknee position estimation\\nlower limbs angular motion\",\"66\":\"costs\\nthree-dimensional displays\\nneural networks\\ncomputer architecture\\nmanipulators\\nrobot sensing systems\\nplanning\\ncollision avoidance\\ncomputational geometry\\nmobile robots\\nneural nets\\nrobot vision\\nstereo image processing\\nsupervised learning\\nhigh dimensional motion planning\\nc2g-hof networks\\nmanipulator motion planning\\nc2g-hof architecture\\nneural network\\ncost-to-go function generating networks\\ncost-to-go function higher order function network\\n3d point cloud\\n2d image\\ncollision checking\\n7dof manipulator arm\",\"67\":\"tracking\\nheuristic algorithms\\nconferences\\ndynamics\\noptimal control\\nmachine learning\\nkinematics\\nmobile robots\\nmotion control\\npath planning\\nrobot dynamics\\nrobot kinematics\\ntrees (mathematics)\\nkinodynamic constraints\\nsmooth curved trajectory\\nnonholonomic robots\\nplanning tests\\nsmooth-rrt*\\nasymptotical optimal motion planning\\nrapidly-exploring random tree methods\\nrrt methods\\nmobile robot\\nmotion planning\",\"68\":\"dynamics\\nprogramming\\nplanning\\nnonlinear dynamical systems\\ntrajectory\\nsafety\\nnumerical models\\napproximation theory\\ndiscrete systems\\ninteger programming\\nmanipulators\\nnonlinear programming\\npath planning\\ntemporal logic\\ntrajectory control\\ntrajectory planning\\ncontinuous optimization based tamp\\ntask and motion planning\\nmixed integer problems\\ndiscrete task specifications\\ncontinuous motion planning\\nhigh-level task specification\\npick-and-place tasks\\nrobotic sequential manipulation\\nsignal temporal logic specifications\\ncontinuous optimization\",\"69\":\"learning systems\\nautomation\\nconferences\\nreinforcement learning\\nbenchmark testing\\nminimization\\ntask analysis\\ndeep learning (artificial intelligence)\\nminimisation\\nrelative pearson divergence\\ndeep reinforcement learning\\nstable learning\\nproximal policy optimization\\nppo-rpe\\nlearned policy\\nppo\\nmachine learning for robot control\\ndeep learning methods\",\"70\":\"motion segmentation\\nconferences\\nkinematics\\ndeburring\\nmanipulators\\nend effectors\\npartitioning algorithms\\nindustrial manipulators\\nmanipulator kinematics\\nmulti-robot systems\\nredundant manipulators\\nend-effector\\nintermittent segments\\nsingle nonredundant manipulator\\njoint-space coverage continuity\\nclassical coverage strategies\\nindustrial manipulator\\nfinite locations\\ntask-space\\nmanipulator motions\\ncontinuous coverage path\\njoint-space continuity\\nsingle location\\nconveyor belt\\nminimum discontinuity nonrevisiting coverage task\\noptimal object placement\",\"71\":\"trajectory planning\\nheuristic algorithms\\nconferences\\ndynamics\\naerospace electronics\\ntrajectory\\nspace exploration\\ncollision avoidance\\ngraph theory\\nmobile robots\\npath planning\\nsearch problems\\nonline trajectory planning\\ncar-like robots\\nhighly dynamic environments\\nsearch-based partial motion planner\\nfeasible trajectories\\nnear-time-optimal trajectories\\nstate graph\\nmotion primitives\\nefficient path searching algorithm\\nfast collision checking algorithm\\nmoving obstacles\\nrelative motions\\nrobot\\nfast searching\\nstate-time space\\nnear-time-optimal solutions\",\"72\":\"manifolds\\nconferences\\ncomputational modeling\\nkinematics\\nsystem recovery\\nmanipulators\\nplanning\\ncollision avoidance\\nconvex programming\\nmanipulator dynamics\\nmobile robots\\nmulti-robot systems\\nlow-dimensional decomposed task-space regions\\nhigh-dimensional constrained configuration space\\nrobot team\\nmultirobot loco-manipulation\\nclosed-chain kinematic constraints\\nlower-dimensional singularities\\ndual-resolution motion planning framework\\nconvex task region decomposition method\\ntask-space decomposed motion planning framework\",\"73\":\"wireless communication\\nautomation\\nheuristic algorithms\\nconferences\\ncharging stations\\ntrajectory\\nbatteries\\nbattery chargers\\ncomputability\\ninductive power transmission\\nmobile robots\\noptimisation\\noptimal trajectories\\nstatic charging stations\\nsmt-based optimal deployment\\nautonomous mobile robots\\nindoor robotic application\\nmobile wireless rechargers\\nsatisfiability modulo theory solving problems\",\"74\":\"costs\\nheuristic algorithms\\nconferences\\ncomputational modeling\\nmemory management\\nswitches\\nfeature extraction\\ncomputational complexity\\ngraph theory\\nmobile robots\\npath planning\\nsearch problems\\nfast replanning multiheuristic\\nsimulating numerous path\\nnovel path replanning algorithm\\nexpanded vertices\\nprevious search\\nfeature vertices\\nprevious path\\nadjacent vertices\\nproper additional heuristic functions\\nshared multiheuristic\",\"75\":\"costs\\nautomation\\ntrajectory planning\\nscalability\\nconferences\\nminimization\\nstability analysis\\ncollision avoidance\\nhelicopters\\noptimisation\\npolynomials\\ntrajectory control\\nquadrotor trajectory planning\\npolynomial trajectory\\nenergy minimization\\nlinear-complexity scheme\\nparameter gradient evaluation\\nenergy optimal trajectory\",\"76\":\"legged locomotion\\nautomation\\ndesign methodology\\nconferences\\nforce\\nmetals\\nsoft robotics\\ndesign engineering\\nforce control\\nfracture\\nrigidity\\nrobot dynamics\\ntensile strength\\nforce design\\nactive self-healing tension transmission system\\ntendon-driven legged robot\\nself-healing function\\ndamage management\\nhigh-load robot applications\\nlife-sized stiff robots\\nunleaked liquid-assisted healing\\nbenchtop module test\\nself-healing sequence\\ntendon-driven monopod testbed\\nimpact fracture\",\"77\":\"bellows\\nshape\\nscalability\\nprototypes\\nbending\\nend effectors\\ntask analysis\\nmanipulator dynamics\\nmanipulator kinematics\\nrigidity\\ntorsion\\norigami-reinforced parallel continuum robot\\nend effector\\ncross-routing tendons\\nparallel curves\\nparallel backbones\\norigami shell\\nplanar quintic pythagorean hodograph curves\\ntranslational parallel continuum robot\\ntorsional stiffness\\nshape reconstruction\",\"78\":\"performance evaluation\\ntorque\\nautomation\\nconferences\\npower transmission\\nhumanoid robots\\npayloads\\nlegged locomotion\\nmotion control\\nrobot kinematics\\nrotation angle formula\\n3-dof coupled tendon-driven waist joint\\ntorque transmission formula\\ntendon-driven mechanism\\nhuman waist\\njoint rotation\\n3m3d tendon-driven structures\",\"79\":\"integrated optics\\nactuators\\nupper bound\\nforce\\nmodulation\\nprototypes\\nrehabilitation robotics\\ncantilevers\\ndesign engineering\\nelasticity\\nhuman-robot interaction\\nmedical robotics\\npatient rehabilitation\\nposition control\\nsprings (mechanical)\\nnonbackdrivable actuator\\nvertical equilibrium position\\nspring parameters\\ndesired range\\nadjuster\\nspring stiffness\\nideal cantilever support model\\nsoft spring\\nbeam deflection model\\ndifferent spring thicknesses\\nvariable-stiffness spring mechanism\\nimpedance modulation\\nphysical human-robot interaction\\nunsupported-length cantilever leaf spring\\nvariable stiffness actuators\\ninteraction force\\nelastic component\\nsupporting structure\\nactuation unit\\ndesired stiffness\\n1-translational degree\\nfreedom body weight support system\\nrehabilitation robot\\nleaf spring mechanism\\nstiffness modulation\\nspring deflection\",\"80\":\"legged locomotion\\nautomation\\nequalizers\\ngears\\nconferences\\nwheels\\nstairs\\ndesign engineering\\nmobile robots\\nmotion control\\nrollers (machinery)\\nvehicles\\nomni-directional vehicles\\ncrank leg\\nmecanum-wheeled vehicle\\nmecanum crank\\nrough-terrain locomotion\\nholonomic vehicles\\nrough terrain locomotion\\ndesign exhibited superior capability\\nlongitudinal wheel direction\\nlongitudinal locomotion experiments\\nmecanum wheel diameter\\nlateral locomotion\\nnovel omni-directional vehicle\\nomni-directional locomotion capability\\nimproved rough terrain vehicle functionality\\nrough terrain vehicles\\nlateral direction\",\"81\":\"shape\\nconferences\\nforce\\nposition control\\nrobot sensing systems\\nsensors\\nplanning\\nattitude control\\ndexterous manipulators\\nmobile robots\\npath planning\\npolygonal objects\\nposition-controlled robot hands\\norientation control\\nsensorless in-hand caging manipulation\\nmotion planning algorithm\\nplanar in-hand caging manipulation\",\"82\":\"legged locomotion\\nstate feedback\\nsolid modeling\\nthree-dimensional displays\\ncomputational modeling\\ndynamics\\npredictive models\\nhumanoid robots\\nmotion control\\nnonlinear control systems\\npendulums\\nrobot dynamics\\n3d biped locomotion control\\nseamless transition\\nwalking running\\n3d zmp manipulation\\nbiped robots\\nzmp three-dimensionally\\nactual ground profile\\nbody height\\naerial phase\\nfoot-guided controller\\nhighly uneven terrains\\nmotion reference\",\"83\":\"location awareness\\ncorrelation\\ntime difference of arrival\\nestimation\\ninterference\\ntransforms\\nrobot sensing systems\\ncorrelation methods\\ndirection-of-arrival estimation\\nfeature extraction\\ninterference suppression\\nreverberation\\nrobots\\nspeech processing\\ntime-of-arrival estimation\\nspeech-oriented attention\\nrobotic sound source localization\\nrobotic audition\\nrobotic system\\nsound source localization techniques\\nhuman speech\\nsound sources\\neffective tdoa estimation\\nspeech signals\\nspeech fundamental frequency\\nspeech mask\\ngcc-phat-sm\\nsound source localization experiments\\nrobotic platform\\ntime difference of arrival estimation\",\"84\":\"global navigation satellite system\\nconferences\\nkinematics\\nreceivers\\nposition measurement\\nreal-time systems\\nreflection\\nelectromagnetic wave reflection\\nfiltering theory\\ngraph theory\\noptimisation\\nradio receivers\\nsatellite navigation\\ngnss positioning\\nreal-time kinematic\\nfactor graph optimization\\nglobal navigation satellite systems\\nautonomous systems\\nurban canyons\\ngnss measurements\\nconventional filtering-based method\\ntime-correlation\\nfiltering-based estimator\\nunexpected outlier measurements\\nfactor graph-based formulation\\nformulated factor graph framework\\ngnss receiver\\nnonminimal state estimation\",\"85\":\"location awareness\\ntraining\\nvisualization\\nconferences\\nrobot vision systems\\nneural networks\\nfocusing\\ncameras\\nfeature extraction\\nimage colour analysis\\nimage registration\\nlearning (artificial intelligence)\\nneural nets\\npose estimation\\nrobot vision\\naccurate point-to-point correspondences\\ncamera relocalization\\ndeep point cloud generation\\nhand-crafted feature refinement\\nvisual localization\\nhand-crafted feature based methods\\nrelocalization process\\nrelocalization framework\\ncoarse localization process\\nsparse point cloud\\nhand-crafted feature space\\nregressing camera pose\\nransac\\nrgb-d data source\\nneural network\",\"86\":\"three-dimensional displays\\nnavigation\\noctrees\\ngraphics processing units\\nray tracing\\nmaintenance engineering\\nprobabilistic logic\\napplication program interfaces\\ncomputational geometry\\ncomputer graphics\\nmobile robots\\npath planning\\nrobot vision\\nset theory\\nprobabilistic volumetric mapping\\n3d environmental map\\nautonomous robotic navigational task\\ngpu-based ray shooting\\ngraphics api\\noctree\\nray-tracing graphics hardware\\nvolumetric voxel grids\\nray-tracing rtx gpu\\nreal time photorealistic computer graphics\\noctomap cpu implementation\\naxis-aligned bounding boxes\\nmassively parallel ray shooting\",\"87\":\"visualization\\nlighting\\nsensor systems\\nrobustness\\nreal-time systems\\nsensors\\nindoor environment\\nimage sensors\\nindoor navigation\\nmobile robots\\nposition control\\nstereo image processing\\nultra wideband communication\\nuvip\\nrobust uwb\\nvisual-inertial positioning system\\ncomplex indoor environments\\nindoor positioning\\ncomplex scenes\\naccurate positioning\\nrobust positioning\\nvisual-inertial odometry\\npl-svio\\npositioning accuracy\\nline features\\nimage patch features\\nviewpoint variation\\nvisual sensor\\nuwb sensor\\nvisual-inertial system\\nreal-time positioning\\ncomplex indoor scenes\\naccurate relocalization\",\"88\":\"location awareness\\nvisualization\\nlaser radar\\nsimultaneous localization and mapping\\nthree-dimensional displays\\npose estimation\\ntwo dimensional displays\\nfeature extraction\\nimage matching\\nimage registration\\nimage sensors\\nmobile robots\\noptical radar\\nrobot vision\\nslam (robots)\\nlidar-based initial global localization\\nmobile robotics\\nnavigation initialization\\ninitial global localization methods\\n2d submap projection image\\ninitial global localization module\\nsuitable initial estimate\\nfrequency 2.4 hz\\nsize 1.2 m\",\"89\":\"measurement\\nlaser radar\\nautomation\\nconferences\\nestimation\\nperformance gain\\ntuning\\nbayes methods\\ndistance measurement\\nfiltering theory\\nlearning (artificial intelligence)\\noptical radar\\noptimisation\\nparameter estimation\\nsearch problems\\nblack-box lidar odometry\\nlidar odometry algorithms\\nhyper-parameters\\nsub-optimal parameter\\nautomatic hyper-parameter tuning approach\\nlidar odometry estimation\\nsequential model-based optimization approach\\nhyper-parameter set\\nblack-box odometry estimation algorithm\\nlidar data augmentation approach\\ndifferent odometry estimation algorithms\\noptimized parameter set exhibits superior performance\",\"90\":\"three-dimensional displays\\nlaser radar\\nsimultaneous localization and mapping\\nestimation\\ntransforms\\nfeature extraction\\nrobustness\\nimage recognition\\nimage representation\\nmobile robots\\noptical radar\\npose estimation\\nrobot vision\\nslam (robots)\\nnonlocal constraints\\nsimultaneous localisation\\nplace recognition method\\n3d lidar point clouds\\nlarge-scale environments\\nextracting information\\nencoding topological information\\ntemporal information\\nauxiliary information\\nplace description\\nrobust scene representations\\ndiscriminative scene representations\\nsecond-order pooling\\nmultilevel features\\nfixed-length global descriptor\\nlidar-based place recognition\\nhigher-order pooling\\nglobally consistent map\\ntrajectory\",\"91\":\"solid modeling\\nlaser radar\\nthree-dimensional displays\\nshape\\nconferences\\nmeasurement by laser beam\\nrobot sensing systems\\ncalibration\\noptical radar\\nstereo image processing\\nautomated extrinsic calibration\\nrange offset correction\\narbitrary planar board\\nautomatic accuracy- enhanced extrinsic calibration method\\nsingle planar board\\ntarget objects\\nmassive point clouds\\nautomated planar board detection\\nlidar range images\\ntarget board\\ntarget completion method\\nrange measurements\\nconstant offset values\\nrange offset model\\nlidar calibration methods\\nbi-directional point- to-board distances\",\"92\":\"legged locomotion\\ntarget tracking\\nrobot kinematics\\nartificial neural networks\\nrobot sensing systems\\nturning\\nreal-time systems\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nmotion estimation\\nneural nets\\nposition control\\nthree-term control\\npredicted position\\nhri field\\nhuman movements\\nmachine learning\\nperson-following\\npredicted future motion\\nfirst-person perspective\\nhuman skeleton\\nmobile robot\\ncog\\nhuman motion\\nwalking human\\nbody joints\\nneural network\\ncenter of gravity\\nproportional-integral-derivative controller\\npid controller\\ntime 0.5 s\",\"93\":\"monte carlo methods\\nautomation\\nconferences\\nmarkov processes\\ncognition\\nreal-time systems\\nplanning\\nbelief maintenance\\ncognitive systems\\ndecision theory\\ngame theory\\ninference mechanisms\\nmobile robots\\nmotion control\\nplanning (artificial intelligence)\\nactive reasoning\\nhumans\\nhuman-centered robot\\ncognitive limitation\\npotential irrationality\\nhuman partner\\nseamless interactions\\nanytime game-theoretic planner\\nreasoning models\\npartially observable markov decision process\\nchance-constrained monte-carlo belief tree search\\nrobot behavioral planning\\nbounded intelligence\\nbehavioral planner\\nanytime game-theoretic planning\",\"94\":\"torque\\nuncertainty\\nfriction\\nobservers\\nrobot sensing systems\\nsensors\\nsystem identification\\ncollision avoidance\\nmobile robots\\nmotion control\\nrobot model\\nprecise dynamics model\\nmomentum observer-based collision detection\\nlstm\\nmodel uncertainty\\ncollision prevention algorithms\\nmob\\ndisturbance torque\\nestimated disturbance\\nmodeling error\\nlong short-term memory\\npurely applied external torque\",\"95\":\"deep learning\\nautomation\\nconferences\\nmixed reality\\nbandwidth\\ntrajectory\\nfeeds\\nlearning (artificial intelligence)\\ntelerobotics\\nvirtual reality\\nautocomplete teleoperation\\nnumerous degrees\\nfreedom users\\nperception bandwidth\\nuser intended motion\\nestimated trajectories\\ndeep learning network\\nmotion primitives\\nestimated motions\\nsuggested motion\\nmixed reality teleoperation scheme\\nteleoperation methods\",\"96\":\"visualization\\ncodes\\nautomation\\nconferences\\npose estimation\\nbenchmark testing\\ngraph neural networks\\nfeature extraction\\ngeometry\\ngraph theory\\nimage classification\\nimage matching\\nimage representation\\nimage sequences\\nlearning (artificial intelligence)\\nneural nets\\npattern clustering\\nspatial context\\ngraph neural network\\nimage-based multiperson pose estimation\\ndetected keypoints\\nperson instances\\ncurrent grouping approaches\\nlearned embedding\\nvisual features\\nspatial configuration\\nhuman poses\\ngrouping task\\ngraph partitioning problem\\naffinity matrix\\ngeometry-aware association gnn\\nspatial information\\nlocal affinity\\nlearned geometry-based affinity\\nappearance-based affinity\\nrobust keypoint association\\npose instances\\nappearance-only grouping frameworks\\nrobust grouping\",\"97\":\"three-dimensional displays\\nshape\\nannotations\\ntraining data\\nmanuals\\nrendering (computer graphics)\\ngenerative adversarial networks\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\nimage texture\\nmanipulators\\nobject recognition\\nrobot vision\\nsolid modelling\\nautomatic hanging point learning\\nrandom shape generation\\nphysical function validation\\nrobotic hanging manipulation\\nhanging points\\nrandom shape generator\\nrandom simulation environment\\ncategory object\\ngan\\n3d models\\ndynamics simulation\\nrendering\\nrandom-textured objects\\ndeep neural network\",\"98\":\"location awareness\\nmagnetic sensors\\nconferences\\ntraining data\\nrobot sensing systems\\nreal-time systems\\nsteel\\ncontrol engineering computing\\nconvolutional neural nets\\ngraph theory\\nrobots\\nself-adjusting systems\\nsensor arrays\\nfreeform modular robot\\nmagnetic sensor array\\nfreeform msrr module\\nfreebot module\\nmagnetic field density\\nsteel spherical shells\\ngraph convolutional network\\nfreebot system\\nmodular self-reconfigurable robotic systems\\nconfiguration detection system\",\"99\":\"training\\nrobot motion\\nanalytical models\\ntransforms\\npredictive models\\nrobot learning\\ntrajectory\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\ntrajectory control\\ndiabolo-string system\\nmotion capture\\nskilled play\\noptimal control\\nrobot trajectories\\ndesired diabolo trajectory\\nrobot motions\\nrobot system\\nanalytical diabolo model\\nrobotic learning\\ntraining agents\\ndual robot arm system\\nanalytical model\",\"100\":\"training\\ndeep learning\\nconferences\\nsemantics\\ncollaboration\\npsychology\\ndata processing\\ncloud computing\\ncognition\\ncomputer aided instruction\\ncontrol engineering computing\\ndata integration\\ndeep learning (artificial intelligence)\\neducational robots\\ngroupware\\nmobile robots\\nmulti-robot systems\\npeer-assisted robotic learning\\ncloud robotic systems\\ndata-driven deep learning\\ndata islands\\nparl\\ndata collaboration\\ndat network\\ndata augmentation\\nmultilocal robots\\nlearning effects\\ndata-driven collaborative learning\\ncognitive psychology\\npedagogy\\nsemantic computing\\ndata transfer\\nself-driving task\\ncars\",\"101\":\"automation\\nconferences\\nprototypes\\nswitches\\nkinematics\\ncontrol systems\\nmobile robots\\nlegged locomotion\\nrobot kinematics\\nwheels\\nlywal\\nleg-wheel transformable quadruped robot\\nleg-mode\\nwheel-mode\\nclaw-mode\\n2-dof transformable mechanism\\nswitching-mode strategy\\nmobile modes\\ntransport functions\\npicking up functions\\nlocomotion\\nmechanical structure\\nkinematics calculation\\nmobile control\",\"102\":\"legged locomotion\\nautomation\\nsimulation\\nconferences\\nbuildings\\nprototypes\\ninterference\\nrobot dynamics\\nstair climbing capability-based dimensional synthesis\\nhexapod legged robot\\nlongitudinal body length\\ntarget staircase size\\nhexapod robot\\nmultilegged robot\\nmechanical design stage\\nvirtual simulations\",\"103\":\"legged locomotion\\nlimiting\\nperturbation methods\\nhumanoid robots\\nflywheels\\nquadratic programming\\ntask analysis\\nmotion control\\nnonlinear control systems\\npendulums\\npredictive control\\nrobot dynamics\\nnonlinear model predictive control approach\\nnonlinear inverted pendulum plus flywheel model\\ncop manipulation\\nvertical height variation\\nsequential quadratic programming\\nversatile locomotion\\ncenter of pressure\\ndivergent component of motion\\nnmpc scheme\",\"104\":\"laser radar\\nimage edge detection\\nurban areas\\nsupervised learning\\nneural networks\\ntraining data\\nrobot sensing systems\\nlearning (artificial intelligence)\\nmobile robots\\noptical radar\\npath planning\\nurban environments\\nlidar sensor-based traversability analysis\\nautonomous mobile robots\\ntraversable regions\\nspecific robot\\nlearning-based methods\\nexplicit training data\\ntraversability mapping\\nself-training algorithm\\nrobot-specific parameters\\nexisting supervised learning method\\nself-training approach-based traversability analysis\",\"105\":\"visualization\\ncomputational modeling\\nbuildings\\nsemantics\\ndiversity reception\\nhuman-robot interaction\\nreinforcement learning\\naudio-visual systems\\ncustomer satisfaction\\ndata mining\\nhumanoid robots\\ninteractive systems\\nman-machine systems\\nmobile robots\\npattern clustering\\nservice robots\\ngeneral greetings\\nbehavior patterns\\nend-to-end framework\\ntfvt-hri\\nframework extracts visual tokens\\ntransformer decision model\\nin-service receptionist robot\\nappropriate proactive behavior\\nsota end-to-end models\\nhumanness\\nproactive interaction framework\\nintelligent social receptionist\\nreceptionist robots\\nmultistage decision processes\\nend-to-end decision models\\nrule-based approaches\\nsedulous expert efforts\\nminimal pre-defined scenarios\",\"106\":\"current measurement\\noceans\\ncomputational modeling\\nsea measurements\\nestimation\\npath planning\\ncomputational efficiency\\nautonomous aerial vehicles\\nbayes methods\\nhydrodynamics\\nmobile robots\\nrecursive estimation\\nsingular value decomposition\\nspatially-correlated ocean currents\\nensemble forecasts\\nonline measurements\\ntwo-dimensional time-invariant oceanic flow fields\\ncomputationally efficient manner\\nmarine robotics\\nkernel methods\\nbasis flow fields\\nspatial correlations\\nocean current\\nmarine robots\\nrecursive bayesian estimation\\ncomputational analysis\\nreal-world ensemble data\\nmeasurement locations\",\"107\":\"automation\\nconferences\\nforce\\ndynamics\\nreinforcement learning\\nrobot sensing systems\\nhardware\\ndexterous manipulators\\nlearning (artificial intelligence)\\nlegged locomotion\\nmanipulator dynamics\\nmotion control\\ncircus anymal\\nquadruped learning dexterous manipulation\\nlimbs\\nquadrupedal robot\\nlocomotion tasks\\nmanipulation skills\\ndexterous manipulation abilities\\nanimal behavior\\ncircus ball challenge\\nmodel-free reinforcement learning approach\\ndeep policy\\nlight-weight ball\\ncontact measurement sensor\\nrandom disturbance force\\ndexterous dynamic manipulation\",\"108\":\"meters\\ntraining\\nservice robots\\nrobot vision systems\\nhuman-robot interaction\\ngesture recognition\\nuniversal serial bus\\nautonomous aerial vehicles\\ncameras\\nmobile robots\\nobject detection\\nattention-based ssd network\\nhuman-robot interaction field\\nlong-range hand gesture recognition\\nattention-based single shot multibox detector model\\nhand gestures\\nuavs\\nusb camera\\nself-built lrhg dataset\",\"109\":\"convolution\\nfrequency-domain analysis\\npredictive models\\nbenchmark testing\\nfeature extraction\\ngraph neural networks\\ntrajectory\\nconvolutional neural nets\\nfourier transforms\\ngraph theory\\nimage representation\\nmobile robots\\nmulti-agent systems\\nroad traffic\\ntraffic engineering computing\\ndynamic state information\\nenvironment graph\\ngraph fourier transform\\nspectral graph convolution\\ntemporal gated convolution\\nmultihead spatio-temporal attention mechanism\\nspectgnn\\ntrajectory prediction\\nmotion forecasting\\nautonomous vehicles\\nsocial mobile robots\\nautonomous agent\\nstatic environment\\ninter-agent correlations\\nspectral temporal graph neural network\\ncontext images\\ntraffic prediction\\ntraffic flow forecasting\",\"110\":\"visualization\\ncosts\\ncodes\\nautomation\\nconferences\\nneural networks\\nconvolutional neural networks\\nfeature extraction\\ngraph theory\\nimage representation\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\npattern classification\\nrobot vision\\nunsupervised learning\\ndark reciprocal-rank\\nteacher-to-student knowledge transfer\\ngraph-convolutional neural network\\nvisual robot self-localization\\ngraph-based scene representation\\nrobust methods\\ndiscriminative methods\\ncomputational storage costs\\ngraph classification problem\\ngraph convolutional neural network\\ngraph classification engine\\nvisual feature descriptors\\nstate-of-the-art self-localization systems\\ngraph node features\\noriginal self-localization system\\ngcn-based self-localization\\nnovel teacher-to-student knowledge-transfer scheme\\nrank matching\\nreciprocal-rank vector output\\noff-the-shelf state-of-the-art teacher self-localization model\\ndark knowledge\\nself-localization network\",\"111\":\"three-dimensional displays\\nconvolution\\nservice robots\\nfast fourier transforms\\ngraphics processing units\\nparallel processing\\nmanipulators\\ncomputational complexity\\nindustrial robots\\nmobile robots\\npath planning\\nreachability analysis\\nhigh-dof robotic manipulators\\nfast fourier transform\\nviewpoint selection\\n3d workspace\\nreachability map generation\\ncomputed reachability map\\nmultirobot plant phenotyping system\\nleverage gpu parallel computing\\nspecial spatial arrangements\\nplanar robots\\ngeneral joint arrangements\\nhigh computational costs\\nintra-planar convolutions\\ninterplanar integration\",\"112\":\"location awareness\\nvisualization\\ntorque\\ntrajectory tracking\\nbiological system modeling\\nposition control\\nmedical services\\ncancer\\nclosed loop systems\\nelectromagnetic actuators\\nfeedback\\nmedical image processing\\nmedical robotics\\nmotion control\\nstability\\nelectromagnetically actuated soft-tethered colonoscope\\npseudorigid-body model\\ncolorectal cancer incidence\\nmagnetic colonoscopes\\ncolon inspection\\nprecise orientation control\\nstable orientation control\\nfloating tether\\nsoft tether\\ntether deflection\\nclosed-loop control\\nprb model\\nmagnetic field model\\nvelocity 3.5 mm\\/s\",\"113\":\"instruments\\nveins\\nsurgery\\nswitches\\nuser interfaces\\ntools\\nretina\\nbiomedical engineering\\ndexterous manipulators\\neye\\nhuman-robot interaction\\nmedical robotics\\nmicromanipulators\\ni2ris\\nsher\\nintegrated system\\nrandomly-assigned targets\\nhigh-dexterity robotic assistant\\nmultiple directions\\nintegrated high-dexterity\\nintraocular micromanipulation\\nretinal surgeons\\nmultiple surgical instruments\\nconfined intraocular space\\nphysiological hand tremor\\ninstrument motion\\nsteady-hand eye robot\\ntremor-free tool manipulation\\ntremor-free manipulation\\nsurgical tools\\nstraight structure\\nrigid structure\\neye lens\\nanterior portion\\nsuitable direction\\nvein cannulation\\nsnake-like robots\\ngreater dexterity\\nintegrated dexterity\\nimproved integrated robotic intraocular snake\",\"114\":\"force measurement\\nforce\\nmuscles\\nrobot sensing systems\\ntime measurement\\nfeedback control\\nelectrical resistance measurement\\nactuators\\nbiomechanics\\nbiomems\\nmedical control systems\\nmicroactuators\\nmuscle\\nyoung's modulus\\nsoft sensor\\nskeletal muscle\\nactuation system\\ncontraction forces\\nbioactuator\",\"115\":\"surface reconstruction\\nthree-dimensional displays\\ntransportation\\nsurface morphology\\nacoustics\\nsystem-on-chip\\nimage reconstruction\\nacoustic streaming\\nbioacoustics\\nbiomems\\nbubbles\\ncellular biophysics\\nlab-on-a-chip\\nmicrochannel flow\\nmicrofluidics\\nmicromanipulators\\nbulky traditional acoustic driving system\\nportable acoustofluidic device\\nmultifunctional cell manipulation\\nbubble array\\narduino-based\\nmultiple bubble-induced microvortices\\nmultifunctional manipulation\\nnoninvasive manner\\nself-propelled transportation\\nsingle cells\\nmultiple cells\\ncontrollable trapping\\ndu145 cells\\nsingle microbubble\\nrotation direction\\ndriving frequency\\n3d cell reconstruction\\nout-of-plane rotation\\ncell structures\\nmicrobubble-induced\\non-chip micromanipulation\\nbiological applications\\nbiocompatible manner\\nexpensive driving system\",\"116\":\"legged locomotion\\ntorque\\nmanufacturing processes\\nplanets\\nmoon\\nshock absorbers\\nsafety\\naerospace robotics\\nimpact (mechanical)\\nmobile robots\\nplanetary rovers\\nrobot dynamics\\nsprings (mechanical)\\nsoft-landing control\\nsix-legged mobile repetitive lander\\nlunar exploration\\nautonomous robots\\nimmovable lander\\nrover\\nfolding landing\\nrepetitive soft-landing\\nlanding impact energy\\n5-dof lunar gravity testing platform\\nvertical landing velocity\\nhybrid compliant leg\\nstandalone active compliant leg\\n5-dof-lgtp\",\"117\":\"training\\nmanifolds\\ntarget tracking\\nautomation\\nheuristic algorithms\\nconferences\\ncomplexity theory\\nlearning (artificial intelligence)\\nreachability analysis\\nunsupervised learning\\nlatent exploration\\nself-supervised goal proposal\\nefficient policy learning algorithms\\nself-supervised approach\\noracle goal\\ndeep exploration\\nlong horizon plans\\nexploration framework\\nreachable states\\ncurrent frontier\\nexploration budget\\nreachable region\\nunderlying ground-truth states\\nreachable latent states\\ndistance-conditioned reachability network\\nspecified latent space distance\\ninitial state\\neasier goals\\nstart state\\ndifficult goals\\nself-supervised exploration algorithm\",\"118\":\"adaptation models\\ncomputer vision\\nadaptive systems\\ncomputational modeling\\nestimation\\nobject detection\\nfeature extraction\\nautonomous underwater vehicles\\ndeep learning (artificial intelligence)\\nimage enhancement\\nimage fusion\\nrobot vision\\nmultiscale feature maps\\nencoder-decoder model\\ncnns\\nsingle image depth estimation\\nsalience object detection\\nchannel attention\\naaf\\nlaffnet\\nlightweight adaptive feature fusion network\\nunderwater image enhancement\\ndeep convolutional neural networks\",\"119\":\"location awareness\\nultrasonic imaging\\nnavigation\\nmagnetic resonance imaging\\nsimulation\\nreal-time systems\\ndoppler effect\\nbiomagnetism\\nbiomedical ultrasonics\\nblood\\nblood flow measurement\\nblood vessels\\ncellular biophysics\\ndoppler measurement\\nhaemodynamics\\nmicrorobots\\nultrasound doppler imaging\\ncollective magnetic cell microrobots\\nmagnetic navigation\\ncollective cell microrobots\\nstem cells\\nexternal magnetic fields\\ntunable magnetic interaction\\ncollective motion\\ninduced blood flow\\ncollective pattern\\nred blood cells\\nultrasound waves\\ninduced doppler signals\\ninput field frequency\\nultrasound parameters\\nthree-dimensional blood flow\\nimaging plane\\ncollective microrobots\\ncollective control approach\\nmicrorobotics\\ncollective behavior\\nultrasound imaging\\nstem cell microrobot\\nbio-fluids\",\"120\":\"torque\\nautomation\\nfriction\\nconferences\\nforce\\nrotors\\ncontrol systems\\nautonomous aerial vehicles\\ndesign engineering\\nend effectors\\nforce control\\nhumanoid robots\\nlegged locomotion\\nmanipulators\\nmedical robotics\\nmobile robots\\nmotion control\\nposition control\\nquadratic programming\\nrobot dynamics\\nstability\\ntorque control\\ntrajectory control\\nend effector\\nmultilink aerial robot\\nmultirotor robots\\nrotor-distributed multilink robot\\nroot footplate\\narm module\\nroot-body perching motion tests\\nroot position\\nfixed-root aerial manipulator\\nmultilink aerial arm\\nadhere foot module\\nrotor thrust\\nprecise aerial manipulation\\nfixed-root approaches\\nrotor suction force\\narm-equipped multirotors\\naerial systems: motion control\\naerial manipulator\\nperching\\naerodynamics\",\"121\":\"torque\\nnavigation\\nmachine vision\\nsnake robots\\nrobot sensing systems\\nhardware\\nfeedback control\\ncollision avoidance\\nfeedback\\nfeedforward\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nrobot vision\\ndensely-cluttered environments\\ndecentralized state-of-the-art compliant controller\\nbi-stable dynamical system\\nautonomous decentralized shape-based navigation\\ndense environment navigation\\nproprioception-only feedback control\\nhigher level planner\\nfeedforward control\\nonboard vision system\",\"122\":\"costs\\nnavigation\\ncomputational modeling\\ntraining data\\nsurfaces\\nsampling methods\\nreal-time systems\\ncollision avoidance\\nlegged locomotion\\nmobile robots\\noptimisation\\npath planning\\ntime optimal navigation\\nlearned motion costs\\nterrain topographies\\noptimal solutions\\nintegrated framework\\nreal-time autonomous navigation\\nelevation maps\\nrapid global path planning\\noptimization\\nlocomotion capabilities\\ngpu-aided\\nsampling-based path planner\\ngradient-based path optimizer\\noptimal paths\\nneural network-based locomotion cost predictor\\ngpu-enabled hardware\\nreal-time deployment\\nmobile platforms\\nanymal c quadrupedal robot\\nmultiple complex terrains\",\"123\":\"pressure sensors\\nnavigation\\nforce\\npulse width modulation\\nunmanned underwater vehicles\\ncameras\\nreal-time systems\\nadaptive control\\nautonomous underwater vehicles\\nkalman filters\\nmobile robots\\nnonlinear filters\\noptimisation\\npath planning\\nstate estimation\\nhigh-performance autonomy\\nlevenburg-marquardt optimization\\nperspective-n-point problem\\nconsistent localization system\\noutlier removal\\nmeasurement noises\\nunknown navigation state estimations\\nadaptive controller\\nautonomous mobility\\nunderwater autonomous performance\\nminiature commercial uuv\\nautonomous navigation\\nfiducial markers\\nmonocular camera\\npressure sensor\\nadaptive unmanned underwater vehicles\\nbluerov2 heavy\\noptimal pose\\nekf\\nextended kalman filter\",\"124\":\"backstepping\\nconferences\\ncomputational modeling\\ndecentralized control\\nlow-pass filters\\nbandwidth\\nstability analysis\\nadaptive control\\nclosed loop systems\\ncontrol nonlinearities\\ncontrol system synthesis\\ndistributed control\\nnonlinear control systems\\nstability\\ntracking\\nvirtual commands\\ntracking performance\\ncommand filtered tracking control\\nhigh-order systems\\nlimited transmission bandwidth\\ntracking control problem\\nhigh-order distributed systems\\ncommunication bandwidth\\nevent-triggered control method\\nspecific events happen\\ncomputational complexity\\ncommand filters\\nvirtual control signals\\nmain design framework\\nn-th order nonlinear system\\nn command-cascaded first order subsystems\\nvirtual control commands\\nlow-pass filter\",\"125\":\"costs\\nthree-dimensional displays\\nautomation\\nnavigation\\nconferences\\nneural networks\\nreal-time systems\\nimage matching\\nneural nets\\nstereo image processing\\nreal-time stereo matching networks\\nmultiscale cost volumes cascade network\\nrobot navigation\\nexpensive computational cost\\nrunning time\\nmultiple 3d cost volumes\\nnovel cascade hourglass network\\ncost aggregation\\nmscvnet\",\"126\":\"automation\\nautonomous systems\\nsimulation\\nscalability\\nconferences\\nsearch problems\\noceanography\\nattitude control\\ncontrol engineering computing\\nmarine systems\\nmobile robots\\nmonte carlo methods\\nmulti-robot systems\\npath planning\\ntrees (mathematics)\\nunmanned surface vehicles\\nhierarchical mcts\\nmultiple low-cost\\nunderactuated floats\\nfully actuated surface vessels\\ncost-effectiveness\\ncoordination problem\\nhierarchical solution\\ndec-mcts algorithm\\nsolution defines\\nmultirobot marine systems\\nmultivessel multifloat problem\\nscalable multivessel multifloat systems\",\"127\":\"training\\nlearning systems\\nconvolution\\nlaw enforcement\\nscalability\\nrobot kinematics\\nheuristic algorithms\\ncollision avoidance\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nreinforcement learning\\nrobot programming\\ncurriculum learning\\nmapf problem\\nlarge-scale robotic systems\\ncollision-free policy\\ndeep q-learning\\nlong-horizon goal-oriented tasks\\nheuristic guidance\\ndistributed heuristic multiagent path finding\\ndecentralized policy learning\\ngraph convolution\\ndistributed system\",\"128\":\"spirals\\nautomation\\nconferences\\ncomputational modeling\\ntools\\naerospace electronics\\ndata structures\\nmobile robots\\nmulti-robot systems\\npath planning\\ngeometric folding pattern\\nrobot coverage path planning\\nspiral patterns\\nflexible traversal order\\nsampling operator\",\"129\":\"uncertainty\\ngrasping\\ngaussian distribution\\nsearch problems\\nmanipulators\\nplanning\\ntask analysis\\nfriction\\nmobile robots\\noptimisation\\npath planning\\ntelerobotics\\ntree searching\\nsearch algorithm\\nobject rearrangement\\nnonprehensile grasping\\nrearranged obstacles\\ntotal execution time\\ntree search-based task\\nmotion planning\\nobstacle rearrangement\\nrobot manipulator\\nprehensile planning\\nrearrangement actions\",\"130\":\"target tracking\\nlimiting\\ntrajectory planning\\nconferences\\nstochastic processes\\noptimal control\\nrobot sensing systems\\nfiltering theory\\ngaussian processes\\niterative methods\\nmobile robots\\noptimisation\\nrobot dynamics\\nsensors\\nsuboptimal control\\ntrajectory control\\ntree searching\\narbitrary unknown disturbances\\ntrajectory optimization\\nlinear gaussian target dynamics\\ndeterministic optimal control\\ntarget dynamic model\\nstatistical properties\\nabrupt maneuvers\\nanomalous misbehaviors\\nunknown input decoupled filter\\ntarget state\\nunknown input evolution tracking\\ninformativeness-based pruning\\nreduced value iteration\\nsensor trajectory planning\\nstochastic active information acquisition\\nsensing robots\\njumping disturbances\\nsystem faults\\nsystem attacks\\nsuboptimal solution\\nsearch tree\\nforward value iteration\\nsuboptimality performance guarantees\\nnumerical simulations\",\"131\":\"torque\\nmotion segmentation\\nrobot vision systems\\nclustering algorithms\\ntools\\nprediction algorithms\\nend effectors\\ncollision avoidance\\nmanipulators\\nmobile robots\\npath planning\\nmanipulator\\nend effector\\nenvironment monitoring\\nrobot-tool segmentation\\ncollision-free motion planning\\nkinect v2 rgb-d camera\\nworking environment\\ncamera occlusion problem\\npotential field algorithm\\nvirtual torque approach\\nsmoother avoidance motion\\nshorter avoidance motion\\nrobotic tool\\nreal-time obstacle avoidance control scheme\",\"132\":\"manifolds\\nautomation\\nconferences\\napproximation algorithms\\nplanning\\ntask analysis\\nrobots\\nend effectors\\nlearning (artificial intelligence)\\npath planning\\nsampling methods\\ntask constraints\\nnull-measure constraint manifold\\nconfiguration space\\nrejection sampling\\nlearning-based sampling strategy\\nconstrained motion planning problems\\ndeep generative models\\nconditional variational autoencoder\\nconditional generative adversarial net\\nconstraint-satisfying sample configurations\\nconstraint parameters\\nconstraint-satisfying samples\\navailable sampling-based motion planning algorithms\\nsampling accuracy\\nsampling distribution\\ndifferent constraint tasks\",\"133\":\"visualization\\nobject detection\\nfasteners\\ncontainers\\nrobot sensing systems\\nsensors\\nreliability\\ncognitive systems\\ncontrol engineering computing\\nfeature extraction\\ngears\\ninference mechanisms\\nmobile robots\\npath planning\\nproduction engineering computing\\nrobot vision\\nrobotic assembly\\nreliable world model\\naction-aware perceptual anchoring\\nreliable perception\\nperceptual information\\nlow-level object detection\\nobject permanence\\nuniversal robot\\ncognitive architecture\\nrealistic gearbox assembly task\\nhigh-level reasoning\\nsnitch localisation task\",\"134\":\"training\\ndeep learning\\nnavigation\\nheuristic algorithms\\ncost function\\nrobot sensing systems\\ntrajectory\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\npath planning\\ndynamic window approach\\nhuman imitating collision avoidance\\nautonomous navigation\\ncrowded environment\\nsensor occlusion\\ncomplex nature\\nabstract social interactions\\ncomplex environment\\neffective navigation method\\nlearning-based model-based methods\\nhuman imitation factor\\nhuman trajectory\\nactual environment\\nnavigation quality shares similar tendency\",\"135\":\"robot motion\\nservice robots\\nrobot kinematics\\ndynamics\\ngesture recognition\\nassistive technologies\\nprogramming\\nforce control\\nhandicapped aids\\nhumanoid robots\\nmanipulators\\nmotion control\\noptimisation\\nsign language recognition\\nmotion retargeting techniques\\nmotion retargeting method\\ndynamic movement primitives\\nmotion rhythm\\nrobot motion planning\\ndual-arm sign language motions\\nefficient programming method\\ndynamic movement primitive based motion\\nchinese sign language motions\\ncsl\\nabb yumi dual-arm collaborative robot\\nmotion retargeting\\ngraph optimization\\nleader-follower\",\"136\":\"location awareness\\nsimultaneous localization and mapping\\nlaser radar\\nthree-dimensional displays\\nautomation\\nconferences\\nsemantics\\ngraph theory\\nimage matching\\nimage segmentation\\nmobile robots\\noptical radar\\nrobot vision\\nslam (robots)\\nnovel semantic-aided lidar slam\\nnamed sa-loam\\nsemantic-assisted icp\\nplane constraint\\nsemantic graph-based place recognition method\\nloop closure detection module\\nglobal consistent semantic map\\nlidar-based slam system\\n3d semantic segmentation\\nsemantic information\",\"137\":\"limiting\\nmonte carlo methods\\nautomation\\nfiltering\\nconferences\\nmarkov processes\\nrobustness\\ndecision theory\\nmobile robots\\npath planning\\non-line pomdp\\ncontinuous observation spaces\\npartial obervability\\nplanning problems\\npartially observable markov decision process\\napproximate pomdp solvers\\non-line solvers\\nobservation space\\ndiscretised observation spaces\\nstate- of-the-art pomdp solvers\\nlabecop\\nlazy belief extraction for continuous observation pomdp\\nmonte-carlo-tree-search\\nparticle filtering\",\"138\":\"legged locomotion\\ntorque\\nautomation\\nsimulation\\nconferences\\nforce\\noptimal control\\nassistive robots\\nmotion control\\nnonlinear control systems\\npendulums\\nrobot dynamics\\nsprings (mechanical)\\nhuman running motion\\ncomplex motion\\nupper body\\nlower body\\nrunning efficiency\\nnatural running status\\nexternal assistant force\\nrunning expansion modeling\\nrunning expansion simulation\\npelvic rotation assist suit\\nadvanced spring-loaded inverted pendulum model\\nexpanded running status\\ntrunk rotation assist suit\",\"139\":\"fabrication\\nforce\\nposition control\\npneumatic systems\\nbending\\nsoft robotics\\nreal-time systems\\nclosed loop systems\\ncontrol system synthesis\\ndexterous manipulators\\npneumatic actuators\\nrigidity\\nshells (structures)\\npneumatic actuation-based bidirectional modules\\nvariable stiffness\\nclosed-loop position control\\npneumatic bending module\\nrotational motions\\ntranslational motions\\nantagonistic chambers\\nrigid shells\\nbidirectional actuation\\nstiffness tuning\\ncontroller design\\nreconfigurable robotic arm\\ndexterity\",\"140\":\"legged locomotion\\ntorque\\nthree-dimensional displays\\nautomation\\ndesign methodology\\nconferences\\nnumerical simulation\\ncontrol system synthesis\\nmetaheuristics\\nmotion control\\nnumerical analysis\\nstability\\nfoot placement adjustment\\nheuristic method\\nvirtual constraint control\\nfive-link underactuated planar\\nunderactuated bipedal walking\\npoincare map\\nstable walking\\ncontroller design\\ncapturability-based control framework\\nfoot-placement based control\\nnumerical simulations\",\"141\":\"manifolds\\nimage segmentation\\nsimultaneous localization and mapping\\npipelines\\nstreaming media\\nliquid crystal displays\\nreal-time systems\\ncomputational complexity\\nfeature extraction\\nimage matching\\nimage sensors\\nmobile robots\\nobject detection\\nrobot vision\\nslam (robots)\\nappearance-based loop closure detection\\nbidirectional manifold representation consensus\\nmapping system\\ntwo-stage pipeline\\nspatial geometric relationship\\nquery image\\ncandidates on-line\\nglobal semantic features\\nrobust geometric confirmation\\nloop-closing pairs\\nrobust feature matching algorithm\\nbmrc\\nlocal neighborhood structures\\nfeature points\\nclosed-form solution\\nsegment image streams\\ngeneral feature matching task\",\"142\":\"legged locomotion\\nsmoothing methods\\nconferences\\nmathematical models\\nbiology\\nsynchronization\\ncollision avoidance\\nbone\\ngait analysis\\nrobot dynamics\\nsprings (mechanical)\\ndeformation control\\nsprings\\nentrainment-based control method\\ntetrapod locomotion\\ntetrapod dynamic walking\\nfore parts\\nlocomotion control mechanisms\\nrigid spine condition\\nquadruped dynamic walker\\nflexible spine\\nquadruped robots\\nhigh-speed walking\\nrear parts\\nantiphase synchronization\\nquadruped walking robots\\nspine dynamics\\nsynergetic effect\",\"143\":\"connectors\\nportable computers\\nrobot vision systems\\npose estimation\\npneumatic systems\\nrobustness\\nsensors\\nassembling\\ncables (electric)\\ncameras\\nclamps\\ndexterous manipulators\\nelectronics industry\\nflexible electronics\\ngrippers\\nindustrial manipulators\\nproduction engineering computing\\nrolling capable\\nsoft fingertips\\nflexible flat cables\\nmodern electronics\\nautonomous robotic systems\\nsimple clamping mechanisms\\nsensing elements\\ncomplicated control laws\\nsuction cup based solution\\ncable connector insertion\\nparallel-jaw robot gripper\\nsoft clamping mechanism\\ncable surface\\nsoft fingertip structure\\ngripper base\\nelectronic components\\nrobust execution\\nffc manipulation\\nlocally-adaptive gripper\\npneumatically driven suction cups\\nshear forces\\ncamera recognition\\nffc grasping\\nassembly tasks\",\"144\":\"rails\\nanalytical models\\nmanufacturing processes\\nmaintenance engineering\\nrobot sensing systems\\nstability analysis\\nsensor systems\\ncompliance control\\ncontrol system synthesis\\nfission reactor decommissioning\\nmechanical contact\\nmobile robots\\nmotion control\\nnuclear power stations\\nremotely operated vehicles\\nrobot kinematics\\nstability\\ntelerobotics\\nhigh-radiation environment\\nautomated construction system\\nmodularized rail structure\\nremotely controlled robot\\nminimal sensors\\ncompliant mechanism\\nmultiple load conditions\\ncompliance-less construction\\nconnection mechanism\\nconnection surfaces\\nconnection success\\nconnection failure\\nconstructor robot\\nunmanned robots\\nstable module connection\\nsensorless module connection\\ncompliance-less module connection\\nfukushima dai-ichi nuclear power plant decommissioning\\nrobot movement\\ngeometrical model\\ncontact analysis\\nkinematic chain transition\",\"145\":\"automation\\nconferences\\nforce\\nnumerical simulation\\nstability analysis\\nend effectors\\nimpedance\\ncontrol nonlinearities\\nforce control\\nmanipulator dynamics\\nforce controller design\\nnumerical simulations\\nimpedance controller\\nadmittance controller\\ndof system\\ncontact stiffness\\nnonlinearities\\nrobotic manipulator dynamics\",\"146\":\"parallel robots\\nasymptotic stability\\nkinematics\\npath planning\\nstability analysis\\nplanning\\ncollision avoidance\\ncables (mechanical)\\ndexterous manipulators\\nmanipulator kinematics\\nmobile robots\\nsplines (mathematics)\\ntrees (mathematics)\\nshort resultant path\\nsmooth resultant path\\ncable-driven parallel robots\\nmotion planning\\ncable tensions\\noptimal path planning strategy\\nkinematic stability based afg-rrt path planning\\nobstacle avoidance\\nwrench capability\\nrobot dexterity\\nasymptotically optimal path finding method\\nrapidly exploring random trees\\ngilbert johnson keerthi algorithm\\ngoal biased artificial field guide\\ndirectional exploration\\npostprocessing algorithm\\nsplines\\nsix-dof spatial cdpr\",\"147\":\"automation\\nshape\\nmicroscopy\\nconferences\\nforce\\nprocess control\\nrobot sensing systems\\ncalibration\\nforce sensors\\nmicroassembling\\nmicromanipulators\\nposition control\\nrobot vision\\nmultiple objects\\nmultiple mutually effected contact surfaces\\nmultiple force sensors\\nmicrorobot manipulation strategy\\nassembly system\\nsimultaneous precision assembly\\ncoordinated microrobot manipulation\\nmicroscopic vision\\ninsertion control strategy\",\"148\":\"automation\\ntracking\\nconferences\\ndynamics\\nend effectors\\ndelays\\ntask analysis\\nbiomechanics\\ndexterous manipulators\\nmanipulator dynamics\\nmotion control\\nnonlinear control systems\\npredictive control\\ndynamic compensation\\nthrowing motion\\nhigh-speed robot hand-arm\\nmultifingered robot hands\\nend-effectors\\ndelicate tasks\\ndexterous tasks\\nhigh-speed movement\\nmultifingered hand-arms\\nmultifingered hands\\nhigh-response hand\\nlow-response arm\\npitching motion\\narm dynamics\\nusing nonlinear model predictive control\\nhand motion\\nball throwing\",\"149\":\"maximum a posteriori estimation\\ntarget tracking\\nmachine learning algorithms\\nrobot kinematics\\nsemantics\\nmachine learning\\nprobabilistic logic\\nboolean algebra\\ngradient methods\\ninference mechanisms\\nmobile robots\\ntemporal logic\\nprobabilistic inference\\nsignal temporal logic synthesis problem\\nrandom stl\\ndeterministic stl\\nrandom predicates\\nprobabilistic extension\\nsynthesis-as-inference approach\\ngradient-based synthesis\",\"150\":\"three-dimensional displays\\nnavigation\\nposition measurement\\nrobot sensing systems\\nreal-time systems\\ntrajectory\\nstate estimation\\nautonomous aerial vehicles\\nclosed loop systems\\ndistance measurement\\nglobal positioning system\\ninformation theory\\noptimisation\\npose estimation\\nposition control\\nrobot vision\\ntime-of-arrival estimation\\nultra wideband communication\\ninitialization approach\\nsimultaneous bias estimation\\nbias compensated uwb anchor initialization\\ninformation-theoretic supported triangulation points\\naccurate initialization\\nreference coordinate system\\nprecise subsequent uwb-inertial\\nraw distance measurements\\naerial vehicle poses\\nlinear distance-dependent bias term\\nestimation process\\n3d position estimates\\ninitial coarse position triangulation\\nrandom vehicle positions\\nroughly estimated anchor position\\nmaximal triangulation related information\\nfisher information theory\\ninformation theoretic optimal points\\nbias term estimation\\nuwb-inertial based closed loop control\\nprecise anchor initialization\\nultra-wide-band based navigation\",\"151\":\"uncertainty\\ncomputational modeling\\nmemory management\\npredictive models\\nrobot sensing systems\\ndata models\\npath planning\\ngaussian processes\\ngeophysical image processing\\nimage representation\\nimage resolution\\nmixture models\\nregression analysis\\nrobot vision\\nslam (robots)\\nterrain mapping\\ngaussian mixture model\\nmultiresolution terrain representation\\nplanetary emulation terrain\\nmultiresolution representations\\nlocal gaussian process regression\\nnoisy sensor data\\nterrain structures\\ndata ambiguity\\nterrain surface extraction\\nquarry\\nlarge-scale field terrains\\nelevation filtering\\nstatic kernel\",\"152\":\"location awareness\\nmagnetic field measurement\\nlaser radar\\nsimultaneous localization and mapping\\ndatabases\\nwheels\\nrobot localization\\ndistance measurement\\nmobile robots\\noptical radar\\nrobot vision\\nslam (robots)\\nintractable scenarios\\nglobal localization tasks\\nhighly similar geometrical structures\\ninsufficient distinctive features\\nlocalization solutions\\npre-deployed infrastructures\\nsingle sensor-based methods whose initialization module\\nunique information\\nnovel multisensor\\ntwo-step localization framework\\nmstsl\\nmobile robot global localization\\ngeometrically symmetric environments\\nmeasured magnetic field\\n2-d lidar\\nwheel odometry information\\npre-built magnetic field database\\nmultiple initial hypotheses poses\\ntwo-stage initialization algorithm\\nlidar-based localization\\nsymmetric environment\",\"153\":\"automation\\nroads\\nconferences\\nmarkov processes\\nrobustness\\nplanning\\nautonomous automobiles\\nautomobiles\\ndriver information systems\\nmobile robots\\npath planning\\nroad safety\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\nholistic understanding\\nroad rules\\ndriving styles\\ninteractive behavior planner\\nroad context\\nshort-term driver intent\\nlong-term driving style\\nspecialized partially observable markov decision process\\nadversarial driving scenarios\\nirrational drivers\\ntravel time-efficient autonomous driving\\nadversarial scenarios\\ninteractive planning\\nautonomous urban driving\\nhuman-driven cars\",\"154\":\"correlation\\nuncertainty\\nheuristic algorithms\\nprediction algorithms\\nparticle measurements\\ninference algorithms\\nunmanned aerial vehicles\\naerospace robotics\\nbayes methods\\nbelief networks\\ninference mechanisms\\nrecursive estimation\\nslam (robots)\\nkernel-based 3-d dynamic occupancy mapping\\nparticle tracking\\ndynamic environments\\n2-d static mapping\\nk3dom\\nstatic objects\\ndynamic objects\\ndynamic cells\\ndynamic mapping\\ndynamic occupancy mapping problem\",\"155\":\"geometry\\ntraining\\nvisualization\\nthree-dimensional displays\\nconvolution\\nshape\\ngrasping\\ndexterous manipulators\\nend effectors\\ngrippers\\nimage matching\\nlearning (artificial intelligence)\\npath planning\\nrobot vision\\nadaptive gripper-aware grasping policy\\nend- effector tools\\nsingle grasping policy\\ndifferent grippers\\npossible grasp poses\\ngrasp scores\\ncross convolution operation\\nscene geometry\\ngripper geometry\\ndifferent grasp poses\\nsuccessful grasp\\nmultigripper grasping policy method\\nadagrasp\\nrobots versatility\\nvisual observation\",\"156\":\"adaptation models\\nautomation\\nservice robots\\naggregates\\nconferences\\ntransfer learning\\nreinforcement learning\\nrobot dynamics\\nrobot programming\\nrobotic assembly\\nsoft robotic assembly\\nindustrial assembly scenarios\\nrobotic agents\\ntransfer reinforcement learning\\nmodel-based rl\\nhigh-level sample efficiency\\nstate-transition dynamics\\ncontact-rich peg-in-hole task\\nsoft robot\\ntransfer rl method\\ntransfer learning by aggregating dynamics models\",\"157\":\"training\\ndeep learning\\ncomputational modeling\\nlearning automata\\nconferences\\ntraining data\\ndata models\\ngraph theory\\nlearning (artificial intelligence)\\nneural nets\\ntemporal logic\\nflexible dnns\\nleverage prior temporal knowledge\\nembed symbolic knowledge\\nlinear temporal logic\\ndeep models\\nltl formula\\ngraph neural network\\nlearnt embeddings\\ndownstream robot tasks\\nsequential action recognition\\nimitation learning\\nsymbolic temporal knowledge\\ndeep sequential models\\ntime-series\\nactivity recognition\\ndeep neural networks\\neffective data-driven methodology\\nrelevant prior knowledge\\nstructured models\\nraw unstructured data\",\"158\":\"training\\ntechnological innovation\\nconferences\\nreinforcement learning\\nrobot sensing systems\\nsensors\\nstate-space methods\\ndeep learning (artificial intelligence)\\nmultimodal deep latent state-space model\\ndensity ratio estimator\\nself-supervised manner\\nmultimodal natural mujoco benchmarks\\nmultimodal mutual information training\\nrobust self-supervised deep reinforcement learning\\nrobust deep world models\\ndownstream tasks\\nmummi training\\nuseful world model learning\\ntable wiping task\",\"159\":\"training\\nlearning systems\\ngradient methods\\nmonte carlo methods\\nstochastic processes\\nreinforcement learning\\nlinguistics\\nlearning (artificial intelligence)\\nnatural language processing\\nneural nets\\nhuman motions\\nnatural language descriptions\\nmotion-to-language translation\\ngenerative adversarial seq2seq learning\\nsequence-to-sequence translation\\nwhole-body motions\\ntraining strategy\\nsequence generative adversarial nets\\ngenerator network\\npolicy gradient method\\nstochastic parameterized policy\\nmonte carlo search\\nfinal reinforcement learning reward\\ngenerative network\\nkit motion-language dataset\\nlinguistic descriptions\",\"160\":\"robot motion\\nautomation\\ntransportation\\nreinforcement learning\\nrobot sensing systems\\nmanipulators\\nproduction facilities\\nacceleration control\\nimage motion analysis\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobots\\nevolvable motion-planning method\\ndeep reinforcement learning\\ndistribution warehouse\\nlabor shortages\\ntransport operations\\ntransportation operation\\nevolvable robot motion-planning method\\noptimized acceleration control\\ntime-series data\\nsimulator environment\\ntransport time\\nmachine environment\",\"161\":\"robust control\\nrobotic assembly\\nvisualization\\nuncertainty\\nsystem performance\\nsemantics\\nreinforcement learning\\nassembling\\nindustrial manipulators\\nlearning (artificial intelligence)\\npath planning\\nposition control\\nlearning sequences\\nmanipulation primitives\\nskillful assembly\\ndynamic sequences\\nchallenging assembly tasks\\ndirect sim2real transfer\\nround peg insertion\",\"162\":\"location awareness\\nsystematics\\ncodes\\nnavigation\\nconferences\\nrobustness\\ntrajectory\\naerospace navigation\\naircraft control\\ncollision avoidance\\ndecentralised control\\ngradient methods\\nhelicopters\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nnonlinear programming\\nobject detection\\nrobot vision\\nrobust control\\ntrajectory control\\nonboard resources\\nplanning system\\ncollision risk\\nnonlinear optimization\\nlocal minima\\nsafe trajectories\\nsmooth trajectories\\ndynamically feasible trajectories\\nrelative localization drift\\nagent detection\\nego-swarm\\ncluttered environments\\nasynchronous systematic solution\\nmultirobot autonomous navigation\\ntrajectory sharing network\\nlightweight topological trajectory generation\\ngradient-based local planning\\nobstacle-rich scenes\\ndecentralized quadrotor swarm system\\nfully autonomous quadrotor swarm system\\ndecentralized systematic solution\\ndepth images\",\"163\":\"legged locomotion\\nconferences\\nforce\\nhumanoid robots\\nstability analysis\\nregulation\\nadmittance\\nmobile robots\\nrobust control\\nstability\\nrobust landing stabilization\\nhumanoid robot\\nuneven terrain\\nheel strike motion\\nhumanoid locomotion\\nimpulsive force\\nunexpected obstacle\\npost-contact reference\\nswing foot\\nhybrid admittance control\\nmomentum-based whole-body control framework\",\"164\":\"location awareness\\nvisualization\\nthree-dimensional displays\\nlaser radar\\nwheels\\nrobot sensing systems\\nmobile robots\\ncameras\\ndistance measurement\\nlegged locomotion\\nmanipulators\\nmonte carlo methods\\noptical radar\\nrobot kinematics\\nrobot vision\\nsensor fusion\\nstate estimation\\nhybrid wheeled-legged robots performing mobile manipulation tasks\\ngeneral state estimation framework\\nmultiple sensor information\\nstate estimator\\nwheel contact status\\ndrift-free state estimation\",\"165\":\"training\\nautomation\\nconferences\\nreinforcement learning\\nrobustness\\ntask analysis\\nrobots\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\nmanipulators\\nrobust control\\nadversarial skill learning\\ndeep reinforcement learning\\nrobotic manipulation\\ndisturbance-free environment\\nadversarial training mechanism\\nsoft actor-critic\\nsac\",\"166\":\"training\\nvisualization\\naffordances\\nfixtures\\ngrasping\\nrobustness\\ntask analysis\\ndeep learning (artificial intelligence)\\ngrippers\\nrobot vision\\nscene observation\\ndisturbing objects\\nvisual affordances\\nto-dqn\\nvisual affordance maps\\nrobot actions\\nsimulated robot manipulator\\nreal-robot experiments\\ngrasp objects\\nharnessing environmental fixtures\\nchallenging object grasping task\\nself-supervised learning approach\\nsingle parallel gripper\\nharnessing environment fixtures\\nheavy objects\\npartial observation\\nslide-to-wall grasping task\\ntarget-orientated deep q-network\\nperception for grasping and manipulation\\ndeep learning in grasping and manipulation\",\"167\":\"crowdsourcing\\ntraining\\nvisualization\\ngrasping\\nreal-time systems\\nplanning\\nobject recognition\\ncomputer games\\ndexterous manipulators\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nrobot vision\\nrobot perception\\ndexterous manipulation\\nmanipulation planning\\nunstructured environments\\ndynamic environments\\nmanipulated objects\\ndeep learning approaches\\nhuman perception\\nreasoning\\nobject classes\\ngamification\\nleverage human intelligence\\nattribute estimation aspects\\nattribute matching system\\ninitial attribute database\\nreal-time perception conflicts\\nautonomous robot grasping\",\"168\":\"performance evaluation\\nactuators\\neducational robots\\ngrasping\\nbiomechatronics\\npower electronics\\ngrippers\\ncomputer aided instruction\\ncomputer science education\\ncontrol engineering education\\neducational courses\\nmobile robots\\nteaching\\nrobotic gripper\\nhand design project\\nmanipulation competition\\ncourse assignment\\nstudent engagement\\nparticular assignment\\ngripper design project\\nengineering courses\\nappropriate hands-on activities\\nuseful task\\npath following tasks\\nproject outcomes\",\"169\":\"adaptation models\\nautomation\\nconferences\\nbiological system modeling\\nexoskeletons\\nprototypes\\nmathematical models\\nbiomechanics\\nmatlab\\nmedical robotics\\npatient rehabilitation\\nmodel based evaluation\\nlower-limb exoskeleton interaction\\nhuman user\\nadaptability\\nprototype variations\\nuser variations\\ntesting phases\\nmatlab-based toolbox\\nhuman model\\nlower limb exoskeleton\\nhuman joint torques\\ninteraction forces\\nexoskeleton design\\nvirtual testbed\",\"170\":\"sensitivity\\nautomation\\nheuristic algorithms\\nconferences\\noptimal control\\nrobustness\\ncomputational efficiency\\ndynamic programming\\nnewton method\\nrecursive estimation\\nriccati equations\\nmultiple-shooting method\\ninput torques\\noptimization variables\\ninverse dynamics\\nforward dynamics\\ndifferential dynamic programming\\noptimal control problems\\nrigid body systems\\nrecursive newton-euler algorithm\\nrnea\\nkarush-kuhn-tucker condition\\nriccati recursion\",\"171\":\"automation\\nabsorption\\nelectric shock\\nconferences\\nforce\\nwearable robots\\nmuscles\\nbone\\ngait analysis\\nhandicapped aids\\nimpact (mechanical)\\nmuscle\\northotics\\npowered exoskeletons\\nground reaction force\\nground impact\\nshock absorption mechanism\\nrepeated ground contacts\\nrepetitive impact forces\\nshank\\nparaplegia\\nwear comfort\\nwalkon suit\\nbones\",\"172\":\"image motion analysis\\ncomputer vision\\nvisualization\\nlaser radar\\nfuses\\npredictive models\\nfeature extraction\\ndeep learning (artificial intelligence)\\ndriver information systems\\nimage sequences\\nmobile robots\\nroad vehicles\\nvelocity control\\nflowdrivenet\\nend-to-end network\\nimage optical flow\\nlidar point flow\\nautonomous driving\\nbenchmark driver behavior dataset\\nlidar data\\nsteering angle\\nsequential visual data\\npoint clouds\\npolicy learning\\npoint feature extraction module\\nvehicle driving commands\\nflow-based method\\ndriving policy learning\\nvehicle speed control problem\\ndeep visual feature extraction module\",\"173\":\"location awareness\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nlaser radar\\nautomation\\nconferences\\ntask analysis\\nimage coding\\nimage matching\\nimage segmentation\\nmobile robots\\noptical radar\\nrobot vision\\nslam (robots)\\nslam-oriented 3d lidar point cloud online compression network\\npoconet\\nsimilar geometric features\\nslam scenarios\\npoint cloud segmentation\\ncomplex geometric information\\npoint-wise priority\\ncompression module\\nslam system\",\"174\":\"deformable models\\nsolid modeling\\nsurface reconstruction\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nshape\\ncolonoscopy\\nbiological organs\\nbiomedical optical imaging\\ncameras\\ncomputerised tomography\\nendoscopes\\nimage reconstruction\\nimage registration\\nimage segmentation\\nimage texture\\nmedical image processing\\nmotion estimation\\nneural nets\\nslam (robots)\\n3d reconstruction\\ndeformable colon structures\\npreoperative model\\ndeep neural network\\ncolonoscopy procedures\\ncolonic surface\\nfast camera motion\\nforward-viewing colonoscopies\\ntraditional simultaneous localization\\ncolon surfaces\\npreoperative colon model\\nmonocular colonoscopic images\\nembedded deformation graph\\nsegmented colon model\\nct scans\\ndnn\\nvisual odometry\",\"175\":\"robot motion\\nrobot kinematics\\ndynamics\\nnoise reduction\\nestimation\\nrobot sensing systems\\nreal-time systems\\nestimation theory\\ngeometry\\nmanipulator dynamics\\nmanipulator kinematics\\nmotion control\\npath planning\\nposition control\\nvelocity control\\nvelocity measurement\\ninertial sensors\\nserial revolute manipulator\\ninertial measurement unit\\nimu\\nrobotic geometric information\\ngeometric configuration\\njoint rotational velocity measurements\\ndynamic robotic motion\\ncollaborative manipulator\\nencoder-free joint velocity estimation\\nstatic robotic motion\",\"176\":\"crowdsourcing\\nadaptation models\\nadaptive systems\\nveins\\nroads\\nreinforcement learning\\nreal-time systems\\nadaptive control\\ndeep learning (artificial intelligence)\\nroad traffic\\nroad vehicles\\nd-acc\\nramp\\ndeep q-learning\\ndesired headway distance\\ncommercial vehicles\\ntraffic flow\\ntraffic conditions\\nintelligent acc system\\ntraffic dynamics\\noptimal headway distance\\ndynamic adaptive cruise control system\\ndeep reinforcement learning\\nvehicle-to- everything communication\\nnumerous traffic scenarios\\nhighway segment\\nvehicle automatically\\nsumo\\nv2x\",\"177\":\"wrist\\nvisualization\\nuncertainty\\nrobot vision systems\\npose estimation\\ntactile sensors\\ncameras\\nbayes methods\\ncalibration\\ncontrol engineering computing\\nforce control\\nforce sensors\\nindustrial manipulators\\nmotion control\\nobject recognition\\nproduction engineering computing\\nrobot vision\\nrobotic assembly\\nstate estimation\\nlow-precision sensors\\nindustrial assembly tasks\\ngrasped object\\nmanipulation tasks\\nvisual recognition\\ntactile sensing\\ntactile fingertip sensors\\noff-the-shelf camera\\nrobot wrist force sensors\\nbayesian state estimation framework\\nvisual observations\\nforce-torque sensor\\nmultimodal in-hand pose estimation\\nvisual information\\ncontact information\\nin-hand object pose\",\"178\":\"deformable models\\nrobotic assembly\\nrobot motion\\nsolid modeling\\nthree-dimensional displays\\nshape\\ncomputational modeling\\nassembling\\nassembly planning\\ncad\\ngenetic algorithms\\nmatrix algebra\\npareto optimisation\\ndeformable parts\\nassembly sequences\\nassembly sequence generation\\nasg\\ntradeoff objectives\\nassembled parts\\nmultiobjective genetic algorithm\\n3d computer-aided design-based method\\ntwo-part relationship matrices\\nother parts\\nscaled part shapes\\npareto-optimal sequences\\nmulticomponent models\\ngenerated sequences\",\"179\":\"training\\nuncertainty\\nheuristic algorithms\\npredictive models\\nrobot sensing systems\\nfeature extraction\\ncollaborative work\\ncomputer vision\\nimage fusion\\nintelligent transportation systems\\nknowledge representation\\nlearning (artificial intelligence)\\nvehicular ad hoc networks\\ndistributed dynamic map fusion\\nfederated learning\\nintelligent networked vehicles\\nsensing accuracies\\nmodel uncertainties\\ndata labels\\nfl algorithm\\nrepresentation learning networks\\nfields of view\\nknowledge distillation\\ncarla simulation platform\\nintelligent networked vehicle system\\nvehicle-to-everything communication\\ncamera images\\nimage based dynamic map fusion\",\"180\":\"analytical models\\nautomation\\nattitude control\\nconferences\\nprocess control\\nstability analysis\\nvehicle dynamics\\naerospace robotics\\nmobile robots\\nremotely operated vehicles\\nstability\\nunderwater vehicles\\nthruster rotation\\nrotational movements\\nmorphable aerial-aquatic quadrotor\\nvariable thruster angles\\naerial-aquatic multirotors\\naerial configuration\\nunderwater locomotion\\nrotational acceleration\\nlateral thruster components\\ncritical angles\\nunderwater stability\\ncoupled moment terms\",\"181\":\"legged locomotion\\nautomation\\nattitude control\\nconferences\\nprocess control\\nbirds\\ncontrollability\\naerospace robotics\\naircraft control\\nthreat approaches\\nornithopter robot\\nflapping robot\\nself-takeoff\\nground capability\\nlocomotion\\nlightweight design\\nmicro aerial vehicle\\nornithoper robot\\nbio-inspired robot\",\"182\":\"target tracking\\nsystematics\\ntrajectory planning\\nprediction methods\\nobject detection\\nbenchmark testing\\nunmanned aerial vehicles\\naerospace computing\\nautonomous aerial vehicles\\ncontrol engineering computing\\nmobile robots\\npath planning\\nposition control\\nrobot vision\\nsafe tracking trajectory\\nspatial-temporal optimal trajectory\\nonboard quadrotor system\\nreal-world tracking missions\\nfast-tracker\\nrobust aerial system\\nagile target tracking\\ncluttered environments\\nunmanned aerial vehicle\\ntracking trajectory planning\\ntarget informed kinody-namic searching method\\nuav\\ntarget motion prediction method\\nhierarchical workflow\\nback-end optimizer\",\"183\":\"training\\nnavigation\\ncomputational modeling\\nreinforcement learning\\ntools\\nrobot sensing systems\\nsafety\\ncontrol engineering computing\\nmobile robots\\npath planning\\nunsupervised learning\\nrobot navigation\\nunsupervised-learning-based architectures\\nlife robot\\nend-to-end methods\\npredictive unsupervised learning\\nunsupervised representations\\ndynamic human environments\\nnavrep\\nlatent features\\nopenai-gym-compatible environments\",\"184\":\"automation\\nconferences\\ncomputational modeling\\nprediction algorithms\\ngenerators\\nplanning\\ntrajectory\\ncollision avoidance\\ndrag\\ninteger programming\\npath planning\\npredictive control\\nquadratic programming\\nhigh-speed planning\\nmultirotors considering drag\\nplanning scheme\\nhigh-speed flight\\nunknown environment\\naccount drag forces\\nunfeasible trajectories\\nstate-of-the-art safe corridors generator\\nstate-of-the-art mapping algorithms\\nhigher computational efficiency\\nsimilar state-of-the-art methods\\nhigh-speed planner\\nsafe trajectories\",\"185\":\"parallel robots\\nfeedback loop\\nuncertainty\\nperturbation methods\\nstability criteria\\npose estimation\\nvisual servoing\\nactuators\\ncables (mechanical)\\nmanipulator dynamics\\nrobot kinematics\\nrobot vision\\nservomechanisms\\ncable-driven parallel robots\\ntension management\\ncomplex cable management\\ncable mass\\ncable slackness\\ncable slack\\ncdpr stiffness\\ntension correction algorithm\\nslackness-related trajectory perturbations\\nrigid links\",\"186\":\"design methodology\\nconferences\\nkinematics\\nsoft robotics\\nthree-dimensional printing\\nend effectors\\ntask analysis\\nactuators\\nindustrial robots\\nmanipulator kinematics\\nrapid prototyping (industrial)\\nrobotic system\\nunderactuated monolithic soft robotic mechanisms\\nautomated design process\\nend effector\\nrobot interactions\\ncompliant structures\\nadditive manufacturing process\\ntask-specific underactuated mechanisms\",\"187\":\"support vector machines\\nart\\nuncertainty\\nthree-dimensional displays\\npipelines\\nclustering algorithms\\ngrasping\\ndexterous manipulators\\ngrippers\\nimage segmentation\\nmotion control\\nobject detection\\nrobot vision\\ndeveloped kernelized synergies framework\\nvisual perception\\nunknown objects\\nobject grasping\\nsimplified perception pipeline\\nobject segmentation\\nrobotic manipulation tasks\\nrobot hand\\nmodified kernelized synergies framework\\nhuman inspired robotic manipulation\\nfine manipulation tasks\\nremarkable dexterity\\nsensorimotor organization\\nhuman behaviour\\nmultisensory information\\nautonomous object interaction\",\"188\":\"learning systems\\nvisualization\\ntarget tracking\\nneural networks\\ngrasping\\nrobustness\\nspace exploration\\nimage segmentation\\nmanipulators\\nmobile robots\\nrobot vision\\npushing actions\\nrobot experiments\\nstone sample collection\\ncomputing resource constraints\\nvision-based robotic pushing\\nobjects manipulation\\nrobust grasping\\nstone segmentation\\ngrasp hypotheses\",\"189\":\"parameter estimation\\nfriction\\npulleys\\nconferences\\nestimation\\nrobot sensing systems\\ntrajectory\\ndexterous manipulators\\nmechanical contact\\nsingle contact point\\nassembled hand\\nstatic friction model\\ndlr david hand\\nfriction estimation\\ntendon-driven robotic hands\\ncontact detection\\nspecial dedicated setups\\ndynamic friction model\",\"190\":\"three-dimensional displays\\nsystematics\\nautomation\\nconferences\\nstacking\\nreinforcement learning\\ntask analysis\\nrobots\\nhigh-dimensional environment observations\\nstructured representations\\ndata-efficiency\\nrobotics tasks\\nauxiliary tasks\\nobservability\\ndisentanglement\\nagent inputs\\ntask generation\\ntask-relevant aspects\\nsystematic understanding\\nlearned hand-engineered representations\",\"191\":\"interpolation\\ngeology\\nscalability\\nurban areas\\nparticle measurements\\nparticle filters\\ntrajectory\\ncartography\\ncomputer vision\\ndistance measurement\\nfeature extraction\\ngeophysical image processing\\nimage matching\\nmobile robots\\nparticle filtering (numerical methods)\\nrobot vision\\nurban environments\\nsuburban environments\\naerial images\\ngeolocated map tiles\\nshared low dimensional embedded space\\neuclidean distance\\nsimilarity measure\\nmap locations\\nvisual odometry\\naircraft\\nparticle filter\\nmap descriptors\\naerial imagery\\nglobal aerial localisation\\npurely vision based geolocation method\",\"192\":\"uncertainty\\nfiltering\\nsystem dynamics\\nestimation\\nrobot sensing systems\\nmathematical models\\ntrajectory\\nindoor radio\\nmobile robots\\npath planning\\nultra wideband communication\\nfinite number\\nglobal observability results\\nuwb indoor global localisation\\nnonholonomic robots\\nunknown offset compensation\\non-board sensors\\nmobile robot travels\\nuwb anchors\\nlocalisation performance\\ndiscrete-time formulation\\nultra-wideband beacons\",\"193\":\"location awareness\\nsurface reconstruction\\nlaser radar\\nthree-dimensional displays\\nmonte carlo methods\\nconferences\\nrobot sensing systems\\nimage reconstruction\\nmobile robots\\noptical radar\\nrobot vision\\nrange image-based lidar localization\\nmap-based localization\\nautonomous mobile systems\\nrange images\\n3d lidar\\nautonomous cars\\nlarge-scale outdoor environment\\ntriangular mesh\\npoisson surface reconstruction\\nmesh-based map representation\\ncurrent lidar scan\\nmonte carlo localization framework\\nlocalization performance\\nlocalization approach\\ndifferent lidar scanners\\nmobile system\\nlidar sensor frame rate\",\"194\":\"location awareness\\ndeep learning\\nlearning systems\\nrobot vision systems\\nurban areas\\nneural networks\\nradar\\ncw radar\\ndeep learning (artificial intelligence)\\nfm radar\\nmobile robots\\npose estimation\\nslam (robots)\\nfmcw radar\\nradar sensory data\\nfrequency-modulated continuous wave radar scans\\nend-to-end neural network\\n6-dof global poses\\noxford radar robotcar\\nradarloc\",\"195\":\"location awareness\\nmeasurement\\nthree-dimensional displays\\nuncertainty\\nconferences\\nneural networks\\nrobustness\\ncalibration\\ncomputer vision\\nlearning (artificial intelligence)\\nstereo image processing\\npractical 3d localization metric\\n3d human localization\\nstereo visions\\ncost-effective solutions\\nstereo cues\\nmonocular cues\\nmonocular perspective projection\\nhuman height distribution\\nkitti 3d metrics\\nself-driving cars\\nsocial robots\\nleft-right images\",\"196\":\"geometry\\nsolid modeling\\nimage segmentation\\nthree-dimensional displays\\nroads\\nobject detection\\nbenchmark testing\\ncameras\\ncomputer vision\\nfeature extraction\\nimage sequences\\nlearning (artificial intelligence)\\nvideo signal processing\\nenabling spatio-temporal aggregation\\nbirds-eye-view vehicle estimation\\nconstructing birds-eye-view maps\\nmonocular images\\ncomplex multistage process\\nseparate vision tasks\\nground plane estimation\\nroad segmentation\\n3d object detection\\nend-to-end solutions\\nimage-based features\\nimage-plane\\ncamera geometry\\ninstantaneous bev estimation\\nstate estimation\\ntemporal information\\nmonocular video\\nfactorised 3d convolutions\\nbev occupancy grid\\nsingle-scene bev estimation\",\"197\":\"training\\nlaser radar\\nconferences\\nbuildings\\nestimation\\ncameras\\nrobustness\\ncontrol engineering computing\\nfeature extraction\\nglobal positioning system\\nimage representation\\nmobile robots\\nroad traffic control\\nrobot vision\\nsupervised learning\\ntraffic engineering computing\\nunsupervised learning\\nvideo signal processing\\nmonocular videos\\nglobal positioning systems\\nappearance-based losses\\nmultimodal training\\nscale signal\\ncamera setup\\nscene distribution\\nfeature representations\\naware depth estimation\\nlow-frequency gps data\\nmonocular self-supervised depth estimation\\ndense depth estimation\\nscene-understanding\\nautonomous driving\\nmultimodal scale consistency\\ndynamically-weighted gps-to-scale loss\\nrelative distance\",\"198\":\"measurement\\nvisualization\\nconferences\\nestimation\\ntraining data\\ncolor\\nback\\ngeophysical image processing\\nimage classification\\nimage colour analysis\\nimage resolution\\nimage segmentation\\nlearning (artificial intelligence)\\nunsupervised learning\\ndense correspondences\\ndifferent spectra\\ncross-spectral images\\ncorrespondence solving techniques\\nvisual domain\\ncross-spectral techniques\\nspectra-specific characteristics\\ndense correspondence estimation problem\\ncycle-consistency metric\\nself-supervise\\nspectra-agnostic loss functions\\nmultiple spectra\\ndense rgb-fir correspondence estimation\\nrgb-rgb\\nself-supervised approaches\\ncross-spectral correspondence estimation\\nself-supervised multispectral correspondence estimation\\nautonomous vehicles\\nmedical imaging\\nmultispectral images\\ncolor images\",\"199\":\"shape\\ntrajectory planning\\ncurrent measurement\\nconferences\\naerospace electronics\\nsize measurement\\ntrajectory\\ndexterous manipulators\\nlearning (artificial intelligence)\\nconditional postural synergies\\ndexterous hands\\ngenerative approach\\nvariational auto-encoders\\nobject size\\ndexterous artificial hands\\nlatent space\\nsynergy space\\npostures\\nnonlinear conditional model\\nobject shape\\nevaluation criterion\\nnonlinear models\\nin-hand manipulation tasks\",\"200\":\"conferences\\nwires\\nrobot control\\nnatural languages\\nbenchmark testing\\ntools\\nrendering (computer graphics)\\nend effectors\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmanipulator kinematics\\nmanipulators\\nposition control\\nrobot vision\\nrobot learning sandbox\\ndeformable linear object manipulation\\nmachine learning\\nlearning-based approaches\\ninvolved bodies\\ndeformable linear objects\\ndlo\\nagriculture\\nmedicine\\nrobotic manipulation research\\ncustom environments\\nreproducibility\\nsimulation sandbox\",\"201\":\"training\\nautomation\\nconferences\\nturning\\ndistortion\\ncontrol systems\\ntrajectory\\ndeep learning (artificial intelligence)\\nmobile robots\\ntrajectory control\\nimitation architecture\\nimitation agent\\nsynthetic experts\\ntrajectory datasets\\ndata augmentation strategy\\ngenerated trajectories\\nreal trajectories\\nadversarial imitation learning\\ntrajectorial augmentation\\ndeep imitation learning\\nexpert demonstrations\\ncontrol tasks\\nsequential nature\\naugmentation method\\naugmented trajectories\\nsemisupervised correction network\\ndistorted expert actions\\nadversarial data\",\"202\":\"robot motion\\nmanifolds\\ncomputational modeling\\nconferences\\nkinematics\\ngrasping\\nmanipulators\\ncontrol engineering computing\\ninverse problems\\nlearning (artificial intelligence)\\npath planning\\nredundant manipulators\\n7-dof robot arm\\nredundant robot manipulator\\ninverse solutions\\nlong-horizon manipulation tasks\\nkinematic feasibility\\nkinematic reachability\\none-shot inverse mapping\\nrobot motion planning\\ntarget pose\\nobject grasping\\nreachable manifold learning\",\"203\":\"motor drives\\nautomation\\nconferences\\nswitches\\npredictive models\\ndata collection\\nmanipulators\\ncontrol system synthesis\\nlearning (artificial intelligence)\\nlearning systems\\nlegged locomotion\\nneurocontrollers\\noptimisation\\ncontroller learning\\nmotor control tasks\\nlearning control\\nforward model prediction error\\nmodel based control\\n7 dof manipulator\",\"204\":\"automation\\nconferences\\nswitches\\nhardware\\nrobot learning\\nsafety\\nbayes methods\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\nrobot programming\\nsearch problems\\ngosafe\\nglobally optimal safe robot learning\\nlearning policies\\nrobotic systems\\nsafety constraints\\nhardware damage\\nbayesian optimization algorithm\\nsearch space\\nlearned backup controller\",\"205\":\"training\\nstate feedback\\nnavigation\\nconferences\\nhumanoid robots\\nreinforcement learning\\ndata models\\ninference mechanisms\\nmulti-agent systems\\nplanning (artificial intelligence)\\nhierarchical policy\\nhierarchical planning\\nlatent variable models\\nhigh-dimensional observations\\nlow-dimensional latent variables\\nplanning module\\nlatent intentions\\nfeedback control policy\\nlow-dimensional latent space\\nlearned policy\\nimitation tasks\",\"206\":\"uncertainty\\ncomputational modeling\\ntraining data\\ndata collection\\ndata models\\nunmanned aerial vehicles\\ntrajectory\\naerospace robotics\\napproximation theory\\nautonomous aerial vehicles\\nclosed loop systems\\nlearning systems\\noptimisation\\nrobot dynamics\\nsampling methods\\nstatistical analysis\\nvehicle dynamics\\ntiltable rotors\\nclosed-loop control\\ninformative trajectories\\ncomplex omnidirectional flying vehicle\\ndynamics model\\nsampling-based method\\noptimization problem formulation\\ndata-driven system\\nstatistical model\\ncomplex dynamics\\nsystem dynamics\\nreal robots\\ninformative trajectory\\nactive model learning\",\"207\":\"parameter estimation\\nautomation\\nconferences\\nreinforcement learning\\npredictive models\\nmanipulators\\ndata models\\ncontrol engineering computing\\nfunction approximation\\ndifferentiable physics models\\nreal-world offline model-based reinforcement learning\\nblackbox models\\nmbrl\\ndata distribution\\nball in a cup\\nbic\\nphysical manipulator\",\"208\":\"training\\ndeep learning\\nsystematics\\nperturbation methods\\nrobot learning\\nrobustness\\nsafety\\ndeep learning (artificial intelligence)\\nneurocontrollers\\nrobot programming\\nrobust control\\nadversarial training\\ndeep learning models\\nneural controllers\\ntransient errors\\nsystematic errors\\nconditional errors\\nsafety-domain optimization\",\"209\":\"deep learning\\nthree-dimensional displays\\nuncertainty\\nshape\\nshape measurement\\nrobot vision systems\\nobject detection\\ncomputational geometry\\ndeep learning (artificial intelligence)\\ngrippers\\nmanipulators\\nmobile robots\\nobject recognition\\nrobot vision\\nshape recognition\\nstereo image processing\\n3d object detection\\nmobile robot manipulator\\n3d point cloud technology\\nplugin charging\\nautomatic plug-in charging testing\\n3d camera\\ngripper\\nshape information\\ndepth information\",\"210\":\"damping\\nautomation\\nconferences\\nforce feedback\\nmanipulators\\nstability analysis\\nregulation\\nforce control\\nmedical robotics\\nposition control\\nrigidity\\nstability\\ntelerobotics\\ntime-varying systems\\nrate mode bilateral teleoperation\\npassivity tank\\nslave manipulator\\ntime-varying environment\\nenergy exchange\\nstable interactions\\nstiffness parameters\\nvariable stiffness environment\\nteleoperation system\\nenergy based variable admittance control\\nmedical application\\npercutaneous vertebroplasty\\nminimally invasive medical intervention\",\"211\":\"performance evaluation\\ntraining\\nbridges\\ntracking\\nconferences\\nmixed reality\\nmuscles\\naugmented reality\\nmanipulators\\nobject tracking\\ntelerobotics\\nuser interfaces\\nfirst person experience\\nmixed reality devices\\nintuitive experience\\ntotal immersion\\npick-and-place tasks\\nhuman performance\\nimmersive system\\nautocorrection system\\ntask autocorrection\\nrobotic arm teleoperation\\nmuscle memory\\nuser motion tracking\",\"212\":\"measurement\\nsolid modeling\\ncollaboration\\noptimization methods\\nvirtual reality\\ntools\\nreal-time systems\\ncontrol engineering computing\\nend effectors\\nmanipulator kinematics\\nmotion control\\nmulti-robot systems\\noptimisation\\npath planning\\ntelerobotics\\ntrajectory control\\nmanipulability optimization\\nmultiarm teleoperation\\nhuman operator\\ndirect human intervention\\nautonomous behaviour\\ncollaborative robot arms\\nhuman motion\\nmultiarm payload manipulation\\nreal-time input motion\\nmultiple collaborative arms\\nlocal optimization method\\nworkspace limitation\\nreal-time teleoperation\\nvirtual reality devices\\nrobot guidance\\nmultiple robot orchestration\\nvr interface\\npayload manipulation\\narm manipulability index\\nkinematic singularities\\nend-effector position error\\njoint motion metrics\",\"213\":\"dynamics\\ntransportation\\ncollaboration\\nobservers\\npredictive models\\nrobot sensing systems\\nminimization\\nend effectors\\nhuman-robot interaction\\nkalman filters\\nmanipulator dynamics\\nmechanical stability\\nmobile robots\\nmotion control\\nnonlinear filters\\npose estimation\\nrobot vision\\ndmp-based reference model\\ntarget pose estimation\\nkuka lwr4+ robot\\nhuman-robot collaborative object transfer\\nhuman motion prediction\\nekf\\nextended kalman filter\\nstability analysis\\nati sensor\\nend effector\\ncartesian pose dynamic movement primitives\\ncartesian pose dmp\",\"214\":\"torso\\nproductivity\\ntrajectory planning\\ndynamics\\ncollaboration\\nreal-time systems\\nsafety\\nassembling\\nhuman-robot interaction\\nindustrial robots\\nmotion control\\nhuman-robot collaboration performance\\ndynamic human motion projection\\nfive-degree-of-freedom rigid-body model\\ncontrol-oriented projection model\\nhuman-state estimator\\ndynamic projection method\\nestimated collision time\\nhrc's efficiency\\nmanufacturing process\",\"215\":\"solid modeling\\nadaptation models\\nthree-dimensional displays\\niso standards\\npose estimation\\nlaser modes\\nreal-time systems\\nhuman-robot interaction\\nsafety\\nstereo image processing\\nhard real-time capability\\nhuman pose estimation\\nindustrial domain\\nsafety function\\nsafety standards\\niso 13849\\nresponse time\\nreal-time requirement\\nspherical cones\\nhuman-robot distance calculation experiments\\niso 13855 safety standard\\ninterchangeable 3d human pose estimation systems\\n3d volume\",\"216\":\"robot control\\ncollaboration\\nswitches\\nkinematics\\nmanipulators\\nlinear programming\\ntrajectory\\ncompliance control\\ncontrol system synthesis\\nhuman-robot interaction\\nleast squares approximations\\nmanipulator kinematics\\nposition control\\nquadratic programming\\ntrajectory control\\nadaptive compliance robot control\\nmultiobjective control problem\\nleast-square optimization techniques\\nobjective functions\\nhierarchical manner\\ninverse kinematics problem\\ncartesian trajectories\\noptimization variable\\ncartesian reference\\nadaptive compliance controller\\nclassical hierarchical quadratic programming formulation\\naugmented hierarchical quadratic programming\",\"217\":\"automation\\nrobust stability\\nconferences\\ncollaboration\\nsafety\\nadmittance\\nrobots\\nhuman-robot interaction\\nmedical robotics\\nmobile robots\\nrobust control\\nrobot behavior\\nreactivity\\nvariable conditions\\ncollaborative scenario\\ncontrol architecture\\nstable behavior\\nvariable admittance architecture\\ncontrol barrier functions\\ncollaborative robot\\noptimization approach\\ncollaborative applications\\nhuman-robot collaboration\\nhighly dynamic environment\",\"218\":\"vibrations\\npredictive models\\nsurface roughness\\ndata models\\nsoftware\\nrough surfaces\\nvelocity measurement\\ndamping\\nimpact (mechanical)\\nmanipulator dynamics\\nmechanical contact\\ntorque control\\nprediction accuracy\\nimpact-aware robot control\\nrobotic arm\\nrigid multibody models\\nimpact-aware manipulation\\ndynamic grasping\\nrecorded dynamic response\\n7dof torque-controlled robot\\nrigid surface\\npost-impact robot velocity prediction\\npre-impact velocity\\nimpact map\\nfrictionless inelastic impact\\nrecorded post-impact data\\nrigid-body robot model\\nrobot simulator software\\nhuman manipulation\",\"219\":\"energy consumption\\nautomation\\nnavigation\\nconferences\\ntrajectory\\nmobile robots\\nconvolutional neural networks\\nlearning (artificial intelligence)\\nneural nets\\npath planning\\nenergy-aware path planner\\nunstructured environments\\ndriving energy consumption\\nenergy recovery\\ncomplex uneven terrains\\nperceived terrain point clouds\\nheuristic inclination-based energy model\",\"220\":\"geometry\\nnavigation\\nfocusing\\nrobot sensing systems\\nrobustness\\nhardware\\nsensors\\nadaptive control\\naerospace robotics\\ncollision avoidance\\ndistance measurement\\nelasticity\\nmobile robots\\nautonomous navigation policy\\nresilient microflyer\\ncollision-tolerant robot\\nextremely confined environments\\nmanhole-sized tubes\\nrigid-compliant design\\nelastic flaps\\nstiff collision-tolerant frame\\npassive flaps\\ncompliant collisions\\ncontact sensing\\nresilient autonomy\\nresource-constrained hardware\\nonboard visual-inertial odometry\\nsafe navigation policy\\ncollision-avoidance\\ntime-of-flight sensing\\nexplicit manhole navigation mode\\nrobot design\\nresilient collision-tolerant navigation\",\"221\":\"attitude control\\noceans\\nforce\\nestimation\\npredictive models\\nhydrodynamics\\nreliability\\nautonomous underwater vehicles\\nposition control\\npredictive control\\nrobot dynamics\\nrobust control\\nvehicle dynamics\\nexperimental validation\\nunsteady wave induced loads\\nshallow water environments\\nunmanned underwater vehicles\\nreliable control methods\\nimmediate ocean environment\\nwave-induced hydrodynamic forces\\nsubmerged vehicle\\nsea state conditions\\nstationary remotely operated vehicle\\nhydrodynamic force estimation\",\"222\":\"frequency-domain analysis\\nforce\\ndynamics\\nocean waves\\nlakes\\ncomputational efficiency\\nrivers\\ngeophysics computing\\nunmanned surface vehicles\\nasvlite\\nhigh-performance simulator\\nautonomous surface vehicles\\nkey distinguishing factor\\nmarine environments\\naquatic environments\\nmarine vehicles\\nefficient control strategies\\nmarine simulators\\nhigh computational overhead\\ncomputationally efficient asv simulator\\nwave force computation\\nlow computational overhead\\nhigh run-time performance\\nmid-range desktop computer show\\nefficiently simulate irregular waves\\ncomponent wave count\",\"223\":\"three-dimensional displays\\nruntime\\ncosts\\nnavigation\\nrobot kinematics\\ncomputational modeling\\nforestry\\ncomputational geometry\\ndifferential geometry\\nmesh generation\\nmobile robots\\npath planning\\nvectors\\ncontinuous shortest path vector field navigation\\narbitrarily shaped 3d triangular meshes\\nrobot navigation\\nreal-world outdoor environments\\nshortest distance\\ngeodesic path\\nmesh triangles\\nmodular extendable multilayer map architecture\\ngeodesic distances\\ncontinuous vector field computation\\ngeometric cost layers\\n3d triangular meshes\\nwavefront propagation\\nmathematical foundation\\nros software stack\\ngazebo simulations\",\"224\":\"deformable models\\nadaptation models\\nsolid modeling\\nthree-dimensional displays\\nshape\\npredictive models\\nbending\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulator dynamics\\npredictive control\\ninteraction effects\\nrobotic manipulation\\ntwisting dynamics\\nbending dynamics\\nneighboring segments\\nrecurrent model\\ndlo segments\\nlocal interaction\\nmodel predictive control\\niterated propagation\\ndeformable linear objects dynamics modeling\\n3d dynamics\\ninteraction network dynamics learning\\nlong-term predictions\\nshort-term predictions\\nshape control\",\"225\":\"deformable models\\nanalytical models\\nsolid modeling\\nshape\\ndynamics\\nbending\\nnumerical models\\nbiomechanics\\nfinite element analysis\\nrobot kinematics\\nvibrations\\ncurved continuum manipulator\\ndifferent posture parameters\\nmodal response\\ngeneralized coordinate method\\nmodal properties\\nvariable posture\\necm physical prototype\\ncircular tracking motion\\nmodal dynamic modelling\\nexperimental validation\\ncurved extensible continuum manipulator\\ndynamic modelling method\\nvariable bending angle\\nkinematic models\\nstatic models\\ngeometric posture deformation\\ndeflection\\nmodal dynamic model\\nmodal frequency\\nmode shape\",\"226\":\"surface reconstruction\\nsolid modeling\\nthree-dimensional displays\\ncorrelation\\natmospheric modeling\\npath planning\\nunmanned aerial vehicles\\ndifferential geometry\\ngaussian processes\\nmobile robots\\nactive information gathering\\nthree-dimensional surfaces\\nsurface inspection focus\\npath offline\\ninformation field\\nmapping surface information fields\\nplan informative paths\\ninformation-gathering paths\\ninformative planning method\\ninformation-theoretic metrics\\ninformation gathering efficiency\\nonline informative path planning approach\",\"227\":\"potential energy\\noptical fiber cables\\nkinematics\\nmechanical cables\\ntools\\npredictive models\\nposition measurement\\nactuators\\ngrippers\\nmanipulator dynamics\\nmanipulator kinematics\\nposition control\\nsprings (mechanical)\\nhighly underactuated tool\\ndouble curved mold\\nforward kinematics\\nprepreg fiber plies\\nsuction cups\\nsprings\\nfiber plies draping\",\"228\":\"geometry\\nforce measurement\\ntrajectory planning\\nconferences\\nforce\\nrobot control\\ntools\\nactuators\\ncomputational complexity\\nend effectors\\nmanipulator kinematics\\nmanipulators\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nsearch problems\\nline force capability evaluation\\nefficient polytope vertex search\\nellipsoid-based manipulability measures\\nappropriate representation\\nassociated vertex search problem\\npolytope approach\\nrobot mechanical design\\nrobot placement\\nwork-space\\noffline trajectory planning\\non-line polytope vertex search algorithm\\ncomputation time\\ncommonly used algorithms\\non-line capability\\nfranka emika panda robots\\nconfiguration dependant task-space force capability\",\"229\":\"visualization\\nlaser radar\\nrobot vision systems\\nlinear programming\\nreal-time systems\\ntrajectory\\nsafety\\ncollision avoidance\\nmobile robots\\noptical radar\\npath planning\\ntelerobotics\\ntrees (mathematics)\\nteleoperated driving\\nreal-time interactive path planning approach\\nextraordinary situations\\nexternal operator\\nautonomous vehicle\\ncomplex urban scenarios\\nhigh workload\\nhuman operator\\nsuboptimal solutions\\nteleoperation paradigm\\nautonomy level\\nmain decision-maker\\ndriving tasks\\nintroduced approach\\ncollision-free paths\\nlidar sensor information\\nhybrid path planning method\\nfeasible paths\\ndriving experiments\\nachieved driving safety\",\"230\":\"automation\\nconferences\\nprogramming\\nsafety\\nstandards\\npredictive control\\nmobile robots\\nremotely operated vehicles\\nroad safety\\nroad vehicles\\ndeceleration profiles\\nstandard transit bus\\nself-driving capabilities\\ncomfortable decelerations\\nsafe decelerations\\nmodel predictive control\\nlexicographic programming\\nself-driving transit bus\\npassenger safety\\npassenger comfort\\nbus speed\",\"231\":\"three-dimensional displays\\nrobot kinematics\\nsoftware\\nsensor systems\\nsensors\\ntime factors\\ntask analysis\\nautonomous aerial vehicles\\nmulti-robot systems\\nobject detection\\nreliability\\nrobot vision\\ndesert conditions\\n3d environment\\neagle\\nautonomous aerial interception system\\nmultiple target detection\\ncomplex multirobot system\\nmultiple target elimination\\nintruder uav interception scenario\\nmbzirc competition\\nonboard sensors\\ncoordination system\\ngeneral control\\ndynamic objects\\nstatic objects\\nmultiple intruder uavs\\nmultiuav system\\ntarget detection\\nmulti-uav system\\nuav planning\\ntarget elimination\",\"232\":\"location awareness\\nglobal navigation satellite system\\nnavigation\\nconferences\\ngreen products\\nforestry\\nrobot sensing systems\\nautonomous aerial vehicles\\nmobile robots\\nmulti-robot systems\\nposition control\\nconstrained environment\\nbio-inspired control law\\nonboard sensor data\\nsafe flocking\\nuav agent\\nrelative position\\nlocal reference frame\\nrealistic robotic simulator\\nonboard relative localization\\nautonomous multiuav applications\\nglobal localization information\\ngnss-denied environments\\ncompact flocking\\nunmanned aerial vehicles\\nuavs\\nhigh obstacle density areas\\nenvironment map\\nautonomous aerial swarming\\naerial systems: perception and autonomy\\nfield robotics\",\"233\":\"surface impedance\\nadaptation models\\nforce\\nestimation\\ninspection\\nrobot sensing systems\\nunmanned aerial vehicles\\nadaptive control\\nautonomous aerial vehicles\\ncameras\\nend effectors\\nrobot vision\\nstability\\nstereo image processing\\ncontact surface\\naerial manipulator\\nadaptive stiffness estimation impedance control\\nsustained contact\\nunmanned aerial vehicle\\nstereo camera information\\ncontact point\\ncontact-based surface inspection\",\"234\":\"visualization\\ncontrol systems\\nreal-time systems\\nvisual servoing\\nunmanned aerial vehicles\\nvehicle dynamics\\ntransient analysis\\naircraft control\\nautonomous aerial vehicles\\ndecentralised control\\nhelicopters\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npredictive control\\nrobot dynamics\\nrobot kinematics\\nrobot vision\\nmodel predictive control\\nmultiagent systems\\ndecentralized controller\\ninter-agent bearings\\nimproved transient formation convergence\\ndynamic quadrotor bearing formation control\\nsystem robustness\\nquadrotor uavs\\nattitude kinematics\\nrobot groups\\nformation control\\nquadrotor swarms\",\"235\":\"system dynamics\\nfriction\\nforce\\ndynamics\\nend effectors\\ntrajectory\\nimpedance\\nactuators\\naerospace robotics\\nattitude control\\nforce control\\nforce sensors\\nmanipulators\\nnonlinear control systems\\nposition control\\npredictive control\\nend-effector\\naerial interaction task\\nforce sensor\\ninteraction force\\nautomatic transition\\noperation modes\\ncontact constraints\\nfree flight\\nstatic friction\\ndynamic friction\\ndifferent mode-specific controller tunings\\nflight experiments which combine force\\nattitude tracking\\nmultiple interaction modes\\nmodel predictive controller\\nfully actuated aerial manipulator\",\"236\":\"sea surface\\nthree-dimensional displays\\nsurface waves\\nsimulation\\nrobot kinematics\\nprototypes\\nnumerical simulation\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\nstability\\nvelocity control\\ncontroller design process\\ntethered quadrotor uav-buoy system\\nmarine locomotion\\nunmanned aerial vehicles\\noffshore applications\\nmarine locomotive quadrotor uav\\neuler-lagrange method\\nstable control system design\\nforward-surge speed\\ncartesian-based velocity controllers\\nmarine robotics\\nlocomotive uav\\nmotion control\\nfloating buoy manipulation\",\"237\":\"automation\\nthree-dimensional displays\\nsimultaneous localization and mapping\\npose estimation\\ncollaboration\\ncameras\\nunmanned aerial vehicles\\naerospace computing\\nautonomous aerial vehicles\\ncontrol engineering computing\\ndistance measurement\\nkalman filters\\nmobile robots\\noptimisation\\npath planning\\nposition measurement\\nrobot vision\\nslam (robots)\\nstereo image processing\\nuav\\nabsolute position measurements\\ngps\\nobservable landmarks\\nhigh-altitude flights\\nvirtual stereo setup\\ninertial measurement unit\\nrelative distance measurements\\ncollaborative vio\\ndecentralized collaborative estimation scheme\\nlocal map\\nagents\\nconsensus-based optimization\\ndistributed variable-baseline stereo slam\\nvisual-inertial odometry\\nrobot navigation\\ncamera\\nview overlap\\nonboard ultra-wideband modules\\nuwb\\nlow pose estimation latency\",\"238\":\"weight measurement\\nvisualization\\npipelines\\nrobot sensing systems\\nmanipulators\\ncognition\\nrobustness\\ncollision avoidance\\ncontrol engineering computing\\ndexterous manipulators\\ngrippers\\ninference mechanisms\\nobject detection\\nrobot vision\\nobject properties\\nnatural language\\nmotion planning\\nrobotic arm\\nmultimodal reasoning\\nexecution module\\n7dof manipulator\\ntwo-finger gripper\",\"239\":\"automation\\nconferences\\nbandwidth\\nfractals\\ndelays\\nhaptic interfaces\\nimpedance\\ndexterous manipulators\\ntelerobotics\\ncommunication bandwidths\\nconsistent stability\\nrobust high-transparency haptic exploration\\ndexterous telemanipulation\\nrobotic teleoperation\\nhuman-in-the-loop capabilities\\ncomplex manipulation tasks\\ndangerous environments\\nremote environments\\nplanetary exploration\\nnuclear decommissioning\\ntelemanipulation architecture\\npassive fractal impedance controller\\nfic\\nactive viscous component\\nimpedance controller\\nmaximum communication bandwidth\\nsuperior dexterity\\ntelemanipulation test scenarios\\nextreme delays\",\"240\":\"visualization\\nautomation\\nconferences\\nestimation\\nlighting\\nrobot learning\\ntask analysis\\nimage denoising\\nimage representation\\nmanipulators\\nrandom processes\\nrobot vision\\nsupervised learning\\nunsupervised learning\\nunsupervised feature learning\\ncontrastive domain randomization\\nrobotic tasks\\nrobot interaction\\nnoise contrastive estimation\\nsimulation-to-real transfer\\nirrelevant visual properties\\nvisual domain variation\\ncontrastive dr\\nself-supervised learning\\nrelevant visual properties\",\"241\":\"training\\nvisualization\\nthree-dimensional displays\\nmotion segmentation\\nsemantics\\ntraining data\\nobject segmentation\\ncalibration\\nimage colour analysis\\nimage motion analysis\\nimage registration\\nimage segmentation\\nimage sensors\\nimage sequences\\nlearning (artificial intelligence)\\nmanipulators\\nneural net architecture\\nobject detection\\nrobot vision\\nsupervised learning\\nmotion cues\\nsemantic knowledge\\ndistraction objects\\nstatic scenes\\nkinematic robot\\nprecise hand-eye calibration\\nsensor data\\nextensive experimental evaluation\\nsemantic segmentation network training\\nmanually annotated training data\\nsingle end-to-end trainable architecture\\nvisual registration\\n3d object models\\nconsecutive rgb frames\\nagnostic foreground segmentation\\nrobotic manipulator\\nself-supervised grasped object segmentation\\nmanipulation sequences\\nunknown object segmentation\",\"242\":\"measurement\\nautomation\\nconferences\\nestimation\\nrobustness\\nmulti-robot systems\\ndistributed algorithms\\ngraph theory\\nmobile robots\\ndistributed algorithm\\nalgebraic connectivity\\ncommunication graph\\nvalid connectivity metric\\nfast convergence properties\\nrobust distributed estimation\\ndistributed networked multirobot systems\\ncrucial operational specification\",\"243\":\"automation\\nconvolution\\nconferences\\ngraph neural networks\\nmulti-agent systems\\nconvolutional neural nets\\ngraph theory\\nlearning (artificial intelligence)\\nmodgnn\\nexpert policy approximation\\nmultiagent system\\nmultiagent domain\\ncoordination strategies\\ngraph convolutional network\\ngcn\\ncommunication graph\\nmultiagent flocking problem\\nmodular graph neural network architecture\",\"244\":\"legged locomotion\\nclosed-form solutions\\nconferences\\nsteady-state\\nsafety\\ncomplexity theory\\ntransient analysis\\ncontrol system synthesis\\ngait analysis\\nmotion control\\noptimal control\\noptimisation\\ntrajectory control\\noptimization-inspired controller design\\ntransient legged locomotion\\nmaneuverability\\nheuristic control\\nanimal locomotion\\ntrajectory optimization\\ntransient locomotion\\nsteady-state gait\\npneumatic-electric monopod robot\\nraibert controller\\nenergy optimal policy\",\"245\":\"legged locomotion\\nsolid modeling\\nthree-dimensional displays\\nrobot sensing systems\\nstability analysis\\nhardware\\nsafety\\nmotion control\\npath planning\\npredictive control\\nquadratic programming\\nrobot dynamics\\nrobot kinematics\\nlonger term dynamic stability considerations\\nlocomotion framework\\ncontrol barrier functions\\ncbf based safety constraints\\nlow frequency kinodynamic mpc formulation\\nhigh frequency inverse dynamics\\nmultilayered safety\\ndynamic locomotion\\nfoot placement\\nmodel predictive control\\nanymal quadruped platform\",\"246\":\"automation\\nconferences\\ndynamics\\nhumanoid robots\\noptimal control\\nkinematics\\ncomputer architecture\\nbiomechanics\\nlegged locomotion\\nmobile robots\\nmotion control\\nrobot dynamics\\nrobot kinematics\\ntorque control\\ndecoupled approach\\nkinematic features\\ncentaur-type humanoid platform\\npowerful actuation\\nagile motions\\ndynamic motions\\nbipedal humanoid\\ncontrol architecture\\ntorque-controlled platforms\\nlower-body primitives\\ntemplate kinematic model\\nupper-body motion\\noptimal control planning\\nquadrupedal humanoid\\ncentauro robot\",\"247\":\"schedules\\nautomation\\nconferences\\nrendering (computer graphics)\\nrobots\\ntrajectory optimization\\nconvergence\\ncollision avoidance\\nlegged locomotion\\nmobile robots\\noptimisation\\npath planning\\noptimization convergence\\noptimization-based stage\\nsystem constraints\\nnonholonomic rolling constraints\\ngood initial guesses\\nrobot model\\ngeneral terrain representations\\nlegged-wheeled machines\\ndegrees of freedom\\nlegged-wheeled planners\\nfalling prey\\nbad local minima\\ncombined sampling\\nchallenging terrain\\nsampling-based stage computes whole-body configurations\\ncontact schedule\",\"248\":\"training\\ncosts\\ncustomer services\\nurban areas\\nneural networks\\nmachine learning\\nimage representation\\nconvolutional neural nets\\nlearning (artificial intelligence)\\npublic transport\\ntraffic engineering computing\\nend-to-end learning\\ngood customer service\\nvehicle repositioning\\nvehicle-to-customer assignment\\napplied policy\\nride-hailing companies\\ntaxi fleet\\nhandcrafted mathematical models\\ncomplex models\\noperational policy\\nsimilarly constructed repositioning action\\ntraining effort low\\nneural network\\nfull-sized traffic networks\\nconvolutional neural networks\\nimage-like representation\\nmachine learning approaches\\nreasonable complexity\\nhandcrafted model\\nsocial effects\\ndemographic effects\\ntopological effects\\nvehicle demand\\ndestinations\\nrequest origins\\nspatiotemporal distributions\\nautonomous vehicles\",\"249\":\"marine vehicles\\nsea surface\\nconvergence\\nnavigation\\nsystem dynamics\\nturning\\ncollision avoidance\\nmobile robots\\npath planning\\nsampling methods\\nships\\ntrees (mathematics)\\nmarine crafts\\nsampling strategy\\noptimal path alteration\\nsurface vessel sailing\\nsafe navigation\\nminimal ship domain\\ncompliant region\\npath deviation\\nrandom tree algorithm\\noptimal cost\\ncolregs-informed rrt\",\"250\":\"training\\nnavigation\\nconferences\\nreinforcement learning\\nbidirectional control\\nentropy\\ntiming\\nlearning (artificial intelligence)\\nmulti-agent systems\\nroad traffic\\ntraffic engineering computing\\nwell-defined traffic rules\\nminimal negotiation\\nunconsidered conflict driving scenario\\nyet everyday conflict driving scenario\\nhigh-conflict driving scenario\\nnegotiations\\nequal rights\\ncentralized control structure\\nmultiagent reinforcement learning\\ndiscrete asymmetric soft actor-critic\\ndasac\\noff-policy marl algorithm\\ncentralized training\\nunknown timing\\nopposing vehicle\\nunencountered policies\\ndefensive driving\\nrobustly negotiate bi-directional lane usage\\nhigh-conflict driving scenarios\\nautonomous driving\\nsubstantial progress\\ncommon traffic scenarios\\nintersection navigation\\nautonomous agents\",\"251\":\"robot motion\\ndatabases\\nconferences\\nhumanoid robots\\nkinematics\\nskeleton\\nreal-time systems\\ncomputer animation\\nimage motion analysis\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npose estimation\\npaired data\\nself-supervised learning procedure\\ngenerated robot\\nshared latent space\\nexpressive robotic motions\\nsupervised motion retargeting\\nsafety guarantee\\ndata-driven motion retargeting method\\nnatural motions\\nmotion capture data\\nhuman poses\\ncorresponding robot configurations\\ntime-consuming data-collection\",\"252\":\"robot motion\\ngeometry\\nautomation\\nconferences\\ntools\\nprobabilistic logic\\nplanning\\nmobile robots\\npath planning\\nplanning (artificial intelligence)\\nstate-space methods\\nsparse roadmap planners\\nchallenging planning problems\\nsparse multilevel roadmaps\\nhigh-dimensional planning problems\\nhigh-dimensional robotic motion planning\\nsparse roadmaps\\nstate spaces\\nplanning performance\\nmultilevel abstractions\\nsparse multilevel roadmap planner\",\"253\":\"location awareness\\nindustries\\ndeformable models\\nthree-dimensional displays\\nfeature extraction\\nplanning\\nmotion measurement\\nassembling\\ncad\\ncomputational geometry\\nmesh generation\\npath planning\\nsolid modelling\\ntriangle mesh\\nfast construction\\ncollision-free disassembly paths\\nrigid body motion planning\\nindustrial disassembly scenarios components\\nflexible fastening elements\\nnonlinear disassembly paths\\nrigid mesh\\nunavoidable collisions\\nmotion planning\\nautomatic disassembly\\nsalient features\\nvertex-based surface feature\\nper-vertex saliency\\nreal-world planning scenarios\\nsaliency features\\ncad-data\\nsampling-based motion planning\\nmesh saliency methods\",\"254\":\"time-frequency analysis\\ntrajectory planning\\ndynamics\\nlattices\\nplanning\\ntrajectory\\nspatial resolution\\nmobile robots\\npath planning\\nsearch problems\\ndynamic mav trajectories\\nlocal multiresolution state lattices\\nmotion primitives\\nsystem\\ndynamically feasible mav trajectories\\nhigh-dimensional state lattices\\nspatial path planning\\nplanning times\\ndynamic environments\",\"255\":\"service robots\\nblades\\nswitches\\nmaintenance engineering\\ntools\\nuser interfaces\\nend effectors\\ninspection\\nmobile robots\\nmotion control\\nposition control\\ntelerobotics\\nwind turbines\\nbespoke user interface\\nhuman-in-the-loop operation\\nautonomous repair actions\\nrobotic repair system\\nfiller material deposition\\nbespoke end-effector tool\\nrelevant manual repair process\\nmaximum end-effector position error\\nmaximum tool switching time\\nmaximum arm\\nstandalone design\\nmultifunctional arm\\nlightweight repair arm\\nmultifunctional robotic repair arm\\nwind turbine blades\\narm housing multiple end-effector tools\\nautonomous end-effector tool-changer\\nextreme environments project\\nmultiplatform inspection\\ntelerobotic wind turbine blade repair\",\"256\":\"analytical models\\nautomation\\nconferences\\npipelines\\ntools\\nmanipulators\\nlibraries\\nbayes methods\\nbelief networks\\nlearning (artificial intelligence)\\nmobile robots\\nsystem recovery\\ntrees (mathematics)\\ntree error recovery framework\\nrobotic systems\\ndynamic environments\\nrobot services\\nonline management\\nlearning module\\nerror detection\\ncontrol flow instructions\\nskill library\\nrecovery solution recipes\\nbayesian network decision model\\nrecovery pipeline\\nrobot arm\\nfailure mode and effects analysis\\nfmea method\",\"257\":\"wireless communication\\nanalytical models\\nminimally invasive surgery\\nmicroorganisms\\nbrushes\\nautomation\\nconferences\\nbiomagnetism\\nendoscopes\\nmagnetic actuators\\nmedical robotics\\npermanent magnets\\nmagnetic actuation system\\nmicrobiota-collection ingestible capsule\\nminimally invasive wireless devices\\ngut bacteria\\nhuman health\\nelectronic components\\ncapsule robot\\nmechanical brushing\\nwireless activation\\nexternal permanent magnetic source\\nmicrobiota\\nmucosa\\nwireless ingestible capsule\",\"258\":\"couplings\\ntracking\\nswitches\\nreal-time systems\\nrobustness\\ntrajectory\\nsafety\\ncollision avoidance\\nmanipulator kinematics\\nmotion control\\nmulti-robot systems\\nswitching systems (control)\\ngeometric primitives\\nflexible task relaxation\\ncircular field\\ncollision-free ball\\ncartesian space\\nreactive cooperative manipulation\\nset primitives\\nreal time planning\\ndual arm manipulation\\nbimanual task-space\\nvector-field based planner\\nrepulsive circulatory field\\ntask-primitive-priorities switching\\nkinematic complexity\\nrobot motion generation\",\"259\":\"adaptation models\\ndecision making\\nreinforcement learning\\nsearch problems\\ncognition\\nreal-time systems\\ncomputational efficiency\\ndecision theory\\nmarkov processes\\nmobile robots\\npath planning\\nreal-time applications\\nhigh solution quality\\ncomputation time speedups\\nefficient multiscale pomdps\\nrobotic object search\\nprior belief\\npossible item locations\\nsolving pomdps\\nhierarchical pomdp framework performs reasoning\\nmultiple spatial scales\\ncoarsely discretized state space\\nincreasing resolution\\nspatial hierarchy\\ntwo-layer multiscale pomdp\",\"260\":\"measurement\\ntorque\\nservice robots\\nshape\\ntools\\nprogramming\\nplanning\\nindustrial manipulators\\npath planning\\njoint manipulation planning\\nsequential manipulation planning\\ntrajectory optimization\\nindustrial applications\\noptimal robot design\\ntool shape\\nrobot station geometry\\nmanipulation tasks\\nstatic design parameters\\nsequential manipulation trajectory\\noptimization objectives\\nvelocities\\njoint torques\\ndesign optimization\\nwrench tool demonstration scenario\\ntool design\\nco-optimizing robot\",\"261\":\"atmospheric measurements\\nneural networks\\nsemisupervised learning\\nparticle measurements\\nparticle filters\\nhistory\\ntask analysis\\nneural nets\\nparticle filtering (numerical methods)\\nrobot vision\\nstate estimation\\nsupervised learning\\nend-to-end semisupervised learning\\ndifferentiable particle filters\\nmeasurement models\\npseudo-likelihood function maximisation\\nstate estimation tasks\\nrobotics\",\"262\":\"location awareness\\nlearning systems\\nsolid modeling\\nvisualization\\nuncertainty\\nthree-dimensional displays\\ncorrelation\\nfeature extraction\\nlearning (artificial intelligence)\\npose estimation\\nregression analysis\\nsynthetic training data\\nvisual localization\\ndetailed 3d model\\nlearning-based methods\\n-rescue scenarios\\nscene-agnostically\\ngeneralization capability\\nspecific changes\\nmodel architecture\\nextended regression part\\nhierarchical correlation layers\\nuncertainty information\\n5-point algorithm\\nequally big images\\nsurpasses all previous learning-based approaches\\nrespective scenes\\nrealistic conditions our learning-based approach\\nexisting learning-based\\nclassical methods\",\"263\":\"location awareness\\nuncertainty\\ncosts\\nautomation\\nconferences\\ndecision making\\nspace exploration\\naerospace robotics\\nmobile robots\\nplanetary rovers\\noutdoor environments\\nmulticriteria decision making\\nplanetary exploration\\npromethee ii\\ntask-specific criteria\\nresource usage\\ncandidate exploration goals\\nresource optimizations\\ncpu usage\\ndecision making method\",\"264\":\"deformable models\\nbundle adjustment\\nsimultaneous localization and mapping\\nconferences\\nestimation\\ntools\\ncameras\\nconvolutional neural nets\\nfeature extraction\\nimage reconstruction\\nimage segmentation\\nimage sequences\\nimage texture\\npose estimation\\nrobot vision\\nsensor fusion\\nslam (robots)\\nsd-defslam\\ndata association steps\\nsemidirect monocular slam\\nconventional slam techniques\\nscene rigidity\\nsemidirect defslam\\ndeforming scenes\\nenhanced illumination-invariant lucas-kanade tracker\\ndeformable map estimation\\nslam benchmark\\nmonocular deformable slam method\\nhighly deforming environments\\nintracorporeal scenes\\nmandala dataset\\nhamlyn dataset\\nweak texture\\nspecular reflections\\nsurgical tools\",\"265\":\"robot motion\\nautomation\\nconferences\\nneural networks\\ndynamics\\nreinforcement learning\\nkinematics\\ncontrol engineering computing\\nmobile robots\\nneural nets\\npath planning\\nrobot dynamics\\nrobot kinematics\\nrobot trajectories subject\\nkinematic joint constraints\\ndynamic robot motions\\nrobot joint\\nneural network\\nsafely executable joint accelerations\\nanalytical procedure\\nsafe joint accelerations\\nconstraint violations\\nprediction frequency\\nlearning performance\",\"266\":\"automation\\nconferences\\ndistributed databases\\nreinforcement learning\\nplanning\\ntask analysis\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nplanning (artificial intelligence)\\nself-imitation learning\\ndemonstration data\\ncurrent policy\\nsuccessfully visited states\\nearly reinforcement learning stage\\ngraph-search based motion planner\\nrobot\\npolicy learning\\nself-generated demonstrations\\nlaborious data preparation process\\nrl methods\\ncomplex motion planning tasks\\nsilp method\\nexpert knowledge\\nlong-horizon motion planning tasks\\nbroadly distributed data\",\"267\":\"training\\nuncertainty\\nautomation\\nconferences\\nneural networks\\ngrasping\\nrobot learning\\ncollision avoidance\\nconvolutional neural nets\\ndexterous manipulators\\nintelligent robots\\nobject detection\\nrobot programming\\ngrasp inference\\nprimitive adaption\\nmodel-based adaptive primitives\\ndata consumption\\nfully-convolutional neural network\\ngrasp primitives\\nlateral degrees\\nmodel-based controller\\ngrasp quality\\nhybrid approach\\ntime 50.0 ms\",\"268\":\"training\\nautomation\\nnavigation\\nconferences\\narchitecture\\nbuildings\\ntools\\ndexterous manipulators\\nhumanoid robots\\nlearning (artificial intelligence)\\npath planning\\nprimitive objects\\nsimulation training\\ndiscriminative models\\nfeasible grasps\\ngrasp space\\nconditional stylegan\\ngrasp generation\\nconditional generative adversarial networks\\ngans\\nfeed-forward manner\\nraw depth image input\\nrecently introduced stylegan architecture\\nearlier proof-of-concept paper\\nsuccessful sim2real transfer\\ngrasp outputs\\nrobot arm\\nshadow dexterous hand\\ngan model\\nreal-world objects\",\"269\":\"legged locomotion\\nconferences\\nfitting\\ngrasping\\nmanipulators\\ndata models\\nreliability\\ngrippers\\nmobile robots\\noptimisation\\npath planning\\nrobust control\\nexternal robotic arm\\ndynamic grasping maneuvers\\nmobile robot\\ncontrol access\\nstraight-forward optimization framework\\ncontrol commands\\ncontrol trajectories\\ngrasping tasks\\nboston dynamics spot\",\"270\":\"training\\nmeasurement\\ntaxonomy\\ngrasping\\nsampling methods\\ntrajectory\\nplanning\\ngrippers\\nimage classification\\nimage colour analysis\\nimage sampling\\nmanipulators\\npath planning\\nmultifinger grasps\\nrigid objects\\nparallel-jaw grippers\\nmultifinger robotic hands\\nplanning collision-free trajectories\\nfast generative multifinger\\ncoarse-to-fine model\\nclassification network\\nrefinement network\\nstandard grasp-sampling method\\ntask informative grasping\\nhigh quality grasp synthesis\\ngenerative coarse-to-fine sampling\\nmultifingan\\nrgb-d imaging\",\"271\":\"training\\nautomation\\nconferences\\ncomputational modeling\\nreinforcement learning\\ngames\\nrobots\\ncomputer games\\ncontrol engineering computing\\ngradient methods\\nmobile robots\\nsport\\nstate-space methods\\nsample-efficient reinforcement learning\\nrobotic table tennis\\nsample-efficient rl algorithm\\ntable tennis robot\\nhigh-dimensional continuous state space\\nrobot system\\nhitting time\\nracket state\\nactor-critic based deterministic policy gradient algorithm\\naccelerated learning\",\"272\":\"accelerometers\\ntraining\\nsolid modeling\\nrecurrent neural networks\\nphilosophical considerations\\npandemics\\nwearable computers\\nbody sensor networks\\ngesture recognition\\nhaptic interfaces\\nimage motion analysis\\nlearning (artificial intelligence)\\nobject detection\\nrecurrent neural nets\\ndiy devices\\nface-touch\\nmachine learning\\ngesture detection\\nmedical recommendations\\ncovid-19 pandemic\\nwearable devices\\nhand motions\\nhaptic feedback\\nrecurrent neural network\\ninput temporal sequences\\naccelerometer data\\nrnn training\\nrule-based detection algorithm\\nfalse detections rate\",\"273\":\"uncertainty\\nmeasurement units\\nhumanoid robots\\nestimation\\nposition measurement\\nrobot sensing systems\\nfiltering theory\\ngaussian distribution\\nkalman filters\\nlie groups\\nmatrix algebra\\nnonlinear filters\\nrobot kinematics\\nproprioceptive base estimator\\nhumanoid robot\\nextended kalman filtering\\nmatrix lie group\\ncontact-aided inertial-kinematic floating base estimation\\nconcentrated gaussian distributions\\ninertial measurement units\\nicub humanoid platform\\ndiligent-kio\",\"274\":\"phase measurement\\nswitches\\nrobot sensing systems\\nmanipulators\\nattenuation\\nsafety\\nsensors\\ncollision avoidance\\ngrippers\\nmanipulator dynamics\\nmotion control\\nposition control\\nproximity information\\nimpedance parameters\\nproximity measurements\\nimproving safety\\nimpedance controlled robot manipulators\\nproximity perception\\nproactive impact reactions\\nimpedance controlled robotic manipulators\\nproximity sensors\\nrobust collision avoidance abilities\\nfallback safety mechanism\\nimpedance controllers\\nrobot grasps objects\\nhigh positional accuracy\\nsoftness\\nproximity servoed manipulators\\nimpact attenuation\\ndamage reduction\\nunavoidable collisions\",\"275\":\"torque\\ntracking\\nsimulation\\ncollaboration\\nreal-time systems\\nsafety\\ntrajectory\\ncollision avoidance\\nmobile robots\\noptimisation\\npath planning\\nposition control\\nroad safety\\nroad vehicles\\nrobot dynamics\\nrobot vision\\ncollaborative robotics\\nbounding volumes\\nsafety controller\\npossible collisions\\nonline smooth stop trajectories\\noptimal trajectories\\ndynamic safety zones\\nsafety control approach\\nonline optimal scaling\",\"276\":\"solid modeling\\nthree-dimensional displays\\nforce measurement\\nforce\\ncollaboration\\nsafety\\ncollision avoidance\\nhuman-robot interaction\\nindustrial manipulators\\nindustrial robots\\niso standards\\nmotion control\\noccupational safety\\npath planning\\nrobot mass\\nimpact forces\\nsafe human-robot collaboration\\ncollaborative robots\\nprescribes force thresholds\\nmoving robot\\nhuman body parts\\nmaximum allowed speed\\ncollaborative manipulators\\nur10e\\nmeasuring device\\nrobot workspace\\nrobot base\\nforce evolution\\n3d collosion-force-map\",\"277\":\"robust control\\nperturbation methods\\nredundancy\\nhuman-robot interaction\\ncontrol systems\\nrobustness\\nsafety\\ncontrol system synthesis\\nmechanical systems\\napproach addresses safety\\nhuman-robot interaction perspective\\npre-defined region\\nvelocity constraints\\nexternal perturbations\\nnominal control law\\noperating region\\nsafety requirements\\nhuman-robot interaction task\\npassive control\\nphysical human-robot interactions\\nnovel safe\\nrobust control law\",\"278\":\"accelerometers\\nlocation awareness\\nforce measurement\\nredundancy\\nhuman-robot interaction\\nkinematics\\nrobot sensing systems\\nacceleration measurement\\nforce sensors\\nhumanoid robots\\nobservers\\ntorque measurement\\ncollision detection\\ndlr sara robot\\nsensing redundancy\\nphysical human-robot interaction\\nmodern lightweight robotics\\nexternal interactions\\neffective collaboration\\nsafe collaboration\\nextended momentum-based disturbance observer\\nadditional force-torque measurements\\nmultiple simultaneous contact locations\\nsingularity-free estimates\\nexternal forces\\ndlr lightweight robots\\nhigh resolution force-torque sensors\\nredundant arrangement\",\"279\":\"sensitivity\\nhuman-robot interaction\\nbenchmark testing\\ntools\\nrobot sensing systems\\nsensors\\nsafety\\ncollision avoidance\\ngrippers\\nimpact (mechanical)\\nmobile robots\\nrobots\\nrobot workspace areas\\ndynamic collision parameters\\nstandardized benchmarking procedure\\nuniversal robots ur10e\\ncsm\\ncontact sensitivity maps\\nbenchmarking robot collision handling systems\\nintended contacts\\nunintended contacts\\nsafe manner\\nproprioceptive sensing capabilities\\ncollision detection\\nidentification techniques\\ncommercially available robots\\ndynamic collisions\\nhuman co-worker\\nstandardized procedure\\nrobotic system\\nrobot user\",\"280\":\"recurrent neural networks\\nuncertainty\\nautomation\\nconferences\\ntime series analysis\\ncollaboration\\ncomplexity theory\\ncontrol engineering computing\\ngaussian processes\\nhuman-robot interaction\\nmixture models\\nrecurrent neural nets\\nautonomous tasks\\nintentional contacts\\naccidental contacts\\nhuman behavior\\nadmittance control strategy\\ninteraction wrench\\nkinova jaco2 robot\\ndata-driven approach\\nhuman-robot collaboration\\ngaussian mixture models\\ncontrol barrier functions\",\"281\":\"wrist\\nmeasurement\\nrobot motion\\ncodes\\nautomation\\nconferences\\ndetectors\\nautonomous aerial vehicles\\nhelicopters\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nmoving robot\\nwrist imu data\\nimu-equipped bracelet points\\nlearned classifier\\nwrist motion\\nreal-world pointing events\\nquadrotors\",\"282\":\"three-dimensional displays\\nnavigation\\nsurveillance\\nrobot vision systems\\ncrops\\nwheels\\nproduction\\nagriculture\\ncameras\\ndistance measurement\\nmanipulators\\nmobile robots\\nrobot vision\\noff-the-shelf pipe-rail trolley\\nmultimodal cameras\\nnavigation sensors\\nrobotic arm\\nclose surveying tasks\\nsurveying platform\\nsweet pepper\\ntomatoes\\nwheel odometry\\ndepth information\\nre-projection\\npathobot's crop surveying capabilities\\nglasshouse crop phenotyping\\nautonomous crop surveying\\nintervention robot\\n3d mapping\",\"283\":\"training\\nimage segmentation\\nhead\\nimage color analysis\\nlighting\\nstreaming media\\nrobot sensing systems\\nagriculture\\ncrops\\nimage colour analysis\\ncolour space variations\\noutdoor robustness\\ninstance segmentation\\nyield detection\\nharvest efficiency\\nvarying environmental conditions\\ncabbage plants\\nimage stream\\ncabbage heads\\ndifferent colour space representations\\ncolour information\\nsegmentation accuracy increase\\ncolour spaces\\nsaturation information\\nadditional segmentation improvements\\ncielab colour space\\ndepth information layer\",\"284\":\"conferences\\ncomputational modeling\\ntraveling salesman problems\\ninspection\\nminimization\\nagriculture\\ndistance measurement\\nagricultural robots\\nmobile robots\\npath planning\\ntravelling salesman problems\\nmp-stsp\\nglobal planning\\norchard inspection\\norchard precision agriculture setting\\nmultiplatform steiner traveling salesman problem\\nsparse targeted per-plant interventions\\nworkload balance\\neu h2020 project\\npantheon\\nrobotics\\nmotion planning\",\"285\":\"symbiosis\\nautomation\\nconferences\\nbuildings\\nmarkov processes\\nsearch problems\\nelevators\\ndiscrete time systems\\nmobile robots\\nprobability\\ntrees (mathematics)\\nmobile social robot\\nclosed door\\nbehavior tree framework\\nmodular action sequence\\neasily extendable action sequence\\ndiscrete time markov chain\\nfinding person\\nsymbiotic autonomous mobile robot tasks\\npeople search\\nbehavior-tree-based person search\",\"286\":\"robot sensing systems\\nsensor systems\\nreal-time systems\\ntrajectory\\nsensors\\nmobile applications\\nrisk management\\nagricultural robots\\nbrakes\\ncollision avoidance\\nhazards\\nmobile robots\\nparametrizable scene risk evaluator\\nagricultural robotics\\nrisk reductions\\npre-defined hazard zones\\nhuman motion predictions\\nhuman avoidance behaviors\\nautonomous mobile application\\nrobotic applications\\nsafety integration components\\nopen-field mobile robot\\nhuman-aware risk-based braking system\",\"287\":\"fluctuations\\nlimiting\\nnavigation\\nneural networks\\nmodulation\\nreinforcement learning\\nkinematics\\nlearning (artificial intelligence)\\nmobile robots\\npredictive control\\nsteering systems\\nwheels\\nonline velocity fluctuation\\noff-road path\\nwheeled mobile robot\\npoor grip conditions\\nlongitudinal velocity\\nsafe navigation\\ntracking errors\\nonline speed fluctuation\\nlateral error\\ngiven threshold\\nneural network\\nreinforcement learning method\\nspeed modulation\\nexisting model-based predictive steering control\\nconsistent travel time\\nclassical constant speed method\\nkinematic speed fluctuation method\",\"288\":\"fuzzy logic\\ncouplings\\nenergy consumption\\nautomation\\nconferences\\nneural networks\\nswitches\\ncontrol system synthesis\\nfuzzy control\\nlinearisation techniques\\nmobile robots\\nnonlinear control systems\\nwheels\\ncomplex nonlinear controllers\\nfield concept\\ncurrent states\\nsimulated tests\\ndynamic criteria\\ncontrol criteria\\ntwo-wheeled self-balancing robots\\nself-balancing robot\\nnonlinear system\\nstrong coupling dynamics\\ncommon practices\\ncontrol systems\\nlinear controllers\\ntwsbr\\ncoppeliasim\\nswitching control\\nsmc\\npid\",\"289\":\"adaptation models\\nautomation\\nconferences\\nrobot control\\nforce\\nstability analysis\\nhardware\\nadaptive control\\nmulti-agent systems\\nmulti-robot systems\\nstability\\nadaptive authority allocation\\nmultiple controllers\\npassivity-disrupting scaling-related terms\\ntime-varying force scaling factors\\nfinite-gain stable multiagent robot control\\nmulti-agent control\\noutput strict passivity\",\"290\":\"machine learning algorithms\\nrobot kinematics\\nheuristic algorithms\\ndelay effects\\ncomputational modeling\\nmaintenance engineering\\ndelays\\ncontrol engineering computing\\ncooperative systems\\ndecentralised control\\nmobile robots\\nmulti-robot systems\\ncommunication delays\\nphysical robot experiments\\ndecentralized connectivity maintenance\\ncontrol barrier functions\\nworld deployment\\nmultirobot system\\nconnectivity maintenance controller\\nundesired real world effects\\ncomputational time delays\\nrobotic setup\\nconnectivity maintenance control strategy\\ncontrolled system\",\"291\":\"jacobian matrices\\nnumerical analysis\\ndesign methodology\\ncomputational modeling\\nmathematical models\\nnumerical models\\nnonlinear dynamical systems\\nmobile robots\\nmulti-robot systems\\nmultirobot implicit control\\nevaders\\nrobotic herders\\nstrong nonlinear reactive dynamics\\nheterogeneous entities\\nflexible control solutions\\nnonlinearities\\nimplicit equations\\nnumerical analysis theory\\ncontinuous time implicit system\\nherding actions\",\"292\":\"measurement\\nfiltering\\nfrequency-domain analysis\\nsemantics\\ntransforms\\nfeature extraction\\nindoor environment\\nhough transforms\\nmobile robots\\nstraight-line elements\\nmetric maps\\ndominant directions\\nclutter\\nfrequency domain\\nmapping applications\\nwall detection\\nstructural feature retrieval\\nwalls\\ncluttered environments\\nrobust frequency-based structure extraction\\nart mapping algorithms\\nhigh-quality maps\\nmap quality\\nmap processing\\nsemantic understanding\\nbuilding-level structure detection\\nrobotic maps\\nindoor environments\",\"293\":\"legged locomotion\\ngeometry\\ncodes\\nhardware\\nlibraries\\nsafety\\ntrajectory\\ncollision avoidance\\nmanipulator dynamics\\nmobile robots\\npath planning\\ntorque control\\nhybrid collision model\\nsafety collision control\\nreactive control\\ndynamics robots\\ncontrol methods\\ncollision-free trajectory\\nlightweight models\\nconservative models\\nsmooth models\\nself-collisions\\nrobot control\\ndistinct legs\\nstandard forward kinematics\\ndata-driven approach\\nstandard collision library\\nsimple torque-based controller\\ncontrol law\\nunexpected self-collision\\nrobot hardware\\nrobot model\",\"294\":\"closed-form solutions\\nautomation\\ncomputational modeling\\nroads\\nconferences\\npose estimation\\nbenchmark testing\\ncameras\\nimage sequences\\npolynomials\\nrobot vision\\nacs\\n6dof relative\\nfirst-order rotation approximation\\nsingle polynomial\\ngeneralized camera model\\nclosed-form solution\\naccurate relative pose estimation\\nexploited correspondences\\nrobust estimators\\nefficient recovery\\nmulticamera motion\\naffine correspondences\\nmulticamera system\",\"295\":\"automation\\nnavigation\\nconferences\\npipelines\\ncost function\\nreliability\\nmobile robots\\ncollision avoidance\\npath planning\\ndynamic-aware autonomous exploration\\npopulated environments\\ninitially unknown territories\\ncomplete representations\\nreal-life applications\\ndynamic obstacles\\nexploration process\\nexits\\nentrances\\nnovel exploration strategy\\ncomplete exploration outcomes\\nreliable exploration outcomes\\ndynamic frontiers\\nunknown regions\\nsimulated environments\\npopulated scenarios\",\"296\":\"geometry\\nrobot motion\\nvisualization\\ndynamics\\npredictive models\\nimage representation\\nplanning\\ngeneralisation (artificial intelligence)\\nlearning (artificial intelligence)\\nlearning systems\\nmanipulators\\nmotion control\\npath planning\\npredictive control\\nrobot vision\\ncontrol primitives\\ntask success\\ngoal-conditioned end-to-end visuomotor control\\nversatile skill primitives\\nvmc\\none-shot imitation learning\\nmodel-predictive control\\nplanning horizon\\ndynamic image representation\\ncomplex manipulation tasks\\nraw image observations\\nscene dynamics forward model\\ntask-specific fine tuning\\ncomplex action sequences\\ngeneralisation capability\\nvisual noise\\ncluttered scene\\nunseen object geometry\",\"297\":\"suspensions (mechanical systems)\\ntorque\\ncomputational modeling\\npredictive models\\nobservers\\nsteady-state\\nvehicle dynamics\\ngrippers\\nmanipulators\\nmobile robots\\nnonlinear control systems\\npredictive control\\ntorque control\\nnonlinear model predictive control\\naerial manipulation\\nrobotic manipulators\\naerial manipulator\\nelastic suspension\\names\\nomnidirectional aerial vehicle\\nnonlinear model predictive controller\",\"298\":\"power cables\\npower distribution\\nstatic analysis\\npower system stability\\npneumatic systems\\nrobot sensing systems\\nunmanned aerial vehicles\\nautonomous aerial vehicles\\nhelicopters\\npower distribution lines\\nsensors\\ntelerobotics\\nuav\\nautonomous alignment algorithm\\npneumatic-mechanical systems\\nautonomous power line sensor unit deployment\\nenergy sector\\npower distribution companies\\nhelicopter\\nmodular mechanical system\",\"299\":\"rotors\\nrobot sensing systems\\nprobabilistic logic\\naerodynamics\\nunmanned aerial vehicles\\nsystem identification\\ncalibration\\nautonomous aerial vehicles\\nhelicopters\\nobservability\\nstate estimation\\nquadrotor uav\\ncontrol design\\nreliable navigation\\nrotorcraft unmanned aerial vehicles\\nonline estimation\\nsystem identification parameters\\nself-calibration states\\nvehicle control\\nobservability analysis\",\"300\":\"power demand\\npropellers\\nconferences\\ncomputational modeling\\nmorphology\\naerodynamics\\nreliability engineering\\naerospace robotics\\nautonomous aerial vehicles\\ncompensation\\ngaussian processes\\nspace vehicles\\ngeometry-aware compensation scheme\\nmorphing drones\\naerodynamic effects\\npartial overlap\\npropeller\\nmorphing quadrotor\\nmorphology-aware control scheme\\ncomplex models\\ncomputationally expensive aerodynamical models\",\"301\":\"three-dimensional displays\\nfiltering\\nnavigation\\nsemantics\\nrobot sensing systems\\nrobustness\\nreal-time systems\\nrobot vision\\nsplatplanner\\nefficient autonomous exploration\\nmicroaerial vehicle\\nmav\\nactive depth sensor\\non-board computation\\nend-to-end autonomous planner\\npff\\nbilateral filtering\\npermutohedral lattices\\nspatial information\\nfrontier-neighborhoods\\nexploration speed\\nend-to-end system\\npermutohedral frontier filtering\\naerial systems\\nperception and autonomy\\nvision-based navigation\\nmotion and path planning\",\"302\":\"three-dimensional displays\\nautomation\\ncomputational modeling\\nconferences\\ncomputational efficiency\\nhistory\\ngraph theory\\nmobile robots\\noptimisation\\npath planning\\nfast sampling-based next-best-view exploration algorithm\\nmav\\nmicroaerial vehicles\\nfrontier-based approaches\\nunexplored areas\\nsampled point\\nyaw angle\\npotential gain\\nexploration objective\\nnearby exploration\\ngain computation bottleneck\\nsampling strategy\\nmap expansion\\nexploration time\",\"303\":\"program processors\\nneuromorphics\\nstatistical analysis\\ncomputational modeling\\nneurons\\nprototypes\\nunmanned aerial vehicles\\naerospace robotics\\ncollision avoidance\\nmobile robots\\nneural nets\\nconstrained systems\\nmicroair vehicles\\nmavs\\nrobust skills\\nautonomous skills\\nobstacle avoidance\\nrobotic platforms\\nreality gap\\nloihi neuromorphic chip prototype\\nflying robot\\nspiking neural network\\nthrust command\\nventral optic flow field\\nautonomous landing\\npython-based simulator\\npysnn library\\nnetwork architecture\\nroot-mean-square error\\nthrust setpoint\\nspike sequences\\nrobotics\\nneuromorphic control\\noptic-flow-based landing\\nloihi processor\\nneuromorphic processors\\ncomputing modules\",\"304\":\"visualization\\nprogram processors\\npower demand\\nneuromorphics\\nprocess control\\nvision sensors\\nhardware\\naerospace control\\nautonomous aerial vehicles\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nneural chips\\nneurocontrollers\\nrobot vision\\nvelocity control\\nneuromorphic chip\\ndrone controller\\nevent-based perception\\nsnn controller\\non-chip learning\\nspiking neuronal network\\nneuromorphic vision-based controller\\nneuromorphic hardware\\nevent-based vision sensors\\nhigh-speed control\\nevent-based cameras\\nultra-fast vision-driven control\\nevent-based vision algorithm\\nevent-driven control\\nhigh-speed uav control\",\"305\":\"neuromorphics\\nnetwork topology\\nneural networks\\nunmanned aerial vehicles\\nstability analysis\\ndata models\\ntopology\\nadaptive control\\naerospace robotics\\nclosed loop systems\\ndeep learning (artificial intelligence)\\nlearning systems\\nneurocontrollers\\nstability\\nvariable structure systems\\nvariable topologies\\ndata stream\\nprogressive layer adaptation\\nadaptive structure\\nsliding mode controller\\naerial robot\\nadaptation capability\\ndynamic topology\\nconventional learning approach\\nrepresentative environment\\ncomputational power\\nonboard computation limitations\\ndeep neuromorphic controller approach\\nmemory allocation\\nvariable layered neural network controller\\nmultilayered neural network controller\\ndynamic depth\\nnetwork parameter update rule\\nclosed loop system stability\\nerror dynamics convergence\\nsliding surface\\ncomputational efficiency\\nembedded model\",\"306\":\"resistance\\nautomation\\nconferences\\nfingers\\nexoskeletons\\nfatigue\\nimpedance\\nbiomechanics\\ndexterous manipulators\\nservice robots\\nmiddle fingers\\nran control\\ntie-exo\\nfinger fatigue\\nfinger impedance control\\ninteractive impedance\\nvariable soft finger exoskeleton\\nfatigue-induced mechanical impedance\",\"307\":\"wrist\\nergonomics\\nwearable computers\\nredundancy\\nnull space\\ntraining data\\nkinematics\\nartificial limbs\\nend effectors\\nhandicapped aids\\nhuman-robot interaction\\nmanipulator kinematics\\nmedical computing\\nmedical robotics\\nposition control\\npositioning error\\nupper-arm robotic prosthesis\\nend-effector task\\nbody compensations\\njoint positioning\\nproximal joints\\namputated subject\\nhand positioning task\\nnatural posture\\njoint posture\\nrobot joints\\nnonamputated subjects\\nerratic positioning\\ndistal joints\\njoint errors\\nhuman-robot kinematic chain\\nrobotic distal side\",\"308\":\"costs\\nergonomics\\nredundancy\\ncollaboration\\nestimation\\nkinematics\\nwearable robots\\nadaptive control\\ncontrol engineering computing\\nhuman-robot interaction\\nmobile robots\\nprosthetics\\nredundant manipulators\\nintent-aware control\\nkinematically redundant systems\\ncollaborative wearable robots\\nhuman-robot collaboration scenarios\\nredundant leader-follower setup\\nstable execution\\nhuman cost\\ncognitive cost\\nsystem redundancies\\nintended task\\ntask null-space\\nsupernumerary robots\\nprosthetic robots\\nkinematic chain\\nkinematic redundancies\\nintention-recognition\\ncost-minimization\\nseamless robotic assistance\\nhuman posture\\nhuman intended tasks\\nwearable robotics\",\"309\":\"automation\\nconferences\\nstability analysis\\nsensors\\nsafety\\nload modeling\\nalarm systems\\nconstruction industry\\ndesign engineering\\nexcavators\\noccupational safety\\nrisk management\\nworking environment\\nconstruction sites\\nriskiest working fields\\nconstruction machines\\nmachine stability\\npotentially harmful situations\\ndynamic fall prediction system design\\nzero moment point concept\\nzmp\\nalarm rings\\nsystem development\\nsystem validation\",\"310\":\"deep learning\\nadaptation models\\nconferences\\nsemantics\\ndynamics\\nkinematics\\ntrajectory\\ndriver information systems\\nlearning (artificial intelligence)\\nneural nets\\nvehicle dynamics\\nkinematic models\\nlearning models\\ncomplex semantic maneuvers\\nvehicle states\\nsurrounding objects\\nnovel maneuver fusion layer\\nlogic-based semantic maneuvers\\ndeep neural networks\\ndifferent loss functions\\ndifferent maneuver types\\nhierarchical multitask learning framework\\nadaptive loss\\nurban driving\\nfeasible multimodal trajectory prediction\\nadaptive multimodal trajectory prediction\\nsemantic maneuver\\npredicting trajectories\\nsafe autonomous driving\\ngeneral unconstrained machine\\nunrealistic predictions\\ndifferent motion constraints\\nshallow maneuvers\",\"311\":\"tracking\\nroads\\nsemantics\\nurban areas\\npredictive models\\nfeature extraction\\nspatial databases\\ncartography\\ncomputational geometry\\nconvolutional neural nets\\ngeophysical image processing\\nimage representation\\nimage segmentation\\nlearning (artificial intelligence)\\nmobile robots\\nroad traffic\\nroad vehicles\\nlatent representation\\nsparse semantic layers\\ncapsule networks\\nsafety systems\\nautonomous vehicles\\nconvolutional neural networks\\nroad structure\\ncapsnets\\nhierarchical representation learning\\nfeature detection\\nshort-term motion prediction\\nrasterised top-down road image\\nhigh-definition map regions\",\"312\":\"uncertainty\\ntarget recognition\\nmeasurement uncertainty\\nhuman-robot interaction\\nmodulation\\nprobabilistic logic\\nlibraries\\ncritical points\\nimage motion analysis\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nobserved trajectory\\nconsecutive critical points\\nestimated trajectory\",\"313\":\"state feedback\\ntarget tracking\\nperturbation methods\\nmemory management\\noptimal control\\nhumanoid robots\\nreal-time systems\\ncollision avoidance\\ndynamic programming\\nmotion control\\nperturbation techniques\\npredictive control\\ntorque control\\ntrajectory control\\ntorque-controlled talos\\nwhole-body model predictive control\\ntorque-control humanoid robot\\nwhole-body target tracking\\nexternal perturbations\\ndifferential dynamics programming\\nstate-feedback control\\noptimal trajectories\\nlow-level torque controller\\ndirect torque measurement\\nreceding horizon\\nmemory of motion\",\"314\":\"benchmark testing\\napproximation algorithms\\nmanipulators\\nreal-time systems\\nstability analysis\\ntask analysis\\ntuning\\napproximation theory\\ncollision avoidance\\nconstraint handling\\ndynamic programming\\niterative methods\\nlinear quadratic control\\nmobile robots\\nnewton method\\nnonlinear control systems\\nnonlinear programming\\noptimal control\\noptimisation\\npredictive control\\nquadratic programming\\ncontinuous-time ddp-based model predictive control\\nsequential linear quadratic algorithm\\ncontinuous-time version\\ndifferential dynamic programming technique\\ngauss-newton hessian approximation\\nrobotics community\\ncomplex trajectory optimization problems\\nddp-based formulations\\nproperly incorporate path constraints\\nconstrained slq algorithm\\nimplemented projection technique\\naugmented-lagrangian approach\\nappropriate multiplier update law\\nsingle inner loop iteration\\nouter loop iteration\\nreal-time model-predictive control applications\\ninequality-constrained case\\naugmented-lagrangian penalty functions\\ncorresponding multiplier update rules\",\"315\":\"satellites\\nnasa\\noptimal control\\nhumanoid robots\\npropulsion\\nreproducibility of results\\nhardware\\ndynamic programming\\nmobile robots\\nrobot dynamics\\nrobot programming\\nhighly dynamic motion\\ncontrol inputs\\npropulsion system\\noptimal control solutions\\nhuber regularization penalties\\nsparsity-inducing optimal control methods\\ninducing sparsity\\ncanonical dynamics systems\\nstate-of-the-art differential dynamic programming-based solvers\\nddp\\nl2 regularization\",\"316\":\"navigation\\ncomputational modeling\\nscalability\\nrobustness\\nplanning\\ntrajectory\\nmobile robots\\ncollision avoidance\\nlearning (artificial intelligence)\\npath planning\\ntorque control\\nwheels\\npassive navigation planning algorithm\\ncollision-free control\\ncomplex environments\\nhighly variable environments\\nmultiple model\\nlearning-based approaches\\nsignificant computational resources\\nglobally stable passive controller\\nsmooth trajectories\\nenvironmental conditions\\nrecently proposed fractal impedance controller\\nelastic bands\\nfinite time invariance\\ninteractive navigation\\nlow bandwidth feedback\\nswarm simulation\\nholonomic wheeled platform validating smoothness\\ncomputational complexity\\nlow-power microcontrollers\",\"317\":\"automation\\nconferences\\nplanning\\nreliability\\nautomobiles\\nassembling\\ncad\\ncomputational geometry\\nproduction engineering computing\\nassembly sequence planning\\nreal-world cad-scenarios\\ndisassembly path\\nnarrow passages\\nasp context\\nrigid body motion planner\\nevt\\nexpansive voronoi tree\\ngeneral voronoi diagram\",\"318\":\"message passing\\nheuristic algorithms\\nestimation\\ntransforms\\ninference algorithms\\nplanning\\ntrajectory\\ncollision avoidance\\nconvergence\\ngaussian processes\\ngraph theory\\nleast squares approximations\\nminimisation\\nmobile robots\\noptimisation\\npath planning\\nfactor graph\\ngraph nodes\\nharder planning problems\\nexemplary motion planning tasks\\nms2mp improves\\nmin-sum message passing algorithm\\ncontinuous-time trajectory\\nmotion planning problem\\nplanned trajectory\\nfast convergence time\\ncompound factor node\\nlinearly structured graph\\nms2mp performs numerical optimization\",\"319\":\"automation\\nnavigation\\nwheelchairs\\nconferences\\ntrajectory\\nmobile robots\\nindexes\\ncurve fitting\\npath planning\\ntrees (mathematics)\\npower wheelchair navigation\\nnonholonomic constraints\\nnonholonomic robot navigation\\nrapidly-exploring random tree\\ntransport context\\npassenger comfort\\nbounded curvature value\\ncbb-rrt\\non-the fly comfortable path generation\\ncubic be\\u0301zier local path planner\\ncontinuous cubic be\\u0301zier piecewise curves connection\\nnon-holonomic motion planning\\nmotion and path planning\\nhuman factors and human-in-the-loop\",\"320\":\"geometry\\nthree-dimensional displays\\nnavigation\\nconferences\\nrobot sensing systems\\npath planning\\nplanning\\ncollision avoidance\\ndistance measurement\\ngraph theory\\nmobile robots\\nsafe global paths\\nglobal consistency\\nodometry drift\\ncomputationally efficient global path planning\\nplanning approach\\nplanning time\\nglobally safe paths\\nsubmap collection\\nreal-world environments\\nmonolithic maps\\nglobal map deformations\\n3d global planner\\ncomplex environments\\ncluttered environments\\nrobotics applications\\nprecomputed local graphs\\n3d signed distance function submaps\",\"321\":\"automation\\ncurrent measurement\\nconferences\\nredundancy\\nkinematics\\nend effectors\\ntrajectory\\nindustrial manipulators\\nmanipulator kinematics\\nmobile robots\\noptimisation\\npath planning\\nredundant manipulators\\nserial robot\\nonline trajectory generation\\nglobally optimal online redundancy resolution\\ndof\\nmanipulability measure\\nse(3) trajectories\\ngraph search techniques\",\"322\":\"measurement\\ncosts\\nconferences\\nend effectors\\nplanning\\ntrajectory\\nspace exploration\\ndifferential geometry\\nmanipulators\\nmobile robots\\nmotion control\\noptimisation\\npath planning\\nacceleration\\ngeodesics\\nnatural manipulator movement\\nsmooth manipulator movement\\nrobot arm motion planning\\njoint trajectories\\nrobotic manipulators\\nend-effector\\njoint movement\",\"323\":\"robot motion\\nservice robots\\ntracking\\nrobot kinematics\\nlinearity\\ntools\\nstability analysis\\nadaptive control\\ncontrol system synthesis\\ndifferential equations\\nfeedforward\\nindustrial manipulators\\nmobile robots\\nmotion control\\nposition control\\nflexible joint robots\\nrobotic motions\\ncomplex coordinated motions\\nindustrial robot\\nflexdmp\\ndynamic movement primitives\\nmultiple degrees of freedom\\nsecond order autonomous differential equations\\nfeedforward control\\nstructural properties\\nzero tracking error\\nhigh fidelity simulation model\",\"324\":\"simultaneous localization and mapping\\nlimiting\\ncodes\\nconferences\\npose estimation\\nbenchmark testing\\ncameras\\nfeature extraction\\nimage colour analysis\\nimage reconstruction\\nmobile robots\\nobject tracking\\nrobot vision\\nslam (robots)\\nrobust planar tracking\\nrobust rgb-d slam system\\nindoor scenes\\ndense mapping\\nmanhattan world assumption\\nlow-drift camera\\nnonmw environments\\ndrift-free rotation estimation\\nmanhattan frame observations\\ntranslation estimation\\nnonmw scenes\\nplane features\\nnonplanar regions\\nnonplanar surfels\\nreconstruction accuracy\\nsurfel-based dense mapping strategy\\ncpu\\nmanhattanslam\",\"325\":\"visualization\\nspatial coherence\\nprototypes\\nparallel processing\\nrobot sensing systems\\nhardware\\ncomputational efficiency\\ncartography\\ndata visualisation\\nimage resolution\\nimage sequences\\nparallel architectures\\nppa sensor\\nprocessing elements\\nvisual information\\nparallel processing power\\nimage sequence\\non-sensor mapping\\nweighted node mapping\\nvisual route mapping\\npixel processor array\",\"326\":\"training\\nvisualization\\nimage recognition\\nfeature detection\\npipelines\\nlighting\\nfeature extraction\\nentropy\\nimage classification\\nimage matching\\nlearning (artificial intelligence)\\nobject recognition\\nrobot vision\\nfeature detections\\nplace recognition pipeline\\nstandard place recognition datasets\\ntraining dataset\\ncompared state-of-the-art approaches\\njoint local feature detection\\nvisual place recognition\\nappearance changes\\nchanging illumination\\ngeometric changes\\nviewpoint changes\\nlearning-based local image feature pipelines\\nnovel attentive feature pooling method\\nlocal image features\\nmoderately sized datasets\\nclassification training setup\\njoint loss function\\ncross-entropy loss\\nclassification task\\ntemperature 24.0 k\",\"327\":\"location awareness\\nvisualization\\nimage recognition\\nsimultaneous localization and mapping\\ncodes\\ndatabases\\nconferences\\nfeature extraction\\nimage matching\\nimage retrieval\\nmobile robots\\nnearest neighbour methods\\nneural nets\\nobject detection\\nobject recognition\\npath planning\\nrobot vision\\nslam (robots)\\nvisual databases\\napproximate nearest neighbor based methods\\nann\\ndatabase image matching\\nsequence-based method\\nimage comparisons\\nimage pairs\\nstructural knowledge\\nvisual place recognition\\nquery images\\nloop closure detection\\nslam\\ncandidate selection\\nglobal localization\\nintradatabase similarities\",\"328\":\"training\\nlocation awareness\\nannotations\\nfuses\\nconferences\\nsemantics\\npipelines\\ncameras\\nfeature extraction\\nimage representation\\nimage segmentation\\nlearning (artificial intelligence)\\ndomain-specific relocalization\\nsemisupervised framework\\nmultilevel description learning\\nrobust camera relocalization\\nweakly-supervised semantic segmentation\\nlocal feature description\\naugmented descriptors\\nstable high-level representation\\nlocal feature dis-ambiguity\\nend-to-end semantic description learning\\ndescriptor segmentation module\\nsemantic descriptors\\ncluster centers\\nstandard semantic segmentation loss\\ndomain-specific usage\\nsemantic annotations\\nlearned descriptors\\n2d-2d pixel correspondences\",\"329\":\"meteorological radar\\nvisualization\\nlaser radar\\nroads\\nradar detection\\nradar\\nobject detection\\nobject tracking\\nradar imaging\\nradar tracking\\nstereo image processing\\ntraffic engineering computing\\nvisual perception\\nradiate\\nautomotive perception\\nbad weather\\nautonomous cars\\nperception systems\\nlidar sensors\\ngood weather conditions\\nradar sensing\\nsafe autonomous driving\\nannotated radar images\\nroad actors\\nradar image\\nfog\\nsnow\\npublic radar dataset\\nhigh-resolution radar images\\npublic roads\\nradar based object detection\\nradar data\\nstereo images\\npublic dataset\\nradar dataset in adverse weather\\ntime 3.0 hour\",\"330\":\"surface reconstruction\\nlaser radar\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nautonomous systems\\nconferences\\npose estimation\\ndistance measurement\\nimage fusion\\nimage matching\\nimage reconstruction\\nimage registration\\nmedical image processing\\nmesh generation\\nmobile robots\\noptical radar\\nrobot vision\\nsensor fusion\\nslam (robots)\\nsolid modelling\\ncommon mapping approaches\\ntruncated signed distance function\\nmap representations\\nlidar-based odometry estimation\\npoisson surface reconstruction\\nlidar odometry\\nessential building blocks\\nmapping quality\\nframe-to-mesh icp\\nslam approaches\\ntriangle mesh\\nsliding window fashion\\naccurate local maps\",\"331\":\"surface reconstruction\\nimage segmentation\\nlaser radar\\nstructure from motion\\ncodes\\nconferences\\nfeature extraction\\ncameras\\nimage fusion\\nimage reconstruction\\nimage registration\\nmesh generation\\nmobile robots\\nobject tracking\\noptical radar\\npose estimation\\nrobot vision\\nleverage common geometric features\\nlidar scans\\nimage data\\nhigher-level space\\ndetected line segments\\noptimized line segments\\nfinal mesh\\nrecently published dataset\\nnewer college dataset\\nsurvey-grade 3d scanner\\nstate-of-the-art lidar survey\\nhighly accurate ground truth\\nlidar-monocular surface reconstruction\\nsfm\\naccurate poses\\nsuitable visual features\\nfinal 3d mesh\\nmonocular camera\\nfeatureless subjects\\nsensor modalities\\nfundamentally different characteristics\\nimage features\\nlidar points\",\"332\":\"legged locomotion\\nvibrations\\ntorque\\nautomation\\ntracking\\ninstruments\\nconferences\\npendulums\\nsprings (mechanical)\\nlower body\\nspring-loaded passive prismatic joint\\nbalance controller\\nspring-loaded joint\\nspringy leg balancing\\nplanar double pendulum balancing\\nhigh-performance monopedal robot\",\"333\":\"legged locomotion\\ntorso\\nautomation\\nnavigation\\nconferences\\nbenchmark testing\\nbatteries\\ndesign engineering\\nmechanical stability\\npath planning\\nrobot dynamics\\nrough terrain\\nnonanthropomorphic stabilization system\\nmechanical design\\ngyrubot\\nplanar bipedal robotics\",\"334\":\"measurement\\nthree-dimensional displays\\nautomation\\nfriction\\nconferences\\ncomputational modeling\\nforce\\nmechanical contact\\noptimisation\\nshapes (structures)\\ncoulomb friction cone\\nresultant friction force\\ntrajectory optimization experiment\\ncompute time\\ncomputational complexity\\n4-sided polyhedral estimation\\nmaximum dissipation principle\",\"335\":\"automation\\nservice robots\\nconferences\\ngenetic programming\\ntask analysis\\ngenetic algorithms\\nindustrial robots\\nlearning (artificial intelligence)\\nrobot programming\\ntrees (mathematics)\\nunpredictable environment\\nmodern industrial applications\\nbehavior tree\\nrobotic task\\nlearned bts\\ntask specific heuristics\\nrobotic applications\\nbehavior trees\\nmobile manipulation\",\"336\":\"manifolds\\nlearning systems\\ncosts\\nmonte carlo methods\\nconferences\\noptimization methods\\nplanning\\ncontrol engineering computing\\ngraph theory\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nnonlinear programming\\npath planning\\nsampling methods\\ntree searching\\nnaive sequential conditional sampling\\ntrajectory optimization\\nmonte-carlo tree search\\nsequential manipulation tasks\\nrobotic sequential manipulation\\nsequential sampling problems\\nrobot configurations\\nlearning efficiency constraint graph sampling\\nsampling-based motion planning\\nsequential robot manipulation planning\\nnonlinear program\",\"337\":\"learning systems\\nvisualization\\ncomputer vision\\nthree-dimensional displays\\nconferences\\nend effectors\\ntrajectory\\nlearning (artificial intelligence)\\nmanipulators\\nrobot vision\\nstate estimation\\nsingle human demonstration\\ncoarse-to-fine imitation\\nvisual imitation learning\\nrobot manipulation task\\nend-effector\\nobject interaction\\ncoarse approach trajectory\\nfine interaction trajectory\\nstate estimator\\ncomplex interaction trajectory\",\"338\":\"uncertainty\\nautomation\\nconferences\\nestimation\\nbenchmark testing\\nstandards\\ndeep learning (artificial intelligence)\\nestimation theory\\ninference mechanisms\\noptimisation\\nregression analysis\\nstereo image processing\\ndisparity distributions\\nstereo disparity prediction networks\\nregression problem\\nsingle scalar output\\nsingle well defined disparity\\nlearning problem\\ndistribution target\\nrobust estimate\\nuncertainty estimate\\nneural-network-based disparity estimation approaches\",\"339\":\"deep learning\\nadaptation models\\ncorrelation\\nautomation\\nconferences\\nneural networks\\nestimation\\ndeep learning (artificial intelligence)\\nmobile robots\\nposition control\\nregression analysis\\ngrasp parameters\\ndetection network\\nmethod correlating graspability\\ngrasp regression\\nbetter grasp prediction\\ngrasping objects\\ndeep neural networks\\nactual graspability score\\nscoring graspabilit\\njacquard dataset\",\"340\":\"semantics\\nrobot vision systems\\nobject detection\\ncomputer architecture\\nmachine learning\\npredictive models\\nrobot sensing systems\\nfeature extraction\\nimage colour analysis\\nimage fusion\\nimage segmentation\\nimage sensors\\nlearning (artificial intelligence)\\nobject recognition\\nvideo signal processing\\npredicted depth\\nobstacle detection\\nsemantic segmentation\\nrobotic navigation systems\\nother autonomous navigation systems\\nstate-of-the-art sod methodologies\\ndepth information\\nrgb-d cameras\\nflexibility\\ndepth map approximations\\nsingle rgb image\\nnovel monocular sod methodology\\nmonosod\\ntwo-branch cnn autoencoder architecture\\ndepth maps\\nestimating saliency\\nstate-of-the-art sod methods\\nrgb-d data\\nmonocular salient object detection\",\"341\":\"three-dimensional displays\\nruntime\\nautomation\\nconferences\\nhumanoid robots\\nplanning\\nproposals\\ncollision avoidance\\ngraph theory\\nminimax techniques\\nmobile robots\\nmotion control\\nsearch problems\\nfootstep planning\\nplanning progress\\nhumanoid robot\\ncomputation time\\nstatic environments\\nuncontrolled environments\\nunforeseen motion\\nreplanning rate\\na* search\\nobstacle-aware heuristic function\\nrotate-translate-rotate motions\\nshortest path\\ntarget-oriented solution\\nbounded computation time\\nlocal minima\\nfrequency 50.0 hz\",\"342\":\"motion planning\\nautomation\\nconferences\\nhumanoid robots\\naerospace electronics\\nvisual servoing\\ntask analysis\\nlegged locomotion\\nmotion control\\nrobot dynamics\\nwhole-body motion control\\nhumanoid robot\\nbig potential\\nversatile machines\\nangular motion\\ncentroidal momentum conservation\\nfloating humanoid\\nspace scenario\",\"343\":\"analytical models\\nphase measurement\\nautomation\\nservice robots\\nconferences\\nemployment\\nsearch problems\\nhazards\\nindustrial robots\\noccupational safety\\npersonnel\\nsafety\\nsafety-critical software\\nvirtual reality\\nworkplace simulation models\\nvirtual humans\\nworkplace hazards\\nhazard analyses\\ntypical industrial robot workplaces\\nvirtual adversarial humans\\nplanning phase\\npotential hazards\\nhuman workers\\nappropriate safety measures\\nhazard analysis methods\\nhuman reasoning\\nabstract system models\\nframes hazard analysis\\nsearch problem\\ndynamic simulation environment\\nsimulation sequences\\nhazardous situations\",\"344\":\"measurement\\nnavigation\\ntools\\nbenchmark testing\\nrobot sensing systems\\nsoftware\\nsensors\\ncomputer simulation\\ncontrol engineering computing\\nmobile robots\\npath planning\\nsimulation-based benchmark tool\\nrobot navigation\\nhuman crowd\\npublic spaces\\nvirtual crowded environment\\nevaluation metrics\",\"345\":\"shape\\nforce\\ncollaboration\\nbenchmark testing\\nrobot sensing systems\\nmanipulators\\nsensors\\ncapacitive sensors\\nhuman-robot interaction\\nindustrial manipulators\\nmanipulator dynamics\\nmotion control\\noccupational safety\\nsafety\\nclose-range perception\\nhrc tasks\\nperception gap\\ntactile detection\\nmid-range perception\\ncps\\npure tactile\\nforce sensing\\noperating speed\\ncollaborative robots\\ncapacitive coupling\\nmaterial properties\\nuniversal benchmark test procedure\\noperation speed\\nunified perception benchmark\\ncapacitive proximity sensing\\nhuman workers\\nundesired contacts\\ncapacitive proximity sensors\\ncost-effective solution\\nentire robot manipulator\\nsafe human-robot collaboration\",\"346\":\"training\\nautomation\\nshape\\nconferences\\nhumanoid robots\\nforce control\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nhand reaching\\nhuman-robot\\nnonverbal interactions\\nhandshake\\ncomplex emotions\\nimportant skill\\nsocial robot\\nhumanrobot handshaking\\nthird-person human-human interaction data\\nnonbackdrivable robots\\ndifferent humanoid robots\\nhuman-interaction partners\",\"347\":\"geometry\\nautomation\\nconferences\\nhaptic interfaces\\nplanning\\ntask analysis\\nrobots\\nmedical image processing\\nmedical robotics\\nsurgery\\ntelerobotics\\nmedical images\\ntask-space\\nonline correction\\nplanned task\\npathfollowing task\\nhaptic interface\\nsimultaneous haptic guidance\\ntask parameters\\nrobotic teleoperation\\ngeometrical approach\\ndexterity\\npre-operative planning\",\"348\":\"mechanical sensors\\nhuman-robot interaction\\ntactile sensors\\ncollaboration\\ncapacitance\\nskin\\nrobustness\\ncompliant mechanisms\\nelastomers\\nflexible structures\\nhuman factors\\npublic domain software\\nrobust control\\nsocial interaction\\ncollaborative tasks\\nrobotic devices\\nmechanical properties\\nphysical human-robot-interaction\\nsilicone elastomers\\nmuca\\nopen-source sensing development board\\nhuman-like artificial skin sensor\",\"349\":\"asymptotic stability\\nautomation\\nconferences\\ndynamics\\neducation\\nencoding\\ntrajectory\\nmobile robots\\nmotion control\\nneural nets\\npattern classification\\ntrajectory control\\nvelocity control\\nbackwards reproduction\\nlearned trajectory\\noriginal dmp\\nvelocity profiles\\nbidirectional drivability\\nencoded path\\ndesired motion pattern\\nsupports reversibility\\nrobotic applications\\nreversible dynamic movement primitive formulation\",\"350\":\"navigation\\nconferences\\ncomputer architecture\\nsoftware agents\\nplanning\\ntrajectory\\ntask analysis\\nhuman-robot interaction\\nmobile robots\\npath planning\\npredictive control\\nautonomous robots\\nrobotics cognitive architecture\\nsnape addresses action planning\\nsocial-awareness navigation\\nhuman-robot cooperation\\nsocially-aware robot navigation\\nhuman-populated environments\\nsnape framework\\nrobots interacting\\nsocially accepted paths\\npath planner\\npredicted robots\\nsocial navigation framework\",\"351\":\"visualization\\nautomation\\nconferences\\ntactile sensors\\ngrasping\\nobject detection\\naerospace electronics\\ndexterous manipulators\\nfeedback\\ngrippers\\nhaptic interfaces\\nmobile robots\\npath planning\\nrobot vision\\ngrasping behavior\\npile\\nopen-loop grasping strategy leveraging\\nidentical objects\\nobject roundness\",\"352\":\"automation\\nplanning\\nreliability\\ngrippers\\ngrasping\\ndexterous manipulators\\nhuman initiated grasp space exploration algorithm\\nunderactuated robot gripper\\nvariational autoencoder\\nrobotics\\nmultifingered adaptive gripper\\nreliable grasps\\nmanually specified expert grasps\\nmixed analytic data-driven approach\\ngrasp planning task\\nmultifingered gripper\\ngrasp space exploration\\ngrasp quality metric\",\"353\":\"shape\\nfingers\\nforce\\ntorque control\\ngrasping\\nstability analysis\\nautomobiles\\nactuators\\ndexterous manipulators\\ngrippers\\nsprings (mechanical)\\nunderactuated gripper\\nself-adaptive grasping\\npassive disturbance rejection\\nrobot gripper\\ncar differential systems\\nindependent finger differentials\\ngripper-object system\\ncomplicated control systems\\nself-adaptive power grasp\\nprecision grasp\\ngrasping position\\ndifferential system\\ngripper design\\nstatic model\\nproximal joint control\\ndistal joint control\\nactuator\",\"354\":\"analytical models\\nuncertainty\\nestimation\\nsea state\\nfeature extraction\\nwavelet analysis\\ndata models\\nhydrodynamics\\nlearning (artificial intelligence)\\nocean waves\\noceanographic techniques\\nships\\ndata-driven sea state estimation\\nmultidomain features\\nmotion responses\\nsituation awareness\\nautonomous ships\\nwave buoy\\nexplicit model\\ndata-driven model\\nship motion data\\nship motion response\\nensemble machine learning model\\nreal-world data\",\"355\":\"location awareness\\nfault tolerant systems\\ncomputer architecture\\nunmanned underwater vehicles\\nsoftware\\nhardware\\nsafety\\nfault tolerant computing\\nfault trees\\nfeedback\\nmobile robots\\nunderwater vehicles\\nfault tolerant methodology\\nlow level faults\\nunderwater robots\\ncontrol architecture\\nrobotic systems\\nbiological monitoring missions\",\"356\":\"visualization\\nsimultaneous localization and mapping\\ntracking\\nrobot vision systems\\ncameras\\nacoustics\\nrobustness\\ndistance measurement\\npose estimation\\nrobot vision\\nslam (robots)\\nunderwater acoustic communication\\norb-slam2\\nsingle map\\nvisual odometry\\nunderwater vehicles\\nbaseline orb-slam\\nunderwater environments\\nunderwater navigation\\nsparse visual slam system orb-slam\\ndrifting estimate\\n6-dof robot\\nacoustic odometry\\nlocal bundle adjustment procedures\\nglobal bundle adjustment procedures\\nrobust underwater visual slam\\nacoustic sensing\\nrobust visual simultaneous localisation and mapping\\ncamera pose estimation\\ndoppler velocity log\",\"357\":\"weight measurement\\nthree-dimensional displays\\nfriction\\nforce\\nestimation\\ntactile sensors\\nreal-time systems\\nbiomechanics\\ndexterous manipulators\\nforce control\\nforce feedback\\nforce measurement\\nforce sensors\\ngrippers\\nhaptic interfaces\\ntorque measurement\\ntime friction estimation\\ngrip force control\\ndexterous precision gripping tasks\\nslipperiness\\nfully-instrumented version\\npapillarray tactile sensor concept\\nobject weight\\nreal-time estimation\\npapillarray sensors\\nclosed-loop grip-force control algorithm\\nnear-optimal grip force\\nvertical lifting task\\ntarget grip force\\nactual grip force\\nreal-time torque measurement\\ngrip force feedback control\",\"358\":\"deep learning\\noptical filters\\nuncertainty\\nshape\\npose estimation\\nrobot sensing systems\\ntrajectory\\nbayes methods\\nfeedback\\nkalman filters\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nrobot vision\\ntactile sensors\\noptical tactile sensing\\n2d test objects\\nuncertainty-aware dl\\ntactile robotics\\nuncertainty-aware deep learning\\nrobot touch\\nbayesian tactile servo control\\ntest scenario\",\"359\":\"integrated optics\\nimage sensors\\noptical design\\ntactile sensors\\nhuman-robot interaction\\noptical computing\\nsoft robotics\\nclosed loop systems\\ncontrol engineering computing\\nconvolutional neural nets\\ndexterous manipulators\\nforce control\\ngrippers\\nanthropomorphic soft robotic hands\\nbrl tactile fingertip\\nsoft biomimetic optical tactile sensor\\nclosed-loop tactile control\\ntactile image\\ntactile sensorimotor control\\ntactip sensors\\nconvolutional neural network\\nadaptive synergies\",\"360\":\"deformable models\\nshape control\\nthree-dimensional displays\\nshape\\ncomputational modeling\\nestimation\\nfinite element analysis\\ndeformation\\nmanipulators\\nmatrix algebra\\nindustrial operations\\nspecific deformation state\\nresearch community\\nimplicit optimization problem\\ncontrol outputs\\ncontrol law\\nreduced finite element models\\nlinear deformable object manipulation\\nclosed-loop shape control\",\"361\":\"automation\\nconferences\\nreinforcement learning\\naerospace electronics\\nspace exploration\\ncontrol theory\\ntask analysis\\nlearning (artificial intelligence)\\nlearning systems\\nmanipulator dynamics\\npath planning\\nstability\\nlearning stable normalizing-flow control\\nstate space coverage\\nstable exploration\\ndeep rl algorithms\\nnormalizing-flow control structure\\ncontrol stability\\ndomain knowledge\\nrobotic manipulation skills\",\"362\":\"adaptation models\\nservice robots\\nconferences\\ndynamics\\npredictive models\\nbenchmark testing\\nmanipulators\\nadaptive control\\nclosed loop systems\\ncontrol system synthesis\\nmanipulator dynamics\\nmobile robots\\nmotion control\\npredictive control\\nrobot dynamics\\ntorque control\\nmodel predictive robot-environment interaction control\\nmobile manipulation tasks\\ntorque-controlled service robots\\ncontact forces\\nmodel predictive control\\nunderlying control problem\\nwhole-body motions\\nsatisfying closed-loop performance\\nmpc-based whole-body controller\\ngeneral mobile manipulator\\ninteracting objects\\nmpc controller\\nball-balancing manipulator\",\"363\":\"recurrent neural networks\\nminimally invasive surgery\\nmedical robotics\\ndatabases\\ndecision making\\ngesture recognition\\nfeature extraction\\ncontrol engineering computing\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\nimage classification\\nimage recognition\\nmanipulators\\nmedical image processing\\nsurgery\\nexplainable spatial feature extraction\\nbasic gestures\\ncontext-aware knowledge\\nsurgical robot control quality\\neffective surgical gesture recognition approach\\nexplainable feature extraction process\\ngradient-weighted class activation mapping\\nexplainable results\\nsurgical images\\nsurgical gesture classification results\\nsuturing task\\ndeep convolutional neural network model\\nbidirectional multilayer independently rnn model\",\"364\":\"training\\nroad transportation\\ncosts\\nsensitivity analysis\\nconferences\\nurban areas\\ntraining data\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npath planning\\nrobot programming\\nmachine-learned planners\\ndata availability\\ndata volume requirements\\nplanner performance\\ndata access\\nimitation-based motion planning\\nsensor data\\nimitation-learning-based av planner\\nperception range function\\ntime 1000.0 hour\",\"365\":\"airplanes\\nsolid modeling\\ncosts\\nthree-dimensional displays\\nneural networks\\nestimation\\nnearest neighbor methods\\niterative methods\\npath planning\\nsampling methods\\nsearch problems\\ntree searching\\ntrees (mathematics)\\ndubins path characteristics\\naerial vehicles\\nasymptotically optimal sampling-based path planners\\npath quality\\nmotion tree increases\\nadditional sample\\nnearest-neighbor search\\nstate transition costs\\ncomplex dynamics\\nnonisotropic cost fields\\ncostly nearest neighbor\\nsearch tree\\nlow-cost paths\\ngiven computational time\\nlightweight neural network\\napproximate nearest neighbor cost calculations\\nnetwork approach\\nlow-dimensional encoding\\ncost space\\ngoal query pair\\npath cost\\ndubins airplane model\\nnetwork method\\nequivalent path lengths\\nsearch time\",\"366\":\"service robots\\nsoftware packages\\nconferences\\nfitting\\nmanuals\\nprogramming\\ngenerators\\nassembling\\ncontrol engineering computing\\ndata visualisation\\nmobile robots\\norganisational aspects\\npharmaceutical industry\\nrobot programming\\nindustrial test platform\\ndanish pharmaceutical company novo nordisk\\nrobot trajectories\\nassembly processes\\nsparse manual input\\noffline programming part assembly operations\\ntight fittings\\nassembly process trajectory generator\\nself programming capabilities\\nsparse input\\nintuitive input\\nexisting skill based robot software package verosim\",\"367\":\"linux\\nconferences\\nquality of service\\nbenchmark testing\\nreal-time systems\\nsafety\\nkernel\\ncontrol engineering computing\\nmiddleware\\nmobile robots\\nlinux kernels\\nrobotic control applications\\nreal-time critical applications\\norocos\\npreempt_rt\\nkernel patches\\nros 2\\nreal-time safe frameworks\\nreal-time constraints\\ncatastrophic consequences\\nsafety critical applications\\nreal-time requirements\",\"368\":\"torque\\nestimation\\nprototypes\\nvoltage\\ntools\\ncameras\\nvisual servoing\\ndexterous manipulators\\nfeedback\\ngrippers\\nposition control\\npressure sensors\\nrobot vision\\ntorque control\\ndata-driven method\\njoint torques\\noutput voltage\\nmotor speed\\ndisassembly actions\\ntorque estimation accuracy\\ndisassembly success rate\\nkit gripper\\nmultifunctional gripper\\ndisassembly tasks\\nmultifunctional robotic gripper\\nelectromechanical devices\\nrobot arm\\njaw gripper\\n1-dof rotation joint\\nbimanual systems\\nsensor system\\njoint encoders\\ninteraction forces\\ntool tip\\nimage-based visual servoing\",\"369\":\"geometry\\nsupport vector machines\\nservice robots\\nwelding\\nsupervised learning\\ntime series analysis\\nestimation\\nacoustic emission\\narc welding\\ncontrollers\\ndecision trees\\nnearest neighbour methods\\nprocess control\\nproduct quality\\nproduction engineering computing\\nrandom forests\\nrobotic welding\\nsensor fusion\\ntolerance analysis\\nwelding equipment\\npart geometry\\ncurrent voltage\\nwelding voltage\\nsupervised learning techniques\\nrobotic arc welding\\nmultisensor inputs\\nmanufacturing tolerances\\nindustrial welding applications\\npart quality\\nrobot configuration\\nprocess controller\\nmanufacturing time\\nprocess data\\nin-process workpiece geometry estimation\\ngeometry parameters\\nwelding current\\nacoustic emissions\\nrandom forest decision trees\\nk-nearest-neighbors\\nwelding tool accessibility\",\"370\":\"surveillance\\ntraffic control\\nunmanned aerial vehicles\\nreal-time systems\\nsensors\\nfeeds\\ntask analysis\\nautonomous aerial vehicles\\ndeep learning (artificial intelligence)\\nglobal positioning system\\nobject detection\\nremotely operated vehicles\\nsecurity of data\\ntraffic information systems\\nvideo surveillance\\naerial monitoring capability\\nemploy multiple sensors\\ndeep neural network-based method\\npoint anomalies\\nsingle instance anomalous data\\ncontextual anomalies\\ncontext-specific abnormality\\nvae\\ncontextual information\\ncontextual anomaly detection method\\nuav-assisted aerial surveillance\\ntraffic surveillance scenario\\nanomaly detection tasks\\ncontext-dependent anomaly detection\\nlow altitude traffic surveillance\\nspecific environmental context\\nunmanned aerial vehicle\",\"371\":\"laser radar\\nbuildings\\nsensor fusion\\ncameras\\nunmanned aerial vehicles\\nwindows\\ntrajectory\\naircraft navigation\\nautonomous aerial vehicles\\ncollision avoidance\\nkalman filters\\nlaser ranging\\nmobile robots\\noptical radar\\npath planning\\nremotely operated vehicles\\nrobot vision\\nstate estimation\\navailable a priori information\\ncollision-free trajectories\\ntarget window\\nsensor fusion algorithm\\nrobust altitude estimation\\nlaser rangefinder data\\ninconsistent elevation\\noutdoor environments\\nindoor environments\\nuav state estimation\\ntarget building\\nautonomous flying\\nfirefighting scenario\\nunmanned aerial vehicle\\nopen window\\ndepth camera\\nlidar\\nwindow detection\\ncontinuous estimation\",\"372\":\"geometry\\nestimation error\\nautomation\\nfriction\\nconferences\\ntransforms\\ntools\\ndexterous manipulators\\nforce control\\nmechanical contact\\nestimated polyhedral friction cone\\npolyhedral friction cone estimator\\nobject manipulation\\nenvironment geometries\\ncontact point locations\\nfriction coefficients\\nestimated friction cone\\nobject moves\",\"373\":\"visualization\\ncodes\\npipelines\\nkinematics\\ninspection\\nreduced order systems\\nconvolutional neural networks\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\nfailure (mechanical)\\nimage classification\\nimage representation\\nmanipulator dynamics\\nmanipulator kinematics\\nrobot vision\\ndnn\\nkinodynamic images\\nkinematic data\\ndynamic data\\ncontact-rich manipulation tasks\\ninterpretability modules\\nvision-based tasks\\nconvolutional neural network\\ndeep neural networks\\ncontact dynamics\\nstate representation\\ngrad-cam\\nvisual explanations\\nvisual inspections\",\"374\":\"automation\\nlearning automata\\nconferences\\nstochastic processes\\ngames\\nreinforcement learning\\ntransforms\\nautomata theory\\ngame theory\\nlearning (artificial intelligence)\\nprobability\\nstochastic games\\ntemporal logic\\nstochastic game\\nlinear temporal logic objectives\\ncontrol strategies\\nturn-based zero-sum\\ntransition probabilities\\nmodel topology\\nwinning condition\\ngiven ltl specification\\nacceptance condition\\ndeterministic rabin automaton\\nmodel-free reinforcement learning methodology\\nrabin condition\\nderived dra\\nsingle accepting pair\\nltl formulas\\nrabin accepting condition\\nsatisfaction probability\",\"375\":\"actuators\\nautomation\\nconferences\\nstochastic processes\\nintrusion detection\\nreinforcement learning\\ngames\\nlearning (artificial intelligence)\\nsecurity of data\\nstochastic games\\ntemporal logic\\nplanning problem\\nltl formula\\nstochastic game\\nmodel-free reinforcement learning\\nrobotic planning case studies\\nsecure planning\\nstealthy attacks\\nsecurity-aware planning\\nunknown stochastic environment\\ncontrol signals\\nemployed intrusion-detection system\\ncombined linear temporal logic formula\",\"376\":\"navigation\\nlayout\\nreinforcement learning\\npath planning\\ndata models\\nplanning\\ntask analysis\\nlearning (artificial intelligence)\\nmobile robots\\ntrainable hl path planning\\nparking task\\nrobot navigation\\nsparse reward\\nlong decision horizon nature\\nhigh-level task representations\\nrough floor plan\\nhierarchal approaches\\nhl representation\\nusing sub-goals\\nrl policy\\nsource task\\ncomplex dynamics\\nsub-optimal sub-goal-reaching\\nnovel hierarchical framework\\ntrainable planning policy\\nrobot capabilities\\ncollected rollout data\\nlearned transition model\\nsimulated robotic navigation tasks\\nvi-rl results\\nconsistent strong improvement\\nvanilla rl\\nvanilla hierarchal rl\",\"377\":\"costs\\nthree-dimensional displays\\nservice robots\\ninstruction sets\\ncollaboration\\nrobot sensing systems\\nneedles\\ncalibration\\nimage registration\\nmanipulators\\nmobile robots\\nmotion control\\nmulti-robot systems\\npose estimation\\ntrajectory control\\ncollaborative robots\\nrelative poses\\npose errors\\nrobot proprioception\\nhigh-precision assembly behaviors\\n7-axis panda robots\\nlaser-based dual-arm system\\n3d models\\ncorrective trajectories\\nusb insertion task\\nlaser-based sensing\",\"378\":\"tensors\\nautomation\\nconferences\\nplanning\\nrobots\\ngraph theory\\nmobile robots\\nmulti-robot systems\\npath planning\\nsampling methods\\noptimal multirobot motion planning\\nsampling-based methods\\ncontinuous multirobot motion planning\\ntensor roadmap\\nmultiple prm graphs\\ntensor product\\nnear-optimal solution\\nmrmp-satisfying\\npopular planners\\ncontinuous domain\\nfinite-sample analysis\\nindividual prm graph\\nnear-optimality\\nprevious asymptotic analysis\\nfinite sample-size analysis\\nquality solutions\\nsampling scheme\\nfinite-sample motion planning\\ninvolved mrmp setting\\nmultiple robots\",\"379\":\"three-dimensional displays\\nshape\\ntransforms\\nreal-time systems\\nplanning\\ntrajectory\\nsafety\\ncollision avoidance\\nmobile robots\\nnewton method\\nnonlinear programming\\noptimisation\\npath planning\\noriginal constrained nonlinear programming problem\\nquasinewton methods\\ncomputation time\\nreal-time motion planning\\nmulticopters\\nhigh maneuverability\\noptimization-based motion\\naerial robot\\naggressiveness\\ndrone maneuvering\\ncluttered environment\\nintersecting polyhedrons\\n3d free spaces\\ntime-indexed trajectory\\nfull-body collision-free guarantee\\ntilted cuboid\\nsafety conditions\",\"380\":\"legged locomotion\\ncomputational modeling\\ndynamics\\nkinematics\\npredictive models\\nplanning\\ntrajectory\\ncollision avoidance\\nkalman filters\\nmobile robots\\npredictive control\\ncollision-free mpc\\nlegged robots\\nmodel predictive controller\\ncollision-free locomotion\\nsystem dynamics\\nfriction constraints\\nkinematic limitations\\nrelaxed barrier function\\noptimization\\ncollision avoidance behavior\\nproblem\\nholistic approach\\nwhole-body motions\\ndynamic obstacles\\neuclidean signed distance field\\nstatic collision checking\\nmoving cylinders\\nkalman filter motion prediction\\npossible future collisions\\ncollision-free motions\\nquadrupedal robot\\ncomplex scenes\\ndynamic agents\\ndynamic limits\\nkinematic limits\",\"381\":\"measurement\\nheuristic algorithms\\nconferences\\ncomputational modeling\\nrefining\\nkinematics\\ncomputational efficiency\\ncollision avoidance\\nmobile robots\\nkinetic energy difference\\nked\\ncollision proximity\\ndifferentially driven robots\\nobstacle avoidance\\nkinetic energy buffer\",\"382\":\"training\\nsmoothing methods\\nautomation\\nconferences\\nreinforcement learning\\npredictive models\\ntrajectory\\nenergy management systems\\ngaussian processes\\nlearning (artificial intelligence)\\nmobile robots\\nnonlinear control systems\\noptimal control\\npower transmission (mechanical)\\npredictive control\\nroad vehicles\\nreinforcement learning-based solution\\nminiature race car platform\\nrelatively simple vehicle model\\nmodel randomization\\nrobotic setup\\npolicy output regularization approach\\nlifted action space\\nsmooth actions\\naggressive race car\\nregularized policy\\nsoft actor critic baseline method\\nmodel predictive controller state-of-the-art method\\nreinforcement learning policy\\nmpc controller\",\"383\":\"robot motion\\ntools\\nprobabilistic logic\\ndynamic programming\\nplanning\\ntask analysis\\ntrajectory optimization\\nmobile robots\\nmotion control\\nnumerical stability\\noptimisation\\npath planning\\ntrajectory control\\ntask-based robot motion planning\\nconvergence guarantees\\npost-processing operation\\nprobabilistic methods\\nunconstrained trajectory optimization problems\\ndifferential dynamic programming methods\\nfast convergence\\nrelatively low computational cost\\naugmented lagrangian approach\\nequality-constrained trajectory optimization problems\\nconvergence speed\\nstandard robotic problems\",\"384\":\"training\\nsolid modeling\\nthree-dimensional displays\\ntracking\\nannotations\\nrobot vision systems\\npredictive models\\nimage sequences\\niterative methods\\nmotion estimation\\nobject detection\\npose estimation\\ndynamic objects\\nintelligent decisions\\ncnn model\\nmonocular images\\nmanual annotations\\niterative closest points\\nicp\\npointclouds\\nunsupervised approach\\nground truth supervision\\nkitti tracking dataset\\nground truth data\\nunsupervised motion estimation\",\"385\":\"training\\nvisualization\\nsupervised learning\\nestimation\\nnetwork architecture\\ncameras\\nunmanned aerial vehicles\\nautonomous aerial vehicles\\nconvolutional neural nets\\nfeature extraction\\nimage matching\\nimage sensors\\nimage sequences\\nlearning (artificial intelligence)\\nmicrorobots\\nmobile robots\\nmotion estimation\\nrobot vision\\ncascaded network blocks\\nnetwork architectures\\ntraining strategies\\nmav flight dataset\\ntraditional feature-point-based methods\\nfast maneuvers\\ncnn-based ego-motion estimation\\nfast mav\\nvisual ego-motion estimation\\nmicroair vehicles\\nbig visual disparity\\nhigher robustness\\nstudy convolutional neural networks\\nsubsequent images\\nmonocular camera\\nplanar scene\\ninertial measurement unit\\ntranslational motion\\nsimilar small model sizes\\nhigh inference speeds\\nmobile gpu\\nrealistic motion blur\\nnetwork framework\\nmemory size 1.35 mbyte\\ntime 10.0 ms\",\"386\":\"visualization\\nmonte carlo methods\\nnavigation\\nrobot sensing systems\\nunmanned aerial vehicles\\nreal-time systems\\nrobustness\\nattitude control\\ndistance measurement\\nlaser ranging\\nmobile robots\\nobject detection\\nrobot vision\\nslam (robots)\\nstate estimation\\nair range-visual-inertial estimator initialization\\nmonocular visual-inertial odometry\\nautonomous microair vehicles\\nstate-of-the-art vio\\nfailure-prone\\ndramatic consequences\\nre-initialize\\nconstant velocity trajectory\\nvisual scale\\nvio batch initializers\\nsmall laser-range finder\\nlightweight laser-range finder\\nscene facet model\\ninitialize vision-based navigation\\nrange constraint\\nvisual-inertial bundle-adjustment initializer\\nmidair state estimation failure\",\"387\":\"linear systems\\ncodes\\nautomation\\nconferences\\nrobot vision systems\\npose estimation\\nbuildings\\ncameras\\nmatrix algebra\\nmotion estimation\\nvehicle-mounted cameras\\negomotion\\ncalibrated camera\\nmoving vehicle\\nsingle affine correspondence\\nspecial homographies\\nsought plane\\nbuilding facade\\nlinear system\\ncoefficient matrix\\nminimal cases\\nover-determined cases\\nstate-of-the-art robust estimators\",\"388\":\"training\\nrecurrent neural networks\\natmospheric measurements\\nheuristic algorithms\\nsemantics\\nparticle measurements\\ntime measurement\\nlearning (artificial intelligence)\\nmobile robots\\nmotion compensation\\nneural nets\\nobject detection\\nobject tracking\\noptical radar\\nprobability\\nrecurrent neural nets\\nroad vehicles\\nspatiotemporal phenomena\\ntraffic engineering computing\\nvideo signal processing\\ncomplex traffic scenarios\\nrecurrent neural network\\ndynamic occupancy grid map\\noccupancy probability\\nvelocity estimate\\nmeasurement grid maps\\nlidar measurements\\nrecurrent layers\\nrobust detection\\nstatic environment\\ndynamic environment\\nmoving ego-vehicle\\nneural network architectures\\nstatic area\\ndynamic area\\nego-motion compensation leads\\ndynamic occupancy grid mapping\",\"389\":\"location awareness\\nthree-dimensional displays\\nlaser radar\\nannotations\\nroads\\nsemantics\\nestimation\\ncameras\\nimage representation\\nimage sequences\\nlearning (artificial intelligence)\\nobject detection\\noptimisation\\ntraffic engineering computing\\nmap inference\\nsingle frame detections\\ntraffic lights\\nautomatic mapping\\ntailored landmark representations\\nautomated driving\\nmap learning\\nautomatic creation\\nautomatic inference\\nhd maps\\nricher landmarks\\nmore precise landmarks\\nsemantic detections\\norientation estimation\\nsemantic map elements\\ntailored representations\",\"390\":\"image segmentation\\nthree-dimensional displays\\nsemantics\\nprobabilistic logic\\nreal-time systems\\nplanning\\ntask analysis\\ndata visualisation\\nfeature extraction\\nimage fusion\\nimage reconstruction\\nimage sequences\\nlearning (artificial intelligence)\\nmedical image processing\\nmobile robots\\nrobot vision\\nstereo image processing\\nlightweight semantic mesh mapping\\nsemantically meaningful environment maps\\nautonomous driving\\nhigher-level tasks\\nnavigation\\nmeaningful map\\nlightweight semantic map\\nmonocular sequence\\nstereo sequence\\nfeature-based visual odometry\\nlearned depth prediction\\nsemantic image segmentation\\nsemantically relevant environment structure\\nprobabilistic fusion scheme\\nsemantic labels\\nintermediate voxel-based fusion\\noutdoor driving scenarios\\nmonocular depth prediction\\nstereo reconstruction results\\npresent quantitative reconstruction results\\nqualitative reconstruction results\\ncurrent state-of-the-art voxel-based methods\",\"391\":\"location awareness\\nsimultaneous localization and mapping\\nsensitivity\\nlaser radar\\nnavigation\\nconferences\\nrobot vision systems\\nlearning (artificial intelligence)\\nmobile robots\\noptical radar\\npath planning\\nsensor fusion\\nslam (robots)\\nfalse matches\\nmobile robot navigating\\nwarehouse-like environment\\nslam algorithms\\nunsupervised multisensor representation\\nbiologically inspired algorithms\\nratslam\\neffective robot navigation\\nrobust robot navigation\\nindoor environments\\noutdoor environments\\nperceptual aliasing\\ntemplate matching\\nlow-dimensional sensory templates\\nunsupervised representation learning method\\nyields low-dimensional latent state descriptors\\nsensor modality\\ncamera images\\nradar range-doppler maps\\nlidar scans\\nmultiple sensors\",\"392\":\"training\\nvisualization\\nautomation\\nnavigation\\nconferences\\nreinforcement learning\\nresource management\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\nchina shop\\nmultitask learning problem\\nvisual navigation tasks\\nlocation-specific navigation behaviour\",\"393\":\"adaptation models\\nneural networks\\npredictive models\\ntools\\nmathematical models\\nunmanned aerial vehicles\\nparametric statistics\\nactuators\\nhydrodynamics\\nmobile robots\\nmotion control\\nrecurrent neural nets\\nunderwater vehicles\\nmodel identification\\nfully-actuated aquatic surface vehicle\\nlong short-term memory neural network\\naquatic robot\\naerial robot\\ndynamic system model\\nsimulation-to-reality gap\\nneural network method\\nvehicle simulation\\nhydrodynamic effects\",\"394\":\"training\\ntrajectory tracking\\nconferences\\ncomputational modeling\\nreinforcement learning\\ncomputer architecture\\nreal-time systems\\ndeep learning (artificial intelligence)\\nfeedback\\nlegged locomotion\\nmotion control\\noptimisation\\nrobust control\\ntracking\\ntrajectory control\\ntime trajectory adaptation\\nquadrupedal locomotion\\ndeep reinforcement learning\\ncontrol architecture\\nreal-time adaptation\\nterrain-aware trajectory optimization solver\\ncomputationally exhaustive task\\nonline trajectory optimization\\napproximated dynamics\\nadditive deviations\\nreference trajectory\\nfeedback-based trajectory tracking system\\nquadrupedal robot\\nsimulated terrains\\ntraining methods\\nterrain information\\nrl environment\\nexteroceptive feedback\\ntrained policy\\ncorrected set points\\nmodel-based whole-body controller\\ntracking behavior\\nsimulation environments\\ncorrective feedback\\nprecomputed dynamic long horizon trajectories\\nflat terrain\\ncomplex modular uneven terrain\",\"395\":\"robust control\\nactuators\\nuncertainty\\nautomation\\nsimulation\\nconferences\\nmuscles\\niterative learning control\\niterative methods\\nlyapunov methods\\npneumatic actuators\\nuncertain systems\\nrobust iterative learning control\\nstate constraint\\nmodel uncertainty\\niterative learning control scheme\\npneumatic muscle actuators\\nthree-element form\\nnonparametric uncertainties\\ncomposite energy function\\nfull state constraints\\nilc\\npm state tracking errors\\nbarrier lyapunov function\",\"396\":\"three-dimensional displays\\nsimultaneous localization and mapping\\nshape\\nconferences\\ntransforms\\ngaussian distribution\\ntransformers\\nfeature extraction\\nimage retrieval\\nnormal distribution\\nobject recognition\\nslam (robots)\\nvisual databases\\nlarge-scale 3d point cloud localisation\\n3d point cloud-based place recognition\\nautonomous driving\\ngps-challenged environments\\nloop-closure detection\\nlidar-based slam systems\\nlarge-scale place recognition\\ndense 3d point cloud\\nprobabilistic distributions\\nndt cells\\ngeometrical shape description\\nnovel ndt-transformer network\\nglobal descriptor\\n3d ndt cell representations\\nndt representation\\nlearned global descriptors\\n3d normal distribution transform representation\\ndescriptor retrieval\",\"397\":\"wireless communication\\nperturbation methods\\ndynamics\\npipelines\\npredictive models\\nsoftware\\ntiming\\nhumanoid robots\\nlegged locomotion\\nmotion control\\npredictive control\\nrobot dynamics\\nreactive walking controller\\nopen-hardware\\ndynamic performance\\nreliability\\nopen-access quadruped robot solo-12\\nopen dynamic robot initiative\\nstate-of-the-art control pipeline\\nmini cheetah\\nmodel predictive controller\\ncentroidal dynamics\\ndesired contact forces\\nreference velocity\\nstate estimation\\nbody controller\",\"398\":\"legged locomotion\\ntraining\\nlocation awareness\\nconferences\\ncloning\\nbenchmark testing\\nprediction algorithms\\ngait analysis\\nlearning (artificial intelligence)\\noptimisation\\npredictive control\\nrobot dynamics\\noriginal mpc implementation\\nsingle learned policy\\nmultiple gaits\\nlearning algorithm\\nsingle policy\\nwalking robot\\nimitation learning approach\\nmodel predictive control\\nmpc-net differs\\ncontrol hamiltonian\\nmixture-of-experts network\\nhybrid system\\nmultigait policies\\nexpert selection behavior\\nquadrupedal multigait control\\nmen\\nbehavioral cloning\",\"399\":\"costs\\nautomation\\nconferences\\ndynamics\\noptimal control\\npredictive models\\nreduced order systems\\ngait analysis\\nlegged locomotion\\nmobile robots\\nmotion control\\npredictive control\\nrobot dynamics\\nrobust control\\nstability\\npredictive controllers\\nbalance recovery\\nquadruped robots\\nlocomotion decisions\\nexisting quadruped controllers\\nmodel predictive controller\\nmpc\\nreduced model\\nbody controller\\nchosen model reductions\\nexisting formulations\\nadditional controllers\\ncontroller capabilities\\nrobust predictive controller\\ncenter- of-mass trajectory\\nground reaction forces\\noptimal control problem\\nreasoned decisions\\nrobot solo-12\",\"400\":\"legged locomotion\\nsurface impedance\\ntransportation\\nstability analysis\\nreal-time systems\\ntrajectory\\ntask analysis\\nkalman filters\\nmotion control\\npath planning\\npredictive control\\nrobot dynamics\\nstability\\nmodel predictive control\\nstability margins\\nvertex-based zero-moment-point constraints\\nquasistatic walking stability\\nkalman filter\\nmpc motion planner\\nrobot arms\\nlocomotion adaptation\\nheavy payload transportation tasks\\nquadruped robot centauro\\ncenter-of-mass trajectories\\nreactive legged locomotion generation\\nmass 20.0 kg\",\"401\":\"visualization\\nautomation\\nconferences\\nreinforcement learning\\ngrasping\\ndata collection\\nrobot learning\\ncontrol engineering computing\\ndata analysis\\nhuman-robot interaction\\nmanipulators\\nself-supervised data collection\\noffline robot learning\\nrobot reinforcement learning\\ndata collection policy\\nrobot manipulation tasks\\nrobot interaction data\\ngoal-conditioned reinforcement-learning\",\"402\":\"pi control\\nconferences\\ncomputational neuroscience\\nestimation\\nrobustness\\nfiltering theory\\npd control\\nadaptive control\\ncontrol engineering computing\\ndamping\\ninference mechanisms\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmanipulator kinematics\\noscillations\\nrobust control\\nstate estimation\\nrobotic manipulator\\nactive inference framework\\nfree-energy minimization\\nactive inference controllers\\nadaptive behaviour\\nrobust behaviour\\nfranka emika panda 7dof manipulator\\nhyperparameters learning\\noscillations damping\",\"403\":\"codes\\nautomation\\nservice robots\\nconferences\\nproduction\\ntask analysis\\nunsupervised learning\\ngradient methods\\nmanipulators\\nmotion control\\nmulti-robot systems\\noptimisation\\nrobot programming\\nmanipulation tasks\\nrobot skills\\nconcrete physical environment\\nhuman programmers\\nforce-controlled skills\\noptimal skill parameters\\ngradient-based model inversion\\noptimal parameters\\nnondifferentiable skills\\nskill variants\\nspi zero-shot\\ntask objectives\\nrobot program parameter inference\\ndifferentiable program representation\\ndifferentiable shadow program inversion\",\"404\":\"cerebellum\\nrobot kinematics\\nbiological system modeling\\npredictive models\\nrobot sensing systems\\ncontrol systems\\nbrain modeling\\nbiocontrol\\nbiomechanics\\nbrain\\nfeedback\\nmanipulators\\nmedical robotics\\nmobile robots\\nmotion control\\nneurocontrollers\\nrobot vision\\nsensors\\nvisual servoing\\njointspace error\\nforward predictive learning\\ncellular cerebellar model\\ndifferential mapping spiking neural network\\nvisual servoing task\\nrobot arm manipulator\\naccurate target reaching actions\\nmotion execution time\\nneural control system\\ncerebellar predictive learning\\nsensor-guided robots\\nmotor control system\\nsmith predictor\",\"405\":\"legged locomotion\\ntraining\\nnavigation\\ncomputational modeling\\nreinforcement learning\\nrobot sensing systems\\ngenerators\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nlocomotion contact planner\\nguide path\\nheuristic model\\nrobot gait\\nsteering method\\nfeasible contact sequence\\nreinforcement learning approach\\nclassical reinforcement learning algorithm\\nsteers\\ncontact generator\\nfeasible contact plans\",\"406\":\"deformable models\\ngeometry\\nshape control\\nshape\\nwires\\nmetals\\nreinforcement learning\\ndeep learning (artificial intelligence)\\ndifferential geometry\\nelastoplasticity\\ngradient methods\\nmanipulators\\nmobile robots\\nobject detection\\ndeformable object manipulation tasks\\nrobotic manipulation methods\\nrigid objects\\nmodel-free reinforcement\\nshape control task\\nelastoplastic properties\\nmanipulation task\\nrl perspective\\nintrinsic shape representation\\nlearning shape control\\ndlos\\nelastoplastic deformable linear objects\\nddpg\\ndeep deterministic policy gradient\",\"407\":\"legged locomotion\\nautomation\\nconferences\\ndynamics\\nplanning\\ntrajectory\\nquadratic programming\\nangular momentum\\nnonlinear programming\\npath planning\\nrobot dynamics\\njump dynamics\\nlow-dimensional optimization problem\\ncentroidal angular momentum\\nbilevel optimization problem\\nlower-level quadratic programming problem\\nlanding point\\none-legged robot\\nmultilegged robots\\ncentroidal dynamics\\nprecise jump planning method\\ncenter of mass\\ncom position\\nnonlinear upper-level problem\\nground reaction forces\\nbiped robot\\nlegged robots\\nwhole-body motion planning and control\",\"408\":\"legged locomotion\\nthree-dimensional displays\\nneural networks\\nhumanoid robots\\nreinforcement learning\\nrobot sensing systems\\nreal-time systems\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\ngait analysis\\nrobot dynamics\\nrobot kinematics\\ncomputational constraints\\nminimal craftsmanship\\ndrl approach\\nhumanoid robot\\nsingle control policy\\nsingle neural network\\ncurriculum learning method\\ntask difficulty\\nomnidirectional bipedal gait\\nbipedal walking\\nhigh-dimensional dynamics\\ndeepwalk\\ndeep reinforcement learning\\nomnidirectional locomotion learning\\ntarget velocity scheduling\\nsim-to-real transfer\",\"409\":\"legged locomotion\\nneuromuscular\\nbiological system modeling\\nsimulation\\nconferences\\ndynamics\\nrobot control\\nactuators\\nbiomechanics\\nclosed loop systems\\ngait analysis\\nmotion control\\nrobot dynamics\\nstability\\nult-model\\nupright trunk\\nupright-trunk locomotion\\nfully unified locomotion template model\\nupright-trunk forward hopping system\\nunified control law\\nlocomotion subfunctions\\nparallel leg actuator\\ntrunk slip model\\nupright-trunk gait control\\nlimit cycle\\nclosed-loop dynamics\\nanchor matching\",\"410\":\"legged locomotion\\nautomation\\nconferences\\nlimit-cycles\\nenergy efficiency\\nnonlinear dynamical systems\\nsprings\\nbiomechanics\\nelasticity\\nmotion control\\nrobot dynamics\\nsprings (mechanical)\\nstability\\nnonlinear stiffness\\npassive dynamic hopping\\none-legged robots\\nupright trunk\\ntemplate models\\ncontrol dynamics\\nrobot hopping\\npassive limit cycle\\nenergy-efficient control\\ntrunk stabilization\\nleg mass\\nrobot hopper model\\nrigid trunk\\nlinear hip spring\\nnonlinear hip spring\",\"411\":\"automation\\nconferences\\ncollaboration\\nplanning\\ntask analysis\\nrobots\\ncontrol engineering computing\\nhuman-robot interaction\\nmanipulators\\nmobile robots\\nmotion control\\npath planning\\nfunctional unit\\ntask planning algorithm\\nweighted foon\\nmanipulation action load\\noptimal performance\\ncomplicated tasks\\nweighted functional object-oriented network\\nmanipulation actions\\ncomplicated manipulation tasks\\nphysical disabilities\\ncollaborative robot\",\"412\":\"robot motion\\nethics\\nautomation\\nconferences\\ntaxonomy\\nplanning\\ntrajectory\\nexplanation\\nmobile robots\\noptimisation\\npath planning\\ncontrastive explanations\\nexplainable motion planners\\ntechnical research agenda\\nsocial research agenda\\nai ethics\\nai algorithms\\nrobot motion planning\\noptimization\",\"413\":\"training\\nautism\\nautomation\\ndatabases\\nconferences\\nestimation\\nfocusing\\nconvolution\\nhandicapped aids\\nhumanoid robots\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmedical disorders\\nneural nets\\npaediatrics\\nasd children\\nnatural child-robot interaction\\nautism spectrum disorder\\nengagement estimation\\ndeep convolutional architectures\\nchild robot interaction\\ndeep convolutional networks focusing\",\"414\":\"codes\\nautomation\\nconferences\\ncleaning\\nmotion measurement\\ntask analysis\\nrobots\\nlearning (artificial intelligence)\\nrobot programming\\nergodic imitation\\nversatile robotics\\nend users\\nrobots tasks\\ncontrol policy\\nnear-optimal demonstrations\\nrobust skill replication\\nrobust task definitions\\nlearning tasks\\nnegative demonstrations\\nimperfect demonstrations\\nrobot performance\\npositive demonstrations\\nsimulated tasks\\nrobust data-efficient lfd\\ncrucial task elements\\ntask executions\\nlearning from demonstration\",\"415\":\"uncertainty\\nconferences\\nneural networks\\nclustering algorithms\\nestimation\\ntraining data\\ninference algorithms\\nlearning by example\\nneural nets\\nrobot programming\\nuncertainty handling\\nimitation learning\\ninconsistent demonstrations\\naleatoric uncertainty estimation\\ndemonstration data\\nuncertainty estimation\\npolicy-learning neural network\\nrobot learning\\nuncertainty based data manipulation\",\"416\":\"wrist\\ncouplings\\nvisualization\\ncomputational modeling\\nrobot kinematics\\nhumanoid robots\\nhandover\\nbiomechanics\\ncognition\\nhuman computer interaction\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nneurophysiology\\nhuman-human\\ncoupled dynamical systems\\nhuman interaction\\nnonverbal communication skills\\npersons actions\\nsensory experiences\\nnonverbal cues\\nhandover actions\\ncomputational model\\nmotor behaviour\\naction-in-interaction situations\\nobject handover action\\naction execution\\nhuman wrist motion\\ninfer\\nobserved action\\nmotor resonance model\\nhuman partner\",\"417\":\"matched filters\\nuncertainty\\nautomation\\nconferences\\nfitting\\nestimation\\nrobot sensing systems\\nassembling\\nlearning (artificial intelligence)\\nmobile robots\\nrobotic assembly\\nrobust control\\nprobabilistic approach\\ntactile sensorimotor trace\\nfailed assembly attempts\\npart position\\nassembly failure\\nobject fitting tasks\\nobject position\\ntype estimation\\ncontact interactions\\novercome failure\\nrobot assembly tasks\\nautonomous multipart object assembly\\nrobust sensorimotor control\\nphysical interaction\\nhierarchical approach\\npart types\",\"418\":\"laparoscopes\\nvisualization\\nminimally invasive surgery\\nsuperresolution\\nrobot vision systems\\nstreaming media\\ntools\\ncameras\\nendoscopes\\nimage matching\\nimage resolution\\nmedical robotics\\nsurgery\\nvideo signal processing\\ntime surgical environment enhancement\\nrobot-assisted minimally invasive surgery\\ncamera assistant\\nlaparoscope\\nsurgical operation\\nmultiscale generative adversarial network-based video super-resolution method\\nautomatic zooming ratio adjustment\\nreal-time zooming\\nsurgical tools\\nupscaling ratio\",\"419\":\"automation\\nconferences\\nhuman-robot interaction\\ntools\\nrobot sensing systems\\nsensors\\nplanning\\ncontrol engineering computing\\npublic domain software\\nsocial robots\\nnode orchestration\\nsocial human-robot interaction\\nusability tests\\nharmoni\\nopen-source tool\\nsocially interactive robots\\ninteraction development\\ndecision management\\nqt robot platform\\nhuman and robot modular open interaction\",\"420\":\"heating systems\\nvisualization\\nautomation\\nconferences\\nmedical treatment\\nhuman-robot interaction\\ncolor\\ndata visualisation\\nface recognition\\nhuman computer interaction\\nhumanoid robots\\nimage colour analysis\\nimage motion analysis\\nmanipulators\\nmedical robotics\\npath planning\\nrobot vision\\nrobots\\nservice robots\\nvirtual reality\\nrobot interaction studio\\nunsupervised hri\\nreal-time markerless motion-capture system\\nrethink robotics baxter research robot\\n75-minute-long user study\\nhead motions\\narm motions\\ncues\\ncaptured user motions\\nrecorded user hand positions\\nuser opinions\",\"421\":\"visualization\\nautomation\\nconferences\\ntools\\ngenerators\\ncultural differences\\nrobots\\ngesture recognition\\nhuman-robot interaction\\nsubjective visual evaluation\\nquantitative evaluation methods\\ngenerative methods\\nsubjective evaluation\\ngesture generation systems\\ngesture generator\\ntalking gestures\\nbody language\\nsocial robots\\nappropriate behavior\\nrule-based approaches\",\"422\":\"training\\nimage segmentation\\nglobal navigation satellite system\\nneural networks\\nsemantics\\ncrops\\nnoise measurement\\nagricultural robots\\ncameras\\nconvolutional neural nets\\nimage colour analysis\\nsatellite navigation\\nsupervised learning\\ncrop row segmentation\\nrobot-supervised learning\\nlabel generation\\nsemantic segmentation\\nconvolutional neural networks\\ncrop row detection\\ntraining robot\\npure vision-based navigation\\nstrawberry field\\nhand-drawn image labels\\nnoisy labels\\nopen-loop field trials\\nagri-robot\\nrow-following\\nnoisy segmentation labels\\nvision-based crop row\\nclosed-loop guidance\\nrtk gnss\\nrgb camera\",\"423\":\"radio frequency\\ntraining\\nloading\\ntraining data\\nartificial neural networks\\nsensors\\ntask analysis\\nmobile robots\\nneurocontrollers\\nrandom forests\\nneural network controller\\nautonomous pile loading\\npile loading controllers\\nhuman demonstrations\\nrandom forest controller\\nrf controller\\nnnetv2\\nneural attention mechanism\\nsensor\\nlearning-based controller\\nheavy-duty machine\",\"424\":\"training\\ntitanium alloys\\nshape\\nreinforcement learning\\nkinematics\\nend effectors\\nrobustness\\nbending\\nelasticity\\nlearning (artificial intelligence)\\nmedical robotics\\nrobot kinematics\\nconcentric tube robot control\\nctrs\\ncontinuum robot\\npre-curved tubes\\nsuperelastic nickel titanium alloy\\nctr end-effector\\ncartesian space\\nprior kinematic model\\ndeep reinforcement learning approach\\ngoal-based curriculum reward strategy\\ncurricula\\nconstant decay functions\\nlinear decay functions\\nexponential decay functions\\nrelative representations\\nabsolute joint representations\\ntraining convergence\\nexponential decay relative approach\\nnoise-induced simulation environment\\ncomplex simulation environment\\ncartesian errors\\nrelative decay curriculum\",\"425\":\"visualization\\nthree-dimensional displays\\nconferences\\ndecision making\\nuser interfaces\\ntools\\nrobustness\\nbrain\\ncatheters\\nmedical computing\\nmedical image processing\\nmedical robotics\\nneurophysiology\\nsurgery\\nkeyhole neurosurgery\\ncomplex anatomy\\ninherent risk\\nvital structures\\nsurgical target\\npath planner\\nsafe interventions\\neffective neurosurgical interventions\\nmultiple risk structures\\ndeductive method\\nintuitive user interaction\\nmodular architecture\\nappropriate surgical trajectory\\nbrain matter\\nminimized risk\\nuser interface\\ndecision making process\\nsteerable catheters\\ndeductive reasoning\",\"426\":\"drugs\\nelectric potential\\nfluids\\ntissue engineering\\nmedical treatment\\nbending\\nmanipulators\\nadhesion\\nbiological tissues\\nbiomedical materials\\ncellular biophysics\\nelectrospinning\\nmedical robotics\\nnanofibres\\nneedles\\npatient treatment\\npolymer fibres\\nrobotic electrospinning actuated\\nnoncircular joint continuum manipulator\\nendoluminal therapy\\nexcellent benefits\\ntissue surface\\nlong-term continuous therapy\\nrobotic electrospinning platform\\nelectrospinning device\\nactuation unit\\ncontrollable spinning direction\\nnoncircular joint profile\\nsteering ability\\nbending limitation\\nendoluminal electrospinning\\nfibrous structure\\npotential endoluminal treatments\",\"427\":\"manifolds\\nsolid modeling\\nthree-dimensional displays\\ndistributed feedback devices\\nnumerical simulation\\nnumerical models\\ntopology\\ncascade systems\\ncontrol engineering computing\\ndistributed control\\nlyapunov methods\\nmobile robots\\nmotion control\\nmulti-robot systems\\ndistributed full-consensus control\\nmultirobot systems\\nfield-of-view constraints\\nfull-consensus problem\\nnonholonomic vehicles\\nleader-follower topology\\nsensing constraints\\npolar-coordinates model transformation\\ncontrol laws\\nlocal measurements\\nasymptotic convergence\\nconsensus manifold\\nsystems theory\\ngazebo-ros environment\",\"428\":\"uncertainty\\nmeasurement units\\ncollaboration\\nmaintenance engineering\\nstreaming media\\nposition measurement\\nsensors\\ninertial navigation\\nkalman filters\\nmulti-agent systems\\nmulti-robot systems\\nnonlinear filters\\nsensor fusion\\nstate estimation\\ninertial measurement unit\\npropagation sensor\\naided inertial navigation\\nmultirobot applications\\nquaternion-based error-state extended kalman filter\\nscalable recursive distributed collaborative state estimation\\nsensor information fusion\\ncollaborating agents\",\"429\":\"conferences\\nrobot vision systems\\ndecision making\\ndistributed databases\\nbenchmark testing\\ncameras\\nboosting\\nkalman filters\\nobject detection\\nsensor fusion\\ntarget tracking\\ntracking\\nwireless sensor networks\\ndistributed multitarget tracking\\ncamera networks\\nmultiple cameras\\ncentralized systems\\nmultitarget tracking approach\\ndistributed camera network\\ndistributed systems\\nlighter communication management\\ngreater robustness\\nlocal decision making\\ninformation fusion\\ncentralized setup\\nglobal information\\ndistributed-consensus kalman filter\\nre-identification network\\ndistributed tracker manager module\\nconsistent information\\ncross-camera data association\\nknown public data sets\\nexisting centralized tracking methods\",\"430\":\"protocols\\nsoftware architecture\\ntwo dimensional displays\\nswarm robotics\\nbidirectional control\\ntools\\nrobot sensing systems\\ncontrol engineering computing\\ndistributed processing\\nmobile robots\\nmulti-robot systems\\ngeneralised distributed experimental environmental grid\\ndistributed platform\\nmodular platform\\nswarm robotics experiments\\nswarm robot data\\nmodular grid\\nattachable computing nodes\\nrobotic agent\\ngrid nodes\\nsoftware architecture design\\ngengrid system\\ncommon experimental studies\\nmultirobot\\nopen-source hardware platform\\nswarm experiments\\nswarm robotics research community\\ncommercial platforms\\nswarm robotic experiments\\nopen-source\\nant optimisation\\ncollective\\npheromone\\nhall effect\\nstigmergy\",\"431\":\"automation\\nconferences\\nend effectors\\nhardware\\ntask analysis\\nmanipulator dynamics\\nstrain\\ndeformation\\nimpact (mechanical)\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmotion control\\noptimisation\\npath planning\\npredictive control\\nredundant manipulators\\nrobot-safe impacts\\nsoft contacts\\nlearned deformations\\ndiscontinuous velocity\\nhigh impact forces\\ngrabbing boxing\\ncontrol paradigm\\ndeformable contacts\\ndata-driven learning\\nshock-absorbing soft dynamics\\njoint-space limits\\nconstrained model-predictive control\\nreal-robot experiments\\npre-impact velocities\\nend-effector soft suction-pump\\nrigid objects\\ndeformable objects\\nredundant panda manipulator\\nrigid bodies impacts\",\"432\":\"legged locomotion\\nforce measurement\\ndynamics\\nestimation\\nposition measurement\\nrobot sensing systems\\nparticle measurements\\ngraph theory\\ninertial navigation\\nmobile robots\\nmotion control\\npath planning\\nrobot dynamics\\nrobot vision\\nsensor fusion\\nstability\\nstate estimation\\nbase position\\nlegged robot stabilization\\nbase state\\nunderactuated dynamics\\nangular momentum\\nrobot model\\nforce measurements\\nbias observable\\nbinary estimation\\ncontact state\\nunderactuation state\\ncomplete state estimator\\nfactor graphs\\nimu pre-integration method\\npositional motion\\nencoder measurements\\nleg odometry displacements\\nobservability\\nimu biases\\npositional states\\ncentroidal states\\nwhole-body estimator\\ncontact forces preintegration\\nlegged robotics\\nparticular estimation\",\"433\":\"road transportation\\ntrajectory planning\\ndecision making\\noptimal control\\nreinforcement learning\\nbenchmark testing\\ntrajectory\\nlearning (artificial intelligence)\\noptimisation\\npath planning\\nroad traffic\\nroad traffic control\\ntraffic engineering computing\\nprovable safe maneuvers\\ndynamic traffic environment\\nlong-term driving strategy\\nclassical trajectory planning\\nbased approach\\ntrajectory planner\\nlong-term decision-making strategy\\nhighways\\noptimal maneuvers\\nlow-level continuous action space\\npredefined standard lane-change actions\\nrandom action selecting agent\\ndiscrete actions agent\\nidm-based sumo-controlled agent\\namortized q-learning\\nmodel-based action proposals\\nautonomous driving\\noptimization-based methods\\noptimal trajectory\\nshort optimization horizon\\nshort horizon\\nlong-term solution\\nresulting short-term trajectories\\neffective maneuvers\\ncomfortable maneuvers\",\"434\":\"measurement\\nlearning systems\\nsimulation\\nconferences\\ndecision making\\nsemantics\\nreinforcement learning\\ntraffic engineering computing\\nautonomous driving\\ncomplex driving environments\\nautonomous agents\\nbehavioral policy\\nreward function\\nsemantic rewards\\naugmented airl\\ninteractive environment\\naugmented adversarial inverse reinforcement learning\\ninverse reinforcement learning\\nlane change\",\"435\":\"monte carlo methods\\nautomation\\nconferences\\nprediction algorithms\\ncognition\\nplanning\\nmobile robots\\npath planning\\nroad vehicles\\ntree searching\\nautonomous driving\\nrational inverse\\ngoal recognition\\nmonte carlo tree search algorithm\\nmcts\\noptimal maneuvers\\nego vehicle\\ninverse planning\\nrationality principles\\nurban driving scenarios\",\"436\":\"measurement\\nnavigation\\nconferences\\nrobustness\\nencoding\\nplanning\\ntrajectory\\nmobile robots\\npath planning\\nroad accidents\\ntemporal logic\\nvehicle dynamics\\nencoding human driving styles\\nautonomous vehicles\\nmotion planning techniques\\nsimple driving styles\\nencode human driving styles\\npenalty structure\\nmotion planning frameworks\\ndifferent automated driving styles\\nsignal temporal logic formula\\nresponsibility-sensitive safety model\\ndifferent driving styles\\nmotion planner\\nunique driving styles\\ndefensive automated driving style\\nautonomous vehicle navigation\\nformal methods in robotics and automation\\nhuman factors and human-in-the-loop\",\"437\":\"training\\nlaser radar\\nconferences\\npipelines\\nrobot sensing systems\\nfeature extraction\\nsensor systems\\nconvolutional neural nets\\nimage recognition\\nmobile robots\\nnearest neighbour methods\\noptical radar\\nslam (robots)\\nspherical multimodal place recognition\\nheterogeneous sensor systems\\nrobust end-to-end multimodal pipeline\\nlidar scans\\nlocal feature extraction modules\\nsensor data\\nmultimodal descriptor\\nspherical convolutional neural network\\nspherical projection model\\narbitrary lidar\\ncamera systems\\nnearest-neighbor lookup\\nvision-based system\\nlidar-based system\\nmobile robotics\",\"438\":\"wrist\\nin vivo\\ncorrelation\\ncomputational modeling\\nkinematics\\nrehabilitation robotics\\nprogramming\\nelectromyography\\ngait analysis\\ngenetic algorithms\\nmedical robotics\\nmedical signal detection\\nmedical signal processing\\npatient rehabilitation\\ndirect collocation method\\nemg-driven wrist muscle musculoskeletal model\\nhuman intention\\nrehabilitation robots\\nmuscle-tendon force\\njoint kinematics\\nmuscle-tendon parameters\\nsubject-specific parameters\\nwrist musculoskeletal model\\noptimized parameters\\ngenetic algorithm\\ndc method\\ndirection collocation method\\noptimization time\",\"439\":\"gradient methods\\nautomation\\nbrushless dc motors\\ncomputational modeling\\nconferences\\noptimal control\\nfasteners\\nautonomous aerial vehicles\\nhelicopters\\ntime optimal control\\nindirect projected gradient method\\nfixed end-time optimal control\\nbattery powered multirotor uavs\\nflight time\\nplanar quadrotor\\nenergy-optimal trajectories\\nrest-to-rest maneuvers\",\"440\":\"automation\\ntrajectory planning\\nconferences\\noptimization methods\\nunmanned aerial vehicles\\nland vehicles\\ntrajectory\\nautonomous aerial vehicles\\ncollision avoidance\\nmobile robots\\nnonlinear programming\\ntrajectory control\\ntether positions\\nrobot velocities\\nmultiobjective optimization framework\\nsmooth trajectories\\nuav\\noptimization-based trajectory planning\\ntethered aerial robots\\nnonlinear optimization method\\nunmanned aerial vehicle\\nunmanned ground vehicle\\ncollision-free trajectory\\nugv position\\nobstacle-free trajectories\",\"441\":\"analytical models\\ncomputational modeling\\nforce\\ngrasping\\nkinematics\\ndata models\\nfinite element analysis\\nforce control\\ngrippers\\nmanipulator dynamics\\nmanipulator kinematics\\nsupports\\ngrasp forces\\nhigher fidelity finite elements methods\\nstructural model\\ngrasp analysis\\nmanipulation kinematics\\nsoft isoperimetric truss robots\\nforce distribution\\nopen-loop control\\ndirect stiffness model\\nstructural properties\\nrobot workspace\",\"442\":\"measurement\\nautomation\\nconferences\\nfingers\\ntactile sensors\\nrandom forests\\ngaussian processes\\nimage classification\\nmanipulators\\nrobot vision\\ngrasp classification\\nspatial metrics\\nnear-contact grasps\\nsensor data\\nimage data\\nrandom forest classifier\\nnear-grasp quality prediction\\nrobotic grasping\",\"443\":\"wrist\\nperformance evaluation\\nsociology\\ngrasping\\nmuscles\\ntools\\nsize measurement\\nbioelectric phenomena\\ngait analysis\\ninjuries\\nneuromuscular stimulation\\nneurophysiology\\npatient rehabilitation\\nassistive supernumerary grasping\\ndorsal grasper\\nassistive wearable grasping device\\nsupernumerary fingers\\nartificial palm\\npower wrap grasping\\nadduction pinching\\nv-shaped soft fingers\\nactive wrist extension\\ngrasp forces\\nreliable grasping\\nfunctional grasping\\ncontact friction\\nc6-c7 spinal cord injury\",\"444\":\"monte carlo methods\\ncorrelation\\nautomation\\nconferences\\nmarkov processes\\nmobile robots\\nbayes methods\\nprobability\\nstatistical distributions\\nmcmc occupancy grid mapping\\ndata-driven patch\\noccupancy grids\\noccupancy probability\\nrecent occupancy grid mapping algorithms\\ncell correlations\\ngiven map\\ndata-driven prior probabilities\\nsample maps\",\"445\":\"training\\nautomation\\nshape\\nconferences\\nstrain\\nimage representation\\nlearning (artificial intelligence)\\nmanipulators\\nshape recognition\\nlatent representation\\nsuccessful execution\\ncategory-level skill\\nlearned manipulation policy\\nunseen objects\\ncomplex variation\\nnonlinear variation\\nsuccessful control parameters\\nshape-based transfer\\ngeneric skills\\ndata-efficient approach\\nskill transfer\\nknown categorical shape variation\\nlow-dimensional shape representation embedding\\ndeformations\\nknown objects\",\"446\":\"uncertainty\\nsystem dynamics\\ncurrent measurement\\nconferences\\nmeasurement uncertainty\\ngaussian processes\\nsafety\\nadaptive control\\nmobile robots\\npath planning\\nquadratic programming\\nrandom processes\\nrobot vision\\nsampling methods\\nset theory\\ncontrol barrier functions\\ndynamical system\\nsystem states\\nset defined a priori\\ntypical constructions\\ncbf framework\\nobservability\\nunmodeled dynamics\\nonline setting\\ncbf smooth function\\naccount safety uncertainty\\nonline measurements\\nmeasurement states\\nincorporating safety uncertainty\\nlocal safety map\\nminimal safety\\ncurrent safety limit\\nsafety margin increases\\nminimally safe exploratory samples\\ncurrent safe set\\ninitial safe set\",\"447\":\"training\\ntime-frequency analysis\\nprocess control\\npredictive models\\ntrajectory\\nplanning\\ntask analysis\\ncollision avoidance\\ncomputational geometry\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\npath planning\\npose estimation\\nrobot vision\\ntabletop rearrangement task\\ncollision-free grasps\\nplacements\\nsimulated scenes\\nphysical cluttered scenes\\nlearned ablations\\nsimulated collision queries\\nlearned implicit collision functions\\nrobotic object rearrangement\\nplacing objects\\nobject models\\ntypical collision-checking models\\npartial point clouds\\ncollision-free grasping\\nlearned collision model\\nquery object point clouds\\n6dof object\\n2 billion collision queries\\nmodel predictive path integral policy\",\"448\":\"location awareness\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nshape\\ntoy manufacturing industry\\ncameras\\nsoftware\\nimage reconstruction\\nmobile robots\\noptical scanners\\nrobot vision\\nslam (robots)\\nstereo image processing\\nactive laser-stripe triangulation hardware\\ninertial sensors\\n3d point cloud\\ninertial measurements\\nvisual-laser-inertial slam\\ncompact 3d scanner\\nthree-dimensional reconstruction\\nsimultaneous localization and mapping software\",\"449\":\"laser radar\\nthree-dimensional displays\\nmonte carlo methods\\nwheels\\ninertial navigation\\nrobot sensing systems\\nreal-time systems\\ncalibration\\nfiltering theory\\nglobal positioning system\\nimage fusion\\nmobile robots\\noptical radar\\nrobot vision\\nsensor fusion\\nefficient lidar integration\\nsliding-window filtering fashion\\n3d lidar scan\\nplane patches\\nsparse lidar point cloud\\nlidar plane patch processing algorithm\\nefficient multisensor aided inertial navigation\\nonline calibration\\nmultisensor aided inertial navigation system\\nwheel encoder\\nonline spatiotemporal sensor calibration\",\"450\":\"performance evaluation\\nsensitivity\\nembedded systems\\nconferences\\nfocusing\\ncomputer architecture\\nreal-time systems\\ndistance measurement\\nimage colour analysis\\nimage matching\\nimage sensors\\nimage sequences\\nstate estimation\\nrobust monocular visual-inertial depth completion\\nvisual-inertial odometry system\\nsparse depth\\nimage guidance\\nreal-time performance\\nvio+depth system\\nnoisy depth\\npredicted dense depth maps\\nimage-only depth network\\ndepth completion networks\\nhigh quality sparse depths\\nembedded devices\\nvio depth completion system\\nindoor handheld rgb-d dataset\\noutdoor simulated handheld rgb-d dataset\\nopenvins\\ncomputational analysis\",\"451\":\"location awareness\\nuncertainty\\nconferences\\npredictive models\\nprobabilistic logic\\nprobability distribution\\nreliability\\ngaussian distribution\\nmobile robots\\nstate estimation\\nnongaussian state belief\\nmotion model parameters\\nstate prediction\\nmodel parameter error\\nrobotic state estimation\\nrobotic localization\\nsecond-order simulated systems\\nmultidimensional nongaussian probability distribution functions\\nprobabilistic simultaneous estimation and modeling\",\"452\":\"roads\\nvelocity control\\ngaussian distribution\\nhardware\\ntable lookup\\ncalibration\\nnumerical models\\nroad vehicles\\nvehicle dynamics\\nautonomous vehicle\\ngaussian model approach\\nefficient online calibration system\\nlongitudinal vehicle dynamics\\ndriverless cars\\ndata-driven method\\ncontrol command\\nreference table\\ncalibration process\\n2-d gaussian distribution\\nmodel acceleration error\\nlook-up table\\nvehicle sensors\\nupdated table\\nend-to-end numerical model\",\"453\":\"conferences\\nsemantics\\nfuzzing\\ntime measurement\\nsoftware\\nhazards\\ncomputer crashes\\nfuzzy set theory\\nmobile robots\\nprogram testing\\nrobot dimensions\\ntest input generation process\\nphys-fuzz\\nbase-fuzz\\nfuzzing mobile robot environments\\nfast automated crash detection\\nautomated test input generation technique\\nfailure inducing inputs\\nsimple fuzzing adaptation\",\"454\":\"legged locomotion\\nestimation error\\nautomation\\nerror analysis\\nconferences\\nhumanoid robots\\ndynamic programming\\nkalman filters\\noptimal estimation\\ncentroidal dynamics\\nlegged robots\\nmulticontact locomotion\\ndifferential dynamic programming approach\\nsimulated humanoid robot\\ncentroidal state\\ndynamics consistency\\nhrp-2 humanoid robot\",\"455\":\"legged locomotion\\nmeasurement\\ncouplings\\nactuators\\ntorque\\nrobot sensing systems\\nenergy efficiency\\ncontrol system synthesis\\nmotion control\\noptimisation\\nrobot dynamics\\nstability\\nstance\\nswing control\\nunified optimization framework\\nperformance metrics\\nrobot leg design\\ncontrollable locomotion\\nstable locomotion\\ndynamic environments\\nseparate evaluation\\ndesign optimization framework\\nanthropomorphic robot leg model\\noptimal actuation configurations\",\"456\":\"legged locomotion\\ntracking\\nsystem dynamics\\ncomputational modeling\\nsystem performance\\ndynamics\\npredictive models\\ncontrol system synthesis\\nforce control\\nhumanoid robots\\nmobile robots\\nmotion control\\npredictive control\\nquadratic programming\\nrobot dynamics\\ndynamic locomotion\\nlegged robots\\nmultioutput\\ntime-variant\\nintricate full-body dynamics\\nempirically simplified model\\nlocomotion capability\\nlegged robot dynamics\\ninteraction forces\\nlower-dimensional dynamics\\nfull-order robot model\\ncorresponding model predictive control framework\\nconstrained leg reaction forces\\nplanar five-link biped robot\\nbody reference tracking\\nblind locomotion\\npromising dynamic motion control scheme\\nnovel model predictive control framework\\ndynamic model decomposition applied\\ndynamic legged locomotion\",\"457\":\"torso\\nlegged locomotion\\nforce\\ndynamics\\nstability analysis\\nreal-time systems\\nplanning\\nmanipulators\\nmobile robots\\npath planning\\npredictive control\\nmanipulator\\ncontinuous plans\\ngenerating continuous motion\\nforce plans\\nlegged robots\\nlegged mobile manipulation planners\\ncontact forces\\ncurrent planning strategies\\nmanipulation autonomous planner\\nrobot\\ndynamic feasibility\\nstability\\nmotion tasks\",\"458\":\"three-dimensional displays\\ncodes\\nautomation\\nconferences\\nweaving\\ntrajectory\\ntask analysis\\ncollision avoidance\\nindustrial manipulators\\nmanipulator dynamics\\nmanipulator kinematics\\nmobile robots\\nmotion control\\nquadratic programming\\nrobot vision\\nsupervised learning\\nknocking\\nvaulting\\nquadratic programs\\n3d apex point\\nfixed-endpoint cable dynamic manipulation\\nlost arc robot\\ntask-specific apex points\\nfixed apex\\nphysical cables\\nminimum-jerk motions\\narcing motion\\ntask-specific trajectory function\\nur5 robot\\nself-supervised learning framework\\nropes\\nhigh-speed robot arm motions\",\"459\":\"visualization\\nthree-dimensional displays\\nautomation\\nconferences\\nbenchmark testing\\nfabrics\\nplanning\\nlearning (artificial intelligence)\\nmanipulators\\npath planning\\nrobot programming\\nrobot vision\\nrobotic manipulation learning\\nrigid objects\\nmultistep planning\\ndeformable structures\\nimage-based goal-conditioning\\nmultistep deformable manipulation\\ndeformable objects\\ndeformable cables\\ngoal-conditioned transporter networks\\ndeformable bags\\ndeformable fabrics\\ndeep features\",\"460\":\"location awareness\\nvisualization\\nautomation\\nconferences\\nnatural languages\\npipelines\\ngrasping\\ncontrol engineering computing\\nimage colour analysis\\nimage texture\\nmanipulators\\nnatural language processing\\nobject detection\\nquery processing\\ncgnet\\ngrasp detection\\ntarget object grasping\\nnatural language command query\\ncommand grasping network\\ntextual command inputs\\nrgb image\\nvmrd dataset\",\"461\":\"training\\nnavigation\\nconferences\\nreinforcement learning\\naerospace electronics\\nbenchmark testing\\ngenerators\\nmanipulators\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nmotion generation\\nmobile manipulation\\njoint control signals\\ncontinuous control tasks\\nmotion generator\\nsampling-based motion planners\\nrl methods\\noriginal action space\\nlearned policy\\nnavigation problems\\nmanipulation tasks\\nrobotics tasks\\nmotion generators\\nhierarchical rl baselines\\nrelmogen\",\"462\":\"uncertainty\\ncomputational modeling\\nartificial neural networks\\nrobot sensing systems\\ncomputational efficiency\\nsafety\\npartitioning algorithms\\nclosed loop systems\\ncontrol system synthesis\\nconvex programming\\nlinear programming\\nneurocontrollers\\noptimisation\\nreachability analysis\\nneural network controllers\\nneural networks\\nempirical performance improvements\\nrobotic systems\\nforward reachable set\\nclosed-loop systems\\nnn controllers\\nreachable sets\\ncomputationally efficient approaches\\noverly conservative bounds\\nonline computation\\nwork bridges\\nprior semidefinite program-based methods\\navailable computation time\\nefficient reachability analysis\",\"463\":\"automation\\nnavigation\\nconferences\\ntools\\naerospace electronics\\nplanning\\nfeedback control\\ncollision avoidance\\nfeedback\\nlegged locomotion\\nmotion control\\ntrajectory control\\nbipedal robots\\nflat-terrain motions\\nunderactuated bipedal robot\\nsteerable variant\\nnonholonomic motion planning\\nsnakeboard gaits\\nfeedback control strategies\",\"464\":\"legged locomotion\\nsolid modeling\\nthree-dimensional displays\\nconferences\\nposition control\\nturning\\nplanning\\ncontrol system synthesis\\nfeedback\\nmotion control\\nnonlinear control systems\\npendulums\\npredictive control\\nrobot dynamics\\nglobal position control\\nh-lip walking\\nactual s2s dynamics\\nfeedback controller\\nmodel predictive control\\nh-lip stepping\\ndesired step\\nstep planning\\n3d underactuated bipedal robot cassie\\nstep-to-step dynamics approximation\\nunderactuated bipedal walking\\nmpc\",\"465\":\"legged locomotion\\npredictive models\\nturning\\nrobot sensing systems\\nfeedback control\\ntrajectory\\ntask analysis\\ncontrol system synthesis\\nfeedback\\ngait analysis\\nmotion control\\nrobot dynamics\\nvelocity control\\nstep ahead prediction\\ncontact point\\nbipedal locomotion\\nlip-inspired controller\\ncurrent state information\\nlinear velocity\\ncontrol objectives\\none-step ahead evolution\\nsimilar prediction\\ncontrol design decisions\\ncurrent step\\nangular momentum objective\\nresulting feedback controller\\n20 degree-of-freedom bipedal robot\",\"466\":\"legged locomotion\\nheuristic algorithms\\nsimulation\\nconferences\\ndynamics\\nbenchmark testing\\nhybrid power systems\\ncollision avoidance\\nmobile robots\\noptimisation\\npath planning\\nrobot dynamics\\nsampling methods\\ntrees (mathematics)\\nagile jumping robots\\nchallenging terrain\\nhybrid planning framework\\ncomplex dynamic motion plans\\nlegged robots\\nmotion primitive\\ntrajectory optimization\\nkinodynamic rapidly-exploring random trees planner\\ncontrol sampling scheme leverages\\nprecomputed velocity reachability map\\nredundant jumps\\ndynamically feasible trajectory\\nhybrid planner\\nmultiple consecutive jumps\",\"467\":\"automation\\nshape\\nconferences\\nobservers\\nunmanned aerial vehicles\\ntrajectory\\nlabeling\\nautonomous aerial vehicles\\ngesture recognition\\nregression analysis\\nremotely operated vehicles\\nhuman gestural communication\\nuav gesture perception study\\nobserver viewpoint perspective\\nuav gestural motion\\nrobot gesture designers\\nviewer perspective\\nonline user-studies\\nparticipants\\nintended shape\\ntwo-dimensional uav gestures\\nparticipant gesture classification accuracy\\nviewpoint angle threshold\\nperceptibility score\\nflight paths impact perception\\nuav gesture systems\\ngesture motions\\nhuman observers\\nunmanned aerial vehicle gesture perceptibility\\nviewpoint variance\\nunmanned aerial vehicle flight paths\",\"468\":\"legged locomotion\\nautomation\\nnavigation\\nrobot kinematics\\nconferences\\nvirtual reality\\ntrajectory\\nmobile robots\\npath planning\\nservice robots\\nperson\\nprecedes head orientation\\nsocial robot navigation\",\"469\":\"training\\nemotion recognition\\nnavigation\\ncomputational modeling\\nconferences\\nreinforcement learning\\ncognition\\ncognitive systems\\nhuman-robot interaction\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nparticular location\\ncognitive map\\ndirectional instructions\\nspoken instructions\\nnavigational guidance\\nnovel deep reinforcement learning based trust-driven robot navigation algorithm\\nhumans\\nlanguage guided navigation task\\ntrustworthy human guidance\\nrobot trust metric\\nlanguage-based instructions\\nhuman trust metric\\ntrust metrics\\noptimal cognitive reasoning scheme\\nlearned policy\\ndrl-based approach\\nhuman-guided navigation\",\"470\":\"training\\nvisualization\\nmonte carlo methods\\nreinforcement learning\\ntools\\naerospace electronics\\nfractals\\nlearning (artificial intelligence)\\nmarkov processes\\nmesh based analysis\\nlow fractal dimension reinforcement learning policies\\nmeshing\\nreachable state space\\ncontinuous systems\\nhybrid systems\\nmarkov chain\\nlocomotion policies\\nempirical guarantees\\nstability properties\\nmodified reward function\\non-policy reinforcement\\nrollout trajectories\\ninduce individual trajectories\\ndiscrete mesh\\nsystem subject\\nmodified policies\\nsmaller reachable meshes\\nfractal dimension reward transfer their desirable quality\\ncompact state space\\nmesh based tools\\nrl policies\\nhigher dimensional systems\\nhigher resolution meshes\",\"471\":\"limiting\\nergonomics\\nimpedance matching\\nexoskeletons\\nsociology\\nforce\\nkinematics\\nmanipulator kinematics\\nwearable robots\\ngrip force\\nstimulation avoidance\\nupperbody exoskeletons\\ncylindrical handles\\nactive grip strength\\nundesirable flexion synergy stimulation\\nergonomic design\\nhandle design\\nexoskeleton-hand attachment interface\\neminence grip\\nkinematic equivalence\",\"472\":\"telepresence\\nautomation\\nrobot kinematics\\nconferences\\nrobot vision systems\\nhumanoid robots\\nsupervisory control\\ncameras\\nhuman-robot interaction\\nmanipulators\\nmedical robotics\\nmobile robots\\nmulti-robot systems\\ntelerobotics\\nvirtual reality\\nmanipulation task\\nrobot action\\nmultiple moving cameras\\ntask supervision\\nautonomous camera control\\ntask performance\\nsupervisory control task\\nautonomous camera selection\\nrobot autonomy\\nactive telepresence assistance\\nmulticamera tele-nursing robot\\nhumanoid robot\",\"473\":\"automation\\nconferences\\nhuman-robot interaction\\ncollaboration\\nprototypes\\nrobot sensing systems\\nsensors\\ncontrol engineering computing\\nforce sensors\\nmotion control\\nsensor arrays\\nphysically demanding subsets\\nhuman-robot collaborative scenarios\\ntensegrity robots\\ncompliant systems\\nphysical interactions\\nuseful rigid properties\\n6-bar spherical tensegrity\\nforce-sensing capabilities\\nforce-sensor array\\nphysical interaction types\\nhuman context\\nunique interactions\\nrepresentative interactions\\nhuman operator\\nhuman-robot collaborative setting\\nforce-sensing tensegrity\\ninvestigating physical human-robot interaction\\ncompliant robotic systems\\nparticular systems\",\"474\":\"uncertainty\\nhospitals\\nservice robots\\npredictive models\\ncost function\\nplanning\\nsafety\\ncontrol engineering computing\\ndecision making\\nfunction approximation\\nhealth care\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmedical robotics\\nmobile robots\\npath planning\\nrisk management\\nrisk-aware decision making\\npatient falls\\nautonomous systems\\nuncertain environments\\ndynamic environments\\nhealth care settings\\nhuman patients\\nrisk-aware planning\\nassistive device\\nmodel-based control\\nfall prevention task\\nfunction approximators\\nrisk metrics\\nplan interventions\\nend-to-end learning\\nrisk minimization\\nlearning-based prediction\\nrobot performance\\nfall score events\",\"475\":\"automation\\nconferences\\nforce\\nhaptic interfaces\\nsafety\\ntask analysis\\nrobots\\ncontrol engineering computing\\nhuman-robot interaction\\nmobile robots\\ntelerobotics\\nuser satisfaction\\nshared-autonomy teleoperation\\nhuman-robot agreement\\nhaptic shared autonomy teleoperation paradigm\\nhaptic feedback\\nshared autonomy paradigm\\nhaptic shared control\\nhuman operator full control\\nassistive teleoperation methods\\ncontrol command\\nassistive teleoperation conditions\\nhuman-generated control\",\"476\":\"automation\\nautonomous systems\\nconferences\\npipelines\\nestimation\\nplanning\\nrobots\\ncollision avoidance\\nintelligent robots\\nmanipulators\\nmobile robots\\npath planning\\nrobot vision\\nenvironment reconfiguration planning\\nautonomous robotic manipulation\\nmobility constraints\\nintelligent robotic environment reconfiguration\\nmobile manipulation system\\nunknown challenging environments\\ngiven manipulation capabilities\\nmid-range traversability estimation graph-based backend\\nfocused environment alteration\\nrelevant autonomous system\",\"477\":\"monte carlo methods\\ncodes\\nconferences\\nmarkov processes\\nunmanned underwater vehicles\\nsearch problems\\nplanning\\ndecision theory\\nsampling methods\\ntree searching\\nrollouts\\nadaptive sampling tasks\\nsingle planning tree\\npomdps\\ndomain-specific considerations\\nmonte carlo tree search based solvers\\npartially observable markov decision processes\\nsampling problems\\nrollout allocation\\naction exploration algorithm\\nplan commitment\",\"478\":\"damping\\nperturbation methods\\nconferences\\nhuman-robot interaction\\nkinematics\\nmaintenance engineering\\nmanipulators\\nmotion control\\nposition control\\nstability\\nhuman arm stability\\ndamping-defined mechanical environments\\nphysical interaction\\nrobotic damping\\nposture maintenance tasks\\narm postures\\nrobotic arm manipulator\\nanterior-posterior direction\\nap\\nmedial-lateral direction\\nml direction\\nhuman-robot system\\nrobotic impedance controller\\nrobotic admittance controller\",\"479\":\"learning automata\\nrobot vision systems\\nmarkov processes\\nprediction algorithms\\ncameras\\nhardware\\ndata models\\nplanning (artificial intelligence)\\nrobot programming\\nstochastic processes\\nnarrative observation\\nstochastic process\\nvideo camera\\nstylistic selections\\nmarkov decision process\\ndecoupled approach\\nplanning algorithm\\nmonolithic planner\",\"480\":\"legged locomotion\\nsolid modeling\\nthree-dimensional displays\\ncosts\\ncomputational modeling\\nfriction\\nreinforcement learning\\ndeep learning (artificial intelligence)\\nhumanoid robots\\nrobot dynamics\\nentire dynamics\\ndeepq stepper\\nnonconvex terrain\\nreactive dynamic walking\\nuneven terrain\\nreactive stepping push recovery\\nnonlinear dynamic models\\noptimal steps\\n3d reactive stepper\\nobstacles\",\"481\":\"temperature measurement\\nsoil measurements\\ninstruments\\nsea measurements\\nsoil\\npollution measurement\\nocean temperature\\ncontamination\\ngeotechnical engineering\\nwetland soil strength tester\\ncore sampler\\nsoil strength testing\\ncollecting soil cores\\nslow process\\ndisturbing soil samples\\ncontaminating soil samples\\ninstrumented dart\\ncore sample tests\\nsoft soils\\npenetrometer tests\\ndrone-mounted mechanism\\nsample return\\nmultiple soil strength tests\\ndart tip diameter\\ndrop height\\nsoil retrieval\\npenetration depth\",\"482\":\"neurodynamics\\nautonomous underwater vehicles\\nuncertainty\\nbackstepping\\nbiological system modeling\\nvelocity measurement\\nvehicle dynamics\\nadaptive control\\ncontrol nonlinearities\\ncontrol system synthesis\\nlyapunov methods\\nmobile robots\\nneurocontrollers\\nnonlinear control systems\\nremotely operated vehicles\\nuncertain systems\\nunderwater vehicles\\nvariable structure systems\\nsliding mode controls\\nbioinspired neural dynamics\\nautonomous underwater vehicle\\ncombined tracking method\\nbioinspired neurodynamics\\ntracking control method\\ncontrol strategy\\nconventional backstepping\\nbackstepping control\\nsliding mode control\",\"483\":\"learning systems\\nautomation\\nshape\\nconferences\\nneural networks\\nkinematics\\nelectron tubes\\ncontrol engineering computing\\nimage representation\\nlearning (artificial intelligence)\\nneurocontrollers\\nrobot kinematics\\nrobot vision\\nconcentric tube continuum robots\\ndiscrete point-wise shape representation\\nvanilla numerical method\\nlearning-based inverse kinematics\\nshape-to-joint inverse kinematics\\nimage-to-joint inverse kinematics\",\"484\":\"nonlinear equations\\nshape\\ncomputational modeling\\nprototypes\\nmathematical models\\nreal-time systems\\nnumerical models\\napproximation theory\\nintegration\\nrobot dynamics\\nrobot kinematics\\nsplines (mathematics)\\nnumerical static model\\ntendon-driven continuum robots\\nin-plane external tip forces\\neuler curves\\ncontinuum robot backbones\\nconfiguration space\\nlinear approximation\\nbackbone curvature\\neuler arc splines\\nbackbone shape approximation\",\"485\":\"support vector machines\\nknowledge engineering\\nactuators\\nlimiting\\nconferences\\nmuscles\\nbenchmark testing\\ncontrol system synthesis\\ninternet\\nmobile robots\\nmuscle\\npneumatic actuators\\nrobot muscle designs\\nweb-searchable interface\\nsvm\\ndata-driven approach\\nmuscle actuation technology\\nappropriate actuation strategy\\nartificial muscle-powered robots\\ndata-driven actuator selection\\nreal-world applications\\nartificial muscle technologies\\nactuation performance criteria\\nspecific needs\\nrobot muscle actuator selection\",\"486\":\"training\\nendoscopes\\nforce\\nartificial neural networks\\npredictive models\\nmuscles\\nelectromyography\\nbiomechanics\\nhaptic interfaces\\nmedical signal processing\\nneural nets\\nemg-based neural network model\\nhuman arm dynamics\\nhaptic training simulator\\nsinus endoscopy\\nhuman forearm\\nconventional lumped mass-spring-damper model\\nmuscle activation level\\nsurface electromyography signals\\nfive-parameter mass-spring-damper model\\nmuscle stiffness\\nlumped parameters\\nhaptic interface\\nimpedance model\\nnonlinear viscoelastic kelvin-voigt model\\nconstant-parameter lumped model\\nsinus tissue\\nlevenberg-marquardt algorithm\",\"487\":\"visualization\\ndynamics\\nforestry\\npredictive models\\nrobot sensing systems\\ndata models\\nsensors\\ncollision avoidance\\nmobile robots\\npath planning\\nvehicle dynamics\\nunstructured environments\\nenvironment interact\\nmultiple sensors\\nmaximal information\\nmotion planning\\nlong-horizon motion predictions\\nlidar\\nproprioception\\nleverage vision\\nrobot navigating\\ndifferent modality combinations\\nraw vision input\\ndynamics modeling\\nmultimodal dynamics\\noff-road autonomous vehicles\\noutdoor environments\",\"488\":\"shortest path problem\\nautomation\\nheuristic algorithms\\nconferences\\nmarkov processes\\nrouting\\nsafety\\naircraft\\ndecision theory\\nstochastic processes\\nstochastic shortest path problem\\ncc-ssp problem\\nchance constrained stochastic shortest path problems\\naircraft routing problem\\nflight automation\\nroute planner\\nchance constrained partially observable markov decision process\\ncc-pomdp problem\",\"489\":\"automation\\nnavigation\\nconferences\\ntrajectory\\nsafety\\nplanning\\nmobile robots\\ncollision avoidance\\npath planning\\ntelerobotics\\nintention guided hierarchical framework\\ntrajectory-based teleoperation\\nhuman-in-the-loop navigation\\nlong-horizon paths\\nnavigation task\\nhierarchical teleoperation framework\\nglobal plan\\nglobal path\\ncircumvent obstacles\\ndynamic-level control inputs\\ntrajectory generation\\ndense obstacles\\noperator engagement\",\"490\":\"automation\\nconferences\\nminimization\\nnp-complete problem\\nrobots\\noptimization\\ncombinatorial mathematics\\ncomputational complexity\\niterative methods\\noptimisation\\npolynomials\\nstate representations\\nminimalist robots\\nexact combinatorial filter reduction\\npolynomial number\\nconjunctive normal form capture\\niterative filter reduction\",\"491\":\"visualization\\nthree-dimensional displays\\nphilosophical considerations\\nnavigation\\nforestry\\nrobot sensing systems\\ncalibration\\ncameras\\npose estimation\\nrobot vision\\nstereo image processing\\nquadrotor navigation\\nmorphable design\\ndepth-based visual control\\nupcoming trends\\nquadrotor autonomy\\nstereo-cameras\\nperfect balance\\ndepth estimation\\ndesign time\\nstereo camera system\\ndiscrete baselines\\nentire baseline range\\nsynchronization errors\\nvariable baseline system\\nvariable baseline stereo vision system\",\"492\":\"automation\\ncontrol design\\nconferences\\nwires\\ncharging stations\\ncameras\\nrobustness\\naircraft landing guidance\\nautonomous aerial vehicles\\nbattery powered vehicles\\ncontrol system synthesis\\nhelicopters\\nposition control\\nrobust control\\nconductive hooks\\nquadrotor\\ncharging wire center\\ndrive-through recharging strategy\\nground charging station\\nportable charging wire\\nrecharging\",\"493\":\"statistical analysis\\nrobot vision systems\\nestimation\\nchebyshev approximation\\nunmanned aerial vehicles\\ntrajectory\\nsensors\\nautonomous aerial vehicles\\ncontinuous time systems\\ngraph theory\\ninterpolation\\noptimal control\\npolynomials\\nrobot dynamics\\npseudospectral parameterization\\ncontinuous time trajectory representation\\nchebyshev polynomial basis\\ndynamics models\\nrobot dynamics estimation\\nhigh-performance robotics applications\\nmodel dynamics\\nfactor-graph based framework\\npseudo spectral optimal control\\ninterpolating polynomials\\ntrajectory estimates\\naccurate flight dynamics estimation\\nmultirotor aerial vehicles\\nrepresentation framework\\nhigh-performance applications\\ncontinuous-time state\\npseudo-spectral parameterization\",\"494\":\"solid modeling\\nuncertainty\\nsemantics\\nprobabilistic logic\\nbayes methods\\ntrajectory\\nvelocity measurement\\ngradient methods\\nimage segmentation\\ninformation theory\\nmobile robots\\nmulti-robot systems\\nroad vehicles\\nsensors\\nactive bayesian multiclass mapping\\nsemantic segmentation observations\\nrobot applications\\nautonomous exploration\\nunknown environments\\nunstructured environments\\ninformation-based exploration techniques\\ncauchy-schwarz\\nactive binary occupancy mapping\\nrange measurements\\nenvision robots\\nsemantically meaningful objects\\nsemantic categories\\nmap representation\\nexploration objective\\nshannon mutual information\\nmulticlass map\\npotential robot trajectories\\nfsmi exploration\\nbayesian multiclass mapping algorithm\\nrange-category measurements\",\"495\":\"costs\\ncomputational modeling\\nactive perception\\nswitches\\npredictive models\\nprobabilistic logic\\nplanning\\ncognition\\ndecision theory\\nmarkov processes\\noptimisation\\nplanning (artificial intelligence)\\nrobots\\ntask-relevant information\\ninformation acquisition\\nnear-optimal task performance\\nattention-based active perception\\nhierarchical planning framework\\nattention sustaining phase\\ncurrent attention\\ncomputation-efficient optimal planning\\nattention-based probabilistic planning\\nattention control\\nattention-based planning\\nattention modes\\nattention mode corresponds\",\"496\":\"navigation\\nrobot sensing systems\\nsearch problems\\nunmanned aerial vehicles\\nsensors\\ntrajectory\\nplanning\\nautonomous aerial vehicles\\ncollision avoidance\\ngraph theory\\nmobile robots\\nmulti-robot systems\\npath planning\\nremotely operated vehicles\\nstate-space methods\\nsearch-based planners\\ngeneral guarantee completeness\\nprovable bounds\\nunderlying graph discretization\\nkinodynamically feasible paths\\njoint space\\nsensor state variables\\nstandard search\\nalternative search-based approaches\\nsensor trajectories\\ndecoupled state spaces\\nsensor headings\\nkinodynamically constrained unmanned aerial vehicle performing coverage\\nactive sensing\\ngoal-directed coverage tasks\\nrobotic coverage\\ncollision-free robot trajectory\\nactive control\\nonboard observational sensors\\nplanning robot\\ninformation gain\\nsensor footprint\",\"497\":\"actuators\\nself-assembly\\nboats\\nposition control\\npropulsion\\nrobot sensing systems\\ntrajectory\\nattitude control\\nmarine robots\\nmotion control\\nmodular underactuated oscillating swimming robot\\ndocking mechanism\\nmodular self-assembly\\nmodboat\\ninexpensive surface-swimming robot\\nunderactuated surface-swimming robot\\noscillating surface-swimming robot\\nsingle motor\\nprecise position\\norientation control\\ndocking strategy\\ndock configuration\\nmodular robotic systems\\nmotion primitive\",\"498\":\"geometry\\nsolid modeling\\nthree-dimensional displays\\nsemantics\\nsonar\\nimaging\\npredictive models\\nbayes methods\\nimage reconstruction\\ninference mechanisms\\noceanographic techniques\\nsonar imaging\\nstereo image processing\\nwide-aperture imaging sonar\\nstereo pair\\northogonally oriented sonars\\nspatial dimension\\nprior assumptions\\nscene geometry\\nfields-of-view\\nsonar image observations\\nunknown elevation angle\\nlarge-scale 3d reconstruction\\nsensor arrangement\\nsemantic classes\\nsubsea environment\\nbayesian inference framework\\nobject class\\northogonal sonar fusion system\\nunknown 3d structure\\noutdoor littoral environment\\nunderwater environments\\nobject-specific bayesian inference\\ndense 3d reconstruction\",\"499\":\"automation\\nanimals\\nrobot kinematics\\nconferences\\ndecision making\\nunmanned underwater vehicles\\nreal-time systems\\nautonomous underwater vehicles\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nrobots display alignment\\nformation control\\ncoordinated escape\\nlocal decision making\\nrobotic platform\\ncollective exploration\\nself-organized evasive fountain maneuvers\\nanimal species\\nleverage vital behaviors\\npredator evasion\\nrobotics\\nengineered multiagent systems\\nadaptable autonomy\\ndecentralized coordination\\ndynamic coordination\\npartial assistance\\ncentral controllers\\nexternal tracking\\nunderwater robot collective\\nspatiotemporal coordination\\nfish-inspired evasive maneuvers\\nbioinspired underwater robot collective\\nreal-time on-board multirobot tracking\",\"500\":\"computer vision\\nforce measurement\\ncomputational modeling\\nconferences\\nestimation\\nmanipulators\\ntrajectory\\nestimation theory\\nimage motion analysis\\nobject detection\\nrobot vision\\ntrajectory control\\nmass estimation\\nmoving object\\ndynamic interaction\\ninertial parameters\\nminimal interaction approach\\ntrajectory estimation\\nmodel-based estimator\\nthree-dimensional problem\\nlinear trajectory\\nmanipulation interaction\\nkuka iiwa 7\\nobject trajectory estimates\\nforce measurements\\ncontact point\",\"501\":\"geometry\\nvisualization\\nthree-dimensional displays\\nshape\\nshape measurement\\noptical variables measurement\\nrobot sensing systems\\ndexterous manipulators\\nimage reconstruction\\nimage resolution\\nmanipulator dynamics\\npose estimation\\nrobot vision\\nstereo image processing\\ntactile sensors\\ncompact shape\\nhigh-resolution 3d reconstruction\\nhuman finger\\n3d geometry reconstruction\\ncompact robot finger\\nvision-based tactile sensors\\ncontact geometry\\nvisual occlusion\\noptical constraint\\nmechanical constraint\\ngelsight wedge sensor\\nhigh-resolution 3d contact geometry\\npose tracking\\n3d space\",\"502\":\"robot motion\\ntorque\\nstatistical analysis\\nestimation\\narms\\nrobot sensing systems\\nmanipulators\\ngradient methods\\nmanipulator dynamics\\nmobile robots\\nmotion control\\nnonlinear programming\\nposition control\\ntorque measurement\\nexternal contacts\\nserial robotic arms\\ntorque sensors\\nindividual joints\\ndistinct contacts\\nidentical joint torque measurements\\ncontact positions\\nnonlinear program\\nactive contact exploration method\\nwhole-body contact estimation\",\"503\":\"automation\\nconferences\\ngrasping\\nplanning\\ntrajectory\\ntask analysis\\ndexterous manipulators\\nmotion control\\npath planning\\ndexterous motions\\nmotion trajectories\\ngrasping pushing\\nquasistatic dexterous manipulation\\ncontact mode guided sampling-based planning\",\"504\":\"geometry\\nshape\\nsemantics\\npipelines\\npose estimation\\nrobot sensing systems\\nhardware\\nimage representation\\nlearning (artificial intelligence)\\nmanipulators\\nobject recognition\\npath planning\\nobject templates\\nintra-category shape variation\\ncategory-level manipulation\\nsemantic keypoints\\ndense geometry\\nperception module\\nmotion planner\\nlearning-based keypoint detection\\nshape completion\\nhybrid object representation\\nobject target configuration\\nmanipulation planners\\nperception-to-action manipulation pipeline\\nmanipulation planning\\nkpam-sc\",\"505\":\"conferences\\nmanipulators\\nreal-time systems\\nplanning\\nmanufacturing\\ncomplexity theory\\ntime factors\\ncollision avoidance\\nalternative paths planner\\nfixed-time manipulation planning\\nsemistructured environments\\nrobot manipulators\\nmovable obstacles\\nmanipulation tasks\\nreliable motion planning capabilities\\nstrict time constraints\\nfixed-time planning guarantees\\ncollision-free path\\n7 dof robot arm\\nsemistructured domains\\nrobotic manipulator\\npreprocessing-based approaches\",\"506\":\"visualization\\nautomation\\nconferences\\ncomputational modeling\\npredictive models\\ngraph neural networks\\nplanning\\ngeometry\\ngraph theory\\nintelligent robots\\nmanipulators\\nmobile robots\\nneural nets\\npath planning\\nsearch problems\\nsearch-based task-and-motion planner\\nlong-horizon manipulation tasks\\nvisually grounded hierarchical planning algorithm\\nkitchen storage task\\nhigh-level task plans\\nmanipulation scenes\\nhierarchical representation\\nsymbolic scene graph\\ngeometric scene graph\\ntwo-level scene graph representation\\nlow-level motion generation\\nneuro-symbolic task planning\",\"507\":\"location awareness\\ntorque\\nconvolution\\nneural networks\\nrobot sensing systems\\nmanipulators\\ndata models\\nmechanical contact\\nmotion control\\nneural nets\\nrobot vision\\ntorque control\\nrobot manipulators\\ncontact-rich tasks\\nrobot skins\\njoint velocities\\ncontact incident\\nsingle contact\\njoint torque sensing\\njoint torque observations\\nrobot arm surface\\ncontact detection accuracy\\nmean contact localization error\\ncontact localization model\",\"508\":\"geometry\\ntorque\\nconferences\\ncomputational modeling\\nforce\\nhumanoid robots\\ngrasping\\nforce control\\ngrippers\\noptimisation\\npose estimation\\ntorque control\\ncomplementarity constraints\\nsemiinfinite programming\\ncontinuous geometry\\ngrasping pose optimization\\npervasive contact\\nsipcc\\ngripper\\nhumanoid robot\\ntorque balance\",\"509\":\"service robots\\nscalability\\ncollaboration\\ngames\\ntransforms\\ntools\\nprobabilistic logic\\ngame theory\\nhuman-robot interaction\\nmanipulators\\nmarkov processes\\nmobile robots\\npath planning\\ntemporal logic\\nindustrial settings\\nsocial settings\\n2-player deterministic game\\nhuman moves\\nsettings determinism\\nnovel planning method\\nhuman-robot manipulation tasks\\nprobabilistic synthesis\\nprobabilistic manipulation domain\\nhuman actions\\nphysical ur5 robot\\nfinite-horizon synthesis\",\"510\":\"three-dimensional displays\\nautomation\\ntoy manufacturing industry\\nreinforcement learning\\nbenchmark testing\\nrendering (computer graphics)\\nrobot learning\\nfurniture\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nikea furniture assembly environment\\nlong-horizon complex manipulation tasks\\nhierarchical manipulation tasks\\nimitation learning\\nsimple toy tasks\\nlong-term planning\\nsophisticated low-level control\\n60 furniture models\\nlong horizon\\nsophisticated manipulation requirements\",\"511\":\"visualization\\naffordances\\nconferences\\nbiological system modeling\\nmorphology\\ngrasping\\nreinforcement learning\\ndeep learning (artificial intelligence)\\ndexterous manipulators\\nrobot vision\\nstability\\ndexterous grasping\\nobject-centric visual affordances\\ndexterous robotic hands\\nhuman-like morphology\\nobject-centric visual affordance model\\ndeep reinforcement learning loop\\ngrasping policies\\nobject regions\\nhuman demonstration trajectories\\nhand joint sequences\\nuseful affordance regions\\npolicy learning\\nrobotic hand simulator\\nstable functional grasps\\naffordance-guided policies\\nmanipulation agents\\nhuman body\",\"512\":\"deep learning\\nvisualization\\nthree-dimensional displays\\nautomation\\nconferences\\ncollaboration\\ngrasping\\ndeep learning (artificial intelligence)\\nflexible manipulators\\ngrippers\\nmanipulator dynamics\\nmotion control\\nrobot vision\\ncollaborative pushing\\ngrasping policies\\ndense clutter\\nflexible manipulation\\ncluttered environments\\nbin-picking\\njoint planar pushing\\n6-degree-of-freedom\\n6-dof\\ndeep neural networks\\n3d visual observations\\nq-learning framework\\ncollaborative pushes\\naction space\\ncluttered scenes\\naction efficiency\\ngrasp success rate\",\"513\":\"automation\\nconferences\\ngrasping\\ntools\\nmanipulators\\ngrippers\\nlearning (artificial intelligence)\\nrobot vision\\nmodel-free imitation learning\\nfine manipulation\\nsmall tips\\ncurved tips\\nslippery tips\\nsuitably complex test case\\npaper leverages human demonstrations\\nautonomous chopsticks-equipped\\ncovariate shift phenomenon\\nchopstick-equipped robot\\nversatile tool\",\"514\":\"automation\\nservice robots\\nnetwork topology\\naffordances\\ngrasping\\nreinforcement learning\\nprediction algorithms\\ncontrol engineering computing\\ndexterous manipulators\\ngrippers\\nlearning (artificial intelligence)\\ntask-oriented dexterous manipulation\\ngrasp context\\nreinforcement-learning based grasping policy\\ntask-oriented strategies\\nsimulated grasping environment\\ngrasping experiments\\nmanipulation tasks\\ntask-oriented dexterous grasping\\nhuman knowledge\\nrobot dexterity\\nexisting robotic systems\\nproper grasp strategies\\ntask designations\\ngrasp knowledge\\ngrasp topology\\ndexterous grasping\\ntask-oriented grasping\",\"515\":\"learning systems\\ndeep learning\\nuncertainty\\nlogic gates\\nhardware\\nrobustness\\nfeedback linearization\\nadaptive control\\naircraft control\\nattitude control\\ncontrol system synthesis\\ndelays\\nfeedback\\nlinearisation techniques\\nnonlinear control systems\\nposition control\\nrobust control\\nstability\\nposition controller\\nattitude controller\\nstandard feedback linearization controller\\nthrust control input delay model\\nlearned acceleration error model\\nthrust input delay mitigation model\\nknown system model\",\"516\":\"road transportation\\nadaptation models\\nsolid modeling\\nlaser radar\\nthree-dimensional displays\\nobject detection\\nrobot sensing systems\\ncomputer vision\\ndecision making\\nimage representation\\nimage resolution\\noptical radar\\nvideo cameras\\nobject representation\\ncirrus\\nlidar point clouds\\nlidar model adaptation\\nlong-range bi-pattern lidar dataset\\nautonomous driving tasks\\n3d object detection\\nhighway driving\\ntimely decision making\\nhigh-resolution video camera\\nlidar sensors\\npublic datasets\\nrecord paired point clouds\\nuniform scanning patterns\\npoint density\\ndistance 250 m\",\"517\":\"dead reckoning\\nthree-dimensional displays\\npropellers\\nwind speed\\nrobot sensing systems\\nsensors\\nbiosensors\\ndistance measurement\\nkalman filters\\nmobile robots\\nremotely operated vehicles\\nstate estimation\\nlow-cost imu\\n3d airflow sensing\\ndeep-learning strategy\\nbio-inspired sensors\\nposition-dependent wind\\nvelocity estimation\\nimu-only dead reckoning\\nposition sensor failure\\nairflow-inertial odometry\\nresilient state estimation\\ndead reckoning strategy\\nmultirotors\\nbio-inspired airflow sensors\\ntime 30.0 s\",\"518\":\"laser radar\\nsimultaneous localization and mapping\\nsmoothing methods\\nconferences\\npose estimation\\nreal-time systems\\nindoor environment\\ncomputational complexity\\nindoor navigation\\noptical radar\\nslam (robots)\\ntrees (mathematics)\\nreal-time dense planar lidar slam system\\nbundle adjustment\\nlow fidelity tracking\\nplane adjustment\\nkeyframe poses\\n\\u03c0-factor\\nefficient loop detection algorithm\\nransac framework\\nglobal registration\\npoint-to-plane correspondences scan\\nlidar scan\\nglobal planes\\nglobal kd-tree\\nlocal-to-global data association\\n\\u03c0-lsam\",\"519\":\"visualization\\nlaser radar\\nimage recognition\\nthree-dimensional displays\\nautomation\\ndatabases\\nconferences\\nfeature extraction\\nimage coding\\nimage matching\\nimage resolution\\noptical radar\\nvisual feature descriptors\\nimaging lidar\\nimage-quality high-resolution 3d point clouds\\npoint cloud\\norb feature descriptors\\nbag-of-words vector\\nplace recognition queries\\n2d image space\\nransac\\nvisual features positions reprojection error\\ndbow\",\"520\":\"automation\\nnavigation\\nconferences\\nneural networks\\naerospace electronics\\nrobot sensing systems\\nprediction algorithms\\ncollision avoidance\\ncontrol engineering computing\\nimage colour analysis\\nimage sensors\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nrobot vision\\nrobust planning\\ngenerative neural network\\nhuman annotated labels\\npredicted spaces\\ncollision-free planning\\nphysical robot\\nrgbd sensor\\npredicted regions\\nhigh-speed robot navigation\\nrobotic systems\\ncomputational bottleneck\\nexplicit mapping\\nalgorithmic approaches\\nsensor horizon\\npredicted occupancy maps\\nfield of view\\nfov\\nmit race car\\nvelocity 4.0 m\\/s\",\"521\":\"legged locomotion\\nautomation\\natmospheric modeling\\nfluid dynamics\\nconferences\\ndynamics\\nland surface\\nactive disturbance rejection control\\nmarine robots\\nmotion control\\npendulums\\nreduced order systems\\nrobot dynamics\\nsprings (mechanical)\\nspring-loaded inverted pendulum model\\namphibious regimes\\naquatic regimes\\ndog-paddle type gaits\\nair-water interface\\nleg stroke frequency\\nfluid field slip model\\nterrestrial-aquatic dynamic legged locomotion\\nsingle reduced-order dynamic model\\nfluid forces\\nmotion pattern stability\\nrapid disturbance rejection\",\"522\":\"legged locomotion\\nadaptation models\\nautomation\\nconferences\\nreinforcement learning\\nrobustness\\ndynamics randomization\\nquadrupedal locomotion\\nlegged robots\\nrobust locomotion policies\\nrobot model\\nsim-to-real transfer\\non-robot adaptation schemes\\nablation\\nsim-to-sim setting\\npolicy transfer\\npolicy robustness\\nlaikago quadruped robot\",\"523\":\"training\\nlearning systems\\nautomation\\nmultimodal sensors\\nconferences\\nwriting\\nrobot sensing systems\\nlearning (artificial intelligence)\\nstudent experiments\\ntask analysis\\nexplicit reward engineering\\nphysical manipulation skills\\nlearning multimodal contact-rich skills\\nprecise task execution\\ntask-specific reward functions\\nsawyer robot\\nmultimodal sensor data representation\\nmodel-free learning-from-demonstration framework\",\"524\":\"learning systems\\nshape\\nimage color analysis\\nfriction\\nmanuals\\nhardware\\nend effectors\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\nimage processing\\nmobile robots\\nsupervised learning\\nrobotic manipulation system\\nrobotic hardware system\\ncomplex interactions\\nrobot end-effector pushes multiple objects\\ndipn imagines\\npush action\\npredicted outcome\\ngrasp network\\ndeep interaction prediction network\\nclutter removal tasks\\nphysical properties\\nfriction coefficients\\nsynthetic image\\nself-supervised manner\",\"525\":\"geometry\\nthree-dimensional displays\\nautomation\\nconferences\\nfires\\nrobot sensing systems\\nmanipulators\\nlearning (artificial intelligence)\\nmobile robots\\noctrees\\npath planning\\nrobots\\nsampling methods\\ncomplex manipulators\\ncombine samplers\\nworkspace decomposition\\nglobal biased sampling distribution\\nspark\\nflame\\nreal simulated fetch robot\\n-place manipulation problems\\ndiverse tasks\\nprior approaches\\nsampling distributions\\nlocal 3d workspace decompositions\\nreusing experience\\nprior motion planning problems\\nsimilar motion planning queries\\nfuture motion planning queries\\ndegrees-of-freedom\\nexperience-based frameworks\\nsampling-based\",\"526\":\"costs\\nimage coding\\nautomation\\nconferences\\nneural networks\\nplanning\\ntask analysis\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nplanning (artificial intelligence)\\nprobability\\ntemporal logic\\npartially revealed environments\\nhigh-level actions\\ntrained neural network\\nestimates guide search\\nminimum-expected-cost plan\\ntask specifications\\ntemporally extended tasks\\nnovel planning technique\\nsatisfying tasks\",\"527\":\"learning systems\\ndirected acyclic graph\\nmonte carlo methods\\nmanuals\\nsimulated annealing\\nsearch problems\\nplanning\\ncomputer games\\ndirected graphs\\nlearning (artificial intelligence)\\npath planning\\ntree searching\\ntrees (mathematics)\\nabstract task selection problem\\nlearned behavior trees\\noutperform baseline learning methods\\nrobotic task planning\\ntree learning\\nmonte carlo dag search\\nformal grammar\\ninfeasible manual design\\nsearch space\\nmonte carlo tree search\\nmarine target search\\nresponse scenario\",\"528\":\"costs\\nautomation\\nconferences\\npredictive models\\nprediction algorithms\\npath planning\\nland vehicles\\ncollision avoidance\\noff-road vehicles\\nhybrid a* algorithm\\npath planning algorithm\\noff-road environments\\nautonomous ground vehicles\\nphysical interactions\\noff-road planning\\nphysical platform\",\"529\":\"training\\nautomation\\nconferences\\ndynamics\\nforce feedback\\nhumanoid robots\\nmanipulators\\ncontrol engineering computing\\nend effectors\\nmobile robots\\nmotion control\\ntelerobotics\\ndynamic teleoperation\\nanthropomorphic robotic arm\\nhuman-level dynamic motion capabilities\\nhuman-level dynamic performances\\nhuman subjects\\nteleoperation proficiencies\\njoint space mapping\\nthree-dimensional task space mapping\\nteleoperation training simulator\\nhuman pilot\\nreaction tests\\nhuman motion-proves\\nmap human motion\\nhumanoid robot matters\",\"530\":\"training\\nautomation\\ncomputational modeling\\nconferences\\npredictive models\\nprobabilistic logic\\ntrajectory\\nbehavioural sciences computing\\nmobile robots\\nmulti-agent systems\\nquery processing\\ntraffic information systems\\ntrajectory control\\nidentifying driver interactions\\nconditional behavior prediction\\ninteractive driving scenarios\\nlane changes\\nunprotected turns\\nchallenging situations\\nautonomous driving\\nfuture actions\\nend-to-end models\\nquery future trajectory\\nego-agent\\nfuture trajectories\\ngeneral-purpose agent interactivity score\\ninteresting interactive scenarios\\nevaluating behavior prediction models\\nagent prioritization\",\"531\":\"analytical models\\ntracking\\nrobot control\\npredictive models\\nturning\\nrobot localization\\nparticle filters\\nimage motion analysis\\nmobile robots\\nmotion control\\nparticle filtering (numerical methods)\\npose estimation\\ntrajectory control\\nautonomous robotic escort incorporating motion prediction\\nconventional motion prediction\\nhuman head\\nnongaussian human trajectory\\nhuman motion\\nrobot control action\\nefficient autonomous escorting\\nhuman intention model\\nhuman position prediction error\\nomnidirectional mobile robotic platform\",\"532\":\"sequential analysis\\nautomation\\nconferences\\npredictive models\\ntask analysis\\nrobots\\nassembling\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nrobotic assembly\\nstage clustering\\nhuman preferences\\naction prediction\\nassembly tasks\\nhuman workers\\ntask actions\\ndominant preferences\\nsimple tasks\\nhigh-level preference\\nindividual actions\\ntwo-stage approach\\nhuman operators\\nikea assembly study\\ncomplex task\\nhuman actions\\ntask execution\\nreal-world robot-assisted ikea assembly\",\"533\":\"automation\\ncomputational modeling\\nconferences\\nswitches\\npredictive models\\nperformance gain\\nplanning\\ncollision avoidance\\nmobile robots\\noptimisation\\npath planning\\nresulting robot plan\\nhuman model\\ndynamically switching human prediction models\\nfuture human actions\",\"534\":\"automation\\nrobot kinematics\\nconferences\\ncollaboration\\nrobot sensing systems\\nreal-time systems\\ntask analysis\\nhuman-robot interaction\\nmobile robots\\nmulti-robot systems\\nreal-time adaptation\\nneurobiologically-inspired algorithms\\ntandem\\nexternal changes\\ninternal temporal changes\\nhuman-level performance\\nteam dynamics\\nfluent human-robot teaming\\ntemporal anticipation and adaptation for machines\\nhuman-robot collaborative drumming task\\ntempo-changing rhythmic conditions\",\"535\":\"uncertainty\\ntracking\\nnavigation\\nrobot kinematics\\ntools\\nrobot sensing systems\\nrobustness\\nbayes methods\\nmulti-robot systems\\npath planning\\npedestrians\\nstochastic games\\nrobust planning\\nhuman-like behavior\\ntravel efficiency\\ncompliant coordinating motions\\nhuman groups\\nsubgoal uncertainty\\nplan evaluation process\\nbad transient states\\ninefficient planning\\ncoordinating motion\\npath coordination\\npartner tracking robustness\\ninference update process\\nrobot behavior design\\nlost tracking\\nbayesian stochastic game\\npedestrian simulation literature\",\"536\":\"automation\\nconferences\\ndecision making\\nbuildings\\ntraining data\\nreinforcement learning\\nplanning\\nhuman-robot interaction\\nmarkov processes\\nplanning (artificial intelligence)\\nrobot programming\\ngenerating progressive explanations\\nplanning tasks\\nhuman-robot teaming\\nai agent decision making\\ngoal-based mdp\\nexplainable robots\\ncognitive load\",\"537\":\"costs\\nconferences\\nprocess control\\nprediction methods\\nmarkov processes\\nmanipulators\\nreal-time systems\\ndexterous manipulators\\nlearning (artificial intelligence)\\npath planning\\nreal-time user goal prediction\\nshared assistive control\\nreal-time user control intention\\nreal-time goal prediction method\\nreactive assistive behaviors\\nlfd-generated assistive motion\\ngoal predictions\\nlfd policy\\npartially observable markov decision process\\n6-dof kinova mico robotic arm\\ncontrol methods\\ntask completion time\\njoystick control inputs\\npomdp-based method\\nuser preference\\ncontrol feeling\\ndistance cost\",\"538\":\"training\\nservice robots\\npipelines\\nlaboratories\\nreinforcement learning\\nrobot sensing systems\\nend effectors\\nagricultural robots\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\nindustrial manipulators\\nmanipulator kinematics\\npath planning\\nrobot vision\\ncut-point locations\\nreaching accuracy\\npruning locations\\ndeep reinforcement learning policy\\n7 dof robot\\ngrapevine canopy\\n6 dof industrial robot arm\\ngrape vine\\nconvolutional neural networks\\ninverse kinematics solver\\nvision system\\nagricultural manipulation\",\"539\":\"solid modeling\\nvisualization\\nthree-dimensional displays\\ntwo dimensional displays\\npipelines\\nsemantics\\npredictive models\\nrealistic images\\nrendering (computer graphics)\\ntelerobotics\\ngenerative model-based predictive display\\nrobotic teleoperation\\nhigh-latency communication links\\nphoto-realistic images\\nhuman operator\\nrgb-d images\\nremote robot\",\"540\":\"support vector machines\\nlegged locomotion\\nconferences\\nmachine learning\\nreal-time systems\\nfeedforward neural networks\\ntask analysis\\nfeedforward neural nets\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nrobot walks\\nautomatic robot joke success assessment\\nsocial robots\\nautonomous robots\\nrobotic systems\\nhumorous statements\\nrobotic comedian\\nindividual human rater accuracy\\nreal-time joke assessment\\nml approaches\\nsingle-hidden-layer feedforward neural networks\\nbaseline approach\\nself-assessment techniques\\nsocial robotics researchers\\nhuman crowds\\nhuman-labeled joke success ground truths\\nlightweight machine learning approaches\",\"541\":\"training\\nvisualization\\ntarget tracking\\ndatabases\\necosystems\\nwater quality\\nobject detection\\naquaculture\\nautomatic optical inspection\\necology\\nfishing industry\\nremotely operated vehicles\\nvideo recording\\nchesapeake bay living ecosystem\\nbay's water quality\\noyster restoration programs\\nchesapeake bay bottom\",\"542\":\"photomultipliers\\niodine compounds\\nthree-dimensional displays\\nscintillators\\nuranium\\nradio navigation\\nrobot sensing systems\\naerospace robotics\\ngamma-ray detection\\nglobal positioning system\\nmicrorobots\\nmobile robots\\nnuclear power stations\\npath planning\\nradiation detection\\nrobot vision\\nscintillation counters\\nsilicon radiation detectors\\nsolid scintillation detectors\\nthallium\\nautonomous distributed 3d radiation field estimation\\nnuclear environment characterization\\nautonomous distributed 3d nuclear radiation field mapping\\nsingle radiation sensor\\nradiation measurements\\ndiscretized 3d grid\\nradiation gradient\\ncuriosity-driven path planner\\nradiologically informative point\\nautonomous gps-denied navigation\\nresilient microflying robot\\nthallium-doped cesium iodide\\nsilicon photomultiplier\\ncustom-built pulse counting circuitry\",\"543\":\"robot motion\\nmotor drives\\nservice robots\\ntracking\\nfriction\\nrobot vision systems\\npredictive models\\naerodynamics\\ncameras\\ncontrol engineering computing\\nfeedback\\nfeedforward\\nhumanoid robots\\nlegged locomotion\\nmotion control\\nposition control\\nrobot kinematics\\nservomotors\\nfeedforward motion generation\\nfeedback controller\\nrobot hardware\\nlive feedback\\nfriction-driven tripedal robot\\nnovel omnidirectional gait design\\nradially symmetric tripedal friction-driven robot\\nservo motors\\nimpart frictional reactive forces\\nmathematical model\\nuniform friction surface\\n3-d printed chassis\\nproportional-integral feedback control framework\",\"544\":\"analytical models\\nvisualization\\ntechnological innovation\\nmonte carlo methods\\nlung\\nneedles\\ntrajectory\\nbiological tissues\\nmedical image processing\\nmedical robotics\\npath planning\\nsurgery\\ntrees (mathematics)\\nmaximize reachable lung volume\\nsteerable needles\\ncurvilinear trajectories\\nanatomical obstacles\\ninterventional procedures\\nconventional bronchoscope\\nreach lung lesions\\ndesign parameters\\nbronchoscope diameter\\npiercing device\\nmedial axis\\nfuture design choices\\nrobot parameters\\nmonte carlo random sampling\\nrapidly-exploring random trees\\nsteerable needle motion planner\\nsimulated human lung environments\\nlung reachable\\nfuture device innovation\\ndesign trade-offs\\ndesign considerations\\nsteerable needle robot\",\"545\":\"robot kinematics\\nconferences\\ndynamics\\nkinematics\\nfasteners\\ncontrol systems\\nsoftware\\npath planning\\nposition control\\nrobot dynamics\\nnth order analytical time derivatives\\ninverse dynamics\\nrigid body dynamics\\nrobotics community\\nrobotic systems\\nrobot control\\nmultibody systems\\nelastic components\\nsmooth trajectories\\neom\\norder time derivatives\\nclosed forms\\nrecursive forms\",\"546\":\"trajectory tracking\\nrobot control\\nlimit-cycles\\nmachine learning\\nmice\\ndata models\\nrobustness\\nlearning (artificial intelligence)\\nmanipulator dynamics\\ninverse dynamics acquisition\\nprecise robot control\\nnonlinearities\\njoint friction\\ninput space dimension\\nserial robotic manipulator\\nsupervised machine learning\\ninformation-rich data\\nexcitation trajectories\\nrobot joints\\ndata quality\\nlearning accuracy\\ndata collection\\nmodel learning\\nkuka iiwa14 robotic arm\\nmax-information configuration exploration\\nconfiguration exploration\",\"547\":\"training\\nautomation\\nconvolution\\nfuses\\nconferences\\nvideo sequences\\nneural networks\\ncomputer vision\\ndistance measurement\\nfeature extraction\\nimage colour analysis\\nimage fusion\\nimage motion analysis\\nimage sequences\\nlearning (artificial intelligence)\\nmean square error methods\\nstereo image processing\\nself-supervised monocular depth completion\\nchallenging indoor environments\\nsparse depth measurements\\ndiverse depth ranges\\nsparse convolution blocks\\nselfdeco\\nneural network training\\nmonocular video sequences\\ntextureless regions\\ntransparent surfaces\\nglossy surfaces\\ncomplex ego-motion\\nsparse depth feature extraction\\npixel-adaptive convolution\\nimage-depth feature fusion\\nnyuv2 indoor dataset\\nkitti indoor dataset\\nnaverlabs indoor dataset\\nroot-mean-square error reduction\\nrmse\",\"548\":\"training\\nanalytical models\\nautomation\\nconferences\\nreinforcement learning\\nrobot sensing systems\\nhardware\\ncontrol engineering computing\\nrobots\\nsensors\\ncorrupted sensors\\nsensor data\\nmultiple modalities\\nproprioceptive feedback\\nvisually-challenging environments\\ncrossmodal compensation model\\nccm\\nsensor modalities\\nunimodal reconstruction loss\\ncorruption detection\\ncorrupted modality\",\"549\":\"robotic assembly\\ninstruments\\nwelding\\nposition measurement\\nnist\\nrobot sensing systems\\nthree-dimensional printing\\ninspection\\nmachining\\nmobile robots\\nmotion control\\nposition control\\nrobot vision\\nsensors\\nuniversal robot\\nadvanced sensing development\\nautomated movements\\nrobot applications\\nrobot machining\\nrobot assembly\\nrobot 3d printing\\nrobot inspection\\nconventional robot part\\nadvanced sensor\\nrobot systems\\nsmart target\\nvision-based measurement instrument\\nhigh accuracy measurements\\nmoving robot arm\\naccuracy degradation assessment\\nrobot accuracy assessment\\nnational institute of standards and technology\",\"550\":\"radio frequency\\ntechnological innovation\\nprototypes\\nreinforcement learning\\nsearch problems\\nrobot sensing systems\\ntrajectory\\ncameras\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nobject detection\\nobject tracking\\nradiofrequency identification\\nrobot vision\\nvisual servoing\\nrobotic grasping\\nfully-occluded objects\\nrf perception\\nrf-grasp\\nrf-visual servoing controller\\nrf-visual deep reinforcement learning network\\ndecluttering grasping\",\"551\":\"deformable models\\nindustries\\nautomation\\ngrasping\\nreal-time systems\\nplanning\\ntask analysis\\ncomposite materials\\ncomputer simulation\\ncontrol engineering computing\\nindustrial manipulators\\nmoulding\\npath planning\\nproduction engineering computing\\nsheet materials\\nautomated generation\\ncomposite sheet layup\\nreal-time sheet tracking system\\nsimulation-based grasp planner\\nrobotic grasping\\ncomposite parts\\nautomotive industries\\naerospace industries\\ncomplex molds\\nsheet grasping\\ndraping\\ngrasping task automation\\nrobot grasp locations\\nintervention controller\\nplan execution\",\"552\":\"uncertainty\\ntarget tracking\\nshape\\ncomputational modeling\\natmospheric modeling\\nmeasurement uncertainty\\nunmanned aerial vehicles\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\ncollision avoidance\\nfeedback\\nlinear systems\\nlinearisation techniques\\npredictive control\\nrobot dynamics\\nvectors\\nvehicle dynamics\\ncollision-free vector field guidance\\nfixed-wing uav\\npath controller\\nfixed-wing unmanned aerial vehicle\\ndynamic obstacle avoidance\\nlower level layer\\nguidance vector field\\nlinear model predictive control\\nlinear mpc\\naircraft\\nfeedback linearization\\ntop level layer\",\"553\":\"sensor phenomena and characterization\\nrobot sensing systems\\ndata models\\nunmanned aerial vehicles\\nstability analysis\\nshock absorbers\\ntrajectory\\naerospace control\\naircraft control\\nautonomous aerial vehicles\\ncollision avoidance\\nhelicopters\\nmobile robots\\nstability\\ntoward impact-resilient quadrotor design\\ncollision characterization\\nsustain flight\\naerial robots\\nlocal stability\\nflight controller\\nnovel collision-resilient quadrotor\\ncompliant arm design\\nfree flight\\nnovel collision detection\\ncharacterization method\\nhall sensors\\nrecovery control method\\nhigh-speed collisions\",\"554\":\"three-dimensional displays\\nconferences\\nprototypes\\naircraft navigation\\nunmanned aerial vehicles\\ntopology\\nprinters\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\nhelicopters\\noptimisation\\nsoft hybrid aerial vehicle\\nbistable mechanism\\nsampling tasks\\nfixed wing aircraft\\nhavs\\nmultiple modes\\nmorphing hav\\nextra actuators\\nfolding wing\\nfixed wing mode\\nextra actuation\\nquadrotor agility\\nactuators\\nfolding wings\\ntopology optimization approach\\n3d printer\",\"555\":\"limiting\\nautomation\\npropellers\\nconferences\\npropulsion\\ntask analysis\\nrobots\\nadaptive control\\nautonomous aerial vehicles\\ncontrollability\\nhelicopters\\nmotion control\\nuav controllability\\ncontrollable degrees of freedom\\ntilted propellers\\nquadrotor adaptability\\n5 controllable dof\\n4 controllable dof\\n6 controllable dof\\nmodular multirotors\\nh-modquad\\ncuboid modules\\nheterogeneous modules\\nmodular robotic system\\nunmanned aerial vehicles\",\"556\":\"adaptation models\\nuncertainty\\nperturbation methods\\ndrag\\nforce\\ndynamics\\nstability analysis\\nadaptive control\\naerospace robotics\\ncables (mechanical)\\ncontrol system synthesis\\ndamping\\ngraph theory\\nlyapunov methods\\nmobile robots\\nmodel reference adaptive control systems\\nmulti-robot systems\\nnonlinear control systems\\nposition control\\nrobot dynamics\\nrobust control\\nstability\\nsynchronisation\\nrobust adaptive synchronization\\ninterconnected heterogeneous quadrotors transporting\\ncable-suspended load\\nmultiple quadrotors\\ncable-suspended point-mass load\\nquadrotor\\nvirtual leader-follower algorithm\\nmultilayer graph\\nphysical interaction\\nreference trajectory\\ndistributed tension force\\nwell-known spring-damping system\\ninterconnected dynamic\\nneglectable mass\\nmodel reference adaptive control approach\\nrobust modification\\nlyapunov approach\",\"557\":\"adaptation models\\nneural networks\\nreinforcement learning\\nsearch problems\\ntrajectory\\nsafety\\ntask analysis\\nfault tolerant computing\\nmarkov processes\\nremotely operated vehicles\\ntraffic engineering computing\\nadaptive failure search\\nsafety critical systems\\nautonomous vehicles\\nsubstantial vehicle miles\\nfailure events\\nnaive random search approaches\\nvehicle operation hours\\nadaptive searching techniques\\nadaptive stress testing\\nast method\\nmarkov decision process\\nreinforcement learning techniques\\nprobability model\",\"558\":\"training\\nheuristic algorithms\\nconferences\\nkinematics\\nreinforcement learning\\nlength measurement\\nhardware\\nbayes methods\\nlegged locomotion\\nreinforcement learning policies\\nphysics simulation\\nsim-to-real gap\\ndynamics discrepancies\\ntarget domains\\nrandomized parameter selection\\npolicy transferability\\ndomain discrepancies\\ndynamic parameters\\ndynamic randomization\\ndomain adaptation algorithm\\nkinematic parameters variation\\nmultipolicy bayesian optimization\\nuniversal policies\\nvirtual kinematic parameters\\ntarget domain rollouts\\nsimulated quadruped robot\\npolicy transfer\\nkinematic domain randomization\\ntarget environments\",\"559\":\"adaptation models\\nsufficient conditions\\nanalytical models\\nactuators\\nadaptive systems\\nbuoyancy\\nobservability\\nadaptive control\\ncontrol system synthesis\\nmobile robots\\nmultidimensional systems\\nnonlinear control systems\\nnonlinear dynamical systems\\nobservers\\nparameter estimation\\nposition control\\nstability\\nuncertain systems\\nstable model-based direct adaptive trajectory-tracking control\\nadaptive parameter estimates\\nplant parameter values\\nuniform complete observability\\nplant inertial parameters\\nstable adaptive identifier\\nuco condition\\nfully-actuated plants\\nunderactuated plants\\nrigid body plant models\\nrotational inertial parameters\\nadaptive identification\\nrigid-body plant dynamics\\ninertia plant parameters\\ndegree-of-freedom rigid-body dynamical systems\",\"560\":\"runtime\\nautomation\\nconferences\\nrobot sensing systems\\nplanning\\nsensors\\ntask analysis\\nformal specification\\nformal verification\\nmobile robots\\nplanning (artificial intelligence)\\nprogram verification\\ntemporal logic\\ntemporal reasoning\\nremediation opportunities\\ndescribing assumptions\\ntemporal data\\nquantitative data\\ncorrect monitors\\nplanning execution\\ntemporal planning\\nstream runtime verification\\nexecution time\\nassumption monitoring\\nuav temporal task plan executions\\nexplicit assumptions\\nimplicit assumptions\\nmonitoring assumption violations\\nflag silent failures\",\"561\":\"legged locomotion\\nnavigation\\ndecision making\\nprocess control\\nmedical services\\nmarkov processes\\ntask analysis\\ngradient methods\\nmobile robots\\npath planning\\nservice robots\\nstochastic processes\\nstochastic node transitions\\nreal household environment\\nrobot navigation task\\npomdp controller\\nformal mathematical description\\ngradient-based algorithm\\npartially observable markov decision processes\\ncirculant controllers\\nscalable pomdp decision-making\",\"562\":\"gradient methods\\nautomation\\nconferences\\ndynamics\\nmathematical models\\nstability analysis\\ntrajectory\\nconcave programming\\nintegration\\niterative methods\\nlegged locomotion\\nmarkov processes\\nnewton method\\nrobot dynamics\\nimplicit integration\\narticulated bodies\\nnonconvex maximal dissipation principle\\ntime integration scheme\\nsimultaneous contacts\\nscheme resolves contact forces\\nmdp solvers\\nforward multistep scheme\\ncoupled system\\nnonlinear newton-euler dynamics\\nbackward integration scheme\\nnmdp scheme\\ncontact models\",\"563\":\"automation\\nconferences\\ncloning\\nreinforcement learning\\ngenerative adversarial networks\\nmanipulators\\ndata models\\ncontrol engineering computing\\noptimisation\\nstate-space methods\\nsuboptimal control\\nrobotics systems\\nuninformed exploration\\ndata-efficiency\\nimitation learning\\npolicy learning\\nhigh-value areas\\naction space\\nhard constraints\\npolicy optimization\\nsuboptimal demonstrations\\nnoisy demonstrations\\nimperfect demonstrations\\nmodel-free reinforcement learning\\nreward function shaping\\nstate-and-action-dependent potential\\nstate space\\nfranka emika 7dof arm\",\"564\":\"automation\\nconferences\\nreinforcement learning\\ntask analysis\\nrobots\\nimage representation\\nmanipulators\\nrobot vision\\ngoal distributions\\ngeneral task representation\\noff-policy algorithm\\ndistribution-conditioned reinforcement learning\\ndisco rl\\nrobot manipulation tasks\\ngeneral-purpose policies\\ngoal-conditioned policies\\ncategorical contexts\\nstate-based reward function\",\"565\":\"manifolds\\ntraining\\nreinforcement learning\\ntransforms\\nlaser modes\\nlaser ablation\\nspace exploration\\nimage motion analysis\\nlearning (artificial intelligence)\\ntask space\\nlearned action space manifold\\nefficient reinforcement learning\\nincorrect action space\\ntask family\\nlatent manifold constraints\\neffective action space\\nentire action space\\nlearning problem\\naction space learning\\npolicy learning\\nsimilar manipulation task instances\\nmap raw actions\\ndisentangled latent action space\\naction reconstruction\\nlatent space dynamic consistency\\ncontact-rich robotic tasks\\ngenerated latent action space\\noriginal action space\",\"566\":\"three-dimensional displays\\nprotocols\\nfriction\\nbenchmark testing\\nrobot sensing systems\\nsearch problems\\nplanning\\ndexterous manipulators\\npath planning\\nextrinsic dexterity\\nthree-dimensional wihm\\ncontact regions\\ncontact friction\\nvariable friction robot fingers\\nrobotic hands\\nwithin-hand object translation\\nextrinsic contacts\\nwithin-hand object rotation\\nmodified a* formulation\\n3d within-hand-manipulation\\nmultiarm robotic systems\\nregion based planning algorithm\\nobject-motion primitives\",\"567\":\"torque\\nautomation\\nconferences\\nforce\\nfasteners\\ncognition\\nplanning\\nforce control\\nfriction\\nmanipulators\\nmobile robots\\npath planning\\nmultistage forceful manipulation tasks\\nnut\\ninterlocking constraints\\ndiscrete choices\\ncontinuous choices\\ndiscrete actions\\ncontinuous parameters\\nforce requirements\\nexecuting forceful manipulation\\nexisting task\\nmotion planner\\nexert wrenches\\nchoosing actions\",\"568\":\"actuators\\nautomation\\nsystem dynamics\\nconferences\\ndecentralized control\\ntools\\naerospace electronics\\ndexterous manipulators\\ndistributed control\\nmanipulator dynamics\\nrobust control\\ndelta-manipulator arrays\\ndistributed manipulator\\nplanar manipulation skills\\ndelta robots\\ndexterous manipulation\\ndynamic contact\\nstatic contact\\nhand-designing\\nhigh-dimensional action space\\nunfamiliar system dynamics\\nrobust cooperative control\\nrobust planar translations\\ndistributed control policies\\npolicy learning\",\"569\":\"adaptation models\\nlimiting\\nautomation\\nrefrigerators\\nconferences\\nsearch problems\\nplanning\\nhumanoid robots\\nmanipulators\\nmobile robots\\npath planning\\nrobot vision\\nrobot actions\\nrobot-object interactions\\nmultiheuristic search framework\\npr2 robot\\nphysics-based simulator\\nmanipulation planning\\nmovable obstacles\\nphysics-based adaptive motion primitives\\nrobot manipulation\\ncluttered scenes\\ncontact-rich interactions\\nnonprehensile actions\\ngrasp pose\\nnonprehensile interactions\\nmultibody interactions\\npybullet\\npick-and-place style tasks\",\"570\":\"training\\nthree-dimensional displays\\nshape\\ngrasping\\nnetwork architecture\\nplanning\\nproposals\\ncollision avoidance\\ndexterous manipulators\\nhumanoid robots\\nimage colour analysis\\nimage reconstruction\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nrobot vision\\nlearned grasp proposal network\\n3d shape reconstruction network\\nsingle rgb-d image\\ntarget object\\ngeometric reconstruction\\nknown objects\\nunknown objects\\nnetwork architectures\\nphysical robot environment\\nsuccess rate\\ngrasping task\\nrobotic grasping\\ncombined image-based grasp proposal\\n3d reconstruction\\nrobotic grasp planning\",\"571\":\"learning systems\\nadaptation models\\nvisualization\\nshape\\nimage color analysis\\ngrasping\\nlogic gates\\nadaptive control\\ndeep learning (artificial intelligence)\\ngrippers\\nimage colour analysis\\nmanipulators\\nobject recognition\\nrobot vision\\nshape recognition\\nsupervised learning\\ntext analysis\\nvisual attributes\\ntextual attributes\\ngrasp data\\nunknown objects\\nattribute-based robotic grasping\\nrobotic manipulation\\nobject attributes\\nend-to-end learning method\\nadaptation capability\\ngated-attention mechanism\\ninstance grasping\\nobject persistence\\nquery text\\none-grasp adaptation\\nobject grasping\\nworkspace image\\nobject color\\nobject shape\\ndeep neural networks\\nvisual-textual representation fusion\\ndeep learning in grasping and manipulation\\nperception for grasping and manipulation\",\"572\":\"deep learning\\nautomation\\nshape\\nconferences\\npipelines\\ngrasping\\nmanipulators\\ncollision avoidance\\ndexterous manipulators\\nneurocontrollers\\npath planning\\nprobability\\ncarp\\n6-dof grasping systems\\ncollision-free probabilities\\ngrasp poses\\nsurrounding structures\\n6-dof grasping rate\\nconstrained environments\\nintensive reasoning\\ntarget object grasping\\n6-dof robotic grasping systems\\nintensive planning computation\\ncollision-aware target-driven object grasping\\ncollision-aware reachability predictor\\ndeep neural networks\\ndeep learning in grasping and manipulation\\nperception for grasping and manipulation\",\"573\":\"training\\nshape\\nrobot vision systems\\nfeature extraction\\nrobustness\\ndata models\\nplanning\\ncameras\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\nobject detection\\nrobot vision\\nsynthetic grasp data\\ncollision-free grasps\\nsingle-view depth image\\n6-dof contrastive grasp proposal network\\nrobot manipulation task\\ncomplex object shape\\nobject information\\n6-dof grasps\\nimage encoder\\ninput depth image\\n3-dof grasp regions\\nrotated region proposal network\\nimage processing\",\"574\":\"costs\\nautomation\\nconferences\\ncomputational modeling\\ndecision making\\ngrasping\\nplanning\\ngreedy algorithms\\nmanipulators\\nmaterials handling\\npath planning\\nsearch problems\\nsorting\\nbilevel planning algorithm\\nhigh-level planner\\nlow-level push planner\\none-step greedy pushing actions\\nsemidiscrete search space\\nnear-optimal actions\\njoint push-grasp action space\\nobject sorting\\nmanipulation actions\\noverhead grasping\\nplanar pushing\\ngrasping action\\ncompleteness guarantee\\nacceleration strategy\\ndesktop pc\",\"575\":\"adaptation models\\nautomation\\naffordances\\nconferences\\nplanning\\ntask analysis\\nrobots\\ninference mechanisms\\nintelligent robots\\nknowledge representation\\nlearning systems\\npath planning\\nrobot kinematics\\ndeep affordance foresight\\nrealistic environment\\nlong horizon planning\\naffordance representation\\nlearning-to-plan method\\nrobot reasoning\\npartial environment model learning\\nparameterized motor skills\\nmultistep tasks\",\"576\":\"training\\nautomation\\nconferences\\ntactile sensors\\nreinforcement learning\\nuniversal serial bus\\ntask analysis\\nfeedback\\nmanipulators\\nsupervised learning\\ncontact-rich manipulation tasks\\nlearned reward function\\ndense reward functions\\nhigh-dimensional observations\\nhigh-dimensional reward\\ntactile feedback\\npeg-in-hole\\nusb insertion\",\"577\":\"automation\\nterminology\\nconferences\\ntools\\nplanning\\nphysics\\nrobots\\ncomputer simulation\\ncontrol engineering computing\\ngrippers\\nlearning (artificial intelligence)\\nmanipulators\\npath planning\\ngrasp performance\\nacronym dataset\\nlarge-scale grasp dataset\\nphysics simulation\\nparallel-jaw grasps\\nlearning based grasp planning algorithms\\nflex physics simulator\",\"578\":\"legged locomotion\\nautomation\\nnavigation\\nconferences\\ndynamics\\nreinforcement learning\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nmobile obstacles\\nnovel deep reinforcement learning based policy\\ndynamically feasible velocities\\nspatially aware velocities\\nrobot navigating\\ndynamic window approach\\ndynamics constraints\\nstate-of-the-art drl-based navigation methods\\npedestrians\\nenvironmental obstacles\\nnovel low-dimensional observation space\\nobstacle\\ndifferential drive robot\\nstate-of-the-art collision avoidance methods\\ndynamics constraint violations\\nobservation space formulation\\ndwa-rl\\ndynamically feasible deep reinforcement learning policy\\nrobot navigation\",\"579\":\"training\\nspace vehicles\\ncouplings\\nnavigation\\nsupervised learning\\nreinforcement learning\\ngraph neural networks\\ndeep learning (artificial intelligence)\\ngraph theory\\nmobile robots\\nlatent state inference\\ndeep reinforcement learning\\ncomplex autonomous driving scenarios\\nsubtle cues\\nopen problem\\nautonomous systems\\nhuman environments\\nencoding spatial-temporal relationships\\nreinforcement learning framework\\nencode prior knowledge\\nreinforcement learner\\nnavigating t-intersections\\nstate-of-the-art baseline approaches\\ndrl\",\"580\":\"location awareness\\nheuristic algorithms\\nsimulation\\npose estimation\\ncost function\\ndistance measurement\\ntrajectory\\ngradient methods\\nminimisation\\nmobile robots\\npath planning\\nrobot vision\\nranging-based location estimation\\nrigidity-constrained crlb-based motion planning\\ncomputationally-efficient navigation solutions\\nrigid bodies\\nnoisy distance measurements\\nconstrained cram\\u00e9r-rao lower bound\\nprimal-dual optimization scheme\\ndistance constraints\\nmotion planner\\nconstrained optimization approach\\nlocalization process accuracy\\ncost function minimization\\ncrlb\\ngradient terms\\ntwo-stage algorithm\",\"581\":\"jacobian matrices\\nmonte carlo methods\\nmeasurement units\\nfiltering\\nconferences\\npose estimation\\nreceivers\\nattitude measurement\\ninertial navigation\\nkalman filters\\nnonlinear filters\\nposition measurement\\nvelocity measurement\\nposition receivers\\ninertial measurement unit\\nkalman filter framework\\nmultiplicative extended kalman filter\\nstate-estimate-independent jacobians\\ntwo-receiver multiplicative extended kalman filter\\ntwo-receiver iekf\\nsingle receiver iekf approach\\nmekf\\ninvariant extended kalman filter ramework\",\"582\":\"location awareness\\nlinear systems\\nmonte carlo methods\\nnavigation\\nmerging\\ninformation filters\\nmathematical models\\ncovariance analysis\\nestimation theory\\nkalman filters\\nradionavigation\\nnovel filter architecture\\ndistributed localization\\ncompartmentalized covariance intersection algorithm\\nexcess conservatism\\nstandard covariance intersection\\nmeasurement stream\\nci method\\ncci algorithm\\nlocalization application\\nreal-world range measurements\\ncooperative navigation networks\\nmonte carlo simulations\\nideal centralized estimator\",\"583\":\"three-dimensional displays\\nlaser radar\\ntracking\\nairborne radar\\nradar detection\\nmillimeter wave radar\\nradar tracking\\ncameras\\ndeep learning (artificial intelligence)\\ndoppler radar\\nmillimetre wave radar\\nmotion capture\\nradar imaging\\nremotely operated vehicles\\nrobot vision\\noptical motion capture\\nactive marker\\npassive marker\\nsingle-chip millimeter wave radar\\ndecimeter level 3d tracking\\ninexpensive drone research\\n3d motion capture\\nunmodified drone\\naccurate motion capture\\naerial robots\\nautonomous operation\\ndeep neural network\",\"584\":\"training\\nuncertainty\\nthree-dimensional displays\\ntransfer learning\\ndecision making\\nvirtual environments\\nreinforcement learning\\ngraph theory\\nmobile robots\\nneural nets\\nautonomous exploration\\nlocalization uncertainty\\nmobile robot\\n3d range sensing\\nhigh-performance exploration policy\\nsingle simulation environment\\ndomain adaptation\\ndomain randomization\\nrandomized conditions\\ndomain knowledge\\ngraph neural networks\\ndeep reinforcement learning\\nexploration information\\ntransferable decision-making strategy\\nzero-shot transfer\\nsimulation environments\\nreal-world environments\\nzero-shot reinforcement learning\\nsim2sim approach\\nsim2real approach\",\"585\":\"measurement\\ntraining\\nuncertainty\\ncomputational modeling\\npose estimation\\nneural networks\\ntraining data\\ncorrelation methods\\ndeep learning (artificial intelligence)\\nobject recognition\\nrobot vision\\nrobot grasping task\\ndeep object pose estimation\\nrobotic tasks\\n6-dof object pose estimation\\nneural network architectures\\ntraining data sources\\naverage pair-wise disagreement\\naverage distance\\nlabeled target data\\nuq method\\nuncertainty quantification yields\\nlearning-free metric\\ndeep learning-based object pose estimators\\nuncertainty quantification\",\"586\":\"three-dimensional displays\\nsimultaneous localization and mapping\\ncomputational modeling\\nsurveillance\\nsemantics\\nterrain mapping\\ncameras\\nautonomous aerial vehicles\\nfeature extraction\\nimage reconstruction\\nimage sequences\\nmesh generation\\nmobile robots\\nrobot vision\\nstereo image processing\\nlocal mesh\\nsparse depth measurements\\nimage features\\nmesh vertices\\naerial images\\nmesh reconstruction\\noutdoor terrain mapping\\noverhead images\\nunmanned aerial vehicle\\ndense depth estimation\\nfeature-based localization\\nsparse points reconstruction\\ndense environment model\\nglobal environment model\\njoint 2d-3d learning\\n3d mesh supervision\\n2d reprojected depth\",\"587\":\"training\\nlearning systems\\nconferences\\nestimation\\ngrasping\\nparallel processing\\nconvolutional neural networks\\ncontrol engineering computing\\nconvolutional neural nets\\ndexterous manipulators\\nestimation theory\\ngrippers\\nlearning (artificial intelligence)\\necnn\\nensemble learning\\nrobotic grasp synthesis\\nensemble convolutional neural network\\nopen-source algorithms\\nplanar grasp quality estimation\\nmixture of experts model\\ngrasping decision\\nmoe\",\"588\":\"stacking\\ntransfer learning\\nneural networks\\ncognition\\ntask analysis\\nrobots\\ncausality\\ninference mechanisms\\nlearning (artificial intelligence)\\nlearning systems\\nmanipulator dynamics\\nneurocontrollers\\nrobust control\\nstate-space methods\\ncausal reasoning\\nrobot manipulation policy\\naction space\\nneural network policies\\ndomain randomization\\npolicy network weights\\nrepresentative manipulation tasks\\npolicy transfer experiment\\ncrest\\nstate space learning\\nblock stacking\\ncrate opening\\nzero-shot sim-to-real transfer\",\"589\":\"deep learning\\nautomation\\ntransfer learning\\ntraining data\\nsurgery\\ntools\\nfeature extraction\\nbiological tissues\\ndeep learning (artificial intelligence)\\nmanipulators\\nmedical robotics\\nobject tracking\\nsuper deep\\nrobotic tissue manipulation\\nrobotic automation\\nprecise tracking\\ndeformable tissue\\nsurgical perception frameworks\\ntissue tracking\\ndeep learning methods\\nintegrated deep neural networks\\nefficient feature extraction\\nsurgical tool tracking processes\\nleveraging transfer learning\\ndeep-learning-based approach\\nreduced feature engineering efforts\\nsurgical scene\\nda vinci surgical system\\nstate-of-the-art tracking performance\\nsurgical environment\\nsurgical perception framework\",\"590\":\"visualization\\nautomation\\nconferences\\nlogic gates\\nplanning\\ntrajectory\\nsafety\\ncognition\\nlearning (artificial intelligence)\\nneurophysiology\\npath planning\\nuser interfaces\\nvisual perception\\nsparse attention module\\nimportant regions\\nmotion planning\\nprior literature\\nperception tasks\\nattention mask\\nperceive\\nspatial attention\\nsafe self-driving\\nend-to-end self-driving network\",\"591\":\"automation\\nconferences\\nhuman-robot interaction\\nprogramming\\nrobot sensing systems\\ntrajectory\\ntask analysis\\nassistive robots\\nlearning (artificial intelligence)\\nmobile robots\\nrobot programming\\nrobot vision\\nhuman objectives learning\\npersonal robots\\ninteractive robots\\nmultiple robots\\nphysical correction sequences\",\"592\":\"conferences\\nneural networks\\nreinforcement learning\\nmanuals\\ndata collection\\ndata models\\ntrajectory\\ncontrol system synthesis\\nmobile robots\\nmotion control\\nneural nets\\ndomain adaptation\\nhybrid physics\\nsimulated trajectories\\ntarget domain\\nlearned discriminative loss\\nmanual loss design\\ntraditional physics simulation\\nadversarial reinforcement learning\\npolicy refinement\\nsimgan\\nhybrid simulator identification\\nlearning-based approaches progress\\nrobot controllers design\\nsim-to-real transfer\\nmanual effort\\nhybrid simulator\",\"593\":\"resistors\\nforce measurement\\nhumanoid robots\\nfootwear\\nrobot sensing systems\\nsensors\\ncalibration\\nforce sensors\\ngait analysis\\nleast squares approximations\\nmobile robots\\nmotion control\\noptimisation\\ndirect sensing\\nopen-source force-sensing shoes\\nshoes measure center\\nnormal ground reaction force\\ntm robot\\nsensing modules\\ncalibration method\\nblue force-sensing shoes\\nhigh-fidelity sensing systems\\nrobot feet\\ngrf\\nregularized least squares optimization\\nrobots factory-installed force-sensing resistors\\nrobots built-in fsrs\",\"594\":\"meters\\nvisualization\\nthree-dimensional displays\\nconferences\\nbuildings\\ntransforms\\nfeature extraction\\nautonomous aerial vehicles\\ncalibration\\ncameras\\nimage motion analysis\\nimage sensors\\nmobile robots\\nobject detection\\npath planning\\nrobot vision\\nslam (robots)\\nvisual scale\\ncoordinate systems\\nuav target-selection\\n3d pointing interface system\\nlarge-scale environment\\ninterface application\\nhuman user\\nlarge-scale indoor environments\\noutdoor environments\\nyolo\\norb-slam\\nuser pose\\nscale-dependent 3d map\\n3d sparse feature points\",\"595\":\"measurement\\nmachine learning\\nrobot sensing systems\\ncameras\\nthroughput\\nsensors\\ntask analysis\\ncontrol engineering computing\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nquality control\\nrobot programming\\nobject pick-up task\\ndestructive system crash\\nsensor input qualities\\nnonexpert programmers\\nsqrp\\nmechanical skills\\n6dof robot arm\\nsensing quality-aware robot programming system\\nrobotic manufacturing\\nskill parameters\\nmonitoring algorithms\\nskill execution\\nmonitoring modules\",\"596\":\"fault diagnosis\\nruntime\\nautomation\\nconferences\\ndebugging\\nsoftware debugging\\nrobots\\ncomputer debugging\\nmobile robots\\nprogram debugging\\nreconfigurable architectures\\nrobot programming\\nsearch problems\\nenvironment space\\nreduced environment\\nspatial-temporal partition strategies\\npotential different robot\\nground robot\\nfailure scenarios\\nenvironment reduction\\nrobotic systems\\ncomplex environments\\nfaster fault isolation\\nautomated approach\\nsoftware debugging techniques\",\"597\":\"visualization\\nconferences\\nnatural languages\\nfocusing\\nuser experience\\nteamwork\\ntask analysis\\naugmented reality\\ncontrol engineering computing\\nhuman-robot interaction\\nmobile robots\\nmulti-robot systems\\narroch\\ncommunication gap\\nhuman adapts\\nrobot behaviors\\nsingle-robot domains\\nindoor multiroom environments\\nhuman-multirobot teamwork\\nhuman participants\\ncollaborative delivery tasks\\nhuman simulation\\nrobot simulation\\nar-based human-robot communication methods\\nhuman-multirobot communication\",\"598\":\"training\\nvisualization\\nautomation\\nconferences\\nmaintenance engineering\\ntrajectory\\ntask analysis\\naugmented reality\\nlearning (artificial intelligence)\\nteaching\\nuser interfaces\\nlearned skills\\naugmented reality interface\\narc-lfd\\ninteractive long-term robot skill maintenance\\nconstrained learning\\nlfd methods\\ntask requirements\\nadaptability\\nskill model\",\"599\":\"silver\\nautomation\\nservice robots\\nchirp\\nconferences\\naudio systems\\nmotion pictures\\nbehavioural sciences\\nhuman computer interaction\\nhuman-robot interaction\\nindustrial robots\\nmobile robots\\nmulti-robot systems\\ntransformative robot sound affects human perception\\nlovable robots\\nmultiple robot archetypes\\nonline video-based surveys\\nmusician-designed transformative sounds\\nrobot videos\\naffective robot behaviors\",\"600\":\"telepresence\\nconferences\\nrobot vision systems\\ncameras\\ncomplexity theory\\nreliability\\ntask analysis\\ncontrol engineering computing\\nimage sensors\\nmanipulators\\nmobile robots\\ntelecontrol\\ntelerobotics\\nvirtual reality\\ntele-manipulation\\ncomplex robotic systems\\nremote control\\nactive telepresence cameras\\nimproved telepresence\\noperator\\nuser study\\ngeneral human performance\\nwearable cameras\\nhuman motion analysis\\nrobot teleoperation interfaces\",\"601\":\"covid-19\\nautomation\\nnavigation\\npandemics\\nconferences\\nmedical services\\nmobile robots\\ncontrol engineering computing\\nemergency management\\nhealth care\\nmedical robotics\\npatient treatment\\nsafety-critical software\\nsocial robots\\nsocial navigation\\nemergency department\\nsafety-critical environment\\nhealthcare workers\\ncovid-19 pandemic\\ncare delivery\\nsafedqn\\nacuity-aware navigation system\\nsafety-critical deep q-network system\",\"602\":\"automation\\nnavigation\\nconferences\\nneural networks\\ndecision making\\nreinforcement learning\\nmobile robots\\nlearning (artificial intelligence)\\nmulti-robot systems\\npath planning\\nrecurrent neural nets\\ntraffic engineering computing\\nefficient navigation\\nhuman crowds\\nrobot crowd navigation\\nprevious methods deteriorates\\npartially observable environments\\ndense crowds\\nstructural-recurrent neural network\\nrobot decision\\nmodel-free deep reinforcement learning\\ncrowd navigation scenarios\\ndecentralized structural-rnn\\nsafe navigation\",\"603\":\"fuses\\nconferences\\nrobot sensing systems\\napproximation algorithms\\ndensity functional theory\\nunmanned aerial vehicles\\nstability analysis\\naerospace robotics\\ndistributed control\\nmobile robots\\nmulti-robot systems\\nsensors\\nstability\\nremotely-accessible\\nlloyds algorithm extension\\naerial ground agents\\nheterogeneous robots\\nfuses coverage information\\nhomogeneous teams\\ncommon coverage control techniques\\nsensing capabilities\\nmultirobot systems\\nair-ground multirobot teams\\nrange limited coverage control\\nmultirobot testbed\\nglobally focused distribution controller\\nlocally focused coverage controller\\ncoarse domain sensors\\naerial robots\",\"604\":\"performance evaluation\\ncosts\\nservice robots\\nfiltering\\nconferences\\nestimation\\npredictive models\\nassistive robots\\nhuman-robot interaction\\npath planning\\ncommunication strategy\\ndomain-structure awareness\\nperformance trade-offs\\npublic environments\\nhuman assistance\\nbehavioral-based qualitative analysis\\nagent-based modeling\\npedestrian behavior prediction\\nperformance maximization\\nguidance providing\",\"605\":\"automation\\nnavigation\\ntrajectory planning\\nconferences\\nreinforcement learning\\ntrajectory\\nplanning\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\ngoal planning\\nhybrid solution\\nclassical trajectory\\nautonomous robot\\nintended trajectory\\nhuman trajectory\\ndeep rl module\\nimplicitly estimates\\nshort-term navigational goals\\ntrajectory planner\\nlow-level execution\\nshort-term goals\\ncurriculum learning\\npure deep rl approach\",\"606\":\"systematics\\nshape\\nconferences\\nobject segmentation\\nhandover\\nrobustness\\nreal-time systems\\nclosed loop systems\\nhuman-robot interaction\\npath planning\\nposition control\\nrobot vision\\nhuman-robot object handovers\\narbitrary appearance\\nvision-based system\\nreactive human-to-robot handovers\\nunknown objects\\nclosed-loop motion planning\\nreactivity\\nmotion smoothness\\nobject positions\\nnonrigid objects\\ndiverse household objects\\narbitrary objects\",\"607\":\"animatronics\\nrobot kinematics\\nconferences\\nrobot vision systems\\nbuildings\\ncameras\\nskin\\nhumanoid robots\\nintelligent robots\\nneural nets\\nsocial robots\\nsupervised learning\\nphysical animatronic robotic face\\nsoft skin\\nvision-based self-supervised learning framework\\nfacial mimicry\\npredefined expression\\nlearning process\\ngenerative model\\ninverse model\\ndiverse human subjects\\nintelligent expressions\\ngeneralizable facial expressions\\nfacial expression\\nrobot behavior\\nhuman labels\\nfast action decisions\\nacquired knowledge\\ndiverse contexts\\ncamera calibration\\nsingle motor babbling dataset\",\"608\":\"automation\\nconferences\\nsenior citizens\\neducation\\ntask analysis\\nrobots\\ntuning\\nassistive robots\\nhuman-robot interaction\\nelderly users\\nhuman teachers\\nhuman objectives\\nphysical disabilities\\nhuman choice set\\nworst-case performance\\nteaching\\ninclusive learners\",\"609\":\"adaptation models\\nanalytical models\\ncomputational modeling\\nindoor navigation\\noptimal control\\ntools\\npredictive models\\nlearning (artificial intelligence)\\nmobile robots\\nparameter estimation\\nreachability analysis\\nsafety-related questions\\nconfident estimate\\nnearby human\\nparameter initializations\\nanalysis questions\\nlearning algorithm\\ncurrent model parameter estimate\\nhuman data\\nanalysis tool\\nhuman-robot domains\\nanalyzing human models\\nadapt online\\npredictive human models\",\"610\":\"uncertainty\\nparameter estimation\\nsystem dynamics\\nneural networks\\nhuman-robot interaction\\ndifferential games\\nplanning\\ncollision avoidance\\ngame theory\\nmulti-agent systems\\nnaive bayes methods\\nprobability\\nempathetic parameter estimation\\nmultiagent interactions\\nhuman-robot interactions\\nhri\\nprivate reward parameters\\nperfect bayesian equilibria\\nmotion planning\\nhigh safety risks\\nuncontrolled intersection scenarios\\nagent updates\\nnonempathetic belief update methods\\ntwo-vehicle uncontrolled intersection case\\nincomplete-information nature\\nnash equilibrial action-values\\navoid collisions\",\"611\":\"location awareness\\nthree-dimensional displays\\nnavigation\\nconferences\\noctrees\\ninspection\\nextraterrestrial measurements\\nmobile robots\\npath planning\\nconfined spaces\\nautonomous navigation\\nmotion planning\\nglobal localization\\nrescue missions\\nautonomous inspection\\nthree-dimensional autonomous exploration method\\nterrain traversability\\nfrontier expected information gain\\nfrontier selection\\nsafe paths\\nfrontier extraction\\n3d map representations\\ndarpa subterranean challenge\\nterrain aware autonomous exploration\\nground robots\",\"612\":\"training\\nvisualization\\nimage segmentation\\nsemantics\\nrobot vision systems\\nsonar\\nsonar navigation\\ncollision avoidance\\nimage recognition\\nmobile robots\\nnavigation\\npath planning\\nrobot vision\\nstereo image processing\\nstereo-visual robotic obstacle avoidance\\nunstructured environments\\nmapless environments\\nstandard avoidance techniques\\nsemantic information\\nvisual instance segmentation\\nharmless obstacles\\nnavigation strategies\\nsemantically-aware strategies\\nobject localization\\nobject classification\",\"613\":\"reflectivity\\ngeometry\\nadaptation models\\nlaser radar\\nsemantics\\nlogic gates\\nontologies\\nimage segmentation\\nlearning (artificial intelligence)\\noptical radar\\nlidarnet\\nboundary-aware domain adaptation model\\nlidar scan full-scene semantic segmentation\\ndomain private features\\nboundary information\\nfull-scene semantic segmentation labels\\ndomain gap\\nlidar point cloud semantic segmentation\\nsemantic segmentation model\\nsource domain\\ntarget domain\\nchannel distributions\\nreflectivity distributions\",\"614\":\"friction\\nconferences\\nswitches\\nclimbing robots\\nhardware\\nplanning\\nsafety\\ncontrol system synthesis\\ndc motors\\ngrippers\\nlegged locomotion\\nmobile robots\\nnonlinear programming\\npath planning\\nrobot kinematics\\nextra consideration\\ncontact sequence planner\\nflat terrain\\nvertical surfaces\\ntransition phase\\nmulticontact contact wrenches\\npredetermined contact sequence\\ndifferent environment setups\\ncontact constraints\\nlimb switchability\\ncomplementarity conditions\\ntoe\\nmotor over-torque\\ndifferent contact sequences\\nfeasible sequences\\ncontact forces\\nsixlegged robot capable\\nwalls\\ntransition motion planning\\nmultilimbed vertical climbing robots\\ncomplementarity constraints\\nautonomous vertical wall climbing\",\"615\":\"legged locomotion\\ncodes\\nautomation\\ntrajectory planning\\nconferences\\nlibraries\\nhardware\\nfeedforward\\nhumanoid robots\\nrobot dynamics\\ninverse dynamics control\\ncompliant hybrid zero dynamic walking\\ncontrol architecture\\nbipedal locomotion\\nhighly underactuated robot\\ncompliant bipedal robot\\ncompliant walking trajectories\\ncompact arrays\\npolynomial coefficients\\ntracking online\\ncontrol implementation\\nfloating-base inverse dynamics controller\\ndynamically consistent feedforward torques\\nwalking using information\\ntrajectory optimization\",\"616\":\"legged locomotion\\nactuators\\nservice robots\\nfriction\\ndynamics\\nposition control\\ntools\\ngears\\nindustrial manipulators\\nrigidity\\nrobot dynamics\\ndynamic effect\\nmechanical losses\\nlegged robots\\nstiff position control\\npick-and-place\\npower losses\\nmechanical transmissions\\nrobotic system\\njoint friction\\nmotion equation\",\"617\":\"training\\nlearning systems\\nconferences\\nreinforcement learning\\nstability analysis\\ncomplexity theory\\nautomobiles\\nautonomous aerial vehicles\\ncontrol system synthesis\\nlearning (artificial intelligence)\\nlyapunov methods\\nneurocontrollers\\nnonlinear control systems\\nstability\\nstability guarantee\\nlearning-based methods\\ncore control problems\\nneural control policies\\nneural lyapunov critic functions\\nmodelfree reinforcement learning setting\\nsample-based approaches\\nlyapunov function conditions\\nlearned lyapunov critic functions\\nneural controllers\\nnonlinear systems including automobile\\nquadrotor control\\nself-learned almost lyapunov critics\",\"618\":\"training\\nheating systems\\npower demand\\nneural networks\\nreinforcement learning\\nsystem improvement\\nmarket research\\naircraft control\\nautonomous aerial vehicles\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\nhelicopters\\nneurocontrollers\\noscillations\\nquadrotor drone\\ncaps\\nconditioning for action policy smoothness\\ndeep reinforcement learning\\nneural network controllers\\nstate-to-action mappings\\ncontrol signal oscillation\\nrl policies\\nsmooth control\\naction policies\",\"619\":\"deep learning\\nlimiting\\nhandheld computers\\ncontrol systems\\nsafety\\ncomplexity theory\\nnonlinear dynamical systems\\ncollision avoidance\\nformal verification\\nlearning (artificial intelligence)\\nmobile robots\\npartial differential equations\\nreachability analysis\\nstate variables\\nsmall-scale systems\\ndeepreach\\nleverages new developments\\nsinusoidal networks\\nneural pde solver\\nhigh-dimensional reachability problems\\ncomputational requirements\\nstate dimension\\nunderlying reachable tube\\nstate-of-the-art reachability methods\\npde solution\\nexternal disturbances\\nadversarial inputs\\nsystem constraints\\nsafety controller\\nhamilton-jacobi reachability analysis\\nimportant formal verification method\\nsafety properties\\ndynamical control systems\\ngeneral nonlinear system dynamics\\nformal treatment\\nbounded disturbances\\ninput constraints\\ncomputational memory complexity scales\",\"620\":\"target tracking\\nnavigation\\ncomputational modeling\\nconferences\\nreinforcement learning\\ncost function\\nplanning\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nessential tasks\\nautonomous systems\\ndeep reinforcement learning approach\\nautonomous agent\\nclassical challenges\\nsystem model dependence\\ninformation-theoretic cost functions\\nlong planning horizon\\neffective planning horizon\\nsystem models\\nunified deep rl policy\\nactive target tracking\\nagile targets\\nanomalous targets\\npartially known target model\\ndistant targets\\nactive tracking target network\",\"621\":\"location awareness\\nnavigation\\nreinforcement learning\\ngames\\nmedia\\nrobot sensing systems\\nsensors\\nautonomous aerial vehicles\\nautonomous underwater vehicles\\ncollision avoidance\\ndecision making\\ndeep learning (artificial intelligence)\\nmobile robots\\nmotion control\\nmulti-robot systems\\nrobot dynamics\\nrobot kinematics\\nrobot programming\\natari-like games\\nautonomous mapless navigation\\nhuauvs\\nsimple sparse range data\\nbehavior-based algorithm\\nhybrid aerial underwater vehicle\\ndeep q-learning\\ncontinuous action domain\\ndeep-rl-based approach\\ndeep reinforcement learning\\nbug2 algorithm\\nmapless navigation\\nobstacle avoidance\",\"622\":\"simultaneous localization and mapping\\nsmoothing methods\\nautomation\\nconferences\\nneural networks\\ninference algorithms\\nbayes methods\\ncontrol engineering computing\\ngaussian processes\\ngraph theory\\ninference mechanisms\\nneural nets\\nrobot vision\\nslam (robots)\\nnf-isam\\nnongaussian factor graphs\\nincremental smoothing\\nnongaussian inference algorithm\\nnormalizing flow isam\\nslam\\nbayes tree\",\"623\":\"legged locomotion\\nspace vehicles\\nsimultaneous localization and mapping\\nlaser radar\\nthree-dimensional displays\\nnavigation\\nroads\\ndistance measurement\\nmobile robots\\noptical radar\\npath planning\\nrobot vision\\nslam (robots)\\nupslam\\npanoramas slam\\nmapping system\\npanoramic depth images\\npanoramic images\\nrange measurements\\nspinning lidar sensor\\ncentimeters\\nexpansive scope\\ncubic meters\\nmapping software\\nhand-carrying\\npaired lidar\\nimu\\nlaboratory space\\nwalking pace\\ncampus environment\\nrunning pace\\nwheeled vehicle\\noff-road\\nlegged robot\\nunderground coal mine\\npublic roads\\nembedded nvidia jetson agx xavier system\",\"624\":\"deep learning\\nimage segmentation\\nthree-dimensional displays\\nlaser radar\\nconferences\\nsemantics\\nurban areas\\nfeature extraction\\nimage classification\\nimage colour analysis\\nlearning (artificial intelligence)\\nmobile robots\\noptical radar\\nsemantic scene understanding\\nrobust navigation\\nsafe autonomous navigation\\noff-road environment\\nrecent deep learning advances\\n3d semantic segmentation\\ntraining data\\nautonomy datasets\\nurban environments\\nlack multimodal off-road data\\nmultimodal dataset\\nrellis campus\\npresents challenges\\nsemantic segmentation models\\nnovel dataset\",\"625\":\"uncertainty\\nsystem dynamics\\ntransportation\\nreinforcement learning\\nstability analysis\\nsafety\\nnumerical models\\ngradient methods\\nlearning (artificial intelligence)\\noptimal control\\npendulums\\nprobability\\nmodel-based reinforcement learning\\nprovable safety guarantees\\ncritical property\\nreinforcement learning settings\\nsafety violations\\ncbfs\\ncontrol actions\\nsafety-critical control\\nmodel-free rl\\nmodel-based rl\\nuncertainty-tolerant control barrier functions\\nmodel uncertainty\\ngradient-based policy search\\nstate-of-the-art rl algorithms\",\"626\":\"training\\nconferences\\ntraining data\\nreinforcement learning\\nswitches\\nrobot learning\\nplanning\\ncontrol engineering computing\\nmanipulators\\nmotion control\\nnetwork theory (graphs)\\noptimal control\\npredictive control\\nmodel-based reinforcement learning\\nmbrl\\nmodel-predictive control\\nmpc\\ntask-conditional hypernetworks\\nfixed-capacity hypernetworks\\ntask-aware dynamics\\nhypercrl\\nrobot locomotion\\nrobot manipulation\",\"627\":\"robot motion\\nuncertainty\\nlearning automata\\nconferences\\nreinforcement learning\\nmarkov processes\\nprobabilistic logic\\nautomata theory\\nformal specification\\nmotion control\\npath planning\\nprobability\\nrobot dynamics\\ntemporal logic\\nprobabilistic labeled markov decision process\\npl-mdp\\nunknown transition probabilities\\nprobabilistic labeling functions\\nltl task specification\\naccepting sets\\ndense rewards\\nembedded ldgba\\ne-ldgba\\nsynchronous tracking-frontier function\\noptimal policy\\nsatisfaction probability\\nrl-based control synthesis\\ntemporal logic control\\nmaximum probabilistic satisfaction\\nmodel-free reinforcement learning\\ncontrol policy\\nlinear temporal logic specifications\\nmotion uncertainties\\nmodel-free rl-based motion planning\\ndiscount return\\nldgba\\nlimit deterministic generalized buchi automaton\",\"628\":\"earth\\nautonomous systems\\nconferences\\ncomputational modeling\\nstationary state\\ndecision making\\nmarkov processes\\ncomputational complexity\\nplanning (artificial intelligence)\\nset theory\\nlazy algorithm\\nreal time decision making problems\\napproximate planners\\npartial state abstractions\\nmarkov decision processes\\nground mdp\\npartially abstract mdp\",\"629\":\"measurement\\nenergy consumption\\ncomputational modeling\\npredictive models\\nrouting\\ndata models\\nbatteries\\naerospace safety\\naircraft control\\nautonomous aerial vehicles\\nenergy management systems\\nmonte carlo methods\\nremotely operated vehicles\\nrisk analysis\\nrisk management\\nsafety\\ntest flights\\nenergy usage\\nconditional value-at-risk\\nworst-case energy consumption\\nrisk space\\nrisk-space distribution\\nrisk evaluation method\\nflight safety\\ncvar-based flight energy risk assessment\\nmultirotor uavs\\ndeep energy model\\nenergy management\\nuncrewed aerial vehicle flights\\nuav\\nvehicle damage\\nhuman injuries\\nproperty damage\",\"630\":\"geometry\\nvisualization\\nanalytical models\\nautomation\\nconferences\\nrobot sensing systems\\npath planning\\nadaptive control\\ngame theory\\nrobot vision\\nautonomous exploration\\nvisual search\\nvolumetric exploration\\nplanning policies\\ndense visual coverage\\nhypergame-based adaptive behavior path planning\\nrobot\",\"631\":\"wind\\nautomation\\nconferences\\ntransforms\\nstability analysis\\nrobustness\\ntask analysis\\naerodynamics\\naerospace control\\nattitude control\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\nremotely operated vehicles\\nrotors\\nstability\\nflight performance\\nmorph induced inertia variation\\ncustom-built in-flight morphing quad-rotor\\nmorphing-size-down\\nflight stability\\nhigh flight performances\\nhigher flight performances\\nhigh-performance flight\\nmorphologically adapatative quad-rotor\\nregular scaled aerial vehicles\\nin-flight morphing quad-rotor\",\"632\":\"measurement\\nautomation\\ntracking\\nconferences\\npredictive models\\nsafety\\ntesting\\ncomputer simulation\\ndriver information systems\\niterative methods\\nroad safety\\nroad vehicles\\nmotion prediction metrics\\nself-driving safety\\nfinal displacement error\\nhigh-fidelity simulations\\ntrack testing\\nfull-system behavior\\nswifter iteration cycles\\nprediction evaluation\\ncomplementary metrics\\nsafety metric\\nbeelines\\nself-driving comfort\\nself-driving vehicle system-level performance\\naverage displacement error\\nade\\nfde\",\"633\":\"measurement\\nassembly systems\\nsoftware packages\\nsoftware algorithms\\napproximation algorithms\\ncomputational efficiency\\nsteady-state\\nassembling\\nmachining\\nreliability\\nvirtual machines\\nperformance metrics calculation\\nexponential reliability machines\\nvirtual serial lines\\noriginal assembly system\\ndecomposition-aggregation-based method\\nmultiple merge operation\\nnumerical experiments\\nconvergence algorithm\",\"634\":\"resistance\\nirrigation\\nautomation\\nstochastic processes\\ntools\\nsoil\\nsensors\\nagriculture\\naquaculture\\ndiseases\\nfarming\\nlearning (artificial intelligence)\\nseed placement algorithm\\nincreases coverage\\nlearned pruning policy\\nsimulation experiments\\nrandom seed placement\\nprocedural lookahead policy\\nhigh leaf coverage\\nplant diversity\\nplant species\\ndiverse growth rates\\nfully-automated system\\npolyculture farming\\ncompanion plants\\nsustainable farming technique\\nsynergistic interactions\\ndiffering plant types\\nreduced uniformity\\nscaled physical testbed\\nhigh resolution camera\\nsoil sensors\\npolyculture plants\\nplant growth\\ncompanion effects\\nirrigation parameters\\nfirst-order garden simulator\\nsize 3.0 m\\nsize 1.5 m\",\"635\":\"damping\\nfrequency synthesizers\\nautomation\\nsynthesizers\\nconferences\\nforce\\nrobot sensing systems\\naerospace computing\\naircraft maintenance\\ncondition monitoring\\nfault diagnosis\\nmulti-agent systems\\nsensors\\ngeneral-purpose anomalous scenario synthesizer\\nrotary equipment\\ndata synthesizing\\ndata-driven anomaly prognostics\\ngpass\\nmultivariate sensor\\ncalibration and identification\\nfailure detection and recovery\\nfault diagnosis and prognosis\",\"636\":\"bridges\\nautomation\\nsystem performance\\nconferences\\nbuildings\\nunmanned aerial vehicles\\nreliability\\naerospace robotics\\nassembling\\nbridges (structures)\\ndisasters\\nfailure (mechanical)\\nmanipulators\\nmobile robots\\nstructural engineering\\nstructural engineering computing\\nassembled structure\\nreliable autonomous assembly\\nconstruction site\\nautonomous vault-building robot system\\ncreating spanning structures\\nautonomous robots\\nground-based robots\\nclimbing robots\\naerial robots\\nautonomous construction\\nlarger buildings\\nmultistory buildings\\nsturdy structures\\nload-bearing spanning vault\\nidentical modular blocks\\ncustom blocks\\nrobotic manipulation\\nlocomotion\",\"637\":\"shape\\ndynamics\\nforce\\nmachine learning\\ninterconnected systems\\nhardware\\nsprings\\nactuators\\nlegged locomotion\\nmobile robots\\nmotion control\\noptimisation\\nrobot dynamics\\nsprings (mechanical)\\nunification\\nsystem design\\nmotion synthesis\\nhigh-performance hopping\\nrobotic hopping\\nextreme interactions\\nintricate process\\ngeneral optimization framework\\nnonlinear springs\\nefficient hopping\\nspring designs\\nnovel hopping robot\",\"638\":\"geometry\\nstrips\\nuncertainty\\nforce measurement\\nconferences\\ngrasping\\nbenchmark testing\\ndeep learning (artificial intelligence)\\nlearning systems\\nmanipulators\\nobject detection\\nrecurrent neural nets\\nobject manipulation\\nrobotic manipulation\\nrigid objects\\nnonrigid objects\\nforce-based manipulation\\nnoisy sensor inputs\\ndeep recurrent network\\nmultistep recurrent q-learning\\nvelcro peeling\",\"639\":\"sequential analysis\\nautomation\\ninstruments\\nconferences\\nreinforcement learning\\ndata collection\\nhardware\\ndexterous manipulators\\nhuman intervention\\nsingle-task learning\\nmultitask rl\\nmultitask learning\\nexplicit resets\\nreset-free reinforcement\\ncomplex robotic skills\\nhuman supervision\\nepisodic resets\\nreset-free learning\\ncomplex dexterous manipulation\",\"640\":\"training\\nheuristic algorithms\\nreinforcement learning\\npredictive models\\nprediction algorithms\\nvalves\\ndata models\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\ndexterous manipulators\\npredictive control\\nrobot programming\\npolicy learning\\nreal-robot training\\nfinger gaiting\\nrobot skill acquisition\\ndeep reinforcement learning\\ninformation theoretic model predictive control\\nmopac\\noptimal trajectories\\nmodel predictive actor-critic\",\"641\":\"geometry\\nautomation\\nfriction\\nconferences\\nforce\\npressing\\nfinite element analysis\\ncutting\\nfracture\\nfracture mechanics\\nfracture toughness\\nrobotic slicing\\nknife geometry\\nimportant skill\\nefficient results\\ncutting experiments\\ncritical strain\\novercoming friction\\nfinite element method\\nprevious work\\nstraight knife edge\\nmodel general slicing\\ncutting force\\nhuman-level skills\",\"642\":\"knowledge engineering\\nvisualization\\nautomation\\nconferences\\npredictive models\\nsearch problems\\ntask analysis\\ndeep learning (artificial intelligence)\\nimage colour analysis\\niterative methods\\nmulti-robot systems\\nauto-tuned sim-to-real transfer\\nreality gap\\nvisual properties\\nsearch problem\\nsimulation system parameters\\nworld system parameters\\nnaive domain randomization\\nrgb images\\nsearch param model\\nsim-to-sim transfer\\nmultiple robotic control tasks\",\"643\":\"robot motion\\nautomation\\nfriction\\nscalability\\nconferences\\ncomplexity theory\\nplanning\\nconvex programming\\nmanipulators\\nmechanical contact\\nmotion control\\npath planning\\nquadratic programming\\nconvex quasistatic time-stepping scheme\\nrigid multibody systems\\nquasistatic models\\nmulticontact situations\\nconvex relaxation\",\"644\":\"automation\\nconferences\\nsearch problems\\ndynamic programming\\nobject recognition\\ncollision avoidance\\ntask analysis\\ncomputational complexity\\ncomputational geometry\\ngraph theory\\noptimisation\\npath planning\\nnonmonotone instances\\ninformed search framework\\nuniform object rearrangement\\ncomplete monotone primitives\\nefficient nonmonotone informed search\\nwidely-applicable task\\nobjects increases\\nuniform objects\\nrobot-object collisions\\nobject-object collisions\\nobject transfers\\nefficiently computable decomposition\\nequivalent collision possibilities\\ncomplete dynamic programming primitive dfsdp\\nmonotone problems\",\"645\":\"automation\\nthree-dimensional displays\\ndatabases\\ncomputational modeling\\ndata collection\\ntools\\nmanipulators\\narchaeology\\ndexterous manipulators\\npottery\\nrobot programming\\nrobotic arm\\nceramics automated locomotion\\narchaeologists\\narchaeological dig\\npottery fragments\\nvaluable time\\ndata collection process\\nautomated system\\nreconfigurable data collection stations\",\"646\":\"automation\\ngrounding\\nconferences\\nsemantics\\ncollaboration\\ngrasping\\nmanipulators\\ncollision avoidance\\nmobile robots\\npath planning\\ntemporal logic\\ntask accomplishment\\nproblem complexity\\nunknown obstacles\\nreactive planning\\nmobile manipulation tasks\\nunexplored semantic environments\\ncomplex manipulation tasks\\nrearrangement planning\\nnumerous objects\\nhard problems\\noffer any rigorous guarantees\\nnovel hybrid control architecture\\nmobile manipulators\\ntemporal logic specification\\nmobile manipulation primitives\\nautomaton representation\\nphysical grounding\\nmobility\\ncontinuous reactive controller\\nway movable objects\",\"647\":\"simulation\\nconferences\\nredundancy\\nthumb\\nnull space\\nkinematics\\ngrasping\\ngrippers\\nmanipulator kinematics\\narm-thumb serial chain\\nserial chains\\nhybrid parallel-serial system\\nclosed-chain formulation\\nvirtual revolute joint\\narm-thumb closed chain\\nprecision grasps\\ninverse kinematics solution\\nprecision grasping\\ndesired grasp configuration\\ncontact points\\ncontact normals\\nhuman-inspired thumb\\nrobotic arm-hand systems\",\"648\":\"geometry\\ntraining\\nschedules\\nautomation\\nconferences\\nsupervised learning\\ntactile sensors\\ncontrol engineering computing\\nfeedback\\nhaptic interfaces\\nimage colour analysis\\nmanipulators\\nobject detection\\nreinforcement learning\\nrobot vision\\ntactile-based policy\\ntactile-rl\\nobject insertion\\ntactile-based feedback insertion policy\\nepisodic policy\\ntactile insertion policy\\nobject geometries\\nlearning agent\\ncurriculum learning\\ntactile representation\\ntactile flow\\nclosed-loop insertion policy\\ncontact-rich manipulation task\\ntactile rgb\",\"649\":\"visualization\\nthree-dimensional displays\\nservice robots\\nbiological system modeling\\ncontacts\\ntactile sensors\\nrobot sensing systems\\ncomputer simulation\\ncontrol engineering computing\\nfinite element analysis\\ngraphics processing units\\nmanipulators\\nsolid modelling\\nrobotic tactile sensing\\nphysics-based simulation\\nlearned latent projections\\nrobotic grasping\\nvisual occlusion\\nrobot arms\\ncameras\\nsyntouch biotac sensor\\n3d finite element method model\\ncpu-based simulator\\nlatent representations\\nsimulated biotac deformations\\nself-supervision\\nlatent spaces\\nsupervised dataset\\ncontact patches\\nfem\\nsim-to-real transfer\\nobject manipulation\\nopen-access gpu-based robotics simulator\\nreal-world biotac electrical output\\ncontact interactions\\ncross-modal transfer\",\"650\":\"simultaneous localization and mapping\\nuncertainty\\nsmoothing methods\\nshape\\nshape measurement\\ngaussian processes\\nreal-time systems\\ngraph theory\\nmanipulators\\npose estimation\\nregression analysis\\nrobot vision\\nslam (robots)\\ntactile slam\\nreal-time inference\\ntactile perception\\nrobot manipulation\\nunstructured environments\\nmature implementation\\nobject models\\nobject shape\\ntactile measurements\\ntactile exploration\\nunknown object\\nonline slam problem\\nnonparametric shape representation\\ntactile inference alternates\\ngaussian process implicit surface regression\\nlocal gaussian processes\\nreal-world planar pushing tasks\",\"651\":\"training\\nautomation\\nnavigation\\natmospheric measurements\\nconferences\\nmachine learning\\nparticle measurements\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\ntelerobotics\\nadaptive planner parameter learning\\nclassical autonomous navigation systems\\ncollision-free manner\\nsuboptimal behavior\\noriginally good scenarios\\nsuboptimal cases\\nteleoperated intervention\\nhuman interventions\\nnavigation performance\\nnavigation parameters\\nunderlying navigation system\\nstatic default parameters\\ndynamic parameters\\nappli's generalizability\\n300 simulated navigation environments\",\"652\":\"training\\nvisualization\\ncosts\\nnavigation\\nvelocity control\\nsemantics\\nswitches\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\ntelerobotics\\napplr\\nadaptive planner parameter learning\\nclassical navigation systems\\nfixed set\\nhand-picked parameters\\nheavy expert re-tuning\\nlearn parameters\\nhuman demonstration\\ntraining environment\\npotentially-suboptimal demonstrator\\nparameter selection scheme\\nreinforcement learning\\nsimulation environments\\nhand-tuned parameters\\ndynamic parameter tuning scheme\",\"653\":\"training\\nrobust control\\ncomputational modeling\\nconferences\\ncost function\\nrobustness\\ncomplexity theory\\ncontinuous systems\\ncontrol engineering computing\\ndiscrete systems\\nmobile robots\\nreinforcement learning\\nrobot programming\\nlong control horizon\\nmodel-free approaches\\ntraining samples\\nhybrid control\\nlearning framework\\nrilqr\\nmodel-free rl policy learning\\ncomputation overhead\\nmodel errors\\nmodel-free baselines\\nsample-efficient robot locomotion learning\\nrobotics\\nmodel-based approaches\\nreinforced ilqr\\nmujoco platform\",\"654\":\"adaptation models\\nrobot kinematics\\nconferences\\ncollaboration\\ndata collection\\nmanipulators\\nreal-time systems\\ndexterous manipulators\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nrobot programming\\ntelerobotics\\ncollaborative teleoperation\\nsingle-arm manipulation\\nmultiarm manipulation tasks\\nrobot arms\\nmultiarm roboturk\\nmultiuser data collection platform\\nmultiarm tasks\\nthree-arm tasks\\nrobot actions\",\"655\":\"uncertainty\\nautomation\\nautonomous systems\\nsystem dynamics\\nconferences\\ncontrol systems\\nsafety\\naircraft control\\ncomputational complexity\\nhelicopters\\nlearning (artificial intelligence)\\nmobile robots\\nreachability analysis\\nsafety systems\\nscalable learning\\nsafety guarantees\\nhamilton-jacobi reachability\\nassistive robots\\nguaranteeing safety\\nsafe sets\\nsafety analysis\",\"656\":\"neural networks\\nestimation\\nreinforcement learning\\nprediction algorithms\\ntopology\\nmanufacturing\\ntask analysis\\ncollision avoidance\\ngrippers\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nobject detection\\nobject tracking\\nrobot vision\\nsupporting item\\nendowing robots\\ncontact point correspondences\\ndeep reinforcement learning algorithm\\ncollision-free path\\nstable hanging\\nneural network based collision estimator\\ninput partial point clouds\\nstable object poses\\narbitrary objects\\nneural collision estimation\",\"657\":\"tracking loops\\nsimultaneous localization and mapping\\nautomation\\ntracking\\nconferences\\nrobot vision systems\\ncameras\\nmobile robots\\nrobot vision\\nslam (robots)\\nasynchronous multiview slam\\nasynchronous multiframes\\namv-bench\\nasynchronous multicamera robotic platform\\nasynchronous sensor modeling\",\"658\":\"fuses\\nmultimodal sensors\\nrobot sensing systems\\nreal-time systems\\ntrajectory\\nindoor environment\\nhistory\\nconvolutional neural nets\\ndata acquisition\\ninertial navigation\\nlocation based services\\nsensor fusion\\nwireless lan\\nfloorplan data fusion\\nindoor environments\\nmultimodal sensor fusion algorithm\\nfloorplan information\\ndense location history\\ninertial navigation algorithm\\nrelative motion trajectory\\nimu sensor data\\nwifi-based localization api\\nconvolutional neural network\\ndata acquisition app\\nfusion-dhl\\nwifi information fusion\",\"659\":\"visualization\\nsmoothing methods\\nlaser radar\\nconferences\\nfeature extraction\\nsolids\\nrobustness\\ndistance measurement\\ngraph theory\\nmobile robots\\noptical radar\\nrobot vision\\nslam (robots)\\nstate estimation\\nlidar measurements\\nlis\\ntightly-coupled lidar-visual-inertial odometry\\nreal-time state estimation\\nmap-building\\nvisual-inertial system\\nlidar-inertial system\\nvisual features\",\"660\":\"location awareness\\nvisualization\\nuncertainty\\nsystematics\\nautomation\\nconferences\\ndata models\\ncalibration\\ndistance measurement\\nkalman filters\\nnonlinear filters\\nuncertainty calibration\\nvisual inertial localization\\nextended kalman filter\\nstraightforward recipe\\nmean covariance\\ncausal fashion\\nrecursive fashion\\nmean estimates\\ntypically inaccurate estimates\\ncovariance estimates\\nstandard ekf\\nvisual inertial odometry system\",\"661\":\"performance evaluation\\nsimultaneous localization and mapping\\nuncertainty\\nsystematics\\nrefining\\nreal-time systems\\nservers\\naugmented reality\\nclient-server systems\\nmobile robots\\noptimisation\\nslam (robots)\\nslam algorithms\\ngeneral client-server slam optimization framework\\nreal-time state estimation\\nlow requirements\\non-board resources\\non-device state estimation\\non-device early loop closures\\nmemory budget\\ndistributed client-server optimization\\non-device resources\\ncrucial functionality\\nexploration robots\\nreality devices\\ncomputational memory cost\",\"662\":\"automation\\nnavigation\\nheuristic algorithms\\nconferences\\nprediction algorithms\\nplanning\\ntrajectory\\nautonomous aerial vehicles\\ncollision avoidance\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\npredictive control\\ncooperative hunting\\ndynamic environments\\nhunting problem\\nobstacle rich environments\\nmoving obstacle\\ncontrol problem\\nplanning problem\\nmodel predictive control method\\nmultiagent planner\\nhunting objective while\\nuav dynamics\\noptimal reciprocal collision avoidance\\nobstacle environments\",\"663\":\"automation\\nnavigation\\nconferences\\nreliability\\nnoise measurement\\nforecasting\\npredictive control\\ndecision making\\nlearning (artificial intelligence)\\nmulti-agent systems\\ntraffic engineering computing\\ninstance-aware predictive navigation\\nmultiagent environments\\nefficient end-to-end learning\\nanticipating future events\\nobject level\\ninformed driving decisions\\ninstance-aware predictive control approach\\nfuture scene structures\\nnovel multiinstance event prediction module\\npossible interaction\\nego-centric view\\nselected action sequence\\nsafe future states\\nsequential action sampling strategy\\nscene-level\\ninstance-level\\nchallenging carla multiagent\\nexplainability\\nsample efficiency\",\"664\":\"deep learning\\ncodes\\nsystem performance\\nroads\\nconferences\\nkinematics\\nmarkov processes\\nautomobiles\\nlearning (artificial intelligence)\\nneural nets\\ntraffic engineering computing\\nsimnet\\nreactive self-driving simulations\\nself-driving system performance\\nsimulation problem\\nmarkov process\\ndeep neural networks\\nkinematic models\\nhistorical traffic episodes\\nhuman driving logs\\nplanning system\\nnonreactive simulation\\nhighly realistic data-driven simulations\\nsimulation development\\nsimple end-to-end trainable machine learning system\\ntime 1000.0 hour\",\"665\":\"grounding\\nfeature detection\\nsemantics\\nnatural languages\\ntraining data\\ndetectors\\nrouting\\nfeature extraction\\nhuman-robot interaction\\nnatural language processing\\nplanning (artificial intelligence)\\ntopology\\ntopology constrained information gathering algorithm\\nautomatic semantic feature detection algorithm\\nrobot plans\\nrobotic information gathering\\nsemantic language instructions\\nlanguage commands\\nautonomous robotic sampling missions\",\"666\":\"measurement\\ncosts\\nconferences\\npipelines\\npredictive models\\ndata models\\nplanning\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nrobot dynamics\\ndeep structured reactive planning\\nintelligent agent\\nsafety\\nsurrounding scene\\njointly reasoning\\nself-driving pipelines\\nnovel data-driven\\nplanning objective\\nenergy-based\\nobservational data\\nprediction problems\\nreal-world driving\\nreactive model\\nnonreactive variant\",\"667\":\"legged locomotion\\ntraining\\nconferences\\ncomputational modeling\\nneural networks\\ndynamics\\nreal-time systems\\ncomputational complexity\\nhumanoid robots\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nneural nets\\npath planning\\nrobot dynamics\\ncentroidal motion planner\\nwhole-body optimizers\\ncomplex dynamic locomotion behaviors\\nonline replanning\\nbody movements\\nlocomotion tasks\\ncentroidal neural network\\ndesired centroidal motion\\ndesired contact plan\\nexisting whole body motion optimizer\\ntraining samples dynamic motions\\nwhole-body control framework\\ntypical full-body optimizers\\nwalking\\njumping motions\",\"668\":\"cellular networks\\nbase stations\\nmonte carlo methods\\nroads\\nconferences\\nurban areas\\npacket loss\\ncellular radio\\nmarkov processes\\noptimisation\\npath planning\\ntelecommunication network management\\ntelecommunication traffic\\ntree searching\\ncontinuously moving base stations\\nroad networks\\ncellular network base stations\\nform factor base stations\\nnetwork infrastructure\\nchanging network traffic usage patterns\\nmobile base stations\\nmbses\\nmonte carlo tree search-based\\nmultiple base stations\",\"669\":\"computer languages\\nruntime\\nautomation\\nconferences\\nprogramming\\nmulti-robot systems\\ntask analysis\\nprogramming languages\\nrobot programming\\nresh programming language\\nmultirobot orchestration\\nmultirobot systems\\nlanguage runtime\\ntemporal operators\\nlocational operators\\nspecific robot types\\nresh runtime\",\"670\":\"location awareness\\nsensitivity\\nwires\\ntactile sensors\\ntransforms\\nrobot sensing systems\\nunmanned aerial vehicles\\naerospace robotics\\nautonomous aerial vehicles\\ncollision avoidance\\nmobile robots\\nparticle filtering (numerical methods)\\npath planning\\nremotely operated vehicles\\nuavs\\ncomplex urban environments\\nnatural environments\\nnoticeable progress\\nobstacle-rich environments\\non-board processing\\navoiding obstacles\\nsignificant computational expense\\nsignificant limitations\\ncross sections\\nphysical contacts\\npassive protective devices\\nfunctional sensors\\nsuspended rim\\ncentral base\\nrelative displacement\\nmechanical design\\nderive solutions\\ninverse kinematics\\ncollision direction\\nknown environment\\nsmart cage\\nquadrotors\\nmicrounmanned aerial vehicles\",\"671\":\"legged locomotion\\ncosts\\nautomation\\nconferences\\nmorphology\\ncarbon\\npayloads\\naerospace robotics\\ndesign\\nevolutionary computation\\nmotion control\\nrobot dynamics\\nvehicle dynamics\\nmorpho-functional robot\\nquadrupedal legged locomotion\\naerial flights\\nmvam problem\\ngenerative design\\nparametric design space\\nheavy quadrupedal robot\\nheavy robot\\nnu husky carbon\\nmobility value of added mass problem\\ngrasshopper evolutionary solver\\ntotal cost of transport\\ntcot\\npayload\\naerial locomotion\",\"672\":\"training\\nautomation\\nconferences\\ndynamics\\nhumanoid robots\\nreinforcement learning\\ncontrol systems\\ngait analysis\\nhuman-robot interaction\\nimage motion analysis\\nlearning (artificial intelligence)\\nlegged locomotion\\nrobot dynamics\\nbipedal robot locomotion\\nhuman movement\\nanthropomorphic robot\\nhuman example\\nhumanlike qualities\\nworld bipedal robot\\nhuman motion capture data\\nsimulation environment\\nphysical robot\\nworld training iterations\\noffline steps\\njoint configurations\\nmotion capture actor\\nmotion re-targeting\\ntraining process\\ndomain randomization techniques\\nsimulated systems\\nphysical systems\\ninternally developed humanoid robot\\nexhibits graceful failure modes\",\"673\":\"training\\nrobot kinematics\\nreinforcement learning\\naerospace electronics\\ntrajectory\\nspace exploration\\nsteady-state\\nend effectors\\nlearning (artificial intelligence)\\nlegged locomotion\\nmobile robots\\nmotion control\\nrobot dynamics\\nlearning process\\ntask space actions\\nrl\\ntraining bipedal locomotion policies\\njoint-coordination controllers\\njoint trajectories\\navailable controllers\\nhigher-level goals\\ndesired end-effector foot movement\\ntask space policy\\nmodel-based inverse dynamics controller\\njoint-level controls\\nnatural action space\\ndesired task space dynamics\\npurely joint space actions\\nlearned policies\\nbipedal robot cassie\\nincorporating bipedal control techniques\",\"674\":\"legged locomotion\\nsolid modeling\\nuncertainty\\nthree-dimensional displays\\nmachine learning\\ncontrol theory\\ntask analysis\\ngait analysis\\nlearning (artificial intelligence)\\noptimisation\\nrobot dynamics\\nstability\\npreference-based learning\\nuser-guided hzd gait generation\\nbipedal walking\\nstable bipedal locomotion\\nrobust bipedal locomotion\\nmanual parameter tuning\\ntrajectory optimization methods\\nextensive tuning\\ngait realization\\nhybrid zero dynamics based optimization\\ndynamically stable walking\\nlearning approach\\ncarefully constructed reward function\\nhuman pairwise preferences\\nrigid point-feet\\ninduced model uncertainty\",\"675\":\"legged locomotion\\ntraining\\nlearning systems\\ntarget tracking\\nautomation\\nsystem dynamics\\nconferences\\ncollision avoidance\\nhumanoid robots\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nrobot dynamics\\ncareful modelling\\nunstable control\\nbipedal locomotion\\nmodel-free reinforcement learning framework\\nrobust locomotion policies\\nbipedal cassie robot\\nsim-to-real transfer\\nlearned policies\\ndiverse behaviors\\ndynamic behaviors\\ntraditional controllers\\nprior learning-based methods\\nresidual control\\nversatile walking\\ntarget walking velocity\\nrobust parameterized locomotion control\\nbipedal robots\\nrobust walking controllers\\nchallenging endeavor\\ntraditional model-based locomotion controllers\",\"676\":\"measurement\\nautomation\\nheuristic algorithms\\nmotion segmentation\\ndelay effects\\nconferences\\ndynamics\\nhuman-robot interaction\\nimage motion analysis\\nimage segmentation\\npost-processing experimental data\\nimitation episodes\\nsocial robot zeno\\nhuman subjects\\nhuman movement\\nhuman reaction time\\nrobot stimuli\\nhuman-robot imitation\\nonline algorithm\\nmotion similarity measurements\\ncyclic human motions\\nsocial interaction\\nhuman-robot motion similarity\\ntime delay\\nrobot imitation\\nsegment-based online dynamic time warping algorithm\\nadaptive rehabilitation interventions\\nhuman motion speeds\\nhri\\nsodtw algorithm\",\"677\":\"visualization\\nautomation\\nconferences\\ntask analysis\\nrobots\\ncognition\\nmobile robots\\ntelerobotics\\naudio elements\\ncognitive workload\\nthreat-defusal task\\nmemory test task\\nprimary tasks\\nsecondary tasks\\nvisual channels\\nauditory channels\\ntask completion time\\nsplitting interface modalities\\nmultiple channels\\nmultiple resource theory design principles\\nrobot teleoperation\\nworkload management\\nrobot interfaces\\nwickens multiple resource theory\",\"678\":\"time-frequency analysis\\nautomation\\ndelay effects\\nconferences\\nforce feedback\\nforce\\ndynamics\\nstability\\ntelerobotics\\ntime-domain analysis\\ntime-domain passivity-based controller\\noptimal two-channel lawrence telerobotic architecture\\nteleoperation leader-follower systems\\ntransparency problems\\nfollower dynamics\\nthree-channel lawrence architectures\\nwave-variables\\ntransparency deterioration problems\\noptimal derivation\\ntwo-port time-domain passivity stabilizer\\none-port passivity stabilizer\\ntwo-channel derivation\\nlawrence architecture\\ntwo-port time domain passivity approach\\nvelocity errors\\ntime delays\\none-port time-domain passivity control\",\"679\":\"autism\\nautomation\\nconferences\\nmedical treatment\\nvirtual reality\\nrobots\\nhandicapped aids\\nhuman-robot interaction\\nmedical computing\\nmedical disorders\\nmedical robotics\\npatient rehabilitation\\npatient treatment\\ntelerobotics\\ntherapists design robot-mediated interventions\\nteleoperate robots\\nvr\\nasd\\nassistive robots\\nrobotics researchers\\nkinesthetic-based interface\\ninperson interventions\",\"680\":\"performance evaluation\\nultrasonic imaging\\nmedical devices\\npandemics\\nforce feedback\\nimaging\\nhazards\\nbiomedical equipment\\ncontrol engineering computing\\ndiseases\\nend effectors\\nhealth care\\nmedical image processing\\nmedical robotics\\nhealth-care workers\\nclinician-to-patient contacts\\nphysical distancing\\nconceptual design\\nsafe distance\\nthree-dimensional cartesian space\\npassive weight-compensating design\\ndynamic manipulability\\nforce feedback quality\\nultrasound probe\\nhigh-quality images\\nlow physical demand\\nmental demand\\nlow-cost intrinsically safe mechanism\\ncovid-19 pandemic\\ncross-infection hazard\\nsafe remote service delivery platform\",\"681\":\"wearable computers\\nconferences\\nsenior citizens\\nrobot vision systems\\ncollaboration\\nsensors\\nconvolutional neural networks\\naccelerometers\\ngeriatrics\\nneural nets\\nobject detection\\nvideo signal processing\\nwearable device\\ncompanion robot communicates\\ncollaborative fall detection method\\npreliminary detection\\nvideo-based final detection\\nvideo-based fall detection\",\"682\":\"visualization\\nautomation\\nminimally invasive surgery\\ninstruments\\nvisual servoing\\nreliability\\ntask analysis\\nlearning (artificial intelligence)\\nmanipulators\\nmedical robotics\\nsurgery\\ntelerobotics\\nrobot encoder estimates\\nivs\\nrepetitive surgical tasks\\ncoarse open-loop policy\\nlaparoscopic surgery peg transfer task\\nautomated surgical peg transfer\\nhigh-precision surgical manipulation\\nhysteresis\\nvariable tensioning\\ncable-driven robots\\nsurgical instruments\\nclosed-loop policies\\nvisual feedback\\ndeep intermittent visual servoing\",\"683\":\"energy consumption\\nrobot kinematics\\narms\\npredictive models\\ntrajectory\\nsynchronization\\nfloors\\nartificial limbs\\nhandicapped aids\\nhuman-robot interaction\\nmedical robotics\\nmotion control\\noptimal control\\nsynchronisation\\nnatural limbs\\ncrawling support\\nwearable superlimbs\\nhuman-robot synchronization\\nmetabolic cost assessment\\nsupernumerary robotic limbs\\nnatural human crawling\\nhand placement\\nsuperlimbs motion\\noptimal coordination pattern\",\"684\":\"legged locomotion\\nexoskeletons\\npredictive models\\ndata collection\\nprediction algorithms\\nsafety\\ntrajectory\\nbayes methods\\nbelief networks\\ngait analysis\\nhandicapped aids\\nlearning (artificial intelligence)\\nmedical robotics\\nexoskeleton user\\nexoskeleton gait parameters\\nusers\\ngait parameters that most influenced user feedback\\ngait utility landscapes\\nhuman trials\\nroial\\ncharacterizing exoskeleton gait preference landscapes\\nexoskeleton gaits\\nwalking trajectories\\nnumerous gait parameters\\nuser safety\\nensures safety\\nordinal preference feedback\\nreliable feedback mechanisms\\nabsolute numerical scores\\nlower-body exoskeleton\",\"685\":\"legged locomotion\\nknee\\ndamping\\ntrajectory tracking\\nmarket research\\nimpedance\\ntuning\\ncontrol system synthesis\\ngait analysis\\nhumanoid robots\\nmotion control\\nposition control\\nprincipal component analysis\\nprosthetics\\nrobot dynamics\\ncontinuous impedance parameters\\nnonlinear impedance parameters\\ntransfemoral prosthesis\\ncomplex control problem\\nnumerous tuning parameters\\nsloped walking control scheme\\nimpedance control\\ncontinuous impedance functions\\nnonlinear impedance functions\\ndesign impedance controllers\\ngiven slope angle\\nhealthy sloped walking data\\nsloped terrain\",\"686\":\"trajectory tracking\\ncomputational modeling\\ndynamics\\nforce\\nestimation\\nstability analysis\\nhardware\\nasymptotic stability\\nlyapunov methods\\nmedical control systems\\nnonlinear control systems\\noptimisation\\nprosthetics\\nrobot dynamics\\nunknown human dynamics\\nprovably stable prosthesis\\nres-clf\\nmodel-based optimization-based controllers\\nnatural dynamics\\nprosthesis trajectory\\nmodel-dependent prosthesis control\\ninteraction force estimation\\nlower-limb prosthesis control methods\\nheuristic tuning parameters\\nmodel-dependence\\nprosthesis controllers\\nexponentially stabilizing control\",\"687\":\"training\\nmonte carlo methods\\nstochastic processes\\ngrasping\\nreinforcement learning\\nelectromyography\\nrobustness\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\ndexterous manipulators\\nhuman-robot interaction\\nemg\\nad hoc manner\\nend-to-end training\\nreaching trajectories\\nimitation learning\\nstochastic simulation environment\\nhuman trajectories\\nmonte carlo simulation method\\nexpert policy data\\nrl policy roll-out\\ndeep policy works\\nend-to-end grasping policies\\nhuman-in-the-loop robots\\ndeep reinforcement learning\\nstate-of-the-art human-in-the-loop robot grasping\\nelectromyography inference robustness issues\",\"688\":\"covid-19\\nautomation\\nconferences\\nmanipulators\\nnoise measurement\\ntask analysis\\nrobots\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nreal-time systems\\nautonomy approaches\\nconfidence-based shared autonomy\\nsituational confidence assistance\\nlifelong shared autonomy\\nuser intent\\nunhindered task execution\\nrobot repertoire\",\"689\":\"automation\\nconferences\\nend effectors\\ndexterous manipulators\\ngrippers\\nhumanoid robots\\nmotion control\\nslip\\nconstrained object\\nnonrigid grasp\\nrotate relative\\nend effector\\norientation slip strategy\\nnatural human demonstrations\\nprehensile orientation slip\\nconstrained interactions\\nhierarchical model selection method\\nphysics-based evidence\",\"690\":\"laparoscopes\\ndeep learning\\nvisualization\\nminimally invasive surgery\\ntaxonomy\\nneural networks\\ndata models\\ncontrol engineering computing\\nimage segmentation\\nlearning (artificial intelligence)\\nmedical computing\\nmedical robotics\\nrecurrent neural nets\\nsurgery\\nintraoperative events\\ntemporal neural network models\\nstate-of-the-art segmentation techniques\\nsurgical phase recognition\\nsufficient statistics model\\nlong-term sufficient statistics\\ntask-specific network representation\\ntemporal network structure\\nunderlying per-frame visual models\\ndeep learning techniques\\nhigh-risk phases\\nspecific keys\\nsurgeon\\ncomplete surgical workflow\\nsurgical assistance robots\\nrobot-assisted surgical workflows\\nlaparoscopic\\nlong-term context\\nlaparoscopic surgery\\nrobot-assisted surgery\\nwork flow recognition\\ntemporal context aggregation\",\"691\":\"learning systems\\nadaptation models\\nautomation\\nconferences\\nreinforcement learning\\nreal-time systems\\nsafety\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nsafe hierarchical planning framework\\ncomplex driving scenarios\\nautonomous vehicles\\ntraffic conditions\\nefficient decisions\\nmaneuvers\\nmotion planner\\nsafe trajectories\\ninteractive vehicles\\nend-to-end learning methods\\nhierarchical behavior\\nlow-level safe controllers\\nhigh-level reinforcement learning algorithm\\nlow-level controllers\\nadaptive behavior planner\\nefficient behavior planner\",\"692\":\"training\\nvisualization\\nheuristic algorithms\\nurban areas\\nreinforcement learning\\nplanning\\nsafety\\ndriver information systems\\nlearning (artificial intelligence)\\nroad traffic\\nvehicles\\nurban intersections\\nhierarchical reinforcement learning\\nautonomous vehicles\\neffective behavior planning\\nego car\\nurban scenarios\\nsufficiently general heuristic rules\\nautonomous vehicle behavior planning\\nhierarchical structure\\nsimulated urban environments\\nheuristic-rule-based methods\\naggressive expert human drivers\\ntraditional rl methods\",\"693\":\"conferences\\nneural networks\\nswitches\\npredictive models\\nencoding\\nautonomous automobiles\\ncollision avoidance\\nfeedback\\npredictive control\\nremotely operated vehicles\\ntwo-car collision avoidance scenario\\nstrategy-guided approach\\nmodel predictive control baseline\\ntightly-constrained environments\\nhierarchical control approach\\nautonomous vehicle\\nhuman driven vehicles\\ntwo-level hierarchy\\nhigh-level data-driven strategy predictor\\nlower-level model-based feedback controller\\nstrategy predictor maps\\ndynamic environment\\nhigh-level strategies\\nselected strategy\\ntime-varying hyperplanes\\ncorresponding halfspace constraints\\nhorizon controller\\nstrategy-dependent constraints\\npredicted strategy\\ndiscrete set\\ndata-driven hierarchical control framework\\nav position space\",\"694\":\"potential energy\\nperturbation methods\\nconferences\\nredundancy\\nmanipulators\\nsteady-state\\nimpedance\\nclassical mechanics\\nclosed loop systems\\nmanipulator dynamics\\nmatrix algebra\\nmotion control\\nredundant manipulators\\nvibrations\\nzero-potential-energy motion\\nrobotic tasks\\ninnovative theory\\nanalytical methodology\\nstiffness matrices\\ncartesian impedance control\\nrigid-body mode\\nclassic mechanical systems\\nsteady-state deviation\\ndynamic systems\\nzp motion\\nexperimental validation\\n7 dof panda robot\",\"695\":\"automation\\nconferences\\nmanipulators\\nplanning\\ntrajectory\\nsynchronization\\nnoise measurement\\njava\\nmobile robots\\npath planning\\nrobot vision\\nstatic target\\noff-the-shelf static planner\\ntrajectory forecasting network\\nrobot\\nfrills dynamic planning\\ndynamic environments\\nmoving ball\\nur5 robotic arm\\ndynamic tasks\",\"696\":\"actuators\\nthree-dimensional displays\\ntrajectory tracking\\nsystem dynamics\\ntransportation\\naerospace electronics\\ncameras\\nautonomous aerial vehicles\\nhelicopters\\nposition control\\npredictive control\\nstate estimation\\ntrajectory control\\ninertial measurement unit\\nimu\\ncable suspended payloads\\nsystem manifold configuration space se\\nmonocular camera\\nstate estimation solution\\nquadrotor\\ncable suspended payload trajectory tracking problem\\npcmpc\\nsuspended loads\\nsingle camera\\nperception-constrained model predictive control\\nvehicle motor speeds\\nreceding-horizon control\\ncamera field of view\\nfov constraint\",\"697\":\"legged locomotion\\ntraining\\nlearning systems\\npower demand\\nautomation\\nsimulation\\nconferences\\ndeep learning (artificial intelligence)\\nrobot dynamics\\nteaching\\nmentor\\nlearned behaviors\\nagile behaviors\\nlegged robots\\ndeep reinforcement learning\\ncurriculum design\\nmultistage learning problem\\ncheckpoint\\nlearning system\\nsimulated quadruped robot\\nsingle-stage rl baseline\\nlearning agile locomotion skills\",\"698\":\"learning systems\\nteleoperators\\ntelepresence\\nautomation\\nconferences\\nreinforcement learning\\ntask analysis\\nlearning (artificial intelligence)\\noptimisation\\nsocial robots\\ntelerobotics\\nuser interfaces\\nvirtual reality\\naffective telepresence robot\\ntabletop robot haru\\nteleoperator\\nemotive routines\\ninput modalities\\nspeech-based intent\\nhuman-in-the-loop reinforcement learning mechanism\\nroutine selection process\\noptimal routine behaviors\\nusers facial expression\\nrobots perception system\\nn-best optimal choices\",\"699\":\"navigation\\nconferences\\ncomputational modeling\\nlattices\\nforestry\\nrobot sensing systems\\nplanning\\ncollision avoidance\\nmobile robots\\nsensors\\ntrees (mathematics)\\nfast path computation\\nsensor-space\\nforest navigation\\nfast autonomous motion\\ncluttered environments\\nmotion planning strategy\\nlocal paths\\nglobal robot task\\nsensor space\\nsearch trees\\ndetected obstacles\\nlattice trees\\nvector field-dependent\\nfree local path\\ncluttered forest\\navoid obstacles\\nplanar robot\",\"700\":\"visualization\\nsolid modeling\\nthree-dimensional displays\\nnavigation\\nvolume measurement\\nconferences\\nestimation\\ncameras\\ncollision avoidance\\nimage reconstruction\\ninertial navigation\\nmobile robots\\npath planning\\npose estimation\\nrobot vision\\naccumulated measurements\\n2d bounding box\\nhigher dimensional 3d volumetric model\\nhierarchical object map estimation\\nobject obstacles\\nrobust navigation\\nhierarchical representation\",\"701\":\"training\\nsolid modeling\\nthree-dimensional displays\\nnavigation\\nscalability\\nlayout\\nbuildings\\ncontrol engineering computing\\nhuman-robot interaction\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\npedestrians\\nreinforcement learning\\nrobot vision\\nrl\\nmultilayout training\\nreal-world buildings\\nrobot navigation\\nconstrained pedestrian environments\\nsocial navigation\\nopen spaces\\npedestrian interaction\\nmulti-agent simulation\\n3d reconstruction\",\"702\":\"graphical models\\nautomation\\nevent detection\\nconferences\\nrobot sensing systems\\nsensors\\nmulti-robot systems\\ncontrol engineering computing\\ngraph theory\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\nteam assignment\\nheterogeneous multirobot sensor coverage\\nrobot teams\\nheterogeneous relationships\\ngraph representation learning\\nphysical multirobot system\\nsensing abilities\\ncommunication connections\\nspatial distribution\\nmathematical framework\\nregularized optimization\",\"703\":\"solid modeling\\nthree-dimensional displays\\nground penetrating radar\\ninstruments\\nposition measurement\\ndata models\\nrobustness\\ndeep learning (artificial intelligence)\\nimage reconstruction\\nradar computing\\nradar imaging\\nunderground objects\\ngpr data collection module\\nobject model\\ndnn-based 3d reconstruction module\\ngprnet\\nunderground utility model\\n3d point cloud\\ndense point cloud model\\npipe-shaped utilities\\ngpr raw data\\ngpr-based model reconstruction system\\nnondestructive evaluation instruments\\nutility pipes\\ngpr image-based feature detection\\nsparse gpr measurements\\n3d model\\nrobotic system\\nnde instruments\\nomnidirectional robot\\ndeep neural network\\ndnn migration module\",\"704\":\"training\\nmeasurement\\nheuristic algorithms\\nstochastic processes\\npredictive models\\nprediction algorithms\\ndata models\\nkalman filters\\nlearning (artificial intelligence)\\nmanipulators\\nnonlinear filters\\nrobot vision\\nvideo signal processing\\nreplay overshooting\\nextended kalman filter\\nnonlinear stochastic latent dynamics models\\nlong-horizon prediction\\novershooting methods\\nvariational learning objective\\nro\\nswinging motorized pendulum\\nplanar position prediction\\nmanipulator\\nmit push dataset\\nquantitative metric\\nqualitative metric\",\"705\":\"three-dimensional displays\\nautomation\\ntrajectory planning\\nwind speed\\nconferences\\npipelines\\ntrajectory\\nautonomous aerial vehicles\\nstate estimation\\ntrajectory control\\nfreyja\\nfull multirotor system\\noutdoor flights\\nmultirotor unmanned aerial systems\\nmultirotor uas\\nchallenging outdoor environments\\noptimal methods\\ndynamic paths\\nopen-source\\naerial docking\",\"706\":\"protocols\\nautomation\\nconferences\\nkinematics\\nmanipulators\\nthree-dimensional printing\\nprediction algorithms\\nindustrial manipulators\\nindustrial robots\\nmanipulator kinematics\\nmobile robots\\noptimisation\\npath planning\\nrapid prototyping (industrial)\\ntrajectory control\\nrobot-based additive manufacturing\\nrobotic manipulators\\ntrajectory execution error\\noptimize part placement\",\"707\":\"training\\nautomation\\nsalivary glands\\nsystems operation\\nproduction\\nmanuals\\nthroughput\\nbiological organs\\ndiseases\\npatient treatment\\nrobot vision\\nsalivary gland extraction\\nsystem operation\\nmosquito detection\\nautomated robotic pick-place-decapitate process\\nrobotic system prototype\\nmosquito salivary glands\\nlarge-scale industrial production\\nplasmodium falciparum sporozoites\\npfspz-based malaria vaccine production\\nautomated mosquito salivary gland extractor\\nmalaria vaccine\\ncomputer vision\\nmicro-robotics\\nsoft object\\nmodular control\\nros\\nsimulation\",\"708\":\"fabrication\\nheuristic algorithms\\ntools\\nprogramming\\nneedles\\nsearch problems\\nfabrics\\nclothing industry\\ngroup theory\\nknitting machines\\noptimisation\\nposition control\\ntextile industry\\nyarn\\nsymmetric normal form\\nmachine operations\\ntransfer operations\\ntransfer planning algorithms\\nfabrication time\\nstate-collapsing mechanism\\noptimal transfer plans\\ntransfer planning problems\\nartin braid group representation\\nknitting machine state\\nfabrication plans\\nindustrial knitting machines\\nmanipulating loops\\ncore problem\\npattern making\\nlow-level operations\\nappropriate needles\\ncorrect final structure\\nlarger piece\\nloop position\\nyarn tangle\\ncomplete representation\\ndiscrete representation\\nloop-tangling process\\nexplicit loop positions\\nloop crossings\",\"709\":\"image segmentation\\nvisualization\\nembedded systems\\ninstruments\\nsemantics\\nredundancy\\nsurgery\\nconvolutional neural nets\\nfeature extraction\\nimage representation\\nlearning (artificial intelligence)\\nmedical image processing\\nmobile robots\\nsurgical robots\\nfine-structure\\nvisual guidance\\nautonomous surgery\\nsurgical instrument segmentation\\nsemantic feature extraction\\naggregator-discriminator mechanism\\ncompetitive segmentation accuracy\\nsemantic segmentation\\nsurgical instrument parsing\\ndiscriminative asymmetric learning\\nconvolutional neural networks\\nvisual representation\",\"710\":\"training\\nadaptation models\\ninstruments\\nrobot vision systems\\nsurgery\\npredictive models\\ntask analysis\\nendoscopes\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nvideo signal processing\\nmdal\\nfast adaptation ability\\nvideo-specific meta-learning paradigm\\ndynamic online adaptation\\ntarget videos\\nrobot-assisted suturing\\ncamera control\\nadaptive instrument segmentation\\nmeta learning\\nrobotic surgical video\\nsurgical instrument segmentation\\nrobot-assisted surgery\\nlearning-based models\\ntesting videos\\nsource model\\nmultiple target domains\\nannotated instruments\\ndynamic online adaptive learning scheme\\nmodel parameters\\nmeta learning in robotics\\nonline adaptation\",\"711\":\"training\\ndegradation\\nbridges\\nimage segmentation\\nautomation\\nconferences\\nsemantics\\nmobile robots\\nneural nets\\ntraffic engineering computing\\nunsupervised learning\\ndomain gap\\nsegmentation map level\\nunsupervised semantic segmentation\\nlabeled images\\nsource supervised domain\\nunsupervised domain\\ntarget predictions\\ndomain shift problem\\nsource domain\\nimage-to-image translation\\ntarget-targeted segmentation adaptation\\ntarget-targeted domain adaptation\\nself-driving\\ntarget unsupervised domain\\nweight-sharing network\\nclosed-loop learning\\ngta5\\nsynthia\\ncityscapes\\nimage space alignment\",\"712\":\"solid modeling\\nimage segmentation\\nimage texture\\nthree-dimensional displays\\nconvolution\\nimage edge detection\\nsemantics\\nfeature extraction\\ngraph theory\\nimage classification\\nlearning (artificial intelligence)\\nneural nets\\ndisordered local point clouds\\nnovel joint-edge graph convolution neural network\\ndynamic features\\nlocal area\\nedge information\\nvertex pairs\\nadjacent vertices\\ngraph convolution module\\nhigh classification confidence\\ncentral vertex\\ntexture features\\n2d image features\\nadjacent feature propagation\\nlocal features\\nglobal features\\npoint cloud segmentation\\nedge-fused local graph learning\\ntraditional convolution\\nlocal structures\\nkey technical limit\\n3d semantic segmentation\\nadjacent points\\ncentral point\",\"713\":\"location awareness\\nlaser radar\\nsmoothing methods\\npose estimation\\nwheels\\nkinematics\\ngaussian distribution\\ndistance measurement\\nfeature extraction\\nimage registration\\nmobile robots\\noptical radar\\nremote sensing by laser beam\\nsensor fusion\\ngeometric distribution features\\nnormal distribution features\\nlidar observation\\nmultisensor fusion\\nlidar feature map\\naccurate pose estimation\\nlong-term localization\\nabsolute localization\\nmultisensor measurements\\nfusion method\\njoint cost function\\nglobal lidar registration\\nsize 200.0 km\",\"714\":\"location awareness\\nsimultaneous localization and mapping\\nlaser radar\\ncodes\\nconferences\\npose estimation\\nfeature extraction\\ncombinatorial mathematics\\ngreedy algorithms\\noptical radar\\noptimisation\\nslam (robots)\\nstochastic processes\\ngreedy-based feature selection\\nefficient lidar\\nmodern lidar-slam\\nexpensive data association\\nnonlinear optimization\\nl-slam system\\ncombinatorial optimization problem\\ncardinality constraint\\ninformation matrix\\nstochastic-greedy algorithm\\napproximate the optimal results\\nfeature number online\\nfeature selector\\nmultilidar slam system\\nenhanced system\\nstate-of-the-art l-slam systems\",\"715\":\"location awareness\\ngeometry\\nvisualization\\nthree-dimensional displays\\nsemantics\\npipelines\\nimage retrieval\\naugmented reality\\ndata analysis\\nfeature extraction\\nimage matching\\nimage representation\\nimage segmentation\\nmedical computing\\npattern clustering\\npose estimation\\nvirtual reality\\nobservation constraints\\naccurate visual re-localization\\nartificial intelligence applications\\nrobotics\\nautonomous driving\\nintegrated visual re-localization method\\nrlocs\\nsemantic consistency\\ngeometry verification\\nlocalization pipeline\\ncoarse-to-fine paradigm\\nretrieval part\\nspatial verification\\ngeometry information\\nsemantic segmentation\\ntotal pipeline\\nchallenging localization benchmarks\",\"716\":\"automation\\nconferences\\nmanipulators\\nplanning\\nclutter\\ntask analysis\\ntime complexity\\ngraph theory\\noptimisation\\npath planning\\nobject configuration\\nincorrect placement\\nremoved objects\\noverhand grasps\\ntarget object\\nrobotic manipulator\\ncluttered confined space\\nobject retrieval\\ngoal positions\\nintegrated approach\\ntime 3.0 min\",\"717\":\"tracking\\nconferences\\nforce\\naerospace electronics\\ntrajectory\\ntask analysis\\nmotion control\\nforce control\\nmanipulators\\nposition control\\njoint robots\\nhybrid position-force controller\\nmotion tracking\\nforce regulation\\njoint space\\ntask space\\ntraditional motion controller\\ncontour tracking-polishing surfaces\\nbox grabbing\\nrobotic arms\\nphysics engines\",\"718\":\"training\\nautomation\\nconferences\\nestimation\\nreinforcement learning\\nbenchmark testing\\ntask analysis\\ncontrol engineering computing\\nconvergence\\ndeep learning (artificial intelligence)\\nmanipulators\\nself-adaptive action noise\\ndimsan\\nexploration-exploitation tradeoffs\\nadaptive action space noise\\nsparse rewards\\noff-policy reinforcement learning\\ndeep reinforcement learning\\nintrinsic-reward generation\\ndensity-based intrinsic motivation\\ndrl\\ndensity estimation\\nmanipulation tasks\\nconvergence speed\",\"719\":\"damping\\nvibrations\\nnanopositioning\\nmodal analysis\\nkinematics\\nfasteners\\nbroadband communication\\nbending\\ncouplings\\nfinite element analysis\\nfrequency response\\nhinges\\nmotion control\\npiezoelectric actuators\\ntransient analysis\\ndamped piezo-driven decoupled xyz stage\\nflexure-based mechanism\\nbroadband control performance\\nhysteresis nonlinearity\\nnanopositioning stage\\northogonal parallel kinematic subchains\\nbridge-type piezo-driven actuator\\ncardan joint\\northogonal- axis flexure hinges\\nshearing damping effects\\njoint damping\\ndecoupled dynamic model\\nno-damping\\ndamping scenarios\\ndamping inserts\\nbroadband motion control\",\"720\":\"analytical models\\nautomation\\nconferences\\ntransportation\\nswitches\\nnumerical simulation\\nmechanical factors\\nlegged locomotion\\nmobile robots\\nmotion control\\npath planning\\nrobot dynamics\\nrobot kinematics\\nwheels\\nrhex's mobility\\nrhex-t3\\ntransformable rhex-inspired robot\",\"721\":\"resistance\\nactuators\\nautomation\\ntracking\\nfriction\\nconferences\\nmanipulators\\ndesign engineering\\nelastic constants\\nhuman-robot interaction\\nmanipulator dynamics\\nrigidity\\nvariable stiffness actuated manipulator\\nstiffness tracking\\nvariable stiffness actuator\\nsecond-order lever mechanism\\nmanipulator integration\\nsymmetric structure design\\nstiffness regulation response\\nstiffness regulation range\\nstiffness regulation module load capacity\\nelastic hysteresis\\nshock-absorbing movement\\nexplosive movement\\nlever mechanism\\nmanipulator\\nrobot arm\",\"722\":\"robot motion\\nsequential analysis\\nrobot kinematics\\nconferences\\narms\\nmanipulators\\nplanning\\nhumanoid robots\\nmobile robots\\nmotion control\\npath planning\\nrobot arm\\nhuman arm motion generation\\nrobot motion planning\\nhuman arm motion primitives\\nmotion samples\\nhuman arm motion patterns\\nmotion planning method\\nmotion time allocation\\nmotion planning app\\nhuman arm movement primitive chains\\nanthropomorphic robot arms\\nmotion pattern decisions\",\"723\":\"automation\\ntrajectory tracking\\nconferences\\nforce\\nhumanoid robots\\ncollaboration\\nkinematics\\nactuators\\ndexterous manipulators\\nmanipulator dynamics\\nposition control\\nsynchronisation\\ntracking\\ntrajectory control\\nmodel-free synchronous control\\nhumanoid robot finger\\nrobot hand\\nhigh-precision synchronization\\ncross-coupling control strategy\\nhigh positioning performance\\nsynchronization error\\nadjacent actuator errors\\ndexterous hand\\ncross-coupled trajectory tracking\\nmodel-free trajectory tracking\\nmultifingered robot han\\nmultifingered hand platform hit\\/dlr-ii\",\"724\":\"maximum likelihood estimation\\ncomputer vision\\nuncertainty\\northopedic surgery\\nconferences\\ncomputational modeling\\nprobabilistic logic\\nbone\\nimage registration\\nmedical image processing\\nmedical robotics\\northopaedics\\nsurgery\\ngeneralized point set registration\\nkent distribution\\nbiomedical engineering\\nanisotropic characteristics\\nerror values\\npositional vectors\\norientational vectors\\npss\\npositional uncertainties\\norientational uncertainties\\npsr problem\\nnormal vectors\\nmaximum likelihood estimation problem\\nmatrix forms\\nobjective function\\nassociated gradients\\ncomputational process\\nregistration method\\nregistration accuracy\",\"725\":\"training\\noptical losses\\nlearning systems\\nminimally invasive surgery\\nbrightness\\nestimation\\nmanuals\\nendoscopes\\nfeature extraction\\nimage motion analysis\\nimage representation\\nimage sequences\\nlearning (artificial intelligence)\\nobject recognition\\nsurgery\\nvideo signal processing\\nurban scenes\\nendoscopic videos\\nbrightness variations\\ntraining phase\\noptical flow\\nmotion alignment\\nstructural stability loss\\nresidual-based smoothness loss\\nappearance flow\\nbrightness inconsistency issue\\ninadequate representation learning problem\\ncurrent state-of-the-art self-supervised methods\\nurban videos\\nmonocular depth estimation\\nminimally invasive surgery scenes\\nself-supervised learning algorithms\\ncompute depth map\\nmonocular videos\",\"726\":\"wireless communication\\nlocation awareness\\nmedical robotics\\nendoscopes\\nmedical services\\ninspection\\ntrajectory\\nclosed loop systems\\nmagnetic actuators\\nmedical image processing\\ntracking\\nclosed-loop tracking control strategies\\ncomplex environments\\nsimulation environment\\nex-vivo pig colon show\\nmagnetic actuation method\\nrotating magnetic actuation\\nautomatic trajectory\\nactive wireless capsule endoscopy\\ninspection time\\ntrajectory following task\\nmedical robots and systems\\ntrajectory following\\nrobust control\\nwireless capsule endoscopy\\nmagnetic actuation and localization\",\"727\":\"automation\\nproportional control\\nconferences\\ndynamics\\nwheels\\nbicycles\\nordinary differential equations\\nmobile robots\\nmotion control\\nnonlinear control systems\\nnonlinear differential equations\\nrobot dynamics\\nreduced dynamics\\nnonlinear behavior\\nproportional control law\\ngibbs-appell equations\\nwhipple bicycle\\nsecond-order nonlinear ordinary differential equation\\nhurwitz criterion\\nlinearized equation\\nsteer coefficient\\ncritical angular velocity\\nuniform straight forward motion\\nsymmetrical stable uniform circular motions\\npowered autonomous bicycle\",\"728\":\"damping\\nregulators\\nautomation\\nconferences\\nstability analysis\\nregulation\\nmobile robots\\ncontrol system synthesis\\nfeedback\\nlegged locomotion\\nlinear quadratic control\\nnonlinear control systems\\nstability\\ndamping assignment - passivity\\nphysical experiments\\ncontrol tasks\\nlinear output regulator\\nstanding\\nnonlinear controller\\nchanging robot height\\nnovel wheel-legged robot\\nbalance control technique\\ndynamic model\\nlinear feedback controller\\noutput regulation\\naccount nonlinearities\",\"729\":\"heating systems\\ncodes\\nconferences\\npipelines\\nrobot vision systems\\ngrasping\\nbenchmark testing\\ncameras\\ncodecs\\nconvolutional neural nets\\ndexterous manipulators\\ngrippers\\nimage coding\\nimage colour analysis\\nimage sensors\\nlearning (artificial intelligence)\\npose estimation\\nrobot vision\\nsingle object scenes\\n7-dof grasp poses\\nmonocular rgbd\\ngeneral object grasping\\npoint cloud\\nrgbd-grasp\\ndepth information\\ngrasp point\\ngrasp detection problem\\nstable rgb modality\\nhigh-quality depth image\\ndepth sensor noise\\nrobot experiments\\nur5 robot\\nrobotiq two-finger gripper\\nfast analytic searching module\\nfas module\\nintel realsense camera\\nencoder-decoder\\nangle-view net\\navn\\ngraspnet-1billion dataset\",\"730\":\"jacobian matrices\\ngeometry\\nheuristic algorithms\\nrobot vision systems\\nforce\\naerospace electronics\\ntools\\nasymptotic stability\\nclosed loop systems\\ncollision avoidance\\nforce control\\nlyapunov methods\\nmanipulators\\nmotion control\\nrobot vision\\nrobust control\\ngeneralized constraint\\nbottleneck constraint\\ndynamic controller\\nforce space\\nimage space\\nhybrid vision\\/force control\\nbottle-like object\\nfixed 3d region\\nrobust algorithm\\njacobian matrix estimation\\nclosed loop system\\nlyapunov theory\",\"731\":\"three-dimensional displays\\nrefining\\ngrasping\\nproposals\\nreliability\\nnoise measurement\\nclutter\\ncontrol engineering computing\\ngrippers\\nobject detection\\nposition control\\nregression analysis\\nrobot vision\\nend-to-end grasp detection\\npoint clouds\\nreliable robotic grasping\\noptimal grasp\\nsingle-view point cloud\\nscore network\\nrefine network\\ngrasp anchor mechanism\\nregion-based grasp network\\ngrasp region network\\ngrn\\nregnet\\n3d grasping\\nsn regresses point\",\"732\":\"legged locomotion\\nactuators\\nshape memory alloys\\ninsects\\nprototypes\\nprocess control\\nsoft robotics\\nintelligent actuators\\nmotion control\\nrigidity\\nshape memory effects\\nsoft jumping robot\\ninsect-scale robots\\nterrain adaptability\\nsoft materials\\nstiffness\\nasynchronous sequential releasing strategy\\nelevation controllable jumping\\nprototype robot\\ncontrollable elevation range\\nelectrostatic pads\\njump height\\ntwo-bars catapult mechanism\\nshape memory alloy actuator\\nrelease structures\\nlock structures\\nhalf-distance\\nsize 62.0 mm\\nsize 41.0 mm\\nmass 80.0 mg\",\"733\":\"vibrations\\nanalytical models\\nthree-dimensional displays\\ngears\\nadhesives\\nforce\\nwheels\\nadhesion\\ncompressed air systems\\nflexible structures\\nforce control\\ninspection\\nlegged locomotion\\nlightweight structures\\npipelines\\npipes\\nrigidity\\nsafety\\nshock absorbers\\nthree-dimensional printing\\nvibration control\\nload-free scenarios\\nvibration\\nelastic damper\\ncomplex pipeline network inspection\\nagile motion\\nflexible pipe reconstruction\\nsoft-rigid air-propelled pipe-climbing robot\\nthrust force\\nlocomotion force\\ncontrollable adhesion force\\nlightweight body\\ntractive force\\ncompressed air\\n3d printed wheel mechanism\\nsoft component\\nbulky robots\\nmass 160.0 g\\nvelocity 1.09 m\\/s\\nvelocity 0.828 m\\/s\\nmass 500.0 g\",\"734\":\"motor drives\\nactuators\\nspirals\\nautomation\\nconferences\\ngrasping\\nmechanical factors\\nadaptive control\\nclosed loop systems\\ndesign engineering\\ndexterous manipulators\\nintelligent control\\nmechanical intelligence\\nadaptive precision grasp\\nrobotic systems\\nversatile power grasps\\nunderactuated fingers\\nopen-loop motor control\\nmechanical-intelligent technique\\nspiral caging power\\nunderactuated robot hand\\nhelical hand\\nself-adaptive precision grasping\\nspiral helical power grasps\",\"735\":\"deep learning\\ntraining\\nlearning systems\\nautomation\\nconferences\\nreinforcement learning\\ntrajectory\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\nlong-horizon tasks\\nrl\\ndemonstrations method\\nbi-level hierarchical imitation learning method\\nrobotic manipulation skills learning\\nobject-centered segmentation\\nteaching trajectories\\nparallel training mechanism\\ntwo-level policies\",\"736\":\"codes\\nruntime\\npipelines\\nsoftware algorithms\\nreinforcement learning\\ndetectors\\nresource management\\ndata flow computing\\ndata flow graphs\\ngraph theory\\nmobile robots\\nobject detection\\npath planning\\npipeline processing\\npublic domain software\\nmodular platform\\nlatency-accuracy tradeoffs\\nautonomous vehicle\\nresearch\\nend-to-end driving behavior\\nmodular structure\\nhigh-performance dataflow system\\nav software pipeline components\\npopular av simulators\\nreal-world vehicles\\nentire pipeline\\npylot-based av pipeline\\ncarla autonomous driving challenge\",\"737\":\"solid modeling\\nautomation\\nsimulation\\nconferences\\nreinforcement learning\\nmanuals\\naerospace electronics\\nadaptive control\\ndecentralised control\\ndistributed control\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\nnonlinear control systems\\nnonlinear dynamical systems\\nrobust control\\ncircle formation control problem\\nunderactuated fish-like\\nunknown nonlinear dynamics\\ncognitive consistency theory\\ndecentralized controller\\nestablished simulation environment\\ntrained controller\\nmodel-free robust formation control method\\ndecentralized circle formation control\",\"738\":\"network topology\\nconferences\\nmeasurement uncertainty\\ndecentralized control\\ndynamics\\nkinematics\\nmathematical models\\nadaptive control\\nasymptotic stability\\nclosed loop systems\\ndistributed control\\nforce control\\nmanipulator dynamics\\nmobile robots\\nmulti-robot systems\\nposition control\\ntorque control\\nuncertain systems\\nrendezvous problem\\nresulting closed-loop multirobot systems\\nmultiple wheeled mobile robots\\nnetworked turtlebot3 burger mobile robots\\nnetworked uncertain robotic systems\\nbearing measurements\\ndistributed rendezvous control problem\\nmotion control\\nnetworked robots\",\"739\":\"location awareness\\nmeasurement\\nsimultaneous localization and mapping\\ntracking\\nconferences\\npose estimation\\ncameras\\nimage colour analysis\\nimage reconstruction\\nmobile robots\\noptimisation\\nslam (robots)\\nmultiparameter optimization\\nrobust rgb-d slam system\\nslam systems\\nmetric scales\\ndepth information\\nrgb-d cameras\\nsensing range\\nobjects structure\\nlocalization methods\\nmaximum-a-posteriori estimation\\nmonocular keypoints\\nvalid depth values\\nbundle adjustment\\nscale factor\\nlocal window\\nscale environments\\nexcellent methods\\ntum rgb-d\",\"740\":\"simultaneous localization and mapping\\nuncertainty\\nsimulation\\nconferences\\nprediction algorithms\\nrobustness\\nkalman filters\\nmobile robots\\nnonlinear filters\\nrobot vision\\nslam (robots)\\ninvariant ekf\\nright invariant extended kalman filter\\nimproved ekf slam methods\\nobservability constrained-ekf slam\\noc-ekf\\noptimization based slam algorithms\\nriekf slam algorithm\\nactive slam algorithm\",\"741\":\"measurement\\nindustries\\nservice robots\\ndynamics\\nbenchmark testing\\nplanning\\nsafety\\ncontrol engineering computing\\nmobile robots\\nplanning (artificial intelligence)\\npublic domain software\\nunified benchmark\\nmobile robot local planning benchmark\\nmotion planning researchers\\nmobile robotics industry\\nmrpb 1.0\\nopen-source local planners\\nbenchmark web site\",\"742\":\"casting\\nautonomous underwater vehicles\\nnavigation\\nconferences\\nmarkov processes\\nplanning\\ntrajectory\\ndecision theory\\nmobile robots\\nmotion control\\npath planning\\nbelief space partitioning\\nsymbolic motion planning\\nsymbolic representations\\nplanning problem solving\\npartially observable markov decision process\\npomdp\\ntraditional symbolic planning solvers\\nautonomous underwater vehicle navigation problem\\nmemory-constrained partition\\nk-means partitioning strategy\",\"743\":\"dynamics\\npredictive models\\nsearch problems\\nprobabilistic logic\\nunmanned aerial vehicles\\ntrajectory\\nsensors\\nautonomous aerial vehicles\\nmobile robots\\nmonte carlo methods\\noptimisation\\npath planning\\nremotely operated vehicles\\nservice robots\\nplanning paths\\nautonomous unmanned aerial vehicles\\nfully integrated framework\\nsearch environment\\nbehavior-based predictive model\\nanticipated human searcher trajectories\\nfixed field\\nview sensors\\nuav\\nposterior risk\\nsituational awareness\\nsearch efforts\\nanticipatory planning\\ndynamic lost person models\\nhuman-robot search\\nlost person motion\",\"744\":\"training\\nrecurrent neural networks\\nkinematics\\nsensor fusion\\ndata processing\\nrobot sensing systems\\nprobabilistic logic\\naccelerometers\\ncontrol engineering computing\\ngyroscopes\\ninertial navigation\\nrecurrent neural nets\\nimu data processing\\ninertial aided navigation\\nrecurrent neural network\\ninertial odometry\\ngyroscope\\naccelerometer readings\\nintegrated imu\\ntheoretical performance guarantee\\nobservable imu integration terms\\nnumerical pose integration\\nperformance gain\\nimu kinematic equations\\ndedicated network design\\ntraditional methods\\ndnn methods\\ndnn inertial navigation methods\",\"745\":\"adaptation models\\nsimultaneous localization and mapping\\ncosts\\nautomation\\nconferences\\npredictive models\\nminimization\\nconvex programming\\nfeature extraction\\nimage matching\\nimage segmentation\\nmobile robots\\nslam (robots)\\ntracking\\nconvex geometric distance minimization\\nline-based slam systems\\nline segment tracking method\\nimu-klt line segment prediction\\nline segment tracking performance\\npredicted line segments\\nextracted line segments\\ndescriptor-based line segment tracking algorithms\\nvio system\\npublic datasets\\n\\u21131-norm model\",\"746\":\"location awareness\\nvisualization\\npose estimation\\nbuildings\\nrobot vision systems\\npipelines\\nprobabilistic logic\\ncameras\\nimage texture\\nmobile robots\\nprobability\\nrobot vision\\nservice robots\\nstereo image processing\\nplanar moving robot\\nindoor service robotic applications\\ntextureless areas\\nfrequent human activities\\nindoor environments\\nsparse depth\\ndense map construction\\ntextureless texture scenes\\nrepetitive texture scenes\\nautomatic solution selection mechanism\\ncomplete visual localization pipeline\\npublic real-world indoor localization dataset\\nrobust visual localization algorithm\\nabsolute camera pose\\n3d-2d correspondence\\n2d-2d correspondence\\nprobabilistic analysis\",\"747\":\"location awareness\\nmonte carlo methods\\nestimation\\nrobot sensing systems\\ntires\\ncalibration\\nplanning\\nattitude control\\nmobile robots\\nmotion control\\nposition control\\nroad vehicles\\nsensors\\nvehicles\\nvelocity control\\nwheels\\nvehicle position\\nautonomous driving vehicle\\ncloud source-based hd map building\\nlocalization system\\nonline slip parameter calibration\\ncompensation method\\nlocalization accuracy\\nvehicle tests\\nintegrated localization\\nimu\\nvehicle calibration\\nmaneuver-based calibration method\\nmultiple orientation-based vehicle\\/imu extrinsic calibration\\nmovie-cali\\nmonte carlo simulations\\nnonholonomic constraints\\nnhc\\/wheel speed sensor measurement\\nsideslip angle model\",\"748\":\"three-dimensional displays\\nexoskeletons\\noptimization methods\\nlightweight structures\\nfasteners\\nlinear programming\\ntopology\\nbending\\nbiomechanics\\nelasticity\\ngrippers\\ninjuries\\nmedical robotics\\noptimisation\\npatient rehabilitation\\nwearable robots\\nlightweight structure\\nfinger exoskeletons\\ntopology configuration\\ndesign domain\\nrigid-compliant parallel exoskeleton\\nrigid-compliant finger joint exoskeleton\\ntopology optimization\\nrobotic hand exoskeletons\\nhand functional disability\\nspinal cord injury\\nuser-friendly design\\nhuman motion\",\"749\":\"knee\\nlegged locomotion\\nexoskeletons\\nthigh\\ninterference\\nstairs\\nskeleton\\nactuators\\nbiomechanics\\nelastic deformation\\ngait analysis\\nmedical robotics\\nmotion control\\northopaedics\\npatient rehabilitation\\nsprings (mechanical)\\nwearable robots\\nspringexo\\nknee assistance\\nspring-based exoskeleton\\nknee flexion\\nassisted knee extension\\nnatural joint angles\\nsix-subject study\\nhuman subjects\\nsprings store energy\\nminimal interference\\nspring stores energy\\ncoil spring\\nhuman skeleton\\ncable-driven soft exoskeletons\\ntextile-based soft exosuits\\nrigid exoskeleton\\nnatural motion pattern\\nanatomical joints\\ntraditional rigid exoskeletons\\nnatural leg movement\\nportable spring-based knee exoskeleton\",\"750\":\"weight measurement\\nsimultaneous localization and mapping\\nmeasurement uncertainty\\nbenchmark testing\\nminimization\\nrobustness\\nmotion measurement\\nimage motion analysis\\nimage registration\\nminimisation\\nweighted motion averaging\\nmaximum correntropy criterion\\nmultiview registration problem\\nglobal motions\\nrelative motions\\nfrobenius norm error\\ncorrentropy measure based optimization problem\\nrobust motion averaging method\\nmcc\\nhalf-quadratic technique\\nselection strategy\\nadaptive kernel width\\nrobot mapping\",\"751\":\"location awareness\\nuncertainty\\nthree-dimensional displays\\nsemantics\\ncollaboration\\nprobabilistic logic\\nrobustness\\ncomputational complexity\\nimage matching\\nimage registration\\niterative methods\\nmatrix algebra\\nmobile robots\\noptimisation\\nrobot vision\\nsemantic web\\ntransforms\\nclassical iterative closet point\\ninitial error\\ncurrent semantic matching algorithms\\nhigh computation complexity\\nnovel semantic map matching algorithm\\nconvergence region\\ninitial transformation optimization algorithm\\nprobabilistic registration model\\ninitial transformation matrix\\nrobust semantic map matching algorithm\\nfusing\\nlocal maps\\nrelative localization\\ncollaborative mapping\\nsemantic matching methods\",\"752\":\"measurement\\nuncertainty\\nnavigation\\nneural networks\\nestimation\\ncameras\\nreal-time systems\\ndata mining\\ndistance measurement\\nfeature extraction\\nleast squares approximations\\nmobile robots\\nobject detection\\noptimisation\\npose estimation\\nrobot vision\\nslam (robots)\\naccurate estimation\\nrobust estimation\\nground plane\\nground point extraction algorithm\\nhigh-quality points\\nextracted ground points\\nlocal sliding window\\nleast-squares problem\\nransac-based optimizer\\nrobust optimizer\\nhighly accurate scale recovery\\nlight-weight design\\nmonocular visual odometry\\nplane geometry\\nscale ambiguity\\nfundamental problem\\ntypical solutions\\nloop closure detection\\nself-driving cars\\nconstant height\\nlight-weight scale recovery framework\\nfrequency 20.0 hz\",\"753\":\"force measurement\\nforce\\ntactile sensors\\ngrasping\\nmanipulators\\nreal-time systems\\nsensors\\nadhesion\\nadhesives\\nbiomechanics\\nbiomedical electrodes\\ngrippers\\nrobot vision\\nadaptiveness\\nsensor feedback\\nviko\\nadaptive gecko gripper\\nvision-based tactile sensor\\nrobotic devices\\nintimate contact\\ndeformable sensors\\ngecko grippers\\ncontact state\\nhigh-resolution real-time measurements\\ncontact area\\nadaptive cost\\nintegrated gecko-inspired adhesives\\nsensor surface\",\"754\":\"image segmentation\\nautomation\\nadaptive systems\\nconferences\\ngrippers\\nrobots\\ndexterous manipulators\\nobject detection\\nrobot vision\\nslam (robots)\\npolicy confidence map\\npolicy execution\\npois\\npolicy-oriented instance segmentation\\nambidextrous robot picking\\nparallel-jaw gripper\\nsuction cup\\nrobotic picking system\",\"755\":\"analytical models\\nautomation\\ncomputational modeling\\ncomputational fluid dynamics\\nconferences\\nmarket research\\nhydrodynamics\\nmarine vehicles\\nremotely operated vehicles\\nrobot dynamics\\nwave power generation\\nwave energy-converting efficiency\\nwave cycle\\ncomputational fluid dynamic simulations\\nwave conditions\\nthrust enhancement\\nasymmetric foil\\ntraditional symmetric foil\\nwave-driven unmanned surface vehicle\\noscillating-foils\",\"756\":\"radio frequency\\nvisualization\\nthree-dimensional displays\\nnavigation\\nmachine vision\\nconferences\\nrobot vision systems\\naerospace computing\\nautonomous aerial vehicles\\ncontrol engineering computing\\nfeature extraction\\nimage fusion\\ninertial navigation\\nmicrorobots\\nmobile robots\\npose estimation\\nrobot vision\\nslam (robots)\\nstate estimation\\nrf-referenced monocular vision\\nagile microaerial vehicles\\nhigh-quality performance\\ntextureless scenes\\nvisual loop closure\\nmav state estimation\\nrfsift\\nstate estimator\\ncentimeter-level accuracy\\nvisual measurements\\nrf-sifting algorithm\\n2d visual features\\nrf-visual-inertial sensor fusion algorithm\\nmonocular vision system\\nrobust state estimation design\\nrf localizability\\npose constraints\\nfeature quality\\n3d uwb measurement mapping\",\"757\":\"analytical models\\nautomation\\nconferences\\ngrasping\\nkinematics\\npredictive models\\ntrajectory\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nend effectors\\nfeedforward\\nhelicopters\\nmanipulators\\nmobile robots\\nmotion control\\nposition control\\npredictive control\\ntrajectory control\\nreplaceable robotic arm\\naerial vehicle\\ndifferent robotic arm\\nflexible control scheme\\nquadrotor-based aerial manipulator\\nmoving robotic arm\\nlinear model predictive control\\nfeedforward controller\\naerial platform hovering stably\\naerial grasping\\nuavs\\nmpc\\nfeedforward control\",\"758\":\"location awareness\\nminimally invasive surgery\\nnavigation\\nprediction algorithms\\nreal-time systems\\nrobustness\\ninference algorithms\\ndiagnostic radiography\\nimage segmentation\\nmedical image processing\\nmedical robotics\\nmobile robots\\nobject tracking\\nsurgery\\nendovascular interventions\\nendpoint localization\\nrobot-assisted minimally invasive surgery\\nradiation dose\\nprocedure time\\nreal-time multitask framework\\nfast attention-fused network\\naccurate guidewire segmentation masks\\nlightweight localization network\\nguidewire endpoint position\\nreal-time requirement\\nrobotic navigation framework\",\"759\":\"uncertainty\\nautomation\\nconferences\\nrobot sensing systems\\nrobustness\\nplanning\\nsensors\\ncollision avoidance\\nmarkov processes\\nmobile robots\\npath planning\\nplanning (artificial intelligence)\\nrobots\\ntowards adjoint sensing\\nacting schemes\\nrobust robot plan\\nrobots operating\\nopen environments\\nrobust plans\\nenvironment uncertainties\\npartial observability\\nenvironment states\\ntask achievement\\nrobot task\\npartially observable states\\ntask goal\\nstate dynamics\\npurposeful interactions\\ntight interactions\\nrobot state-changing actuating actions\\nsensor-based observation actions\\nparallel interaction schemes\\nsequential interaction schemes\\ninterleaving task planning approach\\nasa-style plans\\nasa interaction schemes\\nopen environment robot tasks\",\"760\":\"navigation\\ntransfer learning\\nreinforcement learning\\nrobot sensing systems\\ninformation filters\\nrobustness\\npath planning\\ncameras\\ncontrol engineering computing\\nimage colour analysis\\nimage sensors\\nmobile robots\\nrgb images\\nrobust sim2real transfer learning\\nautonomous multiview navigation\\ndeep reinforcement learning system\\nautonomous navigation\\nmap navigation\\nmultibranch control\\nmulticamera setup\\nmultiview perception module\\ndrl\",\"761\":\"image resolution\\nmotion estimation\\nlighting\\nthermal sensors\\ncameras\\nthermal noise\\nsoftware\\ncalibration\\nimage colour analysis\\nimage matching\\nimage reconstruction\\nimage sensors\\ninfrared imaging\\nlearning (artificial intelligence)\\nstereo image processing\\nlong-wave infrared camera\\ninertial measurement unit\\nmultispectral images\\nthermal images\\nsensor resolution\\ndepth images\\ntrajectory evaluation\\naccurate ground-truth camera\\nmotion capture system\\nmultispectral dataset\\nvisible images\\nnovel dataset\\nmultispectral motion estimation systems\\nhandheld multispectral device\\nstandard visible-light camera\\nfrequency 32.0 hz\",\"762\":\"three-dimensional displays\\ntarget tracking\\nsimultaneous localization and mapping\\nimage edge detection\\nmemory management\\nrandom access memory\\ndetectors\\ndistance measurement\\nedge detection\\nimage colour analysis\\ninternet of things\\nmicroprocessor chips\\nmobile robots\\nmotion estimation\\nobject tracking\\noptimisation\\nego-motion estimation\\nodometry techniques\\niot platforms\\nrobotic vehicles\\nhandheld devices\\nresource-constrained mcu-level processors\\nthrifty resource budgets\\nstate-of-the-art edge-based vo\\noptimization framework\\nmemory footprint\\nebvo-oriented lightweight edge detector\\npre-processing stage\\nsparse-to-dense processing scheme\\ntracking stage\\nkey-frame management\\npost-processing stage\\nalgorithmic optimization\\ndedicated quantization scheme\\n3d feature calculation\\nrealistic rgb-d benchmark datasets\\nhigh tracking precision\\ncortex-m7 mcu\\nhigh-quality vo\\nlightweight rgb-d visual odometry\\nresource-constrained iot devices\\nfrequency 216.0 mhz\\nmemory size 512.0 kbyte\",\"763\":\"measurement\\nvisualization\\nsolid modeling\\nthree-dimensional displays\\nautomation\\nconferences\\npose estimation\\ncameras\\nimage matching\\nimage registration\\nmobile robots\\nrobot vision\\nhierarchical camera relocalization\\nvisual database\\nlearned descriptors\\n3d surfel map-aided visual relocalization\\n3d surfel map rendering\\nimage points\\nsurfel reprojection constraints\\nmap points\",\"764\":\"location awareness\\nvisualization\\nsimultaneous localization and mapping\\nwindings\\nrobot vision systems\\nkinematics\\nreal-time systems\\ncameras\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nrobot vision\\nsnake-like robot\\nself-localization\\npantilt compensation method\\nhigh-precision path\\nsnakelike robot\\nvision-based path following\\nhead swinging\\nbody winding\\ncurve parameter compensation path following controller\\nreal-time positioning\\napritag detection\\napriltag plane\\ncamera plane\\nmodeling approximation\",\"765\":\"heuristic algorithms\\ndynamics\\nwheels\\nmathematical models\\ntrajectory\\nmobile robots\\ntask analysis\\ncontrol system synthesis\\nfeedback\\nlegged locomotion\\nlinearisation techniques\\nmotion control\\nnonlinear control systems\\npendulums\\nrobot dynamics\\nvariable structure systems\\ndynamics equation\\ninverted pendulum\\nawip\\n1 rigid links\\npartial feedback linearization\\nsliding mode control\\ninverse dynamics controller\\ndynamics terms\\nwlr model\\nconfiguration transformation task\\noptimization based configuration transformation algorithm\\nminimum location drift\\nwheel-legged robot\\ninverse dynamics control\\nmultilinks configuration\\ninverted equilibrium manifold\",\"766\":\"legged locomotion\\nforce\\nloading\\nprototypes\\nhydraulic systems\\nvalves\\nenergy efficiency\\nactuators\\nergonomics\\ngait analysis\\nlifting\\nrobot dynamics\\nlegged robot payload\\nload-carrying capability\\nlegged robots\\nunpowered hydraulic auxiliary system\\nintra-abdominal pressure\\nspinal discs\\nheavy objects\\nhuman biomechanical phenomenon\\nnovel loading-carrying strategy\\naccumulators\\nordinary powered hydraulic systems\\ncontinuous support force\\nknee joint actuator\\nbent-leg theoretical model\\nbipedal hydraulic-assisted electric leg prototype\\nhydraulic assistance\\ninjurious compressive force\\npassive hydraulic auxiliary system design\",\"767\":\"legged locomotion\\nrobot vision systems\\nkinematics\\ncameras\\nvelocity measurement\\nkalman filters\\nobservability\\ncalibration\\nmobile robots\\nnonlinear filters\\nobservers\\nposition control\\nstate estimation\\nnonlinear observability analysis\\ndiscrete observability analysis\\nhighly time-varying camera measurement noise\\ncassie bipedal robot\\nslippery terrain\\nrobot state estimation\\nslippery environments\\ninvariant extended kalman filter\\nstate estimator\\nlegged robots\\ntracking camera\\nleg kinematic constraints\\nrobot-frame\\nleg kinematics\\nright-invariant observation\",\"768\":\"training\\nautomation\\nfiltering\\nconferences\\nreinforcement learning\\nmaintenance engineering\\nautomobiles\\ndeep learning (artificial intelligence)\\nfiltering theory\\nimage representation\\nmotion capture\\nrobot vision\\ninteraction attention representation\\nknowledge-guided policy learning\\nkgpnet\\nalgebraic connectivity\\nrobot team\\nknowledge-nested policy filtering\\nmultitarget coverage problem\\nknowledge-guided policy network\\nobservation attention representation\\nconnectivity guaranteed coverage policy\\ndeep reinforcement learning\\ndrl\\nmotion capture system\",\"769\":\"costs\\nsimultaneous localization and mapping\\nconferences\\ndistributed databases\\ncollaboration\\nrobot sensing systems\\nboosting\\nmobile robots\\nmulti-robot systems\\npath planning\\nslam (robots)\\nunknown environment\\nmultirobot applications\\ninter-robot positioning\\nmapping systems\\nplace recognition descriptors\\nsensor data\\nrobots\\nmap-based dslam framework\\nexploration efficiency\\nexploration strategy\\nsubmap-based multirobot exploration method\\nexploration framework\\nmultirobot exploration system\\nmultirobot multitarget potential field exploration method\\ncollaborative exploration\",\"770\":\"measurement\\nautomation\\nconferences\\napproximation algorithms\\nsearch problems\\nplanning\\nfuels\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npareto optimisation\\npath planning\\nsearch space\\nsingle-objective multiagent path finding\\nmultiobjective optimization\\npareto-optimal set\\npareto-optimal solutions\\nmultiagent path planners\\npath length\\nmultiobjective conflict-based search\\nfuel consumption\\ncompletion time\\nmo-cbs\\ncurse of dimensionality\",\"771\":\"robot kinematics\\nmedical treatment\\nreinforcement learning\\nkinematics\\nneedles\\npath planning\\nsafety\\nbiological tissues\\nbiomedical mri\\nblood vessels\\nbrain\\ncomputerised tomography\\ndiseases\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nneurophysiology\\nposition control\\nsurgery\\ntumours\\nremote-center-of-motion recommendation\\nbrain needle intervention\\ndeep reinforcement learning\\nspecific diagnosis\\ntherapy procedure\\nbrain disorders\\nbrain tumors\\npreoperative needle path planning\\nneedle intervention robot\\nrigid needle\\npreoperative path-planning\\noptimal remote center\\nneedle insertion\\nrcm recommendation system\\nrl agents\\nremote center of motion (rcm)\\nrecommendation system\\nintervention robot\\ncriteria (coa\\nmik\\nmlm)\",\"772\":\"image quality\\nultrasonic imaging\\nnavigation\\nmagnetic resonance imaging\\nreinforcement learning\\nreal-time systems\\nprobes\\nbiomedical ultrasonics\\nlearning (artificial intelligence)\\nmedical image processing\\nobject tracking\\npneumodynamics\\nautonomous navigation\\nultrasound probe\\nstandard scan plane\\nhighly complex images\\nvariable images\\ndeep reinforcement learning framework\\nvirtual us probe\\nreal-time image feedback\\nconfidence-based approach\\nus imaging\\nreproducible us probe navigation\\nimage quality optimization\\nnavigation performance\\nautonomous ultrasound acquisition\\ndeep reinforcement learning\",\"773\":\"navigation\\nheuristic algorithms\\nknowledge based systems\\ndynamics\\nfiltering algorithms\\nfeature extraction\\ninformation filters\\ncollision avoidance\\ncontrol engineering computing\\nimage filtering\\nimage fusion\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nrobot vision\\nsampling methods\\ntrees (mathematics)\\nheuristic path\\nonline environmental feature learning\\ndynamic mobile robot navigation\\nobstacle avoidance\\ntopological feature tree\\ndual-channel scale filter\\narbitrary obstacle-free grid points\\nenvironmental feature points\\nknowledge-based fast motion planning\\nsampling-based partial motion planning\\nheuristic motion planning\\nonline topological feature learning\\nrisk-rrt\\nrobot motion\\nsecondary distance fusion\\ntrapped environments\",\"774\":\"analytical models\\nminimally invasive surgery\\nmagnetic confinement\\ntransportation\\nphantoms\\nkinematics\\nmagnetic analysis\\nattitude control\\ncoils\\ncontrol engineering computing\\ngrippers\\nmagnetic fields\\nmanipulator kinematics\\nmedical computing\\nmicrorobots\\nposition control\\nrobot vision\\nsurgical robots\\ndirection control error\\nsmall-scale robots\\ndouble-magnet model\\nexternal magnetic driven field\\n3-axis helmholtz-maxwell coil system\\nuniform magnetic field\\nopened angle control error\\nuntethered magnetic gripper modeling\\nuntethered magnetic gripper control\\nmis\\nspindle\\norientation control\\ngripper kinematics model\\nphantom experiments\",\"775\":\"magnetic flux\\nadaptation models\\nmagnetic field measurement\\ntorque\\nmagnetic separation\\ndata models\\npermanent magnets\\ncalibration\\ndensity measurement\\nnumerical analysis\\noptimisation\\nflexible magnetic field mapping model\\nmagnetic source\\neight-pole magnetic manipulation system\\nmagnetic flux density\\ngradient field\\npermanent magnet\\ncartesian coordinates\\nlevenberg-marquardt algorithm\\nlma\\nwhale optimization algorithm\\nwoa\\nnumerical simulation\\npid controller\",\"776\":\"location awareness\\ntarget tracking\\nthree-dimensional displays\\ntrajectory tracking\\nheuristic algorithms\\nmagnetic sensors\\nrobot sensing systems\\nendoscopes\\nmedical robotics\\nmicrorobots\\nsensor arrays\\ndynamic tracking\\nmagnetic source\\nfree energy storage\\ncapsule endoscopy\\nbronchoscopy\\nrobot-assisted examinations\\nmedical microrobotics\\naccurate position feedback\\nactive magnetic sensor array\\nmagnetic tracking\\nexternal robotic arm\\nmovable sensor array\",\"777\":\"radioactive waste\\nfriction\\ntransportation\\ndetectors\\ngrasping\\nnitrogen\\ninductors\\ndosimetry\\nfission reactor coolants\\nfission reactor core control\\nfission reactor design\\nfission reactor instrumentation\\nnuclear power stations\\nrobots\\ncoolant level\\nnuclear plant\\nreactor core detectors\\nhigh-level radiation need\\nnovel robotic system\\nmodular mechanisms\\nremoval process\\nrobotic removal\\nradiation exposure time\\nradiation dose\\nreactor core detector removal\\nreactor power\\ndesign\\ntime 4.0 year to 5.0 year\",\"778\":\"location awareness\\nvisualization\\nautomation\\ntracking\\nroads\\nconferences\\nlighting\\nagricultural robots\\nagriculture\\ndistance measurement\\nimage sampling\\ninverse problems\\nmobile robots\\nobject tracking\\nposition control\\nrobot vision\\nrobust control\\nslam (robots)\\nstereo image processing\\nbumpy roads\\nagricultural environment\\nunstructured scene\\nunstable features\\nillumination variations\\ndynamic environmental objects\\nrobust stereo direct visual odometry system\\nstereo-dso\\nstatic stereo points\\ninverse depth calculation\\nkeyframe determination\\npoint selection method\\nlocal flat ground assumption\\ntracking failure\\nmap points sampling\\noutlier removal\\nvision-based mapping\\nvision-based localization\\naltitude smoothness verification\\nsystem robustness\\nsliding window\\norientation drift\\ncrop row\\nflourish dataset\\nrosario dataset\",\"779\":\"image segmentation\\nlaser radar\\nautomation\\nfuses\\nroads\\nconferences\\nestimation\\ncameras\\ndriver information systems\\nfeature extraction\\nimage fusion\\nobject detection\\noptical radar\\nradar imaging\\nremote sensing by laser beam\\nroad vehicles\\nsensor fusion\\ncascaded lidar-camera fusion network\\nexisting road detection methods\\nlidar-camera based methods\\nnovel lidar-camera fusion strategy\\nlidar point clouds\\nsingle-modal mode\\ncamera data\\nsegments road points\\nlidar's imagery view\\nsparse lidar feature maps\\ndense road detection results\\ndense lidar feature maps\\ndense camera images\\ncompetitive road detection performance\\nlidar-only methods\",\"780\":\"manufacturing processes\\nconferences\\nkinematics\\ngrasping\\nsoft robotics\\nelasticity\\nsteel\\nbending\\ncontrol system synthesis\\ngrippers\\nlightweight structures\\nmobile robots\\nrigidity\\nrobot kinematics\\nversatile soft-rigid robot\\nrigid robot adaptively\\nsoft robot adaptively\\nunstructured environments\\nstiffness-tunable mechanism\\nrobot lightweight\\nflexible body\\nsteel flexure\\ngrasping experiments\\ntwo-fingered gripper\\nkinematic model\",\"781\":\"solid modeling\\ncomputational modeling\\nloading\\nprototypes\\nkinematics\\npredictive models\\nmanipulators\\ncontinuum mechanics\\ndexterous manipulators\\nmanipulator kinematics\\nmean square error methods\\nmotion control\\npiecewise constant techniques\\nvariable cross-section\\ncross-sectional dimension\\nbackbone cross-sectional area\\ntype-2 manipulator\\ntype-1 manipulator\\nkinetostatics\\ndexterity\\npiecewise-constant-curvature\\npcc\\ncontinuum structures\\ncontinuum manipulators\\nmanipulator statics\\nloading situations\\nactuating situations\\nmanipulator profile\\naverage root-mean-square error\\nspacer dimension\\nsize 1.67 mm\\nsize 2.6 mm\\nkinetostatic model\",\"782\":\"adaptation models\\ncomputational modeling\\ndrag\\nforce\\nsoft robotics\\nnumerical simulation\\nhydrodynamics\\nautonomous underwater vehicles\\ndeformation\\ndifferential geometry\\nelasticity\\nmarine systems\\nmicrorobots\\nmobile robots\\nmotion control\\nrobot dynamics\\nomni-directional star-shaped swimming\\nsoft underwater robot\\nefficient numerical framework\\ngeometrically nonlinear deformation\\nsoft materials\\ndiscrete differential geometry-based model\\nsoft limbs\\nfluid model\\nsingle thread desktop processor\\nnumerical simulation tool\\nsoft robot\\nmultiple swimming gaits\\nrobot design\\nmodel-based control schemes\",\"783\":\"training\\nviscosity\\nanalytical models\\nfriction\\nneural networks\\ntraining data\\nrobot sensing systems\\ngait analysis\\ngradient methods\\nlearning (artificial intelligence)\\nlegged locomotion\\nmotion control\\nneural nets\\noptimisation\\nquadratic programming\\nrobot dynamics\\nsimulation parameters\\nobserved sensor readings\\nnovel differentiable rigid-body physics engine\\nneural network\\ndynamic quantities\\ntraditional simulators\\naugmentation\\nentirely data-driven models\\nhybrid simulator\\ncomplex dynamics\\nmatch known models\\nuseful augmentations\\nbenefiting dynamics\\nmodel-based control architectures\\nmodel-predictive gait controller\\naugmenting differentiable simulators\\nsim-to-real gap\\ngradient-based optimization algorithms\",\"784\":\"training\\ninterpolation\\ntemperature distribution\\ncodes\\nautomation\\nconferences\\nreinforcement learning\\ncontrol engineering computing\\ndata analysis\\ndeep learning (artificial intelligence)\\nmobile robots\\ntrajectory control\\ntrajectory data collection\\npolicy-irrelevant discrete transitions\\ncontinuous transition\\ntrajectory information\\nconsecutive transitions\\ncomplex continuous robotic control problems\\nmodel-free rl methods\\ncontinuous control problems\\ndeep reinforcement learning\\nrobotic control tasks\\nreal-world tasks\\nsample efficiency\\nlinear interpolation\",\"785\":\"automation\\nconferences\\ncomputer crashes\\nsoftware\\nrobustness\\nsynchronization\\ntask analysis\\ncheckpointing\\ncontrol engineering computing\\noperating systems (computers)\\nrobot programming\\nsystem recovery\\ncrash recovery\\nrobot software programs\\nrobot system\\ncrashed program\\nrestart method\\nrory\\nros programs\\ncheckpoint-replay method\",\"786\":\"image segmentation\\ncomputer vision\\nautomation\\nconferences\\nsemantics\\nobject detection\\ncomputational geometry\\nexpectation-maximisation algorithm\\nimage colour analysis\\nimage registration\\nimage sensors\\nopen rgbd datasets\\nsemantic region association likelihood inference\\nregistered rgb image\\ninstance-level semantic segmentation\\npoint set segmentation\\ngeometric problem\\npoint set registration process\\ncascaded expectation-maximization\\nhierarchical association framework\\npoint-pair associations\\nsemantically annotated parts\\nrobotic applications\",\"787\":\"location awareness\\nsimultaneous localization and mapping\\nautomation\\nfuses\\nconferences\\nfeature extraction\\nliquid crystal displays\\ndistance measurement\\ngraph theory\\nmobile robots\\nneural nets\\nobject detection\\npose estimation\\nrobot vision\\nslam (robots)\\nnovel lcd algorithm\\nmotion knowledge\\nflexible detection strategy\\nefficient detection strategy\\nflexible combinations\\nefficient combinations\\nglobal binary feature\\nhand-crafted local binary feature\\ncontinuous motion model\\nmotion states\\nvisual-inertial odometry system\\nlocalization errors\\nstate-of-the-art lcd algorithms\\nloop closure detection\\nessential module\\nlong-term explorations\\nbag-of-words model\\nlow time consumption\",\"788\":\"location awareness\\nvisualization\\nsemantics\\nproduction\\nrobot sensing systems\\nsensors\\nautomobiles\\nautomotive engineering\\ncameras\\ncartography\\ndata visualisation\\ndriver information systems\\nglobal positioning system\\nmobile robots\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\nvehicles\\nlight-weight semantic map\\nvisual localization\\nautonomous driving tasks\\nsensor-rich vehicles\\nhigh-accurate sensors\\nhigh-resolution map\\nlow-cost production cars\\nhigh expenses\\nlow-cost cars\\nlight-weight localization solution\\nlow-cost cameras\\ncompact visual semantic maps\\nsemantic elements\\non-vehicle mapping\\nuser-end localization\\nmap data\\ncrowd-sourced data\\nmultiple vehicles\\nreliable localization solution\\npractical localization solution\",\"789\":\"location awareness\\nvisualization\\nroads\\nsemantics\\nlighting\\nfeature extraction\\nrobustness\\ndistance measurement\\ngraph theory\\nmobile robots\\nobject detection\\noptimisation\\nrobot vision\\nslam (robots)\\ntraffic engineering computing\\nhd map\\nautonomous vehicles\\nurban scenarios\\naccurate localization ability\\nrobust localization ability\\nvision-based methods\\nsemantic features\\nmissed detections\\nfalse detections\\nrobust da method\\nlocal structural consistency\\nglobal pattern consistency\\ntemporal consistency\\nsliding window factor graph optimization framework\\nhigh-precision absolute height information\\nmap features\\nreal urban road\\nvisual semantic localization algorithm\\nodometry measurements\\ndata association\",\"790\":\"training\\nvisualization\\nimage segmentation\\nsimultaneous localization and mapping\\nimage edge detection\\nroads\\nsemantics\\nedge detection\\nfeature extraction\\nimage denoising\\nrobot vision\\nslam (robots)\\ntraffic control\\nincorrect edges\\nincomplete edges\\nconsecutive edges\\nlocal map\\nsemantic edge point cloud map\\noccupancy grid map\\nparking garage\\nmapping solution\\nautomated valet parking task\\nsemantic slam framework\\nhybrid edge information\\nuseful edges\\nfree-space contours\\nslam task\\nsegmentation methods\\nnoisy glare edges\\ndistorted object edges\\ninverse perspective mapping\\nview synthesis\\nfree-space segmentation model\\nsynthesized bird's eye view images\\nvision-based localization\\nhybrid bird's-eye edge based semantic visual slam\\nroad marking based methods\",\"791\":\"visualization\\nsimultaneous localization and mapping\\npose estimation\\ncollaboration\\ninformation sharing\\nrobustness\\nreal-time systems\\naugmented reality\\ndistance measurement\\ngroupware\\nhuman-robot interaction\\nios (operating system)\\nmobile computing\\nmobile robots\\nmulti-agent systems\\nslam (robots)\\nsmart phones\\ncollaborative visual inertial slam\\nmultiple smart phones\\nmultiuser ar interaction\\nrobust communication\\nlocation detection\\nrobust mapping\\nios mobile devices\\ncentralized architecture\\nvisual-inertial odometry module online\\nmobile devices\\ncollaborative slam\",\"792\":\"sequential analysis\\nportable computers\\nsurveillance\\nconferences\\nmobile agents\\ngames\\napproximation algorithms\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\nplanning (artificial intelligence)\\nsearch problems\\ntravelling salesman problems\\nsequence goals\\nmultiagent problem\\nms\\nnew exact algorithm\\nmultiagent applications\\ngoal locations\\nmultiagent planning problem\\nconflict-free paths\\nmultiagent path\",\"793\":\"simultaneous localization and mapping\\nthree-dimensional displays\\nlaser radar\\nautomation\\nimage edge detection\\nconferences\\nbenchmark testing\\ndistance measurement\\nfeature extraction\\nimage matching\\nlearning (artificial intelligence)\\nmobile robots\\noptical radar\\nrobot vision\\nslam (robots)\\nhybrid geometric primitives\\nlarge-scale environments\\ncomprehensive inertial aided 3d lidar slam system\\nlidar-inertial-odometry\\nglobal mapping module\\nlearning-based loop closure detection\\nexplicit plane features\\npoint features\\nraw point cloud\\nlidar keyframes\\nlocal map\\neffective loop closure detection\\nlearning-based point cloud network\\nreal-time plane-driven sub-maps matching algorithm\",\"794\":\"switches\\npredictive models\\nsafety\\ntrajectory\\nplanning\\nautomobiles\\ncollision avoidance\\nformal verification\\nmulti-agent systems\\nreachability analysis\\nmultiple driving modes\\nreachability-based safety controller\\ntrajectory prediction\\ncorresponding safety controller\\nprediction-based reachability method\\ninteracting cars\\ncar\\nautonomous driving\\nserious injury\\nhamilton-jacobi reachability\\nformal method\\nmultiagent interaction\\nworst-case assumption\\nprediction-based reachability framework\",\"795\":\"automation\\nroads\\nconferences\\noptimal control\\ntrajectory\\nplanning\\nproposals\\nmobile robots\\noptimisation\\nroad safety\\nroad traffic\\nroad traffic control\\nroad vehicles\\ntraffic engineering computing\\nlane-free autonomous intersection management\\nbatch-processing framework integrating reservation-based\\nplanning-based methods\\nmultiple connected vehicles\\nautomated vehicles\\nunsignalized intersection\\nconventional aim\\ncentralized optimal control problem\\nconcerned lane-free aim scheme\\nintractably scaled problem\\nsmall-scale ocp\\nreservation-based method\",\"796\":\"uncertainty\\nnetwork topology\\nhandheld computers\\nroads\\nsurveillance\\nimage edge detection\\ndecision making\\ncollision avoidance\\ncomputational complexity\\nmobile robots\\npath planning\\nremotely operated vehicles\\npheromone-diffusion-based conscientious reactive path planning\\nroad network persistent surveillance problem\\nrpsp\\nunmanned ground vehicle\\nugv\\nweighted viewpoints\\neffective decision making\\ncomputation efficiency\\npheromone-diffusion-based conscientious reactive persistent surveillance\\npd-crps\\nsurveillance effect\\npheromone release\\nreactive architecture\\nworst-case computational time complexity\\nexisting cognitive architecture method\",\"797\":\"image resolution\\nfuses\\nsemantics\\nnetwork architecture\\ndata aggregation\\nfeature extraction\\ndata mining\\nconvolutional neural nets\\nimage colour analysis\\nimage fusion\\nkitti depth completion benchmark\\nmultimodal deep aggregation network\\nsparse depth data\\nvanilla convolutional neural network\\nmultimodal deep aggregation block\\ninput depth mapping\\neffective pre-completion algorithm\\nmultimodal signal input information\\nmdanet\\nrgb imaging\",\"798\":\"geometry\\nautomation\\nshape\\nfiltering\\nannotations\\nconferences\\ngrasping\\ncollision avoidance\\ncontrol engineering computing\\ndexterous manipulators\\ngrippers\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nobject detection\\noptimisation\\nrobot vision\\nlocal grasping area\\nunstable grasp configurations\\nfine-tuning low-quality grasps\\nfiltering noisy grasps\\n6-dof grasp\\ncollisionless grasping\\ncluttered scenes\\nsingle-view point cloud\\ndense grasp configurations\\nprecise grasp configurations\\nsynthetic single-object grasp\\ncomplex multiobject cluttered scene dataset\\npoint clouds\\ngrasp pose refinement network\\nobject grasping\\nwidely investigated field\\ncurrent works focus\\nestimating grasp\\nefficient single-shot grasp\\ndetection network\\nrobot manipulation\\nyumi irb-1400 robot\",\"799\":\"training\\nautomation\\ncosts\\nservice robots\\nmachine vision\\nrobot vision systems\\nprototypes\\nfeature extraction\\ngabor filters\\nimage resolution\\nimage sampling\\nlearning (artificial intelligence)\\nrobot vision\\nvisual perception\\ncpienet\\none-shot learning framework\\nsupport-query sample pairs\\nunpaired online public images\\nobject categories\\nsingle-pixel wide contour\\nprecise measurement\\nimage contour based vision measurement\\nobject-agnostic vision system\\nobject-agnostic vision measurement\\nrobotic object contour measurement dataset\\nobject contour primitives\\ncpi extraction task\\ncontour primitive of interest output\\ngabor-filters based nonmaximum suppression\",\"800\":\"shafts\\nminimally invasive surgery\\nautomation\\ninstruments\\nconferences\\nprototypes\\nkinematics\\nactuators\\ndesign engineering\\ndexterous manipulators\\nend effectors\\nmanipulator kinematics\\nmedical robotics\\nmotion control\\nposition control\\nsprings (mechanical)\\nsurgery\\nmodular reconfigurable mini-robotic system\\nminimally invasive procedures\\nmodular mini-robotic system\\nspring-spherical joint mechanism\\nmagnetic spherical joints\\ntraditional spherical joint\\nmagnetic joint connections\\nend platform\\ncontrol system\\n3-dof positioning platform\\n2-dof rotational end-effector\\nfast on-site assembly\\nbowden-cable\\nflexible shaft\\nmanipulator side\",\"801\":\"motion planning\\nwireless communication\\nminimally invasive surgery\\nattitude control\\nmagnetomechanical effects\\nrobot vision systems\\nreinforcement learning\\nclosed loop systems\\ndamping\\nmedical robotics\\nmotion control\\nmuscle\\nposition control\\nsurgery\\nflexible-joint robotic camera system\\nsingle incision laparoscopic surgery\\nnovel magnetic actuated flexible-joint robotic surgical\\n4-dof\\nmotion decoupling\\nmafrs system\\nexternal driving device\\nmotor-free insertable wireless robotic device\\nhollow flexible joint\\nabdominal wall obstruction\\nabdominal wall thickness\\nmultiple permanent magnets\\nmagnetically conductive media\\nhigh- precision position\\ninsertable device\\nonboard motors\\nmagnetic field\\ninternal robotic device\\nautomatic precise tilt motion control\\nmafrs camera system\\nclosed-loop control scheme\\ndeep deterministic policy gradient algorithm\\ndifferent abdominal wall thicknesses\\nmotion control accuracy\",\"802\":\"legged locomotion\\nactuators\\nrobot kinematics\\nenergy conversion\\nmuscles\\nelectrical stimulation\\npulse width modulation\\nbiomechanics\\nbiomimetics\\nhumanoid robots\\nmicrorobots\\nmobile robots\\nmotion control\\nmuscle\\nneuromuscular stimulation\\nrobot dynamics\\nmuscular stimulation\\nbiological actuator\\nlocust\\nactive research field\\npowerful kicking motion\\nkicking process\\nflexion\\nextension motions\\nexogenous stimulation\\nincrease jumping power\\nbiological jumping actuator\\nbilateral hindlegs\\njumping control\",\"803\":\"self-assembly\\nautomation\\nnavigation\\nsimulation\\nconferences\\ndispatching\\nplanning\\ncollision avoidance\\nmobile robots\\nself-adjusting systems\\nmodular robotics\\nassembly process\\ndocking actions\\ngrid-world simulation environment\\nparallel self-assembly planning\",\"804\":\"monte carlo methods\\nuncertainty\\ncomputational modeling\\nscalability\\nrobot kinematics\\ncomputer architecture\\npath planning\\ncollision avoidance\\ngaussian processes\\nmobile robots\\nmulti-robot systems\\noptimisation\\nleader-follower architecture\\ntypical scenarios\\nmultirobot team\\nspatial map\\nspatial correlation\\ngaussian process\\nreal-world constraints\\ntime budget\\nmodel team\\njoint informative path planning problem\\nconvex containment region\\nmonte carlo simulation\\ndistinct sampling locations\\norienteering problem\\ncollision-free path maximizing information gain\\nteam-level adaptive replanning criterion\\nredirecting sampling\\ninformative regions\\nmap estimation\\nmultirobot informative path planning\",\"805\":\"navigation\\nrobot kinematics\\nconferences\\ncollaboration\\nrobot sensing systems\\nprobabilistic logic\\nrobustness\\nautonomous aerial vehicles\\nmobile robots\\npath planning\\ntelerobotics\\nhigh-level coordination strategy\\nlow-level goal-oriented navigation\\nhybrid robots\\ntightly-coupled perception\\nunstructured environments\\nperception-navigation\\ncomplex scenarios\\nheterogeneous land-air robots navigation\\nfully integrated approach\\nflexible probabilistic map fusion algorithm\\nuav-ugv hybrid system\",\"806\":\"visualization\\nuncertainty\\nconferences\\nrandom access memory\\nreinforcement learning\\nmarkov processes\\nmanufacturing\\nindustrial manipulators\\nrobotic assembly\\ntorque control\\nproactive action visual residual reinforcement learning\\ntorque-controlled robot\\ncontact-rich manipulation\\nrobot controller design\\nhaptic information\\ntarget uncertainty problems\\npartially observable markov decision process problem\\npolicy learning\\nindustrial robots\\nassembly tasks\",\"807\":\"codes\\nautomation\\nshape\\ndesign methodology\\nconferences\\npose estimation\\nfitting\\nhough transforms\\nlearning (artificial intelligence)\\nleast squares approximations\\nregression analysis\\n6dof pose estimation network\\nparametricnet\\ntemplate keypoints\\nparametric shape templates\\nparametric dataset\\nsymmetric shape\\nleast-square fitting\\nsileane dataset\\npoint-wise regression\\nhough voting scheme\",\"808\":\"space vehicles\\nschedules\\nscalability\\nurban areas\\noptimal scheduling\\ndynamic scheduling\\nreal-time systems\\nmobile robots\\noptimisation\\nscheduling\\nsearch problems\\ntransportation\\nincremental search algorithm\\nlowest-cost schedule\\nride-sharing trip\\nreduced search space\\nsystem scalability\\noptimal online dispatch\\nurban transportation\\nhigh-capacity ride-sharing vehicles\\nhigh-capacity shared autonomous mobility-on-demand systems\\noptimal schedule pool assignment approach\\niterative online re-optimization strategy\\nnew york city taxi data\",\"809\":\"location awareness\\nnavigation\\ntracking\\nservice robots\\nvelocity control\\nmaintenance engineering\\nsensor fusion\\nautomatic guided vehicles\\nbar codes\\ndistance measurement\\nqr codes\\nbarcode navigation\\nmagnetic ruler\\nadjacent barcode\\nmagnet spot localization accuracy\\nhigh-precision navigation\\nautomated guided vehicles\\ndamaged barcodes\\nimproved magnetic spot navigation approach\\nhigh-precision magnetic tracking method\\nodometer\\nquick response codes\\nindustrial logistics\\nrobot speed control\\nagv-based industrial logistics\\nagv encoders\\nlateral measurement\\nlow-precision longitudinal measurement\\nmean path accuracy\\ndistance 500.0 nm to 1000.0 nm\",\"810\":\"training\\nvisualization\\ntarget tracking\\ncorrelation\\nlighting\\nbenchmark testing\\nunmanned aerial vehicles\\nautonomous aerial vehicles\\ncontrol engineering computing\\nfiltering theory\\nmicroprocessor chips\\nobject tracking\\nregression analysis\\nsingle cpu\\ndual regression\\nimage illumination variation\\ncf-based tracker\\nlow-light image enhancer\\nanti-dark function\\ncorrelation filter-based tracking methods\\nreal-time anti-dark uav tracking\\ntarget-aware dual filter learning\\nuav tracking\\nadtrack\\ntarget-focused filter\",\"811\":\"training\\nresistance\\nvisualization\\ntarget tracking\\ncorrelation\\nsensitivity\\ninformation filters\\nautonomous aerial vehicles\\ncorrelation methods\\nimage filtering\\nobject tracking\\nrobot vision\\ntarget appearance\\ntracking failure\\ndiscriminative correlation filter based trackers\\nappearance mutations\\nadaptive hybrid label\\nuav tracking commissions\\nmutation sensitive correlation filter\\nreal-time uav tracking\\nunmanned aerial vehicle\\nvisual tracking\\nmutation threat factor\\nmscf tracker\",\"812\":\"visualization\\ncasting\\nautomation\\nconferences\\nsemantics\\nrefining\\nbenchmark testing\\nautonomous aerial vehicles\\nimage representation\\nlearning (artificial intelligence)\\nobject detection\\nobject tracking\\nremotely operated vehicles\\nsiamese anchor proposal network\\nhigh-speed aerial tracking\\nvisual tracking\\ndeep learning-based trackers highlight\\naside efficiency\\nreal-world deployment\\nmobile platforms\\nunmanned aerial vehicle\\ntwo-stage siamese network-based method\\nhigh-quality anchor proposal generation\\nanchor-based methods\\nnumerous pre-defined fixed-sized anchors\\nno-prior method\\nadaptive anchor generation\\nanchor numbers\\nanchor-free methods\",\"813\":\"computer vision\\nautomation\\nconferences\\ncomputational modeling\\nreinforcement learning\\ntask analysis\\npositive affect\\nhuman learning\\nextrinsic rewards\\nintrinsically motivated learning\\ntask-independent reward function\\nspontaneous smile behavior\\ndownstream computer vision tasks\\naffective rewards\\naffect-based intrinsic rewards\",\"814\":\"convolutional codes\\ntraining\\nautomation\\nconferences\\npose estimation\\nsemantics\\nspatial resolution\\ncomputer vision\\nfeature extraction\\nimage motion analysis\\nimage resolution\\nlearning (artificial intelligence)\\nobject detection\\nmultilevel network\\nmultiperson human\\noccluded keypoints\\nnovel multilevel\\nestimation network\\nmultilevel features\\nstrong semantic clues\\nkeypoint prediction\\nmultilevel prediction network\\nfeature enhancement strategy\\nhigh-resolution fine network\\nhigh spatial resolution information\",\"815\":\"adaptation models\\ncomputational modeling\\nlayout\\npredictive models\\nprobability distribution\\ntrajectory\\ntask analysis\\ndata mining\\ngraph theory\\nprobability\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\nmatches intersection elements\\nmap-centric feature space\\nopen-set intentions\\ncurrent intersection\\nsimulated dataset\\nreal-world intersections\\nintersection intention prediction\\nautonomous driving\\nhuman driver\\ndifferent intersections\\nunseen intersections\\nirregular intersections\\nopen-set prediction problem\\ncontext specific matching\\ntarget vehicle state\\ndiverse intersection configurations\\ncapture map-centric features\\nintersection structures\\nspatial-temporal graph representation\\nmutually auxiliary attention module\\nrespectively lane-level\\nexit-level intentions\",\"816\":\"computer vision\\nautomation\\nmotion estimation\\nquaternions\\nconferences\\npose estimation\\nrobot vision systems\\nalgebra\\ncameras\\nmatrix algebra\\noptimisation\\nrotation matrix\\ncamera motion estimation\\nrelative pose estimation\\ngeneral elimination strategy\\nmotion parameters\\nessential matrix\\nhomography matrix\\nsingular constraint\\ntrace constraints\\nsimple technique\\ngeneral technique\\ncomplete algebraic constraints\",\"817\":\"deep learning\\nneural networks\\nmedical services\\nhuman factors\\nsensitivity and specificity\\nfeature extraction\\nrobot sensing systems\\nbrain\\ncognition\\ndiseases\\nelectroencephalography\\nlearning (artificial intelligence)\\nmedical disorders\\nmedical image processing\\nmedical signal processing\\nneurophysiology\\npatient diagnosis\\nrobots\\ngroup feature learning\\namci diagnosis system\\nmedical diagnostic robot systems\\nmild cognitive impairment\\nalzheimer's disease\\ndoctors diagnose mci\\nclinical examinations\\nrobot diagnostic system\\nhigher accuracy rate\\nnovel group feature domain adversarial neural network\\ngf- dann\\namnestic mci diagnosis\\nimportant modules\\ngroup feature extraction module\\ngroup- level features\\nadversarial learning\\ndual branch domain adaptation module\\ntarget domain\\ndomain adaption way\\ngf-dann\\nclassic machine learning\\namci diagnose robot system\",\"818\":\"laser radar\\nthree-dimensional displays\\nfeature extraction\\ncameras\\nsensor systems\\nreal-time systems\\ncalibration\\ndistance measurement\\nimage sensors\\noptical radar\\nstereo image processing\\nline-based automatic extrinsic calibration\\nlidar\\nreal-time extrinsic parameters\\n3d light detection\\nranging\\nmultimodal perception systems\\nextrinsic transformation\\ndecreased accuracy\\nperception system\\nline-based method\\nline feature\\npoint clouds\\naccurate extrinsic parameters\\nrobust geometric features\\ncalibration approach\",\"819\":\"geometry\\nadaptation models\\nanalytical models\\nsensitivity\\nparameter estimation\\ncomputational modeling\\ntransforms\\ngradient methods\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmotion control\\nrobot programming\\nriemannian incremental learning\\nrobot body schema\\nclassical robot inertial parameter identification\\nonline learning problem\\ngradient descent techniques\\nfirst-order principles\\ndifferential geometry\\nriemannian gradient descent\\nvirtual manipulator\\nparameter feasibility\\nril\\nmanipulator\",\"820\":\"training\\nthree-dimensional displays\\nnatural languages\\nfeature extraction\\nmanipulators\\nspatiotemporal phenomena\\ntask analysis\\ndecoding\\nhumanoid robots\\nintelligent robots\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\npath planning\\nrobot programming\\nvideo signal processing\\n2d residual network\\n3d residual network\\nspatio-temporal features\\nextended manipulation dataset show\\nbaxter robotic arm\\nvideo demonstrations\\nrobot manipulations\\nhuman demonstration videos\\nmanipulation skills\\nintelligent robotic systems\\nend-to-end approach\\nrobot plans\\ngeneral video captioning methods\\nmanipulation tasks\",\"821\":\"uncertainty\\nthree-dimensional displays\\nhuman-robot interaction\\ntransportation\\nmachine learning\\nrobustness\\ntrajectory\\ncollision avoidance\\niterative methods\\nlearning (artificial intelligence)\\nlearning systems\\nmanipulators\\noptimisation\\npath planning\\nilc\\nphysical human-robot interaction\\nlearning tracking\\nhuman partner\\nadam optimization algorithm\\niteration learning control\\npath learning method\\nreference waypoints\\n7-dof sawyer robot\",\"822\":\"convolutional codes\\nadaptation models\\nthree-dimensional displays\\npose estimation\\nsemantics\\nkinematics\\nskeleton\\nconvolutional neural nets\\nimage motion analysis\\nimage sequences\\nlearning (artificial intelligence)\\nspatiotemporal phenomena\\nvideo signal processing\\nlocal kinematic connections\\nlocal information\\nglobal spatial information\\nattention mechanisms\\nmultiframe estimation\\ndilated temporal model\\ninterleaved temporal convolutional attention blocks\\ngraph attention blocks\\nhalf upper body estimation\\nspatio-temporal information\\nocclusion\\ndepth ambiguity\\n3d human pose estimation\\ntemporal contexts\\nlocal-to-global architectures\\nfixed-length spatiotemporal information\\nspatiotemporal sequences\\nhuman skeleton\\ngraph attention spatio-temporal convolutional network\\nyoutube videos\\n2d-to-3d video pose estimation\\ngast-net\",\"823\":\"force measurement\\nforce\\nmeasurement uncertainty\\nmuscles\\nmanipulators\\nfeedback control\\nelectron tubes\\nbiomechanics\\nbiomedical materials\\ncellular biophysics\\nfeedback\\nmedical robotics\\nmicromanipulators\\nmuscle\\ntissue engineering\\nviscoelasticity\\nforce stimulation\\nmuscle cell alignment\\nfunctional muscle tissue\\nexternal mechanical stimulation\\nvision-based microrobotic manipulation system\\nautomatic mechanical stimulation\\nuniaxial mechanical stimulation\\nsingle muscle fiber-like cell structures\\nc2c12 myoblasts\\nmyoblasts proliferation\\nmyoblasts differentiation\\nmfcs\\ndegree-of-freedom manipulator\\ndof manipulator\\nviscoelastic property\",\"824\":\"wireless communication\\nvisualization\\nneuroscience\\nrobot kinematics\\nolfactory\\nswarm robotics\\nvehicular ad hoc networks\\nmicrorobots\\nmobile robots\\nmulti-robot systems\\nrobot vision\\ncomplements current swarm robotics platforms\\npheromone communications\\nvisual interaction\\nbio-robotics\\ncontrollable dynamic visual environment\\nmultipheromones platform\\nmicromobile robots\\nvisual sensor\\nwireless communication system\\ninsect-vision\\nswarm robotics research\\nmultiple pheromone communication\\ndynamic visual scene\\ntime data transmission\\nmultiple-robots\\nsocial insect behavior\\nolfactory cues\\nvisual cues\\nvision-pheromone-communication platform\\nmicrorobot agents\\nreal-time bidirectional data\\nled panels\\nmultipheromone platform\\ncolcos\\u03c6\",\"825\":\"coils\\nthree-dimensional displays\\ntracking\\nmagnetic cores\\nrobot sensing systems\\nreal-time systems\\niron\\nfeedback\\nmagnetic fields\\nmedical robotics\\nmicrofabrication\\nmicromanipulators\\nmicrorobots\\nmobile robots\\nsolenoids\\n3d periodic magnetic servoing system\\nmicrorobot actuation\\ndecoupled asynchronous repetitive control approach\\nuntethered microrobots\\ntypical actuation\\nuntethered tiny robots\\nmagnetic field-based approaches\\nrotational methods\\ngradient type method\\nrotational approach\\nfield strength\\nefficient actuation\\nmagnetic microrobots\\nprecise periodic magnetic field\\nprecise magnetic field control\\ndecoupled asynchronous repetitive control scheme\\ndesirable 3d periodic magnetic field\",\"826\":\"costs\\nheuristic algorithms\\ncomputational modeling\\nconferences\\nrefining\\ngenerative adversarial networks\\npath planning\\nmobile robots\\nneurocontrollers\\noptimal control\\ntrees (mathematics)\\nheuristic generation\\nrobot path planning\\nrecurrent generative model\\noptimal path\\nrgm model\\ngeneral generative adversarial networks\\nrgm module\\ngan\\nrapidly-exploring random tree star\\nrrt*\\n2d environments\",\"827\":\"robot kinematics\\nscalability\\nclustering algorithms\\npath planning\\niterative algorithms\\npartitioning algorithms\\nbatteries\\nautonomous aerial vehicles\\nconcave programming\\nmobile robots\\nmulti-robot systems\\nnearest neighbour methods\\nuav\\ncpp algorithm\\ncomputationally scalability\\nmission completion time\\ncomputational time\\npost-flood assessment application\\nmultiunmanned aerial vehicle\\npath planning procedure\\ndiscretized space\\nload-balanced partitioning\\nnearest neighbor path\\ncomputationally efficient neighbor path\\ndiscrete neighbor path\\nmultirobot system\\nworkload balanced plans\\ntime-efficient solution\\nscopp\\nmultirobot coverage path\\nnonconvex areas\\nscalable coverage path planning\",\"828\":\"solid modeling\\ncosts\\nprediction algorithms\\nenergy efficiency\\ntrajectory\\nplanning\\nsafety\\naerospace robotics\\naircraft landing guidance\\ncollision avoidance\\nenergy conservation\\nmulti-agent systems\\noptimisation\\npredictive control\\nremotely operated vehicles\\ntrajectory control\\nmultiagent constellation changes\\nmultiple agents\\nplanning algorithm\\nreduced collision potential\\ntrajectory approximation\\nmodel predictive control\\npotential fields\\nflight energy optimization\\nminimum snap trajectories\\nenergy-efficiency trajectories\\nenergy optimized trajectory generation\\ntime optimized trajectory generation\\ndrones\",\"829\":\"codes\\nautomation\\nconferences\\nkinematics\\nbidirectional control\\nprobabilistic logic\\ntrajectory\\ncollision avoidance\\nmobile robots\\npath planning\\nrobot kinematics\\nsampling methods\\ntrees (mathematics)\\nonline rrt-based path planning algorithm\\nackermann-steering vehicles\\nonline path planning algorithm\\nkinematically-feasible paths\\ndense environments\\nnarrow passage\\nkinematically constrained rrt-based path\\ntrajectory parameter space\\ntp-space\\nrrt-based algorithm\\nkinematic constraints\",\"830\":\"three-dimensional displays\\ngaussian noise\\nmeasurement uncertainty\\nfocusing\\nprocess control\\nglass\\nsearch problems\\nbiocontrol\\ncurve fitting\\nmicromanipulators\\nobject detection\\nposition control\\nrobot vision\\nservomechanisms\\nvisual servoing\\nsearch strategy\\ndynamic curve fitting\\nlocal extremum issues\\nplanar positioning algorithm\\nvisual servo control\\nhill-climbing search method\\nfibonacci search method\\ntip positioning algorithm\\nin-plane tip positioning\\nintracytoplasmic sperm injection\\nicsi\\nmale infertility\\nthree-dimensional positioning\\nglass injection micropipette\\noocyte membrane\\nautomatic system\\nmicropipette positioning\\nmicroscopic positioning\\nfocus algorisms\\nrandom gaussian noise\\nreal-time in-plane tip positioning\\nmicromanipulation system\",\"831\":\"visualization\\nthree-dimensional displays\\nautomation\\nmicroinjection\\nveins\\nfluorescence\\npath planning\\nbiological techniques\\nbiomems\\nblood vessels\\ncancer\\ncellular biophysics\\ndrugs\\ntumours\\ninjection system\\n20 zebrafish larvae\\nblood circulation system\\ndanio rerio\\nimportant model organism\\nbiomedicine\\ndrug discovery\\ncirculatory system\\nconventional exposing administration\\nrobotic cardinal vein microinjection system\\ninjection pipette\\n3d path planning\",\"832\":\"sociology\\nneurons\\nhuman-robot interaction\\nsensor phenomena and characterization\\nmuscles\\nrobot sensing systems\\nencoding\\nbiomechanics\\nelectromyography\\nmedical signal processing\\nmuscle\\nneural nets\\nneurophysiology\\nphysical human-robot interaction\\nbipolar myoelectric sensor-enabled human-machine interface\\nspinal module activation\\nsurface electromyography signal-based human-machine interface\\nbipolar myoelectric sensors\\nglobal semg features\\nbipolar semg signals\\nintention recognition\\nwearable robotics\\nprosthetics and exoskeletons\",\"833\":\"training data\\nprediction algorithms\\nrobustness\\nreal-time systems\\nnonlinear dynamical systems\\ntrajectory\\nmobile robots\\nnonlinear control systems\\nrobust control\\nwheels\\nextended dynamic mode decomposition\\nrobot control strategy\\ndata-driven systems\\nnonholonomic wheeled robot\\nnonlinear system modeling\\nnonholonomic mobile robot control\\nkoopman operator-based data-driven mobile robotic system robustness\\ngazebo\\nvan der pol oscillator\",\"834\":\"attitude control\\nwind speed\\nrobot kinematics\\ninterference\\npropulsion\\ntools\\nreal-time systems\\ncollision avoidance\\nmarine propulsion\\nmobile robots\\nunmanned surface vehicles\\nautonomous sailing robots\\nautonomous surface vehicles\\noceanvoy\\nlong range energy-saving voyage\\nenvironmental interference\\nreal-time collision risk assessment\\nhybrid propulsion system\\nsailboat safety zone\\nssz\\npotential collision\\nenvironmental factors\\nsail\\nobstacle avoidance control\\naxial thrusters\\nlateral thruster\\nemergency propulsion\",\"835\":\"spirals\\nautomation\\nconferences\\npath planning\\ntask analysis\\nrobots\\nload modeling\\nmobile robots\\nmulti-robot systems\\ntrees (mathematics)\\nterrain traversability\\nmaterial load capacity\\nspiral-stc based mcpp methods\\nmultirobot coverage path planning\\nmultiple robots\\nefficient algorithm mstc\\nspiral spanning tree coverage\\nstrict physical constraints\",\"836\":\"legged locomotion\\nfriction\\nwires\\ndynamics\\ndogs\\nnonlinear dynamical systems\\nimpedance\\ncompensation\\ncompliance control\\nfeedforward\\nforce control\\nlinear motors\\nmotion control\\nposition control\\nrobot dynamics\\nsprings (mechanical)\\ndynamic legged robots\\nnonlinear active compliance control\\nstable locomotion\\nhigh-speed dynamic locomotion\\nleg locomotion system\\nactive impedance control\\nsteel wire transmission-based legged robot\\ndeveloped control system\\nexcellent impact mitigation\\nleg position\\nfeed-forward controller\\nlinear motor-leg model\\nideal virtual spring compliance characteristics\\nimpact mitigation performance\\nlinear scheme\\nguarantee position tracking performance\\nideal impact mitigation ability\\nimpact mitigation\\nactive compliance control\\nsteel wire transmission\\nlegged robot\",\"837\":\"location awareness\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nconferences\\nsemantics\\nfocusing\\ndetectors\\ncameras\\ncomputer vision\\nmobile robots\\nobject detection\\nslam (robots)\\ntracking\\nobject landmarks\\ncubic surface\\nquadric surface\\n2d object bounding boxes\\nobject detector\\nbounding box noises\\ninaccurate 3d object landmark inference\\ndual quadric enveloping property\\nquadric representation\\nquadric parameters\\nrelatively accurate inference\\nrobust improvement\\nsemantic mapping\\nsemantic simultaneous localization\\nrobustness\\nslam\\nobject reconstruction\\ndual quadric\",\"838\":\"training\\nthree-dimensional displays\\nlaser radar\\ncomputational modeling\\nrobot vision systems\\ngraphics processing units\\nobject detection\\ncameras\\nimage matching\\nimage reconstruction\\nmobile robots\\nrobot vision\\nstereo image processing\\nvision detection\\nbinocular images\\nstereo features\\nlight-weight stereo matching module\\nyolostereo3d\\nstereo 3d detection\\nstereo cameras\\ncomputer vision\\nlow-cost autonomous mobile robots\\ndense depth reconstruction\\ndisparity estimation\\n2d image-based detection frameworks\\ninference structure\\nreal-time one-stage 2d-3d object detector\\ngpu\",\"839\":\"target tracking\\nbiological system modeling\\natmospheric modeling\\nconferences\\nforce\\ndynamics\\nshock absorbers\\naerospace control\\nvariable structure systems\\nvehicle dynamics\\nvibration control\\nbioinspired skyhook damper model\\ngravitational force\\ninertial force\\ncarrying loads\\nimpact force\\nhuman body\\nenergy expenditure\\ncause injury\\nsemiactive hover backpack\\ncontrollable air damper\\npractical backpack\\nsliding mode control\\ncontrol method\",\"840\":\"automation\\nconferences\\neducation\\nphysical layer\\nfast light\\nmathematics\\ntrajectory\\nart\\ncomputer aided instruction\\nmulti-robot systems\\nstem\\nuser interfaces\\nui layer\\ncommand layer\\nsteam education\\nfast light show design platform\\ndrone swarm light show design platform\\nplatform contents\\nk-12 children\\nscience-technology-engineering-art-and-mathematics education\\neasy-to-use interface\",\"841\":\"solid modeling\\nreinforcement learning\\ngames\\ndata models\\nautomobiles\\nvehicle dynamics\\ntask analysis\\ncomputer games\\ndrives\\nlearning (artificial intelligence)\\noptimisation\\nroad vehicles\\nsport\\nmodel-based game ai\\ncurriculum reinforcement learning\\nprofessional race-car drivers\\nextreme overtaking maneuvers\\nsimplified assumptions\\nexpensive trajectory-optimization problems\\nmodel-based controllers struggle\\nhighly nonlinear dynamics\\nlearning-based method\\nautonomous overtaking problem\\npopular car racing game gran turismo sport\\ncars\\nleveraging curriculum learning\\nvanilla reinforcement learning\",\"842\":\"mechanical sensors\\nshafts\\nanalytical models\\nkinematics\\ntransforms\\nfiber gratings\\nrobot sensing systems\\nangular measurement\\nbending\\nbiological tissues\\nbiomedical mri\\nbragg gratings\\ncalibration\\nclosed loop systems\\nfibre optic sensors\\nmedical image processing\\nmedical robotics\\nposition control\\nrotation measurement\\nmr safe encoders\\nclosed-loop robotic control\\nmr safe absolute rotary encoder\\neccentric sheave\\nfbg sensors\\nstrain model\\nfiber bragg grating sensors\\npseudorigid body 3r\\naccurate rotary encoding\\nmr safe robots\\nmr safe rotary encoder\\nmri-guided robotic systems\\nminimally invasive intervention\\nhigh positioning accuracy\\nexcellent tissue contrast\",\"843\":\"performance evaluation\\nlaser radar\\nthree-dimensional displays\\nhandheld computers\\nscalability\\nrobot sensing systems\\nmobile robots\\ngraph theory\\nimage colour analysis\\nimage reconstruction\\nimage resolution\\noptical radar\\npose estimation\\nrobot vision\\nslam (robots)\\nlarge-scale exploration tasks\\nefficient d lidar reconstruction framework\\nelastic 3d lidar reconstruction framework\\nrobot exploration\\nlarge-scale reconstruction\\nlong-range lidar scans\\nsubmap fusion feature\\nsubmapping technique\\nrgb-d volumetric reconstruction technique\\nmaximum lidar ranges\\npose graph clustering\\nmobile robot\",\"844\":\"laser radar\\ncorrelation\\nautomation\\nconferences\\npose estimation\\nfeature extraction\\ndistortion\\ndistance measurement\\nmatrix algebra\\nmotion estimation\\noptical radar\\nlightweight lidar inertial odometry\\nfeature-based lidar odometry methods\\nlow computational cost\\nkey-feature selection\\nkfs-lio\\nlidar feature constraints\\nfeature distribution\\nstate errors\\nquantitative evaluation method\\nlidar constraints\\nlidar motion distortion\\nloam features\\nstate-of-the-art odometry\",\"845\":\"visualization\\nsimultaneous localization and mapping\\nlaser radar\\ncodes\\nautomation\\nconferences\\nhardware\\ncalibration\\ncameras\\noptical radar\\nrobot vision\\nslam (robots)\\nautomatic lidar-camera calibration method\\ncamvox\\nlidar-assisted visual slam system\\ncamera-based simultaneous localization\\noutdoor large scale scenes\\nlow-cost lidars\\nlivox lidar\\nslam systems\\norb-slam2\\nlidar slam\\nloam\\nvins-mono\",\"846\":\"laser radar\\nheuristic algorithms\\nroads\\nsemantics\\nbenchmark testing\\nsurface fitting\\nreal-time systems\\nconvolutional neural nets\\ndistance measurement\\nimage matching\\nimage registration\\nmobile robots\\nmotion estimation\\noptical radar\\ntraffic engineering computing\\npsf-lo\\nparameterized semantic features\\nmapping systems\\nautonomous driving\\ngeometric information\\npoint cloud registration\\npoint cloud semantic information\\nsemantic lidar odometry method\\nlow-drift ego-motion estimation\\nautonomous vehicle\\nconvolutional neural network-based algorithm\\npoint-wise semantics\\ninput laser point cloud\\nsemantic labels\\ntraffic sign\\npole-like point cloud\\nfast psf-based matching\\ngefs matching\\nsemantic point cloud\\npublic dataset kitti odometry benchmark\\ninstance-level method\\nself-designed parameterized semantic features\\ngeometric feature registration\",\"847\":\"learning systems\\ntraining\\nautomation\\nnavigation\\nconferences\\nreinforcement learning\\npredictive models\\ngraph theory\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\ntraffic engineering computing\\nactor network\\ncritic network\\nnavigation policy\\nstate-of-the-art algorithm\\ncrowd navigation\\nmodel-based reinforcement learning method\\nmodel prediction\\nnavigation learning method\\npedestrian state prediction network\\nrelation features\\npedestrians\\ngraph convolutional network\\ngeneralization ability\\ndifferential-drive mobile robots\\nrelational graph\\ncontinuous action space\\nrelational navigation learning\\nstate-of-the-art rl methods\",\"848\":\"learning systems\\nuncertainty\\nconferences\\ncomputational modeling\\nprobabilistic logic\\ndata models\\nsafety\\ncontrol engineering computing\\ndriver information systems\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nprobability\\nroad traffic control\\nsafety-critical software\\nexplicit safety guarantees\\ndata-driven methods\\nhuman agents\\nprobability 1- \\u03b4\\nsafety-critical applications\\nhuman driving data\\nhuman behavior\\nsafety-critical systems\\nprobabilistic safety guarantees\\nhuman uncertainty\\nautonomous robot interaction\\nautonomous driving\\nlearning probabilistic bounds\",\"849\":\"uncertainty\\nautomation\\nconferences\\nneural networks\\npredictive models\\nbenchmark testing\\nprobabilistic logic\\nbelief networks\\ncontrol engineering computing\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nprobability\\nprobabilistic human motion prediction\\nbayesian neural network\\nsafe human-robot-interaction systems\\nhuman motion prediction algorithms\\ndeterministic models\\nconventional deterministic motion prediction neural network\\nfuture motions\\nobserved motion sequence\\nrobot-interaction systems\\nepistemic uncertainty\\nheteroscedastic aleatoric uncertainty\\nhri\",\"850\":\"directed acyclic graph\\nlimiting\\nneural networks\\ndynamics\\nhuman-robot interaction\\npredictive models\\nlogic gates\\ndirected graphs\\nmultilayer perceptrons\\npose estimation\\nrecurrent neural nets\\nsensor fusion\\ntemporal update operator\\nhuman motion prediction\\ndirected acyclic graph neural network\\ndirected edges\\nencoder-decoder structure\\nmultiple encoder blocks\\ndirected acyclic graph computational operator\\ngated recurrent unit\\nmultilayered perceptron\\nambient sensors\",\"851\":\"training\\ntracking\\nshape\\nveins\\nconferences\\nprediction algorithms\\nrobustness\\nfeature extraction\\nhidden feature removal\\nimage colour analysis\\nimage matching\\nimage motion analysis\\nimage segmentation\\nimage sensors\\nmobile robots\\nmotion estimation\\nobject detection\\ncurrent shape\\ndeformable object\\nhigh-fidelity physics simulation\\ngeometric motion estimates\\nsevere occlusion\\ntracking estimate\\nconvex geometric constraints\\nenforcing geometric constraints\\nunstructured environments\",\"852\":\"visualization\\ntarget tracking\\ncomputational modeling\\nbenchmark testing\\nfeature extraction\\nrobustness\\nspatiotemporal phenomena\\nautonomous aerial vehicles\\ncomputer vision\\nimage representation\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\nobject tracking\\nrobot vision\\ntracking\\nvideo signal processing\\nrecommended feature maps\\nweighted sum\\ntarget percepts\\npre-trained cnn backbone\\ntracking process\\nscale changes\\nspatiotemporal-based min-channel method\\ntarget size variation\\ncnn features\\nstate-of-the-art cnn-based trackers\\nscale adaptation\\nonline recommendation-based convolutional features\\nscale-aware visual tracking\\nonline learning-based visual tracking framework\\nscale variation\\nrecommender-based tracker\\ndiscriminative target percept\\nrecommended layer\",\"853\":\"bridges\\ntraining\\nvisualization\\nuncertainty\\ntraining data\\nprobabilistic logic\\nprobability distribution\\nbayes methods\\ndeep learning (artificial intelligence)\\nimage representation\\nobject tracking\\nregression analysis\\nrobot vision\\nstatistical distributions\\nconditional variational autoencoder\\nembedded deep learning model\\ndeterministic features\\nregression maps\\ndata uncertainty\\nprobabilistic siamese visual tracking\\ncvae\\nsiamese architecture\\nbayesian visual tracking method\\nlow-dimensional latent space\\nrobots\\ncomplete probability distribution\\nvot2016 dataset\\nvot2018 dataset\\ntcolor-128 dataset\\nnoise-injection training\",\"854\":\"sensitivity\\ntracking\\nlasers\\nimaging\\ngraphics processing units\\nendomicroscopy\\nstreaming media\\nbiological tissues\\nbiomedical optical imaging\\ndeformation\\nendoscopes\\nimage reconstruction\\nimage registration\\nimage resolution\\nimage segmentation\\nlaser applications in medicine\\nmedical image processing\\noptical microscopy\\nlens tissue paper\\ntissue highlight\\nexcellent mosaicking accuracy\\nmosaic-king rate\\npcle imaging systems\\naccurate mosaicking\\ntoward intraoperative endomicroscopy\\ngpu-accelerated deformable video mosaicking algorithm\\nprobe-based confocal laser endomicroscopy imaging system\\nvideo mosaicking approaches\\ntissue deformations\\nintensity fluctuations\\nnovel pcle mosaicking algorithm\\nsimultaneously implements rigid probe motion tracking\\ninter-frame tissue deformation correction\\nconditional variance\\nlow calculational complexity\\nintensity variation invariance\\npcle mosaics\\nscv metric\\nparallel acceleration mechanism\\nmosaicking efficiency\\nprobe-based confocal laser endomicroscopy\\nvideo mosaicking\\nreal-time\\nscv\\ngpu\",\"855\":\"vibrations\\nmilling\\ntools\\nharmonic analysis\\nbones\\nacoustics\\ntrajectory\\nbiomechanics\\nbone\\ncutting\\nfast fourier transforms\\nmachine tool spindles\\nmedical robotics\\nsurgery\\nrobotic-assisted laminectomy\\npreoperatively planned feed rate\\ndepth direction\\nrobot cutting trajectory\\nmilling acoustic signal\\nmilling dynamic model\\nharmonic components whose frequency\\ncutting depth range\\nharmonic amplitude signal\\ndepth control experiments\\ndepth compensation\\nsize 0.15 mm\\nsize 0.0 mm to 1.2 mm\",\"856\":\"simultaneous localization and mapping\\nimage color analysis\\nunified modeling language\\nlighting\\nfeature extraction\\ngenerative adversarial networks\\nvisual effects\\ncomputer vision\\ndriver information systems\\nimage colour analysis\\nimage enhancement\\nimage texture\\nneural nets\\nunsupervised learning\\nlow-light image enhancement\\ntexture details\\nautomated driving\\nlow-light scenarios\\nvision-based applications\\nunsupervised generative adversarial network\\nmultiscale discriminator\\ntexture discriminator\\ncolor discriminator\\nfeature fusion attention module\\nchannel attention\\nimage feature extraction\\nunsupervised multidiscriminator network\\ncolor recovering\\nunsupervised gan\",\"857\":\"training\\nthree-dimensional displays\\nlaser radar\\nlaplace equations\\npose estimation\\ntraining data\\ncameras\\nimage registration\\nimage sequences\\niterative methods\\nlearning (artificial intelligence)\\nmotion estimation\\nobject detection\\nunsupervised learning\\nscene flow estimation\\nmonocular camera\\ntraining scene flow network\",\"858\":\"solid modeling\\nthree-dimensional displays\\nnavigation\\nconferences\\ngrasping\\nnatural language processing\\nencoding\\ndeep learning (artificial intelligence)\\ngraph theory\\nrobot vision\\nsolid modelling\\nrobotic research\\nrobotic grasping\\nautonomous vehicle navigation\\npoint clouds\\nsingle depth image\\nrank task\\ndeep3dranker\\nrobotic applications\\n3d models\\nrobotic vision\\nycb video datasets\\nlearning to rank\",\"859\":\"three-dimensional displays\\nautomation\\nannotations\\nvehicle detection\\nconferences\\nestimation\\nobject detection\\ncomputational geometry\\nimage segmentation\\nroad vehicles\\nspatial reasoning\\nstereo image processing\\nfgr\\nweakly supervised 3d vehicle detection\\n3d object detection\\n3d data\\nsupervision signals\\nfrustum-aware geometric reasoning\\ncoarse 3d segmentation\\n3d bounding box estimation\\ncontext-aware adaptive region growing algorithm\\n2d bounding boxes\\nfully supervised methods\\n3d pseudolabels\\nkitti dataset\\n3d space\\nsparse point clouds\",\"860\":\"location awareness\\nshape\\nconferences\\nforce\\nestimation\\nsoft robotics\\nmanipulators\\ncables (mechanical)\\ncollision avoidance\\nestimation theory\\nforce control\\nmotion control\\noptimisation\\nrobot vision\\nslam (robots)\\ncollision detection\\nsoft cable-driven robot manipulator\\ncosserat-rod statics\\nthreshold method\\ncable tension\\ncollision localization\\nforce estimation\\nshape information\\nvicon system\",\"861\":\"jacobian matrices\\nshape\\ninstruments\\nmotion segmentation\\nkinematics\\nbending\\nmaster-slave\\niterative methods\\nmanipulator kinematics\\nmedical robotics\\nnewton method\\nsurgery\\nkinematic analysis\\nflexible surgical instrument\\nrobot-assisted minimally invasive surgery\\nsurgical tasks\\nkinematic difficulty\\ninverse kinematics\\nflexible instrument kinematics\\nkinematics solution method\",\"862\":\"deep learning\\nsimultaneous localization and mapping\\nlaser radar\\nconferences\\npose estimation\\npipelines\\nneural networks\\ncomputer vision\\nfeature extraction\\nimage matching\\nlearning (artificial intelligence)\\nmobile robots\\nmotion estimation\\nneural nets\\noptical radar\\nrobot vision\\nslam (robots)\\nencode\\ndeep point cloud odometry network\\nego-motion estimation\\nfeature matching\\nmanually designed features\\nstrong performance\\ndeep learning methods\\nrobotics tasks\\nhand-crafted features\\nneural network\\ninput point cloud\\nmultichannel vertex map\\nmultilayer network backbone\\nabstracted features\\nmap-to-map optimization module\\nlocal poses\\nsmooth map\",\"863\":\"jacobian matrices\\ngeometry\\ncodes\\nautomation\\nnavigation\\nconferences\\npose estimation\\ndistance measurement\\nfeature extraction\\ngeometric codes\\ngraphics processing units\\nimage coding\\noptimisation\\nvisual-inertial odometry\\nlearned optimizable dense depth\\ntightly-coupled deep depth network\\naccurate state estimates\\ndense depth maps\\nlightweight conditional variational autoencoder\\ndepth inference\\nencoding\\nmarginalized sparse features\\ninitial depth prediction\\ngeneralization capability\\ncompact representation\\nnavigation states\\nsliding window estimator\\ndense local scene geometry\\ncvae\\nsparse measurements\\nestimated depth maps\\nestimation accuracy\\ncodevio\\ndepth code\",\"864\":\"location awareness\\nsimultaneous localization and mapping\\nfluctuations\\nnavigation\\nheuristic algorithms\\nconferences\\nsemantics\\nmobile robots\\nobject detection\\nrobot vision\\nslam (robots)\\nlifelong localization\\nsemidynamic environment\\nnonstatic environments\\nstatic objects\\nhighly dynamic objects\\nlocalization failure\\nsemidynamic scenarios\\nlower dynamics\\nsemantic mapping\\nsemidynamic objects\\nmainstream object detection algorithms\\nlocalization algorithms\\nobject detection algorithm\\nsemantic map\\nlocalization method\\nnonstatic objects\\ninvalid observation\\nlocalization fluctuation\\nnonstatic scenarios\",\"865\":\"learning systems\\ndeep learning\\nconferences\\npipelines\\ntransforms\\nbenchmark testing\\ncomputational efficiency\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\ndistance measurement\\ngradient methods\\ninference mechanisms\\ndeep online correction\\nmonocular visual odometry\\ndepth maps\\ninitial poses\\nconvolutional neural networks\\ncnns\\nself-supervised manners\\nphotometric errors\\ngradient updates\\ninference phases\\nonline-learning methods\\ngradient propagation\\nhybrid methods\\ndeep learning frameworks\\nkitti odometry benchmark\\nmonocular vo frameworks\",\"866\":\"location awareness\\nglobal navigation satellite system\\nvisualization\\nmagnetometers\\ntracking\\nmagnetic sensors\\nrobot vision systems\\ncameras\\ndistance measurement\\nimage fusion\\nimage reconstruction\\nimage representation\\nmobile robots\\nslam (robots)\\nsolid modelling\\nstate estimation\\nstereo image processing\\ndirect sparse stereo visual-inertial global odometry\\nautonomous driving\\nrobot applications\\ncomplementary properties\\nstereo cameras\\nmagnetometer\\nmodel parameters\\nactive window\\nvisual part\\nstatic stereo\\nphotometric bundle adjustment pipeline\\ndynamic multiview stereo\\nimu information\\ngnss measurements\\ndrift-free state estimation\\nlocally accurate state estimation\",\"867\":\"training\\nvisualization\\ncodes\\nconferences\\nlighting\\nbenchmark testing\\nfeature extraction\\nconvolutional neural nets\\nimage retrieval\\ntraffic engineering computing\\nvehicle re-identification\\nimage pool\\nocclusion variations\\nvisual feature\\nreid system\\nattention-guided hierarchical feature extractor\\nbackbone cnn\\nfine-grained features\\nhard negative adversarial framework\\nextreme variations\\nminute inter-class differences\\nextreme intra-class difference\\nadversarially-trained hierarchical feature extractor\\nquery vehicle image retrieval\\nveri-wild\\nvric\\nveri-776 datasets\",\"868\":\"geometry\\nthree-dimensional displays\\nautomation\\nfuses\\nconferences\\nsemantics\\nobject detection\\nfeature extraction\\ngeophysical image processing\\nimage reconstruction\\nimage segmentation\\nmedical image processing\\nloss-free feature extraction\\npoint branch\\nvoxel branch\\nefficient proposals generation\\nefficiently encode geometry structure features\\nraw point clouds\\nencoded point features\\npoint2voxel\\nmultiscale p2v\\nlocal detail features\\nsparse voxel backbone\\npoint backbone\\nfine geometry features\\nvic-net outperforms other onestage methods\\ntwo-stage method vic-rcnn\\npoint cloud 3d object detection\\nvoxel-based methods\\ninformation loss\\npoint cloud voxelization\\none-stage voxelization information compensation network\",\"869\":\"visualization\\nimage recognition\\nautomation\\nconferences\\nsemantics\\nbenchmark testing\\nimage representation\\nfeature extraction\\nlearning (artificial intelligence)\\nobject recognition\\nrobot vision\\ndata-driven manner\\nnovel semantic reinforced attention learning network\\ninferred attention\\nsemantic priors\\ndata-driven fine-tuning\\nmisleading local features\\ninterpretable local weighting scheme\\nhierarchical feature distribution\\nsemantic constrained initialization\\nlocal attention\\ncity-scale vpr benchmark datasets\\nlarge-scale visual place recognition\\ntask-relevant visual cues\\nfeature embedding\\nexisting attention mechanisms\\nartificial rules\",\"870\":\"visualization\\nautomation\\nadaptive systems\\nconferences\\nobject detection\\nprediction methods\\nrobustness\\nactive vision\\ndeep learning (artificial intelligence)\\nrobot vision\\nmultiview object detection\\ndesirable perceptual feature\\nadaptive view\\nactive object detection\\nmultiobject detection task\\nactive multiview object detection problem\\nnovel adaptive action prediction method\\ndeep q-learning network\\nmultiple objects\\ntask status\\nunfamiliar environments\\nmultiobject detection boosts efficiency\\nequivalent detection accuracy\",\"871\":\"three-dimensional displays\\nsemantics\\nestimation\\ntraining data\\nobject detection\\nunmanned vehicles\\nclassification algorithms\\nconvolutional neural nets\\ndata analysis\\ndeep learning (artificial intelligence)\\nestimation theory\\ngeometry\\ngradient methods\\nimage classification\\nimage colour analysis\\nsupervised learning\\ngeometric information\\nmonocular depth estimation algorithms\\nnyu depth v2 dataset\\ngeometric representation\\ndata-efficient depth estimation\\ngradient field\\nsingle rgb image\\n3-dimensional object detection\\ndeep convolutional neural networks\\nconvnets\\nleverage rgb images\\nself-supervised learning algorithm\\nmomentum contrastive loss\\nself-supervised learning algorithms\\nsemantic information\\nrandom initialization\\ndepth annotation tasks\",\"872\":\"knowledge engineering\\nextrapolation\\ncomputer vision\\nthree-dimensional displays\\nlaser radar\\nconferences\\ncomputer architecture\\nimage matching\\nlearning (artificial intelligence)\\nneural nets\\nstereo image processing\\nsingle rgb-lidar image\\naccurate dense depth\\ndepth refinement\\nkitti depth completion benchmark\\nstereo-augmented depth completion\",\"873\":\"convolutional codes\\nthree-dimensional displays\\nautomation\\nfuses\\nimage color analysis\\nconferences\\ncolor\\nimage colour analysis\\nimage fusion\\nimage reconstruction\\nimage sampling\\nimage segmentation\\nimage sensors\\nimage texture\\nstereo image processing\\nvideo signal processing\\ndepth-dominant branch\\nbranch inputs\\ncolor image\\nsparse depth map\\ndense depth map\\npredicted depth map\\ndepth maps\\ngood depth completion results\\nfused depth map\\nkitti depth completion\\nprecise image guided depth completion\\nefficient image guided depth completion\\nhigh quality image\\ndepth modalities\\ntwo-branch backbone\\ncolor-dominant branch\",\"874\":\"measurement\\nsolid modeling\\nthree-dimensional displays\\nlaser radar\\nfuses\\nnavigation\\ndetectors\\nlearning (artificial intelligence)\\nobject detection\\nobject tracking\\noptical radar\\nprobability\\nsensor fusion\\nautonomous driving\\nautonomous vehicle\\ntracking-by-detection paradigm\\ntracking accuracy\\ndata association\\ntrack life cycle management\\nmultiobject tracking system\\ndata-driven tracking results\\n3d lidar point clouds\\nunmatched object detection\\nobject detectors our method\\nprobabilistic 3d multi-modal multi-object tracking\\nfeature distances\",\"875\":\"visualization\\nautomation\\nnavigation\\nconferences\\naggregates\\nstochastic processes\\nbenchmark testing\\nconvolutional neural nets\\nfeature extraction\\ngaze tracking\\ngraph theory\\ninference mechanisms\\nlearning (artificial intelligence)\\npedestrians\\ntraffic engineering computing\\navgcn\\ngraph convolutional network\\nhuman attention\\npedestrian trajectory prediction\\ngcn\\nvisual field constraints\\nattention network\\nneighboring pedestrians\\nlearned attention weights\\ncrowd navigation task\\ngaze data\",\"876\":\"training\\nuncertainty\\nnavigation\\npredictive models\\nprobabilistic logic\\ntrajectory\\nsensors\\nconvolutional neural nets\\ngraph theory\\npedestrians\\nattentional-gcnn\\nintelligent vehicle platform\\ngraph convolutional neural network\\nspatio-temporal graphs\\nmotion prediction\\non-board sensors\\naerial view\\nrepeated sampling\\ncrowd motion\\nshared pedestrian environments\\nautonomous vehicle navigation\\nadaptive pedestrian trajectory prediction\",\"877\":\"target tracking\\ncorrelation\\nregulators\\nautomation\\nconferences\\ngaussian processes\\nfeature extraction\\ncorrelation methods\\nfeature selection\\ngraph theory\\nlearning (artificial intelligence)\\nobject tracking\\nregression analysis\\nspatial graph regularized multikernel subtask cross-correlation tracker\\nmultitask correlation filter trackers\\nmultichannel features\\nspatial structure\\nhierarchical subtask multikernel cross-correlation tracker\\ngaussian process regression\\nmultikernel multisubtask\\nspatial feature selection\\nhierarchy subtasks\\ncross similarity\\ngeometric structure\\nkernel cross-correlation filter learning\\ngroup structure sparsity\",\"878\":\"performance evaluation\\ndeep learning\\ncovid-19\\nimage segmentation\\nnavigation\\nconferences\\nbenchmark testing\\ncharacter recognition\\ndeep learning (artificial intelligence)\\nlifts\\nmobile robots\\nobject recognition\\nrobot vision\\nlarge-scale dataset\\nbenchmarking elevator button segmentation\\nhuman activities\\nhuman workers\\nservice work\\nhuman assistance\\nfully autonomous inter-floor navigation\\nlarge-scale publicly available elevator panel dataset\\npanel images\\nbutton labels\\nautonomous elevator operation\\ndeep learning based implementations\",\"879\":\"visualization\\nuncertainty\\ndesign methodology\\nsemantics\\ndecision making\\nestimation\\nstochastic processes\\nestimation theory\\nimage segmentation\\ninference mechanisms\\nrobot vision\\nsampling methods\\nstatistical distributions\\nuncertainty estimation\\npoint cloud semantic segmentation\\nspace-dependent method\\nuncertainty-aware framework\\nsemantic inference\\nneighborhood spatial aggregation\\nneighborhood probabilistic outputs\\nnsa-mc dropout\\nspace-dependent sampling\\noutput distribution\\nstochastic forward pass\",\"880\":\"solid modeling\\nlaser radar\\nthree-dimensional displays\\nconvolution\\nsemantics\\npoles and towers\\ntopology\\nchannel coding\\nconvolutional neural nets\\nimage coding\\nimage segmentation\\noptical radar\\nsolid modelling\\nvisual perception\\nsparse convolution\\nsemantic segmentation approaches\\nperception systems\\nautonomous driving\\naccurate environmental perception\\nlidar semantic segmentation\\nrange-view\\nbirds-eye-view\\n3d topology\\n3d driving-scene point cloud\\nlidar point cloud semantic segmentation\\nsparse interchannel attention module\\nsparse intrachannel attention module\\nprojection-based approaches\\nvoxel-based approaches\\n3d lidar sparse semantic segmentation network\\nsparse residual tower\\nsintraam\\nconvolutional neural network\",\"881\":\"force measurement\\ntorque\\nforce\\ndynamics\\npose estimation\\nestimation\\ntransportation\\ndata visualisation\\ndistance measurement\\nmobile robots\\noptimisation\\nrobot vision\\nrobust visual-inertial-dynamics odometry\\naccurate external force estimation\\nvisual-inertial-dynamics system\\nstate-of-the-art optimization-based visual-inertial system\\nexternal force factor\\nexternal force ranges\\nground truth force measurements\",\"882\":\"location awareness\\nvisualization\\nlaser radar\\ncosts\\nsimultaneous localization and mapping\\nestimation\\nsensor fusion\\ndistance measurement\\nmobile robots\\nobject detection\\noptical radar\\nrobot vision\\nfixed landmarks\\noperation environments\\nsensor fusion scheme\\ninertia measurements\\nultra-wideband range measurements\\nuwb\\nrobot\\ndrift issue\\nliro\\nlidar-inertia-ranging odometry\\ncontinuously reduced cost\\nestimation drift\\ntracking loss\\nprevalent concerns\\ntheory these issues\",\"883\":\"location awareness\\nvisualization\\nmachine vision\\nfiltering algorithms\\ninformation filters\\nfiltering theory\\nrobustness\\ncomputer vision\\nhumanoid robots\\nhuman-robot interaction\\nimage motion analysis\\nimage sequences\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nobject detection\\noptimisation\\nrobot vision\\nspatiotemporal phenomena\\nvideo signal processing\\nverbal focus-of-attention system\\nlearning-from-observation framework\\nmap human demonstrations\\nlfo system\\nhuman demonstration\\ntask models\\nsuccessful task-model encoders\\ntask-model encoder\\nspatio-temporal noises\\ncluttered objects\\nunrelated human body movements\\nverbal instructions\\nobserver\\nobject manipulation\\ntarget object\\nfoa filters\\nverbal foa\\nverbal input\\nstate-of-the-art vision system\",\"884\":\"legged locomotion\\nrobust control\\ntracking\\nheuristic algorithms\\nconferences\\ndynamics\\nwearable robots\\nbiomechanics\\nhandicapped aids\\nmedical robotics\\nmotion control\\nneuromuscular stimulation\\nobservers\\npatient rehabilitation\\nrobot dynamics\\nhybrid model control\\nwalkon suit\\nrobust gait assistance\\npowered exoskeleton\\njoint reference trajectories\\nexoskeletal joints\\ndisturbance observer\\nhybrid nominal model\\nparameter adatpation algorithm\\nprecise gait assistance\\nparaplegia\",\"885\":\"legged locomotion\\nmeasurement units\\ntrajectory tracking\\niron\\nrobustness\\nneuromuscular stimulation\\ndetection algorithms\\nbioelectric phenomena\\nbiomechanics\\nbiomedical equipment\\ndiseases\\ngait analysis\\nmedical control systems\\nmedical robotics\\nmedical signal processing\\nmuscle\\nneurophysiology\\noptimal control\\northotics\\npatient rehabilitation\\nnovel gait phase detection algorithm\\nfoot drop correction\\noptimal hybrid fes-orthosis assistance\\nlife-threatening disease\\nlong-term problems\\npost-stroke patients\\nfunctional electrical stimulation\\nhybrid assistive system\\neffective assistance\\nfast muscular fatigue\\nexcessive muscular stimulation\\ngait cycles\\noptimization control strategies\\nhybrid aafo\\nfes system\\naccurate gait phase detection algorithms\\nswing sub-phase detection algorithm\\nmoving average convergence divergence indicator\\naffected leg\\ngait-phase based control strategy\\nassistive effect\\nsatisfactory ankle joint trajectory tracking\\nstimulation intensity\\nconventional fes assistance\",\"886\":\"training\\nsolid modeling\\nthree-dimensional displays\\npose estimation\\npipelines\\nhardware\\nsensors\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\nimage colour analysis\\nimage representation\\noptimisation\\nrobot vision\\nstereo image processing\\nvisual perception\\nunknown objects\\nconvolutional neural networks\\nsupervised training\\nrgb-d data\\narbitrarily chosen keypoints\\nrgb-d videos\\nsparse based representation\\nkeypoint-based representation\\nrapid pose label generation\\ncnn based 6-dof object pose estimator\",\"887\":\"location awareness\\nvisualization\\nautomation\\nconferences\\nlighting\\nrobots\\ndeep learning (artificial intelligence)\\nregression analysis\\nrobot vision\\nslam (robots)\\npotentially repeatable interest points\\ninterest point repeatability prediction\\nlearning\\nrobotics applications\\nappearance changes\\nrepeatability predictor\\nregressor training\\nmap summarization\\nvisual localization framework\\ndeep neural network regressor\\nrobotic vision\",\"888\":\"training\\nthree-dimensional displays\\nshape\\nrobot kinematics\\nsemantics\\nrobot vision systems\\npose estimation\\ncameras\\nfeature extraction\\nimage colour analysis\\nimage matching\\nimage reconstruction\\nimage segmentation\\nlearning (artificial intelligence)\\nobject recognition\\nstereo image processing\\ndraco\\nobject shape\\nrgb images\\ncanonical shape reconstruction\\ncoordinate space\\nrobotic applications\\npainstakingly gathered dense 3d supervision\\nsparse canonical representations\\nreal-world applicability\\ndense canonicalization\\ndense object-centric depth maps\\ncanonical coordinate-space\\nfully-supervised methods\\nweakly supervised dense reconstruction\",\"889\":\"technological innovation\\ncosts\\nhead\\ncorrelation\\nestimation\\npredictive models\\nfeature extraction\\ndeep learning (artificial intelligence)\\nestimation theory\\nrobot vision\\ncoarse-to-fine paradigm\\nhead enhanced pooling pyramid\\nhigh-resolution pyramid features\\ncenter dense dilated correlation layer\\ncompact cost volume\\nlightweight network\\nfast optical flow estimation\\ndense optical flow estimation\\nrobotic vision tasks\\ndeep learning\\nheavy computation costs\\nlow power-consumption devices\\nfast flow prediction\\noptical flow prediction\\nfastflownet\\nhepp\\nfeature extractor\\nsbd\\nshuffle block decoder\\nkitti datasets\",\"890\":\"visualization\\nobject detection\\nlogic gates\\nperformance gain\\nfeature extraction\\nencoding\\nimage sequences\\nimage classification\\nimage motion analysis\\nspatiotemporal phenomena\\nvideo on demand\\nvideo signal processing\\njoint representation\\ntemporal image sequences\\nobject motion\\nvideo object detection method\\ntemporal feature aggregation\\ntm-vod\\nstrong spatiotemporal features\\ntemporally redundant information\\nimage sequence\\nmotion context\\nfeature level\\nregion proposal stage\\nrefinement stage\\nvisual features\\npixel level motion features\\nadjacent visual feature maps\\nnovel feature alignment method\\ninitial region proposals\\ninstance level motion features\\ninstance level features\\nachieves performance\\nstate-of-the-art vods\",\"891\":\"image segmentation\\nsolid modeling\\nthree-dimensional displays\\nlaser radar\\nshape\\npose estimation\\nsemantics\\ncalibration\\ncameras\\nimage colour analysis\\nimage reconstruction\\nimage sensors\\nobject detection\\noptical radar\\ntargetless multiple camera-lidar extrinsic calibration\\ntargetless method\\nmultiple cameras\\nlidar sensor\\nprevious targetless methods\\ngeometric features\\ncalibration parameters\\nsingle-scan configuration\\nsemantic objects\\nmultiple images\\n3d point cloud\\nup-to-scale point cloud\\nlidar point cloud\\ninitial calibration\\npoint cloud segment\\nimage object segments\\ncorrespondence information\\ncolor appearance model\\ncorresponding object instance segment\\nappearance models\\nmultimodal calibrations\",\"892\":\"information geometry\\ncomputer vision\\nautomation\\nannealing\\nstatistical analysis\\nconferences\\nestimation\\ncomputational geometry\\nset theory\\nbandwidth estimation strategy\\ngeometric approaches\\nstatistical registration algorithms\\npoint set registration\",\"893\":\"robot motion\\nimage coding\\ntracking\\nsystem performance\\nsurgery\\naerospace electronics\\nrobot sensing systems\\nfeedback\\nmotion control\\nrobot vision\\nvisual servoing\\ncompressive feedback\\nrobot motion control aims\\ncontrol inputs\\nrobotic system\\nplanned trajectory\\nfeedback signals\\nheavy computational burden\\nrobot motion control scheme\\nfeedback rate\\nfeedback image\",\"894\":\"deep learning\\nthree-dimensional displays\\nconferences\\npose estimation\\ngrasping\\nprediction algorithms\\nmarket research\\ndeep learning (artificial intelligence)\\nimage colour analysis\\nrobot vision\\nmfpn-6d\\n6d pose estimation\\ncross stage partial network\\npespective-n-point algorithm\\nmultidirectional feature pyramid network\\nreal-time one-stage pose estimation\\nrgb images\\nrobot grasping\\ndeep neural network\\nobject pose estimation\\nspatial information\\nplane information\\noccluded-linemod datasets\",\"895\":\"accelerometers\\nartificial limbs\\nautomation\\nconferences\\ntactile sensors\\nprototypes\\nskin\\ndecoding\\nfeedback\\ntouch (physiological)\\nsensory modality\\ntactile feedback system\\ndirect-texture-feedback\\ntactile perception\\nsensory feedback\\ntexture information\\non-line direct-texture decoding\\naccelerometer\",\"896\":\"couplings\\nautomation\\nconferences\\npose estimation\\nneural networks\\ngraphics processing units\\ncameras\\ngeometry\\npluggable geometric consistency loss\\nwasserstein distance\\nmonocular depth estimation\\nmonocular camera images\\npoint clouds\\nsymmetric coupling\\nhighly accurate depth\",\"897\":\"three-dimensional displays\\nimage resolution\\nshape\\nconferences\\nneural networks\\nestimation\\nreal-time systems\\nimage reconstruction\\niterative methods\\nmesh generation\\nneural nets\\nstereo image processing\\nimplicit function\\ndecision boundary\\ncomposite function\\nsingle-view reconstruction\\nreal-time mesh extraction\\n3d object shape estimation\\nmarching cubes\\nreverse tracing\\nbinary-coded input neural network\",\"898\":\"deep learning\\nuncertainty\\nthree-dimensional displays\\nurban areas\\nbenchmark testing\\nfeature extraction\\nsafety\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\nimage classification\\nimage sequences\\nobject detection\\noptical radar\\nsensor fusion\\nshape recognition\\ntraffic engineering computing\\nuncertainty-aware fast curb detection\\nconvolutional networks\\nautonomous vehicles\\nhand-crafted features\\ndeep neural networks\\ncurb detection problem\\nmultimodal sensor-based methods\\nautonomous system safety\\ncurb detection method\\nuncertainty quantification\\nautoencoder-based network\\nconditional neural processes\\ndnn-based curb detectors\",\"899\":\"training\\nindustries\\ncosts\\ntext recognition\\ninventory management\\noptical imaging\\nrobustness\\noptical character recognition\\nproduction engineering computing\\ntext detection\\ninventory procedures\\noptical character recognition-based inventory management algorithm\\ninventory management dataset\\nfast inventory management algorithms\\ncomprehensive management\\nocr-based inventory management algorithms\",\"900\":\"training\\nadaptation models\\nsmoothing methods\\nsemantics\\nsurgery\\ntransformers\\nadversarial machine learning\\nimage segmentation\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\ndomain adaptation\\nmodel calibration\\nsurgical report generation\\nrobot-assisted surgery\\nnatural language expression\\nsurgical scene understanding\\ndocument entry tasks\\nsurgical training\\npost-operative analysis\\ndeep learning algorithm\\ndeployment performance\\ntarget domain data\\nmultilayer transformer-based model\\nmultidomain surgical images\\nsurgical region\\ngradient reversal adversarial learning scheme\\ngradient multiplies\\ntarget domains\\nemerging domain-invariant features\\npenultimate layer\\nrobotic surgery datasets\\nmiccai robotic scene segmentation\\ntransoral robotic surgery\\ntarget domain surgical reports generation\\nfew-shot learning\",\"901\":\"laparoscopes\\nvisualization\\nsolid modeling\\nthree-dimensional displays\\ntwo dimensional displays\\nrobot vision systems\\ntools\\nbiomedical optical imaging\\ncameras\\nlearning (artificial intelligence)\\nmedical computing\\nmedical robotics\\nsurgery\\nsurgical assistants\\nhigh-quality fov\\ndata-driven framework\\nautomated laparoscopic optimal fov control\\nmotion strategy\\nsurgeon\\nin-house surgical videos\\ncontrol domain knowledge\\noptimal view generator\\nlearning-based method\\ntwo-dimensional position\\nsurgical tool\\nscale-aware depth\\ndense depth estimation results\\nreal-time 3d position\\ncontrol loop\\nmotion constraints\\nnull-space controller\\nlearning enabled framework\\nautomated camera control\\ndata-driven holistic framework\\nautomated laparoscope optimal view control\\nlearning-based depth perception\\nlaparoscopic field\\nfundamental components\\nminimally invasive surgery\\ntraditional manual holding paradigm\",\"902\":\"training\\nlearning systems\\naquatic robots\\ncomputational modeling\\nneural networks\\nrobot vision systems\\nreinforcement learning\\ncomputer vision\\ncontrol engineering computing\\nintelligent robots\\nmarine control\\nmobile robots\\nneurocontrollers\\nrobot vision\\ndeep reinforcement learning\\nsoft swimming robot\\nmobile soft robots\\nsoft robot underwater locomotion problem\\ndata-based control framework\\ndrl\\ndielectric elastomer actuator\\ndea\\nneural network controller\\ncamera\",\"903\":\"couplings\\ndeformable models\\nanalytical models\\nautomation\\nconferences\\nreinforcement learning\\nreal-time systems\\ncables (mechanical)\\nfeedback\\niterative methods\\nmanipulators\\nlong flexible cable manipulation\\nmultiple rigid link segments\\ncomplementarity-based contact model\\ninter-segment compliant coupling\\nconsecutive links\\ninter-subsystem consistency constraint\\ninter-subsystem coupling\\ncontact impulses\\nsubsystem dynamics\\nparallelized iterative algorithm\\nreal-time simulation\\nreal-time physically-accurate simulator\",\"904\":\"deep learning\\nvisualization\\ntorque\\nendoscopes\\ninstruments\\nsurgery\\nthree-dimensional printing\\ndna\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nrobot vision\\nvisual servoing\\nautonomous robotic flexible endoscope system\\nur5 robot\\nnovel continuum joint\\ndouble helix continuum mechanism\\ndeep learning algorithm named ternausnet-16\\ndna-inspired continuum mechanism\",\"905\":\"couplings\\nactuators\\nwelding\\nthermal variables control\\ndesign methodology\\nmodulation\\nsoft robotics\\nbuilding materials\\ndesign engineering\\nelastic constants\\nindustrial robots\\njamming\\nrigidity\\namplifying laminar jamming\\ngeometry-induced rigidity\\nvariable stiffness technology\\nhigh-force rigid robots\\nvariable curvature jamming mechanism\\nstiffness modulation\\ncross sectional curvature\\ninitial compliant laminar structures\\nbidirectional stiffness behaviors\\nvcj actuator\\ntransverse curvature\\nload capacity\\ncoupling\\nbuilding soft actuators\\neffective design method\\nvariable stiffness\\nlaminar jamming\\nvariable curvature\\ngeometry-induced rigid\",\"906\":\"actuators\\nthree-dimensional displays\\nmedical robotics\\nendoscopes\\nrobot kinematics\\nmagnetic resonance imaging\\nend effectors\\nbending\\nbiomedical mri\\ncontinuum mechanics\\ninteractive devices\\nmanipulator kinematics\\nmedical computing\\npredictive control\\nshape memory effects\\nsurgical robots\\nmultiimager compatible continuum robot\\nmodular sma\\ncontinuum distal end effector\\nintraoperative image-guided minimally invasive surgery\\n3-dimensional printed polyamide continuum robot\\nmodular shape memory alloy spring actuators\\nmri-conditional actuators\\ndistal manipulation\\nactive cooling\\nsterile barrier\\nsinusoidal signals\\ndistal bending angle\\nflexible endoscope manipulator\\nsteered under inputs\\njoystick\",\"907\":\"couplings\\ndoppler shift\\nglobal navigation satellite system\\nmeasurement errors\\nsimultaneous localization and mapping\\nconferences\\nmeasurement uncertainty\\noptimisation\\nsatellite navigation\\nslam (robots)\\nraw gnss measurement error\\nnoisy measurements removal\\nstate-of-the-art visual-inertial slam\\ngnss single point positioning\\nloose coupling approach\\noptimization-based visual-inertial slam\\nekf-based approaches\\nraw global navigation satellite system measurements\\nreprojection error\\nimu pre-integration error\\nsliding window\\nmarginalization\",\"908\":\"simultaneous localization and mapping\\ncosts\\nlaser radar\\nshape\\nconferences\\ngaussian distribution\\nbenchmark testing\\nimage registration\\noptical radar\\nprobability\\nslam (robots)\\npoint cloud registration\\nnovel icp metric\\nregistration process\\nsymmetric kl-divergence\\nicp cost\\nstate-of-the-art slam method\\nlitamin2\\nultra light lidar-based slam\\ngeometric approximation\\nthree-dimensional light detection and ranging simultaneous localization and mapping method\\nprobabilistic distributions\\nfrequency 500.0 hz to 1000.0 hz\",\"909\":\"graphics\\nsimultaneous localization and mapping\\nautomation\\nconferences\\nrendering (computer graphics)\\nreliability\\nopen source software\\ngraphics processing units\\nimage colour analysis\\nimage reconstruction\\nmobile robots\\noptimisation\\nrobot vision\\nslam (robots)\\nsimultaneous localization and mapping system\\nindoor scenes\\nartificial environments\\nrecognizable objects\\ncompositional object mapping formulation\\nscalable object mapping formulation\\nrobust slam solution\\ndrift-free large-scale indoor reconstruction\\nunambiguous persistent object landmarks\\nframe-to-model rgb-d tracking\\nfast localization\\nsemantically assisted data association strategy\",\"910\":\"image segmentation\\nanalytical models\\nrobot vision systems\\npipelines\\nmanipulators\\noptical imaging\\nreal-time systems\\nautomatic optical inspection\\ncomputer vision\\nimage capture\\nimage registration\\nimage resolution\\nscanning path\\nreal-time workpiece localization\\nhigh-resolution line scan\\nrobotic defect inspection system\\nfree-form specular surfaces\\nimage acquisition sub-system\\n6-dof robot manipulator\\nsmooth regions\\nk-means based region segmentation algorith\\npoint cloud\\nimage processing pipeline\\nprojection registration method\",\"911\":\"automation\\nservice robots\\nconferences\\ncollaboration\\ncomputer architecture\\nservers\\ntask analysis\\ncloud computing\\ninternet of things\\nservice robot management system\\nmultiple units\\nserverless architecture\\ncloud\\ncellular based iot communication\",\"912\":\"autism\\npandemics\\nrobot kinematics\\ntoy manufacturing industry\\neducation\\nhumanoid robots\\nvariable speed drives\\ncomputer aided instruction\\nhandicapped aids\\nhuman-robot interaction\\nmedical disorders\\nrobot vision\\nteaching\\nnonhumanoid social robots reduce workload\\nspecial educators\\nin-premises field study\\nsocially assistive robotics\\nautism spectrum disorder interventions\\nses\\nexpensive humanoid robots\\nnonhumanoid toy robots\\nasd education\\nindividualized education plans\\nasd children - communication\\ntoy robot cozmo\\nverbal lessons\\nschool premises\\nreduced workload\\nonline asd interventions\\nonline intervention\\ndesigning online interventions\\nexpensive humanoids\\nautism education\",\"913\":\"pregnancy\\nestimation error\\nthree-dimensional displays\\nultrasonography\\nanatomical structure\\nbones\\nfetus\\nbiomedical ultrasonics\\nbone\\ncardiology\\nfeature extraction\\nmedical image processing\\nmedical robotics\\nobstetrics\\nfetal bone distribution\\nfetal head position\\nfetal chest position\\nbone detection\\nfalse positive rate\\nautonomous fetal cardiac diagnosis\\nrobotic us scanning\\nautonomous robotic fetal ultrasonography\\nrobotic ultrasound\\nfetal skeleton\\nthree-dimensional bones position\\nfetal heart position estimation\\nprenatal care\\nacoustic shading\\nskull\\nspine\\nshadow features\\n3d bone distribution\\nthoracic sagittal axis\\npregnant volunteers\",\"914\":\"wireless communication\\nfabrication\\nwireless sensor networks\\nsilver\\nrobot sensing systems\\nsensors\\nantennas\\nink jet printers\\nink jet printing\\nradiofrequency identification\\ndetected mechanical stimuli\\nchipless sensor tags\\ninkjet-printing method\\nwireless mechano-responsive sensors\\nrobotic applications\\nchip-less wireless sensing\\nkirigami structural morphing\\nvarious mechanical stimuli\\nhome-based ink-jet printable materials\\nchip-based rfid designs\\nwireless sensor tags open a wide range\\napplication possibilities\\nmultistep lithography manufacturing\\nmems techniques\\nindustrial-grade fabrication\\nsimple home-based\\ntwo-step fabrication process\\nchipless rf-based wireless sensors\\noffice-based inkjet printer\\nsilver conductive ink\\nkirigami-inspired designs\\ndiy robotic projects\\nwireless transmission\",\"915\":\"couplings\\nautomation\\nservice robots\\nconferences\\nreal-time systems\\ncomputational efficiency\\ncalibration\\nindustrial manipulators\\noptimisation\\npose estimation\\nposition control\\nrobot vision\\ntree searching\\npositioning accuracy\\nscara robots\\n2-dimensional search space\\ncalibration parameters\\ncomputational cost\\nglobal optimal hand-eye calibration\\nself-calibration\\n2-dimensional branch-and-bound algorithm\\nindustrial fields\\nbranch-and-bound optimization\\nbnb optimization\\nrotation component\\nhand-eye pose\\ndegrees of freedom\\nrotation dof\",\"916\":\"employee welfare\\ntransducers\\nshape\\ninternal stresses\\nloading\\nvoltage\\nmuscles\\nactuators\\ndielectric materials\\nelastomers\\nelectric actuators\\nelectroactive polymer actuators\\nmuscle\\nrheology\\nsprings (mechanical)\\nstress-strain relations\\nviscoelasticity\\nlong-term multiple time-constant model\\nspring roll dielectric elastomer actuator\\nnonlinear viscoelastic stress-strain relationship\\nnonlinear materials\\nlinear actuator\\nwrapped dielectric elastomers\\nactuation performance\\ntime-dependent actuation behavior\\nlong-term dynamic loading\",\"917\":\"shafts\\ngears\\nfriction\\nforce\\nwheels\\nswitches\\nmobile robots\\nactuators\\nmotion control\\npipes\\nwheel shaft\\nroll joint\\nunderactuated mechanism\\none-way clutch\\nwheel movement\\nhemispherical wheel\\nroll rotations\\nwheeled v-shaped in-pipe robot\\nclutched underactuated joints\\nmiter gear mechanism\\nrobot rolling movement\\nresilience force\\nfriction force\",\"918\":\"legged locomotion\\nresistance\\nhydraulic drives\\ndesign methodology\\nconferences\\nforce\\nhydraulic systems\\nmotion control\\npumps\\ntrajectory control\\nhydraulic legged robot\\npowered pump\\nhydraulic drive system\\npump unit\\nflow rate\\nleg operation\\npump capacity\\nleg motion\\ntrajectory design walking\\nhydraulic\\/pneumatic actuators\\nlegged robots\\nleg trajectory\",\"919\":\"automation\\nconferences\\nhuman-robot interaction\\nswitches\\ndogs\\nprogramming\\nspace exploration\\ncollision avoidance\\nhandicapped aids\\ninteger programming\\nlegged locomotion\\npath planning\\nleash tension\\nrobot-guiding-human system\\nhybrid model\\nleash-guided robot framework\\nrobotic guide dog\\nleash-guided hybrid physical interaction\\nautonomous robot\\nnarrow spaces\\ncluttered spaces\\nbig boon\\nprior robotic guiding systems\\nwheeled platforms\\nactuated rigid guiding canes\\nactuated arms\\nprior approaches\\nnarrow environments\\ncluttered environments\\nhybrid physical human robot interaction model\\nrobot-guiding human system\\nmini cheetah quadrupedal robot\",\"920\":\"legged locomotion\\ncouplings\\nactuators\\nautomation\\nconferences\\nrobots\\noptimization\\ncontrol engineering computing\\nmobile robots\\nmotion control\\nrobot dynamics\\noverconstrained robotic leg\\ncoaxial quasidirect drives\\nomni-directional ground mobility\\nmodern designs\\nlegged robots\\nremote actuator placement\\nrobust agility\\nrobotic leg modules\\nbennett linkage\\nquasidirect actuators\\nomnidirectional ground locomotion\\noverconstrained linkages\\nthree-dimensional spatial motion\\nunparalleled joint axes\\nbennett leg module\\nlateral locomotion\\nrobotic legs\\ncommon planar mechanisms\\noverconstrained robots\\nadvanced robots\\ndesign reconfiguration\\npotential direction\\nreconfigurable design\",\"921\":\"geometry\\nlocation awareness\\nvisualization\\nsimultaneous localization and mapping\\nsemantics\\nreal-time systems\\ncomputational efficiency\\ncameras\\nimage colour analysis\\nobject detection\\npattern clustering\\nslam (robots)\\nvisual slam methods\\ndeep learning-based semantic information\\nsemantic segmentation\\nsemantic rgb-d slam methods\\ndynamic region environments\\ncomputational cost reduction\\ngeometry module\\nlow-power embedded platform\",\"922\":\"three-dimensional displays\\nnavigation\\nrobot sensing systems\\npath planning\\nreal-time systems\\nplanning\\nnoise measurement\\ncollision avoidance\\nmobile robots\\nsearch problems\\ntrees (mathematics)\\nvisibility information\\nclassic tree-based\\nnoisy points\\nvisible region\\npath tree\\nrvt algorithm\\npath planning task\\nunknown environment\\ntime robot path planning\\npath planning strategy\\nrapid visible tree algorithm\\ncomplex environment\\ndangerous collisions\",\"923\":\"image segmentation\\nthree-dimensional displays\\nimage resolution\\ncosts\\nroads\\nconferences\\nsemantics\\nimage matching\\nimage reconstruction\\nimage texture\\noptical radar\\nstereo image processing\\ndense 3d road mapping\\nwidely used lidar-based mapping\\nautonomous driving field\\nimage-based mapping method\\nimage-based 3d mapping\\nlow-textured areas\\nroad surface\\nmultiview stereo method\\nsemantic information\\npatchmatch-based mvs pipeline\\nimage semantic segmentation\\nneighbor views selection\\ndepth-map initialization\\ndepth-map completion\",\"924\":\"automation\\nconferences\\nnatural languages\\ntraining data\\ndetectors\\nmanipulators\\ncognition\\nnatural language processing\\nspatial reasoning\\nnatural language instructions\\nrobot manipulation\\nunstructured environments\\npipelined architecture\\ntext input\\nlocalized co-ordinates\\nlocalized objects\\nrobot arm\\npick-and-place playing cards\\n2d co-ordinates\",\"925\":\"gsm\\nautomation\\nconferences\\nrobot vision systems\\ncameras\\nrobustness\\noptimization\\nestimation theory\\nfeature extraction\\nleast squares approximations\\nmatrix algebra\\nmedical image processing\\noptimisation\\n8-point algorithm\\n360-fov images\\npreconditioning strategy\\n8-pa\\nessential matrix\\nequirectangular images\\nspherical projection\\nuneven key-feature distributions\\nnonrigid transformation\\nspherical camera\\nrandom synthetic points\\nfish-eye images\\nreliable solution\\nrobust 360-8pa\\ngold standard method\\nleast-square optimization\\nransac iterations\",\"926\":\"training\\nvisualization\\nautomation\\npose estimation\\ninspection\\nsensor fusion\\ncameras\\nautomatic optical inspection\\ncomputer vision\\nconvolutional neural nets\\nlocalisation methods\\nuncontrolled outdoor environments\\ncommercial aircraft\\npan-tilt-zoom camera\\ninspection task\\ndeep convolutional neural network\\nknown aircraft geometry\\nautonomous aircraft visual inspection systems\\ncnn-based camera pose estimation\\ngeneral visual inspection\\nmanual inspection process\\nboarding gate\\nhuman labour\\ndomain randomisation\\non-site infrastructure-less initialisation method\\ntime 2.0 hour\\nlocalisation\",\"927\":\"three-dimensional displays\\nlaser radar\\nconferences\\ngraphics processing units\\ntransforms\\nnearest neighbor methods\\ngaussian distribution\\nimage registration\\niterative methods\\nnormal distribution\\noptical radar\\nthree-dimensional point cloud registration\\niterative closest point approach\\nnearest neighbor search\\nnormal distributions\\nvoxel distributions\\nvoxelized gicp\\nvoxelized generalized iterative closest point algorithm\\nreal-time 3d lidar\",\"928\":\"navigation\\nneural networks\\nswitches\\nlogic gates\\ndata collection\\ncameras\\ntrajectory\\ncollision avoidance\\nfeature extraction\\nimage texture\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nobject detection\\npath planning\\nremotely operated vehicles\\nrobot vision\\nlow-level control commands\\ncurrent camera image\\ncnn input\\ndrone states\\nswitchable navigation behaviors\\nsingle trained network\\ncnn-based perception module\\nground truth navigation commands\\nrobust navigation\\ndrones\\nimitation learning\\nmodularization\\nvision-based modularized drone racing navigation system\\ncustomized convolutional neural network\\nhigh-level navigation commands\\nstate-of-the-art planner\\ncontroller\",\"929\":\"space vehicles\\nsemantics\\ncollaboration\\nreal-time systems\\nplanning\\ntrajectory\\nsafety\\nbayes methods\\nclosed loop systems\\ncollision avoidance\\ncomputer vision\\nfiltering theory\\nimage sensors\\nimage sequences\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npath planning\\nprobability\\nroad vehicles\\ntraffic engineering computing\\nvision-based methods\\ndeep networks\\ninterpretable end-to-end vision-based motion planning approach\\nautonomous driving\\nivmp\\nfuture egocentric semantic maps\\nbird\\nfuture semantic maps\\nuseful interpretable information\\noptical flow distillation paradigm\",\"930\":\"training\\nrain\\ncomputational modeling\\nmemory management\\ncomputational efficiency\\ntask analysis\\nautonomous vehicles\\ndeep learning (artificial intelligence)\\nimage enhancement\\nimage restoration\\nnavigation\\nroad traffic\\nvehicles\\nvisual perception\\nautonomous driving\\nheavy rain\\ndeep learning-based perception methods\\nmultiple degrading effects\\nreal-world bad weather cases\\nhigh-level vision task\\ncomputational cost\\ntask-driven training strategy\\nhigh-level task model\\nhighly accurate perception\\nadverse weather\\nlow memory network\\ntask-driven deep image enhancement network\",\"931\":\"image segmentation\\nvisualization\\ncorrelation\\nimage coding\\nautomation\\nconferences\\nnatural languages\\nfeature extraction\\nnatural language processing\\nquery processing\\nlanguage-driven attention\\nquery natural language sentence\\nresulting cross-modal representation\\ncross-modal correlation\\nchannel-attention\\nspatial-attention\\nchannel attention\\nspatial attention\\nfinal cross-modal representation\\nreferring image segmentation model\\nlda\\nfeature attention\\nmultireceptive-field\\nmultilevel-semantic\",\"932\":\"deep learning\\nautomation\\nconferences\\ncomputational modeling\\nsemantics\\ngraphics processing units\\nreal-time systems\\ncomputational complexity\\ndeep learning (artificial intelligence)\\nimage segmentation\\ncomputation complexity\\nsingle gtx 1080ti card\\nfcn decoder\\ncityscapes test set\\ndenseenet\\ngpu-efficient dense convolutional network\\ndense connectivity\\nmodern gpu device\\ndeep neural network\\ninference speed\\nreal-time semantic segmentation\",\"933\":\"training\\nknowledge engineering\\nvisualization\\nautomation\\nconferences\\nsemantics\\nbenchmark testing\\nfeature extraction\\nimage segmentation\\ninference mechanisms\\nlearning (artificial intelligence)\\nfeature enhanced projection network\\nzero-shot semantic segmentation\\nsemantic word embeddings\\nfepnet\\nknowledge transfer\\nprojection layers\\nu-space features\\ns-space\\nseen categories\\nunseen categories\\nautonomous driving\\nfeature mapping\\ninference stage\",\"934\":\"training\\nrecurrent neural networks\\nmotion estimation\\nconferences\\ndynamics\\nestimation\\ncameras\\ndistance measurement\\nfeature extraction\\nimage sequences\\nrecurrent neural nets\\nregression analysis\\nunsupervised learning\\n6-dimensional pose vector regression\\ngeneralized feature representation\\nunsupervised training\\nsequential learning\\nrecurrent neural network\\nself-encoding manner\\nsequential images\\ncamera ego-motion estimation\\nend-to-end network\\nmonocular visual odometry problem\\nr-cnn\\noptical flow\",\"935\":\"laser radar\\nconferences\\npipelines\\nbuildings\\nradar\\ngaussian distribution\\nprobabilistic logic\\ndistance measurement\\nimage matching\\nmobile robots\\nnormal distribution\\noptical radar\\nroad vehicle radar\\nslam (robots)\\ntransforms\\nradar odometry\\nautomotive radars\\nradar sensors\\nro method\\nprobabilistic submap building\\nnormal distribution transform-based radar scan matching\\nndt-based radar scan matching\\npublic radar datasets\\noxford radar robotcar dataset\\nautomotive radar data\\nnuscenes dataset\\nrotational error\\nlidar odometry\\nodometry\\nscan matching\\nautonomous driving\",\"936\":\"manifolds\\nvisualization\\nsimultaneous localization and mapping\\nmeasurement units\\nautomation\\nconferences\\nmemory management\\ncameras\\ndistance measurement\\ninertial navigation\\nkalman filters\\nlie groups\\nnonlinear filters\\noptimisation\\nposition measurement\\nrobot vision\\nslam (robots)\\nsmoothing methods\\nmonocular camera\\ninertial measurement unit\\nextended kalman filter\\nlow memory requirements\\ncpu\\noptimisation-based methods\\nvio problem\\nsmooth quotient manifold\\nvisual measurements\\nekf-based vio algorithms\\nequivariant filter design\\nvisual inertial odometry\\neuroc dataset\\nlie group\",\"937\":\"legged locomotion\\npipelines\\ntactile sensors\\nrobot sensing systems\\nskin\\nsensors\\ntask analysis\\nfeedback\\nhumanoid robots\\nlearning (artificial intelligence)\\nmotion control\\nrobot vision\\nstability\\nsingle robot leg stabilization\\nhuman feet\\nrobotic counterparts\\ntactile sensing signals\\nvision-based tactile sensing foot system\\nfeedback control\\nfoot skin\\nhigh-level contact information\\nstabilization task\\ntactile foot\\nsurface tilting angle\\ntactile system\\nsingle-legged robotic systems\\nvision-based tactile sensors\\nlegged robots\",\"938\":\"location awareness\\ngeometry\\nannotations\\nroads\\nsemantics\\nlayout\\ndata models\\ncomputer vision\\nfeature extraction\\nmobile robots\\nobject detection\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\nparametric scene parsing\\naerial imagery\\nscene attributes\\nroad scene\\nground imagery\\nargoverse dataset\\nautonomous navigation\",\"939\":\"dynamics\\ntoy manufacturing industry\\nvision sensors\\nfiltering algorithms\\nfeature extraction\\nrobot sensing systems\\nspatiotemporal phenomena\\nconvolutional neural nets\\nimage filtering\\nimage motion analysis\\nimage representation\\nimage sensors\\nobject detection\\nrobot vision\\ncomputer vision systems\\nrelatively simple vision systems\\ndvs\\ncausal exponential filters\\nconvolutional neural network\\nfast motion understanding\\nspatiotemporal neural networks\\ndynamic vision sensor based system\\nhigh-speed motion\\nfast approaching object\\ncnn\\nsize 18.4 mm\",\"940\":\"fabrication\\nsurface waves\\nconferences\\nmimics\\ndynamics\\nsoft robotics\\nmuscles\\nadhesion\\nbiomechanics\\nbiomimetics\\nelasticity\\nlegged locomotion\\nmotion control\\nproteins\\nrobot dynamics\\nbiomimetic gastropod-like soft robot\\nwet adhesive locomotion\\nsoft crawling robots\\nelastic environment\\nslippery environment\\nhumid environment\\nwet adhesion\\nsnail-like soft robot\\ndynamic model\\nsoft robot applications\\nsoft robot materials and design\\nmodeling\",\"941\":\"motion planning\\nactuators\\ntorque\\nperturbation methods\\ndynamics\\nhuman-robot interaction\\ndisturbance observers\\ncontrol nonlinearities\\ncontrol system synthesis\\nelectrostatic actuators\\nlyapunov methods\\nmanipulator dynamics\\nmotion control\\nnonlinear control systems\\nobservers\\nperturbation techniques\\nrobust control\\ntorque control\\nsingular perturbation\\nsea-level dynamics\\nrobot-level dynamics\\nunmodeled dynamics\\nexternal disturbance\\nnonlinear disturbance observer-based robust motion control\\nintrinsic oscillatory dynamics\\nhigh-order robotic dynamics\\nlow-bandwidth inner loop\\ndynamic nonlinearities\\nnonlinear disturbance observer-based robust controller\\nstable motion control\\nprecise motion control\\nmultijoint sea-driven robots\\nfast-time control term\\nmultijoint series elastic actuator-driven robots\\ndofs sea-driven robot\\ncomposite controller\\nndob-ctc\\ncomputed torque controller\\nbounded stability\\nlyapunov-type analysis\\nseries elastic actuator (sea)\\nmulti-joint sea-driven robots\\nnonlinear disturbance observer\",\"942\":\"correlation\\nautomation\\nbiological system modeling\\nconferences\\nmuscles\\nfatigue\\nimpedance\\nbiomimetics\\nelectromyography\\ngait analysis\\nmedical control systems\\nmedical signal processing\\nprosthetics\\nmyoelectric prosthetic hand\\nlambda-type muscle model\\nrobotic prosthetics\\nhuman movements\\nnovel biomimetic control method\\nimpedance model\\n\\u03bb-type muscle model\\nmuscle relaxation\\nactual human joint angles\\nactual human motion\\nnatural prosthetic hand movement\\nvoluntary hand movements\\nprosthetics and exoskeletons\\nhuman-centered robotics\",\"943\":\"adaptation models\\nuncertainty\\ntoy manufacturing industry\\nreinforcement learning\\nprobabilistic logic\\ninference algorithms\\nrobustness\\nmulti-agent systems\\nprobability\\nsafety\\nsampling methods\\nsafe adaptation\\nnonstationary environments\\nprobabilistic latent variable model\\nposterior environment transition distribution\\nsafety constraints\\nuncertainty-aware trajectory sampling\\nrealistic safety-critical environments\\nnonstationary disturbances\\nreinforcement learning agents\\ncontext-aware safe reinforcement learning\\ncasrl\\nmeta-learning\\ndomain knowledge\",\"944\":\"measurement\\nanalytical models\\nredundancy\\ndynamics\\nreinforcement learning\\nmanipulators\\ndistance measurement\\ndeep learning (artificial intelligence)\\ndexterous manipulators\\nmanipulator dynamics\\nredundant manipulators\\nrobot programming\\nrobotic structures\\nrobotic arm manipulation\\ncomplex robotic locomotion\\ndynamic feasibility\\nrobotic joint redundancy\\nrobotic dexterity\\ndeep reinforcement learning\\nsynergy exploration area metric\",\"945\":\"training\\ndeep learning\\nemotion recognition\\nimage recognition\\nhead\\nautomation\\nface recognition\\nlearning (artificial intelligence)\\npattern classification\\ndeep balanced learning\\nlong-tailed facial expressions recognition\\ncomplex problem\\nautomated facial expression recognition\\ndeep learning networks\\ndata imbalance\\nlong-tail distribution problems\\nlarge-scale datasets\\ncontinual learning method\\nmultisubsets\\nhead classes\\nup-sampling tail classes\\npre-trained backbone\\nrepeatedly train-prune fashion\\nunion parameters\\nextra parameters\\ngradual-prune technique\\nclassic networks\\ndeep network\",\"946\":\"gold\\nemotion recognition\\ncorrelation\\ncodes\\nautomation\\nannotations\\nface recognition\\nimage representation\\nintelligent robots\\nlearning (artificial intelligence)\\nfacial expression recognition\\ndeep-learning-based algorithms\\nfacial action units\\nsubtle facial behaviors\\nuncertain expressions\\nambiguous expressions\\nfacial expressions\\nau-expression knowledge constrained representation learning\\nau representations\\nau annotations\\nau-expression correlations\\nau classifiers\\nfacial representation\",\"947\":\"deep learning\\nimage segmentation\\ncorrelation\\nautomation\\nconferences\\ncomputed tomography\\nfeature extraction\\ncomputerised tomography\\ndeep learning (artificial intelligence)\\nimage representation\\nmedical image processing\\ntumours\\ncovariance self-attention dual path unet\\nrectal tumor segmentation\\nmultiscale detailed feature information\\ncsa-dpunet\\nfeature processing\\ncriss-cross self-attention module\\nfeature representation\\nct images\",\"948\":\"performance evaluation\\nvisualization\\nautomation\\nconferences\\ntactile sensors\\nproduction\\ncolor\\ncontrol engineering computing\\ndata visualisation\\nfabrics\\npattern classification\\ntactile information\\nfabric structure defect detection\\nvisual-based detection methods\\ntactile sensor\\ndefect detection system\\nvisual method\\nstructural defects\\ntactile sensing device\\nfabric production\\nfabric defect detection\\nirregular dyeing patterns\",\"949\":\"target tracking\\ncosts\\ntransportation\\nradar detection\\nobject detection\\nmillimeter wave radar\\nradar tracking\\ncollision avoidance\\nglobal positioning system\\nkalman filters\\nmobile robots\\noptical radar\\nposition control\\nremotely operated vehicles\\nroad vehicles\\nrobot vision\\nsensor fusion\\nstability\\nvehicle dynamics\\nmmw radar\\nunmanned ground vehicle leader-follower formation transportation\\ntracking method\\nmultisensor fusion perception\\ntarget detection stability\\ncontinuous tracking\\nstatic vehicles\\ndynamic vehicles\\nleader vehicle\\ndynamic leader\\necho vehicle\\nreal-time performance\\nleader-follower formation transportation experiments\\nmaximum tracking speed\\nmaximum tracking distance\\nvehicle target detection\\nunmanned ground vehicles\\nreal-time 3d-lidar\\ngps\\/imu fusion\\nvehicle detection\\nunstructured environment tracking\\nstatic obstacles\\nvehicle detection and tracking\\nunstructured environment\\nmulti-sensor fusion\\nunmanned ground vehicle\",\"950\":\"convolutional codes\\nvisualization\\nminimally invasive surgery\\ncorrelation\\nkinematics\\ngesture recognition\\nfeature extraction\\ncognition\\nlearning (artificial intelligence)\\nmedical robotics\\nrobot kinematics\\nsurgery\\ntelerobotics\\nmultimodal embeddings\\nhierarchical relational graph learning module\\nmultimodal methods\\nin-house visual-kinematics datasets\\nkinematics embeddings\\naccurate gesture recognition\\nrobotic surgery\\nautomatic surgical gesture recognition\\nintelligent cognitive assistance\\nrobot-assisted minimally invasive surgery\\nsurgical videos\\nrobotic kinematics\\ncomplementary knowledge\\nsurgical gestures\\nuni-modal data\\nmultimodal representations\\ninformative correlations\\nkinematics data\\ngesture recognition accuracies\\nmultimodal relational graph network\\nkinematics information\\ninteractive message propagation\\nlatent feature space\\nkinematics sequences\\ntemporal convolutional networks\\nmultirelations\",\"951\":\"three-dimensional displays\\nautomation\\nchange detection algorithms\\nconferences\\nurban areas\\nlighting\\ncameras\\ngeophysical image processing\\nimage registration\\nlearning (artificial intelligence)\\nmedical image processing\\npoint clouds\\nstructural change detection\\nscene changes\\ncity-scale scene change detection\\nstructural changes\\ntraversal\",\"952\":\"deep learning\\nvisualization\\nsemantics\\nfocusing\\nimage representation\\nfeature extraction\\nreal-time systems\\ndeep learning (artificial intelligence)\\nimage matching\\nrobot vision\\ncoarse-to-fine paradigm\\ncandidate frames\\nquery image\\nspatial geometric relationship\\nfinal place match\\ncoarse match stage\\ndeep learning network\\nglobal features\\ncandidate list\\nquery place\\nfine match stage\\nefficient feature matching algorithm\\nreal-time geometrical verification\\ncandidate places\\nlocal affine preserving\\nvisual place recognition\\nlong-term mobile robot autonomy\\nsemantic information\\nlocal affine preserving matching\",\"953\":\"productivity\\nembryo\\noptical microscopy\\nmicroinjection\\nmicroscopy\\nhigh-resolution imaging\\nprocess control\\nbiomems\\ncellular biophysics\\nimage resolution\\nmicromanipulators\\nmicroinjection technology\\nbiomedical research\\ngene manipulation\\nmicroinsemination\\noptical microscope environment\\nimage presentation\\nmicroinjection process\\nmultiple cells\\nmultiple injections\\ndifferent magnifications\\nembryo transfer processes\\nmagnification\\nlight intensity each time\\nmultiple microinjections\\nmicromanipulation system\\nhigh-resolution video\\nview-expansive microscope system\\nreal-time high-resolution\\nsimplified microinjection experiments\",\"954\":\"solid modeling\\nthree-dimensional displays\\nrobot vision systems\\nvirtual reality\\nuser experience\\nreal-time systems\\ntrajectory\\nend effectors\\ntelerobotics\\nimmersive teleoperation\\nvirtual reality frameworks\\nrobotic systems\\nvr systems\\naccurate trajectories\\nenhanced virtual reality framework\\nrobotic teleoperation\\nrobotic end-effector\\nemployed robot\\nremote user\\nvr controllers\\ntask execution progress\\nvr headset\\nevr system\\npure vr\\nrobot teleoperation\\nrobot model\",\"955\":\"visualization\\ntechnological innovation\\nheuristic algorithms\\nrobot vision systems\\npipelines\\nreal-time systems\\nstability analysis\\ncameras\\ncomputer vision\\ncontrol engineering computing\\nimage classification\\nimage colour analysis\\nimage segmentation\\nimage sensors\\nlyapunov methods\\nmanipulators\\nmobile robots\\nobject detection\\nobject tracking\\noptimal control\\nrobot vision\\nservomechanisms\\nvisual servoing\\nmultiinstance robotic reaching\\npipeline visual servo control algorithms\\nclf formulation\\nend-to-end visuomotor learning algorithm\\nvisually identical objects\\nimage-to-control\\nmonocular vision\\ncontrol-lyapunov function value\\noptimal control candidate\\nfully-convolutional network\\nmonocular rgb image\",\"956\":\"deformable models\\ndeep learning\\ncorrelation\\nlaser radar\\nautomation\\nconferences\\ntransfer learning\\ndata mining\\ndeep learning (artificial intelligence)\\ngeometry\\nimage processing\\noptical radar\\nsupervised learning\\nlarge-scale lidar point clouds\\ndeformable convolutions\\ngeometric patterns\\nfeature pyramid residual learning network\\nfg-conv\\nfeature correlation mining\\ngeometric-aware modeling\\nsupervised transfer learning\",\"957\":\"geometry\\ntraining\\nsolid modeling\\nthree-dimensional displays\\nsemantics\\nbenchmark testing\\ngraph neural networks\\ncad\\ncomputational geometry\\ncomputer vision\\nfeature extraction\\ngraph theory\\nimage classification\\nimage recognition\\nimage representation\\nimage segmentation\\nmultilayer perceptrons\\nobject recognition\\nsolid modelling\\neffective improvements\\npoint representations\\nlocal neighborhood graph construction\\nvertex representations\\nimportant local geometric information\\n3d point clouds\\ntraditional gnn\\ngeneral graphs\\nlocal geometry\\n3d point cloud processing\\nstanford 3d indoor scenes dataset\",\"958\":\"measurement\\nthree-dimensional displays\\ncosts\\nautomation\\nconferences\\nestimation\\nobject detection\\nimage fusion\\nimage matching\\nstereo image processing\\ndisparity axis\\nprivileged structure\\n3d contextual information\\ndisparity values\\n3d metric space\\ncost volume space\\nobject boundary regions\\nstereo object matching network\\n2d contextual information\\nstereo matching methods\\npixel-level correspondence\\nstereo images\\n3d object-level information\\nselective sampling\\n2d-3d fusion\\nvirtual-kitti 2.0 dataset\",\"959\":\"actuators\\ntorque\\nautomation\\nconferences\\nnull space\\naerospace electronics\\nquadratic programming\\ncontrol system synthesis\\nmanipulator dynamics\\nmanipulators\\nmotion control\\nnonlinear control systems\\nposition control\\nstability\\ntorque control\\nn-1 underactuated manipulators\\northogonal projection\\noperational space control formulation\\nplanar n-link underactuated manipulator\\npa n\\npassive first joint subject\\nactuator constraints\\nstabilization\\ntracking tasks\\ninherent first-order nonholonomic constraint\\noperational space controllers\\nunderactuation\\ncontrol framework\\nplanar pan-1 underactuated manipulators\\nquadratic programming based controller\",\"960\":\"feedback loop\\nautomation\\nservice robots\\nconferences\\nkinematics\\nrobot sensing systems\\nmanipulators\\nclosed loop systems\\ncontrol system synthesis\\nfeedback\\nforce control\\nindustrial manipulators\\nmotion control\\nnonlinear control systems\\nposition control\\nstability\\nuncertain systems\\nstable control strategy\\nexternal feedback loop\\nindustrial robotic manipulators\\nclosed architecture system\\njoint velocity\\nuncertain closed control architecture\\ninner control loop configuration\\ninner loop\\ncontroller design\",\"961\":\"actuators\\ntorque\\nforce\\nredundancy\\nbandwidth\\naerospace electronics\\ntask analysis\\nmanipulators\\nstability\\ntorque control\\noperational space control\\nactuator bandwidth limitation\\ntorque-based robot controllers\\nrobot manipulator\\nfranka emika panda\",\"962\":\"couplings\\ncovid-19\\npandemics\\nphantoms\\nmanipulators\\ncoronaviruses\\nsafety\\ndiseases\\nforce control\\nmanipulator dynamics\\nmanipulator kinematics\\nmedical robotics\\nmicroorganisms\\nmobile robots\\npath planning\\npneumatic actuators\\nservice robots\\nsafe rigid-flexible coupling manipulator\\nsars-cov-2 pandemic\\nop-swabs\\nmedical staff\\ncontact force\\nop-swab sampling tasks\\ntypical sampling paths\\nforce control capacities\\nswab quality\\nop-swab sampling approach\\ncovid-19 oropharyngeal swab sampling\\nrobotic system\\nintrinsically safe bionic micropneumatic actuator\\nmpa\\nrfc manipulator\\nkinematic modeling\\nmotion planning\\noral cavity phantom\\nfully-automated sampling\",\"963\":\"degradation\\nmicromanipulators\\ntracking\\nforce\\nmicrosurgery\\nrobot sensing systems\\nreal-time systems\\nactuators\\nmedical robotics\\nmotion control\\nsurgery\\nlateral load\\nrcm\\nrobot-aided operation\\ntremor cancellation\\nfully handheld microsurgical robot\\nrobot-assisted microsurgery\\noptimization framework\\n6pus parallel micromanipulator\\nremote center of motion\\nactuation modules\",\"964\":\"visualization\\nforce\\npneumatic systems\\nclimbing robots\\nfinite element analysis\\nclamps\\nrobots\\ncasting\\nmechanical contact\\nmobile robots\\npneumatic actuators\\nthree-dimensional printing\\npneumatic soft climbing robot\\nsoft materials\\npneumatic networks\\nprototype soft climbing robot\\nclimbing experiments\\n3-d printing\\nlayer-by-layer casting\\ncomplex unstructured environments\\nvisco-mechanical properties\\nminimum potential energy\\ncontact surface\",\"965\":\"actuators\\ncoordinate measuring machines\\nrobot kinematics\\nconferences\\nkinematics\\ncomputer graphics\\nsilicon\\ncomputational geometry\\nrobot dynamics\\nshear deformation\\nlocal coordinate system\\ngeometric robot\\ns-isothermic-surface robots\\ninverse kinematics\\ncoordinate transformation\",\"966\":\"geometry\\nautomation\\nart\\nmagnetomechanical effects\\nconferences\\nmechanical factors\\nflexible structures\\nactuators\\ndeployable structures\\ndesign engineering\\nmechanical stability\\nrigidity\\nrobot kinematics\\nstress-strain relations\\ncurved structures\\ndeployable mechanisms\\nmechanical response tuning\\ncurved mechanisms\\ndesign inspirations\\nmechanical structures\\norigami designs\\nactuations\\nstiffness tuning regimes\\nrobotic mechanical design\\nmechanical properties\\ntendon-driven mechanisms\\nparallel structures\\norigami-inspired snap-through bistability\\ndegree four vertexes\\nfolding paper\\nsnap-through bistable designs\\ngeometry analysis\\nstress-strain experiments\\nmagnetic mechanisms\\nbistable snap-through designs\",\"967\":\"simultaneous localization and mapping\\nautomation\\nconferences\\npipelines\\nestimation\\nmarkov processes\\nparallel processing\\nmobile robots\\nrobot vision\\nslam (robots)\\nstate estimation\\nprocessing loop-closure measurements\\nmarkov assumption\\nprobabilistic slam problem\\nnovel estimator design\\nmarkov property\\nbatch probabilistic slam estimator\\ntermed markov parallel tracking\\nproposed estimator\\nstate-of-the-art slam system\\nptam\\ntime-efficient framework\\nfavorite parallel-pipeline design\\ninaccurate state estimates\\nimperfect design\",\"968\":\"integrated optics\\nsimultaneous localization and mapping\\nautomation\\nimage matching\\nconferences\\nsonar\\noptical imaging\\ngraph theory\\nmarine engineering\\noptical images\\nsensors\\nslam (robots)\\ninter-session opti-acoustic two-view factor\\nconcurrent mapping\\ntemporal modality differences\\nsensor modality differences\\nmultivehicle mapping scenario\\nsensor modalities\\ninter-session sonar-optical image matching\\nopti-acoustic pair\\nopti-acoustic pairwise factor\\ninter-session measurements\\nopti-acoustic feature\\nmultisession underwater pose-graph slam\\ndata association\\nfield of view difference\\nfov difference\\nsuperglue\",\"969\":\"location awareness\\nintegrated optics\\nvisualization\\nsimultaneous localization and mapping\\nautomation\\nconferences\\nadaptive algorithms\\nimage sequences\\nmobile robots\\nrobot vision\\nslam (robots)\\nmonocular visual slam\\nline features\\ndegeneracy avoidance method\\nvisual slam algorithm\\ndiscernable features\\nmapped point features\",\"970\":\"codes\\nthree-dimensional displays\\neducation\\ndynamics\\ntutorials\\ntools\\ntrajectory\\ncomputer aided instruction\\ncontrol engineering computing\\ncontrol engineering education\\ngraphical user interfaces\\ninternet\\nonline front-ends\\npublic domain software\\npython\\nsoftware packages\\nteaching\\nros\\nide\\nurdf import\\ncollision checking\\nbranched mechanisms\\ntwists\\ntriple angles\\nquaternions\\nhomogeneous transformations\\nspatial mathematics\\ngithub\\nweb-browseable notebooks\\nmatlab\\nbrowser-based 3d graphics\\npython syntax\\nopen-source packages\\nrobotics toolbox\\nrobotics education\\nrobot models\\nopen-source community\",\"971\":\"automation\\nheuristic algorithms\\nconferences\\ndynamic scheduling\\nrobots\\ncollision avoidance\\nmobile robots\\nmulti-robot systems\\nparticle swarm optimisation\\nmultiple-place swarm foraging\\ndynamic robot chains\\nrobot swarms\\ncentral collection zone\\nmultiple-place foraging algorithm\\ndynamic depots\\nsearch areas\\nforaging location\\ndynamic chains\\nargos robot simulator\",\"972\":\"training\\nlocation awareness\\nautomation\\nconferences\\nprediction methods\\nfeature extraction\\ncollaborative work\\nautonomous aerial vehicles\\nimage fusion\\nlearning (artificial intelligence)\\nobject detection\\nrobot vision\\nvideo signal processing\\nmultiple-discontinuous-image saliency prediction\\ndrone exploration\\nmultiple discontinuous images\\ndeep relative saliency model\\nms-net starts\\nsingle-image saliency feature extraction network\\nrelative saliency information\\nvideo saliency prediction methods\\n360\\u00b0 image saliency prediction\\ngcn-based mechanism\\nmultiimage saliency fusion\",\"973\":\"heuristic algorithms\\nconferences\\nforce\\ndynamics\\nresists\\ncontrol systems\\nstability analysis\\nadaptive control\\ncontrol system synthesis\\nlyapunov methods\\nmarine vehicles\\nmobile robots\\nmotion control\\nnonlinear control systems\\nposition control\\nremotely operated vehicles\\nstability\\ntrajectory control\\nunderactuated unmanned surface vehicles\\nresist environmental disturbances\\npositioning control problem\\nunknown external disturbances\\ncontrol objectives\\nunderactuated usvs\\nlook-ahead point\\nvehicle heading\\npositioning error\\npredefined desired position\\nresultant environmental force\\npositioning control system\",\"974\":\"navigation\\nwheelchairs\\nconferences\\npipelines\\ntraining data\\nturning\\npath planning\\nhandicapped aids\\nlearning (artificial intelligence)\\nmobile robots\\ns2p2\\nrgb-d data\\nrobotic wheelchairs\\nfundamental capability\\nautonomous navigation\\nimpressive development\\ndeep-learning technologies\\nimitation learning-based path planning approaches\\nextensive time\\nhigh-level commands\\nself-supervised goal-directed path planning approach\\nplanned path labels\\ndata-driven path\\nexisting map-based navigation systems\\ntraditional path planning algorithms\",\"975\":\"visualization\\nservice robots\\nvision sensors\\nrobot sensing systems\\nreliability\\nmobile robots\\nsynchronization\\ncameras\\ndata integration\\ndeep learning (artificial intelligence)\\ndexterous manipulators\\nforce sensors\\nimage sensors\\nobject detection\\nrobot vision\\nmultimodal anomaly detection\\nobject slip perception\\nmobile manipulation robots\\nsensor signals\\nmultisensory data\\ndeep autoencoder model\\nheterogeneous data streams\\nrobot sensors\\nforce-torque sensor\\ninput data reconstruction\\nmobile service robot\\nhousehold objects\\nrobot behaviors\\nrobot arms slip perception\\ntactile-vision sensors\\nrgb\\ndepth cameras\",\"976\":\"systematics\\nroads\\npetri nets\\nweb and internet services\\ntransportation\\ntopology\\nplanning\\ncollision avoidance\\nroad traffic\\nroute coverage testing\\nautonomous vehicles\\nmap modeling\\ntransportation systems\\ntraffic congestion\\npublic roads\\nsystematic testing\\nav route planning\\ncroute\\nav testing criterion\\nlabeled petri net\\ntraffic signs\\njunctions\\nroute feature\\njunction classification\\ntopology feature\\nroute types\\nav collision avoidance\\nbaidu apollo\\nlgsvl simulator\\nsan francisco\",\"977\":\"image segmentation\\nthree-dimensional displays\\nshape\\nrobot vision systems\\nhuman-robot interaction\\nestimation\\nvirtual reality\\ngesture recognition\\nneural nets\\ndirection estimation\\ndepth images\\n3d pointing devices\\nhand gesture-based interfaces\\n3d pointing purposes\\nrobust hand gesture-based\\naccurate hand gesture-based\\nnonexistence\\naccurate data-set\\naccurate ground-truth\\n3d convolutional neural network\\naccurate methods\\nrobust methods\\nless computationally expensive methods\",\"978\":\"laser radar\\nautomation\\nnavigation\\nconferences\\nbuildings\\nreinforcement learning\\nrobot sensing systems\\ncontrol engineering computing\\ngraph theory\\nlearning (artificial intelligence)\\nmobile robots\\noptical radar\\npath planning\\ndeep reinforcement learning framework\\nautonomous indoor robot exploration\\npresented method features\\npattern cognitive nonmyopic exploration strategy\\nextendable navigation network\\nenn\\nhigh-dimensional indoor euclidean space\\nlearned q-network\\nexploration time\",\"979\":\"location awareness\\ngeometry\\nforce measurement\\nconferences\\nestimation\\ntactile sensors\\nkinematics\\nfriction\\nmechanical contact\\nmotion measurement\\nrobot dynamics\\nrelative-motion tracking\\ndistributed tactile measurements\\ntactile sensing\\nkinematic constraints\\nfrictional constraints\\nconstraint-based estimation problem\\nobject motion\\nrigid-body mechanics\\nline contacts\\ndistributed tactile sensing\\ncontact-rich manipulation scenarios\\ncontact sensing problem\\ngrasped rigid object\",\"980\":\"robotic assembly\\nuncertainty\\nforce\\neducation\\ntrajectory\\ntask analysis\\nforce control\\nassembling\\nlearning (artificial intelligence)\\nmotion control\\ncontact forces\\nnominal motion trajectory\\nhuman demonstrations\\nrealistic force profile\\nsimulated force profiles\\nactual robotic assembly\\nassembly process\\ncombined learning-based framework\\nhuman assembly skills\\nhybrid trajectory learning\\nforce learning\\nhierarchical imitation learning\\nreinforcement learning-based force control scheme\\noptimal force control policy\\ngoal-conditioned imitation learning\\ntrajectory learning policy\\nskill level offline\\nassembly task\\nhigh-quality trajectories\\nsuitable force control policies\\nrobotic imitation\\nrobotic assembly tasks\\nlow-clearance insertion trajectories\",\"981\":\"actuators\\nautomation\\nconferences\\nprototypes\\ngrasping\\ncomplexity theory\\nrobots\\nforce control\\ngrippers\\npower transmission (mechanical)\\ntwisted string actuation\\ninput-output transmission ratio\\nvariable transmission ratio\\ncompact robotic systems\\nautomatic transmission shifting algorithm\\nautonomous control strategies\\ncompact automatic transmission\\nrobotic gripper\",\"982\":\"actuators\\npower demand\\nconferences\\nmodulation\\npower transmission\\nhuman-robot interaction\\nbending\\nclosed loop systems\\nelastic constants\\nelasticity\\npower transmission (mechanical)\\nsprings (mechanical)\\nenergy efficiency\\nstiffness range\\nactive stiffness modulation\\npositive stiffness modulation\\npassive negative stiffness element\\npower transmission design\\nenergy-efficient stiffness modulation\\ncompliant actuator concept\\nhigh energy-efficiency\\nfast stiffness modulation speed\\nwide range stiffness modulation\\nhuman power assistance\\nrobot actuation\\nsafe physical human-robot interaction\\nconventional compliant actuators\\nvariable stiffness actuators\\nseries elastic actuators\\nequilibrium position\\ncompliance\\nactuator\\nsafe human-robot interaction\\npassive\\nnegative stiffness\\nfast and energy-efficient\",\"983\":\"mechanical sensors\\nvibrations\\nanalytical models\\ntorque\\nsensor phenomena and characterization\\nstrain measurement\\nshock absorbers\\ncalibration\\ndesign engineering\\nfinite element analysis\\nmachine tools\\nrigidity\\nstrain gauges\\ntorque measurement\\nnovel variable resolution torque sensor\\nvariable stiffness principle\\nsensor design\\napplied external torque\\nlarge range force-torque measurements\\nresistive strain gauge\\ndynamic models\\nati industrial automation\\nnatural frequency\\nfrequency 67.3 hz\",\"984\":\"sensitivity\\nthree-dimensional displays\\nnavigation\\nimage edge detection\\ntools\\nrobot sensing systems\\nfeature extraction\\ninertial navigation\\nmobile robots\\nprogram testing\\ncomparison methods\\nexisting simulators\\nphoto-realistic frameworks\\nspecific environmental conditions\\nsensor specifications\\nunwillingly many polluting effects\\ncorresponding failure modes\\ntest sequences\\nunified evaluation framework\\ndifferent vins methods\\nspecific environmental parameters\\nsensor parameters\\ntested parameters\\nunified radar charts\\nunified testing\\nvisual-inertial navigation system algorithms\\nresearch community\\nhand-held devices\\ncompeting approaches\",\"985\":\"visualization\\nsimultaneous localization and mapping\\nnavigation\\nanimals\\ndata acquisition\\nvegetation mapping\\nvision sensors\\nagriculture\\nimage sensors\\nmobile robots\\noptical radar\\npath planning\\nevent-based visual navigation methods\\ndynamic vision sensor\\nvisual odometry\\nevent-based visual slam methods\\nevent-based vision dataset\\ncomputer vision\\nneuromorphic vision\\nmobile robot navigation\\ncustom-designed sensor bundle\\nrgb-d camera\\nlidar\\nagricultural environmental sensors\",\"986\":\"performance evaluation\\nthree-dimensional displays\\natmospheric measurements\\nplants (biology)\\nrendering (computer graphics)\\nparticle measurements\\nloss measurement\\nagriculture\\ncrops\\ngenetic engineering\\nimage reconstruction\\nmobile robots\\nrobot vision\\nin-field phenotyping\\nself-consistency loss\\nplant growth monitoring\\nautomated fashion\\nfield environments\\nstatic scanning environment\\ndifferentiable rendering\\ngeneric 3d template\\nplant template\\nphenotypic traits\\n3d reconstruction\\nplant species\\nmodern agriculture\",\"987\":\"solid modeling\\nfriction\\nconferences\\nmoon\\nvirtual reality\\nsoil\\nrendering (computer graphics)\\ngravitation\\ngravity\\nhaptic interfaces\\nplanetary surfaces\\ntelerobotics\\nefficient haptic rendering\\nplanetary regolith\\nmodel-mediated teleoperation\\nmodelling regolith\\nmodel parameters\\nphysical characteristics\\ndifferent density profiles\\ndifferent gravitational fields\\ngravity field-an\",\"988\":\"interpolation\\ncomputational modeling\\nquaternions\\nkinematics\\ntrajectory\\ntime factors\\ncomputational complexity\\nconvex programming\\nmanipulators\\nmobile robots\\nmotion control\\noptimisation\\npath planning\\npolynomials\\nposition control\\npredictive control\\nkinematic constraints\\ntrapezoidal acceleration model\\nnonlinear constraint\\nnonconvex optimization problem\\nresultant trajectory\\nstraight-line movement\\nrobot manipulator\\nbridged optimization strategy\\nmodel predictive control\\nmultiple waypoints\\nspecified blending parameters\\ncartesian space\\nrobot motion planning\\nspherical cartesian waypoints\",\"989\":\"location awareness\\nlegged locomotion\\nvisualization\\nnavigation\\nrobot vision systems\\nstairs\\ncameras\\nindustrial robots\\nrobot dynamics\\nrobot vision\\nslam (robots)\\nstereo image processing\\ndynamic industrial environment\\nsingle-camera\\nnonsynchronized multicamera vt&r\\ncamera streams\\ndynamic quadruped robot\\nactive multicamera visual teach and repeat\\ncpm\\ncamera performance models\\ncramped industrial environment\\nanymal\\nbackward facing stereo camera\\nforward facing stereo camera\",\"990\":\"deep learning\\ncosts\\nautomation\\nconferences\\nneural networks\\nmemory management\\nsystem-on-chip\\ncomputational complexity\\ndeep learning (artificial intelligence)\\nimage classification\\nsearch problems\\nms-ranas\\nhandcrafted neural networks\\nhighly efficient deep neural networks\\nmultiscale resource-aware neural architecture search\\nimage classification task\\ncomputational constraints\\nmemory requirements\\none-shot architecture search approach\\nsearch cost reduction\\nanytime prediction setting\\nearly classifier\",\"991\":\"training\\nnavigation\\nrobot vision systems\\nreinforcement learning\\ngray-scale\\naerospace electronics\\ncameras\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nvision-based mobile robotics obstacle avoidance\\ndeep reinforcement learning\\nfundamental problem\\nsimple 3d environments\\nseparate problems\\nobstacle detection\\nunderstanding highly complex situations\\nobstacle avoidance problem\\ndata-driven end-to-end deep learning approach\",\"992\":\"navigation\\nscalability\\nrobot sensing systems\\nturning\\nsoftware\\nreal-time systems\\nrobots\\nmobile computing\\nmobile robots\\npath planning\\nrobot programming\\nsmart phones\\nelectric vehicle\\nrobot body\\nstandard android smartphones\\nsoftware stack\\nadvanced robotics workloads\\nopenbot\\nsensory richness\\ncomputational power\\ncommunication capabilities\\nsmartphones\",\"993\":\"sufficient conditions\\nuncertainty\\ntorque\\nmotorcycles\\nobservers\\nstability analysis\\nrobustness\\ncontrol system synthesis\\nconvergence\\nlinear matrix inequalities\\nlinear systems\\nlyapunov methods\\nnonlinear control systems\\nrobust control\\nstability\\nuncertain systems\\nvehicle dynamics\\nquasilpv unknown input observer\\nnonlinear outputs\\nmotorcycle lateral dynamics\\npertinent states\\nunknown inputs\\nrider action\\nmotion transformation frames\\ninertial sensors\\nlocal frame\\nunknown input observers\\nvariable output matrix\\nground truth measurements\\nbody-fixed frame\\nparametric uncertainties\\nsensors noise\\nnonlinear parameter-dependent output equation\\nunmeasured premise variables\\nobserver design\\nobserver synthesis\\nstability study\\nstate stability property\",\"994\":\"fabrication\\nactuators\\ntorque\\nforce\\nprototypes\\nkinematics\\nmuscles\\nbeams (structures)\\nelectric actuators\\nfinite element analysis\\nmuscle\\npiezoelectric materials\\npneumatic actuators\\npneumatic control equipment\\npolymers\\nseals (stoppers)\\nsprings (mechanical)\\ntorsion\\nfoldable twisting skeleton\\noutput torque\\ngiven twisting angle\\nsoft artificial muscle\\npneumatically actuated systems\\nnovel torsional actuator augmenting twisting skeleton\\nartificial muscle technology\\norigami-inspired twisting skeleton\\ntorsional actuator design\",\"995\":\"laser radar\\nthree-dimensional displays\\nheuristic algorithms\\nredundancy\\nkinematics\\ncameras\\nsensors\\ndistance measurement\\nmotion estimation\\noptical radar\\npose estimation\\nautonomous vehicles\\nreliable ego-motion\\nautonomous mobile systems\\nodometry estimation methods\\nmultiple odometry algorithms\\n3d lidar scanner\\nmonocular camera\\nreliable state estimation\\nsanity checks\\ndynamic constraints\\nkinematic constraints\\ncurrent lidar scan\\npoint cloud map\\nkitti odometry dataset\\nindividual odometry methods\\nredundant odometry\\nego-motion estimating approaches\",\"996\":\"deep learning\\nheating systems\\nvisualization\\nlaser radar\\nuncertainty\\nthree-dimensional displays\\ngraphics processing units\\ncameras\\ncomputational complexity\\nconvolution\\ndistance measurement\\nimage sensors\\nlearning (artificial intelligence)\\nmarkov processes\\nmobile robots\\nmonte carlo methods\\nneural nets\\nobject detection\\nparticle filtering (numerical methods)\\npose estimation\\nrobot vision\\nslam (robots)\\nheatmap regression\\ndeep convolutional odometry\\nself-driving vehicles\\nstrong competition\\nvisual localisation\\nlidar\\nimportant depth information\\nhigh localisation performance\\nlocalisation problem\\nlikelihood field\\nmemory size\\nexpensive convolutions\\nentire likelihood volume\\ngrid based approaches\\nefficient particle filters\\nmcl\\nlikelihood volumes\\ngpu-bound 3d convolutions\\ngrid based methods\\nnovel cnn-based localisation approach\\nleverage modern deep learning hardware\\ngrid-based markov localisation approach\\nhybrid convolutional neural network\\nimage-based localisation\\nodometry-based likelihood propagation\\nresulting approach\\nstate-of-the-art localisation systems\",\"997\":\"location awareness\\nvisualization\\nsimultaneous localization and mapping\\nplanets\\nnavigation\\nliquid crystal displays\\ntrajectory\\naerospace robotics\\nautonomous aerial vehicles\\ndistance measurement\\nmobile robots\\npath planning\\nrobot vision\\nplanetary exploration robots\\nplanetary robotics navigation\\nplanetary environment\\nplanetary analogue real dataset\\nautonomous cooperative visual navigation\\ngps\\ncooperative visual odometry\",\"998\":\"jacobian matrices\\nenergy consumption\\nmorphology\\nlibraries\\nend effectors\\ntask analysis\\nrobots\\nmanipulator dynamics\\noptimisation\\non-demand robot morphology optimization\\nreconfigurable robots\\nmodular reconfigurable\\nminimum-effort task-based design optimization\\noptimal solution\\nmultiarm robots\\nsingle-arm\\nminimum effort optimization problem\\nminimum-effort objective\\noptimal fit-to-task robot structures\\nsolution space\\nmultiarm robotic systems\\nvarying tasks\",\"999\":\"legged locomotion\\nactuators\\nsimulation\\nenergy efficiency\\nhardware\\ntrajectory\\ntask analysis\\nenergy conservation\\nfriction\\ngenetic algorithms\\nstructural integrity\\ntwo-joint monoped robot\\nenergy savings\\nconcurrent optimization\\nrobot size\\ncomputational design\\nenergy-efficient legged robots\\nrobotic systems\\nhardware parameters\\ncontrol trajectories\\nrobot design\\nelectro-mechanical models\\nbi-level optimization\\nrobot structure\",\"1000\":\"legged locomotion\\nactuators\\nenergy consumption\\npower demand\\nconferences\\nfocusing\\nenergy efficiency\\npower consumption\\nrobot kinematics\\nmechanical antagonism\\nserial legs\\nextensive power waste\\nactuator\\nlocomotion task\\nconsumes power\\ntotal actuation power consumption criterion\\nnominal robot toe trajectory\\nrobotic leg design\\nalternative leg designs\\ngaits\\nconstant horizontal velocity\\nconstant height\\nsimplified parallel designs\\nserial designs\\nrealistic mechanical parameters\\nactuation parameters\\npower demands\\nleg workspace\\nuseful conclusions\\nmechanical power antagonism\",\"1001\":\"smoothing methods\\nergonomics\\nforce\\ncollaboration\\nhumanoid robots\\ngrasping\\ntorque measurement\\ncollaborative filtering\\nforce control\\nhuman-robot interaction\\nmobile robots\\nmulti-agent systems\\noptimisation\\nwhole-system optimization\\nicub humanoid robots\\nforce ergonomic optimization\\nagent postural\\nrobot-robot collaborative lifting\\nshared control\",\"1002\":\"adaptation models\\nautomation\\nperturbation methods\\nconferences\\nswarm robotics\\nrobot sensing systems\\nsensors\\nbayes methods\\ngradient methods\\nmobile robots\\nmulti-robot systems\\noptimisation\\nsearch problems\\nrobot swarms\\nswarm map-based bayesian optimisation\\nrapid performance recovery\\nunforeseen environmental perturbations\\nbehaviour adaptation\\nhomogeneous swarm\\nswarm map-based optimisation decentralised\\nsmbo-dec\\nasynchronous batch-based bayesian optimisation\\nthymio robot swarm\\nevolutionary robotics\\nbayesian optimisation\",\"1003\":\"uncertainty\\natmospheric measurements\\ntransforms\\nprobabilistic logic\\nrobot sensing systems\\nparticle measurements\\ninference algorithms\\nbayes methods\\ncomputational geometry\\ngraph theory\\niterative methods\\nmobile robots\\nnavigation\\nnormal distribution\\npose estimation\\nrobot vision\\nsensor fusion\\nbayesian pose estimation\\npoint clouds\\nnonprobabilistic data association\\nbayesian estimation\\nprobabilistic scan matching\\norientation estimation\\nhigh resolution sensor\\nautonomous navigation\\ndata association\\npose inference\\njoint factor graph\\nnear-optimum maximum a posteriori estimates\\nnormal distributions transform\\nimplicit moving least squares\\niterative message passing\",\"1004\":\"estimation error\\nmagnetometers\\ninertial sensors\\nquaternions\\nreinforcement learning\\nsolids\\nnumerical simulation\\nacceleration measurement\\nangular velocity measurement\\ncomputerised instrumentation\\ndeep learning (artificial intelligence)\\ngradient methods\\nlyapunov methods\\nneural nets\\nposition measurement\\nsensors\\nstate estimation\\ndrl-based orientation estimation method\\nperformance guarantee\\norientation estimation errors\\nestimator gains\\nlyapunov function\\ndeep neural networks\\ndrl estimator\\narbitrary estimation initialisation\\ndeep reinforcement learning\\nmagnetometer\\nangular velocity profile\\nnumerical simulations\",\"1005\":\"manifolds\\nlocation awareness\\ncrawlers\\nmetals\\ninspection\\nmobile robots\\nstate estimation\\nkalman filters\\nnonlinear filters\\nservice robots\\nsplines (mathematics)\\nultra-wideband localization\\nautonomous metal structure inspection\\ndifferent manifolds\\nmanifold constraints\\ncurved metal surface\\nconsistent state estimation\\nmanifold invariant extended kalman filter\\nhigh noise potential\",\"1006\":\"visualization\\nruntime\\nprototypes\\ngrasping\\ndigital signal processing\\nswitches\\ntrajectory\\nbiomechanics\\nelectromyography\\nmedical control systems\\nmedical signal processing\\nneural nets\\npattern classification\\nprosthetics\\nsignal classification\\nbinary-lorax\\nlow-latency runtime adaptable xnor classifier\\nprosthetic hands\\nelectromyographic signals\\nsemiautonomous prosthesis\\nlow-latency runtime adaptable classifier\\nsemiautonomous grasping task\\nbinary neural network accelerator\\nhigh-throughput xnor operations\\npower consumption\\nemg signals\",\"1007\":\"training\\nthree-dimensional displays\\npipelines\\ngrasping\\nrobot sensing systems\\nreliability\\ngrippers\\ndexterous manipulators\\nhaptic interfaces\\nlearning systems\\nmobile robots\\nrun-times unsuitable\\nclosed-loop grasping\\nend-to-end network\\n6-dof parallel-jaw grasps\\ndepth recording\\ngrasp representation\\nrecorded point cloud\\npotential grasp contacts\\nobserved point cloud\\n4-dof\\nclass-agnostic approach\\nrobotic grasping study\\nunconstrained environments\\ncluttered environments\\nautonomous robotic manipulation\\ncomplex sequential pipelines\\npotential failure points\\ncontact-graspnet\\n6-dof grasp generation\\nunseen object grasping\\n6-dof grasp learning\",\"1008\":\"automation\\nconferences\\npose estimation\\ngrasping\\nmanipulators\\nfeature extraction\\ninference algorithms\\nconvolutional neural nets\\nimage fusion\\nimage sampling\\nlearning (artificial intelligence)\\nmultiscale spatial pyramid module\\nfast robotic grasping detection\\nefficient network\\nfully convolutional neural network\\nrobotic grasps\\ndepth images\\nresidual squeeze-and-excitation network\\ndeep feature extraction\\nrsen block\\nmsspm\\nmultiscale contextual information\\nhierarchical feature fusion\\nfused global features\\nhigh grasp detection accuracy\\nur5 robot arm\\ntime 5.0 ms\",\"1009\":\"deep learning\\nautomation\\nconferences\\nsemantics\\nfeature extraction\\nconvolutional neural networks\\ngrippers\\nconvolutional neural nets\\nimage segmentation\\nobject detection\\nrobot vision\\nend-to-end trainable deep neural network\\nrobotic grasp detection\\nsemantic segmentation\\nend-to-end trainable cnn-based architecture\\nhigh quality results\\nparallel-plate gripper\\ncalculated grasp detection\\nincreases grasp detection\\npopular grasp dataset\\ngrasp candidates\\nrgb\",\"1010\":\"visualization\\nautomation\\nconferences\\npose estimation\\nbenchmark testing\\nrendering (computer graphics)\\ntask analysis\\nobject recognition\\ndomain randomisation\\nsim-to-\\nrobotics\\nreal-world images\\noptimal transfer\\ncomprehensive benchmarking study\\ndifferent choices\\nkey experiments\\nreal-world object\\nrendering quality\\nhigh-quality images\\nlow-quality images\",\"1011\":\"training\\nlaser radar\\nradar measurements\\nstochastic processes\\nradar\\nradar imaging\\npredictive models\\nimage segmentation\\noptical radar\\noptimisation\\nradar signal processing\\nremote sensing by radar\\ncyclical consistency criterion\\nbackward model\\ndown-stream segmentation model\\nreal-world deployment\\nsimulate radar data\\nreal-world applications\\nrealistic radar data\\ndata-driven approaches\\nradar processing\\nnotoriously complex image formation process\\nradar sensor model\\nfaithful radar observations\\nsimulated elevation maps\\nadversarial approach\\nforward sensor model\\nunaligned radar examples\\nworld state\",\"1012\":\"automation\\nsystematics\\nservice robots\\nconferences\\nproduction\\nmultitasking\\nsoftware\\nfinite state machines\\nindustrial robots\\nmobile robots\\nmulti-robot systems\\ntrees (mathematics)\\nproductive multitasking\\nsmall-batch production\\neconomical constraints\\nmodular software components\\nconcurrent tasks\\npotentially game-changing\\ncycle time constraints\\nsystematic approach\\npartial executions\\nfinite-state machines\\nagile robotics for industrial automation competition\\nbehavior trees\\nintelligent and flexible manufacturing\\nrobot skill\",\"1013\":\"automation\\nconferences\\nlayout\\naluminum\\nplanning\\ntask analysis\\ncollision avoidance\\naluminium\\ndesign engineering\\ngenetic algorithms\\nindustrial robots\\nmanipulators\\ndual-arm robotic system\\nautomated planning\\nrobotic workcell layout\\nconstrained nonlinear optimization problem\\ninitial design\\ngenetic algorithm\\naluminum profiles\\nal\",\"1014\":\"visualization\\nthree-dimensional displays\\nultrasonic imaging\\ntracking\\nrobot vision systems\\nskin\\ntrajectory\\nbiomechanics\\nbiomedical ultrasonics\\nblood vessels\\ncameras\\nimage motion analysis\\nmedical image processing\\nmedical robotics\\nphantoms\\nrobot vision\\ninteroperator variability\\nobject movement\\nvision-based robotic us system\\n3d compounded images\\nmanually planned sweep trajectory\\nmotion-aware robotic 3d ultrasound\\nrobotic three-dimensional ultrasound imaging\\nlimb artery tree\\nextracted 3d trajectory\\ngel phantom\\ndepth camera\",\"1015\":\"solid modeling\\nthree-dimensional displays\\nautomation\\ncomputational modeling\\nbiological system modeling\\nsurgery\\nkinematics\\nbone\\ncomputerised tomography\\ngait analysis\\nmedical image processing\\nprosthetics\\nnovel 3d printed knee model\\nprosthesis placement\\nfour-bar mechanism\\nimplant insertion\\nmotion model\\nfemoral component\\n3d printed mechanical model\\nct images\\nknee coronal axis\\ntotal knee replacement surgery\\npatient knee flexion\\npatient kinematics\",\"1016\":\"integrated optics\\ngeometry\\nthree-dimensional displays\\ncolonoscopy\\npredictive models\\nreal-time systems\\ncomplexity theory\\nbiological tissues\\ncancer\\ncomputerised tomography\\nedge detection\\nendoscopes\\nimage reconstruction\\nmedical image processing\\nnoncurved model\\noptical colonoscopy\\ncolorectal cancer screening\\ninternal tissue\\nblindspot detection\\n3d reconstruction\\nparametric model\\ncolonoscopy simulator\\ncurved model\",\"1017\":\"location awareness\\nsurface reconstruction\\nthree-dimensional displays\\nultrasonic imaging\\nshape\\nreal-time systems\\ntrajectory\\nbiological tissues\\nbiomedical mri\\nbiomedical ultrasonics\\ncancer\\nimage reconstruction\\nimage registration\\nmedical image processing\\nphantoms\\nscanning trajectory\\nus feedback\\npatient specific model\\nus images\\nrealistic breast phantom\\nfully autonomous quality robotic us volume acquisitions\\nhigh quality robotic us volume acquisitions\\nof-plane corrections\\nautonomous robotic breast ultrasound acquisitions\\nbreast cancer\\ndiagnostic workflow\\nbiopsy phase\\nus acquisitions\\nmultiple benefits\\nenhanced lesion localization\\nmri data\\ncurrent commercial 3d\\nrobotic us scanners\\nhigh quality volumes\\nconventional linear probe\",\"1018\":\"fabrication\\nactuators\\nfuses\\nmorphology\\nbending\\nsoft robotics\\nmanipulators\\nmobile robots\\nmotion control\\nrobots\\nhighly manoeuvrable eversion robot\\nsoft robot\\nconventional robots\",\"1019\":\"training\\nfault tolerant systems\\nrotors\\nreinforcement learning\\naerospace electronics\\nsupervisory control\\nstability analysis\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nfault tolerant control\\nfuzzy control\\nhelicopters\\nlarge-scale systems\\nlearning systems\\nmobile robots\\nremotely operated vehicles\\nself-adjusting systems\\nstability\\nthree-term control\\nuav\\ncomplex autonomous systems\\nunmanned aerial vehicles\\nmodel-based controllers\\ncomplex systems\\nlearning-based controllers\\nlearned supervisory controller\\npid controllers\\nkey stability guarantees\\nrobust reinforcement learning approach\\nsupervisory control parameters\\nrotor faults\\nfault-tolerant control properties\\nhybrid ftc approach\\nquadcopter\",\"1020\":\"missiles\\natmospheric modeling\\nreinforcement learning\\ntools\\nmathematical models\\nsafety\\nrisk management\\naerospace computing\\ncontrol engineering computing\\nmilitary aircraft\\nmilitary computing\\nmissile guidance\\noptimisation\\ncontrol barrier functions\\nexplicit risk mitigation\\nadversarial environments\\nhigh-risk activity\\nworst-case future separation\\nenemy missile\\ncbfs\\nclosed-form expressions\\nguided missile\\nhigh fidelity simulation models\\nvalue functions\\nair combat scenario\\nsafe exploration\",\"1021\":\"visualization\\nthree-dimensional displays\\ntarget tracking\\nsensor fusion\\nrobot sensing systems\\nsensors\\ntrajectory\\ncameras\\nimage sensors\\nmobile robots\\nobject detection\\nobject tracking\\npath planning\\ntrajectory control\\ndistant objects\\nsimple tracking formulation\\nobject observations\\nsensor modalities\\ndistant incoming objects\\nprecise trajectory localization\\ndepth-sensing range\\nmultiobject tracking\\nwell-informed motion planning\\nnavigation\\ndepth sensors\\nrich visual signal\\neagermot\\n3d space\\nlimited sensing range\\nimage domain\\nkitti\",\"1022\":\"visualization\\nautomation\\nconferences\\nforce\\ndecision making\\nsurface texture\\nresponse surface methodology\\ncellular neural nets\\nedge detection\\nfeature extraction\\nimage colour analysis\\nimage denoising\\nimage texture\\nmedical image processing\\nmedical robotics\\nmobile robots\\nneural nets\\nrobot vision\\nmobility\\nmanipulation abilities\\nspines\\nvacuum suction\\ndifferent contact surfaces\\ntextural properties\\noutdoor surfaces\\nindoor surfaces\\nadditional 200 synthetic images\\nenhanced dataset\\nvisual surface examination model\\nfaster rcnn\\nfaster r-cnn model\\nideal surfaces\\nfaster r-cnn-based decision making\\nnovel adaptive dual-mode robotic anchoring system\\nnovel adaptive anchoring module\",\"1023\":\"adaptation models\\nautomation\\nconferences\\ngrasping\\nintelligent robots\\nfault diagnosis\\ngrippers\\nlearning systems\\nrobot action diagnosis\\nexperience correction\\nexecution failure\\nintelligent robot\\nexecution policy\\nexecution process\\nconstraint learning\\nexperience acquisition process\\nfailure diagnosis\\nparameterised action execution models\\ndiagnosis process\\ndiagnosis algorithm\\nparameterised execution model falsification\\nprecondition model learning\\nhandle grasping\",\"1024\":\"visualization\\nsimultaneous localization and mapping\\nautomation\\nconferences\\npose estimation\\ncameras\\nrobustness\\nfeature selection\\nimage fusion\\nimage reconstruction\\nimage representation\\nmobile robots\\nnonlinear programming\\nobject tracking\\npattern clustering\\nrobot vision\\nslam (robots)\\nnonlinear optimization\\nsuperpixelized regions\\nclustering\\nplanar template-based trackers\\nmonocular camera\\nvisual slam\\nplanar environments\\ndense monocular slam\\ntt-slam\\ndense planar reconstruction\\norb-slam\\nkeypoint-based techniques\\nkeyframe selection\\ndata association\\ntemplate trackers\\nmultiplanar scene representation\\ncamera poses\",\"1025\":\"geometry\\nimage segmentation\\nvisualization\\nsimultaneous localization and mapping\\nmotion segmentation\\nheuristic algorithms\\ndynamics\\ncameras\\nimage reconstruction\\nimage representation\\nimage sensors\\nmotion estimation\\nobject detection\\nobject tracking\\nrobot vision\\nslam (robots)\\nsolid modelling\\nstereo image processing\\ndynamic object tracking\\nvisual slam\\nexisting slam systems\\nhighly dynamic environments\\ndot combines instance segmentation\\nmultiview geometry\\ndot segments first instances\\npotentially dynamic objects\\nshort-term tracking\\norb-slam2\\nhighly dynamic scenes\",\"1026\":\"image segmentation\\nthree-dimensional displays\\nshape\\npose estimation\\noptimization methods\\nmanuals\\nkinematics\\ncameras\\nconvolutional neural nets\\nimage denoising\\nobject detection\\noptimisation\\nrobot vision\\nshape recognition\\nprimal 2d keypoint detection\\nshape-based keypoint adjustments\\nperspective-n-point algorithm\\n3d point correspondences\\npublic dataset\\nannotated segmentations\\nuniversal robot ur5 manipulator\\ninitial key-point detection\\nsingle-shot pose estimation\\nshape segmentation\\noptimization method\\nrobotic manipulators\\nmonocular images\\nsegmented shape\\nconvolutional neural networks\\nkeypoint-based single-shot camera-to-robot pose estimation\",\"1027\":\"automation\\nconferences\\npose estimation\\ntoy manufacturing industry\\nconvolutional neural networks\\noptimization\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\ngradient methods\\nimage colour analysis\\nobject detection\\noptimisation\\nsymmetry\\nvectors\\nsymmetric objects\\nclosed loop\\nsymmetry-aware loss\\ngradient-based optimization\\nclosed symmetry loop\\ncontinuous rotational symmetry\\ndiscrete rotational symmetry\\nrgb\\ncnn-based pose estimation\\nobject symmetry handling\\nconvolutional neural network-based pose estimation\\nt-less dataset\\ndeep learning\",\"1028\":\"three-dimensional displays\\nimage color analysis\\ntraining data\\nestimation\\ntools\\nnetwork architecture\\ndata models\\nagriculture\\ncollision avoidance\\ncrops\\nimage colour analysis\\nimage segmentation\\nmobile robots\\nobject detection\\nreliable time estimation\\nreal-time estimation\\ncompact network architecture\\nsoft fruit\\nstrawberries\\ndepth images\\nrealistic simulation environment\\npublicly available images\\nstate space\\nfruit picking applications\\nripe fruit\\nrobotic harvester\\ncollision-planning\",\"1029\":\"extrapolation\\nmeasurement uncertainty\\nurban areas\\nprobabilistic logic\\nreal-time systems\\ntime measurement\\nstate estimation\\ncollision avoidance\\ngaussian processes\\nmobile robots\\nprobability\\nstochastic processes\\nterrain mapping\\ngeometric terrain model\\nprobabilistic terrain estimation\\noff-road driving\\nautonomous driving\\nrecursive gaussian state estimation\\nposteriori estimation\\nprobabilistic terrain estimate\",\"1030\":\"pipelines\\nstreaming media\\ncameras\\nprobabilistic logic\\nrobustness\\ntrajectory\\nvelocity measurement\\nimage motion analysis\\nobject tracking\\nrobot vision\\n6-dof object motion\\nlow latency tracking\\nhigh-dynamic range imaging\\ndegree-of-freedom\\nobject motion tracking\\nframe-based cameras\\nprobabilistic generative model\\nobject trajectory\\nslower rate image frames\\nimage alignment\\nobject tracking scenarios\",\"1031\":\"deformable models\\nvisualization\\nthree-dimensional displays\\ncomputational modeling\\nminimization\\nfinite element analysis\\nnumerical models\\ncameras\\ncomputer vision\\ndeformation\\nimage colour analysis\\nobject tracking\\noptimisation\\nfem\\ncorotational finite element method\\ngeometric error\\njoint minimization\\ncoarse 3d template\\ndeforming objects\\nrgb-d camera\\nsoft objects\\nphysics-based models\\nvisual tracking\\ndirect photometric intensity error\\ndeformation tracking\\nnonlinear error terms\\nnumerical optimization\\nphysical model based deformation representation\\ncomputer vision based tracking methodology\\nmaterial properties\",\"1032\":\"training\\ntarget tracking\\nautomation\\ntrajectory planning\\nperturbation methods\\nconferences\\ngrasping\\ngrippers\\nimage registration\\niterative methods\\nmanipulator dynamics\\nobject tracking\\ntrajectory control\\ninferior grasping points\\nunknown objects\\nreactive grasping\\nunknown environment\\npoint sampling\\ndynamic trajectory planning\\niterative closest point approach\\n6-dof unknown object tracking\\nrobotic manipulation system\\nunseen objects\\nobject perturbations\",\"1033\":\"surface reconstruction\\nsolid modeling\\nthree-dimensional displays\\nshape\\npipelines\\ncameras\\nrobustness\\nimage reconstruction\\nimage representation\\nmobile robots\\nnavigation\\nobject tracking\\npath planning\\nrobot vision\\nautonomous navigation\\nreconstruction volumes\\nexplicit occlusion handling\\nobject surfaces\\ndynamic object tracking\\nrobotic tasks\\nmultiobject tsdf formulation\\ntsdf++\\nmap representation\\nobject reconstruction\",\"1034\":\"visualization\\nautomation\\nservice robots\\nconferences\\nlighting\\ngenerative adversarial networks\\ncameras\\nimage motion analysis\\nmobile robots\\nobject tracking\\nposition control\\nrobot vision\\nvisual servoing\\nautonomous positioning\\nreliable positioning\\nvisual servoing scheme\\ngeneralization ability\\ngenerative adversarial network\\nsynthetic dataset\\nintermediate visual keyframe\\nsubsequent predictions\\nvisual features\\nscene appearance\\nsubsequent keyframe generation\",\"1035\":\"training\\nprotocols\\ndeep architecture\\ncameras\\nvisual servoing\\nreal-time systems\\nend effectors\\nlearning (artificial intelligence)\\nmobile robots\\nposition control\\nrobot vision\\nconsidered approach\\nrobot\\npositioning tasks\\nsiame-se\\nend-to-end visual servoing\\nassociated learning strategy\\nend-to-end direct visual servoing\",\"1036\":\"legged locomotion\\nscalability\\noperating systems\\nstatic analysis\\nrobot sensing systems\\nmathematical models\\nsafety\\nformal logic\\nformal specification\\nformal verification\\nmiddleware\\nmobile robots\\nprogram diagnostics\\nrobot programming\\ntheorem proving\\nrobotic domains\\nmodel representation\\nrobot operating system components\\nsystem model\\nstatic analysis framework\\nrobotic programming frameworks\\nrobotic developers\\nrobotic system\\nlinear logic theorem prover\\nsafety property\\nrobot kobuki\\nros based systems\\nnovel representation\\nverification technique\\nsoftware components\",\"1037\":\"fuzzing\\nprogramming\\nlicenses\\nsoftware\\nsoftware reliability\\ntask analysis\\nrobots\\ncontrol engineering computing\\nprogram testing\\nrobot programming\\nsoftware architecture\\nrandomly generated data\\ncomplex robotic-software\\nbehavior-based robotics\\nsource code files\\nprogramming language\\nsoftware engineering\\nrobotic behaviors\\nerror-prone task\\nreliable software\\nfuzz testing\\nsoftware components\",\"1038\":\"automation\\nscalability\\nconferences\\ncontainers\\nrobustness\\ntask analysis\\nrobots\\ncollision avoidance\\nhumanoid robots\\nintelligent robots\\nmanipulators\\nman-machine systems\\nmobile robots\\nservice robots\\nrobot household marathon experiment\\nrobustness aspects\\nmobile manipulation\\nmobile pick\\nplace actions\\nhuman household\\nscientific challenges\\nrobotic system\",\"1039\":\"geometry\\nthree-dimensional displays\\nautomation\\nconferences\\ndebugging\\nprobabilistic logic\\ndata models\\nmanipulators\\nprogramming languages\\nspecification languages\\nprobrobscene\\nprobabilistic specification language\\nrobotic manipulation environments\\nrobotic control tasks\\ndata augmentation\\nprobabilistic programming languages\\nrobotic controller\\ntabletop robot manipulation environment\\n3d robotic manipulation environments\",\"1040\":\"industries\\nfabrication\\nvisualization\\nservice robots\\noperating systems\\nconferences\\nprogramming\\ncomputational geometry\\ncontrol engineering computing\\nindustrial robots\\nmiddleware\\nproduction engineering computing\\nprogramming environments\\nsoftware tools\\nvisual programming\\nreal-time interaction\\ncreative industries\\nreal-time control\\nnonstandard fabrication\\naccessible flow-based visual programming environments\\nrobotics middleware\\ndesign software\\ntoolpath generation\\nrobot simulation\\ninteractive processes\\nnonreal-time capable operating systems\\nrealized projects\\ncyclical dataflows\\nfluent interaction\\nrobot technologies\\nnonreal-time capable environments\",\"1041\":\"performance evaluation\\nsolid modeling\\nsemantics\\nvirtual environments\\ngaze tracking\\ngrammar\\nrobots\\ncomputer games\\ndata loggers\\nvirtual reality\\nmanipulation activities\\nhuman vr demonstration\\nhuman movements\\nsimulated world\\noff-the-shelf virtual reality devices\\neye tracking capabilities\\nvirtual world\\nactivity execution\\nsubsymbolic data logger\\nhuman gaze\\noffline scene reproduction\\nreplays\\nphysics engine\\nsymbolic data loggers\\nautomated acquisition\\nstructured models\\nsemantic models\",\"1042\":\"wrist\\nmotion estimation\\nrobot sensing systems\\nelectromyography\\ncleaning\\ndecoding\\ntrajectory\\nbiomechanics\\nend effectors\\nforce measurement\\nmedical signal processing\\nmobile robots\\nmuscle\\nposition control\\ntelerobotics\\nforce measurements\\nhuman muscles\\ncorresponding human wrist positions\\nrobot arm end-effector\\nhuman wrist position estimations\\nwhiteboard surface\\ncompliance controller\\nemg\\nshared control framework\\nrobotic telemanipulation\\nelectromyography based motion estimation\\ncompliance control\\nwearable used method\\nnoninvasive used method\\nhuman muscular activations\\nelectromyography based telemanipulation tasks\\nrobotic platform\\nrandom forests regression method\",\"1043\":\"service robots\\nproduction\\nkinematics\\nfootwear\\ntools\\nturning\\nmanufacturing\\nindustrial robots\\nmobile robots\\npath planning\\nposition control\\nrobot kinematics\\nindustrial robot\\nflexible production lines\\ntool center point\\noptimal tcp placement\\nfast joint-space path planner\\nindividual robot axes\\noptimal robot base placement\\nkinematic singularities\\ntrim application\\nshoe production\\ncomplex continuous tool paths\",\"1044\":\"automation\\nscalability\\nconferences\\nprobabilistic logic\\nreal-time systems\\nprobability distribution\\nobservability\\npredictive control\\nprobability\\nrobust control\\noptimization subproblems\\nstate-hypotheses\\nclassical mpc scheme optimizing w.r.t\\ncontrol-tree optimization\\ndiscrete partial observability\\nmodel predictive control\\nessential variables\\ndiscrete variables\\ngiven state-hypothesis\\nprobabilistic belief state information\\nnonlinear mpc\",\"1045\":\"robot motion\\nnavigation\\nfuses\\nneural networks\\ndynamics\\nhuman-robot interaction\\npredictive models\\ncollision avoidance\\ngradient methods\\nman-machine systems\\nmobile robots\\nmulti-agent systems\\nneural nets\\npath planning\\ntrajectory optimization\\nproactive human-robot interactions\\ncomplex interaction dynamics\\nfuture human behaviors\\nmotion planning process\\nstate-of-the-art neural network-based human behavior models\\nrobot motion planners\\ndownstream planning\\nsimplified behavior model\\nplanning problem\\npredictive power\\nstate-of-the-art human trajectory prediction models\\nleverage gradient information\\ndata-driven prediction models\\nhuman-robot interaction dynamics\\nefficient behaviors\\nproactive behaviors\\nnuanced behaviors\\nneural network gradients\",\"1046\":\"costs\\nmachine vision\\ncurrent measurement\\nrobot sensing systems\\nreal-time systems\\nsensors\\ntrajectory\\nactuators\\nmobile robots\\noptimal control\\noptimisation\\nreachability analysis\\ntrajectory control\\nconstructability gramian\\nreachability gramian\\ncost functions optimization\\ngaussian measurement noise\\ngramian-based optimal active sensing control\\nonline optimal perception-aware strategy\\nunicycle-like vehicle\",\"1047\":\"legged locomotion\\npipelines\\ndynamics\\nrobot sensing systems\\nplanning\\nsensors\\ntiming\\noptimisation\\npath planning\\npredictive control\\nregression analysis\\nrobot dynamics\\nreceding-horizon perceptive trajectory optimization\\ndynamic legged locomotion\\nlegged robots\\nfootfalls\\ndynamic quadruped locomotion\\nheuristic initializer\\nsimple guess\\noptimization\\nfully onboard sensing\\nlatent-mode trajectory regression\\nhigh-speed motions\\nshort-horizon dynamic reactions\\nintuitive locomotion planning\\nadaptive locomotion planning\\nflexibly-parametrized trajectories\\nanymal c quadruped\\nlmtr\\nrobot capabilities\",\"1048\":\"analytical models\\nfriction\\nmodulation\\njitter\\nharmonic analysis\\nmathematical models\\nkinetic theory\\nbiomechanics\\nbiomimetics\\nlegged locomotion\\nmobile robots\\nmotion control\\nrobot kinematics\\nservomotors\\nfriction-driven three-foot robot inspired\\nsnails\\nstable movement\\nmuscular exploiting travelling waves\\nfriction modulation\\nsnaillike robots\\ngrowing research\\nnovel friction-driven three-foot snaillike robot\\nsnail-like motion\\nfrictional symmetry\\ncyclic motion\\nrobot distinctive movements\\nharmonic-peristaltic movement\\nlower cost biomimetic mobile robots\",\"1049\":\"tracking\\ndynamics\\nhumanoid robots\\noptimal control\\ntransforms\\nrobot sensing systems\\nmathematical models\\nlegged locomotion\\nmobile robots\\npath planning\\nrobot dynamics\\nrope-assisted rappelling maneuvers\\nrope-assisted humanoid robots\\nmotion planning problems\\nrope-assisted bipedal robot\\ntypical humanoid robots\\nrope pulling effect\\nrobot body\\nrope master-point\\nmotion planning challenges\\nthree-mass model\\nextensible rope\\noptimal control problems\",\"1050\":\"uncertainty\\nnavigation\\nrobot sensing systems\\npath planning\\ntopology\\nobject recognition\\nmobile robots\\ncollision avoidance\\ncomputational geometry\\ngraph theory\\nreconfigurable robot\\ncluttered environments\\nports\\nmirrax robot\\naccess environments\\ntightly spaced obstacles\\nglobal path planner\\ncoarse path planning\\njoint angle changes\\ncollision-free paths\\nrobot reconfiguration\\nadjacent poses\\nvoronoi diagram\\nsparse graph\\nheuristic pose fitting routine\\nsize 150.0 mm\",\"1051\":\"measurement\\ntraining\\nheuristic algorithms\\ndata preprocessing\\neuclidean distance\\ngames\\nprobabilistic logic\\nmobile robots\\npath planning\\nsampling methods\\ntrees (mathematics)\\nassisting metric\\npath length\\ninformed sampling-based planning\\nrt-rrt\\ncomplex environments\\ndynamic environments\\nsampling-based approaches\\nrrt-based\\nassisting distance\\nstandard euclidean metric\\nprevious rrt variants\",\"1052\":\"automation\\nconferences\\npareto optimization\\ndata structures\\npath planning\\nreal-time systems\\nstandards\\ncollision avoidance\\nmobile robots\\noptimisation\\npareto optimisation\\nset theory\\ncomplete path planning\\nsimultaneously optimizes length\\nfundamental path planning problem\\noptimal path planning problem\\nsimultaneously minimizing path length\\nmaximizing obstacle clearance\\nsimple planar settings\\ndisc obstacles\\npareto-optimal solutions\\ncomplete algorithm\\npareto front\\npareto- optimal paths\\nparticular optimal paths\\ncomputed data structure\",\"1053\":\"visualization\\nconferences\\nsemantics\\nbuildings\\ncomputer architecture\\nreal-time systems\\ncomputational efficiency\\ncomputer vision\\nconvolutional neural nets\\nimage resolution\\nimage segmentation\\njetson xavier nx\\nnvidia rtx 2080ti\\ncityscape validation\\nefficient context aggregation network\\ncontext aggregated bilateral network\\nhigh resolution branch\\nlow computational overheads\\nlocal contextual dependencies\\nlocal distribution blocks\\nglobal aggregation\\ncontext branch\\neffective spatial detailing\\nhigh-speed semantic segmentation\\nmultibranch architectures\\ncompetitive prediction accuracy\\ncomputational costs\\ndual branch convolutional neural network\\nreal-time applications\\nvisual scene understanding\\npixel-wise semantic segmentation\\nautonomous machines\\nlow-latency semantic segmentation\\ncabinet\",\"1054\":\"image segmentation\\nimage analysis\\nnavigation\\nsemantics\\nnetwork architecture\\nreal-time systems\\ndecoding\\nimage colour analysis\\nmobile robots\\nrobot vision\\nindoor scene analysis\\nfree space detection\\nrgb-d semantic segmentation approach\\nnvidia tensorrt\\nefficient scene analysis network\\nesanet\\nsunrgb-d\\noutdoor dataset cityscapes\\ncommon indoor datasets\",\"1055\":\"legged locomotion\\nimage segmentation\\nmotion segmentation\\nmerging\\nrobot sensing systems\\nreal-time systems\\nlibraries\\nc++ language\\ncontrol engineering computing\\nimage colour analysis\\nmobile robots\\nrobot vision\\ntabletop segmentation\\nplane segmentation\\nregion growing algorithm\\nrgb-d sensors\\npoint clouds\\nflood fill\\nplanar primitives\\nfirst-line scene interpretation\\nmobile robotics\\nc++\",\"1056\":\"deep learning\\nthree-dimensional displays\\nconvolution\\nsemantics\\nmemory management\\nobject detection\\nfeature extraction\\ndata mining\\nimage classification\\nimage segmentation\\nlearning (artificial intelligence)\\nneural nets\\npattern classification\\nsemantic feature mining\\n3d object classification\\n3d point cloud\\nintelligent perception\\nautomated systems\\nrobotic systems\\nstructured 2d images\\nconvolutional networks\\nunordered points\\npoint cloud recognition\\nlocal point information\\nsemantic information\\ndeep neural network\\nspherical kernel convolution\\nsemantics mining\\ndeep network\\nparallel feature feed-forward mechanism\\nbottleneck layers\\npart segmentation\",\"1057\":\"training\\nthree-dimensional displays\\ndynamics\\nobject detection\\nperformance gain\\nfeature extraction\\ntime measurement\\nimage motion analysis\\noptical radar\\nvelocitynet\\nmotion-driven feature aggregation\\npoint cloud sequences\\nlidar-based 3d object detection\\nincreased data density\\ntemporal aggregation\\ncommon aggregation methods\\nautonomous driving scenarios\\nhighly dynamic traffic situations\\nnovel network architecture\\ntraditional 3d convolutions\\nmotion-driven deformation\\nconvolution kernels\\ntemporal dimension\\nrequired motion information\\nintegrated network branch\\nobject detection task\\nexplicit feature alignment\\nobject detection problem\\ndetection performance\\nstatic objects\",\"1058\":\"training\\ndeep learning\\ncorrelation\\ndensity measurement\\nconferences\\nobject detection\\nfeature extraction\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\nimage classification\\nneural net architecture\\nrobot vision\\ncnn\\nopen-ended environments\\nconvolutional neural network training\\nintrospection methods\\nconvolutional variational auto-encoder\\nspearman's rank correlation\\ncompetency-aware object detection\\ndensity-adjusted distance measure\\nmisclassification detection\",\"1059\":\"training\\nthree-dimensional displays\\ntracking\\nvehicle detection\\nrobot vision systems\\nneural networks\\nestimation\\ncameras\\ndistance measurement\\nimage motion analysis\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\npose estimation\\nroad vehicles\\nrobot vision\\ntraffic engineering computing\\nself-supervised monocular 3d vehicle detector\\nobserved vehicle\\nautonomous vehicle\\nmonocular camera data\\n6 dof pose\\ncurrent deep\\nbased solutions\\nmonocular visual odometry\\nself-supervised fine-tuning\\norientation estimation pre-trained\\nvirtual dataset\\nfully supervised method\\noptimization-based monocular 3d bounding box detector\\nself-supervised vehicle orientation estimator\\nexpensive labeled data\\n3d vehicle detection algorithms\\ncommercial vehicle fleets\",\"1060\":\"training\\ncodes\\nannotations\\nrobot vision systems\\ndetectors\\nmanuals\\ncameras\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\nobject detection\\noptical radar\\nrecurrent neural nets\\nsupervised learning\\ncalibrated camera\\ndeep learning\\ndifferent lidar models\\nimage-based detector\\nfaster r-cnn\\njackrabbot dataset\\nself-supervised detectors\\nrobust training techniques\\ntarget dataset\\n2d lidar-based person detectors\\ndrow3 detector models\\ndr-spaam detector models\\nself-supervised person detection\",\"1061\":\"manifolds\\ngeometry\\nuncertainty\\ngaussian processes\\npredictive models\\nprobabilistic logic\\ndata models\\ncomputational geometry\\ndifferential geometry\\nlearning (artificial intelligence)\\nnonlinear control systems\\noptimal control\\noptimisation\\npath planning\\nprobability\\nstochastic processes\\nstochastic systems\\nlearned multimodal dynamical systems\\nlatent-ode collocation\\ntwo-stage method\\nunknown nonlinear stochastic transition dynamics\\npreferred dynamics mode\\ntransition dynamics model\\nstage leverages\\ngaussian process experts method\\npredictive dynamics model\\ngating function\\nparticular dynamics mode\\nlatent riemannian manifold\\nshortest trajectories\\ntrajectory optimisation problem\\nstage formulates\\nensure trajectories\",\"1062\":\"automation\\nsystem dynamics\\nconferences\\nbenchmark testing\\nlibraries\\nnonlinear dynamical systems\\ncomplexity theory\\nhumanoid robots\\noptimisation\\nrigidity\\nrobot dynamics\\ntrajectory control\\nforward dynamics\\ndirect transcription formulations\\ntrajectory optimization\\ninverse dynamics\\nrigid body dynamics\\nnonlinear system dynamics\\nopen-chain robot dynamics\\nhumanoid robot dynamics\",\"1063\":\"limiting\\ntrajectory planning\\nnavigation\\nconferences\\ndynamics\\nreal-time systems\\ncomputational efficiency\\ncollision avoidance\\nmanipulator dynamics\\nmobile robots\\noptimisation\\ntrajectory control\\ndynamic obstacle avoidance\\nglobal path planner\\nreceding horizon trajectory optimization\\nrobotic arm\\nnonholonomic base\\ncoupled trajectory planning\\nconvex region generation\\nfree space decomposition\\nwhole-body trajectory optimization\\nmobile manipulators\\nmobile base\\ncoupled mobile manipulation\",\"1064\":\"automation\\nconferences\\nrobot vision systems\\ncollaboration\\nhandover\\nrobot sensing systems\\ncameras\\ncapacitive sensors\\ngrippers\\nhuman-robot interaction\\nrobot vision\\nrobust control\\nfluent robot-human handover\\nnatural robot\\ncapacitive proximity sensor\\ncps\\nrobust grasp detection\\ncollaborative robot\\neye-in-hand depth camera\\nobject release\\ncapacitive sensing\\nrobots perception\\nmotor skills humans drop objects\",\"1065\":\"force measurement\\nconferences\\nbiological system modeling\\ntissue engineering\\nforce\\ncontrol systems\\nforce control\\nbiological tissues\\nbiomechanics\\nbiomedical materials\\nbioreactors\\ncellular biophysics\\nclosed loop systems\\nmedical robotics\\nrigidity\\nin vitro tissue engineering\\nnative tissues\\ncell-seeded scaffold\\nstiffness measurements\\nclosed-loop control system\\nstiffness based force control\\ntissue-scaffold responsive technology\\nrobotic bioreactor\\ntissue scaffold stiffness calculation\",\"1066\":\"minimally invasive surgery\\nautomation\\ncomputational modeling\\nconferences\\nkinematics\\nsoft robotics\\nfasteners\\nmanipulator kinematics\\npiecewise constant techniques\\nrigidity\\ninherent passive stiffness properties\\nscrew theory-based stiffness analysis\\nfluidic-driven continuum soft robotic manipulators\\nparameter-based piece-wise constant curvature model\\nrigid robots\\nfreespace forward kinematic model\\nmapping methodology\",\"1067\":\"sensitivity\\nscalability\\nheuristic algorithms\\nlimit-cycles\\nstability analysis\\nnonlinear dynamical systems\\nnoise robustness\\ncontrol system synthesis\\ndiscrete time systems\\nend effectors\\niterative methods\\nmotion control\\nnonlinear control systems\\nposition control\\nrobot dynamics\\ndesign principle\\ncompliant robots\\ncontrol complexity\\npassive dynamics\\ngood regions\\nachieving flexibility\\ninformation perspective\\ndirected information\\ndiscrete-time nonlinear systems\\ndesign parameters\\ndoor opening task\\ncontrol gains\",\"1068\":\"actuators\\nautomation\\nconferences\\nsoft robotics\\nrobustness\\nload modeling\\nbuckling\\ncontinuum mechanics\\nelectric actuators\\nmembranes\\nrigidity\\nrobots\\nsoft-rigid actuators\\nsoft-rigid continuum ballooning robot\\nstackable hyperelastic ballooning membranes\\nneo-hookean model\\nmooney-rivlin model\",\"1069\":\"simultaneous localization and mapping\\ncodes\\nautomation\\nconferences\\npose estimation\\nbenchmark testing\\nfeature extraction\\nimage reconstruction\\nslam (robots)\\nplane instances\\nrgb-d slam\\nstructural regularities\\nslam system\\nstructured environments\\nimproved tracking\\nmapping accuracy\\ngeometrical features\\nmapping components\\ntracking part\\ngeometric relationships\\nmanhattan world\\ndecoupling-refinement method\\nmanhattan relationships\\nadditional pose refinement module\\nmapping part\\nlow computational cost\\ninstance-wise meshing strategy\\ndense map\",\"1070\":\"uncertainty\\nthree-dimensional displays\\nparameter estimation\\nservice robots\\nconferences\\nrobot vision systems\\nestimation\\ncalibration\\ncameras\\nindustrial robots\\nrobot vision\\nstatistical analysis\\ngeneric hand-eye calibration\\nuncertain robots\\nvision-guided industrial robots\\nstatistically sound manner\\nrobot calibration\\nscara robots\",\"1071\":\"three-dimensional displays\\nstatistical analysis\\nrobot kinematics\\nperturbation methods\\nrobot sensing systems\\nsensor systems\\ncalibration\\nend effectors\\nindustrial robots\\nlasers\\noptical scanners\\nproduction engineering computing\\nrobot vision\\nsensors\\nstereo image processing\\nlaser profile scanners\\nrobot-sensor setup\\nsitu translational hand-eye calibration\\narbitrary objects\\nlaser profile sensor frame\\nend-effector frame\\nhomogeneous transformation\\n2d image\\n3d object\",\"1072\":\"image segmentation\\nvisualization\\nsemantics\\ncomputer architecture\\nreal-time systems\\nsafety\\nconvolutional neural networks\\nautonomous aerial vehicles\\ncameras\\nfeature extraction\\nimage colour analysis\\nimage recognition\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\npath planning\\nremotely operated vehicles\\nvideo signal processing\\nnovel neural architecture\\nsingle rgb images\\nimage-to-image translation\\nneural branch\\nnovel multitask loss function\\ncrowd detection task\\nsynthetic human crowd rgb image dataset\\nprevious aerial crowd detection methods\\nautonomous uav safety\\nvisual human crowd detection\\nmultitask deep neural networks\\ncamera-equipped uavs\\ndrone\\nsafe flight\\ndeep neural network-based method\\nuav footage\\nsemantic segmentation maps\\nhuman crowds\\nuav flight safety\",\"1073\":\"training\\nsolid modeling\\nimage segmentation\\nthree-dimensional displays\\npipelines\\npose estimation\\nmanuals\\nimage colour analysis\\nlearning (artificial intelligence)\\nneural nets\\nobject recognition\\nregression analysis\\nstereo image processing\\n6d object pose regression\\npoint clouds\\n6d pose estimation systems\\nsynthetic data\\ndomain gap\\ndepth information\\nlightweight data synthesis pipeline\\nsynthetic point cloud segments\\ntexture-less 3d object models\\ndata synthesis process\\nrgb image data\\non-line data synthesis\\ncolor image synthesis\\ncloudaae\",\"1074\":\"supervised learning\\nsociology\\nstochastic processes\\nreinforcement learning\\nsearch problems\\ntrajectory\\nstatistics\\ncollision avoidance\\nevolutionary computation\\nmobile robots\\noptimisation\\nvariational inference\\nstochastic latent space\\nreward function\\nstable learning progress\\nrobot execution\\nreward conditioned neural movement primitives\\nreward-based policy exploration problem\\nsupervised learning approach\\ncomplex movement trajectories\\nsearch spaces\\nrobotic reinforcement learning methods\\npopulation-based variational policy optimization\\nneural processes-based deep network training\\nevolutionary strategies\\ncrossover operation\\nself-organized latent space\\nrobotic reinforcement learning method\\nobstacle avoidance\",\"1075\":\"monte carlo methods\\ncodes\\nheuristic algorithms\\nconferences\\nreinforcement learning\\nprobabilistic logic\\nrobot learning\\nmanipulators\\nnonlinear control systems\\noptimisation\\nprincipal component analysis\\nrobot dynamics\\ntrajectory control\\nprobabilistic principal component analyzers\\ncontextual off-policy rl algorithm\\nlampo\\nself-normalized importance sampling\\nlearning iterations\\nsample-efficient off-policy optimization\\nhigh-dimensional manipulation skills\\nrobotic manipulation skills\\nparameterized movement primitives\\nimitation learning\\nrobotic tasks\\nphysical robots\\ndemonstrated trajectories\\nnonlinear latent dynamics\\ncontextual latent-movements off-policy optimization\\nlatent-movements policy optimization\\nmppca\\nparameter space high-dimensionality\",\"1076\":\"biomechanics\\nlegged locomotion\\nsolid modeling\\nneuroscience\\nanimals\\npose estimation\\ntools\\ncalibration\\ncameras\\ncomputer vision\\ndeep learning (artificial intelligence)\\nimage motion analysis\\nkalman filters\\nmotion control\\nstereo image processing\\ntrajectory optimisation (aerospace)\\nvideo signal processing\\n3d pose estimation dataset\\nwild\\ncomplex dynamics\\necological implications\\nbiomechanical implications\\nevolutionary implications\\nnext-generation autonomous legged robots\\nacinonyx jubatus\\nwholebody 3d kinematic data\\ndeep learning-based methods\\nextensive dataset\\nhuman-annotated frames\\nmarkerless animal\\nsparse bundle adjustment\\nextended kalman filter\\ntrajectory optimization-based method\\n3d trajectories\\nhuman-checked 3d ground truth\\nacinoset\\nbaseline models\\nfull trajectory estimation\\nfree-running cheetahs\\ncamera calibration files\\nmarkerless animal pose estimation\\n3d pose estimation tool development\\nmultiview synchronized high-speed video footage\",\"1077\":\"training\\nautomation\\nconferences\\npose estimation\\ndeep architecture\\ngrasping\\nfeature extraction\\nlearning (artificial intelligence)\\nobject tracking\\nrobot vision\\nmultiscale features\\ndifferent feature map resolutions\\nsynthetic data\\nfeature pyramids\\naccurate object pose estimation\\ndomain shift\\ndeep learning-based approaches\\nencoder-decoder networks\\ndifferent scene characteristics\\npatch-based approaches\\nsynthetic-to-real transfer\\nlocal to global object information\\nspecialized feature pyramid network\",\"1078\":\"computer vision\\nautomation\\nconferences\\npose estimation\\nneural networks\\ngrasping\\nbenchmark testing\\nlearning (artificial intelligence)\\nrobot vision\\noutput parameterizations\\nneural network\\nsingle shot 6d object\\nsingle shot approaches\\ncomputer vision tasks\\ngood parameterizations\\ndifferent novel parameterizations\\nlearning-based approach\\npose estimates\",\"1079\":\"legged locomotion\\ntraining\\nlocation awareness\\nlaser radar\\npose estimation\\npipelines\\nnetwork architecture\\ndistance measurement\\nlearning (artificial intelligence)\\nmobile robots\\nmotion estimation\\noptical radar\\npath planning\\nrobot vision\\ngeometric losses\\nscan points\\nlabeled ground-truth data\\npresented approach suitable\\naccurate ground-truth\\npresented network architecture\\nloss function adjustments\\nreal-world applications\\nlegged robots\\ntracked robots\\nwheeled robots\\nlearning-based lidar odometry\\ncomplex robotic applications\\nself-supervised learning\\nreliable robot\\nrobot autonomy pipelines\\nlidar localization\\nactive research domain\\nself-supervised lidar odometry estimation method\\navailable lidar data\\nreal-time performance\",\"1080\":\"location awareness\\nbundle adjustment\\nsimultaneous localization and mapping\\nautomation\\nconferences\\npipelines\\ncameras\\ndistance measurement\\ngraph theory\\nimage matching\\nmobile robots\\npose estimation\\nslam (robots)\\nfeature-based relocalization\\nmonocular direct visual odometry\\nmap-based relocalization\\nonline direct visual odometry\\nimage features\\ndirect sparse odometry\\ndirect image alignment\\nfront-end tracking\\nback-end bundle adjustment\\nonline fusion module\\nrelative vo poses\\nglobal relocalization\\nglobally accurate poses\\ndso\\npose graph\\ncamera tracking accuracy\\nfeature matching\\nslam\\nrelocalization\\nmap-based localization\",\"1081\":\"codes\\nconferences\\nprediction algorithms\\ninference algorithms\\nentropy\\nreliability\\niterative methods\\nlearning (artificial intelligence)\\nmulti-agent systems\\noptimisation\\npattern classification\\nentropy loss\\ntransductive learning\\nefficient initialization module\\nsteepest descent based optimization algorithm\\nbase learner objective\\nmeta-training\\noptimization-based meta-learning framework\\nfew-shot classification datasets\\nfast few-shot classification\\nautonomous agents\\nlow-data regime\\nhighly challenging problem\\nfast optimization-based meta-learning method\\nbase learner module\\nlinear classifier\\nunrolled optimization procedure\\ninner learning objective\\nrobust classification loss\",\"1082\":\"geometry\\nthree-dimensional displays\\nshape\\nperturbation methods\\nneural networks\\nharmonic analysis\\nrobustness\\ndeep learning (artificial intelligence)\\nfeature extraction\\nimage classification\\nimage representation\\nobject detection\\ndeep hierarchical rotation invariance learning\\npoint cloud classification\\npoint feature representation\\nrotation-invariant features\\n3d object classification\\ngeometry feature representation\\ndata augmentation\\nwigner d-matrix\\nknn\\nspherical harmonics energy descriptor\",\"1083\":\"training\\nautomation\\nconferences\\nneural networks\\ntraining data\\ntask analysis\\nrobots\\nimage representation\\nimage segmentation\\nlearning (artificial intelligence)\\nneural nets\\nrobust selective segmentation\\ncontrastive learning\\nneural network\\nood dataset\\ndata augmentation scheme\\nrobust feature representation\\nreal-world dataset\\ndriving scenes\\nsegmentation accuracy\\npixel-wise out-of-distribution detection\\ncontrastive objective scheme\\nsegmentation\\nscene understanding\\nintrospection\\nperformance assessment\\ndeep learning\\nautonomous vehicles\\nnovelty detection\",\"1084\":\"automation\\ncomputational modeling\\nconferences\\nkinematics\\nobservers\\nsoft robotics\\nmathematical models\\nboundary-value problems\\ncontrol system synthesis\\nnonlinear control systems\\nrobots\\nstability\\ncontinuum robots\\ncosserat rod equations\\nnonlinear partial observer\\nsoft robots\\nnonlinear observer\\nobserver design\",\"1085\":\"automation\\nautonomous systems\\nconferences\\ncollaboration\\nreliability\\nproblem-solving\\ntask analysis\\ncognition\\ncontrol engineering computing\\ngroupware\\nhuman-robot interaction\\nsocial robots\\nvoluntary interaction setting\\ntask performance\\ntask-related social interactions\\ninteraction design concept\\nrobot cognitive reliability\\nsocial positioning\\nchild-robot team dynamics\\nhuman collaboration\\ncognitive growth\\nautonomous robotic system\\nproblem-solving setting\\nrobot behaviour\\nautonomous system\\nharu robot\\nhuman-to-human social dynamics\",\"1086\":\"regulators\\noptimal control\\nkinematics\\ncost function\\nrobustness\\nnumerical models\\ncontrol theory\\nlinear quadratic control\\nlinear systems\\nnonlinear control systems\\npendulums\\nlqr controllers\\nlqr control laws\\nclosed kinematic loops\\nmultibody system\\nparameterization\\ndynamical systems\\nnonminimal state parameterizations\\ncontrol-theoretic advantages\\njoint coordinates\\ngeneralized coordinates\\nminimal coordinates\\nlinearized systems\\nefficient control method\\nlinear-quadratic regulator\\nmaximal coordinates\\nlinear-quadratic optimal control\\nnonlinear systems\",\"1087\":\"adaptation models\\nuncertainty\\ncomputational modeling\\nheuristic algorithms\\nbayes methods\\nsafety\\nadaptive control\\ncontrol system synthesis\\nlevel control\\noptimisation\\nadaptive control approach\\nhigh-performance controllers\\nprecise system model\\nexisting data-driven approaches\\nstandard model-based methods\\npurely data-driven\\nmodel-free approach\\nlow-level controllers\\nsystem data\\nunderlying algorithm safety\\ncomputational performance\\nsample-efficient bayesian optimization\\ncomputational modifications\\nalgorithmic modifications\\nrotational motion system\",\"1088\":\"bridges\\nconferences\\nlayout\\ncomputer architecture\\nsystem recovery\\nsoftware\\nproduction facilities\\nautomatic guided vehicles\\nindustrial robots\\nmobile robots\\nmulti-robot systems\\npath planning\\nroad traffic control\\nindustrial environment\\ntraffic management\\nmultiple automated guided vehicles\\nautomatic factory\\ninnovative methods\\nmultilayer control architecture\\ntraffic model\\npath planner\\noptimal path\\nagvs movement coordination\\ncentralized control\\nindustrial applications\\nmedium size factories\\nreliable software\\nrobust software\\nmultiple agvs\\ntraffic manager software\\nmultiagv systems\",\"1089\":\"shape\\nrobot kinematics\\nsearch methods\\nscalability\\ndynamics\\ninterference\\nplanning\\ncollision avoidance\\nmobile robots\\npath planning\\nrobot dynamics\\nmultirobot motion planning\\nroadmaps\\nautomating fleets\\nnonholonomic dynamics\\nmultirobot goal allocation\\nmultirobot roadmap\\nreduced configuration space\\nenvironment connectivity\\ninterference cost\\nroad-map results\\nrobot-goal assignment\\nabstract multirobot trajectory\\nmultirobot motion planner\",\"1090\":\"measurement\\noptimization methods\\nlattices\\ninspection\\nreliability engineering\\npath planning\\nplanning\\nautonomous aerial vehicles\\ndistributed control\\ngenetic algorithms\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\noptimisation\\nasynchronous reliability-aware multiuav coverage path planning\\ngraceful degradation\\npotential advantage\\nmultirobot systems\\nsingle-robot systems\\naerial robotics applications\\nmission reliability\\nfailure-prone low-cost drones\\nreliability-aware multiagent coverage path planning problem\\npath plans\\nmission completion\\npath planner\\nreliability evaluation framework\\nscalable ra-mcpp solver\\naccurate ra-mcpp solver\\ncoverage path planning\\nreliability analysis\",\"1091\":\"automation\\nrobot kinematics\\natmospheric modeling\\nconferences\\nmobile robots\\nenvironmental monitoring\\ntask analysis\\nmotion control\\nmulti-robot systems\\npath planning\\nsmooth paths\\nguiding vector field\\nscalable coordinated motion control\\nrepetitive execution\\nmultiple robots\\npossibly different desired paths\\npath parameter\\nneighboring robots\\nsaturated control algorithm\",\"1092\":\"actuators\\npower system measurements\\ntorque\\ntrajectory planning\\ndensity measurement\\nconferences\\noptimal control\\ncontrol system synthesis\\nmanipulators\\nmobile robots\\npath planning\\ntorque control\\ntrajectory control\\nvelocity control\\nconstrained bangbang transitions\\nmotor power\\ntsa\\nhighly-dynamical applications\\nsmooth time-optimal trajectory planning\\ntwisted string actuators\\ncompliant cable actuators\\nhigh power density\\nrobotics applications\\ncomparatively slow positioning\\nundesired oscillations\\ncable tension\\ntime optimal trajectory generation\\npoint-to-point transitions\\nsmooth trajectories\\noptimal control problem\\nmotor torque speed\\nparallel manipulators\\nantagonistically-controlled systems\\nbangbang transitions\",\"1093\":\"three-dimensional displays\\ndatabases\\ntrajectory planning\\nsystem dynamics\\nheuristic algorithms\\ncollaboration\\nkinematics\\ncontrollability\\nfeedback\\njacobian matrices\\nlinear quadratic control\\nmanipulator kinematics\\nmobile robots\\nmotion control\\nnonlinear control systems\\noptimal control\\npath planning\\npendulums\\nposition control\\nquadratic programming\\nspherical pendulum\\n7-dof collaborative robot\\nexperimental swing-up\\ncomplete mechanical system\\nfast trajectory planning\\noffline trajectory optimization\\naverage computing time\\nfast trajectory replanner\\nfast swing-up trajectory optimization\\ndynamics constraint\\nkinematic constraint\\nconstrained quadratic program\\ndiscrete time-variant linear quadratic regulator\\nfeedback controller\",\"1094\":\"monte carlo methods\\ncollaboration\\nhuman-robot interaction\\nsearch problems\\nlinear programming\\nmobile handsets\\nprobability distribution\\ncomputer games\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\nplanning (artificial intelligence)\\nprobability\\ntree searching\\nhuman-robot collaborative multiagent path planning\\nmonte carlo tree search\\nsocial reward sources\\nobject search task\\nshared plans\\ncommunicating negotiating\\nrobot computes\\nmultiagent plan\\nhuman scrutiny\\nhuman-robot communication mobile phone interface\\nreal-life search experiments\",\"1095\":\"location awareness\\nimage segmentation\\nvisualization\\nsimultaneous localization and mapping\\nautomation\\nconferences\\nestimation\\naugmented reality\\ncomputational geometry\\nconvolutional neural nets\\nfeature extraction\\nimage colour analysis\\nneural net architecture\\nreal-time systems\\nslam (robots)\\nrobust plane estimation\\nsingle-stage instance segmentation\\nvisual slam\\npiece-wise planar regions\\nsingle rgb image\\nsingle-stage cnn architecture\\nfeature pyramid network\\nplanesegnet\\nplanar slam\\nresidual feature augmentation module\\nfast feature nonmaximum suppression\\nindoor scenes\\nreal time deep neural architecture\",\"1096\":\"training\\ncomputer vision\\npipelines\\npose estimation\\nobject segmentation\\nobject detection\\ngrasping\\nfeature extraction\\nimage colour analysis\\nimage motion analysis\\nimage segmentation\\nlearning (artificial intelligence)\\nmanipulators\\nrobot programming\\nrobot vision\\nvideo signal processing\\nregressors\\nclassifiers\\nycb-video dataset\\npretrained mask r-cnn\\ndeep architectures\\ncomputer vision tasks\\nobject manipulation\\nvisual system\\nrobotics\\nkernel-based methods\\nfast object segmentation learning\",\"1097\":\"deep learning\\nimage segmentation\\nthree-dimensional displays\\nsemantics\\nneural networks\\ntraining data\\nprobabilistic logic\\ndeep learning (artificial intelligence)\\nimage colour analysis\\nimage fusion\\nprobability\\nstereo image processing\\ndiffuser\\nmultiview 2d-to-3d label diffusion\\nsemantic scene segmentation\\ncomputer vision\\nrobotics\\nmultidomain 3d semantic segmentation\\n2d neural networks\\n2d semantic segmentation\\nmultiple image views\\nrefined 3d segmentation\\ntransductive label diffusion problem\\n3d geometric properties\\nsemantic labels\\n2d image space\\nindoor datasets\\nsingle rgb-d frame segmentation\\n3d segmentation accuracy\\nstate-of-the-art multiview approaches\\nannotated 3d datasets\",\"1098\":\"training\\nlaser radar\\ncodes\\nthree-dimensional displays\\nconferences\\nsemantics\\ndetectors\\ndistance measurement\\nimage segmentation\\nobject detection\\noptical radar\\nlidar-based panoptic segmentation\\ntackles semantic segmentation\\ninstance segmentation\\nkitti odometry benchmark\\ntemporally consistent instance information\\nsemantic labels\\nlidar point clouds\\noriginal semantic segmentation\\nsemantic scene completion tasks\\nsemantic annotation\\nlidar-based semantic segmentation approaches\\nsemantic kitti\\npanoptic segmentation\",\"1099\":\"three-dimensional displays\\nimage recognition\\nactivity recognition\\nfeature extraction\\nskeleton\\nconvolutional neural networks\\nrobots\\nhuman-robot interaction\\nimage classification\\nimage colour analysis\\nimage motion analysis\\nimage representation\\nimage sequences\\nlearning (artificial intelligence)\\nneural nets\\nspatiotemporal phenomena\\nvideo signal processing\\nvideo features\\nattention mechanism\\npooling mechanism\\nactivity-aware spatio-temporal cues\\nefficient activity recognition\\nlearn-able pooling\\nhumans robot interaction\\nfuture operations\\nhuman activity classification\\nfuture activity prediction\\nattention-based learn-\\nrgb videos\\nperforming human activity recognition approaches\\n3d skeleton positions\\nrobotics applications\\nrich spatio-temporal information\\nspatial information\\nlong-term temporal dependencies\\nhigh recognition accuracy\",\"1100\":\"training\\nadaptation models\\nhospitals\\nconferences\\ngraphics processing units\\ndetectors\\nreal-time systems\\nimage sampling\\nlearning (artificial intelligence)\\nobject detection\\nrobot vision\\nobject detector\\nmulticlass detectors\\nfast model updates\\nflexible model updates\\nobject sets\\ndynamic sampling-based training strategy\\ntime instance detection\\nfast incremental learning\\nobject instance detection\\nrobotic applications\\nholistic scene labeling\\ngrocery store objects\\nsingle-class instance detectors\\nmonolithic multiclass detectors\",\"1101\":\"measurement\\nlocation awareness\\nvisualization\\nlaser radar\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nimage resolution\\ndeep learning (artificial intelligence)\\nfeature extraction\\nimage registration\\noptical radar\\nslam (robots)\\nnavigation\\nautonomous systems\\npointcloud registration methods\\nfeature-based registration methods\\nvisual slam\\nlidar range images\\nillumination-independent modalities\\nlight-weight 3d lidar\\nlidar scan images\\ndeep learning\",\"1102\":\"legged locomotion\\ncomputational modeling\\nheuristic algorithms\\nconferences\\ndynamics\\nstairs\\nplanning\\nmotion control\\npath planning\\nrobot dynamics\\ntrajectory control\\nenvironmental changes\\nmomentum-based task space\\nmoving obstacles\\nplanning capability\\nonline replanning\\nbase motion\\ncontact locations\\nfootstep trajectories\\nuneven terrain\\nreference trajectories\\nlegged robot locomotion\\nquadruped robot\\nonline dynamic trajectory optimization\",\"1103\":\"tracking\\nperturbation methods\\nheuristic algorithms\\nconferences\\ncomputational modeling\\ndynamics\\nhumanoid robots\\nlegged locomotion\\nmobile robots\\nmotion control\\npath planning\\nrobust control\\nswing foot\\npush recovery solution\\nonline dcm trajectory adaptation\\nstumble recovery\\nhumanoid locomotion\\ndivergent component of motion\\ncenter of mass\\nfootstep adjustment strategy\\ndcm dynamics\\nrobot hardware\\ntoro\\nhumanoid robot\",\"1104\":\"codes\\nautomation\\nautonomous systems\\nconferences\\nlayout\\nbenchmark testing\\ntrajectory\\nlearning (artificial intelligence)\\nmobile robots\\ntrajectory control\\nmultipath trajectory prediction\\nfuture positions\\ntraffic scenarios\\nintelligent autonomous systems\\ntarget agent\\nsocially possible paths\\ndcenet\\ntwo-stream encoders\\nrespective observed trajectories\\nextracted dynamic spatial context\\nspatial-temporal context\\nlatent space\\nconditional variational auto-encoder module\\nfuture trajectories\\npopular challenging benchmarks\\ntrajectory forecasting trajnet\\ndynamic context encoder network\",\"1105\":\"laparoscopes\\ndynamics\\nsurgery\\nposition control\\ngrasping\\ntools\\ncontrol systems\\nbiomechanics\\nclosed loop systems\\ncontrol engineering computing\\nendoscopes\\nfeedback\\nforce control\\nforce feedback\\ngrippers\\nhaptic interfaces\\nmedical robotics\\npatient rehabilitation\\nsprings (mechanical)\\ntelerobotics\\nfoot control\\nsurgical laparoscopic gripper\\nfoot devices\\ncontrol surgical equipment\\nfoot switches\\nelectro-surgery\\ncontinuous control\\nhaptic foot interface\\ncontinuous assistance\\nposition mapping\\nrobotic tool\\nhaptic feedback\\ndynamic model compensation\\nhaptic fixtures\\nselective dynamic compensation\\nposition control surgical task\\ntelerobotic consoles\\n5-dof haptic robotic platform\\nclosed loop force feedback\\nfoot manipulation\\nrobotic surgery\\nfoot-robot interaction\\nsurgery training\\nlaparoscopy\\ngripper\",\"1106\":\"sensitivity\\ncorrelation\\nautomation\\ndatabases\\nconferences\\nartificial neural networks\\nrobot sensing systems\\nfeedback\\ngradient methods\\nlearning (artificial intelligence)\\nmedical robotics\\nneural nets\\npattern classification\\nregression analysis\\nsurgery\\nnetwork decision\\nregression problem\\nquality score prediction\\nregression task\\nautomatic quality assessment datasets\\nsimple temporal task\\nproviding automatic feedback\\ntrainees\\nautomatic evaluation\\nprecise gestures\\ncontrolled gestures\\nvirtual coaches\\nscore task quality\\nassigned scores\\nneural network explanation\\nneural networks explicability\\nclassification tasks\",\"1107\":\"training\\npain\\nfingers\\nmedical services\\nmuscles\\nsoft robotics\\nmice\\nbiomechanics\\nbiomedical education\\nelastic constants\\nergonomics\\nhaptic interfaces\\nmedical computing\\nmedical robotics\\nmouse controllers (computers)\\nmuscle\\npatient monitoring\\nhaptic mouse design\\nabdominal palpation training\\nsurface muscles\\nmuscle guarding\\nexperiencing discomfort\\nphysical palpation\\naffected location\\npalpation forces\\nmuscle tension\\ntunable stiffness mechanisms\\nmedical simulator designs\\neffective clinical education\\ncontrollable stiffness muscle layer\\nsoft jamming\\nrigid granular jamming\\nnonstretchable layer jamming mechanisms\\nmuscle samples\\nfine granular jamming\\nlatex layers\\nstretchable layer jamming\\nstiffness\\ntested palpation gestures\\nshort pretraining\\nfine jamming\",\"1108\":\"adaptation models\\nanalytical models\\ntrajectory tracking\\ncurrent measurement\\nposition control\\ncontrol systems\\nmathematical models\\ncontrol engineering computing\\ndigital control\\nelectric current control\\nfield programmable gate arrays\\nmedical robotics\\nsurgery\\ntelerobotics\\nachieved digital implementation\\nlow-level control\\nmodel-based design\\nda vinci research kit telerobotic surgical system\\naffordable source platform\\nopen-source platform\\nrobotic minimally-invasive surgery\\nanalog controller\\nmotor current\\ncontrol performance\\nmodel-based controller design\\ndigital current controller yields superior performance\\noriginal analog design\\nidentified system model\\nimproved position controller\\ncontrol pc\\ncurrent position control\",\"1109\":\"visualization\\ninverse problems\\nvirtual environments\\nimaging\\nmanuals\\nneedles\\nphysiology\\nbiological tissues\\nclosed loop systems\\nfinite element analysis\\nmanipulators\\nmedical image processing\\nmedical robotics\\npatient treatment\\nsteering systems\\ntelerobotics\\nvirtual reality\\nshared control strategy\\nneedle insertion\\ndeformable tissue\\ninverse finite element simulation\\ndeformable tissues subject\\nphysiological motions\\nautomatic needle steering algorithm\\ncore motivation\\npotentially dangerous decisions\\nnonintuitive manipulations\\nclosed-loop automatic needle steering control method\\nfully automatic solution\\nshared control solution\\nneedle tip placement\",\"1110\":\"automation\\nenergy exchange\\nconferences\\nforce\\nmanipulators\\nmedical robotics\\nrendering (computer graphics)\\nstability\\nsurgery\\ntelerobotics\\ntorque control\\ntwo-layer approach\\nefficient bilateral teleoperation\\nrobustly stable bilateral teleoperation\\nnovel bilateral teleoperation architecture\\nremote interaction force\\nrobustly stable behaviour\\nproper energy exchange\\nremote sides\",\"1111\":\"trajectory tracking\\ncomputational modeling\\noptimal control\\nsoft robotics\\nstability analysis\\nfinite element analysis\\ncomputational efficiency\\ncables (mechanical)\\ncontrol system synthesis\\nconvex programming\\nnonlinear control systems\\nreduced order systems\\nrobot dynamics\\nrobot kinematics\\ntrajectory control\\nsoft robot optimal control\\nnonlinear dynamic behavior\\nmodel order reduction\\nnonlinear reduced order finite element model\\ncable-driven soft robot\\ncontroller design\\nsequential convex programming\",\"1112\":\"measurement\\nlaser radar\\nsimultaneous localization and mapping\\nthree-dimensional displays\\niterative closest point algorithm\\nlinear approximation\\nfeature extraction\\ngraph theory\\nimage registration\\niterative methods\\nleast squares approximations\\nmobile robots\\nobject detection\\noptical radar\\nprincipal component analysis\\nrobot vision\\nslam (robots)\\nprincipal components analysis\\nlocal submap\\nmultimetric linear least square iterative closest point algorithm\\npoint-to-point\\npoint class\\nstatic feature points\\nregistered frame\\nlocal map\\nregularly stored history submaps\\nmulls\\nlidar-only slam systems\\nversatile lidar slam\\nautonomous driving mapping calls\\nmobile mapping calls\\noff-the-shelf lidar slam solutions\\ndifferent specifications\\nefficient drift\\nlow-drift\\ndual-threshold ground filtering\",\"1113\":\"laser radar\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nheuristic algorithms\\npipelines\\nneural networks\\nurban areas\\ndistance measurement\\nmobile robots\\nneural nets\\nobject detection\\noptical radar\\npath planning\\nrobot vision\\nslam (robots)\\nautomatic generation\\nhighly dynamic environments\\nmoving objects\\ncars\\nperformance challenge\\nlidar slam systems\\nlargely static scenes\\ndynamic object aware lidar slam algorithm\\nreal-time capable neural network\\nnecessary training data\\nend-to-end occupancy grid based pipeline\\narbitrary dynamic objects\\nexpensive manual labeling\\nknown objects\\n12000 lidar scans\\nurban environment\\nlidar slam odometry performance\\ntest data\",\"1114\":\"location awareness\\nindustries\\ngeometry\\nlaser radar\\nservice robots\\nsemantics\\nbuildings\\nbuilding information modelling\\ndistance measurement\\neducational institutions\\ngraph theory\\nindustrial robots\\nmobile robots\\noptical radar\\nquery processing\\nrobot vision\\nslam (robots)\\nvisual databases\\nbuilding information modeling\\nuniversity building\\nbim model\\nindustry foundation classes data\\nrobot-specific world model representation\\nquery semantic objects\\nexplicit data associations\\nsemantic structural objects\\ngraph-based approach\\nexplicit map-feature associations\\nexplainable model\\nodometry\\n2d lidar-based localization\\nsemantic building information models\\nsemantic dataset\\nindoor semantic localization\\nrobot feature detectors\",\"1115\":\"training\\nheuristic algorithms\\nneural networks\\nperformance gain\\napproximation algorithms\\ngenerators\\ncontrol theory\\nclosed loop systems\\ncontrol system synthesis\\nlearning (artificial intelligence)\\nmobile robots\\nnonlinear control systems\\nnonlinear dynamical systems\\nrobot dynamics\\nrobust control\\ntracking\\nneural ode\\nmultistep dynamics\\nrobust tracking controller\\nlearned ndi controller\\nexpert policies\\nimitation learning algorithms\\nil problems\\nbehavior cloning\\nbc-like methods\\nend-to-end differentiable nonlinear closed-loop tracking problem\\nrobust model-based imitation learning\\nnonlinear dynamics inversion algorithm\\nrmbil\",\"1116\":\"sensitivity\\nuncertainty\\ntrajectory planning\\nperturbation methods\\nconferences\\nrobot sensing systems\\nminimization\\nclosed loop systems\\nnonlinear control systems\\noptimal control\\noptimisation\\npath planning\\nrobust control\\ntrajectory control\\nuncertain systems\\nreference trajectories\\nbe\\u0301ziers curves\\nnonlinear constraints\\noptimization process\\nrobust trajectory planning\\nparametric uncertainties\\nclosed-loop state sensitivity\\ntrajectory optimization framework\\noptimal reference trajectory\\ninput sensitivities\\nmodel parameters\\ninherently robust motion plans\\nextensive statistical campaign\",\"1117\":\"training\\nadaptation models\\nsolid modeling\\nlaser radar\\nthree-dimensional displays\\ndetectors\\ndata models\\nneural nets\\nobject detection\\noptical radar\\nstereo image processing\\ntask-specific network\\nintegrated architecture\\nlabeled target domain frames\\nintegrated cyclegan\\n3d object detector\\njoint learning delay\\nsupervised lidar perception methods\\nlabeled point cloud data\\nlabeling process\\ncyclegan framework\\nlidar features\\nend-to-end training\\ntask-specific layers\\nunpaired lidar datasets\\nlidar few-shot domain adaptation architecture\",\"1118\":\"integrated optics\\nunderwater communication\\noptical design\\noptical feedback\\ndistance measurement\\noptical sensors\\nmirrors\\nautonomous underwater vehicles\\nbeam steering\\nfeedback\\nhall effect transducers\\nlaser beams\\nmicromirrors\\noptical communication equipment\\noptical design techniques\\nsensors\\nunderwater acoustic communication\\nranging challenges\\nunderwater environment\\ndual-modal design\\ngreen laser beams\\ndual-modal beam steering\\ntwo-axis water-immersible microscanning mirror\\nwimsm\\nhinge design\\nscanning range\\nclosed-loop scanning\\naiming control\\nultrasonic-assisted laser handshaking method\\noptical underwater communication\\ndevice design\\nsystem integration\\ndual-modal optical communication\\nacoustic communication\\ndual modal devices\\ncoaxial ultrasonics\\nsignal diverging patterns\\nhigh speed hall effect sensor-based pose feedback channel\\nwater tank\",\"1119\":\"visualization\\ncasting\\nlaser radar\\nconferences\\npose estimation\\ngreen products\\nfeature extraction\\ndistance measurement\\nimage matching\\noptical radar\\nstate-of-the-art visual-inertial odometry\\nlidar-visual estimator\\nvanishing point aided lidar-visual-inertial estimator\\nsequential modules\\nvanishing point detection module\\nvoxel-map based feature depth association module\\nfixed-lag smoother module\\nimu-aided vp detection module\\nfeature points\\nline segments\\n1-line ransac method\\nstable vp hypotheses\\nvanishing point hypothesis validation\\nnovel voxel-map based feature depth association method\\nvisual feature\",\"1120\":\"training\\ngeometry\\nroads\\nconferences\\ntraining data\\nreinforcement learning\\nrobustness\\nlearning (artificial intelligence)\\npath planning\\nrobot programming\\nrobust driving policies\\nmultitime-scale predictive representation\\nroad geometries\\ndamaged distracting lane conditions\\noffline training data\\nrepresentation learning method\\noffline reinforcement learning\\nstandard batch rl methods\\ndeep reinforcement learning\\nreal-world robot training\",\"1121\":\"visualization\\ncodes\\nautomation\\nnavigation\\nconferences\\nsemantics\\nplanning\\ndeep learning (artificial intelligence)\\nimage representation\\ninference mechanisms\\nmobile robots\\nobject detection\\npath planning\\nreinforcement learning\\nrobot vision\\nconfidence-aware semantic scene completion module\\nssc-nav\\nscene representation\\nsemantic label\\nunobserved scene\\nconfidence map\\nnavigation policies\\nsscnav\\nvisual semantic navigation\\nactive agent\\ntarget object category\\nunknown environment\\npoint goal navigation\\nindoor environment\\nscene prior modeling\\nagent navigation planning\\npolicy network\\naction inference\\ndeep reinforcement learning\",\"1122\":\"geometry\\nvisualization\\nthree-dimensional displays\\nnavigation\\nsensor fusion\\nrobot sensing systems\\npath planning\\ncollision avoidance\\nmobile robots\\nobject detection\\nrobot vision\\nstereo image processing\\nego-centric stereo navigation\\nstixel world\\npassive sensing\\nstereo sensing\\nvision-based navigation\\ndense depth algorithms\\ndrawbacks compound\\nadditional computational demands\\ncollision checking\\npath planning modules\\ndense depth measurements\\nstixel representation\\ncompact representation\\nsparse visual representation\\nlocal free-space\\nperception space\\nhierarchical navigation framework\\nscalable navigation\\ndifferent robot geometries\\nfavorable scaling properties\\ncomparable dense depth methods\\nnavigation benchmarking\\nhigh performance compute hardware\\nlow performance compute hardware\\npips-based stixel navigation\\ntraditional hierarchical navigation\",\"1123\":\"software libraries\\nsoftware architecture\\npose estimation\\ntactile sensors\\nmachine learning\\nlibraries\\nsensors\\ninteractive systems\\nlearning (artificial intelligence)\\ntouch (physiological)\\ntouch sensitive screens\\nintegrated software capable\\nprocessing raw touch measurements\\nhigh-level signals\\ndecision-making\\ntouch sensing signals\\nstate-of-the-art touch processing capabilities\\ntactile sensing community\\nperformance-validated modules\\ntouch processing tasks\\ntouch detection\\nmachine learning library\\nrich tactile sensors\\nopen-source\",\"1124\":\"automation\\nconferences\\nstochastic processes\\nbenchmark testing\\nplanning\\nautomobiles\\nphysics\\nclosed loop systems\\ncomputer simulation\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\npath planning\\nplanning (artificial intelligence)\\ntraffic engineering computing\\ntractable contingency planning\\nlearned behavior models\\nbusy intersection\\npassing lane\\ncompetent drivers\\npotential future behaviors\\ngeneral-purpose contingency planner\\nhigh-dimensional scene observations\\nlow-dimensional behavioral observations\\nconditional autoregressive flow model\\nmultiagent scenarios\\ncontingency planning\\ndriving simulator\\ncarla\\nclosed-loop control benchmark\",\"1125\":\"image segmentation\\nvisualization\\nrobot vision systems\\nestimation\\ntraining data\\npredictive models\\nfasteners\\nestimation theory\\nimage motion analysis\\nintelligent robots\\nmobile robots\\nrobot vision\\nscrewnet\\ncategory-independent articulation model estimation\\ndepth images\\nscrew theory\\narticulation model category\",\"1126\":\"training\\nvisualization\\nautomation\\nconferences\\ngames\\npredictive models\\nrobots\\ncognition\\ncognitive systems\\ngame theory\\nlearning (artificial intelligence)\\nmulti-agent systems\\ncomplex social interaction\\nyoung age\\npoint-of-view\\nvisual prediction framework\\ncritical cognitive skills\\ncognitive milestone\\nhuman development\\nvisual behavior modeling\\nphysical robots\\nopponent behavior modeling\\nvisual hide-and-seek\",\"1127\":\"force measurement\\nshape\\ntactile sensors\\npredictive models\\ndata models\\nsensors\\ntrajectory\\ngraph theory\\nimage sequences\\ninference mechanisms\\nlearning (artificial intelligence)\\nobject tracking\\npose estimation\\ntactile models\\nfactor graph-based estimation\\nvision-based tactile sensors\\nimage measurements\\nlatent object state\\ninference problem\\ntactile measurements\\nlocal observation models\\nmap highdimensional tactile images\\nlow-dimensional state space\\nlow-dimensional force measurements\\nlocal tactile observation models\\ngeometric factors\\nfactor graph optimizer\\nreliable object tracking\\ntactile feedback\\nobject shapes\",\"1128\":\"deep learning\\ntracking\\nconferences\\ndynamics\\nneural networks\\nobject detection\\npredictive models\\nedge detection\\nimage restoration\\nmobile robots\\nmotion estimation\\nneural nets\\nsensor fusion\\nautonomous vehicles\\ncommon challenges\\ndynamic objects\\nblurred predictions\\nlong prediction horizons\\ndouble-prong neural network architecture\\nspatiotemporal evolution\\nstatic environment\\nmoving ego vehicle\\nprongs\\nreducing blurriness\\ndouble-prong convlstm\\nspatiotemporal occupancy prediction\\ndynamic environments\\nfuture occupancy state\\ninformed decisions\",\"1129\":\"measurement\\nautomation\\nconvolution\\nconferences\\nbenchmark testing\\nprediction algorithms\\ntrajectory\\nimage representation\\nlearning (artificial intelligence)\\nneural nets\\nspatiotemporal phenomena\\nmultimodality\\nmultiple plausible path predictions\\nsocial-stage\\nsocial interaction-aware spatio-temporal multiattention\\nranking using interaction\\nmultimodal predictions\\nspatio-temporal multimodal future trajectory forecast\",\"1130\":\"interpolation\\nextrapolation\\nrobot sensing systems\\nreal-time systems\\nplanning\\nbayes methods\\nnoise measurement\\nmatrix algebra\\nmobile robots\\nnavigation\\npath planning\\nslam (robots)\\nlow-rank matrix completion\\nautonomous mapping tasks\\nsparse sensor measurements\\nnoisy sensor measurements\\npartial sensor measurements\\nmap interpolation\\nbayesian hilbert mapping\\nreal time map prediction\\nrepresentative coverage planning\\nautonomous robot mapping\\nmap extrapolation\\nnavigation guidance\",\"1131\":\"wrist\\nautomation\\nconferences\\ntransforms\\ncameras\\nvisual servoing\\nstability analysis\\nlyapunov methods\\nrobot vision\\nstability\\nimage-based visual servoing\\ncamera field\\nbarrier function\\nimage plane\\nibvs controller\\nstate error\\nlyapunov\",\"1132\":\"image segmentation\\ntime-frequency analysis\\nautomation\\nphantoms\\ndata visualization\\ntools\\nrobot sensing systems\\nbiological tissues\\nbiomedical optical imaging\\nbiomedical ultrasonics\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\nneedles\\npath planning\\nphotoacoustic effect\\nrobot vision\\nsurgery\\nvisual servoing\\nphantom\\nex vivo tissue\\nrobotic systems\\nraw sensor data\\nrobot controller\\ntool tip visualization\\nmultiple robotic surgical procedures\\ninterventional procedures\\nreal-time photoacoustic visual servoing system\\nraw acoustic sensor data\\nimage formation\\nrobot path planning decisions\\ndeep learning-based visual servoing system\\ninterventional tool\\nneedle tip tracking performance\\ndeep learning-based approach\\nimage-based segmentation approach\\ndeep learning-based system\\nlaser pulse repetition frequency rate\\nfrequency 10.0 hz\",\"1133\":\"jacobian matrices\\nrobot kinematics\\nneural networks\\nkinematics\\nmanipulators\\nvisual servoing\\nmathematical models\\nadaptive control\\nend effectors\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nneurocontrollers\\nrobot vision\\nkinematic control\\nkinematic differences\\nneural networks ability\\ncartesian control schemes\\nkinematic jacobian\\nkinematic equations\\nkinematic behavior\\nalternative data-driven methods\\nvisual servoing experiment\\nbetter-conditioned jacobian matrices\\nkinematics jacobian matrix\\nneural kinematics model\\nneural jacobian methods\\nrobotic manipulators\",\"1134\":\"automation\\nconferences\\noptimal control\\nbenchmark testing\\nreal-time systems\\nrobots\\ntrajectory optimization\\nconvex programming\\npredictive control\\nbenchmark control problems\\nconic model-predictive control\\ncomplex robotic systems\\nreal-time rates\\noptimization problems\\nmpc problem\\nsupport efficient warm starting\\nhigh-performance solver\\naugmented lagrangian method\\ngeneral convex conic constraints\\naltro-c\\nspeed convergence\\nquadratic cone programs\",\"1135\":\"uncertainty\\ncontrol design\\nforce\\ndynamics\\ngaussian processes\\npredictive models\\nplanning\\nmobile robots\\nnonlinear control systems\\npath planning\\npredictive control\\nstochastic processes\\nuncertain systems\\nresultant scenario-based model predictive control approach\\nnonlinear latent force model\\nscenario-based approach\\nnonlinear uncertain systems\\nlatent uncertainty\\ngaussian process\\nstochastic model predictive control problem\",\"1136\":\"runtime\\nperturbation methods\\nreinforcement learning\\ntools\\nplanning\\ntrajectory\\nfunction approximation\\napproximation theory\\ninfinite horizon\\nlearning (artificial intelligence)\\noptimal control\\nplanning (artificial intelligence)\\npredictive control\\nmpc\\napproximate value function\\ninfinite-horizon model predictive control\\nclassic tool\\nreal-world systems\\nprediction horizon\\nmyopic decisions\\nlearned value function\\nterminal cost\\nreinforcement learning solutions\\nvalue function approximation\\nrobotics tasks\\ngoal-directed problems\",\"1137\":\"software packages\\nconferences\\nreinforcement learning\\nlinear programming\\nsystem identification\\ntask analysis\\ndynamical systems\\ncontrol system synthesis\\nfeedback\\nlearning (artificial intelligence)\\nnonlinear control systems\\noptimisation\\npendulums\\npredictive control\\ndata-driven model predictive control\\npowerful feedback technique\\ndata-driven robotics\\ndata-driven mpc\\ncareful tuning\\nfeedback policy\\ndata-driven system identification\\ncontrol synthesis\\ndata-driven control policies\\nautomatic tuning\",\"1138\":\"minimally invasive surgery\\nsurgery\\nbandwidth\\ndelays\\nmirrors\\ntask analysis\\nrobots\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\nmedical robotics\\nmobile robots\\ntelerobotics\\nvirtual reality\\ndelay-tolerant semiautonomous robot teleoperation\\ntelesurgery\\nlow-bandwidth communication networks\\ntelerobotic surgeries\\ndeserts framework\\nnovel simulator interface\\nremote robotic agent\\nalpha-blended view\\natomic surgical maneuvers\\nvideo information\\nlive recognition\\nsemiautonomy\\ndeep learning based architecture\\nvirtualize reality simulation\",\"1139\":\"actuators\\ntuners\\ndynamics\\ntorque control\\nartificial neural networks\\nmathematical models\\nsoftware\\nadaptive control\\nclosed loop systems\\ncontrol system synthesis\\ndelay systems\\ndelays\\ndiscrete time systems\\nfeedback\\nlinear quadratic control\\nlinear systems\\nmanipulators\\nneural nets\\nneurocontrollers\\nnonlinear control systems\\nrobust control\\nstability\\ntime-varying systems\\ndiscrete time delay feedback control\\nstewart platform\\nintelligent optimizer weight tuner\\ngeneralizable robust control technique\\n6-degree\\nfreedom stewart integrated platform\\n6dof\\nrevolving time-delayed torque control actuators\\nreliable efficiency\\nparallel control manipulators\\noptimal solution\\ntime-delay linear quadratic integral controller\\non-line artificial neural network\\ncost function gain tuner\\nnonlinear system\\ntime-delay\\nintegral control\\nartificial neural network\",\"1140\":\"delay effects\\nperturbation methods\\nsimulation\\nstability analysis\\nsafety\\ndelays\\nautomobiles\\nroad safety\\nroad traffic\\nroad traffic control\\nroad vehicles\\nstability\\nsingle-lane car\\nhuman-driven vehicles\\nhdvs\\nsafety conditions\\ntime delays\\nplant stability\\nstring stability analysis\\noptimal number\\nautonomous vehicle\\nintelligent driver model\\ninput signals time-delay\\nstandard car following models\\nconnected vehicles\\ncfms models\\nlinear string analysis\",\"1141\":\"solid modeling\\nthree-dimensional displays\\nminimally invasive surgery\\nconferences\\nrobot vision systems\\nmanuals\\npredictive models\\nbiological tissues\\nmedical robotics\\nsurgery\\ntelerobotics\\ncomplex surgical environments\\nconfidence-based supervised autonomous suturing method\\nrobotic suturing tasks\\nsuture positioning adjustments\\nrobotic suturing tests\\nsynthetic vaginal cuff tissues\\nconfidence-based method\\npure autonomous suture placement\\nsuture bite sizes\\nrobotic vaginal cuff closure\\nleveraging accuracy\\nautonomous robotic suturing\\nsuture placement accuracy\\nsmart tissue autonomous robot\",\"1142\":\"minimally invasive surgery\\nphantoms\\nimplants\\nkinematics\\nmaintenance engineering\\ntopology\\ncatheters\\ncardiology\\ncardiovascular system\\ndexterous manipulators\\ndiseases\\nmedical robotics\\nphysiological models\\nprosthetics\\nsurgery\\ntorsion\\n5-dof robotically steerable catheter\\nmitral valve implant\\npercutaneous treatment\\nminimally invasive treatment\\nimpaired mitral valve topology\\nsevere heart-related diseases\\nopen-heart surgery\\nsevere mitral regurgitation\\ndirect torquing\\nmitral implant delivery\\ntranscatheter\\nmitral clip implantation\\nmitral valve repair-replacement\\ndirect torsional capability\\nbending joints\\ntorsion joints\\ndexterous manipulation\\nkinematic models\\nphantom heart model\",\"1143\":\"integrated optics\\nbiomedical optical imaging\\nthree-dimensional displays\\nautomation\\nshape\\nconferences\\nsurgery\\nbone\\ncomputerised tomography\\ncutting\\nimage registration\\nmedical disorders\\nmedical image processing\\noptical tracking\\nproduction engineering computing\\nprosthetics\\nlarge-sized skeletal defects\\nexact size\\nskull resection\\noversized cci\\nmanual-cutting process\\nsurgeon\\nmanual resizing\\noperating time\\nnoncontact approach\\nexact contour\\nresection area\\npatient\\nhandheld 3d scanner\\npreoperative ct model\\ncutting toolpath\\nscanned defect model\\nresection contour\\ncutting robot\\nresizing performance\\ndifferent resection shapes\\ncutting experiments\\ndefect contour\\noptical tracking system\\nresizing accuracy\\nrobotic system\\nimplant modification\\nsingle-stage cranioplasty\\ncraniomaxillofacial reconstruction\\npatientspecific customized craniofacial implants\",\"1144\":\"measurement\\ncosts\\nboundary value problems\\nautomation\\nconferences\\ndynamics\\nreal-time systems\\ngraph theory\\nmobile robots\\npath planning\\nsearch problems\\nmotion primitive graph\\ngraph search\\ndispersion-minimizing motion primitives\\nsearch-based motion planning\\npowerful motion planning technique\\nreal-time computation times\\npower-constrained platforms\\nmotion planning graph\\nbaseline motion primitives\",\"1145\":\"training\\nadaptation models\\nsimulation\\nfrequency-domain analysis\\nconferences\\ndecision making\\nreinforcement learning\\nadapted policy\\nmeta-airl\\nmeta-adversarial inverse reinforcement learning\\ndecision making tasks\\nadaptable imitation learning model\\nlearning from demonstrations\\nreward functions\",\"1146\":\"automation\\nconferences\\nswitches\\nentropy\\ntask analysis\\nrobots\\noptimization\\ngradient methods\\nlearning (artificial intelligence)\\nmaximum entropy methods\\nminimax techniques\\noptimisation\\nmme-mtirl\\nrobotic task\\nreward functions\\nmin-max entropy inverse rl\\ninterleaving demonstrations\\nmaximum entropy approach\\nminimum entropy clustering\\nsingle nonlinear optimization problem\\ngradient descent methods\\nminmaxent multitask irl\",\"1147\":\"location awareness\\nthree-dimensional displays\\nshape\\nroads\\ntransportation\\ntools\\ncameras\\nglobal positioning system\\nimage reconstruction\\nintelligent transportation systems\\nlocation based services\\nroad traffic\\nroad vehicles\\nrobot vision\\nstereo image processing\\ntraffic engineering computing\\nvideo signal processing\\ntraffic monitoring cameras\\ntraffic management\\nintelligent road infrastructure systems\\ntraffic scene reconstruction\\ntraffic monitoring videos\\nanonymous data structures\\nvehicle type\\nroadside camera videos\\nvehicle tracking\\nmonocular cameras\\ncarom\\nvehicle localization\\ncars on the map\\ngps data\\ndrone videos\",\"1148\":\"simultaneous localization and mapping\\nautomation\\nconferences\\npose estimation\\nprediction algorithms\\nconvolutional neural networks\\nobject recognition\\nconvolutional neural nets\\nimage fusion\\noptimisation\\nprobability\\nslam (robots)\\nunsupervised learning\\ndense monocular slam\\nsingle rgb imaging\\nclassical probability model\\ndepth estimate fusion\\noutlier-resistant tracking performance\\nfeature-based sparse slam methods\\nper-pixel dense prediction\\ndense cnn-assisted slam methods\\nsimultaneous localization and mapping algorithm\\noptimization process\\ntensorflow datasets\",\"1149\":\"training\\nthree-dimensional displays\\nimage coding\\nlaser radar\\nrobot vision systems\\npipelines\\ncameras\\nfeature extraction\\nimage motion analysis\\nimage reconstruction\\nimage registration\\nimage sensors\\nlearning (artificial intelligence)\\nmedical image processing\\nneural nets\\noptical radar\\nrobot vision\\nhypermap\\ncompressed 3d map\\nmonocular camera registration\\ncomparing lidar scans\\npoint cloud based map\\nexpensive lidar sensor\\ncheaper cameras\\ncamera images\\nonline depth map feature extraction\\noffline 3d map feature computation\\n2d-3d camera registration task\\nend-to-end training\\noffline 3d sparse convolution\\nvoxelwise hypercolumn features\\ncompressed map features\\nrough initial camera\\nvirtual feature image\\ncamera image\\npoint clouds\\nfeature computation load offline\\ncompressing\\nmap size\\nautonomous driving\\nmap\\n2d-3d registration\",\"1150\":\"recurrent neural networks\\nconferences\\ncomputational modeling\\nmemory management\\nestimation\\nmachine translation\\nconvolutional neural networks\\nadaptive control\\nconvolutional neural nets\\nestimation theory\\nfeature extraction\\nfeedforward neural nets\\nlearning (artificial intelligence)\\nrecurrent neural nets\\nbidirectional attention network\\nend-to-end framework\\nlocal information\\nglobal information\\nstrong conceptual foundation\\nneural machine translation\\nlight-weight mechanism\\nbidirectional attention modules\\nfeed-forward feature maps\\nbidirectional attention model\\nstate-of-the-art monocular depth estimation methods\",\"1151\":\"geometry\\nimage resolution\\nestimation\\npredictive models\\ngray-scale\\nfeature extraction\\nrobustness\\nimage enhancement\\nimage restoration\\ninterpolation\\nlearning (artificial intelligence)\\ndepth enhancement\\nself-guided instance-aware network\\ninstance-aware learning\\ndepth estimation\\nedge clarity\\nnetwork learning\\ndepth restoration\\ninstance-level features\\npixel-wise image content\\nmissing depth measurements\\nglossy surface\\nsparse depth measurement\\ndense depth image\\ndepth completion\",\"1152\":\"autonomous systems\\npipelines\\nmerging\\nvirtual reality\\nrobot sensing systems\\ndata models\\nmicrophone arrays\\naugmented reality\\ncomputer simulation\\ndata integration\\nsensor fusion\\nworld-in-the-loop simulation\\nautonomous systems validation\\ndevelopment life cycle\\nmixed-reality\\nsensed data\\nsimulation-reality gap\\nwil simulation\\nconfigurable transformation operations\\nfiltering operations\\nmerging operations\",\"1153\":\"training\\nvisualization\\ngrasping\\nreinforcement learning\\nfasteners\\ndata collection\\ngenerative adversarial networks\\nimage sensors\\nimage texture\\nmanipulators\\nobject detection\\nobject recognition\\nrobot vision\\nimitation learning\\nvision-based robotic manipulation\\nlarge-scale data collection\\nvisual gap\\ngenerative adversarial network approach\\nsimulated images\\nobject-detection consistency\\ntask loss dependencies\\nadapted images\\nrl-based object instance grasping\\ndata regime\\npushing task\\nobject-aware approach\\nsim-to-real transfer\\ndeep reinforcement learning\\ngeneral object structure\\nretinagan\",\"1154\":\"visualization\\nconferences\\nneural networks\\nhumanoid robots\\npredictive models\\nrobot sensing systems\\nhaptic interfaces\\ncomputer vision\\nimage fusion\\nneural net architecture\\nunsupervised learning\\nvisual perception\\nmultisensory foresight\\nembodied agents\\nfuture sensory states\\nmultiple sensory modalities\\npredictive neural network architecture\\nmanually annotated datasets\\nvisual data\\nsingle modality\\nunsupervised method\\nmultimodal perceptions\\nspatio-temporal dynamics\\nhaptic signals\\naudio signals\\ntactile signals\\nsensory modalities\\nhaptic tactile\\nhumanoid robot\\nvisual information\\ndominant modality\\nnonvisual modalities\\naudio tactile\\nvisual frame prediction\",\"1155\":\"training\\nautomation\\nconferences\\nswitches\\nsafety\\ntask analysis\\nrobots\\nlearning (artificial intelligence)\\nmobile robots\\noccupational safety\\nprotective policy\\nexisting skills\\nkey capability\\nreal-world environments\\nsuccessful transfer algorithm\\npolicy transfer algorithm\\nrobot locomotion skills\\ncontrol policies\\ntraining environment\\ntask policy\\nsafety estimator model\\nsimulated robot locomotion problems\\nnotably different environments\",\"1156\":\"legged locomotion\\ntorque\\nrobot kinematics\\nexoskeletons\\nreinforcement learning\\nmuscles\\ncost function\\nadaptive systems\\nbiomechanics\\ncontrol engineering computing\\nhuman-robot interaction\\niterative methods\\nmedical robotics\\nmuscle\\noptimal control\\nrobot dynamics\\ntorque control\\ndata-driven reinforcement learning solution framework\\nadaptive personalization\\nhip exoskeleton\\nrobotic exoskeletons\\nseamless integration\\nhuman user\\nhuman movement\\nhuman-robot dynamics\\nadaptive personalized torque assistance\\nhuman efforts\\nwalking\\nautomatic personalization solution framework\\nassistive torque profile\\ncontrol timing parameters\\nleast square policy iteration\\nparameter tuning policy\\nunilateral hip extension\\noptimal controller\\nadaptive rl controller\\nhuman actions\\nhip extensor muscle\\nhuman mobility augmentation\\ndata-driven solution framework\\nlspi\\nexoskeleton\\noptimal adaptive control\\ndata driven\",\"1157\":\"rotors\\nmachine learning\\nelectromyography\\nreal-time systems\\nrobustness\\ntrajectory\\nsystems support\\nartificial limbs\\nbiomechanics\\ndamping\\nlearning (artificial intelligence)\\nmedical computing\\nmedical control systems\\nrigidity\\nstiffness\\nimpedance controller\\nemg data\\nnatural bouncing patterns\\ndrumming arm\\nupper-limb prosthetic system\\ntransradial amputee drummer\\nquasipassive transradial prosthesis\\ndrumstick grip control\\ncompact motor\\nrotor resistance\\ndrumming grip\\ntriple-stroke rolling\\nperiodic grip\\nparadiddle drumming\\ndrummer healthy hand\\ndouble-stroke rolling\",\"1158\":\"exoskeletons\\nthumb\\nsoft robotics\\nsensor phenomena and characterization\\nsensor systems\\nspinal cord injury\\nadmittance\\nactuators\\nbiomechanics\\ndexterous manipulators\\nforce feedback\\ninjuries\\nmedical robotics\\npatient rehabilitation\\npatient treatment\\nflexotendon glove-iii\\ncervical spinal cord injury\\nhand motor\\nsensory function\\ntendon-driven soft robotic hand rehabilitation exoskeleton\\nhand function improvement\\ncervical sci\\nhigh consistency rubber silicone\\nformfitting exoskeleton glove\\ndurable, exoskeleton glove\\ncustomizable exoskeleton glove\\ntendon tension sensors\\nvoice-controlled soft robotic hand rehabilitation exoskeleton\",\"1159\":\"training\\nresistance\\ntorque control\\nmedical services\\nmuscles\\nlead\\nmathematical models\\nactuators\\nbiomechanics\\nbiomedical education\\ncomputer based training\\ndiseases\\nelasticity\\nforce feedback\\nhaptic interfaces\\nmedical computing\\nmedical disorders\\nmedical robotics\\nmuscle\\nneurophysiology\\npatient diagnosis\\npatient rehabilitation\\npatient treatment\\npipes\\nrigidity\\nvirtual reality\\nseries elastic elbow neurological exam training simulator\\nlead-pipe rigidity\\narm training simulator\\nrigidity simulation tests\\n1-dof kinesthetic force\\nparkinson disease\\nneurological examination\\nhealthcare trainees\\nelastic actuator\\nmuscle resistance\\nmathematical model\\nhyperbolic tangent\\nsize 0.27 nm\",\"1160\":\"simultaneous localization and mapping\\nheuristic algorithms\\natmospheric modeling\\ndynamics\\nestimation\\nlinear programming\\nplanning\\naircraft control\\ngraph theory\\nmobile robots\\noptimisation\\npath planning\\nslam (robots)\\nvehicle dynamics\\nmode transitions\\npose graph optimization process\\nunnecessary transitions\\noptimized mode sequences\\nmultimodal trajectory optimization\\ndynamics models\\ntaxi\\nhover\\nhorizontal flight modes\\ncomposite pose graph optimization\\nmotion planning framework\\nmultimodal vehicle dynamics\\noptimization objective function\\ncontrol constraints\\nsparse factor graphs\\nmode transition constraints-constitute\\nmultimodal motion planning problem\\ncomposite pose graph form\\nsparse graphs\\nresulting motion planning algorithm\",\"1161\":\"automation\\nconferences\\nkinematics\\ncognition\\nplanning\\ndynamical systems\\nmobile robots\\npath planning\\nquery processing\\nrobot dynamics\\nsampling methods\\nsearch problems\\ntrees (mathematics)\\noptimal kinodynamic planning\\nhigh-dimensional configuration spaces\\ntheoretical underpinning\\neffective sampling-based motion planners\\ntypical strategies\\nunderlying search structure\\nroadmap-based planners\\nmultiple kinematic motion planning problems\\nsteering function\\npairwise-states\\nkinodynamic systems\\nsingle-query tree-based planners\\nforward search trees\\ncurrent work leverages these recent results\\nmultiquery framework\\nmotion planning query\\nstate space reachable\\nforward search tree reasoning\\nedge bundle\\ntree node\\nasymptotic optimality\\ndimensional simulated systems\\nhigh-quality kinodynamic solutions\\nsampling kinodynamic edges\",\"1162\":\"robot motion\\nautomation\\nconferences\\ndynamics\\noptimization methods\\nlinear programming\\nreal-time systems\\ncollision avoidance\\nmobile robots\\noptimisation\\nmotion objectives\\nweighted sum nonlinear constrained optimization-based ik problem\\nperformance-critical optimization loop\\nper-instant pose optimization method\\nrobot motions\\nenvironment collision avoidance\\ncollisionik\",\"1163\":\"three-dimensional displays\\nuncertainty\\ntrajectory planning\\npulleys\\ncomputational modeling\\ndynamics\\nwires\\nbelts\\nindustrial manipulators\\nnonlinear programming\\npath planning\\nposition control\\nrobotic assembly\\nrobotic manipulations\\ndeformable objects\\nsystem dynamics\\nhybrid dynamical systems\\nbelt drive unit assembly task\\ntrajectory optimization problem\\ncontact mode sequences\\nfeasible assembly trajectories\\ndynamic modeling\\ndiscrete modes\\nmathematical program with complementarity constraints\\nphysics engine\",\"1164\":\"regulators\\nheuristic algorithms\\nconferences\\noptimal control\\nprocess control\\ntrajectory\\nfeedback control\\ncomputational complexity\\ndiscrete time systems\\ndynamic programming\\nfeedback\\ngraph theory\\nlinear quadratic control\\nlinear systems\\nlinear optimal control\\nfactor graph-based approach\\nauxiliary linear equality constraints\\ntime step\\noptimal control problems\\nconstrained factor graphs\\noptimal trajectory\\nfeedback control policies\\nvariable elimination algorithm\\nmodified gram-schmidt process\\ndynamic programming approach\\ncurrent dynamic programming approaches\\nlinear complexity\\ndiscrete-time finite-horizon linear quadratic regulator problem\\nequality constraints\",\"1165\":\"uncertainty\\nshape\\nrobot sensing systems\\nrobustness\\nsensors\\nplanning\\ntrajectory\\nconvex programming\\nmobile robots\\npath planning\\nposition control\\nprobability\\nrobust optimization-based motion planning\\nsensing uncertainty\\ndegree-of-freedom robots\\ncomplex environments\\nstate uncertainty\\ncomplex geometry\\noptimization-based motion planners\\nrobust trajectories\\nconvex obstacles\\nsequential convex programming\\nconvex subproblem\\nrobust optimization problem\\nrobust problem\\nrobust formulation\\nbody square robot\\nfetch robot\",\"1166\":\"radiation effects\\npathogens\\nautomation\\nconferences\\ngraphics processing units\\ntrajectory\\nplanning\\nmicroorganisms\\nmobile robots\\noptimisation\\nultraviolet sources\\nexposure time\\nuv robot designs\\nnear-optimal plans\\noptimized coverage planning\\nuv surface disinfection\\nuv radiation\\ndisinfection strategy\\nirradiation strategies\\nenvironmental surfaces\\nlong disinfection times\\nnear-optimal coverage planner\\nmobile uv disinfection robots\\nirradiation time efficiency\\ndosage plan\\noptimized taking collision\\nlight occlusion constraints\\ninduced np-hard optimization\\nkey irradiance\\nocclusion calculations\",\"1167\":\"uncertainty\\ncosts\\nautomation\\nheuristic algorithms\\nconferences\\noptimal control\\ndynamic programming\\npenalty-lagrangian functions\\nsecond-order differentiability\\ndifferential dynamic programming\\nddp\\nunconstrained trajectory optimization\\nrobotics\\npenalty methods\\nactive-set approaches\\nconstrained optimal control\\nbellman's principle\\nauxiliary slack variables\\naugmented lagrangian methods\",\"1168\":\"adaptation models\\nsequential analysis\\nschedules\\nupper bound\\ngaussian processes\\nrobot sensing systems\\nunmanned aerial vehicles\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\nwireless sensor networks\\ndslc\\ncoverage performance\\nmultirobot team\\nexpected cumulative coverage regret\\ncoverage task\\ndeterministic sequencing algorithm\\nregret analysis\\nmultirobot coverage\\nunknown field\\nnonuniform sensory field\\ngaussian process\\nsensory function\\nadaptive coverage algorithm\",\"1169\":\"costs\\nmonte carlo methods\\nautomation\\nconferences\\npath planning\\nplanning\\ndistributed control\\nmulti-agent systems\\noptimisation\\npower engineering computing\\nsearch problems\\ntree searching\\noptimal path planner\\ndecentralized monte carlo tree search algorithm\\nnear-optimal paths\\nshort planning horizon\\nappended terminal cost\\nrelated receding horizon methods\\nhorizon search\\nmultiagent approach\\nreceding horizon path\\nutilizes terminal costs\\nreadily-available solution\\nnaive solution\",\"1170\":\"training\\nautomation\\nconferences\\nreinforcement learning\\nmanipulators\\ntrajectory\\noptimization\",\"1171\":\"integrated optics\\nuncertainty\\nrobot kinematics\\nsimulation\\nobject detection\\nsearch problems\\nrobot sensing systems\\nbayes methods\\ncomputer games\\nmobile robots\\nmulti-agent systems\\nsampling methods\\nubiquitous computing\\nmultiagent active search\\nrealistic depth-aware noise model\\nunknown environment\\nrobotics applications\\ngas leaks\\nlocating animal poachers\\nactive search scenarios\\nmultiple agents\\nnoise-aware thompson sampling\\nnats\\nmultiple ground-based robots\\nobject detection uncertainty\\nexhaustive search\\npseudorealistic environment\",\"1172\":\"target tracking\\nrobot kinematics\\nheuristic algorithms\\nmaintenance engineering\\ndrives\\nnumerical simulation\\nminimization\\napproximation theory\\ndecision making\\ngreedy algorithms\\nmobile robots\\nmulti-robot systems\\noptimisation\\npath planning\\ncommunication-aware multirobot coordination\\nmultirobot task planning problems\\nmultirobot setting\\nsubmodular objective\\ncommunication network\\nsubmodular function\\ncommunication-aware submodular maximization\\ncsm\\ndecision-making process\\ndeviation minimization\\ngreedy strategy\",\"1173\":\"heuristic algorithms\\ndecentralized control\\nunmanned aerial vehicles\\nreal-time systems\\ntrajectory\\nvehicle dynamics\\ncollision avoidance\\nautonomous aerial vehicles\\ndistributed control\\nmarkov processes\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nremotely operated vehicles\\naerial robot swarms\\nflocking behavior\\nunmanned aerial vehicle dynamics\\nuav\\nunattainable control inputs\\nfeasible trajectories\\ninter-agent relationships\\npairwise energy function\\ninteracting robot swarms\\nmarkov random field\\nmean-field approximation\\ncollective behavioral rules\\ndistributed control scheme\",\"1174\":\"couplings\\nprotocols\\nautomation\\nconferences\\nclustering algorithms\\nhazards\\nmulti-robot systems\\ninteger programming\\nlinear programming\\nmobile robots\\npattern clustering\\ntelecommunication network topology\\ntelerobotics\\n12 remote robots\\nreachable cluster teleoperation\\ndistributed multirobot system\\nmultiple users\\nlocal robot\\nremote robot team\\nmultiple user commands\\ndistributed clustering algorithm\\nremote robots proportional\\nteleoperation context\\n2 local robots\",\"1175\":\"multiplexing\\nautomation\\nconferences\\nrobots\\ntuning\\noptimization\\ncontrol engineering computing\\nmobile robots\\nmultiplexing robot experiments\\ntheoretical underpinnings\\ninterleaving execution\\nsharing resources\\ntheoretical groundwork\",\"1176\":\"measurement\\nmachine learning algorithms\\nestimation\\ncrops\\nmachine learning\\ngaussian processes\\ninspection\\nagriculture\\ncomputational geometry\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\nrobot programming\\ninspection targets\\nmultirobot inspection problem\\ntask allocation algorithm\\ngaussian process machine\\ncrop rows\\ngeodesic voronoi regions\\nrobot locations\\nrow crops\\nkernel estimation\\nregion-based task allocation\\nremote sensing\\naerial imagery\\nground robots\\nfiner ground-level data\\nmultirobot inspection\",\"1177\":\"state feedback\\ntorque\\noptimal control\\nkinematics\\nreal-time systems\\ntrajectory\\nperformance analysis\\nclosed loop systems\\ndynamic programming\\nend effectors\\nmanipulator dynamics\\nnonlinear control systems\\npredictive control\\ntorque control\\ndifferential dynamic programming\\ncontrol trajectories\\nexhaustive performance analysis\\nopen-loop mpc\\nrapid cyclic end-effector task\\nrobot dynamics\\ninverse dynamics\\nhigh-frequency nonlinear model predictive control\\ndynamic environments\\nmotion plan\\nclosed-loop nonlinear mpc\\ntorque-controlled robot\",\"1178\":\"trajectory tracking\\nheuristic algorithms\\nrobot sensing systems\\nreal-time systems\\ntrajectory\\nsensors\\nnumerical models\\nadaptive control\\nnonlinear control systems\\npredictive control\\ntrajectory control\\nunmanned surface vehicles\\nadaptive nonlinear model predictive control\\nautonomous surface vessels\\nasv trajectory tracking\\nsevere payload variation\\nnonlinear dynamic model\\npayload changes greatly\\nquarter-scale vessel\\nlargely varying payload\\nparametric cost function\\na-nmpc strategy\",\"1179\":\"solid modeling\\nheuristic algorithms\\ndynamics\\nsolids\\nhardware\\ntrajectory\\ntask analysis\\nlegged locomotion\\nmobile robots\\nmotion control\\nnonlinear control systems\\noptimisation\\npredictive control\\nrobot dynamics\\nvehicle dynamics\\ntime-varying model predictive control\\nhighly dynamic motions\\nquadrupedal robots\\nhigh computational power\\nbackflip\\noffline optimization\\ndifficult motions\\nrobotics\\nonline optimization\\nonline-optimized backflips\\nfull-body dynamics\",\"1180\":\"learning systems\\ndictionaries\\nsystem dynamics\\ncomputational modeling\\ntransforms\\npredictive models\\ntools\\nactuators\\nclosed loop systems\\nlearning (artificial intelligence)\\nnonlinear control systems\\npredictive control\\nrobot dynamics\\nkoopman nmpc\\ncontrol-affine systems\\nkoopman-based learning methods\\npractical tools\\ndynamical robotic systems\\nkoopman representations\\nlifted linear models\\nnonlinear actuation effects\\ncontrol methodology\\ncontrol-affine dynamics\\nlifted bilinear model\\nnonlinear model predictive control design\\nexisting koopman-based methods\\nprediction error\\nmodel knowledge\",\"1181\":\"smoothing methods\\nautomation\\nconferences\\ncomplexity theory\\niterative methods\\ntask analysis\\nrobots\\ncomputational complexity\\ngraph theory\\nmobile robots\\nmulti-robot systems\\npath planning\\nsampling methods\\nvisibility roadmap sampling approach\\nmultirobot visibility-based pursuit-evasion problem tasks several pursuer robots\\nsampling-based techniques\\nemploying existing sampling-based methods\\njoint motion strategy\\nsample-generated pursuit-evasion graphs\",\"1182\":\"trajectory planning\\nspraying\\ntraveling salesman problems\\nturning\\nencoding\\ntrajectory\\nplanning\\nautonomous aerial vehicles\\ncomputational complexity\\ngraph theory\\ninteger programming\\nlinear programming\\ntrajectory control\\ntravelling salesman problems\\nsub-optimal strategy\\noptimality constraints\\ntime-optimal multiquadrotor trajectory planning\\ntime-optimal trajectories\\ninfected regions\\nagricultural field\\nquad-rotor\\npesticide tanks\\nspraying refilling\\nproblem formulation\\nmultiple quad-rotors\\nmultiple traveling salesman based time-optimal trajectory generation\\nde-compositional time-optimal trajectory generation\\nmultiple traveling salesman problem\\nmixed integer linear programming problem\\nindependently solving time-optimal trajectory generation problem\\nmulti-robot path planning\\noptimal path planning\\nagricultural applications\",\"1183\":\"tracking\\nrobot kinematics\\nrobot vision systems\\ndynamics\\nproduction\\ncinematography\\ncameras\\nautonomous aerial vehicles\\ncollision avoidance\\ncontrol engineering computing\\nmobile robots\\nmotion control\\nmulti-robot systems\\nrobot vision\\nunscripted action scenes\\nreal-time multiuav coordination system\\ncluttered environments\\nrobot cinematography\\naerial cinematography\\nfilm-makers\\nautonomous unmanned aerial vehicles\\nunstructured cluttered environments\\nprofessional productions\\npost-production\\nmotion coordination\\nscene viewpoints\\naerial cameras\\ndynamic targets\\nshot diversity\\nmutual visibility\\nphoto-realistic simulator\\ncomputational cost\\ntime 1.17 ms\",\"1184\":\"learning systems\\ngeometry\\nnavigation\\nroads\\ndecision making\\nurban areas\\npredictive models\\nautomobiles\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmidas\\nmultiagent interaction-aware decision-making\\nadaptive strategies\\nurban autonomous navigation\\ncrowded environments\\ncomplex urban environments\\nprediction model\\nfuture actions\\noverly conservative plans\\ninteracting agents\\nreinforcement learning-based method\\nego agent\\ncontrol actions\\nurban driving scenarios\\narbitrary number\\ndriver-type parameter\\nsingle policy\\ndifferent planning objectives\\nsimulation environment\\ndiverse interaction experiments\\ndifferent road geometries\\nadaptive ego policy\\ndriving policies\\nexternal agents\\ninteraction-aware driving\\nreinforcement learning\",\"1185\":\"reinforcement learning\\ngrasping\\nneedles\\nend effectors\\nplanning\\ntrajectory\\ntask analysis\\ncollision avoidance\\ndexterous manipulators\\ngrippers\\nimage colour analysis\\nmedical robotics\\nmobile robots\\npath planning\\nrobot vision\\nsurgery\\ntrajectory control\\nneedle pose\\nplanning time\\nbimanual regrasping\\nsuture needle\\nrapid motion planning\\ntime-consuming process\\nsuturing\\ntask-specific mechanism\\nspecific pick-up point\\nproper grasping\\nworking space\\nrapid trajectory generation\\nbimanual needle regrasping\\nsampling-based motion planning algorithm\\nego-centric state\\nbimanual planning problem\\nreference frames\\nfixed frame\\nlearned policy\\nmotion planning algorithms\",\"1186\":\"location awareness\\nrobot motion\\nuncertainty\\nperturbation methods\\npose estimation\\nposition measurement\\ntools\\ncalibration\\nmanipulators\\nmedical robotics\\nneedles\\npath planning\\nrobot vision\\nsurgery\\ntrajectory control\\nda vinci surgical robot\\npath correction method\\nneedle handoff manipulations\\nsurgical suturing\\ncamera-robot calibration\\nplanned trajectory\\npath planner\\ndual-arm needle manipulation\\nsurgical robotics: laparoscopy\\nmedical robots and systems\",\"1187\":\"medical robotics\\nrobot kinematics\\ninstruments\\nsurgery\\nkinematics\\naerospace electronics\\nskull\\nbiomedical optical imaging\\nendoscopes\\ngaussian processes\\nlearning (artificial intelligence)\\nmedical computing\\nsurgical motion pattern\\nskull base surgeries\\nsurgical outcomes\\nsurgical data\\nsurgical motion patterns focus\\nspecific concise surgical tasks\\nsimple surgical procedures\\nsurgical instrument motions\\nspecific surgical tasks\\nsurgical motions\\nsurgical skill levels\\nlearn from small data\\nsupervised autonomy\\nrobotic surgery\\ngaussian process\",\"1188\":\"shortest path problem\\ncosts\\nsimulation\\nroads\\nconferences\\napproximation algorithms\\npath planning\\ngraph theory\\nminimisation\\nmobile robots\\noperations research\\nprobability\\nstochastic processes\\ntransportation\\nchance constrained simultaneous path planning\\ncombined task assignment\\nstochastic costs\\ninitially unassigned robots\\ntask location\\ntotal travel cost\\nstochastic travel costs\\nchance-constrained shortest path problem\\nrobot-task pairs\\nlinear bottleneck assignment problem\\noptimal objective value\\ndeterministic shortest path problems\\nedge costs\",\"1189\":\"automation\\nnavigation\\nsimulation\\nperturbation methods\\nconferences\\ncost function\\npath planning\\naircraft control\\nautonomous aerial vehicles\\ncollision avoidance\\nmobile robots\\noptimisation\\ntrajectory control\\nwind\\ntravel time\\npontryagin's minimum principle\\nanalytical expression\\noptimal heading\\nminimum-time flight\\ngeneral wind case\\nconstrained path navigation\\nartificial potential fields\\ngeneral wind fields\\nuav point-to-point navigation\\nwind influence\\nimposition\\ngeneral desired airspeed profile\\noptimal guidance\\nzermelo\\nuav\\nmotion planning\",\"1190\":\"legged locomotion\\nrobot motion\\nruntime\\ntools\\ncost function\\nplanning\\nreliability\\naerospace robotics\\ncomputational complexity\\ngraph theory\\nmobile robots\\nmotion control\\noptimisation\\npath planning\\nrobot dynamics\\nsolution cost\\nplanning time\\nplanning process\\nlto\\nhigh dof robots\\ncluttered environments\\nmotion planning tools\\nlong-horizon global optimal trajectory\\ntime complexity\\nlazy trajectory optimization\\ngsp\\nglobal graph-search planning\\n21dof legged robot\\n2dof free-flying robot\",\"1191\":\"space vehicles\\nactuators\\nvelocity control\\nrefining\\noptimization methods\\nkinematics\\ntransforms\\ncollision avoidance\\ngradient methods\\nmanipulator kinematics\\nmobile robots\\noff-road vehicles\\noptimisation\\nsteering systems\\npath optimization\\nkinematic constraints\\nactuator space approach\\noff-road ground vehicles\\npath length\\noff-road terrain\\noff-road environments\\nsteering angle\\ngradient descent solver\",\"1192\":\"automation\\nconferences\\nprogramming\\nrobustness\\ngenerators\\ntrajectory\\nsplines (mathematics)\\napproximation theory\\nautonomous aerial vehicles\\ncollision avoidance\\nconvex programming\\noptimisation\\npath planning\\ntrajectory control\\nasymptotic optimality\\nfeasible initial guess\\nasymptotic exact piecewise approximation\\nnonconvex programming problem\\njoint optimality\\noptimal uav-trajectory generation\\nspline subdivision\\nlocally optimal uav-trajectories\\nnonconvex constraints\\nactuation limits\\nlocal optimization-based uav-trajectory generator\",\"1193\":\"smoothing methods\\nthree-dimensional displays\\ntrajectory planning\\ndynamics\\nplanning\\nsplines (mathematics)\\nvehicle dynamics\\ncollision avoidance\\nconvex programming\\nmobile robots\\noptimal control\\noptimisation\\npath planning\\ntrajectory control\\nautonomous vehicle motion planning\\nrecurrent spline optimization\\ndynamic environments\\nstatic obstacles\\ndynamic obstacles\\npath-speed decomposition\\noptimization perspective\\nautonomous vehicles\\nnonconvex constrained nonlinear optimization\\nconvex spline optimization\\nconstrained spline optimization\\noptimization horizon size\\nmotion planning problems\",\"1194\":\"location awareness\\ncorrelation\\nmonte carlo methods\\ncurrent measurement\\nconferences\\nrobot vision systems\\ncameras\\ndistance measurement\\ndistributed algorithms\\nkalman filters\\nmulti-robot systems\\nvisual-inertial odometry algorithms\\nmultistate constraint kalman filter framework\\nlocalization accuracy\\ncentralized-equivalent algorithm\\nrobot-to-robot cross correlations\\ndistributed algorithm\\nmultirobot cooperative visual-inertial localization\\nmonte-carlo simulations\",\"1195\":\"measurement\\nuncertainty\\nthree-dimensional displays\\nrobot vision systems\\ncameras\\ncalibration\\nsensors\\nimage matching\\nremotely operated vehicles\\nrobot vision\\nvideo signal processing\\nhard-to-reach cameras\\ncalibration methods\\nsecurity cameras\\ndrone localization\\nreliable path parameters\\ndrone-based camera calibration\\n2d matching points\",\"1196\":\"training\\nvisualization\\nnavigation\\nurban areas\\nreinforcement learning\\ninspection\\nplanning\\ngraph theory\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nlearning open-world navigation\\nvisual goals\\nlearning-based navigation system\\nvisually indicated goal\\nmobile robot platform\\nappealing alternative\\nrobotic navigation\\nnavigational affordances\\nconventional planning algorithms\\nlearned policy\\ngoal image\\nwaypoint proposal\\ngraph pruning\\nnegative mining\\nreal-world environments\\nprior methods struggle\\noutdoor ground robot\\nving\\ngoal-conditioned reinforcement learning\\nincorporate reinforcement learning\\nunseen environments\",\"1197\":\"training\\nvisualization\\nsystematics\\nnavigation\\nsemantics\\nreinforcement learning\\ntransformers\\ncomputer vision\\nfeature extraction\\nimage segmentation\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nobject tracking\\nrobot vision\\negocentric map representation\\nmap attention\\nmultilayer transformer networks\\n3-d reconstructed indoor pointgoal visual navigation\\nattention schema\\nutilize scene semantics\\nimplicit semantic information\\nagent\\nmaast\\nsemantic transformers\\nefficient visual navigation\\nautonomous agents\\ncore task\\nrobotics\\nlearning-based methods\\ndeep reinforcement learning\\nclassical solutions\\nincreased computational load\\nexisting learning-based solutions\\nvital scene semantics\\ntraversable paths\\nunexplored areas\\nscene objects-alongside raw visual streams\\nsemantic segmentation masks\",\"1198\":\"visualization\\nnavigation\\nimage color analysis\\nshape\\nscalability\\nrobot vision systems\\ncameras\\ndeep learning (artificial intelligence)\\ninference mechanisms\\nmobile robots\\nrobot vision\\nmeta-learning\\ninference network\\nlearned navigation policy\\nsensor configurations\\ntarget colors\\nshot adaptation\\nvisual navigation skills\\nvisual inputs\\ndeep reinforcement learning\\ndeep rl\\nrobotic platforms\\ntypical end-to-end learning\\npoor extrapolation capability\\nsensor configuration\\nlearning algorithm\\nrapid adaptation\\ntarget objects\\npolicy architecture\\ninference networks\\nperception network\",\"1199\":\"training\\nvisualization\\nthree-dimensional displays\\nnavigation\\nnatural languages\\nbenchmark testing\\nrobot sensing systems\\ndecision making\\ndeep learning (artificial intelligence)\\nimage reconstruction\\nmobile robots\\nrobot vision\\ndeep learning\\ncomplex problems\\nvisual sensory inputs\\nnatural language instructions\\nnavigation graph\\ndiscrete action space\\ncomplex vln setting\\ncontinuous 3d reconstructed environments\\nworld navigation\\nrobo-vln tasks\\nlonger trajectory lengths\\ncontinuous action\\nstate-of-the-art works\\ndiscrete vln\\nhierarchical cross-modal agent\\nrobotics vision-and-language navigation\\nhcm\",\"1200\":\"road transportation\\nupper bound\\ncomputational modeling\\npredictive models\\ntools\\ntrajectory\\ncollision avoidance\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\noptimisation\\ntrajectory control\\ncongestion-aware multiagent trajectory prediction\\nai systems\\nmultiagent systems\\ncongestion patterns\\nplanning-based method\\nlearning phases\\ncollision avoidance evaluation\\ncontextual cues\\nsense-learn-reason-predict\\nlatent factors\\nsocial force\\nphysics-based approaches\\noptimization\\nvariational parametrization\\ncollision-free trajectory predictions\\nngsim us-101 highway dataset\\ndataset tools\\ngithub\",\"1201\":\"radio frequency\\nindustries\\nsolid modeling\\nplaning\\nthree-dimensional displays\\nuncertainty\\nnavigation\\ndeep learning (artificial intelligence)\\nobject tracking\\npoisson distribution\\nrandom processes\\ntarget tracking\\nvehicles\\n3d multiobject tracking\\nrandom finite\\nautonomous vehicles\\nmultiple object tracking\\ncritical module\\nsafe planing\\ntracking-by-detection systems\\ninevitably many false positives\\nlearning-based input detections\\nmot\\naccurate tracks\\npersistent tracks\\nautonomous driving applications\\nrfs-m\\npoisson multibernoulli mixture filter\\nlearning-based detections\\ndetection confidence score\\nstate-of-the-art deep learning-based approaches\\nrfs-based approach\\nlearning-based amodal detections\",\"1202\":\"codes\\nautomation\\nconferences\\nobject detection\\nfeature extraction\\ngraph neural networks\\ndata models\\ngraph theory\\nlearning (artificial intelligence)\\nneural nets\\nobject tracking\\nsensor fusion\\njoint object detection\\ncritical components\\nmultiobject tracking systems\\ndata association modules\\nseparate objectives\\nmot system\\nsub-optimal performance\\nmot framework\\ngnn-based joint mot approach\\ndiscriminative feature learning\",\"1203\":\"automation\\nannotations\\nconferences\\nbuildings\\nhuman-robot interaction\\nmachine learning\\ntools\\nlearning (artificial intelligence)\\nmulti-agent systems\\npublic domain software\\nsoftware agents\\nmultimodal agents\\nend-to-end machine learning system\\nrobotics\\nlarge-scale learning\\nopen-source droidlet\\nmodular agent architecture\\nheterogeneous agent architecture\\ninteractive annotation\",\"1204\":\"training\\nlaser radar\\nsimultaneous localization and mapping\\nannotations\\nsemantics\\nray tracing\\nprediction algorithms\\nimage segmentation\\nindoor communication\\nlearning (artificial intelligence)\\nmobile robots\\noptical radar\\npath planning\\nrobot vision\\nslam (robots)\\ntraining pool\\nnetwork predictions\\ncommon localization techniques\\nlidar segmentation\\nautonomous indoor navigation\\nself-supervised learning approach\\nsemantic segmentation\\nlidar frames\\ndeep point cloud segmentation architecture\\nhuman annotation\\nannotation process\\nslam\\nray-tracing algorithms\\nmultiple navigation sessions\\npermanent structures\\ndisentangle short-term\\nlong-term movable objects\\nnew sessions\\nsemantic labels\\nsession\\nsemantically filtered point clouds\",\"1205\":\"training\\nimage segmentation\\nsystematics\\nnavigation\\nimage color analysis\\nsemantics\\nclustering algorithms\\nimage classification\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\nterrain-adaptive off-road autonomous driving\\nself-supervised method\\ntraversable terrain\\ngoal position\\nunknown off-road environment\\ncolor discriminant bias\\noff-road terrain types\\nvehicle-mounted camera\\nviewpoint transformation\\nspatial layout\\ncluster terrain types\\nregister corresponding traversability features\\nfuture navigation decisions\\ncontemporary end-to-end navigation schemes\\noff-road environments\\nunknown traversability characteristics\\nsupervised semantic segmentation schemes\\nself-supervised data labeling\",\"1206\":\"manifolds\\nimage segmentation\\nvisualization\\nroads\\nsemantics\\npipelines\\nneural networks\\nfeature extraction\\ngraph theory\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\npattern clustering\\nrandom walks\\nself-supervised system\\nunseen categories\\ndepth information\\nweighted graph\\ngeneric visual features\\nsample triplets\\ntriplet examples\\nycb-video\\nrgbd-object\\nlow-dimensional features\\nunknown objects\\nunlabeled videos\\nsiamese neural network\\nunsupervised clustering techniques\",\"1207\":\"degradation\\ntraining\\nneural networks\\noptimization methods\\ntraining data\\nrobustness\\ndata models\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\noptimisation\\npath planning\\nrobot vision\\nrobust control\\nvehicles\\nplanning\\nrobotic systems\\nautonomous vehicles\\nrobust optimization\\ncontrol systems\\nimage corruptions\\ndomain shifts\\nadversarial attacks\\nadversarial training\\nimage data augmentation\\nimage degradations\\nsystem performance\\ndegradation parameters\\nadversarial differentiable data augmentation\\nautonomous systems\\nworst-case augmentation parameters\\npgd\",\"1208\":\"training\\ncosts\\nautomation\\nconferences\\nstacking\\nreinforcement learning\\ntask analysis\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\nmanipulators\\nrobot vision\\nlong-horizon tasks\\ndense reward shaping\\nreward machines\\nvision-based robotic manipulation\\nrobot agents\\ncorrect action selection\\noptimal q function\\ndeep q learning\\ntask completion\\ndqn\",\"1209\":\"deep learning\\nvisualization\\ncomputer vision\\naffordances\\nconferences\\ndata models\\nnatural language processing\\nimage representation\\nlearning (artificial intelligence)\\nmanipulators\\nrobot programming\\nrobot vision\\naffordance-directed exploration scheme\\nvisual affordances\\ngeneralist robot\\nlearned skills\\nzero-shot generalization\\ngenerative models\\npotentially possible outcomes\\nnew skills learning\\nvisual representations\\nraw image inputs\\nnew object manipulation\",\"1210\":\"knowledge engineering\\nvisualization\\nshape\\nprogramming\\nhandover\\nrobot sensing systems\\ncognition\\ncomputer vision\\ngeometry\\nimage segmentation\\ninference mechanisms\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nobject recognition\\npath planning\\naccurate pointing task\\nearly actions\\ngeometric reasoning\\nlong-horizon tasks\\nvisual input\\nlong-horizon manipulation tasks\\njoint reasoning\\ndiscrete actions\\nassociated continuous control parameters\\nmotion planning approaches\\nmotion plans\\ncomputation times not suitable\\nreal-time control\\nlearning framework\\nhigh-level reasoning network\\nparameter values\\nlow-level controllers\\nlearned energy function\\ntime-invariant controllers\\nframework end-to-end\\ntamp solutions\\nlogic geometric programming\\ngeometrically precise manipulation tasks\",\"1211\":\"geometry\\nimage color analysis\\ntactile sensors\\nlighting\\ngraphics processing units\\noptical imaging\\nrendering (computer graphics)\\nhuman-robot interaction\\nmanipulators\\ntactile sensing\\nvision-based tactile sensors\\nfully general optical tactile simulation system\",\"1212\":\"tracking loops\\nsmoothing methods\\nsimultaneous localization and mapping\\nestimation\\nlibraries\\ninference algorithms\\nsparse matrices\\nautomobiles\\nestimation theory\\ngraph theory\\nmatrix algebra\\nmobile robots\\noptimisation\\ntrajectory control\\ntrajectory smoothing\\nfactor graph chains\\ntrajectory optimization\\nself-driving car tracking\\naxle\\ntrajectory estimation\",\"1213\":\"iris\\nconferences\\nsurgery\\ninspection\\nmaintenance engineering\\ncomputational efficiency\\nplanning\\ncollision avoidance\\nmobile robots\\noptimisation\\npath planning\\nplanning (artificial intelligence)\\nplan quality\\nplan length\\nincremental random inspection-roadmap search\\nasymptotically-optimal inspection planner\\nhigher-quality inspection\\nprior state-of-the-art method\\nchallenging real-world applications\\nkey computational challenge\\niris faces\\ninspection plans-a procedure\\nlazy edge-evaluation techniques\\niris's search algorithm\\nsearch efforts\\niris's asymptotic optimality\\noriginal iris\\nsimulated bridge inspection\\nsurgical inspection tasks\\nsimilar-quality inspection\\ncomputationally-efficient roadmap-based inspection planning\\nincremental lazy search\\ninspection-planning problem\",\"1214\":\"computers\\ncosts\\nroads\\nscalability\\nconferences\\nparallel processing\\nprobabilistic logic\\ncloud computing\\nparallel algorithms\\npath planning\\nprobability\\nsampling methods\\nserverless multiquery motion planning\\nfog robotics\\nsemistructured environments\\nhigh end computers\\nsampling-based multiquery motion planner\\ncloud-based serverless functions\\nphysical fetch robot\\ndecluttering motions\\nfog based parallelization\\ncloud based parallelization\\nserverless on-demand computing\\nasymptotically-optimal probabilistic road maps\\nprobabilistic completeness\",\"1215\":\"manifolds\\nmeasurement\\nrobot motion\\nplanning\\ntask analysis\\ndynamical systems\\nstress\\ncollision avoidance\\ngeometry\\nmanipulators\\nmobile robots\\nmotion control\\noptimisation\\npath planning\\nposition control\\ncomposable geometric motion policies\\nmultitask pullback bundle dynamical systems\\nfast reactive planning\\nreactive motion policies\\nnoneuclidean manifolds\\nenforcing constraints\\nundesirable potential function local minima\\nprincipled method\\nstable robot motion policy\\ngeometric structure\\nrobot configuration\\ntask spaces\\ntask behaviors\\nseparate position-dependent\\nindividual task design\\nmetric-based tasks\\nnonconflicting potential functions\\ngeometric optimization problem\\nriemannian motion policies\\npbds framework\\ntask design guidance\\nunwanted potential function local minima\\ngeneral manifolds\\npullback bundle dynamical systems framework\\ndesired robot task behaviors\",\"1216\":\"navigation\\nconferences\\nrobot sensing systems\\nairports\\nprobabilistic logic\\nplanning\\nspace exploration\\nmobile robots\\npath planning\\nprobability\\nsampling methods\\nairport floor plans\\nhomotopy-driven exploration\\nhuman-made spaces\\npre-provided maps\\ngeometric maps\\nmotion planning\\nlocal-sensing\\nrobot navigation\\nprobabilistic completeness\\nmotion planners\",\"1217\":\"geometry\\nthree-dimensional displays\\nautomation\\nconferences\\nbuildings\\nspace exploration\\nplanning\\ngraph theory\\nimage resolution\\nmobile robots\\noptimisation\\npath planning\\nrobot vision\\nslam (robots)\\nstereo image processing\\ngraph-based topological exploration planning\\n3d environments\\nhigh-resolution map representations\\ninformation gain maximization\\ngraph-based topological planning framework\\nthree-dimensional space\\nhigh-level intents\\ngeometry information\\nexploration maneuvers\",\"1218\":\"laparoscopes\\nminimally invasive surgery\\nmagnetomechanical effects\\nrobot vision systems\\nforce\\nmagnetic devices\\ncameras\\nbiological tissues\\nbiomechanics\\nbiomedical equipment\\nforce feedback\\nforce measurement\\nsurgery\\nviscoelasticity\\nstress distribution\\nviscoelastic camera-tissue interaction model\\ntissue indentation\\nporcine abdomen tissue\\nnoninvasive force measurement\\nmagnetic actuated insertable laparoscopic surgical camera\\ndeformable tissue\\ninsertable laparoscopic cameras\\ntransabdominal anchoring\\nmagnetic coupling\\ninappropriate stress\\ncamera-tissue interaction force\\nnoninvasive approach\\ncontact angle\",\"1219\":\"location awareness\\nultrasonic imaging\\nthree-dimensional displays\\nultrasonic variables measurement\\nforce\\nrobot sensing systems\\nsafety\\nbiomedical ultrasonics\\ncalibration\\nend effectors\\nforce control\\nmedical image processing\\nmedical robotics\\nmobile robots\\nphantoms\\nposition control\\nslam (robots)\\nstereo image processing\\nthree-term control\\nultrasound-guided peripheral vascular localization\\n2d ultrasound-guided robotic system\\nperipheral vessels\\nforce limits\\npid force controller\\n3d trajectory\\nsurface point cloud\\nsystem calibration\\nend-effector\\nrobot base\\n3d vessel positions\\nultrasound images\\nautonomous rgb-d robotic system\\nrobot scanning\\narm phantom\",\"1220\":\"solid modeling\\nsurface reconstruction\\nthree-dimensional displays\\nbiological tissues\\nsurgery\\ntools\\nsafety\\nimage reconstruction\\nimage registration\\nmanipulators\\nmedical image processing\\nrobot vision\\nstereo image processing\\nsurgical robots\\ndeformable soft tissue\\nsurgical robot autonomy\\nrobotic surgery\\nunstructured environments\\nmodel-based control\\ndeformation dynamics\\ntissue manipulation\\nvision-based perception\\nmodel-based controllers\\ndynamic properties\\nreal-to-sim registration method\\nbridge 3d visual perception\\npbd method\\nsoft tissue dynamics\\nrigid tool interactions\\nvision-based strategy\\n3d reconstructed point cloud surfaces\\nreal-to-sim approach\\ntissue experiments\\nregistration error online\\nautonomous control\\nfusion-based reconstruction\\nsimulated dynamical model\\nposition-based dynamics\",\"1221\":\"training\\nvisualization\\nrecurrent neural networks\\nforce\\nestimation\\nsurgery\\nkinematics\\ncomputer vision\\nforce feedback\\nimage colour analysis\\nlearning (artificial intelligence)\\nmedical robotics\\nneural nets\\nposition control\\nrecurrent neural nets\\ntelerobotics\\nforce estimation neural network\\nrobot state\\nsingle input type\\nworkspace positions\\nstate inputs\\nvision inputs\\nunseen tool\\nunseen material\\nforce features\\nrecurrent neural network\\ntoward force estimation\\ndeep learning\\ninteraction forces\\nteleoperated robot-assisted surgery\\ndirect force sensing\\nend-effector\\ncost-effective sensors\\nvision-based neural networks\\nuseful force estimates\\nreal-time inference\",\"1222\":\"adaptation models\\nautomation\\nrobot kinematics\\nconferences\\ndecentralized control\\nrobot sensing systems\\nsensors\\nadaptive control\\ndecentralised control\\nrobots\\nstability\\ncollective transport\\nunconstrained objects\\nimplicit coordination\\nadaptive compliance\\ndecentralized control algorithm\\ntask-relevant subspace\\ncompliant robotic platform\",\"1223\":\"automation\\nconferences\\noptimal control\\ngames\\napproximation algorithms\\nreal-time systems\\ncomputational efficiency\\napproximation theory\\ncollision avoidance\\ngradient methods\\noptimisation\\nreachability analysis\\noriginal game\\nminimally-invasive control context\\nsingle-player setting\\napproximate solutions\\nreachability games\\nnash equilibria\\ngoal satisfaction\\nmultiple players\\nstate dimensions\",\"1224\":\"automation\\nheuristic algorithms\\nconferences\\nplanning\\ntask analysis\\ngraph theory\\nmanipulators\\nmobile robots\\npath planning\\ntemporal logic\\ntrees (mathematics)\\ntemporal logic specifications\\ndynamically reconfigurable planning methodology\\nbehavior tree-based control strategies\\nreactive tamp\\ntemporal logic-based reactive synthesis\\nreplanning steps\\ncomplete tamp solution\\nreactive task and motion planning\\nincremental graph search\",\"1225\":\"three-dimensional displays\\nshape\\nrobot vision systems\\nsurgery\\ncameras\\nend effectors\\nsensors\\ndexterous manipulators\\nelasticity\\ngeometry\\nimage reconstruction\\noptimisation\\nrobot vision\\nvision-based shape reconstruction\\ngeometric strain parametrization\\nthree-dimensional shape sensing\\nvision-based shape estimator\\ncurved shape\\nsoft arm\\ngeometric strain based representation\\nsoft continuum arms shape\\noptimization\\nend effector\",\"1226\":\"deformable models\\nbridges\\nthree-dimensional displays\\ncomputational modeling\\ndynamics\\nfitting\\nsoft robotics\\ncomputer simulation\\ncontrol engineering computing\\nmobile robots\\npath planning\\nrobot dynamics\\nvine robots\\nhigh-speed dynamics simulator\\nrobot-object interactions\\nmultilink rigid-body model\\nmodel parameters\\nhigh-speed dynamics\\ncluttered environments\\ncomplex dynamics\\nrobot behaviors\\nmotion planning\\ninflated-beam soft growing robots\\ncontact constraints\\nsim-to-real gap\\nvideo data\\nsoft robot control\",\"1227\":\"deep learning\\nlocation awareness\\nsolid modeling\\nthree-dimensional displays\\nautomation\\nconferences\\npose estimation\\nconvolutional neural nets\\ndeep learning (artificial intelligence)\\ngraph theory\\nimage matching\\nspatial variables measurement\\npairwise compatible geometric features\\nrobust pose estimation\\nlow-texture scenarios\\nend-to-end deep neural network model\\ndepth measurements\\npairwise consistency\\n3d geometric features\\nspectral convolutions\\npairwise compatibility graph\\ngraph matching solver\\n6-dof pose estimation\\nocclusion linemod\",\"1228\":\"location awareness\\nsurface reconstruction\\nsimultaneous localization and mapping\\nultrasonic imaging\\ntracking\\nrobot vision systems\\ntools\\nbiomedical ultrasonics\\ncameras\\ndata visualisation\\nfeature extraction\\nimage colour analysis\\nimage motion analysis\\nimage reconstruction\\nimage registration\\nimage sequences\\nmedical image processing\\nmedical robotics\\nmobile robots\\nmotion estimation\\nobject tracking\\nrobot vision\\nskin\\nslam (robots)\\nsurgery\\nrobust skin-feature tracking\\nfree-hand video\\nrobot-held camera\\nclinical-tool localization\\nanatomic vslam\\nclinical tool\\n3d geometric model\\nsmartphone-camera video sequence\\nanatomic simultaneous localization\\ncamera motion\\nvisual methods\\napriltag visual fiducial\\nskin-feature tracking method\\nfree-hand smartphone video\\n3d model\\ncamera trajectory\\nfreehand smartphone-camera tracking\\nnatural skin features\\nanatomic tracking\\nsurgical tools\\nskin-feature visual-tracking algorithm\",\"1229\":\"geometry\\nautomation\\nconferences\\nrobot sensing systems\\napproximation algorithms\\nsensors\\noptimization\\napproximation theory\\ncomputational complexity\\ninteger programming\\nmobile robots\\nnonlinear programming\\npath planning\\nmultitarget sensory coverage\\noptimization-based robotic sensory coverage approach\\ntarget nodes\\nmixed integer nonlinear optimization problem\\nminlp\\npolynomial-time approximation algorithm\\nbounded approximation ratio\\ncoverage path geometry\\ncomplete sensory coverage problem\\npolynomial time solution\\noffline shortest path sensory coverage problems\\nnp-hard problem\\narbitrary convex polygons\",\"1230\":\"measurement\\ndeep learning\\nnavigation\\nheuristic algorithms\\nconferences\\nreinforcement learning\\nsafety\\ncollision avoidance\\ndeep learning (artificial intelligence)\\nlyapunov methods\\noptimisation\\nsafety-critical software\\nlyapunov functions\\nconstraint-satisfaction\\nmultiple linear inequality constraints\\ndnn-based optimizer\\nnumerical constrained optimization\\nforward invariant safe reinforcement learning\\nsafety-critical environments\\nfisar\\ngeneric deep neural network\\nobstacle-avoidance navigation\",\"1231\":\"training\\ncomputer aided instruction\\nautomation\\ndistance learning\\nsimulation\\nconferences\\nreinforcement learning\\ngradient methods\\nmulti-agent systems\\nmulti-robot systems\\nparity check codes\\nrandom codes\\ndistributed multiagent reinforcement learning\\nstraggler effects\\nmultiagent reinforcement learning problems\\ndistributed learning system\\nsystem disturbances\\nmarl algorithms\\nmultiagent deep deterministic policy gradient algorithm\\ndifferent coding schemes\\nrandom sparse code\\nreplication-based code\\nregular low density parity check code\\nmultirobot problems\",\"1232\":\"graphical models\\nautomation\\nconferences\\nstochastic processes\\nprobabilistic logic\\ninference algorithms\\ncognition\\nhuman computer interaction\\nhuman-robot interaction\\ninference mechanisms\\nmobile robots\\ntelerobotics\\ntask-level human intent\\ncontrol interface\\nrobot teleoperation\\ninterface-level\\nphysical actions\\nreasoning\\nunobserved intentions\\nmodel-based inference techniques\\nassistive system\\ncustomized modifications\\n10-person human subject study\\nassistance paradigms\\nuser frustration\\nuser satisfaction\\ncustomized handling\\nunintended interface operation\\nassistive robots\\nassistance system\\nappropriate modifications\\nunintended behavior\\nblack box\\nmeasured user input\\nnoise-free\",\"1233\":\"measurement\\nnetwork topology\\nnavigation\\nmorphology\\naerospace electronics\\nsearch problems\\ntopology\\nevolutionary computation\\ngraph theory\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\npareto optimisation\\nmultiobjective graph heuristic search\\nterrestrial robot design\\nsingle-objective robot co-design\\nmultiobjective control\\njoint multiobjective\\nco-design problem\\nalgorithmically designed robots\\nsingle-objective graph heuristic search\\nhighly efficient multiobjective search\\ncombinatorial design topology space\\nseven terrestrial locomotion\\ndesign tasks\\nthree-objective example\",\"1234\":\"legged locomotion\\ntrajectory planning\\nrobot kinematics\\npneumatic systems\\nmuscles\\naerodynamics\\nplanning\\ngraph theory\\nmobile robots\\nmuscle\\noptimisation\\npneumatic actuators\\nrobot dynamics\\ntrajectory optimisation (aerospace)\\nfactor graph-based trajectory optimization\\npneumatically-actuated jumping robot\\nmechanical compliance\\nrobot legs\\npneumatic artificial muscle\\npneumatic actuator\\nplanar two-legged robot\\nleaping trajectories\\npneumatic muscle-driven robots\\nburst inflation control\\ngtsam optimizer\",\"1235\":\"measurement\\nautomation\\nconferences\\ngrasping\\nbayes methods\\ngrippers\\nrobots\\noptimal control\\npareto optimisation\\nrobot design\\noptimal robot performance\\nmo-bbo\\ngrasping gripper design\\nbimanual arm placement\\nmultiobjective bilevel bayesian optimization\\nbehavior co-design\\npareto front\",\"1236\":\"tracking loops\\nuncertainty\\ntrajectory tracking\\nsimulation\\ndynamics\\nkinematics\\ntools\\nadaptive control\\ncontrol system synthesis\\nmanipulator dynamics\\nmanipulator kinematics\\nmotion control\\nposition control\\nrobust control\\ntracking\\nuncertain systems\\ndynamics uncertainties\\nouter task-space control loop\\nunmodelled dynamics\\ndynamic parameters\\nunknown kinematic parameters\\nrobot manipulators\\ntask-space trajectory tracking\\ncontrol scheme\\nmanipulator task space trajectory tracking\\nunknown kinematics\\nregressor matrix-parameter vector\\nsimple control structure\\nprecise task-space tracking\\ninner joint space control loop\\nmanipulator trajectory tracking\\ndynamics and kinematics uncertainties\",\"1237\":\"legged locomotion\\nrobot kinematics\\nconferences\\nhumanoid robots\\nkinematics\\nlinear programming\\nend effectors\\ngait analysis\\nmanipulator kinematics\\noptimisation\\nearly design goal\\ndesired workspace\\nmultidegree-of-freedom systems\\nextant optimization techniques\\nconceptually simple approach\\nsingle-objective optimization\\ngradient vector\\nsophisticated optimization heuristics\\ngradient zeroes\\nrobotic hand\\ndiscretization-invariant formulation\\noptimal mechanism design\\nmechanical structures\\ncomprise robots\\nsubcategories\\numbrella\",\"1238\":\"measurement\\ngeometry\\noptimization methods\\nsurgery\\nwriting\\nmanipulators\\ntrajectory\\ndexterous manipulators\\nmedical robotics\\noptimal control\\noptimisation\\nposition control\\noptimal multimanipulator arm placement\\nmaximal dexterity\\nrobotics surgery\\nrobot arm placements\\nsurgical preoperative procedures\\noptimal position\\npatient anatomies\\ninformed choice\\nmanipulator workspaces\\noptimal manipulator base positions\\nmultiport da vinci surgical system\\nenvironment-collision\\nscoring functions\\nmultimanipulator setups\\nproxy collision-checker\\noptimization method\\nrobot arms\\noptimization strategy\\nsurgical robotic platforms\\npatient-side manipulator positioning\",\"1239\":\"automation\\nconferences\\ntransportation\\napproximation algorithms\\nrobot sensing systems\\nsensors\\nresource management\\nmobile robots\\nmulti-robot systems\\noptimal control\\nresource allocation\\nmin-cost flow\\nflowdec algorithm\\napproximation factor\\nmultirobot systems\\nflow decomposition\\nfast near-optimal heterogeneous task allocation\\noptimal reward\",\"1240\":\"robust control\\nadaptation models\\ncosts\\nconferences\\ngaussian processes\\ndynamic scheduling\\nresource management\\nmobile robots\\nmulti-robot systems\\ndata-driven adaptive task allocation\\nheterogeneous multirobot teams\\nrobust control barrier functions\\nmultirobot task allocation\\nubiquitous problem\\nrobotics\\nadaptive task-allocation algorithms\\nunknown disturbances\\nunpredicted phenomena\\nrobot models\\nallocation effectiveness\\ntask assignment online\\nenvironmental disturbances\\nrobust task execution\\nmultirobot system\",\"1241\":\"automation\\nsurveillance\\nconferences\\napproximation algorithms\\norbits\\ntrajectory\\npartitioning algorithms\\naerospace robotics\\nautonomous aerial vehicles\\nmotion capture\\nmulti-agent systems\\nmulti-robot systems\\nrobot vision\\nelliptical orbits\\nagent motion\\nmultiagent aerial monitoring\\ndynamic ground convoy\\nnonlinear trajectory\\ntime-varying elliptical orbit\\nmoving convoys\\nmotion capture environment\\nquadrotors\\ncooperative strategy\",\"1242\":\"couplings\\nenergy consumption\\nautomation\\nnavigation\\nconferences\\ncollaboration\\nhardware\\nmobile robots\\nmulti-robot systems\\nknobs\\nlow-cost robotic swarm system\\ninter-robot collaboration\\nrobot swarms\\nphysical coupling\\npuzzlebots\",\"1243\":\"visualization\\nautomation\\nrobot kinematics\\nconferences\\ncollaboration\\nreinforcement learning\\nmanipulators\\ndeep learning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nspatial coordination\\nmultiagent environments\\nincorporating spatial intention maps\\nmultiagent mobile manipulation\\nmultiagent robots\\nintention representation\\nmultiagent vision-based deep reinforcement learning\\ndecentralized mobile manipulators\\noverhead 2d map\\nphysical tasks\\nheterogeneous robot\\nspatial action maps framework\",\"1244\":\"automation\\nconferences\\nswarm robotics\\nrobot sensing systems\\nsensors\\ntask analysis\\nmarkov processes\\nmobile robots\\nmulti-robot systems\\nsegregation\\nswarm intelligence\\nlocal sensing\\nappropriate potential functions\\nflocking-segregative swarming behaviors\\ngibbs random fields\\nheterogeneous robots\",\"1245\":\"automation\\nheuristic algorithms\\nconferences\\nbuildings\\nurban areas\\ncameras\\ndistance measurement\\nbuilding management systems\\ncollision avoidance\\ncomputational geometry\\nimage capture\\nimage processing\\nmulti-agent systems\\ntown and country planning\\nnaive grid method\\nurban environment\\nmultiagent ergodic coverage\\nstreet-level environment\\ndynamic ergodic methods\\nboustrophedon coverage\\nvoronoi region based coverage\\nflying agents\\nlawn-mower sweep\\nmulti-agent\\ncoverage\\nlawn-mower\\nergodic\\nboustrophedon\\nvoronoi\",\"1246\":\"location awareness\\nlearning systems\\nuncertainty\\nfuses\\ncollaboration\\nsensor fusion\\nspatiotemporal phenomena\\nbayes methods\\ngraph theory\\nimage filtering\\nimage fusion\\nlearning (artificial intelligence)\\nmulti-agent systems\\npedestrians\\nstate estimation\\nmodel-based state estimation\\nlearning-based localization methods\\nspatiotemporal graph filter approach\\ngraph learning\\nmultiview sensor fusion\\ncollaborative object localization\\ncomplex object relationships\\nspatiotemporal graph representation\\nlocation estimation\\nmultiple pedestrian localization\\nmultiagent systems\\nbayesian fashion\",\"1247\":\"deep learning\\nuncertainty\\nautomation\\nconferences\\nrefining\\nmeasurement uncertainty\\nestimation\\nimage colour analysis\\nimage matching\\nimage reconstruction\\nimage sequences\\niterative methods\\nneural nets\\nobject detection\\nstereo image processing\\ntriangulation accuracy\\ndepth-refinement network\\ndrn\\ninitial depth map\\nimage\\niterative refinement module\\ndepth accuracy\\ndeep features\\nrefined depths\\npredicted uncertainty\\nactual depth error\\nmultiview depth estimation\\ndense depth\\ndeep neural networks\\ndense-optical-flow network\\npoint cloud\",\"1248\":\"learning systems\\nconvolutional codes\\ncosts\\nthree-dimensional displays\\nrefining\\nestimation\\nfeature extraction\\nimage matching\\nimage representation\\nlearning (artificial intelligence)\\nstereo image processing\\nfast multiview stereo depth estimation\\nincremental viewpoint-compensated feature extraction\\nnovel learning-based method\\nmvs methods\\ncandidate depths\\nscale invariant representations\\nviewpoint changes\\nextraction layers\\nscale invariance\\nfeature layers\\ndepth hypothesis\\nmatching cost volume\\ncoarse image scale\\nprojected features\\nmultiviewstereonet\",\"1249\":\"uncertainty\\ntarget tracking\\nsimultaneous localization and mapping\\nheuristic algorithms\\nsurveillance\\nsimulation\\nswitches\\nmobile robots\\nmulti-robot systems\\npath planning\\nsensor fusion\\nslam (robots)\\nlocal tasks\\ndynamic space-partitioning method\\nlocal subtasks\\nactive information gathering roles\\nhybrid control approach\\ndecomposition-based algorithm\\nlarge-scale estimation tasks\\nexisting centralized approaches\\nscalable active information acquisition\\nmultirobot systems\\nnovel highly scalable nonmyopic planning algorithm\\nmultirobot active information acquisition tasks\\naia scenarios\\ntarget localization\\ntracking\\nactive slam\\ncontrol policies\\nmultiple robots\\nstatic hidden state\\na priori unknown horizon\\naia approaches\\nonline algorithm\\naia task\",\"1250\":\"image segmentation\\nvisualization\\nmotion segmentation\\ndynamics\\ntrajectory\\nplanning\\nsafety\\ncontrol engineering computing\\nimage representation\\nimage sequences\\nmotion control\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\nrobot vision\\nsafety-critical software\\ntrees (mathematics)\\nmultiagent plan segmenting-x\\nimage sequence\\nmultiple robots\\nexplainable multirobot motion planning\\nmaps-x\\ntime segment\\nmmp\\nsafety-critical applications\",\"1251\":\"integer programming\\nheuristic algorithms\\nscalability\\nconferences\\nmachine learning\\nperformance gain\\nsearch problems\\ncomputational complexity\\ngraph theory\\nmotion control\\nmulti-robot systems\\noptimisation\\nspatial temporal splitting heuristics\\nmultirobot motion planning problem\\ngraph-theoretic setting\\nnp-hard problem\\ndivide-and-conquer principle\\ntemporal splitting schemes\\ninteger programming solvers\\nspatial partition\\nenhanced conflict based search algorithm\\nmrmp algorithm\\necbs\\ndao maps\\nsolution optimality\",\"1252\":\"meters\\nsea surface\\nprotocols\\nregulation\\nplanning\\ntrajectory\\nmarine vehicles\\ncollision avoidance\\nmobile robots\\noptimisation\\npath planning\\nremotely operated vehicles\\nrisk analysis\\nships\\nprimitive-based approach\\ngood seamanship path planning\\nautonomous surface vessels\\nmultilayer planning approach\\ngood seamanship practices\\ninternational regulations\\nnovel situational awareness logic\\nmotion primitive-based planners\\nreceding horizon framework\\nship domain\\nship arena concepts\\nrisk metrics\\ncapture colregs compliance\\nmetrics-driven motion planning\\nframework scales\\nnontrivial single-vessel\\nmultivessel situations\\nsimulation-based\",\"1253\":\"heuristic algorithms\\ncomputational modeling\\nstochastic processes\\nreinforcement learning\\nsearch problems\\nprobability distribution\\nbayes methods\\ncomputational complexity\\nmobile robots\\nprobability\\nservice robots\\ngeneral-purpose service tasks\\nhuman-populated environment\\nlongstanding grand challenge\\nvaluable skill\\nscavenger hunt\\nnp-hard stochastic traveling purchaser problem\\nprobability distributions\\nsolution algorithms\\nsh problem\\nmobile robot\\nrl agent\\noptimal performance\\nfuture hunts\",\"1254\":\"runtime\\nautomation\\nelectric breakdown\\nconferences\\nplanning\\ncomputational efficiency\\nrobots\\nimage representation\\nmobile robots\\nmulti-robot systems\\npath planning\\nglobal scale\\ndata resolution\\ncomplex indoor environments\\noutdoor environments\\ncomplex environments fast\\nautonomous exploration\\nhierarchical structure\\nsparse representation\\ndense representation\\nlocal planning horizon\\nexploration path\",\"1255\":\"robot motion\\nthermal expansion\\nshape\\ncomputational modeling\\nkinematics\\nplanning\\nthermal loading\\npath planning\\nrobot dynamics\\nrobot kinematics\\nphysical properties\\nfoldable structure\\nthermal simulation\\ntarget shape\\nself-folding robots\\nfoldability\\ngeometric aspects\\nkinematic aspects\\ndeformations\\nrobot motion planners\\nlaser-forming folding motion planning\\nrobot design\",\"1256\":\"costs\\ntaxonomy\\nmorphology\\noptimization methods\\ngrasping\\nrobot sensing systems\\nhardware\\nbayes methods\\ndexterous manipulators\\ngrippers\\nlearning (artificial intelligence)\\noptimisation\\ndata-driven approach\\nconstant evaluation\\nblack-box function\\ngrasping skills\\nbayesian optimization algorithm\\nlatent-space representations\\ngrasping tasks\\nhuman grasp types\\ncost-efficient hand morphologies\\nhand morphology\\noptimizing robust grasps\\nsensorimotor skills\\nenvironmental changes\\nembodied agent\",\"1257\":\"training\\ngeometry\\nvisualization\\nautomation\\nshape\\nconferences\\nsemantics\\ndexterous manipulators\\nneural nets\\nobject detection\\nrobot programming\\nobject-centric task-axes controllers\\nmonolithic neural network policies\\ncontroller parameters\\nmanipulation tasks\\ngeometric models\",\"1258\":\"perturbation methods\\nheuristic algorithms\\ndynamics\\nrobot control\\ntrajectory\\nfeedback control\\nimpedance\\nfeedback\\nmanipulator dynamics\\nmanipulator kinematics\\nmotion control\\noptimal control\\noptimisation\\ntrajectory control\\nzero-force trajectory\\ncup-and-ball system\\ncomplex objects manipulation\\nrobotic manipulation\\nbio-inspired controllers\\noptimal controller\\nhuman trajectories\\nhuman movement data\\ninternal dynamics\\nunderactuated object\\nhuman behavior\\noptimal feedback control\\ndynamic primitives\",\"1259\":\"sequential analysis\\nsystem dynamics\\nswitches\\ngrasping\\npredictive models\\nrobustness\\nplanning\\ndiscrete time systems\\nlearning (artificial intelligence)\\nlearning systems\\nlinear quadratic control\\nlinear systems\\nmobile robots\\nreactive controllers\\npredictive differentiable controllers\\nlinear dynamical models\\nhumans leverage\\nswitching dynamics\\nmodel inaccuracies\\ncontact regions\\ncomposite dynamical behaviors\\nexpert demonstrations\\nswitching linear dynamical model\\nswitching conditions\\ndiscrete-time lqr\\ndifferentiable policy class\\ncontrol strategy\\nmultiple dynamical modes\\npredicting interactions\\ninaccurate predictions\\nunanticipated contacts\\nlearned behaviors\",\"1260\":\"solid modeling\\nanalytical models\\nthree-dimensional displays\\nautomation\\nconferences\\noptimal control\\nmathematical models\\naircraft control\\ncollision avoidance\\nhelicopters\\nreachability analysis\\nvehicle dynamics\\nreachable set\\nposition isochrones\\nrobotics applications\\npositions reachable\\ntime budget\\npursuit-evasion strategies\\nspecified time limit\\nsimplified 2d model\\nquadcopter dynamics\\ngiven time horizon\",\"1261\":\"trajectory planning\\ncomputational modeling\\nconferences\\naerodynamics\\ndata models\\nplanning\\nvehicle dynamics\\naerospace components\\naerospace control\\naircraft control\\nautonomous aerial vehicles\\noptimisation\\npath planning\\nposition control\\nremotely operated vehicles\\ntwo-stage optimization routine\\nflapping flight trajectories\\ntrajectory optimization problem\\ndata-driven fixed-wing approximation model\\nexperimental flight data\\ninitial guess\\nflapping-wing model\\nbat robot\\nexperimental flight results\\noptimized trajectory\\nstage trajectory optimization\\ndata-driven models\\nunderactuated robots\\ncomplex dynamics\\nflapping-wing aerial vehicles\\nunsteady aerodynamics\\nperiodic gaits\\nplanning procedure\\nflight planning\",\"1262\":\"legged locomotion\\nschedules\\nuncertainty\\ntracking\\ndynamics\\nreliability engineering\\nrobustness\\nfeedback\\nmobile robots\\nnonlinear control systems\\nnonlinear programming\\noptimal control\\npath planning\\npredictive control\\nrobot dynamics\\nstability\\ntrajectory control\\nonline trajectory optimization\\ndynamic aerial motions\\nquadruped robot\\npart framework\\nonline planning\\ncentroidal momentum-based nonlinear optimization\\ndynamic motions\\nuser-specified contact schedule\\nreal-time receding horizon control\\nvariational-based optimal controller\\nmotion planning\\nmit mini cheetah\\nfrequency 500.0 hz\\ntime 0.05 s to 0.15 s\",\"1263\":\"three-dimensional displays\\nradar measurements\\nradar\\ntransforms\\ncameras\\ncalibration\\ntrajectory\\nimage sensors\\nmillimetre wave radar\\nradar imaging\\nradar receivers\\nretroreflectors\\nsensor fusion\\nvelocity measurement\\ncontinuous-time approach\\nsafe autonomous vehicles\\nstandard av sensor suite\\nweather robust sensors\\nmillimetre-wavelength radar\\nsensor data fusion\\nsensor pairs\\nextrinsic calibration algorithms\\n2d radar sensors\\nradar velocity measurements\\nradar retroreflectors\\ncontinuous-time 3d radar-to-camera extrinsic calibration algorithm\\nrigidbody transform\",\"1264\":\"shape\\nurban areas\\nsemantics\\nlighting\\ncameras\\nrobustness\\ncalibration\\nimage recognition\\nintelligent control\\nroad vehicles\\nrobot vision\\nsensors\\nnatural environments\\nnatural objects\\ntraffic sign recognition\\nstop signs\\nauto-calibration method\\nurban autonomous driving applications\\nrobust performance\\nintelligent vehicles\",\"1265\":\"automation\\nconvolution\\nconferences\\nmemory management\\nnetwork architecture\\nreal-time systems\\ninference algorithms\\nconvolutional neural nets\\ninference mechanisms\\nlearning (artificial intelligence)\\ntime series\\npersistent memory footprint\\ntime slices\\ntcn architecture guidelines\\ncomputational constraints\\ntime series data\\nreal time inference\\ntemporal convolution networks\\ntrained tcn\\nrt-tcn algorithm\",\"1266\":\"deep learning\\ntraining\\nautomation\\nconferences\\nbenchmark testing\\nfeature extraction\\nobject recognition\\ndeep learning (artificial intelligence)\\nrobot vision\\nf-siol-310\\nrobotic dataset\\nobject recognition tasks\\ndeep learning systems\\nobject recognition datasets\\nincremental learning capability\\nrobotic vision\\nincremental object learning capability\\nfew-shot incremental object learning problem\\nincremental learning algorithms\\nimagenet\\nhuman assistance\",\"1267\":\"automation\\nheuristic algorithms\\nconferences\\noptimal control\\naerospace electronics\\npredictive models\\nprediction algorithms\\nlearning (artificial intelligence)\\noptimisation\\npath planning\\ndeformable linear object prediction\\nlocally linear latent dynamics\\ndeformable objects\\nnonlinear dynamics\\ninfinite-dimensional configuration spaces\\nnonlinear space\\nlinear space\\nlinear dynamics\\neasier learning\\nmore efficient prediction\\nlocally linear action-conditioned dynamics model\\nfuture latent states\\npredicted latent state\\npredicted state\",\"1268\":\"manifolds\\nthree-dimensional displays\\nmeasurement units\\ntracking\\nsensor systems\\ntrajectory\\ngyroscopes\\naccelerometers\\ncameras\\ndeep learning (artificial intelligence)\\ngait analysis\\nimage filtering\\nimage motion analysis\\ninertial navigation\\nmedical image processing\\nvisual-inertial filtering\\nhuman walking quantification\\nhuman lower-body motion\\nlarger movement assessment system\\nclinical evaluation\\nmultiple wearable inertial measurement unit sensors\\nsingle external rgb-d camera\\nsliding window filter formulation\\nrgb images\\ndeep neural network\\nraw depth information\\nraw imu gyroscope readings\\naccelerometer data\\nnoisy depth data\\n2d joint data\\ngait estimations\\nswf formulation\",\"1269\":\"couplings\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nestimation\\ngraphics processing units\\ntools\\nprobabilistic logic\\ncameras\\ndistance measurement\\ngraph theory\\nimage colour analysis\\npose estimation\\nprobability\\nslam (robots)\\nstereo image processing\\ndense-indirect slam system\\nexternal dense optical flows\\ngeometric priors\\nmonocular capture\\nstereo rgb-d\\nintermediate geometric estimates\\nadaptive priority scheme\\nincremental pose graph\\ndense optical flow methods\\nrobust camera\\nfine-grain globally-consistent dense environmental maps\\nprobabilistic visual odometry model\\nvoldor\",\"1270\":\"computer vision\\nautomation\\ncurrent measurement\\nconferences\\nestimation\\nrobustness\\nfaces\\nconvex programming\\nestimation theory\\ngraph theory\\nimage matching\\nlearning (artificial intelligence)\\ngraph-theoretic approach\\nrobust estimation\\ncorresponding estimation problem\\ncompatibility graph\\ngeneric estimation problems\",\"1271\":\"runtime\\ncodes\\nautomation\\nconferences\\nnoise measurement\\ntime complexity\\noptimization\\ngradient methods\\ngraph theory\\nimage registration\\noptimisation\\ngraph-theoretic framework\\nrobust data association\\nconsistent linking\\ngeometric consistency\\nframework utilize\\ncombinatorial optimization techniques\\nlarge-sized problems\\nhigh-noise regimes\\nhigh-outlier regimes\\ncombinatorial problem\\nlow time complexity\\nconsistently low runtime\\nsmall-sized problems\\nnoisy point cloud registration problems\\npairwise error rectification\\noutlier regimes\\noutlier associations\\ninlier associations\\nclipper\\ntime 15.0 ms\\ntime 24.0 s\\ntime 138.0 ms\",\"1272\":\"training\\nmeasurement\\ntracking\\ndynamics\\nmuscles\\nfatigue\\nkinetic theory\\nbiomechanics\\nelectromyography\\nmedical robotics\\nmuscle\\npatient rehabilitation\\nhuman-exoskeleton system\\nmuscle activity\\nmovement variables\\ndynamic relationship\\nfatigue-induced degradation\\nexoskeleton-administered resistive exercise\\nrobotic intervention\\nfatigue monitoring efforts\\nrobot-aided movement training\\nmonitoring fatigue-induced changes\\nrobot-mediated dynamic\\nrobotic exoskeletons\\nadministering therapeutic exercises\\nassessing human movement quality\\nrobot-mediated exercise\\ncurrent techniques focus\\nlocalized muscle fatigue\\ncomplex relationship\\ndynamic movement\\nsystem-based\\ntime-series model\\napproximate the dynamics\",\"1273\":\"ultrasonic imaging\\ntransducers\\nphotoacoustic effects\\ngrasping\\nrobot sensing systems\\ndistance measurement\\nsensors\\nbiomedical optical imaging\\nbiomedical ultrasonics\\nlead compounds\\nphotoacoustic effect\\nultrasonic transducers\\nfingertip pulse-echo ultrasound\\noptoacoustic dual-modal\\ndual sensing mechanisms near-distance sensor\\nrobotic grasping\\nnoncontact fingertip-mounted sensor\\nnear-distance ranging\\ndirect pulse-echo ultrasound\\noptoacoustic effects\\noptically targets\\nacoustically challenging targets\\noacts\\ndmdsm sensor\\ndistance ranging tests\\nmaterial sensing tests\\noptoacoustic ranging\\nlight-absorbing materials\\npulse-echo ultrasound ranging\\nreflective materials\\ntransparent materials\\ndual-modal spectra\",\"1274\":\"laser radar\\nsoftware algorithms\\nobject detection\\nsensor fusion\\ncameras\\nrobot sensing systems\\nradar tracking\\ncollision avoidance\\nmarine radar\\nmarine vehicles\\nmobile robots\\noptical radar\\nremotely operated vehicles\\nrobot vision\\nperception system\\nautonomous surface vehicle\\nmonocular camera\\nlidar\\nsoftware modules\\nmaritime object detection\\ntracking\\nmaritime surface vessel\\ndescribed system\\ninput sensors\\nobstacle data\\nconsolidated report\\nlive system\\nsize 450.0 m\\nautonomous systems\\nmarine robotics\\nintelligent robots\\nmobile agents unmanned autonomous vehicles\\nautonomous surface vehicles\\nsegmentation\\nclassification\\ncalibration\",\"1275\":\"meters\\nthree-dimensional displays\\npipelines\\ntraining data\\ndetectors\\ncameras\\nrobot sensing systems\\nconvolutional neural nets\\nimage representation\\nkalman filters\\nlearning (artificial intelligence)\\nobject detection\\nobject recognition\\noptical radar\\nradar imaging\\nstereo image processing\\nvegetation mapping\\nmapping system\\ntree detection\\nstereo point clouds\\nrobust tree recognition\\nnoisy stereo data\\nstereolabs zed 2 camera\\nmapping trees\\nunstructured environments\\nnoisy stereo camera point clouds\\nlearned 3d object detector\\n3d object detection\\npseudolidar representation\\npointrcnn detector\\nforest-like environments\\ndetector training data\\nautomatic labeling process\\nglobal point cloud\\nstereo point cloud training data\\npseudolidar detection pipelines\",\"1276\":\"training\\nlaser radar\\nthree-dimensional displays\\ninverse problems\\nshape\\nlips\\nsensor fusion\\nimage colour analysis\\nlearning (artificial intelligence)\\nneural nets\\noptical radar\\nlinear inverse problem\\nrgb image\\ndepth information\\nautonomous driving\\nsparse lidar sensors\\nlow-density point cloud\\nhigh-density counterparts\\nsensor fusion architecture\\nsparse lidar depth completion\\nend-to-end neural network-based algorithm\\nlip\\nmultimodal proximal operator\\npredicted depth map\\nreal indoor nyudepthv2 datasets\\nreal outdoor kitti datasets\\ntartanair simulation dataset\\ndata consistency\",\"1277\":\"conferences\\ntime series analysis\\ndynamics\\ndistributed databases\\npneumatic systems\\nrobot sensing systems\\nhaptic interfaces\\nactuators\\naugmented reality\\nfeedback\\nforce sensors\\ntactile sensors\\ntelerobotics\\ntime series\\ninherent trade-off\\nspatial coverage\\npressure output\\ndynamic response\\nspatial resolution\\nmacro-mini actuation approach\\nsmaller inflatable pouches\\nlarger inflatable pouch\\nstatic responses\\ndynamic responses\\nsingle stacked pouches\\nwearable stacked pneumatic displays\\npneumatic macro-mini approach\\nhuggable robot\\ndistributed force sensors\\npneumatic pouches\\nsoft wearable haptic displays\\npneumatic wearable haptic devices\\npressure feedback\\nhuman operators\\nrobot teleoperation\\nvirtual reality\",\"1278\":\"training\\nparallel robots\\ntensors\\ncomputational modeling\\nwearable computers\\nkinematics\\nartificial neural networks\\ncouplings\\ndesign engineering\\nend effectors\\nmanipulator kinematics\\nmedical robotics\\nneurocontrollers\\npatient rehabilitation\\nposition control\\nwearable robots\\nrobot linkages\\njoints misalignments\\nmanufacturing tolerances\\nmechanical members\\nend-effector orientation\\nend-effector position\\nhuman usage\\nrobot workspace\\nforward kinematics\\njoint angle measurements\\nmoving inertia\\nlight-weight designs\\nrehabilitation training\\nphysical assistance\\nsemirigid links\\nwearable parallel robot\",\"1279\":\"databases\\nsemantics\\nrobot vision systems\\naerospace electronics\\ncinematography\\ncameras\\ntrajectory\\ncrowdsourcing\\nimage representation\\nlearning (artificial intelligence)\\nrobot vision\\nvideo signal processing\\nexpressive robot cinematography\\naerial vehicles\\ndynamic viewpoints\\nautonomous flight technology\\nexpressive camera behaviors\\nnontechnical users\\nunintuitive control parameters\\ndata-driven framework\\nediting\\ncomplex camera positioning parameters\\nsemantic space\\nvideo clips\\nphoto-realistic simulator\\ncrowd-sourcing framework\\nsemantic descriptors\\nsemantic control space\\ncinematography guidelines\\ngenerative model\\nlow-level camera trajectory parameters\\nsemantic video descriptors\",\"1280\":\"legged locomotion\\ntraining\\ncosts\\nconferences\\nmorphology\\nreinforcement learning\\nprobabilistic logic\\ncontrol engineering computing\\nmotion control\\nposition control\\nprobability\\nperiodic reward composition\\nbipedal locomotion\\nsim-to-real reinforcement learning\\nrandom seeds\\njoint positions\\nhigh-quality reference motions\\npolicy behavior\\nreward-shaping\\nprobabilistic periodic costs\\nparametric reward function\\nbipedal gaits\\nsim-to-real transfer\\nbipedal robot\\ntwo-beat gaits\\nreward-specification\\nreference-free reward functions\\nstanding\\nwalking\\nhopping\\nrunning\\nskipping\\ncassie\",\"1281\":\"training\\nruntime\\nnavigation\\nconferences\\npipelines\\ntraining data\\nmachine learning\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nmachine learning paradigm\\nautonomous navigation\\ncompletely safe environments\\nnumerous imaginary obstacles\\nnavigation planners\\nfeasible navigation\\nrobot perception\\nhallucinated training data\\nsometimes-infeasible prior knowledge\\nconservative planning\\nlfh paradigm\\nruntime hallucination\\nrealistic navigation scenarios\\nnovel hallucinated learning\\nsimulated navigation environments\\noriginal lfh method\\nclassical navigation planner\\nagile robot navigation\\nsober deployment paradigm\",\"1282\":\"conferences\\nmorphology\\nprocess control\\ncomputational efficiency\\nbayes methods\\ntask analysis\\nkernel\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nneural nets\\noptimisation\\nrobots\\nmultifidelity bayesian optimization framework\\ncomputational resource\\nneural fidelity warping\\nrobot morphology design\\nrepresentations learning\",\"1283\":\"fabrication\\nanalytical models\\nautomation\\nconferences\\ndesign tools\\ngrippers\\nrobots\\ndesign engineering\\nfinite element analysis\\noptimisation\\nplates (structures)\\nsandwich structures\\ncomputational design\\ncorrugated mechanisms\\nfoldable structures that harness origami-inspired methods\\nstructural stiffness\\nhigh strength-to-weight ratio\\ndesign oadlc mechanisms\\ndesired behavioral specifications\\nin-plane stiffness\\nout-of-plane stiffness\\nvalidate analytical formulas\\ndesign parameters\\ngiven design constraints\\noptimized oadlc mechanisms\\nrapid design method\\nstiffness-enhanced mechanisms\",\"1284\":\"legged locomotion\\nautomation\\ngears\\nconferences\\nexoskeletons\\nforce\\nprototypes\\nartificial limbs\\nbicycles\\ndesign engineering\\nmedical robotics\\npower transmission (mechanical)\\nrobots\\ntorque control\\nenergetically-passive robot exoskeletons\\nbicycle\\nsophisticated mechanism\\nvariable gear transmission mechanism\\nhuman-driven compliant transmission mechanism\\nsupplied energy\\nstored energy\\nhuman-driven artificial limb\\nhuman mobility\\nhuman driven compliant transmission mechanism\",\"1285\":\"shafts\\nautomation\\nconferences\\nforce\\nprototypes\\nsprings\\nrobots\\nmanipulator dynamics\\nsprings (mechanical)\\nantagonist tendons\\nagonist tendons\\nsingle motor\\nmotor shaft\\nmultiple tendons\\nmultiple joints\\nunderactuation method\\ndesign matrix\\nunderactuated robot hands\\nspring agonists\\ndesign paradigms\\nmultiple desirable behaviors\\nresulting spring force cancellation\",\"1286\":\"training\\nwireless communication\\nimage segmentation\\ncomputer architecture\\nmanuals\\ngastroenterology\\ngastrointestinal tract\\nbiomedical optical imaging\\nconvolutional neural nets\\nendoscopes\\nfeature extraction\\nimage classification\\nlearning (artificial intelligence)\\nmedical image processing\\nangiodysplasia segmentation\\nattention-guided networks\\ndomain adaptation\\ngastrointestinal bleeding\\nangiodysplasia diagnosis\\nwireless capsule endoscopy images\\ncomputational methods\\nconvolutional neural networks\\nadnet gains accuracy\\ndomain-adversarial training\\nmultibranch cnn architecture\\ncore branch\\nad segmentation\\nattention module\\nattention branch\\nnetwork feature learning\\ninformative ad relevant regions\\nadaptation branch\\ndomain-invariant features\\nwce image acquisition\\nwce datasets\",\"1287\":\"adaptation models\\nautomation\\nfluid dynamics\\nsurgery\\ntools\\npredictive models\\ntrajectory\\nbiological tissues\\nblood\\ncomputational fluid dynamics\\nhaemodynamics\\nmanipulators\\nmedical robotics\\nposition control\\npredictive control\\ndifferentiable fluid dynamics\\nsuction model\\nhandcrafted human-intuitive suction policies\\nmodel-predictive control\\nblood suction\\nsurgical hemostasis\\nsurgical robotics\\nsoft tissue manipulation\\ncutting\",\"1288\":\"visualization\\nsmoothing methods\\nshape\\nheuristic algorithms\\nfabrics\\nrobustness\\ntask analysis\\nindustrial manipulators\\nlearning (artificial intelligence)\\ndense visual correspondences\\nrobotic fabric manipulation\\ninfinite dimensional configuration space\\npoint-pair correspondences\\ninitial fabric configuration\\nmultistep fabric smoothing\\nfolding tasks\\nphysical robotic systems\\naverage task success rate\",\"1289\":\"geometry\\nshape\\nforce\\ntactile sensors\\nestimation\\nnumerical simulation\\nnumerical models\\nelasticity\\nend effectors\\ngrippers\\noptical sensors\\ndeformable objects\\nhigh-resolution contact geometry\\nobject compliance\\nrobotic arm\\nelastic membrane\\nrobotic fingers\\ndepth sensor\\nstretch effective modulus\\nsubmillimeter accurate estimates\\nvariable stiffness soft tactile end-effector\\nsoft optical tactile sensors\\nsoft to resistive elastic tactile hand\\nparallel-jaw gripper\",\"1290\":\"hand tools\\nautomation\\nshape\\nfriction\\nconferences\\ngrasping\\ninterference\\ndexterous manipulators\\nmotion control\\nstability\\nphysical experiments\\nvirtual experiments\\npalm diameter\\nin-hand manipulation performance\\nobject shapes\\nkey manipulation primitive motions\\nobject sizes\\nactuated palm mechanism\\npalm height\\nadjustable palm\\nmid-air shelving in-hand manipulation task\\nactive palm enhances dexterity\\nstabilizing contact\\nin-hand manipulation capabilities\\nsoft hand\\nsoft robotic in-hand manipulation\\nlimited finger dexterity\",\"1291\":\"motion segmentation\\nfriction\\nconferences\\nneural networks\\nsoft robotics\\nhardware\\nend effectors\\ncontrol engineering computing\\ndeformation\\nforce control\\nmanipulator dynamics\\nneural nets\\npath planning\\ntrees (mathematics)\\nrrt*-based planner\\nunmodeled force compensation\\nmotion planning\\nend effector position error\\nneural network\\nsimplified quasistatic model\\nsoft robots\\nmotion planners\\ndeformable materials\\nsoft manipulator planning\",\"1292\":\"automation\\nconferences\\nknowledge representation\\ntask analysis\\ncontext modeling\\nautonomous robots\\ncontrol engineering computing\\nmobile robots\\nrobot programming\\nsemantic networks\\nexecution environment\\ntask plan constituents\\nknowledge graph embeddings\\ntasks plan execution\\nsingle end-user demonstration\\nrobust one-shot task execution\\nautonomous robot operation\",\"1293\":\"tracking\\nconferences\\nkinematics\\nreliability engineering\\nstability analysis\\nrobustness\\nplanning\\ncontrol engineering computing\\nforce control\\niterative methods\\nmanipulator dynamics\\noptimisation\\nprogram testing\\nsearch problems\\nvelocity control\\ncontact constraints\\noptimal solution\\niterative search\\nsearch-based technique\\nkinematic singularity\\noptimal hybrid force-velocity control\\nclosed-form method\\nkinematic conditioning\\nmechanical system\\nfree objects\\nrigid environment\",\"1294\":\"manifolds\\nautomation\\nattitude control\\nquaternions\\nsimulation\\nconferences\\naerospace electronics\\nadaptive control\\nalgebra\\nfeedback\\ngeometry\\nnonlinear control systems\\npd control\\nrobust control\\ntracking\\nuncertain systems\\nvariable structure systems\\nglobal attitude tracking\\nfeedback controllers\\nsliding control\\nuncertain dynamics\\ngeometric attitude control\\nquaternion-based sliding variable\\nerror dynamics\\nattitude trajectory\\nnoneuclidean space\\nnonlinear pd control\\nadaptive sliding control\",\"1295\":\"couplings\\nnavigation\\ngames\\nprobabilistic logic\\nrobustness\\nencoding\\nsafety\\ngame theory\\nroad traffic\\nprobabilistic constraint satisfaction\\ngeneral-sum dynamic game theory\\nadversarial phase\\nencoding safety\\nmultiple traffic scenarios\\ndefensive driving\\ndynamic nash game\",\"1296\":\"covid-19\\nair cleaners\\npathogens\\nfiltration\\nprototypes\\ncleaning\\nrobustness\\ncollision avoidance\\ncomputer aided instruction\\nhuman-robot interaction\\nindoor radio\\nintelligent robots\\nmicroorganisms\\nmobile robots\\noccupational safety\\nenhancing safety\\nmobile air filtration\\nschool reopening\\noccupant-safe continuous protection\\nfixed air filters\\nkey pandemic prevention method\\npublic indoor spaces\\nairborne pathogens\\nentire room\\ndisinfection co-robot prototype\\noccupant-friendly protection\\nclassroom scenario\\nstatic classroom\\ngrid pattern\\nmobile robot\\nworst-case pathogen dosage\\nstatic filter\\nrobot protection\\nworst-case dosage\",\"1297\":\"road transportation\\nuncertainty\\nmerging\\nprobabilistic logic\\nreal-time systems\\nsafety\\nquadratic programming\\nautomobiles\\nmobile robots\\noptimal control\\noptimisation\\nprobability\\nroad safety\\nroad traffic control\\nroad vehicles\\nformally provable feasibility guarantee\\nprobabilistic safety-assured adaptive merging control\\nautonomous vehicles\\ntremendous challenges\\nhuman drivers\\ncontrol methods\\nsafety guarantees\\nongoing research goal\\nreal-time safe control framework\\ncontrol barrier function\\nautonomous ego vehicle\\nhuman-driven cars\\nconsistent safety guarantee\\nmotion uncertainty\\ncontrol barrier functions\\nprobabilistic setting\\nprovable chance-constrained safety\\ncontrol design\\nformulated bi-level optimization framework\\nnominal controller\\nprobabilistic safety constraints\\ndifferent driving strategies\",\"1298\":\"visualization\\nadaptation models\\nautomation\\nconferences\\ndata models\\nhaptic interfaces\\nrobots\\nassistive robots\\nlearning systems\\nmanipulators\\nrobot vision\\nhaptic context\\nvisual model\\nmodified linear contextual bandit framework\\npost hoc context model\\nfaster learning\\nbandit settings\\nautonomous robot-assisted feeding\\nfood items\\nmanipulation strategy\\nunseen food item\\nlinear bandit\\nvisual context\\nmultimodal properties\",\"1299\":\"knee\\nultrasonic imaging\\nkinematics\\nstairs\\nrobot sensing systems\\nsteady-state\\nsensors\\nelectromyography\\ngait analysis\\nlearning (artificial intelligence)\\nmedical control systems\\nmedical signal processing\\nmuscle\\ncontinuous joint kinematics\\ntransient ambulation\\nultrasound sensing\\ncontinuous adaptation\\ncontinuous volitional control\\nspatiotemporal ultrasound features\\ncontinuous knee kinematics\\ntask-invariant learning paradigm\\ntask-specific paradigm\\nsteady-state level-walk\\nsteady-state stair ambulation\\ncontinuous estimations\\ntask-specific control schemes\",\"1300\":\"limiting\\nforce feedback\\nlead\\nfatigue\\nrobot sensing systems\\nregulation\\ntopology\\nartificial limbs\\nbiomechanics\\nelectromyography\\nfeedback\\nforce control\\nhaptic interfaces\\nmedical robotics\\nprosthetics\\nkinesthetic feedback\\nnovel prosthetic designs\\ncable-driven prostheses\\nmodern myoelectric prostheses\\nhaptic feedback\\ncable-driven body-powered prostheses\\nkinesthetic sensory information\\ngrasp force control\\nbody-powered prosthesis emulator\\ndisplayed force feedback\\nlower grasp forces\\nsteadier grasp forces\\ngrasp performance benefits\",\"1301\":\"geometry\\ntensors\\nautomation\\nconferences\\ntools\\nfabrics\\ncalculus\\ncomputational geometry\\ndifferential geometry\\nrobots\\nfinsler geometry\\nrobotics research\\nriemannian geometry\\nspray geometry\\ngeneralized nonlinear geometry\",\"1302\":\"costs\\nautomation\\nconferences\\napproximation algorithms\\ncognition\\nplanning\\ncomputational efficiency\\napproximation theory\\ncollision avoidance\\nmobile robots\\nsampling methods\\nsafe motions\\nexplicit computation\\ncomputational cost\\nmotion planning\\napproximation algorithm\\ncollision detection invocations\\nmedial axis sampling\\nrobot\",\"1303\":\"legged locomotion\\ndeformable models\\ndatabases\\ndynamics\\nmedia\\ndata models\\ntrajectory\\ngait analysis\\ngradient methods\\nlearning (artificial intelligence)\\nmechanical contact\\noptimal control\\noptimisation\\nquadratic programming\\ncontact model\\nlower-level problem\\nupper-level problem\\nlocomotion trajectories\\ncontact-implicit trajectory optimization\\nlearned deformable contacts\\nbilevel optimization\\nrobot trajectories\\nlearned soft contact models\\ncontact forces\",\"1304\":\"energy consumption\\nuncertainty\\nsimulation\\noceans\\npredictive models\\nrobot sensing systems\\nminimization\\nautonomous underwater vehicles\\nhydrological techniques\\nmobile robots\\noptimisation\\npath planning\\nremotely operated vehicles\\nenergy optimal path planning\\nestimated flow parameters\\nactive flow perception\\naccurate flow predictions\\nflow information\\nflow prediction uncertainty\\nenergy-optimal path planning approach\\nvehicle energy consumption minimization\\nempirical flow model\\npod\\ncramer-rao bound\\ncr\\nflow predictions\\nenergy-saving performance\",\"1305\":\"learning systems\\nadaptation models\\nuncertainty\\nlimiting\\nreinforcement learning\\ndata models\\ntask analysis\",\"1306\":\"trajectory planning\\nheuristic algorithms\\nneural networks\\ncost function\\ntrajectory\\nplanning\\nsafety\\ncollision avoidance\\niterative methods\\nmobile robots\\noptimisation\\ntrajectory control\\nadversarial attacks\\nalgorithmic architecture\\nsmooth trajectories\\noptimization cost function\\ntrajectory planners\\nrobot physical specifications\\niterative optimization based planners\\nstate configurations\\neigenstructure\",\"1307\":\"deep learning\\ntraining\\nlaser radar\\nthree-dimensional displays\\ntv\\nsemantics\\nfeature extraction\\nentropy\\nimage colour analysis\\nimage recognition\\nimage segmentation\\nmobile robots\\noptical radar\\nrobot vision\\nweighted cross-entropy losses\\nlidar data\\nuse circular padding\\ntornado-net\\nmultiview total variation semantic segmentation\\ndiamond inception module\\npoint clouds\\nscene understanding\\nautonomous driving\\nneural network\\n3d lidar point cloud semantic segmentation\\nbird-eye\\nencoder-decoder resnet architecture\\ndiamond context block\\ncurrent projection-based methods\\nlocal neighbourhood information\",\"1308\":\"laser radar\\nthree-dimensional displays\\nsemantics\\nrobot sensing systems\\nharmonic analysis\\nreal-time systems\\nsensor systems\\ncomputational complexity\\nconvolutional neural nets\\nimage coding\\nimage resolution\\nimage segmentation\\noptical radar\\nvisual perception\\nautonomous driving systems\\nsensor readings\\nsemantic segmentation methods\\nlidar sensors\\nlite-hdseg\\nreal-time convolutional neural network\\n3d lidar point clouds\\nlight-weight harmonic dense convolutions\\nsemantic segmentation approaches\\nrobotic driving applications\\nautonomous driving applications\\nlidar semantic segmentation\\nautonomous driving vehicles\\nrobotic systems\\nperception modules\\nlite harmonic dense convolutions\\nmulticlass spatial propagation network\\nglobal contextual module\",\"1309\":\"image segmentation\\nthree-dimensional displays\\nshape\\nnavigation\\nsemantics\\ntraining data\\nrobot sensing systems\\ngraph theory\\nimage colour analysis\\nimage sensors\\nimage sequences\\nobject detection\\noptical radar\\nscene data\\nlearning-based semantic instance segmentation methods\\nrobotics applications\\nexpensive training data annotations\\nsingle sensor modality\\nknown object classes\\nnovel graph-based instance segmentation approach\\n2d image sequence\\n3d point cloud\\ngeneral graph representation\\nphotometric information\\ncomplementary sensor modalities\\nrgb-d data\\nlidar+image sensor data\\ngraph-based method\\nclass agnostic joint instance segmentation\",\"1310\":\"location awareness\\nautomation\\nconferences\\nrobot sensing systems\\nhardware\\nsensors\\nplanning\\ncooperative communication\\nmobile robots\\nroad vehicles\\nsensor fusion\\nvehicular ad hoc networks\\naacsp concept\\ndriving functions\\nautonomous mode\\naffordable autonomy\\nl4 autonomous vehicles\\ncooperative sensing\\nlongitudinal control\\nlateral control\\nlevel-26 vehicle\\ncooperative perception\\nlocalization modules\",\"1311\":\"deep learning\\nportable computers\\nlimiting\\ncosts\\nheuristic algorithms\\nconferences\\nmemory management\\ncontrol engineering computing\\ndeep learning (artificial intelligence)\\niterative methods\\npendulums\\nreachability analysis\\nrobots\\nreachable polyhedral marching\\nsafety verification\\nrobotic system\\ndeep neural network components\\nexact reachable sets\\ndeep neural networks\\nrectified linear unit activation\\nsafety analysis\\nrobotic perception\\nrelu network\\nperception-action loop\\npolyhedral cells\\nreachability computation\\nacas xu aircraft advisory system\\npendulum dynamics\",\"1312\":\"upper bound\\nheuristic algorithms\\nbuildings\\nestimation\\nnumerical simulation\\nplanning\\nsteady-state\\nmobile robots\\nmulti-robot systems\\npath planning\\nmultirobot dynamical source\\nalgorithmic framework\\nmultirobot system\\ndummy confidence upper bound\\nd-ucb\\nstandard ucb algorithm\\nmultiarmed bandits\\nmultirobot task planning\\ndoss algorithm\\ndistributed online source seeking algorithm\",\"1313\":\"automation\\nsimulation\\nconferences\\nrobot vision systems\\ncameras\\nobject recognition\\ntask analysis\\ndistributed sensors\\nmobile robots\\nmulti-robot systems\\npath planning\\ncoverage objectives\\nexpected coverage\\ninformation-based approximations\\ninformation-based objectives\\nvolumetric objective\\nmultirobot exploration\",\"1314\":\"schedules\\nneural networks\\ncollaborative work\\ndata models\\nspace exploration\\ntrajectory\\ntopology\\nclient-server systems\\ndata structures\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\nneural nets\\nflow-fl\\ndata-driven federated learning\\nspatio-temporal predictions\\nmultirobot systems\\nfederated learning framework\\ndistributed data\\nconnected robot teams\\nneural network weights\\nglobal model\\ntraditional fl approach\\nserver aggregates\\nlocal models\\naggregation process\\ngossip-based shared data structure\\ndata-driven mechanism\\nlearning process\\nmodel updates\\nagent trajectory forecasting problem\\nmultiagent setting\\nstaggered online data collection\\ndata flow\\nparticipating robots\\nmultirobot setting\",\"1315\":\"target tracking\\ncosts\\nuncertainty\\nrobot kinematics\\nheuristic algorithms\\nrobot sensing systems\\nlinear programming\\ngreedy algorithms\\nmobile robots\\nmulti-robot systems\\noptimisation\\npath planning\\nsearch problems\\nnonmonotone energy-aware information gathering\\nheterogeneous robot teams\\nsensor-equipped robots\\ndynamical process\\ntrade-off between information gain\\nenergy cost\\nnonmonotone objective function\\nrobot trajectories\\ncommon multirobot planning algorithms\\nlocal search\\nnonmonotone submodular function\\ndistributed planning approach\\nnaive distributed execution\\ncoordinate descent based algorithm\\ndesirable trade-off between sensing\",\"1316\":\"conferences\\nsemantics\\nsupervised learning\\nbuildings\\nbandwidth\\npath planning\\nclassification algorithms\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\nunsupervised learning\\nmultirobot distributed semantic mapping\\nunfamiliar environments\\nonline matching\\nlearned representations\\nstate-of-the-art semantic mapping systems\\nsupervised learning algorithms\\nunsupervised learning algorithms\\nmultiple robots\\nerroneous matches\\ninconsistent matches\\nlocal maps\\nconsistent global map\\nunsupervised semantic scene model online\\nmultiway matching algorithm\\nlearned semantic labels\\nhigher quality global maps\",\"1317\":\"sensor placement\\nsolid modeling\\nthree-dimensional displays\\nsurveillance\\ngeology\\nfocusing\\nrobot sensing systems\\napproximation theory\\nmathematical programming\\nstereo image processing\\ntelecommunication network planning\\nwireless sensor networks\\nglobally optimal coverage\\n3d-embedded surfaces\\nstructural study\\nmobile sensor coverage optimization problem\\nuvc-based surface disinfection\\ndisease agents\\nsars-cov-2\\nunified general sensor coverage problem\\nsingle-best coverage quality\\ncumulative quality\\nmathematical programming models\\ncamera network deployment\\napproximation schemes\",\"1318\":\"analytical models\\nautomation\\nsimulation\\nconferences\\ndata models\\nsafety\\nreachability analysis\\nautomobiles\\nroad safety\\nroad traffic control\\nvelocity control\\nstop-and-go traffic waves\\nhuman driving behaviors\\nfollowerstopper controller\\nrelative safety\\ndistance-based criterion\\ntime headway-based safety analysis\\ntime-based safety criterion\",\"1319\":\"uncertainty\\nconferences\\noptimal control\\ngames\\nreal-time systems\\ntrajectory\\nplanning\\ncollision avoidance\\nfeedback\\ngame theory\\nmobile robots\\npath planning\\nprobability\\ntrajectory control\\nuncertainty handling\\nmultihypothesis interactions\\ngame-theoretic motion planning\\nnonego players\\ntrajectory games\\nautonomous vehicles\\nmultiple hypotheses\\ncandidate hypothesis\\nbernoulli random variable\\nleverage constraint asymmetries\\nfeedback information patterns\\nhypothesized agent\\nego agent\\ninteractive trajectories\\neasy-to-model uncertainty\",\"1320\":\"shortest path problem\\nautomation\\nconferences\\napproximation algorithms\\npath planning\\nrobots\\ngraph theory\\nmobile robots\\nassisted shortest path problem\\npath planning algorithm\\nsupport robot\\ncardinal robot\\ntravel time\",\"1321\":\"solid modeling\\nthree-dimensional displays\\nshape\\nrobot kinematics\\nmanuals\\nthree-dimensional printing\\ntrajectory\\nmanipulators\\nmobile robots\\nproduction engineering computing\\nrapid prototyping (industrial)\\nrobot vision\\ntrajectory control\\nvelocity control\\nvibrations\\nvisual servoing\\nprojector-guided nonholonomic mobile 3d printing\\nfused deposition modeling\\ngantry-based 3d printer\\nadditive manufacturing\\nmobile manipulator\\nlearning-based visual servoing\\nspeed control\\ntrajectory accuracy\\nsize 80.0 cm\\nsize 30.0 cm\",\"1322\":\"radiation effects\\ncosts\\nnavigation\\nsoftware algorithms\\nsoftware\\npath planning\\nindoor environment\\nmobile robots\\nrobot vision\\nstandard navigation software\\noptimal irradiation locations\\nuvc irradiation\\nautonomous integrated robotic system\\nindoor ultraviolet light disinfection\\nregular irradiation\\nultraviolet c light\\ncovid-19\\npurpose-made inexpensive robotic platform\",\"1323\":\"automation\\nconferences\\nplanning\\ntime factors\\noptimization\\napproximation theory\\niterative methods\\nmobile robots\\nmotion control\\npath planning\\npiecewise linear techniques\\npiecewise-linear motion planning\\nmorphing obstacles\\nshortest length piecewise-linear motions\\nsemidefinite programs\\npath length\\nglobal moment optimization approach\\ncontinuous time constraints\\ntime discretization\\niterative motion planner\\nnonlinear optimization baselines\\nmoving obstacles\\nstatic obstacles\\nmotion and path planning\\nsemidefinite programming\\nconvex optimizaton\",\"1324\":\"industries\\nservice robots\\nmedical services\\nkinematics\\ntrajectory\\nsafety\\nmanufacturing\\nflexible manipulators\\nmanipulator dynamics\\nmanipulator kinematics\\nmulti-robot systems\\npath planning\\nredundant manipulators\\ncontinuum manipulators\\ngenerated trajectories\\ncontinuum arm\\nprevious path planning approaches\\nsmooth path planning\\nhyper-redundancy serves\\nrigid-link robots\\ninverse kinematics-based approach\\nco-robots\\nhuman healthcare\\nhyper-redundancy\\ndynamics constraints\",\"1325\":\"machine learning algorithms\\nautomation\\nheuristic algorithms\\nconferences\\nmachine learning\\npredictive models\\nprediction algorithms\\ncollision avoidance\\nmobile robots\\npath planning\\nrobot dynamics\\ncontinuum arms\\ndynamic environment\\nanticipatory path planning\\nhyper-redundancy\\ndynamic environments\\nobstacle prediction\",\"1326\":\"training\\nnavigation\\nrobot vision systems\\nsemantics\\ncameras\\nreal-time systems\\nsensors\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nprobability\\nrobot vision\\nsensed data\\nrobot setup\\nsingle camera\\nnavigation performance\\nraw maps\\ninpainted maps\\nbaseline approach\\nonline robot navigation\\nmobile robot navigation\\nindoor environments\\nfield-of-view\\nlearning-based approaches\\nautonomous navigation\\ndepth camera\\nlocal occupancy map\\ninpainting network\\noccupancy probabilities\\nunseen grid cells\\ndirect supervision\\nsize 0.5 m\",\"1327\":\"training\\nautomation\\nshape\\nconferences\\ncomputational modeling\\npredictive models\\ndata models\\nimage motion analysis\\ninference mechanisms\\nmobile robots\\nmotion control\\npath planning\\nroad safety\\nroad vehicles\\nrobot vision\\nsupervised learning\\ntraffic engineering computing\\ntrajectory control\\nellipse loss\\nscene-compliant motion prediction\\nautonomous vehicle surroundings\\nsafe operations\\nscene compliance\\noff-road predictions\\ndifferentiable trajectory rasterizer module\\ntrajectory prediction\\njoint detection-prediction model\\nactor orientation\\nactor dimension\\nself-driving technology\\ntraffic actor future behavior inference\\nautonomous driving\",\"1328\":\"runtime\\ncosts\\nmonte carlo methods\\nreal-time systems\\nbayes methods\\ntrajectory\\nnoise measurement\\nmarkov processes\\nmobile robots\\npath planning\\nprobability\\ntemporal logic\\nlogic-based bayesian intent inference\\npredictive runtime monitoring framework\\nfuture position\\nimpending property violations\\ntemporal logic formulas\\ntemporal logic-based optimal cost path\\npossible intents\\ntemporal logic formulae\\nbayesian approach\\nposterior predictive samples\",\"1329\":\"measurement\\nsolid modeling\\nthree-dimensional displays\\nannotations\\nroads\\nurban areas\\npredictive models\\ngraph theory\\nlearning (artificial intelligence)\\nobject detection\\npedestrians\\nroad safety\\ntraffic engineering computing\\ngraph-sim\\ngraph-based spatiotemporal interaction modelling\\npedestrian action prediction\\ncrucial yet challenging tasks\\nautonomous vehicles\\nurban environments\\nfuture behaviour\\nnearby pedestrians\\npredicting behaviour\\nsocial factors\\nenvironmental factors\\nthree-dimensional space\\ncurrent pedestrian behaviour benchmark datasets\\nnovel graph-based model\\npedestrian crossing action\\nmethod models pedestrians\\nnearby road users\\nclustering\\nrelative importance weighting\\nbird\\npedestrian behavioural annotations\\nexisting nuscenes dataset\",\"1330\":\"uncertainty\\nthree-dimensional displays\\nlaser radar\\ncosts\\nnavigation\\nfuses\\nconvolution\\ndeep learning (artificial intelligence)\\nmobile robots\\noptical radar\\noptimisation\\nradionavigation\\ntraffic engineering computing\\nlidar sensors\\nend-to-end driving solutions\\nmemory footprint\\nsampling-based methods\\nrobust lidar-based end-to-end navigation framework\\ndeep learning\\nend-to-end neural network\\nautonomous vehicle control\\nraw sensory input\\nhybrid evidential fusion\\nfast-lidarnet\\nsparse convolution kernel optimization\\nhardware-aware model design\",\"1331\":\"simulation\\nscalability\\ngaussian processes\\ncameras\\nrobot sensing systems\\nunmanned aerial vehicles\\nsensors\\nautonomous aerial vehicles\\nenvironmental monitoring (geophysics)\\nregression analysis\\nrobot vision\\nenvironmental hotspot identification\\nuav equipped\\ndownward-facing camera\\nenvironmental monitoring tasks\\nglobal maxima\\nunmanned aerial vehicle\\ntime budget\\nmultifidelity variant\\nbandit problem\\nmultiarmed bandit settings\\ngp regression\\nuav sensing locations\\ngaussian process\",\"1332\":\"training\\nadaptation models\\nannotations\\ncomputational modeling\\nconferences\\nvideo sequences\\npipelines\\ncomputer vision\\nimage sequences\\nlearning (artificial intelligence)\\nobject detection\\nvideo signal processing\\nexisting object-centric video prediction pipelines\\ndense object annotations\\nobject-centric prediction\\nobject-centric video prediction method\\nstacked objects\\nend-to-end video prediction training\\npixel-to-pixel video prediction\\nlearned dynamics\",\"1333\":\"training\\nautomation\\nconferences\\nreinforcement learning\\ntask analysis\\noptimization\\nrobots\\ngeneralisation (artificial intelligence)\\nmanipulators\\ngeneralization ability\\nsample efficiency\\nunstable training\\nsoft data augmentation\\nsoda\\npolicy learning\\nsoft constraint\\nnonaugmented data\\nrl optimization\\nvision-based rl methods\\ndeepmind control suite\",\"1334\":\"training\\ndeformable models\\nimage segmentation\\nimage registration\\nthree-dimensional displays\\nestimation\\ntask analysis\\ncomputerised tomography\\nlearning (artificial intelligence)\\nmean square error methods\\nmedical image processing\\nmedical robotics\\ntest-time training\\nmean square error\\ntissue dense tracking tasks\\ndeformable multiscale image registration\\ndownstream tasks\\nmotion analysis\\nintra-operative tracking\\npopular registration methods\\nsequential images\\ncomplex deformations\\ndeep learning-based registration approaches\\ndeep deformable image registration\\nconventional learning-based registration model\\nmultiscale deep networks\\nresidual deformations\\nhigh variational deformations\\nmultiscale deep registration\\ncomputer vision for medical robotics\\nvisual tracking\",\"1335\":\"psnr\\nfluctuations\\nconferences\\nstability criteria\\nrobustness\\nreal-time systems\\ncovariance matrices\\ncomputer vision\\ndata structures\\nexpectation-maximisation algorithm\\ngaussian processes\\nimage reconstruction\\nimage representation\\nmixture models\\nfast-converging hgmm point clouds\\ndiscriminative data representations\\npoint cloud data\\ncomputer vision applications\\nhierarchical gaussian mixture model\\nreal-time execution\\nrobust initialization criterion\\nlow discriminative clustering capability\\noptimal initialization scheme\\nrandom initialization\\nhgmm point cloud reconstruction\\ndifferent initialization methods\\nreconstruction quality\\nclustering-based initialization methods\\nad-hoc initializations\\nfcm\\nfuzzy c-means\\nexpectation-maximization optimization\",\"1336\":\"location awareness\\ndeep learning\\nuncertainty\\nnavigation\\nconferences\\nrobustness\\ncomputational efficiency\\ncollision avoidance\\nmobile robots\\nneural nets\\nnoise\\npendulums\\nrecursive filters\\nstate estimation\\nstochastic processes\\nof-distribution robustness\\ndeep recursive filters\\naccurate state\\nuncertainty estimation\\nself driving vehicles\\nsafe navigation\\npedestrian rich environments\\nrobot navigation\\nout-of-distribution noise\\nstate estimation decouple perception\\nnoisy data\\nhigh dimensional data\\ndeep neural networks\\nsimple noisy pendulum state estimation problem\",\"1337\":\"training\\nconferences\\npose estimation\\npipelines\\nfocusing\\ngrasping\\nplanning\\nlearning (artificial intelligence)\\nobject recognition\\npath planning\\nrobot vision\\nzero-shot pose hypothesis\\nrobot manipulation pipelines\\nannotated training sets\\nlong training times\\nzero-shot object\\nhypothesis generation\\nscoring framework\\nzero-shot generalization\\ntextured objects\\nuntextured objects\",\"1338\":\"uncertainty\\nthree-dimensional displays\\nheuristic algorithms\\nsemantics\\ncolor\\nrobot sensing systems\\nmanipulators\\ngeometry\\ngraph colouring\\nmobile robots\\noptimisation\\npath planning\\ngeneralized a* algorithm\\nweighted colored graph\\nsemantic information\\nsearch space\\ngeometric information\\nvertex color\\nshortest path\\nternary graph\\n3d robotic arm\\n5d robotic arm\\nedge color\\n2d mobile robot\\npath optimality\\nclass-ordered a* algorithm\",\"1339\":\"analytical models\\ntorque\\nautomation\\nmagnetoelasticity\\nconferences\\nswitches\\nmanipulators\\nelasticity\\nmagnetic actuators\\nmechanical stability\\nmedical robotics\\nmicroactuators\\nsprings (mechanical)\\ntorsion\\nminiature magnetic robots\\nelastic springs\\nmillimeter-scale magnetically-actuated robots\\ntorsion spring-like behavior\\nsimultaneous magnetic actuation\\nmillimeter-scale magnetically-actuated mechanisms\\nnonlinear bistable smart springs\\ntailored magnetic torsion springs\\nmagnetic robot manipulator\\nrestoring torque spring design\\ntorque-displacement responses\",\"1340\":\"costs\\nmicromanipulators\\nmicroscopy\\nsurgery\\nmanuals\\nkinematics\\nglass\\ncalibration\\ncellular biophysics\\ncontrol system synthesis\\nend effectors\\nmanipulator kinematics\\noptimisation\\nsurgical robots\\nrobotic micropipette alignment\\nautomated end-effector alignment\\nrobotic cell manipulation\\nfragile end-effector\\nclinical applications\\nbiomedical applications\\nmisalignment error\\nglass micropipette\\nfield of view\\nrotational degree of freedom\\nmicromanipulator\\ntranslational degrees of freedom\\nquadratic optimization\\nkinematics modeling\\ncontroller design\\nparameter optimization\\nrotation-induced translation\\nhuman sperm immobilization\\nclinical cell surgery\",\"1341\":\"total harmonic distortion\\ninsects\\nwires\\nmodulation\\npiezoelectric actuators\\nhigh-voltage techniques\\npower electronics\\naerodynamics\\naerospace components\\naerospace robotics\\nforce control\\nharmonic distortion\\nmobile robots\\npower convertors\\ntorque control\\nvoltage control\\nhigh-voltage power electronics unit\\nwing thrust\\nflapping-wing insect-scale robots\\nscaling laws\\nmaterials cost reduction\\npower requirements\\nboost converter\\noutput storage capacitor\\noutput voltages\\nthrust modulation\\non-board power systems\\nflying insect robots\\noscillating signal\\nforces\\ntorques\\ndriver load\\nsinusoidal wing flapping signals\\nmass 90.0 mg\\nmass 15.0 mg\\nvoltage 240.0 v\\nvoltage 160 v to 220 v\\nvoltage 40 v to 200 v\",\"1342\":\"legged locomotion\\nanalytical models\\nlimiting\\ntrajectory tracking\\nreinforcement learning\\ndata collection\\nheat-assisted magnetic recording\\nmicrorobots\\ncompliant materials\\ntraditional model-based controllers\\nsimulated models\\ncurrent model-based\\nsim-to-real transfer methods\\nframework residual model\\nrml\\napproximate models\\naccurate robot model\\nharvard ambulatory microrobot\\npassively collected interaction data\\nmodel-free reinforcement learning algorithms\\nresidual model learning\\nmicrorobot control\",\"1343\":\"micromechanical devices\\nieee 802.15 standard\\nwireless communication\\nwireless sensor networks\\nradiation effects\\nphotovoltaic cells\\nmicroprocessors\\ncmos integrated circuits\\ngrippers\\nlow-power electronics\\nmicroprocessor chips\\nmicrorobots\\nmicrosensors\\nradio transceivers\\nsilicon-on-insulator\\nsolar cell arrays\\nsolar-powered wireless mems gripper\\nsolar-powered actuation\\nieee 802.15.4 rf signals\\nelectrostatic inchworm motors\\narm cortex-m0 microprocessor\\nsolar cells\\nsmall autonomous robot actuator\\nelectrostatic mems gripper\\nhigh voltage buffers\\nmultioutput array\\ncmos soi chip\\nvoltage 119.0 v\",\"1344\":\"automation\\nnavigation\\nmicrocontrollers\\nconferences\\nrobot learning\\ninference algorithms\\ntask analysis\\naircraft control\\nautonomous aerial vehicles\\ncollision avoidance\\ncontrol engineering computing\\nfinite state machines\\nhelicopters\\nlearning (artificial intelligence)\\nmicrorobots\\nmobile robots\\nmulti-robot systems\\npath planning\\ntrajectory control\\nend-to-end application-specific system design\\ncompetitive learning-based navigation approach\\ncluttered test environments\\nrandomized test environments\\nsource seeking\\nto-end tiny robot\\ntiny robot learning\\nfully autonomous source\\nhighly constrained nanoquadcopter\\nobservation feature design\\ndeep-rl policy\\ndeep-rl algorithm\\nhigh-performance solution\\nhigh noise levels\\ncheap cortex-m4 microcontroller unit\\nubiquitous cortex-m4 microcontroller unit\\nmotion and path planning\\naerial systems: applications\\nreinforcement learning\",\"1345\":\"temperature sensors\\nmechanical sensors\\ntemperature\\ntransportation\\ngrasping\\nsoft robotics\\nobject recognition\\ndexterous manipulators\\nelastomers\\ngrippers\\nhydrogels\\nlearning (artificial intelligence)\\ntactile sensors\\nsoft robotic gripper\\nhydrogel-based sensors\\nlearning-based object recognition\\nhigh structural compliance\\nirregular shapes\\nsoft sensors\\nmechanical properties\\nelastomer materials\\nthree-finger soft gripper\\nanti-freezing property\\ntactile sensing\\ndeep-learning model\\nsensory soft gripper\\nobject grasping\\nfreezing temperatures\\nhigh recognition accuracy\",\"1346\":\"recurrent neural networks\\nuncertainty\\nestimation\\nsoft robotics\\nbending\\nrobot sensing systems\\nskin\\nadaptive control\\nclosed loop systems\\ncontrol engineering computing\\npiezoresistive devices\\nrecurrent neural nets\\nrobots\\ntracking\\nsingle-segment soft robot\\nintegrated soft sensing skin\\nadaptive tracking control\\nintegrated sensing skins\\nrecurrent neural network\\ncurvature tracking control\\nstretchable sensing skin\\nreliable proprioception\\nspray-coated piezoresistive sensing layer\\nlatex membrane\\nbidirectional bending\",\"1347\":\"printing\\nneuromorphics\\nneurons\\ncomputer architecture\\nartificial neural networks\\nrobot sensing systems\\nrapid prototyping\\ncontrol engineering computing\\nembedded systems\\nmemristors\\nmobile robots\\nneural nets\\nneurophysiology\\noptimisation\\nthree-dimensional printing\\nconventional silicon components\\nann\\nartificial neural network\\n3-d printing\\nminimal approximating set\\ntransistor plus memristors\\nform + function 4-d printing\\nprintable organic neurons\\noptimized neurons\\nembedded neuromorphic architecture\\npresented soft robotic skin\\nmultifunctional robotic materials\\nneuromorphic computer\\noptimization technique\\nsynaptic weights\\nprintable analog neural network\\narchitecture prioritizes simplicity\",\"1348\":\"training\\ndeep learning\\nautomation\\nconferences\\nestimation\\nfeature extraction\\ndeep learning (artificial intelligence)\\nneural nets\\nstereo image processing\\nefficient online adaptation\\ndeep stereo depth estimation\\ndeep neural networks\\nself-supervised online adaptation\\nunsolved challenges\\ndownstream systems that depth predictions\\nfeature similarity scores\\ndeep stereo network\\nout-of-distribution detection\\nnecessary starting criterion\\nonline validation\\ncontinuous adaptation cause catastrophic forgetting\\ntraining domain\\naugmenting adaptation\\nrobust system\\nefficient deep stereo system\",\"1349\":\"solid modeling\\nthree-dimensional displays\\nsemantics\\nlayout\\nvirtual environments\\nmanuals\\nrobot sensing systems\\ncad\\ncontrol engineering computing\\ndata visualisation\\ngraph theory\\nimage colour analysis\\nimage reconstruction\\nimage representation\\ninteractive systems\\nmobile robots\\nobject detection\\nrobot vision\\nsolid modelling\\nvirtual reality\\ncad model\\ninteractive 3d scenes\\nactionable interactions\\nbest-fitted cad models\\nfiner-grained robot interactions\\npart-based articulated cad models\\ndense panoptic map\\nobject meshes\\ngraph-based scene representation\\n3d volumetric panoptic mapping module\\ninteractive scene\\nreconstructed scene\\nreconstruction accuracy\\nembodied agent\\nscene reconstruction\",\"1350\":\"solid modeling\\nthree-dimensional displays\\nrobot kinematics\\nconferences\\nreinforcement learning\\nrobot sensing systems\\nmanipulators\\ncad\\ncomputational geometry\\ncontrol engineering computing\\nimage reconstruction\\nimage registration\\nrobot vision\\nstereo image processing\\n3d point clouds\\ntopological features\\nnoisy 3d sensor\\nrobotic sensor\\ncustom robotic manipulator\\npoint cloud data streaming\\ntopology-based information gain metric\\ncad design\",\"1351\":\"visualization\\ntensors\\nsystematics\\nsimultaneous localization and mapping\\nsemantics\\ngraphics processing units\\nsensors\\ncameras\\nfeature extraction\\nhilbert spaces\\nimage colour analysis\\nimage registration\\nstereo image processing\\nsemantic point clouds\\nrgb-d cameras\\nnonparametric rigid point cloud registration framework\\nsemantic labels\\nalignment process\\ndata association\\nnonparametric functions\\nreproducible kernel hilbert space\\nalignment problem\\nweighted kernels\\nlocal geometric features\\nsemantic features\\nrigid body transformation group\\npoint cloud alignment metric\\nsemantic information\\nframe-to-frame registration methods\\nsemantic continuous visual odometry\\nstereo cameras\",\"1352\":\"legged locomotion\\nthree-dimensional displays\\nautomation\\nconferences\\nsoft robotics\\nreal-time systems\\nhardware\\ngears\\nmotion control\\nrobot dynamics\\nstability\\nmultiterrain environments\\nsoft legs\\ngeared servo motors\\nopen-source untethered quadrupedal soft robot platform\\ndynamic locomotion\\nagile soft robots\\nuntethered legged soft robots\\nreal-time soft body simulation\\nvelocity 0.9 m\\/s\",\"1353\":\"tracking\\nsimulation\\nforce\\nsoft robotics\\nobservers\\nrobot sensing systems\\nreliability engineering\\nactuators\\ncontrol system synthesis\\ndifferential equations\\nnonlinear control systems\\nposition control\\nvariable structure systems\\nassumed contact forces\\ncontact forces estimation\\nfabric-reinforced inflatable soft robot\\nsoft bodies\\ncontact force\\nfree robot\\nexternal contact forces\\nnonlinear robot\",\"1354\":\"transducers\\nchirp\\nsoft robotics\\nrobot sensing systems\\nhardware\\nskin\\nsynchronization\\nacoustic communication (telecommunication)\\nelasticity\\nfrequency response\\nmulti-robot systems\\nrobots\\nhigh extension ratio elastic bodies\\nacoustic signals\\nelastic robot skins\\ndirectional contact-based communication\\naudible-range communication\\nexteroceptive sensing\\ninflatable modular soft robots\\nmultirobot hardware\",\"1355\":\"location awareness\\nvisualization\\nsimultaneous localization and mapping\\nnavigation\\ndatabases\\nsemantics\\nreal-time systems\\nfeature extraction\\ngraph theory\\nmobile robots\\npath planning\\nquadratic programming\\nrobot vision\\nsensor fusion\\nslam (robots)\\nsemantic slam\\nautonomous object-level data association\\nmap semantic information\\nvisual features\\nhigh-level tasks\\nsemantic-level data association\\nsemantic objects\\nnovel object-level data association algorithm\\nquadratic-programming-based semantic object initialization scheme\\nintegrated semantic-level slam system\\nhigh-accuracy object-level data association\\nreal-time semantic mapping\\nonline semantic map building\\nsemantic-level localization capabilities\\nsemantic-level mapping\\ntask planning\",\"1356\":\"solid modeling\\nthree-dimensional displays\\nsimultaneous localization and mapping\\nprotocols\\ncomputational modeling\\ndata models\\ntrajectory\\ngraph theory\\nmobile robots\\nmulti-robot systems\\noptimisation\\npath planning\\npose estimation\\nrobot vision\\nslam (robots)\\nlocal mesh\\ndistributed place recognition\\nlocal trajectory estimate\\ninter-robot loop closures\\ntrajectory estimate\\nmesh deformation\\n3d metric-semantic meshes\\ndistributed multirobot metric-semantic simultaneous localization\\nsemantic label\\nkimera-multi\\ndistributed slam backends\\nrobust pose graph optimization protocol\\nincremental maximum clique outlier rejection\",\"1357\":\"visualization\\nthree-dimensional displays\\nnavigation\\nmessage passing\\nsemantics\\nneural networks\\ncontainers\\ncomputational linguistics\\ngeometry\\ngraph theory\\nimage representation\\nmobile robots\\nmulti-agent systems\\nnatural language processing\\nneural net architecture\\nrobot vision\\nvectors\\nsemantically related objects\\nneural message passing\\nhierarchical mechanical search\\nindoor organized environments\\ntarget object\\n3d scene graph representation\\nsearch process\\nvisual information\\nlinguistic information\\nsemantic cues\\ngeometric cues\\ngeometric modeling\\nsemantic modeling\\nobject searching\\nhms\\nagent actions\\nnatural language description\\nneural network architecture\\nstorage locations\\noracle policy\",\"1358\":\"training\\nvisualization\\nautomation\\nconferences\\ngrasping\\nmanipulators\\nrobot learning\\nimage representation\\nlearning (artificial intelligence)\\nneural net architecture\\nrobot programming\\nrobot vision\\nsearch problems\\ngrasping performance\\nattention connectivity search\\nneural architecture search\\nrobot experiments\\ntask success rates\\nimage feature representations\\nhigh dimensional visual inputs\\nlow dimension action inputs\\nrobot manipulation learning\\nvision-based architecture search algorithm\\nvision architecture discovery\\nreal-robot task\",\"1359\":\"training\\nlearning systems\\nthree-dimensional displays\\ngames\\npredictive models\\nbiology\\nintelligent agents\\ncomputer games\\nlearning (artificial intelligence)\\npredictive control\\nzero-shot policy\\nspatial temporal reward decomposition\\ncontingency-aware observation\\nintelligent agent\\nunseen environment\\ndata collection\\nfinetuning\\nzero shot generalization problem setup\\nbiological intelligent agents\\nprevious experiences\\ntraining environment\\ntask description\\ntrajectory-level sparse rewards\\ntesting environment\\nsetting natural\\nbiological creatures\\nstate-of-art rl\\nzero-shot learning methods\\nsparse reward\\nstep reward\\ndecomposed rewards\\ndynamics model\\nfiner-granularity observations\\nrobotic continuous control task\",\"1360\":\"training\\nvisualization\\nnavigation\\nreinforcement learning\\nstreaming media\\ncost function\\nfeature extraction\\ndeep learning (artificial intelligence)\\nmobile robots\\npath planning\\npredictive control\\nrobot vision\\nreal-time control\\ncostmap generators\\nvision-based autonomous driving\\napproximate inverse reinforcement learning\\nvision-based imitation learning\\nimplicit objective function\\nvision-based navigation\\nmodel predictive control\\nmpc\\ndeep neural networks\\napproximate cost function generator\\nvisual navigation challenge\",\"1361\":\"neuromuscular\\nperturbation methods\\nexoskeletons\\nshoulder\\nmanipulators\\nshock absorbers\\nimpedance\\nactuators\\nbiomechanics\\nbiomedical measurement\\nbone\\ndamping\\nmedical robotics\\nmuscle\\nsprings (mechanical)\\nneuromuscular properties\\nhuman shoulder joint\\nshoulder mechanical impedance\\ndamper\\n4-bar spherical parallel manipulator\\n4b-spm\\ninherently low inertia\\nphysical shoulder mockup\\nadjustable spring\\nmass properties\\nmockup test\\nmockup properties\\nmultidimensional human shoulder impedance\\nhuman shoulder stiffness\\nparallel-actuated shoulder exoskeleton robot\\nsize 30.9 nm\",\"1362\":\"upper bound\\nheuristic algorithms\\nconferences\\nprobabilistic logic\\nplanning\\nsafety\\nvehicle dynamics\\ncollision avoidance\\nmobile robots\\noptimal control\\npath planning\\npredictive control\\nprobability\\nroad vehicles\\nsafe nonconservative planning\\nprobabilistic dynamic environments\\nagents whose future actions\\nefficient planning algorithms\\nguaranteed bounds\\nsafety violation\\nnonconservative performance\\nsystem\\nnatural criterion called interval risk\\nirbs\\nnovel receding horizon algorithm\\ndesired irb\\ndynamic risk budget\\nallowable risk\\nguarantees recursive feasibility\\nsafe set\\ncontingency plan\",\"1363\":\"navigation\\nheuristic algorithms\\ndynamics\\nneural networks\\nprobabilistic logic\\nprediction algorithms\\nplanning\\ncollision avoidance\\ngraph theory\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\navoidance critical probabilistic roadmaps\\ndynamic environments\\ndynamic obstacles\\nsampling-based motion planning algorithms\\nrobot\\npathfinding algorithm\\ndynamic environment navigation\\nsparse probabilistic roadmaps\\nlocal environment features\\nhierarchical probabilistic roadmaps\\nmotion structures\\nefficient obstacle avoidance\\nplanning space\\nroadmap\",\"1364\":\"concurrent computing\\nautomation\\nconferences\\ncolor\\ninterference\\nparallel processing\\ntask analysis\\nmobile robots\\noptimisation\\npath planning\\nacceleration meta-algorithm\\nrobot paths\\nconcurrent interleaved sub-epoch pods\\nexisting path optimization algorithms\\nwaypoint path\\nconsecutive groupings\\nadjacent pods\\nopposite color\\nconcurrent computations\\npath splitting algorithm\\nblue pod groupings\\nred pod groupings\\nsimulated path optimization scenarios\\noptimization tasks\\nconcurrency\\noptimization quality\",\"1365\":\"legged locomotion\\nrobot motion\\ncouplings\\nconferences\\nforce\\nprogramming\\nplanning\\nconvex programming\\nevolutionary computation\\nlearning (artificial intelligence)\\nnonlinear systems\\npath planning\\nproblem-specific parameters\\ndata-driven mccormick envelope relaxations\\nmotion planning\\nmultilimbed robots\\nforce constraints\\nnonlinearity\\nmultistage optimization\\ndata-driven inter-stage coupling constraints\\nclustering\\nmultistage coupled convex programming\\nwalking tasks\\nclimbing tasks\\nhexapod robot\",\"1366\":\"image segmentation\\ntechnological innovation\\nthree-dimensional displays\\nuncertainty\\ntracking\\nshape\\nmotion segmentation\\nimage colour analysis\\nobject tracking\\ntrees (mathematics)\\nrgbd tracking\\nmultihypothesis volumetric segmentation\\n3d segmentation methods\\nmst\\nscene ambiguity\\nsample possible segmentations\\nfusing tracking results\\nmultiple segmentation estimates\\nsegmentation tree sampling\\nmultihypothesis segmentation tracking\\ncluttered tabletop environments\",\"1367\":\"image segmentation\\nimage resolution\\ncodes\\nautomation\\nimage edge detection\\nconferences\\nredundancy\\ndistributed processing\\nneural nets\\nobject detection\\nvideo signal processing\\nyolactedge\\nreal-time instance segmentation\\nedge devices\\nreal-time speed\\njetson agx xavier\\nresnet-101 backbone\\nyolact\\nbox detection\\ntensorrt optimization\\nfeature warping module\\ntemporal redundancy\\nyoutube vis\\nms coco\\nmask detection\",\"1368\":\"training\\nconvolution\\nconferences\\nclustering methods\\nsemantics\\nneural networks\\nestimation\\nconvolutional neural nets\\nimage classification\\nimage segmentation\\nlearning (artificial intelligence)\\nobject detection\\ninstance segmentation networks\\nobject detectors\\ninstance clustering methods\\ninstance contours\\nsemantic segmentation yield\\nboundary aware semantic segmentation\\noutput panoptic segmentation\\ncityscapes dataset\",\"1369\":\"systematics\\ntracking\\nmotion segmentation\\nrobot vision systems\\nvelocity control\\npipelines\\ncameras\\nimage motion analysis\\nimage resolution\\nimage restoration\\nimage segmentation\\nmotion blurring\\nmonocular multimotion segmentation\\nmotion propagation\\ncluster keyslices\\naverage detection rate\\n0-mms\\nzero-shot multimotion segmentation\\nmonocular event camera\\nnavigation tasks\\nmod datasets\\need datasets\\nev-imo datasets\\nmod++ dataset\",\"1370\":\"adaptation models\\nautomation\\nconferences\\nrobot sensing systems\\nsensors\\nmulti-robot systems\\niterative methods\\nmobile robots\\nteam composition changes\\nheterogeneous multirobot sensor coverage\\nsparsity-inducing terms\\nrobot sensing quality\\nmultirobot team member\",\"1371\":\"target tracking\\npower measurement\\nsimulation\\nrobot sensing systems\\ngenerators\\nsensors\\nreliability\\ndistributed control\\nmobile radio\\nwireless sensor networks\\ncurrent usage rates\\ndistributed multitarget tracking\\nheterogeneous mobile sensing networks\\nnormalized unused sensing capacity\\ntheoretical maximum\\nentirely local information\\narbitrary sensor models\\ndistributed coverage control strategy\\nheterogeneous sensors\\ncurrent unused capacity\\nteam member\\nmultitarget tracking scenario\",\"1372\":\"automation\\nsystems operation\\nconferences\\nkinematics\\nrobot sensing systems\\nunmanned aerial vehicles\\nsensors\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\nmobile robots\\nremotely operated vehicles\\nlimited range sensing constraints\\nkinematic constraints\\nlimited sensing\\nfixed wing unmanned aerial vehicles\\nsafe distances\\nsafe system operation\\nsensing range\\nexisting barrier function\\nnewly constructed barrier function\\n20 fixed wing aircraft\",\"1373\":\"fuzzy logic\\nadaptive systems\\nheuristic algorithms\\nreinforcement learning\\ncontrol systems\\ninference algorithms\\nrobustness\\nadaptive control\\naircraft control\\ncollision avoidance\\nfeedback\\nfuzzy control\\nfuzzy reasoning\\nlearning (artificial intelligence)\\nmulti-robot systems\\nrobust control\\nequilibrium conditions\\nuncertain dynamic environments\\npre-tuned fuzzy inference architectures\\nunmodeled conditions\\nadaptive distributed technique\\nautonomous control\\nflock systems\\nrelatively flexible structure\\nonline fuzzy reinforcement learning schemes\\nflock velocity consensus\\ndynamic disturbances\\nfeedback signal\\nsimilar technique\\nadaptive fuzzy reinforcement\\nflock-guidance problem\\nchallenging structure\\nmultiple optimization objectives\\ndifferent control approaches\\nguidance schemes\\ncomplex tracking-error dynamics\\nlinear feedback strategies\",\"1374\":\"uncertainty\\nautomation\\nconferences\\nprobability distribution\\nland vehicles\\npartitioning algorithms\\nrobots\\nmobile robots\\nmulti-robot systems\\nprobability\\nsequential estimation\\nstochastic processes\\ndeployment algorithm\\noptimal sequential stochastic deployment\\nmarsupial robot system\\ncarrier robot\\npassenger robot deployment\\ndeployment decision points\\ndeployment exploration experiments\\nmultiple passenger robots\\nsequential stochastic assignment problem\\nssap\",\"1375\":\"greedy algorithms\\ncosts\\nconferences\\nmaintenance engineering\\ndynamic scheduling\\nnumerical models\\nresource management\\ndistributed control\\nmobile robots\\nmulti-robot systems\\noptimal control\\ndynamic task allocation\\nconnectivity maintenance\\nheterogeneous robot team\\nsub-optimal robot controllers\\nonline connectivity-aware dynamic deployment\\nheterogeneous multirobot systems\\nnetworked robots\\nconnectivity-aware multirobot redistribution\",\"1376\":\"simulation\\nconferences\\nmultitasking\\ncognition\\ninference algorithms\\nmulti-robot systems\\nobject recognition\\nautonomous aerial vehicles\\nmultitasking robots\\nmultirobot tasks\\ntask synergies\\nphysical constraints\\nmultirobot systems\\nrealistic uav simulator\",\"1377\":\"automation\\nconferences\\ncollaboration\\nmaintenance engineering\\ntopology\\nmulti-robot systems\\ntask analysis\\ngraph theory\\ndistributed topology correction\\nconnectivity maintenance\\nmultirobot systems\\ntopology correction controller\\ntask behaviors\\nflexible connectivity graph topology\\ncluttered environments\",\"1378\":\"automation\\nconferences\\ngaussian processes\\nprediction algorithms\\ntopology\\ncomputational efficiency\\nmulti-robot systems\\napproximation theory\\ncontrol engineering computing\\ndistributed algorithms\\niterative methods\\nnearest neighbour methods\\nmultirobot systems\\ndecentralized approximate algorithms\\niterative method\\nconsensus methods\\ninter-robot communications\\ncovariance-based nearest neighbor robot selection strategy\\ndecentralized nested gaussian processes\\nempirical evaluations\",\"1379\":\"analytical models\\nsimulation\\ncontrol systems\\ndata models\\ntrajectory\\nstate-space methods\\nnoise measurement\\nautoregressive moving average processes\\nautoregressive processes\\nclosed loop systems\\ncontrol system synthesis\\nfeedback\\nlinear quadratic gaussian control\\nlinear systems\\nmobile robots\\noptimal control\\noptimisation\\ntime-varying systems\\nnominal trajectory\\nlinear time-varying autoregressive-moving-average model\\noutput measurement data\\ninformation state\\ninput-output information\\nfeedback gain\\nspecific lqg problem\\nclosed-loop problem\\ndecoupled data-based control approach\\ncomplex models\\npartially-observed robotic systems\\ndata-based approach\\npartially-observed feedback\\nopen-loop optimization problem\\nrobotic path planning\\nlearning and control\\ndata-based control\\npartial-state feedback\",\"1380\":\"upper bound\\nautomation\\nconferences\\nfocusing\\ngames\\nrobot sensing systems\\nsensors\\ngame theory\\nmobile robots\\nnonlinear programming\\npath planning\\nprobability\\nremotely operated vehicles\\ntrees (mathematics)\\ncircular target region\\ntarget defense problem\\npursuit-evasion game\\noriginal target guarding problem\\ngame phases\\nasymmetric information\\nengagement phase\\nparticular parameter regime\\nsimple defender strategy\\nsubsequent phase\\npartial information target defense game\\nautonomous defender\",\"1381\":\"automation\\nconferences\\nmemory management\\nreal-time systems\\ntrajectory\\nplanning\\nperformance analysis\\ncollision avoidance\\nmobile robots\\npath planning\\nvehicle dynamics\\nhierarchical motion planning approach\\nreal-time parking plans\\nautonomous vehicles\\nhigh-level route planner\\ncollision-free routes\\ntraffic\\nobstacle information\\nlow level motion planner\\nsmooth trajectories\\nreasonable parking\\nlow memory consumption\\nhierarchical approach\\nnewly detected obstacles\\noffline map\\noriginal planned trajectory\\nfast clearance checking procedure\\nrapid trajectory repairing\\nreal-time replanning\\nparking tasks\\ntrajectory quality\\nplanning time\\nlong-horizon motion planning\\nautonomous vehicle parking incorporating incomplete map information\",\"1382\":\"uncertainty\\nrobot kinematics\\ndynamics\\ngames\\npredictive models\\nmarkov processes\\nprediction algorithms\\ncollision avoidance\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\nstochastic games\\ntowards safe motion planning\\nhuman workspaces\\nrobust multiagent approach\\nagile performance\\nhuman motions\\nmarkov decision processes\\nrobot planning problems\\nsingle-agent formulation\\nhuman reaction\\nevaluating robot actions\\nunsafe motions\\nnearby humans\\nmodel robot planning\\nrobust planning algorithm\\nhuman responses\",\"1383\":\"fault tolerance\\nautomation\\nconferences\\nfault tolerant systems\\nrouting\\nrobustness\\nplanning\\nfault tolerant control\\nmobile robots\\nmulti-robot systems\\noptimisation\\nsearch problems\\nanytime fault-tolerant adaptive routing\\nmultirobot teams\\ncorrelated team orienteering problem\\ncollected rewards\\nstatic instances\\nctop instance\\nvehicle budget\\nanytime heuristic\",\"1384\":\"measurement\\ntracking\\nheuristic algorithms\\ndecision making\\npredictive models\\nnearest neighbor methods\\nprediction algorithms\\ncollision avoidance\\nhelicopters\\nsampling methods\\nmotion primitives\\ncollision-inclusive trajectories\\nsampling-based multicopter motion planning\\ncollision-resilient designs\\nrrt* algorithm\",\"1385\":\"costs\\nautomation\\nconferences\\ndynamics\\nplanning\\ntrajectory\\nnonlinear dynamical systems\\ncollision avoidance\\nmobile robots\\nmulti-robot systems\\npath planning\\nsampling methods\\ntrees (mathematics)\\ndifferential constraints\\nmultirobot motion-planning problem\\nunlabeled goals\\nn robots\\nm goals\\ndynamically-feasible trajectories\\nsampling-based motion\\ngoal assignment\\nmultiagent search\\ngoal-assignment layer\\nremaining goals\\nsampling-based expansion\\nmotion tree\\nmultiagent paths\\nmotion-tree expansion\\nmultirobot motion planning\\nmotion and path planning\\nnonholonomic motion planning\"},\"Benchmark Setup\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":-1,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1,\"1022\":-1,\"1023\":-1,\"1024\":-1,\"1025\":-1,\"1026\":-1,\"1027\":-1,\"1028\":-1,\"1029\":-1,\"1030\":-1,\"1031\":-1,\"1032\":-1,\"1033\":-1,\"1034\":-1,\"1035\":-1,\"1036\":-1,\"1037\":-1,\"1038\":-1,\"1039\":-1,\"1040\":-1,\"1041\":-1,\"1042\":-1,\"1043\":-1,\"1044\":-1,\"1045\":-1,\"1046\":-1,\"1047\":-1,\"1048\":-1,\"1049\":-1,\"1050\":-1,\"1051\":-1,\"1052\":-1,\"1053\":-1,\"1054\":-1,\"1055\":-1,\"1056\":-1,\"1057\":-1,\"1058\":-1,\"1059\":-1,\"1060\":-1,\"1061\":-1,\"1062\":-1,\"1063\":-1,\"1064\":-1,\"1065\":-1,\"1066\":-1,\"1067\":-1,\"1068\":-1,\"1069\":-1,\"1070\":-1,\"1071\":-1,\"1072\":-1,\"1073\":-1,\"1074\":-1,\"1075\":-1,\"1076\":-1,\"1077\":-1,\"1078\":-1,\"1079\":-1,\"1080\":-1,\"1081\":-1,\"1082\":-1,\"1083\":-1,\"1084\":-1,\"1085\":-1,\"1086\":-1,\"1087\":-1,\"1088\":-1,\"1089\":-1,\"1090\":-1,\"1091\":-1,\"1092\":-1,\"1093\":-1,\"1094\":-1,\"1095\":-1,\"1096\":-1,\"1097\":-1,\"1098\":-1,\"1099\":-1,\"1100\":-1,\"1101\":-1,\"1102\":-1,\"1103\":-1,\"1104\":-1,\"1105\":-1,\"1106\":-1,\"1107\":-1,\"1108\":-1,\"1109\":-1,\"1110\":-1,\"1111\":-1,\"1112\":-1,\"1113\":-1,\"1114\":-1,\"1115\":-1,\"1116\":-1,\"1117\":-1,\"1118\":-1,\"1119\":-1,\"1120\":-1,\"1121\":-1,\"1122\":-1,\"1123\":-1,\"1124\":-1,\"1125\":-1,\"1126\":-1,\"1127\":-1,\"1128\":-1,\"1129\":-1,\"1130\":-1,\"1131\":-1,\"1132\":-1,\"1133\":-1,\"1134\":-1,\"1135\":-1,\"1136\":-1,\"1137\":-1,\"1138\":-1,\"1139\":-1,\"1140\":-1,\"1141\":-1,\"1142\":-1,\"1143\":-1,\"1144\":-1,\"1145\":-1,\"1146\":-1,\"1147\":-1,\"1148\":-1,\"1149\":-1,\"1150\":-1,\"1151\":-1,\"1152\":-1,\"1153\":-1,\"1154\":-1,\"1155\":-1,\"1156\":-1,\"1157\":-1,\"1158\":-1,\"1159\":-1,\"1160\":-1,\"1161\":-1,\"1162\":-1,\"1163\":-1,\"1164\":-1,\"1165\":-1,\"1166\":-1,\"1167\":-1,\"1168\":-1,\"1169\":-1,\"1170\":-1,\"1171\":-1,\"1172\":-1,\"1173\":-1,\"1174\":-1,\"1175\":-1,\"1176\":-1,\"1177\":-1,\"1178\":-1,\"1179\":-1,\"1180\":-1,\"1181\":-1,\"1182\":-1,\"1183\":-1,\"1184\":-1,\"1185\":-1,\"1186\":-1,\"1187\":-1,\"1188\":-1,\"1189\":-1,\"1190\":-1,\"1191\":-1,\"1192\":-1,\"1193\":-1,\"1194\":-1,\"1195\":-1,\"1196\":-1,\"1197\":-1,\"1198\":-1,\"1199\":-1,\"1200\":-1,\"1201\":-1,\"1202\":-1,\"1203\":-1,\"1204\":-1,\"1205\":-1,\"1206\":-1,\"1207\":-1,\"1208\":-1,\"1209\":-1,\"1210\":-1,\"1211\":-1,\"1212\":-1,\"1213\":-1,\"1214\":-1,\"1215\":-1,\"1216\":-1,\"1217\":-1,\"1218\":-1,\"1219\":-1,\"1220\":-1,\"1221\":-1,\"1222\":-1,\"1223\":-1,\"1224\":-1,\"1225\":-1,\"1226\":-1,\"1227\":-1,\"1228\":-1,\"1229\":-1,\"1230\":-1,\"1231\":-1,\"1232\":-1,\"1233\":-1,\"1234\":-1,\"1235\":-1,\"1236\":-1,\"1237\":-1,\"1238\":-1,\"1239\":-1,\"1240\":-1,\"1241\":-1,\"1242\":-1,\"1243\":-1,\"1244\":-1,\"1245\":-1,\"1246\":-1,\"1247\":-1,\"1248\":-1,\"1249\":-1,\"1250\":-1,\"1251\":-1,\"1252\":-1,\"1253\":-1,\"1254\":-1,\"1255\":-1,\"1256\":-1,\"1257\":-1,\"1258\":-1,\"1259\":-1,\"1260\":-1,\"1261\":-1,\"1262\":-1,\"1263\":-1,\"1264\":-1,\"1265\":-1,\"1266\":-1,\"1267\":-1,\"1268\":-1,\"1269\":-1,\"1270\":-1,\"1271\":-1,\"1272\":-1,\"1273\":-1,\"1274\":-1,\"1275\":-1,\"1276\":-1,\"1277\":-1,\"1278\":-1,\"1279\":-1,\"1280\":-1,\"1281\":-1,\"1282\":-1,\"1283\":-1,\"1284\":-1,\"1285\":-1,\"1286\":-1,\"1287\":-1,\"1288\":-1,\"1289\":-1,\"1290\":-1,\"1291\":-1,\"1292\":-1,\"1293\":-1,\"1294\":-1,\"1295\":-1,\"1296\":-1,\"1297\":-1,\"1298\":-1,\"1299\":-1,\"1300\":-1,\"1301\":-1,\"1302\":-1,\"1303\":-1,\"1304\":-1,\"1305\":-1,\"1306\":-1,\"1307\":-1,\"1308\":-1,\"1309\":-1,\"1310\":-1,\"1311\":-1,\"1312\":-1,\"1313\":-1,\"1314\":-1,\"1315\":-1,\"1316\":-1,\"1317\":-1,\"1318\":-1,\"1319\":-1,\"1320\":-1,\"1321\":-1,\"1322\":-1,\"1323\":-1,\"1324\":-1,\"1325\":-1,\"1326\":-1,\"1327\":-1,\"1328\":-1,\"1329\":-1,\"1330\":-1,\"1331\":-1,\"1332\":-1,\"1333\":-1,\"1334\":-1,\"1335\":-1,\"1336\":-1,\"1337\":-1,\"1338\":-1,\"1339\":-1,\"1340\":-1,\"1341\":-1,\"1342\":-1,\"1343\":-1,\"1344\":-1,\"1345\":-1,\"1346\":-1,\"1347\":-1,\"1348\":-1,\"1349\":-1,\"1350\":-1,\"1351\":-1,\"1352\":-1,\"1353\":-1,\"1354\":-1,\"1355\":-1,\"1356\":-1,\"1357\":-1,\"1358\":-1,\"1359\":-1,\"1360\":-1,\"1361\":-1,\"1362\":-1,\"1363\":-1,\"1364\":-1,\"1365\":-1,\"1366\":-1,\"1367\":-1,\"1368\":-1,\"1369\":-1,\"1370\":-1,\"1371\":-1,\"1372\":-1,\"1373\":-1,\"1374\":-1,\"1375\":-1,\"1376\":-1,\"1377\":-1,\"1378\":-1,\"1379\":-1,\"1380\":-1,\"1381\":-1,\"1382\":-1,\"1383\":-1,\"1384\":-1,\"1385\":-1},\"Experimental Results\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":-1,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1,\"1022\":-1,\"1023\":-1,\"1024\":-1,\"1025\":-1,\"1026\":-1,\"1027\":-1,\"1028\":-1,\"1029\":-1,\"1030\":-1,\"1031\":-1,\"1032\":-1,\"1033\":-1,\"1034\":-1,\"1035\":-1,\"1036\":-1,\"1037\":-1,\"1038\":-1,\"1039\":-1,\"1040\":-1,\"1041\":-1,\"1042\":-1,\"1043\":-1,\"1044\":-1,\"1045\":-1,\"1046\":-1,\"1047\":-1,\"1048\":-1,\"1049\":-1,\"1050\":-1,\"1051\":-1,\"1052\":-1,\"1053\":-1,\"1054\":-1,\"1055\":-1,\"1056\":-1,\"1057\":-1,\"1058\":-1,\"1059\":-1,\"1060\":-1,\"1061\":-1,\"1062\":-1,\"1063\":-1,\"1064\":-1,\"1065\":-1,\"1066\":-1,\"1067\":-1,\"1068\":-1,\"1069\":-1,\"1070\":-1,\"1071\":-1,\"1072\":-1,\"1073\":-1,\"1074\":-1,\"1075\":-1,\"1076\":-1,\"1077\":-1,\"1078\":-1,\"1079\":-1,\"1080\":-1,\"1081\":-1,\"1082\":-1,\"1083\":-1,\"1084\":-1,\"1085\":-1,\"1086\":-1,\"1087\":-1,\"1088\":-1,\"1089\":-1,\"1090\":-1,\"1091\":-1,\"1092\":-1,\"1093\":-1,\"1094\":-1,\"1095\":-1,\"1096\":-1,\"1097\":-1,\"1098\":-1,\"1099\":-1,\"1100\":-1,\"1101\":-1,\"1102\":-1,\"1103\":-1,\"1104\":-1,\"1105\":-1,\"1106\":-1,\"1107\":-1,\"1108\":-1,\"1109\":-1,\"1110\":-1,\"1111\":-1,\"1112\":-1,\"1113\":-1,\"1114\":-1,\"1115\":-1,\"1116\":-1,\"1117\":-1,\"1118\":-1,\"1119\":-1,\"1120\":-1,\"1121\":-1,\"1122\":-1,\"1123\":-1,\"1124\":-1,\"1125\":-1,\"1126\":-1,\"1127\":-1,\"1128\":-1,\"1129\":-1,\"1130\":-1,\"1131\":-1,\"1132\":-1,\"1133\":-1,\"1134\":-1,\"1135\":-1,\"1136\":-1,\"1137\":-1,\"1138\":-1,\"1139\":-1,\"1140\":-1,\"1141\":-1,\"1142\":-1,\"1143\":-1,\"1144\":-1,\"1145\":-1,\"1146\":-1,\"1147\":-1,\"1148\":-1,\"1149\":-1,\"1150\":-1,\"1151\":-1,\"1152\":-1,\"1153\":-1,\"1154\":-1,\"1155\":-1,\"1156\":-1,\"1157\":-1,\"1158\":-1,\"1159\":-1,\"1160\":-1,\"1161\":-1,\"1162\":-1,\"1163\":-1,\"1164\":-1,\"1165\":-1,\"1166\":-1,\"1167\":-1,\"1168\":-1,\"1169\":-1,\"1170\":-1,\"1171\":-1,\"1172\":-1,\"1173\":-1,\"1174\":-1,\"1175\":-1,\"1176\":-1,\"1177\":-1,\"1178\":-1,\"1179\":-1,\"1180\":-1,\"1181\":-1,\"1182\":-1,\"1183\":-1,\"1184\":-1,\"1185\":-1,\"1186\":-1,\"1187\":-1,\"1188\":-1,\"1189\":-1,\"1190\":-1,\"1191\":-1,\"1192\":-1,\"1193\":-1,\"1194\":-1,\"1195\":-1,\"1196\":-1,\"1197\":-1,\"1198\":-1,\"1199\":-1,\"1200\":-1,\"1201\":-1,\"1202\":-1,\"1203\":-1,\"1204\":-1,\"1205\":-1,\"1206\":-1,\"1207\":-1,\"1208\":-1,\"1209\":-1,\"1210\":-1,\"1211\":-1,\"1212\":-1,\"1213\":-1,\"1214\":-1,\"1215\":-1,\"1216\":-1,\"1217\":-1,\"1218\":-1,\"1219\":-1,\"1220\":-1,\"1221\":-1,\"1222\":-1,\"1223\":-1,\"1224\":-1,\"1225\":-1,\"1226\":-1,\"1227\":-1,\"1228\":-1,\"1229\":-1,\"1230\":-1,\"1231\":-1,\"1232\":-1,\"1233\":-1,\"1234\":-1,\"1235\":-1,\"1236\":-1,\"1237\":-1,\"1238\":-1,\"1239\":-1,\"1240\":-1,\"1241\":-1,\"1242\":-1,\"1243\":-1,\"1244\":-1,\"1245\":-1,\"1246\":-1,\"1247\":-1,\"1248\":-1,\"1249\":-1,\"1250\":-1,\"1251\":-1,\"1252\":-1,\"1253\":-1,\"1254\":-1,\"1255\":-1,\"1256\":-1,\"1257\":-1,\"1258\":-1,\"1259\":-1,\"1260\":-1,\"1261\":-1,\"1262\":-1,\"1263\":-1,\"1264\":-1,\"1265\":-1,\"1266\":-1,\"1267\":-1,\"1268\":-1,\"1269\":-1,\"1270\":-1,\"1271\":-1,\"1272\":-1,\"1273\":-1,\"1274\":-1,\"1275\":-1,\"1276\":-1,\"1277\":-1,\"1278\":-1,\"1279\":-1,\"1280\":-1,\"1281\":-1,\"1282\":-1,\"1283\":-1,\"1284\":-1,\"1285\":-1,\"1286\":-1,\"1287\":-1,\"1288\":-1,\"1289\":-1,\"1290\":-1,\"1291\":-1,\"1292\":-1,\"1293\":-1,\"1294\":-1,\"1295\":-1,\"1296\":-1,\"1297\":-1,\"1298\":-1,\"1299\":-1,\"1300\":-1,\"1301\":-1,\"1302\":-1,\"1303\":-1,\"1304\":-1,\"1305\":-1,\"1306\":-1,\"1307\":-1,\"1308\":-1,\"1309\":-1,\"1310\":-1,\"1311\":-1,\"1312\":-1,\"1313\":-1,\"1314\":-1,\"1315\":-1,\"1316\":-1,\"1317\":-1,\"1318\":-1,\"1319\":-1,\"1320\":-1,\"1321\":-1,\"1322\":-1,\"1323\":-1,\"1324\":-1,\"1325\":-1,\"1326\":-1,\"1327\":-1,\"1328\":-1,\"1329\":-1,\"1330\":-1,\"1331\":-1,\"1332\":-1,\"1333\":-1,\"1334\":-1,\"1335\":-1,\"1336\":-1,\"1337\":-1,\"1338\":-1,\"1339\":-1,\"1340\":-1,\"1341\":-1,\"1342\":-1,\"1343\":-1,\"1344\":-1,\"1345\":-1,\"1346\":-1,\"1347\":-1,\"1348\":-1,\"1349\":-1,\"1350\":-1,\"1351\":-1,\"1352\":-1,\"1353\":-1,\"1354\":-1,\"1355\":-1,\"1356\":-1,\"1357\":-1,\"1358\":-1,\"1359\":-1,\"1360\":-1,\"1361\":-1,\"1362\":-1,\"1363\":-1,\"1364\":-1,\"1365\":-1,\"1366\":-1,\"1367\":-1,\"1368\":-1,\"1369\":-1,\"1370\":-1,\"1371\":-1,\"1372\":-1,\"1373\":-1,\"1374\":-1,\"1375\":-1,\"1376\":-1,\"1377\":-1,\"1378\":-1,\"1379\":-1,\"1380\":-1,\"1381\":-1,\"1382\":-1,\"1383\":-1,\"1384\":-1,\"1385\":-1},\"Code Link\":{\"0\":null,\"1\":null,\"2\":\"'https:\\/\\/github.com\\/intelligent-control-lab\\/Auto_Vehicle_Simulator'\\n\\n'https:\\/\\/github.com\\/intelligent-control-lab\\/Auto_Vehicle_Simulator'\",\"3\":null,\"4\":null,\"5\":null,\"6\":\"'In this paper, we propose vehicle motion extensions for VINS, which is tightly coupled with monocular visual measurement, inertial measurement, and motion information in the back-end optimization. Extensive quantitative evaluations in benchmark dataset indicate that the inclusion of motion information (stop detection module and orientation\\/velocity constraint) effectively enhances the robustness and accuracy of VINS. The proposed constraints can be further used to optimize other sensors, such as GPS and wheel encoder, to continuously enhance the accuracy of our tightly-coupled vehicle pose estimation framework.'\",\"7\":null,\"8\":null,\"9\":\"'The temperature compensated strain gauge was printed using the Prusa i3 Mk3 printer with the multi-material unit 2.0 attachment to print two filaments concurrently. This printer is a popular open-source extrusion material printer that can handle five filaments simultaneously. Table I shows the print parameters related to the printing of the sensor.'\",\"10\":null,\"11\":null,\"12\":null,\"13\":null,\"14\":\"'https:\\/\\/www.youtube.com\\/watch?v=Bw3-SQ4G-6Q&feature='\\n\\n'In this study, we focus on robotic closed-loop grasping using off-policy actor-critic deep RL, in which the ability to grasp a diverse range of unseen objects is directly learned from images. Simulated images are used to minimize the number of required real-world data samples. We adapt SRL to encourage actor-critic RL to attain efficient robotic grasping skills. We first encode essential information using SRL and then use it for RL following the method presented in [23]. Based on this learning procedure, a possible technique that we can use to efficiently perform a simulation-to-real-world transfer is a typical pixel-level domain adaptation, which attains a considerable decrease with respect to the visual reality gap. Specifically, we train a generative adversarial network (GAN) [30], which can map simulated images to adapted images. Next, we perform SRL using real and adapted images. We subsequently use the trained SRL model as a fixed module during RL in both the simulation and the real world, where the learned representation is given as the input for RL. In the simulation, GAN translates synthetic images into realistic ones. In general, this pixel-level domain adaptation enables the robot to learn grasping skills in a real environment using small amounts of real-world data.'\\n\\n'Our SRL is based on feature-level domain adaptation. In this study, we employed a variational autoencoder (VAE) for SRL and modified the classic adversarial loss [30] for domain adaptation. First, we briefly cover the VAE and adversarial loss and then describe the proposed SRL.'\\n\\n'A. Variational Autoencoder'\\n\\n'In general, the objective of VAE is to encode some form of prior knowledge that can help simplify and regularize the state representation. Our extended version of VAE, denoted by VAE+, additionally encourages our model to learn domain-invariant features between the simulator and the real world. To this end, we incorporate VAE loss and adversarial loss, and the full loss is'\\n\\n'Overview of our SRL. The generator part of the pixel-level domain adaptation translates the simulated images into fake images. The discriminator assigns a binary label to the latent state projected by the encoder. The latent state is concatenated with the image type and processed by the decoder to reconstruct the corresponding input.'\\n\\n'Fig. 1 shows an overview of our SRL. In the case of combining pixel- and feature-level domain adaptation, images are sampled from a distribution of adapted images generated with pixel-level domain adaptation instead of sampling them from data distribution in the simulator. An adapted image and latent state encoded from the adapted image are referred to fake image and fake latent, respectively. The image type is encoded as one-hot vector [Ireal, Ifake] \\u2208 {0, 1}2. The vector is concatenated with the latent state and processed by the decoder. This enables the decoder to efficiently reconstruct the corresponding input, even though the encoder makes the state indistinguishable.'\\n\\n'The encoder in the variant DANNs minimizes the success rate of the discriminator\\u2019s misclassification. The objective of DANNs is to solve typical minimax optimization problem that still poses great theoretical and empirical challenges.'\\n\\n'GANs trained using first-order optimization methods commonly fail to converge to a stable solution where the players cannot improve their objective, i.e., the Nash equilibrium of the underlying game. Since both sides want to undermine the others, a Nash equilibrium happens when one player will not change its action regardless of what the opponent may do. On the other hand, we relax typical GAN objectives, where the both sides do not have exactly opposite objectives. This implies that, unlike our model, the DANNs might often fails to enforce the reduced dimension of state representations encoded from a real and simulated image to be identical.'\\n\\n'We have presented a method to address the visual simulation-to-reality gap in actor-critic RL settings. The proposed method attains considerable performance with a small number of real-world grasps. To this end, we added the proposed adversarial loss to a typical VAE loss to explicitly enforce the reduced dimension of state representations encoded from a real and simulated image (or adapted image) to be identical. The combination of this technique and pixel-level domain adaptation reached a 74% success rate with only 500 real-world grasps.'\",\"15\":\"'Manipulation in a densely cluttered environment creates complex challenges in perception to close the control loop, many of which are due to the sophisticated physical interaction between the environment and the manipulator. Drawing from biological sensory-motor control, to handle the task in such a scenario, tactile sensing can be used to provide an additional dimension of the rich contact information from the interaction for decision making and action selection to manoeuvre towards a target. In this paper, a new tactile-based motion planning and control framework based on bioinspiration is proposed and developed for a robot manipulator to manoeuvre in a cluttered environment. An iterative two-stage machine learning approach is used in this framework: an autoencoder is used to extract important cues from tactile sensory readings while a reinforcement learning technique is used to generate optimal motion sequence to efficiently reach the given target. The framework is implemented on a KUKA LBR iiwa robot mounted with a SynTouch BioTac tactile sensor and tested with real-life experiments. The results show that the system is able to move the end-effector through the cluttered environment to reach the target effectively.'\\n\\n'A. Tactile Information Representation using Autoencoder'\\n\\n'In this paper, a multilayer perceptron-based autoencoder (MLP-AE) is designed for representation learning because the MLP is simple and efficient. It consists of an encoder and a decoder, which can be defined by the following transitions.'\\n\\n'The MLP-AE is trained using the MATLAB Deep Learning Toolbox by setting the sizes of the hidden layer and the output layer of the encoder to be 10 and 5, respectively, i.e., l = 10, and p = 5. The scaled conjugate gradient backpropagation algorithm is used for the MLP-AE network training. The training process is shown in Fig. 6, where the loss function converges during 1500 iterations of training. Moreover, the goodness of fit on average measured by (14) is 95.36% for the training data and validation data.'\\n\\n't-SNE on encoded data from MLP-AE'\",\"16\":null,\"17\":\"'https:\\/\\/lixinghang12.github.io\\/IndoorSceneCaptioning\\/'\\n\\n'With the development of deep learning, there are now many methods which are able to generate natural language captions from images and videos. The state-of-art methods usually utilize sequence-to-sequence framework, in which the image or video is firstly encoded into a feature vector by CNN, and then an RNN is used to generate the caption from this feature vector. Additionally, this caption generation paradigm often works with some attention mechanisms [6] [7] to get better captions. Although the existing image captioning methods [8] [9] [10] are available to generate brief captions for the indoor scene, they cannot provide informative captions that contain enough appeared objects and the exact number of the objects. Meanwhile, The existing video caption works [11] [12] mainly focus on giving a general description of the video rather than the indoor scene itself in details. For robotic indoor scene description, it is significant to generate real-time video caption incrementally because of the mobility of the robotic platform. This motivates us to propose the problem of robotic indoor scene captioning from streaming video.'\\n\\n'https:\\/\\/lixinghang12.github.io\\/IndoorSceneCaptioning\\/'\",\"18\":null,\"19\":null,\"20\":\"'In this paper, HueCode, a meta-marker that robustly and simultaneously exposes the relative pose between a marker and a camera along with additional information, is proposed. It occupies the area of a single marker by overlaying multiple types of markers in different colored layers. Using perspective information from the first (most recognizable) type of element marker, the second or higher marker can be recognized with a better success rate. An experiment using a HueCode made from an ArUco marker and a QR code showed that the QR code within the HueCode is recognizable at an elevation angle of up to 15\\u00b0, compared to 25\\u00b0 for a normal QR code. In addition, the versatility of HueCode is demonstrated using two robotic applications. The first is a lightweight estimation of absolute 6-DoF poses for mobile robots in a GNSS-denied environment, and the other is object annotation in two-dimensional image or three-dimensional space without any prior knowledge. The former realized pose estimation with a position error of less than 0.05 m, and the latter enabled annotation of a mirror and a transparent object, which are difficult for other sensors and machine learning to recognize.'\\n\\n'The details of this last merit are as follows. The ease of recognition of markers generally depends on the grid size used in the marker. For instance, the grid size of the ArUco marker is 1\\/5 the size of the whole marker at maximum, and that of the QR code is 1\\/21, which is the width of the frame-surrounding squares in the three corners, at maximum. In general, markers for relative pose (e.g., the ArUco marker) have a greater grid size than markers for additional information (e.g., QR codes), and are easy to recognize. HueCode uses this difference. In the recognition procedure, the element marker with the largest grid size, which is assumed to be the most recognizable marker, is tried at the beginning. After that, the image that the marker appears in, which is usually distorted, is corrected using perspective information about the first recognized marker. The corrected image is then used for recognizing the remaining element markers to maximize the success rate.'\\n\\n'Fig. 2 illustrates the proposed recognition process. For simplicity, we assume that HueCode is composed of an ArUco marker and QR code. First, (1: Camera) an RGB image is captured from the camera, and then (2: Splitter) the original RGB image is split into three single-channel images. (3: AR Reader) the relative pose between the marker and camera is recognized from the first element marker appearing in one of the split images. (4: Barcode Clipper) perspective transforms to correct the contour of the first marker are applied to another split image. This procedure generates a warped image that shows the second marker in its most recognizable pose (i.e., centered and perpendicular to the line-of-sight). (5: Barcode Reader) additional information is recognized from the second marker that appears in the corrected image.'\\n\\n'The range of elevation angles between a HueCode and a camera and the size in pixels required for successful recognition were evaluated. The evaluated HueCode was made by overlaying an ArUco maker (in the blue channel, 4 \\u00d7 4) for relative pose and a QR code (in the red channel, version 1, error correction level M) for additional information. It was printed at a size of 3.8 cm square using a color printer and laminated to reduce the gloss of ink and prevent reflections. The recognition of element markers was implemented by popular third-party libraries (ArUco module in OpenCV 3.2.0 for ArUco markers, and ZBar 0.1.0 for QR codes). No parameter tuning for the recognition of element markers was performed, and default values of the libraries were used. Images were captured by a pan focus camera (GoPro Hero4, 1980 \\u00d7 1080, diagonal field of view: 133.6\\u00b0). An ArUco marker and QR code were also printed separately in monochrome to be evaluated for comparison.'\\n\\n'Fig. 4 shows images obtained through the recognition of a HueCode whose elevation angle and height were 15\\u00b0 and 268 pixels, respectively. This shows that HueCode could be seen as an ArUco marker and a QR code by splitting the original image into single-channel images, and the perspective of the single-channel QR code image was successfully corrected using the result of ArUco marker recognition.'\\n\\n'An experiment was conducted to evaluate the accuracy of the proposed pose-estimation method. The result of the estimation was compared with motion capture (Optitrack, Motive) results. For the measurement, two HueCode markers were pasted together back-to-back, located at (x, y, z) = (1, 0, 1) (Units: m) in the world coordinate. The marker was printed on ordinary paper and was given a matte laminate to suppress reflections. Laminate processing is unnecessary, depending on the environment, so it is possible to create even cheaper markers. For each QR code, information on the absolute position of the location, the roll, pitch, and yaw values, which were (0\\u00b0, 0\\u00b0, 0\\u00b0) and (0\\u00b0, 0\\u00b0, 180\\u00b0), respectively, and the length of one side of the marker (29 cm) was embedded as data. The difference in the trajectory between the pose estimation result from the drone\\u2019s aerial video (1980 \\u00d7 1080) and from motion capture was compared by having a drone (DJI, MavicAir) fly around the marker while recording the trajectory by the motion capture system.'\\n\\n'In both demonstrations, the name of the object, object pose with respect to the marker, object shape, and side length of the marker were encoded on the marker as additional information. The shape of the object was described in terms of the radius and height in the case of the driveway mirror, and in terms of the width, length, and thickness in the case of the transparent panel.'\\n\\n'In this paper, we proposed HueCode, a meta-marker that robustly and simultaneously exposes the relative pose between a marker and a camera along with additional information. It occupies the area of a single marker by overlaying multiple types of markers in different colored layers. Using perspective information from the first (most recognizable) type of element markers, the second or higher marker can be recognized with a better success rate. An experiment with a HueCode made by combining an ArUco marker and a QR code showed that the QR code in HueCode was recognizable at an elevation angle up to 15\\u00b0, while that of a normal QR code was recognizable at up to 25\\u00b0.'\\n\\n'HueCode: A Meta-marker Exposing Relative Pose and Additional Information in Different Colored Layers'\\n\\n'Description of HueCode'\\n\\n'Applications Using HueCode'\\n\\n'In this paper, HueCode, a meta-marker that robustly and simultaneously exposes the relative pose between a marker and a camera along with additional information, is propo...'\\n\\n'HueCodes'\\n\\n'In this paper, we propose HueCode, a meta-marker that robustly and simultaneously exposes the relative pose between the marker and the camera along with additional information, but only occupies the area of a single marker by overlaying multiple types of markers in different colored layers. As shown in Fig. 1, a HueCode is generated by overlaying arbitrary types of markers, one for relative pose and another for additional information according to the requirement of the application or allowed space for the marker. It is possible to overlay additional visible text or images as the third or higher layer.'\\n\\n'As explained in the introduction, HueCode is a metamarker that robustly and simultaneously exposes relative pose and additional information within the area of a single marker by overlaying multiple markers in different colored layers.'\\n\\n'To clarify the characteristics of HueCode, a brief comparison of the proposed HueCode with markers that have been developed to expand the capability of other existing markers is given in Table 1. C2Tag is a marker that consists of several concentric circles that can be used for camera tracking [2] as one of its applications, and it is known to be robust against blurs and occlusions. A marker that integrates AR markers and QR codes was developed to display different objects in augmented reality with the same AR marker [4]. Another combination of AR markers and QR codes was proposed by Dandan et al. [5]. In this case, they are combined with a chessboard to form a hybrid QR-Aruco-Chessboard (QAC) pattern that can be used to realize reliable visual tracking for robot-assisted microsurgery. Unlike the fiducial markers that have been explained so far, which solely use black and white, the use of colors has been introduced to the markers to provide additional features. ChromaTag by Joseph et al. [6] is a marker that enables rapid detection by using different colors to eliminate false positives quickly. A color-based and recursive fiducial marker (CRFM) was developed to achieve quick detection, good accuracy, and partial-occlusion-tolerant marker recognition [7]. The colors are used to deal with partial occlusion. Yingcai et al. [8] developed a colored marker in which the color helps a drone to locate the landing point to accomplish autonomous visual tracking and landing. Layered QR codes have three QR codes embedded in different colored layers, which can be extracted by a camera independently [9].'\\n\\n'On the other hand, HueCode has the merits listed below that the related methods did not have. These are the result of overlaying different types of markers, which, to the best of our knowledge, is the first time this feature has been adopted for any fiducial marker.'\\n\\n'HueCode offers relative pose plus additional information at once via a camera, which is useful in a wide variety of robotic applications.'\\n\\n'HueCode places each element marker at the maximum size without increasing the overall size.'\\n\\n'The implementation cost of HueCode is low because existing algorithms or libraries can be applied to read each element marker.'\\n\\n'HueCode handles unknown types of element markers by embedding how to read them in other markers as additional information.'\\n\\n'HueCode recognizes the second or higher marker at a better success rate by using perspective information from the first (most recognizable) marker.'\\n\\n'In addition, by using a physical bandpass filter that passes light of a target color, a HueCode element marker can be recognized by existing reader software for each element marker type. Fig. 3 demonstrates an ArUco marker and a QR code layered in a single HueCode and filtered to be read by their corresponding reader apps.'\\n\\n'In the previous sections, we proposed HueCode, a meta-marker that can expose relative pose and additional information for general purposes. In this section, to demonstrate the versatility of HueCode, two robotic applications based on HueCode are introduced. The first is lightweight estimation of the absolute 6-DoF pose for mobile robots in a GNSS-denied environment, and the other is object annotation in two-dimensional image or three-dimensional space without any prior knowledge.'\\n\\n'Detection process of HueCode (Elevation Angle: 15\\u00b0Height: 268 pixels)'\\n\\n'HueCode can automate the aforementioned annotations on two dimensional images and three-dimensional spaces.'\\n\\n'HueCode for two- and three-dimensional annotation exposes the shape and pose of an object with respect to the marker as additional information. The robot obtains the relative pose of the marker with respect to the robot, and the shape and relative pose of the object with respect to the marker. By coordinate transformation based on this information, the robot can determine the shape and pose of the object with respect to the robot.'\\n\\n'Using the above procedures, the camera system can obtain the contour of an object without any manual operations. In other words, HueCode can automate the time-consuming task of object annotations of images for machine learning. Furthermore, the object name (i.e., the label to learn) can be added as additional information on the HueCode to deal with new object types without any modification of the annotation system.'\\n\\n'Annotation using HueCode requires only a camera and does not need expensive LiDAR or depth sensors. In addition, HueCode can annotate transparent objects and mirrors, cases where LiDAR and depth sensors cannot capture point clouds.'\\n\\n'In addition, the versatility of HueCode was demonstrated by two robotic applications. The first was a lightweight estimation of the absolute 6-DoF pose for mobile robots in a GNSS-denied environment. Here, a new pose-estimation method for mobile robots was proposed that is cheap, learning-free, and absolute pose obtainable using HueCode, in which the global pose of the marker was embedded as additional information. The performance of the proposed method was confirmed in a comparative accuracy evaluation experiment using a motion capture system, which achieved a position error of less than 5 cm.'\\n\\n'In the future, we intend to improve both the generation and recognition of HueCode. One challenge in generating the markers is to print a marker in the colors as designed. From this viewpoint, the generation method based on the RGB color model proposed in this paper could be improved by a method based on the CMYK color model, where the primary colors are the same as those of the ink in most printers. A CMYKbased method could avoid a mixture of inks that makes the colors darker than designed.'\\n\\n'HueCode recognition can also be improved by using the HSV color space, which is more robust than the RGB color space against changes in illumination. For instance, changes in brightness affect only color brightness (the V of HSV) and changes in hue (for example, daylight vs. sunset) affects only the color hue (the H of HSV). Independent filtering in the HSV color space would work better than the RGB-based recognition method proposed in this paper.'\\n\\n'As mentioned in the introduction, controlling swarm robots can be done using HueCode by placing a marker exposing an individual status to each robot. Successive landing of one hundred drones [22] is one of the possible applications.'\",\"21\":\"'Visual Simultaneous Localization And Mapping (SLAM) is a long-standing problem within the computer vision and robotics communities. Nonetheless, pure vision-based solutions lack the level of robustness found in laser-based solutions, and are thus often complemented by additional sensors such as\\u2014on ground vehicles\\u2014encoders measuring the rotational velocity of the wheels. The installation of wheel encoders on existing platforms is however difficult, and accessing the signals of existing encoders may be prevented by the manufacturer. As a result, the development of robust, purely vision-based (or inertial-supported) SLAM solutions remains a relevant topic in the development of self-driving vehicles. The present paper presents such a solution.'\\n\\n'To conclude, we let our kinematically constrained opti-mization compete against an established alternative from the open-source community: ORB-SLAM [28]. RPE results are again indicated in Table I.'\",\"22\":\"'As we perform a map-based localization, the quality of the global map is crucial for the localization accuracy. We therefore use a dedicated vehicle which equipped with a high-class XW-GI5651 inertial navigation system (INS), an RTK system, a 32-layer and two 16-layer Robosense LiDARs, and a set of encoders. Such a well-equipped vehicle can provide a highly accurate point-cloud map. We use the same method as in Section II to get the line and surfel features in the global map.'\",\"23\":\"'We would like to thank Dr. Fangbo Qin for his suggestions on scientific writing. We also thank Xin Li for his help in the simulation code.'\",\"24\":null,\"25\":\"'https:\\/\\/github.com\\/YaoYao1995\\/MEEE'\\n\\n'https:\\/\\/github.com\\/YaoYao1995\\/MEEE'\",\"26\":\"'In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cos...'\\n\\n\\\"In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihood- free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.\\\"\\n\\n'To avoid this complex reconstruction, some previous MBRL studies have proposed decoder-free representation learning [16], [21] based on contrastive learning [22], [23], which trains a discriminator instead of the decoder. The discriminator is trained by categorical cross-entropy optimization, which encourages latent embeddings to be sufficiently distinguishable among different embeddings. Nevertheless, to the best of our knowledge, no MBRL methods have achieved the state-of-the-art results on the difficult bench-mark tasks of DeepMind Control Suite (DMC) [20] without reconstruction.'\\n\\n'We derive a likelihood-free (decoder-free) and InfoMax objective for contrastive learning by reformulating the variational evidence lower bound (ELBO) of the graphical model of the partially observable Markov decision process.'\\n\\n'We propose a decoder-free variant of Dreamer, which we call Dreamer with InfoMax and without generative decoder (Dreaming). Dreaming trains a policy as almost same way with the original Dreamer. The only difference between the methods is that we alternatively use the contrastive learning scheme introduced in the previous section to train RSSM. We implement Dreaming in TensorFlow [35] by modifying the official source code of Dreamer1. We keep all hyperparameters and experimental conditions similar to the original ones. A newly introduced hyperparameter K in Eq. (8) (overshooting distance) is set to be K = 3 based on the ablation study in Appx. III.'\\n\\n'Open-loop video predictions. The left 5 consecutive images show reconstructed context frames and the remaining images are generated open-loop. The decoder is trained independently without backpropagating reconstruction errors to other models.'\\n\\n'In the present paper, we proposed Dreaming, a decoderfree extension of the state-of-the-art MBRL method from pixels, Dreamer. A likelihood-free contrastive objective was derived by reformulating the original ELBO of Dreamer. We incorporated the two indispensable components below to the contrastive learning: (i) independent and linear forward dynamics, (ii) the random crop data augmentation. By making the most of the decoder-free nature and the two components, Dreaming was able to outperform the baseline methods on difficult tasks especially where Dreamer suffers from object vanishing.'\",\"27\":\"'https:\\/\\/github.com\\/hanyas\\/mimo'\\n\\n'https:\\/\\/github.com\\/hanyas\\/mimo'\",\"28\":\"'Preliminary, our model assumes one-to-one mapping between the observation and the latent variable. A bijective loss is used for training an encoder-decoder structure as a soft constraint of the one-to-one mapping.'\\n\\n'There have been many approaches to learn the generative model, but variational methods such as variational autoencoder (VAE) [12], [13] and an adversarial method such as a generative adversarial network (GAN) [14], [15] have been popular for the deep generative model. In the following section, we discuss each method and claim that the combination of the variational and adversarial approaches is adequate for our MNIW embedding.'\",\"29\":\"'The ground truth for the decoder network is obtained from the collected expert trajectories E = {ei(xi, yi)}. In order to fit in the decoder network, this trajectory is first transformed into the body frame and then projected into the same grid as the global path. To calculate the ground truth for the classification network, we first define the situation where re-planning is required as follows:'\",\"30\":null,\"31\":\"'This paper studies push recovery for humanoid robots based on a variable-height inverted pendulum (VHIP) model. We first develop an approach for treating zero-step capturability of the VHIP with a novel methodology based on Hamilton-Jacobi (HJ) reachability analysis. Such an approach uses the sub-zero level set of a value function to encode capturability of the VHIP, where the value function is obtained by numerically solving a HJ variational inequality offline. Based on this analysis, a simple and effective method for adjusting foothold locations is then devised for cases where the VHIP state is not zero-step capturable. In addition, the HJ reachability analysis naturally induces an optimal control law that allows for rapid planning with the VHIP during push recovery online. To enable use of the strategy with a position-controlled humanoid robot, an associated differential inverse kinematics based tracking controller is employed. The effectiveness of the overall framework is demonstrated with the UBTECH Walker robot in the MuJoCo simulator. Simulation validations show a significant improvement in push robustness as compared to the methods based on the classical linear inverted pendulum model.'\",\"32\":null,\"33\":null,\"34\":null,\"35\":\"'An efficient data processing is required to decode essential information from a touch on the proposing sensor. The microphones are connected to a single DAQ (myRIO-1900, National Instruments, USA), which has a FPGA chip and a processor chip. Each microphone is sampled with a 50 kHz sampling rate and stored in a buffer until 128 samples are collected. A Hanning window and Fast Fourier Transform, both length of 128, are performed in FPGA chip to compute in high speed. Signals in both the time domain and frequency domain are further processed in the myRIO processor, as shown in Fig. 3, for tactile communication interpretation, which is given in detail in the next Section.'\",\"36\":\"'The Interaction Network (INet) is designed to encode the interaction features among all agents. In contrast to previous studies, our method is able to capture the collective influence among the agents into one single feature vector. In addition, our method could consider the planned future movement of the ego-agent for reference. The INet takes three information sources of all agents as input: the observed trajectories, the hidden states of LSTMs, and the planned trajectory of the ego-agent. Given these data, INet computes the features for both the global spatiotemporal inter-agents interactions and the future ego-others interaction.'\\n\\n'The global spatiotemporal interaction and the environment information are encoded by the AIN and the EN, respectively. Given the location of a moving agent, i.e., the i-th one, we first compute the local area interaction around the agent using an attention model. This is because an individual always focuses on the surrounding regions as it moves. The attention model is presented as.'\\n\\n'Following previous works, we utilize an LSTM-based sequence-to-sequence model to solve the prediction problem. For each obstacle, the encoder takes the observed trajectory as input at the first Tobs time steps:'\\n\\n'Furthermore, to capture the multi-modal distribution of the movement and increase the robustness of the proposed method, we introduce random white noise into the decoder to generate multiple plausible trajectories. Specifically, we initialize the hidden state of the LSTMD using the last state of the LSTME:'\\n\\n'A new method for trajectory forecasting of multiple agents in dynamic scenes is presented in this study. The method is able to extract the global spatiotemporal interaction feature from the observed trajectories, and consider the temporal interactions among agents by soft tracking. An environment net is introduced in our method to encode the road topology for accurate prediction. The prediction net combines the features of spatiotemporal interactions and environment to prediction the future trajectories of agents. Experiments on four benchmark datasets are presented and an ablation study is implemented to demonstrate the effectiveness of each component in the method.'\",\"37\":null,\"38\":null,\"39\":null,\"40\":null,\"41\":null,\"42\":null,\"43\":\"'https:\\/\\/www.alaris.kz\\/'\\n\\n'https:\\/\\/github.com\\/alarisnu\\/alaris_hand'\\n\\n'https:\\/\\/www.alaris.kz\\/'\\n\\n'https:\\/\\/github.com\\/alarisnu\\/alaris_hand'\\n\\n'This paper presents a new open-source mechanical design of a 6-DOF anthropomorphic ALARIS robotic hand that can serve as a low-cost design platform for further customizat...'\\n\\n'This paper presents a new open-source mechanical design of a 6-DOF anthropomorphic ALARIS robotic hand that can serve as a low-cost design platform for further customization and utilization for research and educational purposes. The presented hand design employs linkage-based three-phalange finger and two-phalange adaptive thumb designs with non-backdrivable worm-and-rack transmission mechanisms. Combination of design improvements and solutions, discussed in the paper, are implemented in a functional robotic hand prototype with powerful grasping capabilities, which utilizes off-the-shelf inexpensive components and 3D printing technology ensuring the hand low manufacturing cost and replicability. The open-source mechanical design of the presented ALARIS robotic hand is freely available for downloading from the authors\\u2019 research lab web-site https:\\/\\/www.alaris.kz and https:\\/\\/github.com\\/alarisnu\\/alaris_hand.'\\n\\n'To extend the choice of robotic hand designs freely available to robotics and prosthetics researchers, in this paper the authors present a novel open-source mechanical design of a 6-DOF anthropomorphic robotic hand (Fig. 1). The proposed ALARIS hand design utilizes linkage-based three-phalange finger and two-phalange adaptive thumb designs with non-backdrivable worm-and-rack transmission mechanisms resulting to low-cost hand prototype demonstrating satisfactory object grasping performance. We also report various mechanical and hardware issues encountered and solved during the hand design process. The proposed open-source mechanical design of the robotic hand has been created using SolidWorks CAD software and is freely available for downloading from the authors\\u2019 research lab web-site https:\\/\\/www.alaris.kz and https:\\/\\/github.com\\/alarisnu\\/alaris_hand, so it can serve as a low-cost design platform for further customization and utilization for research and educational purposes.'\\n\\n'The requirement for an overall low cost of the hand was the another factor that justified many design choices. As the cost of finger actuators constitutes the largest part, identical off-the-shelf low-cost miniature brushed DC motors with integrated gearbox for higher torque output were utilized in the presented proof-of-concept hand design prototype. The hand design CAD files will be made publicly available for straightforward prototyping using any low cost 3D printer, which would eliminate the need for expensive, labor-intensive fabrication processes and facilitate the widespread use of the presented hand design in research and education applications. Additionally, by being open-source, the hand is fully customizable and, thus, could be modified for specific needs. For instance, the finger and thumb mechanisms can be redesigned to accommodate more reliable and accurate miniature servomotors for advanced hand motion control implementations.'\\n\\n'In this paper, a new open-source mechanical design of a 6-DOF anthropomorphic ALARIS robotic hand is presented to extend the choice of robotic hand designs freely available to robotics and prosthetics researchers. Combination of design improvements and solutions presented and discussed in the paper are implemented in a functional robotic hand prototype with powerful grasping capabilities, which utilizes off-the-shelf inexpensive components and 3D printing technology ensuring the hand low manufacturing cost and replicability. The robotic hand was experimentally tested with variety of objects in grasping tasks and output force measurements that demonstrated its feasibility for utilization in robotic research and teaching applications. The proposed open-source mechanical design of the robotic hand is freely available for downloading from the authors\\u2019 research lab web-site https:\\/\\/www.alaris.kz and https:\\/\\/github.com\\/alarisnu\\/alaris_hand, so it can serve as a low-cost design platform for further customization and utilization in research and education applications.'\\n\\n'https:\\/\\/github.com\\/alarisnu\\/alaris_hand'\",\"44\":null,\"45\":null,\"46\":null,\"47\":\"'In this work, we first benchmark the performance of various observation spaces on the standard benchmark problems. Our configuration includes a basic setting with joint encoders and IMUs, a reduced coordinate, a maximal coordinate, and manually designed observations, where all can be further augmented with contact flags and a short history. Then we establish a few principles based on the experimental results. In addition, we propose a search algorithm for automatically optimizing the observation space, which is designed based on the identified principles. Our algorithm consists of two main components: an optimization algorithm for searching observation spaces and a statistical test for deleting malicious observation channels. We demonstrate that our algorithm can find an optimal observation space that outperforms the manually designed observations.'\\n\\n'Raw sensors (RS): Raw sensor readings from motor encoders and an inertial measurement unit (IMU);\\\\nO\\\\nRS\\\\n={\\\\nq\\\\njt\\\\n,\\\\nq\\\\n\\u02d9\\\\njt\\\\n,\\u03b8,\\\\n\\u03b8\\\\n\\u02d9\\\\n,\\\\nC\\\\n\\u00a8\\\\n1\\\\n}\\\\n. We assume that an IMU can provide a global orientation using on-board integration and filtering algorithms.'\",\"48\":null,\"49\":null,\"50\":\"'The composite image, visualization, and the error statistics on 3 directions for the larger quadrotor flying through a narrow gap. In the visualization, the color code indicates the height of the obstacles, the axes indicates the quadrotor\\u2019s position, the yellow arrow indicates the position command and the black corridor indicates the calculated reachable sets along the trajectory according to the estimated disturbance. In the figure of the error statistics, the blue lines indicate the real error while the red and green lines indicate the estimated upper and lower bound of the error.'\\n\\n'The composite image, visualization, and the error statistics on 3 directions for the smaller quadrotor flying in a complex environment. In the visualization, the color code indicates the height of the obstacles, the axes indicates the quadrotor\\u2019s position, the yellow arrow indicates the position command and the black corridor indicates the calculated reachable sets along the trajectory according to the estimated disturbance. In the figure of the error statistics, the blue lines indicate the real error while the red and green lines indicate the estimated upper and lower bound of the error.'\\n\\n'The proposed disturbance estimator and motion planning method in this paper is implemented in C++11 standard with an open-source non-linear optimization solver NLopt 1. The visual-inertial state estimation module VINS-Fusion [29] and the deformable dense mapping module Dense Surfel Mapping [30] are adopted. For the experiments in complex environments with various obstacles, the map of the environment is firstly constructed using the Dense Surfel Mapping module as well as the VINS-Fusion module with loop closure detection, preventing the visual-inertial odometry (VIO) from severe drift, which can cause unexpected crash during the subsequent flight. Then, during the flight, the planning algorithm runs fully onboard and the commands are rectified according to the loop closure results to ensure the correct relative position between the obstacles and the quadrotors. In order to validate the proposed disturbance estimation and motion planning method, two quadrotors with different size and configurations as shown in Fig. 6 are adopted and tested individually in the experiments.'\",\"51\":null,\"52\":\"'https:\\/\\/www.youtube.com\\/watch?v=HcwBNcah0eo'\\n\\n'The quadrotor is popularly used in challenging environments due to its superior agility and flexibility. In these scenarios, trajectory planning plays a vital role in generating safe motions to avoid obstacles while ensuring flight smoothness. Although many works on quadrotor planning have been proposed, a research gap exists in incorporating self-adaptation into a planning framework to enable a drone to automatically fly slower in denser environments and increase its speed in a safer area. In this paper, we propose an environmental adaptive planner to adjust the flight aggressiveness effectively based on the obstacle distribution and quadrotor state. Firstly, we design an environmental adaptive safety aware method to assign the priority of the surrounding obstacles according to the environmental risk level and instantaneous motion tendency. Then, we apply it into a multi-layered model predictive contouring control (Multi-MPCC) framework to generate adaptive, safe, and dynamical feasible local trajectories. Extensive simulations and real-world experiments verify the efficiency and robustness of our planning framework. Benchmark comparison also shows superior performances of our method with another advanced environmental adaptive planning algorithm. Moreover, we release our planning framework as open-source ros-packages 1 .'\",\"53\":null,\"54\":null,\"55\":\"'Architecture of our transition predictive model. The input includes ten consecutive obstacle map representation laser scans, relative goal position and current velocity of robot, while the output includes the future obstacle map transformed according to the velocity change and corresponding reward. All the observation inputs are encoded and combined with action embedded by fully connected layers. Specifically, the sequential obstacle maps are further processed into motion and content, in which motion is composed of differences between adjacent frames and content is represented by the last frame.'\\n\\n'Moving in dynamic pedestrian environments is one of the important requirements for autonomous mobile robots. We present a model-based reinforcement learning approach for robots to navigate through crowded environments. The navigation policy is trained with both real interaction data from multi-agent simulation and virtual data from a deep transition model that predicts the evolution of surrounding dynamics of mobile robots. A reward function considering social conventions is designed to guide the training of the policy. Specifically, the policy model takes laser scan sequence and robot\\u2019s own state as input and outputs steering command. The laser sequence is further transformed into stacked local obstacle maps disentangled from robot\\u2019s ego motion to separate the static and dynamic obstacles, simplifying the model training. We observe that the policy using our method can be trained with significantly less real interaction data in simulator but achieve similar level of success rate in social navigation tasks compared with other methods. Experiments are conducted in multiple social scenarios both in simulation and on real robots, the learned policy can guide the robots to the final targets successfully in a socially compliant manner. Code is available at https:\\/\\/github.com\\/YuxiangCui\\/model-based-social-navigation.'\",\"56\":\"'The ConvRNN encoder-forecaster model. Observations from the past\\\\nC\\\\n\\u22121\\\\nand\\\\nC\\\\n0\\\\nare used to generate predictions into the future\\\\nC\\\\n\\u02c6\\\\n1\\\\nand\\\\nC\\\\n\\u02c6\\\\n2\\\\n. This structure can be repeated for arbitrary input and output sequence lengths. Figure adapted from [28].'\\n\\n'A tower of ConvRNN and downsampling blocks is iterated temporally for each input observation, encoding a sequence of input images to a set of hidden feature images at multiple resolutions. A similar tower, reversed with upsampling replacing downsampling, transforms the encoded images into output predictions at future time steps. The recurrent nature of the encoder-forecaster framework lends itself particularly well to online usage, as hidden states can be stored, rendering each observation integration or prediction a constant-time operation.'\",\"57\":null,\"58\":null,\"59\":null,\"60\":null,\"61\":null,\"62\":null,\"63\":null,\"64\":null,\"65\":\"'Seven healthy subjects (4 males and 3 females, age: 24 \\u00b1 1 years, height: 1.66 \\u00b1 0.09 meters, weight: 61 \\u00b1 10 Kg) were recruited in this study. All subjects gave their informed consent for inclusion before they participated in the study. This study was approved by the Medical Ethics Committee of School of Medicine, Zhejiang University (Project identification code: 2018-005).'\",\"66\":\"'Fig. 1 presents an overview of our method, which we call c2g-HOF: The cost-to-go-generating HOF encodes an input workspace\\\\nW\\\\n, and generates the weights of a radial basis function network (RBFN) which represents the cost-to-go function over the configuration space (C-space). During training, the values generated by the cost-to-go function are compared to values generated by a traditional planner such as Dijkstra\\u2019s algorithm over grid samples or a probabilistic roadmap over the configuration space. This way, the network learns to generate the cost-to-go function for any input workspace. During runtime, the weights are used to generate a compact neural network whose gradient yields the desired motion plan.'\\n\\n'The HOF generating network requires an encoder for the workspace\\\\nW\\\\nprovided as input. We use PointNet [5] for 3D point clouds. For 2D image or 3D voxel of workspace, we can apply image and voxel based convolution encoders. The c2g-network takes two configurations as input and outputs the optimal distance between two configurations. To summarize, the c2g generating HOF takes the point cloud as input\\\\nW\\\\nand generates weights of a cost-to-go function\\\\nf\\\\nW\\\\n:C\\u00d7C[0,\\u221e)\\\\n, such that for all\\\\nq\\\\n1\\\\n,\\\\nq\\\\n2\\\\n\\u2208C\\\\n,\\\\nc=\\\\nf\\\\nW\\\\n(\\\\nq\\\\n1\\\\n,\\\\nq\\\\n2\\\\n)\\u2208[0,\\u221e)\\\\nis the cost-to-go between q1 and q2.'\",\"67\":null,\"68\":null,\"69\":null,\"70\":null,\"71\":null,\"72\":\"'Alg.1 shows the pseudocode of our main algorithm. Given a start and goal configuration of object\\\\nx\\\\ns\\\\nobj\\\\n and \\\\nx\\\\ng\\\\nobj\\\\n, the planner is initialized by setting the root node of the two trees\\\\nT\\\\ns\\\\n,\\\\nT\\\\ng\\\\nwith DTRs\\\\nP\\\\ns\\\\n,\\\\nP\\\\ng\\\\n, which contain convex regions for mobile bases of each robot (blue and green regions in Fig. 2(d)) and task region for an object (red region in Fig. 2(b)), where for each DTR we have\\\\nP={\\\\nP\\\\n1\\\\nrob\\\\n,\\u2026,\\\\nP\\\\nN\\\\nrob\\\\n,\\\\nP\\\\ntask \\\\n}\\\\n. More details about SpaceDecomposition is in Sec. IV-B. Afterward, two trees start to grow until either time elapsed or maximum iterations are reached. The RandomTaskConfig function generates a random sampled configuration xrand in task space (yellow point in Fig. 2(a)), which is utilized to compute a random DTR that is\\\\nP\\\\nrand\\\\n. Then, the NearestNeighbor finds not only the neighbor with minimum\\\\nd\\\\nP\\\\n, but also defines a N \\u00d7 1 vector of binary connection status k between two adjacent DTRs.\\\\nd\\\\nP\\\\nis the distance metric for any two adjacent DTRs:'\",\"73\":null,\"74\":\"'The following is the pseudo-code of FRMHA*. We give the current vertex as the input vertex, vi, and the remaining segment, starting right after the blocked vertex, of the previous path to travel as \\u03c0p = (v0,v1,v2,\\u2026,vN). And the goal vertex should be the same, vg = vN.'\\n\\n'In the above pseudo code,\\\\nh(v)=\\\\n[\\\\nh\\\\n0\\\\n(v),\\\\nh\\\\n1\\\\n(v),\\u2026,\\\\nh\\\\nN\\\\nf\\\\n(v)]\\\\nT\\\\n\\u2208\\\\nR\\\\nN\\\\nf\\\\n+1\\\\n, where h0(v) is the admissible heuristic cost of v, hn(v) for n \\u2260 0 is the nth additional heuristic function of (1). OPEN.top(n) returns the vertex in OPEN with the smallest value of fn(v) = g(v) + w1hn(v) and OPEN.pop(n) returns this vertex and removes it from OPEN. The suboptimality bound is controlled by multiplication of two variables w1 and w2, where w1 controls the inflation of heuristic values and w2 controls the priority between the inadmissible and admissible heuristics.'\\n\\n'FRMHA* is a variation of SMHA*, and both algorithms are based on A*, while switching heuristic functions to speed up searching. Skipping common parts of the algorithm, we will explain the above pseudo-code focusing on distinctive features of FRMHA*. The line 14) checks if the search is stuck in local minima w.r.t. hn(\\u2022). If so, the search is guided by the next active heuristic function or pulled toward the next unexpanded feature vertex by switchHeuristic(). The line 18) checks if the search escaped from local minima w.r.t. the admissible heuristic. If so, now the search is guided by the admissible heuristic at line 19). To verify escaping from local minima, two variables, flast and fprev, save the value of local minima w.r.t. admissible and additional heuristic functions, respectively. Then the search procedure and terminating criteria (line 20)) are the same with SMHA* and A*.'\",\"75\":\"'High-performance implementation of solution and gradient computation are open-sourced for the reference of the community1.'\",\"76\":null,\"77\":null,\"78\":null,\"79\":\"'The base module is mounted on the moving part of the vertical equilibrium-position adjuster. The module consists of the leaf spring and the horizontal driving unit. The left end of the spring strip is mounted to the base module. At the deflectable right end of the spring, the ball joint interface is designed for the external force to be applied on. To measure the vertical deflection, the optical linear-encoder reader is also attached to the spring\\u2019s endpoint. For the proper alignment of the spring strip, two rectangular slots of the strip\\u2019s width are cut from the (upper) endpoint part and the (lower) fixed-end part. Providing 90 mm traveling distance of the stiffness adjuster, the horizontal driving unit uses a precision ball screw (1 mm pitch) actuated by a high-speed DC motor, along with two shafts of the linear bearings. The bigger bearing supports the major load transferred through the elastic component to the structure whereas the smaller bearing supports the moment that parallel to the horizontal driving axis.'\\n\\n'In this experiment, the stiffness adjuster was fixed at seven different positions (from 12 to 90 mm of deflected length). At each position, the endpoint deflection was gradually adjusted through the linear-drive equilibrium-position adjuster (using a rotary encoder). The force applied at the endpoint through the double-ball-joint interface was recorded and plotted against the deflection (Fig. 8).'\\n\\n'Endpoint force varies against the deflection (measured by the rotary encoder of the vertical ball screw system), at different positions of the stiffness adjuster (deflected length).'\\n\\n'In this experiment, the endpoint was set at three different deflections: 1 mm, 2 mm, and 3 mm. Starting from the minimum-stiffness position (far away from the endpoint), the stiffness adjuster was controlled to start moving with the rapid acceleration, travel with the maximum possible speed, and stop moving at the target position (the maximum-stiffness position). From that position, the stiffness adjuster was controlled to travel backward to the minimum-stiffness position. The force applied on the ball-joint interface and the deflection read by the optical linear encoder were also recorded (see the results in Fig. 10). The experiment was repeated with the three different deflections.'\\n\\n'By allowing a small overshoot, the stiffness adjuster takes less than 0.25 seconds to modulate the full range of stiffnesses (about 72.6 cm traveling distance) in both forward and backward adjustments. Although the endpoint deflection was constrained by the ball-joint interface, the position data read from the optical linear encoder contains the small variation due to the couple effect of the change in the deflection slope. The force overshoots are observed from the three forward motions of the stiffness adjuster to increase the stiffness. The magnitude of the overshoot increases with magnitude of the deflection.'\\n\\n'Full-range stiffness modulation at fixed different deflections. The applied force, the deflection read by the linear encoder, and the stiffness adjuster (roller) position, are plotted against time.'\\n\\n'Through some preliminary experiments conducted on the developed prototype, two important discrepancies between the theory and the \\\"real-world\\\" implementation were observed. Based on the assumption of zero bending moment in the left segment of the spring, our improved model should be the lower bound of the stiffness while the basic model based on the ideal cantilever support should be the upper bound. However, there are still some data points (stiffness of the soft spring with short deflected length) outside this bound. It is possible that the actual hardware geometry of the spring\\u2013 endpoint connection causes the spring to generate the lower- than-expected force at a constrained deflection, which might be particularly noticeable at high deflection angle. The optical linear encoder reading is affected not only by the endpoint deflection but also the change in inclination.'\",\"80\":null,\"81\":null,\"82\":null,\"83\":\"'https:\\/\\/youtu.be\\/weiSPIVv0AQ'\\n\\n'B. Azimuth Estimation\\\\nWe employ MLP and SNN to derive azimuths of N sound sources from GCC-PHAT-SM features, with the architectures illustrated in Fig. 2(b) and Fig. 2(c), respectively. We use Mean Squared Error (MSE) as the objective function to train the multi-speaker SSL system:\\\\nL(y,\\\\ny\\\\n^\\\\n)=\\\\n1\\\\nk\\\\n\\u2225y\\u2212\\\\ny\\\\n^\\\\n\\u2225\\\\n2\\\\n(5)\\\\nView Source where y is the ground truth vector encoded from \\u0398, \\u0177 is the output of MLP or SNN classifier, and k is the length of \\u0177. We pick N peaks in \\u0177 as the sound source azimuth estimates when test the SSL system [4].\\\\nWe employ different label encoding schemes for the MLP and the event-driven SNN back-end classifier, as displayed in Fig. 3(a) and Fig. 3(b). Each output neuron of MLP represents an azimuth candidate where a higher value suggests a more likely sound source. We follow the label coding scheme of [4] with an example of two concurrent speakers given in Fig. 3(a). This coding scheme resembles a spatial spectrum that peaks at each ground truth azimuth. It considers the correlation between adjacent directions for a better generalization of NNs and works for any number of sound sources. Unlike MLP, each output neuron of the event-driven SNN represents a spike time. Therefore, a neuron fires a spike at an earlier time corresponds to a more probable sound source. Thus, different from MLP which finds the maximum, SNN seeks the minimum as the azimuth estimate.\\\\nFig. 2.\\\\n(a) Architecture of the SSL system with a segment of M-channel audio signals. (F : number of frequency bins in phase and magnitude spectrum;\\\\nM\\\\n~\\\\n=M(M\\u22121)\\/2\\\\n: number of unique microphone pairs; \\u2297: element-wise multiplication). Three convolutional layers are employed to estimate the speech mask from magnitude spectrum. For simplicity, ReLU and layer normalization are omitted; (b) the MLP classifier, and (c) the SNN classifier.\\\\nShow All\\\\nFig. 3.\\\\nOutput coding with two concurrent speakers for the (a) MLP, and (b) SNN back-end classifiers.\\\\nShow All\\\\nFig. 4.\\\\nGCC-PHAT-SM Spikes encoding. A GCC-PHAT-SM element with a higher value contributes more to the SNN inference by firing spikes at an earlier time.\\\\nShow All\\\\nMLP: An MLP takes the GCC-PHAT-SM feature, normalized into [0, 1], as the input. A batch normalization layer is attached to each Rectified Linear Unit (ReLU) function.\\\\nSNN: We also perform SSL on event-driven SNN that is more power-efficient than MLP by 2 order of magnitude [47], thus is more suitable for an energy-aware robotic platform.\\\\nAn SNN first encodes the GCC-PHAT-SM features into spiking trains following latency encoding scheme as shown in Fig. 4, and take the spiking trains as input as shown in Fig. 2(c). Let us denote the jth neuron at the layer l as\\\\n\\u03c3\\\\nl\\\\nj\\\\n, the dynamics of the membrane potential\\\\nV\\\\nl\\\\nj\\\\nof the\\\\n\\u03c3\\\\nl\\\\nj\\\\nis formulated as [47]:\\\\nV\\\\nl\\\\nj\\\\n(t)=\\\\n\\u2211\\\\ni\\\\nN\\\\n\\u03c9\\\\nl\\\\nij\\\\nK(t\\u2212\\\\nt\\\\nl\\u22121\\\\ni\\\\n)\\\\nt\\\\nl\\\\nj\\\\n=F{t\\u2223\\\\nV\\\\nl\\\\nj\\\\n(t)=\\u03d1,t\\u22650}\\\\nK(t\\u2212\\\\nt\\\\nl\\u22121\\\\ni\\\\n)={\\\\nt\\u2212\\\\nt\\\\nl\\u22121\\\\ni\\\\nift>\\\\nt\\\\nl\\u22121\\\\ni\\\\n0otherwise\\\\n(6)\\\\n(7)\\\\n(8)\\\\nView Source where \\u03d1 is the spike threshold. l is the layer index.\\\\n\\u03c9\\\\nl\\\\nij\\\\nis the weight between the\\\\n\\u03c3\\\\nl\\u22121\\\\ni\\\\nand the\\\\n\\u03c3\\\\nl\\\\nj\\\\n.\\\\nt\\\\nl\\u22121\\\\ni\\\\nis the time when the\\\\n\\u03c3\\\\nl\\u22121\\\\ni\\\\nfires a spike.\\\\nK\\\\nis the kernel of the postsynaptic potential (PSP).\\\\nWhen the\\\\n\\u03c3\\\\nl\\u22121\\\\ni\\\\nfires a spike, its PSP starts to change according to the PSP function in Eq.8, and influences the membrane potential\\\\nV\\\\nl\\\\nj\\\\nwith a weight\\\\n\\u03c9\\\\nl\\\\nij\\\\naccording to the Eq.6. Once\\\\nV\\\\nl\\\\nj\\\\nacross the \\u03d1, the\\\\n\\u03c3\\\\nl\\\\nj\\\\nfires a spike.'\\n\\n'An SNN first encodes the GCC-PHAT-SM features into spiking trains following latency encoding scheme as shown in Fig. 4, and take the spiking trains as input as shown in Fig. 2(c). Let us denote the jth neuron at the layer l as\\\\n\\u03c3\\\\nl\\\\nj\\\\n, the dynamics of the membrane potential\\\\nV\\\\nl\\\\nj\\\\nof the\\\\n\\u03c3\\\\nl\\\\nj\\\\nis formulated as [47]:'\",\"84\":\"'The source code for the proposed framework is available online. Meanwhile, several datasets are also provided for algorithm development and evaluation of researchers. This is the first open-source implementation for GNSS positioning and GNSS-RTK using the FGO.'\\n\\n\\\"To fill this gap, this paper develops a factor graph-based formulation that provides the capability of GNSS positioning and GNSS-RTK positioning. Regarding the GNSS positioning, the pseudorange and Doppler measurements are integrated using FGO where the historical measurements are utilized simultaneously. Regarding the GNSS-RTK, the DD pseudorange and carrier-phase measurements are integrated using FGO where the states are connected using velocity estimated using Doppler measurements. To the best of the authors' knowledge, this is the first integrated framework that solves the GNSS positioning and GNSS-RTK using the state-of-the-art FGO. Meanwhile, we open-source the implementation of the proposed factor graph-based formulation1. The rest of the paper is summarized as follows: the derivation of FGO-based GNSS positioning and GNSS-RTK methods are described in Section III after the overview of the proposed method is presented in Section II. Then, the experiment is conducted to evaluate the performance of the proposed framework in Section IV. Finally, conclusions are drawn, together with future work.\\\"\",\"85\":\"'The DenseNet architecture for feature extraction is composed four Dense Blocks. With RGB-D source, color and depth information are passed to two networks with same architecture. In this situation, layer numbers of Dense Block are 2, 3, 6, 4 respectively and the first three Dense Blocks share the same parameter. Both images are transformed to 512-dimension features, which are then concatenated to 1024 dimensions. When input only contains color image, it is converted to 1024 dimensions with layer number 6, 12, 24, 16. In camera pose learning part, the paper adopts two LSTM layers with feature size 512 to learn features of 8 previous poses. Finally, the camera pose feature and image feature are connected to 1536 dimensions for point cloud generation. In decode stage, the paper outputs 2048 points with TopNet decoder. In detail, tree level is 6 and the node feature number is set to 8.'\\n\\n'For the future work, from authors\\u2019 opinions, two aspects of efforts can be made to promote pose precision. For one aspect, more efficient point cloud decoder can be focused to decrease the construction error. For the other aspect, point cloud registration algorithm by deep neural network is also recommended.'\",\"86\":null,\"87\":\"'Since the mature open sources of visual-inertial-UWB systems are unavailable, and most existing visual-inertial systems such as VINS fail in sequence 2 and 4, we compare the UVIP with the proposed PL-sVIP.'\",\"88\":null,\"89\":\"'The contribution of this paper is three-fold. First, we propose an automatic and general hyper-parameter tuning approach for odometry estimation algorithms. Second, a data augmentation method to prevent the parameter tuning process from overfitting is proposed. Third, we have released the code, as open source, to automate the odometry tuning process 1.'\",\"90\":\"'Finally, temporal feature pooling encodes co-occurrences of segments in the past kt point cloud frames providing robustness to sensor motion and dynamic objects. Once multi- level features are extracted, second-order pooling aggregates information of local features over a point cloud frame to form a holistic representation for place recognition. Second-order pooling captures the multiplicative interactions between the multi-level features and outputs a fixed-length descriptor which is invariant to the number and permutation of the input features. Furthermore, the fixed dimension of the global descriptor enables maintaining the computational complexity.'\\n\\n'We introduce multi-level features which encode structural appearance, topological relationships and temporal correspondences related to components in a scene.'\\n\\n'We formulate the generation of a global descriptor which encodes these multi-level features into a single viewpoint invariant representation using second-order pooling and demonstrate how these multi-level features contribute to place recognition performance.'\\n\\n'To address the challenge of place recognition, the scene- level point cloud is often encoded in three different ways; a set of local descriptors, a single global descriptor or a set of object\\/segment descriptors. Local descriptor based methods first detect a set of keypoints in the point cloud and then form local descriptors by encoding information in each keypoint neighbourhood [3], [9]. Local descriptors and keypoint detection suffer from low repeatability in noisy point clouds and changing environments.'\\n\\n'For each segment, a compact deep feature which encodes the structural appearance of the segment is obtained using the SegMap-CNN network proposed in [15]. The network represents each segment in a voxel grid of 32x32x16 of which the voxel sizes (0.1m by default) are scaled in order to fit larger segments. The description network consists of three 3D convolutional layers plus max-pool layers followed by two fully connected layers. For a given set of m segments S, it outputs a set of compact features\\\\nF\\\\na\\\\n={\\\\nf\\\\na\\\\n1\\\\n,\\u2026,\\\\nf\\\\na\\\\nm\\\\n}\\\\n(where\\\\nf\\\\na\\\\ni\\\\n=\\\\nf\\\\na\\\\n(\\\\ns\\\\ni\\\\n)\\u2208\\\\nR\\\\nd\\\\n,d=64)\\\\nwhich discriminates segments based on structural appearance.'\\n\\n'Feature pooling computes a new feature for a segment by taking the weighted average of features of all related segments. Encoding topological information is achieved via feature pooling based on spatial relationships within a point cloud frame. Temporal information is encoded via pooling features based on temporal correspondences across multiple point cloud frames. For a query segment\\\\ns\\\\nP\\\\nn\\\\ni\\\\nfrom the current point cloud frame Pn, we find topological relationships and temporal correspondences with all other segments\\\\ns\\\\nP\\\\ni\\\\nj\\\\n\\u2208\\\\nS\\\\nn,\\\\nk\\\\nt\\\\nfrom the current frame and kt frames into the immediate past,'\\n\\n'Place Recognition enables the estimation of a globally consistent map and trajectory by providing non-local constraints in Simultaneous Localisation and Mapping (SLAM). This paper presents Locus, a novel place recognition method using 3D LiDAR point clouds in large-scale environments. We propose a method for extracting and encoding topological and temporal information related to components in a scene and demonstrate how the inclusion of this auxiliary information in place description leads to more robust and discriminative scene representations. Second-order pooling along with a non- linear transform is used to aggregate these multi-level features to generate a fixed-length global descriptor, which is invariant to the permutation of input features. The proposed method outperforms state-of-the-art methods on the KITTI dataset. Furthermore, Locus is demonstrated to be robust across several challenging situations such as occlusions and viewpoint changes in 3D LiDAR point clouds. The open-source implementation is available at: https:\\/\\/github.com\\/csiro-robotics\\/locus.'\",\"91\":\"'This paper proposes an automatic and accuracy- enhanced extrinsic calibration method for 3D LiDARs with a range offset correction, which needs only an arbitrarily-shaped single planar board. One of the most exhaustive parts of existing LiDAR calibration procedures is to manually find target objects from massive point clouds. To obviate user interventions, we propose an automated planar board detection from LiDAR range images. To extract a target completely, we suppress outliers and restore rejected inliers of the target board by introducing a target completion method. We empirically find that range measurements of various LiDARs are mainly skewed by constant offset values. To compensate for this, we suggest a range offset model for each laser channel in calibration procedures. The relative pose between LiDARs and range offsets are jointly estimated by minimizing bi-directional point- to-board distances within the iterative re-weighted least squares (IRLS) framework. To verify the suggested range offset model, we obtain and analyze extensive real-world measurements. By conducting experiments using the various sensor configurations and shapes of boards, we quantitatively and qualitatively confirm accuracy and versatility of the proposed method by comparing with the state-of-the-art LiDAR calibration methods. All the source code and data used in the paper are available at : https:\\/\\/github.com\\/JunhaAgu\\/AutoL2LCalib.'\",\"92\":null,\"93\":null,\"94\":null,\"95\":null,\"96\":\"'Bottom-up approaches for image-based multi-person pose estimation consist of two stages: (1) keypoint detection and (2) grouping of the detected keypoints to form person instances. Current grouping approaches rely on learned embedding from only visual features that completely ignore the spatial configuration of human poses. In this work, we formulate the grouping task as a graph partitioning problem, where we learn the affinity matrix with a Graph Neural Network (GNN). More specifically, we design a Geometry-aware Association GNN that utilizes spatial information of the keypoints and learns local affinity from the global context. The learned geometry-based affinity is further fused with appearance-based affinity to achieve robust keypoint association. Spectral clustering is used to partition the graph for the formation of the pose instances. Experimental results on two benchmark datasets show that our proposed method outperforms existing appearance-only grouping frameworks, which shows the effectiveness of utilizing spatial context for robust grouping. Source code is available at: https:\\/\\/github.com\\/jiahaoLjh\\/PoseGrouping.'\",\"97\":null,\"98\":null,\"99\":\"'We release the diabolo model and code along with a wrapper for the open-source simulator Gazebo [11], so it can be used as a plugin with existing robot systems or stand-alone.'\\n\\n'We proposed a diabolo model that can be used for robot learning, and a system to generate motion sequences for and control a robot system playing diabolo. The results showed that our model significantly outperforms a purely learning-based predictor, and that the system can generate stable motions that accelerate the diabolo. The model as well as the dataset we recorded will be released open-source, so that the community can test learning algorithms on this new task. We hope that its unique set of characteristics and incremental challenges will facilitate the development of new approaches and techniques. Future work will extend the diabolo model to include gyroscope dynamics and string wrapping, testing residual physics learning to improve the model, and training machine learning agents to perform tricks.'\",\"100\":\"'The framework of DAT Network. The input of the DAT Network is one scenario image and the output of the DAT Network is multi scenario images. A trapezoid placed vertically represents a neural network that includes the functions of an encoder or functions described on it. The arrows indicate the direction in which the data is transferred. The different colored network layers below represent different local robots, and the same colored ones represent the same parameters and structure. The blue icon in the left bottom denotes dataset of real scenarios.'\",\"101\":null,\"102\":null,\"103\":\"'https:\\/\\/www.youtube.com\\/watch?v=IDainHodcU'\",\"104\":null,\"105\":\"'Illustration of the TFVT-HRI framework, which is composed of three modules: (a) Visual token extractor that extracts visual tokens of objects in one frame. (b) Transformer-based decision model which encodes video clip to predict proactive behaviors (c) Multi-modal action encoder that encodes the natural language, expression, and body motion to form a representation vector.'\\n\\n'Multi-modal Action Encoder'\\n\\n'To enable the decision model to reason over time, normally, we could try to track each individual and encode the trajectory. Unfortunately, tracking one individual requires registration of visual tokens across different frames, which can introduce additional error to the decision. To this end, instead of tracking each individual, we present a transformer decision model. Transformer is widely used in natural language processing [24] by allowing the tokens to fully interact with each other through self-attention. In our case, it is not only able to capture the trajectory of each pedestrian by at-tending over time, but also possible to capture the interaction between the target and other individuals, e.g., talking with another person, holding a cell phone, carrying a suitcase. Also, to preserve consistency for training and inference, for each visual token\\\\ne\\\\n(t)\\\\ni\\\\nthat we consider, we allow it to attend to\\\\ne\\\\n(\\\\nt\\\\n\\u2032\\\\n)\\\\ni\\\\nonly when t\\u2032 \\u2264 t, which implies masked self-attention in transformer blocks (MTRN for short). In our case, we use 6 transformer blocks.'\",\"106\":null,\"107\":\"'https:\\/\\/youtu.be\\/lmWSwHl9l8'\",\"108\":null,\"109\":\"'An effective understanding of the contextual environment and accurate motion forecasting of surrounding agents is crucial for the development of autonomous vehicles and social mobile robots. This task is challenging since the behavior of an autonomous agent is not only affected by its own intention, but also by the static environment and surrounding dynamically interacting agents. Previous works focused on utilizing the spatial and temporal information in time domain while not sufficiently taking advantage of the cues in frequency domain. To this end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which can capture inter-agent correlations and temporal dependency simultaneously in frequency domain in addition to time domain. SpecTGNN operates on both an agent graph with dynamic state information and an environment graph with the features extracted from context images in two streams. The model integrates graph Fourier transform, spectral graph convolution and temporal gated convolution to encode history information and forecast future trajectories. Moreover, we incorporate a multi-head spatio-temporal attention mechanism to mitigate the effect of error propagation in a long time horizon. We demonstrate the performance of SpecTGNN on two public trajectory prediction benchmark datasets, which achieves state-of-the-art performance in terms of prediction accuracy.'\\n\\n'For the nuScenes dataset, we compared SpecTGNN with the baselines with publicly available code and conducted all the experiments under the same settings. We predicted the trajectory at future 4s based on the previous 4s observations. As shown in Table II, our model outperforms the baselines by a large margin, especially for long-term prediction. Compared with the previous state-of-the-art base-lines STGAT [42] and S-STGCNN [14] which only leverage the spatial and temporal information, the proposed model consistently performs better, indicating that the information in frequency domain can further improve the conventional graph neural network. SpecSTGNN reduces minFDE20 of the final 4.0s by 8.1% on the nuScenes dataset compared with the SOTA baseline STGAT.'\",\"110\":\"'In visual robot self-localization, graph-based scene representation and matching have recently attracted research interest as robust and discriminative methods for self-localization. Although effective, their computational and storage costs do not scale well to large-size environments. To alleviate this problem, we formulate self-localization as a graph classification problem and attempt to use the graph convolutional neural network (GCN) as a graph classification engine. A straightforward approach is to use visual feature descriptors that are employed by state-of-the-art self-localization systems, directly as graph node features. However, their superior performance in the original self-localization system may not necessarily be replicated in GCN-based self-localization. To address this issue, we introduce a novel teacher-to-student knowledge-transfer scheme based on rank matching, in which the reciprocal-rank vector output by an off-the-shelf state-of-the-art teacher self-localization model is used as the dark knowledge to transfer. Experiments indicate that the proposed graph-convolutional self-localization network (GCLN) can significantly outperform state-of-the-art self-localization systems, as well as the teacher classifier. The code and dataset are available at https:\\/\\/github.com\\/KojiTakeda00\\/Reciprocal_rank_KT_GCN.'\\n\\n'The main contributions of this work are summarized as follows: (1) We propose a novel graph node descriptor, which transfers the prediction of an off-the-shelf state-of-the-art teacher self-localization model to the student GCN classifier. (2) We show that a class-specific reciprocal-rank vector is a proper and effective representation of the dark knowledge to transfer. (3) We experimentally show that the proposed graph-convolutional self-localization network (GCLN) can significantly outperform state-of-the-art self-localization systems, as well as the teacher classifier. (4) We make the code and dataset publicly available1.'\",\"111\":null,\"112\":null,\"113\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34721938'\",\"114\":null,\"115\":\"'Conceptual overview of the open-source acoustic driving platform based on the Arduino prototyping board and the signal waveforms when the power is off and on.'\",\"116\":\"'The structure of whole control system consisted of outer inputs, PLC real-time controller and actuation system. The outer inputs included the user interface, IMU, torque sensor and encoder. A remote host computer running Windows operation system exchanged commands and messages with HexaMRL via Wi-Fi about every 50 ms. Another embedded computer (Intel Core i7-6600U Dual Core 2.6G Hz CPU) running PLC program was installed on body as the robot controller. The real-time control program included impedance controller, data filter, kinematics, dynamics, foot force model and adaptive landing algorithm. An EtherCAT bus system was used to exchange message. The controller cycled at 4K Hz for force sensing, posture sensing, online gait trajectory planning and torque-based motor controlling in real time, enabling quick reaction to the touch-ground event. The body posture provided by IMU and the eighteen joint torques measured by torque sensors were used to estimate the robot state. The planning trajectory, the actual trajectory obtained by encoder and the joint torques were inputted to the impedance controller, then the expected torques would be calculated and sent to the actuation system consisting of driver, motor, and encoder.'\",\"117\":\"'encoder'\\n\\n'(a) An overview of LEAF on the Sawyer Push and Reach task. The initial and goal images are encoded by the encoder of the VAE into latents z0 and zg respectively. Random states are sampled from the currently learned manifold of the VAE latent states, and are used to infer the current frontier pk* as described in Section IV-A. The currently learned deterministic policy is used to reach a state in the frontier z* from the initial latent state z0. After that, the currently learned stochastic policy is used to reach the latent goal state zg. A reconstruction of what the latent state in the frontier z* decodes to is shown. For clarity, a rendered view of the actual state\\\\nz\\\\n^\\\\n\\u2217\\\\nreached by the agent is shown alongside the reconstruction. (b) A schematic of the reachability network used to determine whether the latent state zj (statej) is reachable from the latent state zi (statei) in k time-steps. The reachability vector k consists of a vector of the same size as zi and zj with all its elements being k. The output 0 indicates that state zi is not reachable from state zj .'\",\"118\":\"'Underwater image enhancement is an important low-level computer vision task for autonomous underwater vehicles and remotely operated vehicles to explore and understand the underwater environments. Recently, deep convolutional neural networks (CNNs) have been successfully used in many computer vision problems, and so does underwater image enhancement. There are many deep-learning-based methods with impressive performance for underwater image enhancement, but their memory and model parameter costs are hindrances in practical application. To address this issue, we propose a lightweight adaptive feature fusion network (LAFFNet). The model is the encoder-decoder model with multiple adaptive feature fusion (AAF) modules. AAF subsumes multiple branches with different kernel sizes to generate multi-scale feature maps. Furthermore, channel attention is used to merge these feature maps adaptively. Our method reduces the number of parameters from 2.5M to 0.15M (around 94% reduction) but outperforms state-of-the-art algorithms by extensive experiments. Furthermore, we demonstrate our LAFFNet effectively improves high-level vision tasks like salience object detection and single image depth estimation.'\\n\\n'Our LAFFNet is the encoder-decoder structure and subsumes two main blocks: the AFF module and the residual module. The architecture of LAFFNet is shown in Fig. 3. We describe these two modules before elaborate on the whole network. Furthermore, we also provide a detailed analysis of our LAFFNet. Finally, the loss function is described to train the proposed model.'\",\"119\":null,\"120\":null,\"121\":\"'http:\\/\\/bit.ly\\/ICRA21-SnakeSteering'\\n\\n'http:\\/\\/bit.ly\\/ICRA21-SnakeSteering'\\n\\n'http:\\/\\/bit.ly\\/ICRA21-SnakeSteering'\",\"122\":null,\"123\":\"'The BlueROV2 Heavy runs the open-source ArduSub, as part of the ArduPilot project, to control the vehicle. On the setup, a companion computer, Raspberry Pi 3, and a 3DR Pixhawk Autopilot are used as the processing and control unit. Communication between the ardupilot and the onboard computer uses the protocol of MAVLink. The Raspberry Pi 3 is also connected to a digital HD USB camera, that is, the frontal monocular camera on the BlueROV2 Heavy. The camera publishes video stream with a resolution of 1920 \\u00d7 1080 to a topside computer by a neutral buoyancy Fathom tether. Thanks to the Robot Operating System (ROS), the topside computer collects sensor data, processes calculation and sends control output to the electronic speed controller for thrusters control. The flowchart of all modules running in the proposed underwater autonomous system is depicted in Figure 7. The framework starts from subscribing to raw measurements from the on-board sensors. By feeding the video stream to the visual navigation system, pose estimation of the setup is estimated. Once the reference trajectory is dynamically planned and enters the control module as inputs, the control unit calculates the output of the PWM signal for generating thruster control. For improving user experiences, a Graphical User Interface (GUI) is developed with the tool of Qt.'\",\"124\":null,\"125\":\"'Architecture overview of proposed MSCVNet. We use census transform and Absolute difference to construct the initial cost volumes and then use per-pixel convolutions to adjust the cost volumes to a low-dimensional feature vector. Meanwhile, after extracting image features through a small Unet, we use a 1D Correlation layer to generate two cost volumes with different dimensions. The traditional cost volume is leveraged by an encoder to predict multi-scale guidance features, which will be fed into two cascade hourglass networks with other cost volumes to predict the final disparity map.'\\n\\n'Our cost aggregation network mainly consists of the following two modules: Multi-scale guided feature encoder and Cascade hourglass network. The specific implementation of our network is as follows:'\\n\\n'1) Multi-scale Guided Feature Encoder'\\n\\n'We use an encoder to extract feature vectors from the other two cost volumes, leveraged by a decoder to cooperate with the features generated from traditional cost volume. The architecture of one cascade hourglass network is shown in Fig. 2.'\\n\\n'The detailed structure of the cascade hourglass network. We use residual blocks to extract features with multiple resolutions from CNN and combine them with the traditional method in a decoder.'\",\"126\":null,\"127\":\"'observation encoder'\\n\\n'DHC consists of three modules: observation encoder (blue), communication block (green), and Q-network (orange). The two communication blocks are identical.'\",\"128\":null,\"129\":null,\"130\":null,\"131\":null,\"132\":\"'Sampling-based motion planning under task constraints is challenging because the null-measure constraint manifold in the configuration space makes rejection sampling extremely inefficient, if not impossible. This paper presents a learning-based sampling strategy for constrained motion planning problems. We investigate the use of two well-known deep generative models, the Conditional Variational Autoencoder (CVAE) and the Conditional Generative Adversarial Net (CGAN), to generate constraint-satisfying sample configurations. Instead of precomputed graphs, we use generative models conditioned on constraint parameters for approximating the constraint manifold. This approach allows for the efficient drawing of constraint-satisfying samples online without any need for modification of available sampling-based motion planning algorithms. We evaluate the efficiency of these two generative models in terms of their sampling accuracy and coverage of sampling distribution. Simulations and experiments are also conducted for different constraint tasks on two robotic platforms.'\",\"133\":null,\"134\":null,\"135\":null,\"136\":null,\"137\":null,\"138\":null,\"139\":null,\"140\":null,\"141\":null,\"142\":null,\"143\":null,\"144\":null,\"145\":null,\"146\":null,\"147\":null,\"148\":null,\"149\":null,\"150\":null,\"151\":null,\"152\":\"'Symmetric environments, such as office corridors and carparks, remain as critical and intractable scenarios for mobile robot to perform global localization tasks due to the highly similar geometrical structures and lack of distinctive features [1] \\u2013[9]. It is challenging to accomplish the initialization step or to restore the state of the kidnapped robot. Current localization solutions in such environments either count on pre-mounted infrastructures such as artificial landmarks, wireless beacons, guiding tapes, and QR-codes; or lean upon onboard sensors such as LiDAR, camera, and magnetic sensor (i.e. magnetometer). On the one hand, the infrastructure-dependent methods are neither credible nor scalable. Moreover, the costs of deploying and maintaining the infrastructures are quite high [10] \\u2013[14]. On the other hand, most of the initialization modules of the single sensor-based global localization algorithms are incapable to collect enough distinctive information, thus the robot may fail to localize in such symmetric environments [2], [3], [6], [7].'\\n\\n'Vision based distance measurement system using two-dimensional barcode for mobile robot'\",\"153\":null,\"154\":null,\"155\":\"'This paper aims to improve robots\\u2019 versatility and adaptability by allowing them to use a large variety of end- effector tools and quickly adapt to new tools. We propose AdaGrasp, a method to learn a single grasping policy that generalizes to novel grippers. By training on a large collection of grippers, our algorithm is able to acquire generalizable knowledge of how different grippers should be used in various tasks. Given a visual observation of the scene and the gripper, AdaGrasp infers the possible grasp poses and their grasp scores by computing the cross convolution between the shape encodings of the gripper and scene. Intuitively, this cross convolution operation can be considered as an efficient way of exhaustively matching the scene geometry with gripper geometry under different grasp poses (i.e., translations and orientations), where a good \\\"match\\\" of 3D geometry will lead to a successful grasp. We validate our methods in both simulation and real- world environments. Our experiment shows that AdaGrasp significantly outperforms the existing multi-gripper grasping policy method, especially when handling cluttered environments and partial observations. Code and Data are available at https:\\/\\/adagrasp.cs.columbia.edu.'\",\"156\":null,\"157\":null,\"158\":\"'This work focuses on learning useful and robust deep world models using multiple, possibly unreliable, sensors. We find that current methods do not sufficiently encourage a shared representation between modalities; this can cause poor performance on downstream tasks and over-reliance on specific sensors. As a solution, we contribute a new multi-modal deep latent state-space model, trained using a mutual information lower-bound. The key innovation is a specially-designed density ratio estimator that encourages consistency between the latent codes of each modality. We tasked our method to learn policies (in a self-supervised manner) on multi-modal Natural MuJoCo benchmarks and a challenging Table Wiping task. Experiments show our method significantly outperforms state-of-the-art deep reinforcement learning methods, particularly in the presence of missing observations.'\",\"159\":null,\"160\":null,\"161\":null,\"162\":\"'This paper presents a decentralized and asynchronous systematic solution for multi-robot autonomous navigation in unknown obstacle-rich scenes using merely onboard resources. The planning system is formulated under gradient-based local planning framework, where collision avoidance is achieved by formulating the collision risk as a penalty of a nonlinear optimization problem. In order to improve robustness and escape local minima, we incorporate a lightweight topological trajectory generation method. Then agents generate safe, smooth, and dynamically feasible trajectories in only several milliseconds using an unreliable trajectory sharing network. Relative localization drift among agents is corrected by using agent detection in depth images. Our method is demonstrated in both simulation and real-world experiments. The source code is released for the reference of the community.'\",\"163\":null,\"164\":\"'This paper introduces a general state estimation framework fusing multiple sensor information for hybrid wheeled-legged robots performing mobile manipulation tasks. At the core of the state estimator is a novel unified odometry for hybrid locomotion which can seamlessly maintain tracking and has no need to switch between stepping and rolling modes. To the best of our knowledge, the proposed odometry is the first work in this area. It is calculated based on the robot kinematics and instantaneous contact points of wheels with sensor inputs from IMU, joint encoders, joint torque sensors estimating wheel contact status, as well as RGB-D camera detecting geometric features of the terrain (e.g. elevation and surface normal vector). Subsequently, the odometry output is utilized as the motion model of a 3D Lidar map-based Monte Carlo Localization module for drift-free state estimation. As part of the framework, visual localization is integrated to provide high precision guidance for the robot movement relative to an object of interest. The proposed approach was verified thoroughly by two experiments conducted on the Pholus robot with OptiTrack measurements as ground truth.'\",\"165\":null,\"166\":null,\"167\":\"'Robot grasping and manipulation planning in unstructured and dynamic environments is heavily dependent on the attributes of manipulated objects. Although deep learning approaches have delivered exceptional performance in robot perception, human perception and reasoning are still superior in processing novel object classes. Moreover, training such models requires large datasets that are generally expensive to obtain. This work combines crowdsourcing and gamification to leverage human intelligence, enhancing the object recognition and attribute estimation aspects of robot perception. The framework employs an attribute matching system that encodes visual information into an online puzzle game, utilizing the collective intelligence of players to expand an initial attribute database and react to real-time perception conflicts. The framework is deployed and evaluated in a proof-of-concept application for enhancing object recognition in autonomous robot grasping and a model for estimating the response time is proposed. The obtained results demonstrate that given enough players, the framework can offer near real-time labeling of novel objects, based purely on visual information and human experience.'\",\"168\":null,\"169\":null,\"170\":null,\"171\":null,\"172\":null,\"173\":null,\"174\":null,\"175\":\"'An Encoder-Free Joint Velocity Estimation Method for Serial Manipulators Using Inertial Sensors'\\n\\n'This paper focuses on developing a real-time and flexible velocity estimation approach for serial revolute manipulator using only one inertial measurement unit (IMU) mounted on each link side of the manipulator. Particularly, the proposed approach has no requirement for the installation position and orientation of the IMU, which improves the flexibility of the implementation procedure. A joint velocity model is established based on the proposed principle of constructing coordinate system according to the robotic geometric information. The general solutions are derived in detail, thereby the proposed algorithm can be generalized into any other robots with the same geometric configuration. With the method, the joint rotational velocity measurements of static and dynamic robotic motion are provided compared to encoders. Experimental results based on the six degrees of freedom (DOF) collaborative manipulator have validated the feasibility and effectiveness of the proposed approach. The proposed method has the benefits of low cost and flexibility, which could work as a redundant velocity monitor criterion to provide assistant joint velocity measurements.'\",\"176\":null,\"177\":null,\"178\":null,\"179\":null,\"180\":null,\"181\":null,\"182\":null,\"183\":null,\"184\":null,\"185\":null,\"186\":null,\"187\":null,\"188\":null,\"189\":null,\"190\":null,\"191\":null,\"192\":null,\"193\":null,\"194\":\"'Relocalization is a fundamental problem in robotics and computer vision. A robot has to localize itself when moving in urban or indoor environments to achieve competent autonomy. Several existing solutions employ Global Navigation Satellite System (GNSS) to perform localization. However, GNSS is not always available such as in indoor environments and the accuracy of GNSS cannot be guaranteed in urban environments with high-rising buildings since they can block GNSS signals. There is a significant body of knowledge in visual localization, as it has been studied for decades. Conventional geometry-based visual localization systems mainly utilize handcrafted features and descriptors, which are typically sensitive to illumination variation, dynamic objects and viewpoint change [11]. Recently, learning-based visual localization methods such as PoseNet and variants [5], [12]\\u2013[14] have been proposed to solve these challenges, which leverage either a single image or a sequence of images to predict 6-Degree-of-Freedom (6-DoF) poses directly. Unlike retrieval-based learning approaches e.g. CamNet [7], RelocNet [1] and Camera Relocalization CNN [16], location-related information of these deep learning methods is implicitly encoded within the parameters of these deep neural networks, and therefore these methods require agents that have previously traversed the same environment. However, vision sensors inherently suffer from several drawbacks which restrict their ability to be used in scenarios where reliability is highly desirable, such as self-driving cars. Visual inputs are easily impacted by ambient environmental conditions e.g. sunshine, rain, fog; and further by their narrow Field-of-View (FoV).'\",\"195\":null,\"196\":null,\"197\":null,\"198\":null,\"199\":\"'Learning Conditional Postural Synergies for Dexterous Hands: A Generative Approach Based on Variational Auto-Encoders and Conditioned on Object Size and Category'\",\"200\":null,\"201\":null,\"202\":null,\"203\":null,\"204\":null,\"205\":null,\"206\":null,\"207\":null,\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":null,\"213\":\"\\\"In this work, the problem of human-robot collaborative object transfer to unknown target poses is addressed. The desired pattern of the end-effector pose trajectory to a known target pose is encoded using DMPs (Dynamic Movement Primitives). During transportation of the object to new unknown targets, a DMP-based reference model and an EKF (Extended Kalman Filter) for estimating the target pose and time duration of the human's intended motion is proposed. A stability analysis of the overall scheme is provided. Experiments using a Kuka LWR4+ robot equipped with an ATI sensor at its end-effector validate its efficacy with respect to the required human effort and compare it with an admittance control scheme.\\\"\",\"214\":null,\"215\":null,\"216\":null,\"217\":null,\"218\":null,\"219\":null,\"220\":null,\"221\":null,\"222\":\"'The energy of ocean waves is the key distinguishing factor of marine environments compared to other aquatic environments such as lakes and rivers. Waves significantly affect the dynamics of marine vehicles; hence it is imperative to consider the dynamics of vehicles in waves when developing efficient control strategies for autonomous surface vehicles (ASVs). However, most marine simulators available open-source either exclude dynamics of vehicles in waves or use methods with high computational overhead. This paper presents ASVLite, a computationally efficient ASV simulator that uses frequency domain analysis for wave force computation. ASVLite is suitable for applications requiring low computational overhead and high run-time performance. Our tests on a Raspberry Pi 2 and a mid-range desktop computer show that the simulator has a high run-time performance to efficiently simulate irregular waves with a component wave count of up to 260 and large-scale swarms of up to 500 ASVs.'\",\"223\":null,\"224\":null,\"225\":null,\"226\":null,\"227\":null,\"228\":null,\"229\":null,\"230\":null,\"231\":null,\"232\":null,\"233\":null,\"234\":null,\"235\":null,\"236\":null,\"237\":null,\"238\":null,\"239\":null,\"240\":\"'If a robot is to perform object manipulation tasks, it must process its sensory inputs to extract physical properties that are relevant to the objects and the task. For example, for picking an object, the robot may extract the pose of the object and its size from a camera image. However, for general objects and tasks, defining the relevant properties can be difficult \\u2013 what properties should be extracted for manipulating a deformable object such as a rope? And what should be encoded about objects with nontrivial geometries?'\",\"241\":\"'We present a novel framework for self-supervised grasped object segmentation with a robotic manipulator. Our method successively learns an agnostic foreground segmentation followed by a distinction between manipulator and object solely by observing the motion between consecutive RGB frames. In contrast to previous approaches, we propose a single, end-to-end trainable architecture which jointly incorporates motion cues and semantic knowledge. Furthermore, while the motion of the manipulator and the object are substantial cues for our algorithm, we present means to robustly deal with distraction objects moving in the background, as well as with completely static scenes. Our method neither depends on any visual registration of a kinematic robot or 3D object models, nor on precise hand-eye calibration or any additional sensor data. By extensive experimental evaluation we demonstrate the superiority of our framework and provide detailed insights on its capability of dealing with the aforementioned extreme cases of motion. We also show that training a semantic segmentation network with the automatically labeled data achieves results on par with manually annotated training data. Code and pretrained model are available at https:\\/\\/github.com\\/DLR-RM\\/DistinctNet.'\",\"242\":null,\"243\":null,\"244\":null,\"245\":null,\"246\":null,\"247\":null,\"248\":\"\\\"In recent years, mobility on demand has experienced a major revival due to various ride-hailing companies entering the market. Competing in this field requires an efficient operation. Therefore, the applied policy, which cares for vehicle-to-customer assignment and vehicle repositioning, has to achieve good customer service and minimize cost while trying to keep the impact on the environment as low as possible. A promising approach is to coordinate the control of the entire fleet, which is foreseen to become even easier with the possibility of autonomous vehicles in mind. Anticipating future demand requires a good understanding of the spatiotemporal distributions of request origins and destinations, and the resulting imbalance between vehicle demand and availability. This results from a multitude of topological, demographic, and social effects, which are almost impossible to sufficiently capture in a handcrafted model of reasonable complexity. This can be circumvented by leveraging machine learning approaches. In this paper, an image-like representation of the city and its fleet's state is introduced. It is comprehensive and intuitive to use as input to convolutional neural networks, which in the past have already been proven to capture spatial relationships very well. This allows operating on realistic, full-sized traffic networks without greatly increasing the number of parameters the neural network has to learn and, hence, keeps the training effort low. Additionally, this state is combined with a similarly constructed repositioning action, reflecting a 2D distribution of a well-performing operational policy. This approach allows replacement of complex, handcrafted mathematical models by a single, compact, auto-encoder-like neural network.\\\"\",\"249\":\"'The paper proposes novel sampling strategies to compute the optimal path alteration of a surface vessel sailing in close quarters. Such strategy directly encodes the rule...'\\n\\n'The paper proposes novel sampling strategies to compute the optimal path alteration of a surface vessel sailing in close quarters. Such strategy directly encodes the rules for safe navigation at sea, by exploiting the concept of minimal ship domain to determine the compliant region where the path deviation is to be generated. The sampling strategy is integrated within the optimal rapidly-exploring random tree algorithm, which minimizes the length of the path deviation. Further, the feasibility of the path with respect to the steering characteristics of own ship is verified by ensuring that the position of the new waypoints respects the minimum turning radius of the vessel. The proposed sampling strategy brings a significant performance improvement both in terms of optimal cost, computational speed and convergence rate.'\",\"250\":null,\"251\":null,\"252\":null,\"253\":null,\"254\":null,\"255\":null,\"256\":null,\"257\":null,\"258\":null,\"259\":null,\"260\":null,\"261\":null,\"262\":null,\"263\":null,\"264\":null,\"265\":null,\"266\":null,\"267\":null,\"268\":null,\"269\":\"'The field of robotics will soon enter a new era where robots are no longer confined to production halls, but are prevalent in our day to day lives. Highly publicized demonstrations of companies such as Boston Dynamics paint a picture of a future where robots also trot around in our neighborhoods. Beside that, the academic community has invested vast amounts of resources into developing mobile platforms such as quadrupeds [1], [2], [3]. In contrast to their wheeled counterparts, they can safely and robustly navigate through not only flat, but also rough and challenging terrains [4]. This extended locomotive versatility comes with the price that these platforms are notoriously difficult to control and stabilize. While academia has been making significant contributions to an open-sourced quadruped ecosystem (e.g. [5]), industry decisively competes in an effort to commercialize new technologies as well. In particular, Boston Dynamics has been pioneering legged robotic platforms for decades, and recently made one of their platforms commercially available: The Spot\\u00ae robot [6] is a mid-sized quadruped with 360\\u00b0 vision and integrated obstacle avoidance. Its primary intended use is to perform autonomous inspection missions of industrial plants, construction sites or other facilities that pose a rather high level of danger to humans. Spot is designed to be stable, robust and easy to operate, allowing it to successfully navigate through diverse environments. This ability arguably makes it to one of the most sophisticated legged platforms currently available. However, being a commercial product, some of the technology built into Spot remains a trade secret, and is inaccessible to the user. Control commands, for example, are limited exclusively to body pose and velocity, while commands to individual legs or motors are impermissible. This design relieves the user from the burden of motion optimization and provides a safeguard from reckless operation. However, direct leg control is not enabled, and consequently, users have no means of regulating foot placement. In other words, the behavior of Spot must largely be treated as a black box.'\",\"270\":\"'While there exists many methods for manipulating rigid objects with parallel-jaw grippers, grasping with multi-finger robotic hands remains a quite unexplored research topic. Reasoning and planning collision-free trajectories on the additional degrees of freedom of several fingers represents an important challenge that, so far, involves computationally costly and slow processes. In this work, we present Multi-FinGAN, a fast generative multi-finger grasp sampling method that synthesizes high quality grasps directly from RGB-D images in about a second. We achieve this by training in an end-to-end fashion a coarse-to-fine model composed of a classification network that distinguishes grasp types according to a specific taxonomy and a refinement network that produces refined grasp poses and joint angles. We experimentally validate and benchmark our method against a standard grasp-sampling method on 790 grasps in simulation and 20 grasps on a real Franka Emika Panda. All experimental results using our method show consistent improvements both in terms of grasp quality metrics and grasp success rate. Remarkably, our approach is up to 20-30 times faster than the baseline, a significant improvement that opens the door to feedback-based grasp re-planning and task informative grasping. Code is available at https:\\/\\/irobotics.aalto.fi\\/multi-fingan\\/.'\",\"271\":null,\"272\":null,\"273\":null,\"274\":null,\"275\":null,\"276\":null,\"277\":null,\"278\":null,\"279\":null,\"280\":null,\"281\":null,\"282\":null,\"283\":null,\"284\":null,\"285\":null,\"286\":null,\"287\":null,\"288\":null,\"289\":null,\"290\":null,\"291\":null,\"292\":null,\"293\":\"'Self-collision detection and avoidance are essential for reactive control, in particular for dynamics robots equipped with legs or arms. Yet, only few control methods are able to handle such constraints, and it is often necessary to rely on path planning to define a collision-free trajectory that the controller would then track. In this paper, we introduce a combination of two lightweight, conservative and smooth models to generically handle self-collisions in robot control. For pairs of bodies that are far from one another on average (e.g. segments of distinct legs), we rely on a standard forward kinematics approach, using simplified geometries for which we provide analytical derivatives. For bodies that are moving close to one another, we propose to use a data-driven approach, with datasets generated thanks to a standard collision library. We then build a simple torque-based controller that can be implemented on top of any control law to prevent unexpected self-collision. This controller is meant to be implemented as a low-level protection, directly on the robot hardware. We also provide an open-source library to generate ANSI-C code for any robot model, experimented on the real quadruped Solo.'\",\"294\":null,\"295\":null,\"296\":null,\"297\":null,\"298\":null,\"299\":null,\"300\":null,\"301\":null,\"302\":null,\"303\":null,\"304\":null,\"305\":null,\"306\":null,\"307\":null,\"308\":null,\"309\":null,\"310\":null,\"311\":null,\"312\":null,\"313\":null,\"314\":\"'Model Predictive Control (MPC) is a prominent and well-established technique that combines continuous feedback with a lookahead strategy to synthesize stabilizing actions for a broad range of dynamical systems. Its ability to encode complex high-level tasks in simple and intuitive cost functions, while accounting for system constraints, has made it quite compelling in the robotics community. For instance, with regards to locomotion research, this approach has proven its effectiveness in generating dynamic motions for highly articulated underactuated machines such as humanoids [1], [2] or quadrupeds [3]\\u2013[7]. Fundamentally, MPC operates by repeatedly solving a finite-horizon optimal control problem (OCP) in a receding-horizon fashion. It is therefore clear that the quality of the resulting control law heavily relies on the underlying optimal control formulation and on the scheme used to solve it [8]. These two components dictate how much of the problem\\u2019s true complexity is captured in the formulation, in addition to the speed at which optimal trajectories are calculated. Direct Trajectory Optimization (TO) approaches transcribe the OCP through a time-discretization of the states and inputs; thereby transforming the infinite-dimensional optimization problem into a finite-dimensional one that could be solved with standard nonlinear programming (NLP) solvers. These optimization-based methods have drawn great interest due to their ability to naturally incorporate any form of path constraints. However, they typically carry a high computational burden that renders them inapplicable in real-time settings, with a few notable exceptions that tend to exploit the problem\\u2019s sparse structure [9], [10].'\",\"315\":\"'Optimal control is a popular approach to synthesize highly dynamic motion. Commonly, L2 regularization is used on the control inputs in order to minimize energy used and to ensure smoothness of the control inputs. However, for some systems, such as satellites, the control needs to be applied in sparse bursts due to how the propulsion system operates. In this paper, we study approaches to induce sparsity in optimal control solutions\\u2014namely via smooth L1 and Huber regularization penalties. We apply these loss terms to state-of-the-art Differential Dynamic Programming (DDP)-based solvers to create a family of sparsity-inducing optimal control methods. We analyze and compare the effect of the different losses on inducing sparsity, their numerical conditioning, their impact on convergence, and discuss hyperparameter settings. We demonstrate our method in simulation and hardware experiments on canonical dynamics systems, control of satellites, and the NASA Valkyrie humanoid robot. We provide an implementation of our method and all examples for reproducibility on GitHub.'\",\"316\":null,\"317\":null,\"318\":null,\"319\":null,\"320\":null,\"321\":null,\"322\":null,\"323\":null,\"324\":\"'In this paper, a robust RGB-D SLAM system is proposed to utilize the structural information in indoor scenes, allowing for accurate tracking and efficient dense mapping on a CPU. Prior works have used the Manhattan World (MW) assumption to estimate low-drift camera pose, in turn limiting the applications of such systems. This paper, in contrast, proposes a novel approach delivering robust tracking in MW and non-MW environments. We check orthogonal relations between planes to directly detect Manhattan Frames, modeling the scene as a Mixture of Manhattan Frames. For MW scenes, we decouple pose estimation and provide a novel drift-free rotation estimation based on Manhattan Frame observations. For translation estimation in MW scenes and full camera pose estimation in non-MW scenes, we make use of point, line and plane features for robust tracking in challenging scenes. Additionally, by exploiting plane features detected in each frame, we also propose an efficient surfel-based dense mapping strategy, which divides each image into planar and non-planar regions. Planar surfels are initialized directly from sparse planes in our map while non-planar surfels are built by extracting superpixels. We evaluate our method on public benchmarks for pose estimation, drift and reconstruction accuracy, achieving superior performance compared to other state-of-the-art methods. We will open-source our code in the future.'\",\"325\":null,\"326\":\"'Visual place recognition is the task of finding matchings of images that show the same place in the world. Combinations of appearance changes (e.g. changing illumination or weather) and geometric changes (e.g. viewpoint changes or occlusions) challenge existing approaches. Learning-based local image feature pipelines are a promising approach to this type of problem. We present a novel attentive feature pooling method that can be used to train a CNN to jointly detect and describe local image features. It can be trained on small or moderately sized datasets with weak supervision in a classification training setup (e.g. we use a set of 24k images of publicly available web-camera images in our experiments). We propose to use a joint loss function that combines the cross-entropy loss for the classification task with a mean squared error in order to increase the repeatability of feature detections. We show how the approach can be integrated in a place recognition pipeline and run experiments on several standard place recognition datasets. Despite the small training dataset, we demonstrate a 15% improvement in the average performance compared to the best of a number of compared state-of-the-art approaches, and, probably more importantly, a 3x improvement in the worst-case performance. Open source code is available.'\",\"327\":\"'Visual place recognition is the task of recognizing same places of query images in a set of database images. It is important for loop closure detection in SLAM and candidate selection for global localization. Many approaches in the literature perform computationally inefficient full image comparisons between queries and all database images. There is still a lack of suited methods for efficient place recognition that allow a fast, sparse comparison of only the most promising image pairs without any loss in performance. While this is partially given by approximate nearest neighbor (ANN) based methods, they trade speed for precision and additional memory consumption, and many cannot find arbitrary numbers of matching database images in case of loops in the database. In this paper, we propose a novel fast sequence-based method for efficient place recognition that can be applied online. It uses relocalization to recover from sequence losses, and exploits usually available but often unused intra-database similarities for a potential detection of all matching database images for each query in case of loops or stops in the database. We performed extensive experimental evaluations over five datasets and 21 sequence combinations, and show that our method outperforms two state-of-the-art approaches and even full image comparisons in many cases, while providing a good tradeoff between performance and percentage of evaluated image pairs. Code is available 1 .'\",\"328\":null,\"329\":null,\"330\":null,\"331\":\"'Structure from Motion (SfM) often fails to estimate accurate poses in environments that lack suitable visual features. In such cases, the quality of the final 3D mesh, which is contingent on the accuracy of those estimates, is reduced. One way to overcome this problem is to combine data from a monocular camera with that of a LIDAR. This allows fine details and texture to be captured while still accurately representing featureless subjects. However, fusing these two sensor modalities is challenging due to their fundamentally different characteristics. Rather than directly fusing image features and LIDAR points, we propose to leverage common geometric features that are detected in both the LIDAR scans and image data, allowing data from the two sensors to be processed in a higher-level space. In particular, we propose to find correspondences between 3D lines extracted from LIDAR scans and 2D lines detected in images before performing a bundle adjustment to refine poses. We also exploit the detected and optimized line segments to improve the quality of the final mesh. We test our approach on the recently published dataset, Newer College Dataset. We compare the accuracy and the completeness of the 3D mesh to a ground truth obtained with a survey-grade 3D scanner. We show that our method delivers results that are comparable to a state-of-the-art LIDAR survey while not requiring highly accurate ground truth pose estimates. We plan to release our code before publication.'\",\"332\":null,\"333\":null,\"334\":null,\"335\":null,\"336\":null,\"337\":null,\"338\":null,\"339\":null,\"340\":\"'Salient object detection (SOD) can directly improve the performance of tasks like obstacle detection, semantic segmentation and object recognition. Such tasks are important for robotic and other autonomous navigation systems. State-of-the-art SOD methodologies, provide improved performance by incorporating depth information, usually acquired using additional specialized sensors, e.g., RGB-D cameras. This introduces an overhead to the overall cost and flexibility of such systems. Nevertheless, the recent advances of machine learning, have provided models, capable of generating depth map approximations, given a single RGB image. In this work, we propose a novel monocular SOD (MonoSOD) methodology, based on a two-branch CNN autoencoder architecture capable of predicting depth maps and estimating saliency through a trainable refinement scheme. Its application on benchmark datasets, indicates that its performance is comparable to that of state-of-the-art SOD methods relying on RGB-D data. Therefore, it could be considered as a lower-cost alternative of such methods for future applications.'\",\"341\":null,\"342\":null,\"343\":null,\"344\":null,\"345\":null,\"346\":null,\"347\":null,\"348\":\"'Physical Human-Robot-Interaction (pHRI) is beneficial for communication in social interaction or to perform collaborative tasks but is also crucial for safety. While robotic devices embed sensors for this sole purpose, their design often is the results of a trade-off between technical capabilities and rarely considers human factors. We propose a novel approach to design and fabricate compliant Human-like artificial skin sensors for robots, with similar mechanical properties as human skin and capable of precisely detecting touch. Our artificial skin relies on the use of different silicone elastomers to replicate the human skin layers and comprises an embedded electrode matrix to perform mutual capacitance sensing. We present the sensor and describe its fabrication process which is scalable, low-cost and ensures flexibility, compliance and robustness. We introduce Muca, an open-source sensing development board and then evaluate the performance of the sensor.'\",\"349\":\"'In this work, a novel Dynamic Movement Primitive (DMP) formulation is proposed which supports reversibility, i.e. backwards reproduction of a learned trajectory. Apart from sharing all favourable properties of the original DMP, decoupling the teaching of position and velocity profiles and bidirectional drivability along the encoded path are also supported. Original DMP have been extensively used for encoding and reproducing a desired motion pattern in several robotic applications. However, they lack reversibility, which is a useful and expedient property that can be leveraged in many scenarios. The proposed formulation is analyzed theoretically and its practical usefulness is showcased in an assembly by insertion experimental scenario.'\\n\\n'The technological advancements in the field of robotics over the past few years have triggered a surge in research interest for incorporating robots more and more actively in everyday life. However, programming a robot to perform even simple tasks usually requires considerable amount of time and advanced robotic and programming knowledge. To facilitate this cause, the use of Programming by Demonstration (PbD) has been proposed for passing on human skills to a robot [1]. At the core of this approach is a model used for encoding the demonstrated trajectory encapsulating the desired motion pattern for performing a task. The properties of this model play a major role, as they determine whether the encoded motion is amenable to generalization, online adaption, reverse execution etc, depending on the target application.'\",\"350\":null,\"351\":null,\"352\":\"'Human Initiated Grasp Space Exploration Algorithm for an Underactuated Robot Gripper Using Variational Autoencoder'\\n\\n'Grasp planning and most specifically the grasp space exploration is still an open issue in robotics. This article presents an efficient procedure for exploring the grasp space of a multifingered adaptive gripper for generating reliable grasps given a known object pose. This procedure relies on a limited dataset of manually specified expert grasps, and use a mixed analytic and data-driven approach based on the use of a grasp quality metric and variational autoencoders. The performances of this method are assessed by generating grasps in simulation for three different objects. On this grasp planning task, this method reaches a grasp success rate of 99.91% on 7000 trials.'\",\"353\":null,\"354\":null,\"355\":null,\"356\":null,\"357\":null,\"358\":null,\"359\":null,\"360\":null,\"361\":null,\"362\":null,\"363\":null,\"364\":null,\"365\":null,\"366\":\"'OpenSYMORO: An open-source software package for symbolic modelling of robots'\",\"367\":null,\"368\":\"'We introduce a multi-functional robotic gripper equipped with a set of actions required for disassembly of electromechanical devices. The gripper consists of a robot arm with 5 degrees of freedom (DoF) for manipulation and a jaw gripper with a 1-DoF rotation joint and a 1-DoF closing joint. The system enables manipulation in 7 DoF and offers the ability to reposition objects in hand and to perform tasks that usually require bimanual systems. The sensor system of the gripper includes relative and absolute joint encoders, force and pressure sensors to provide feedback about interaction forces, a tool- mounted camera for screw detection and precise placement of the tool tip using image-based visual servoing. We present a data-driven method for estimating joint torques based on the output voltage and motor speed. Further, we provide methods for teaching disassembly actions based on human demonstration, their representation as movement primitives and execution based on sensory feedback. We provide quantitative results regarding positioning and torque estimation accuracy, disassembly success rate and qualitative results regarding the successful disassembly of hard disc drives.'\",\"369\":null,\"370\":\"'The detection of contextual anomalies is a challenging task for surveillance since an observation can be considered anomalous or normal in a specific environmental context. An unmanned aerial vehicle (UAV) can utilize its aerial monitoring capability and employ multiple sensors to gather contextual information about the environment and perform contextual anomaly detection. In this work, we introduce a deep neural network-based method (CADNet) to find point anomalies (i.e., single instance anomalous data) and contextual anomalies (i.e., context-specific abnormality) in an environment using a UAV. The method is based on a variational autoencoder (VAE) with a context sub-network. The context sub-network extracts contextual information regarding the environment using GPS and time data, then feeds it to the VAE to predict anomalies conditioned on the context. To the best of our knowledge, our method is the first contextual anomaly detection method for UAV-assisted aerial surveillance. We evaluate our method on the AU-AIR dataset in a traffic surveillance scenario. Quantitative comparisons against several baselines demonstrate the superiority of our approach in the anomaly detection tasks. The codes and data will be available at https:\\/\\/bozcani.github.io\\/cadnet.'\",\"371\":null,\"372\":null,\"373\":\"'Deep Neural Networks (NNs) have been widely utilized in contact-rich manipulation tasks to model the complicated contact dynamics. However, NN-based models are often difficult to decipher which can lead to seemingly inexplicable behaviors and unidentifiable failure cases. In this work, we address the interpretability of NN-based models by introducing the kinodynamic images. We propose a methodology that creates images from kinematic and dynamic data of contact-rich manipulation tasks. By using images as the state representation, we enable the application of interpretability modules that were previously limited to vision-based tasks. We use this representation to train a Convolutional Neural Network (CNN) and we extract interpretations with Grad-CAM to produce visual explanations. Our method is versatile and can be applied to any classification problem in manipulation tasks to visually interpret which parts of the input drive the model\\u2019s decisions and distinguish its failure modes, regardless of the features used. Our experiments demonstrate that our method enables detailed visual inspections of sequences in a task, and high-level evaluations of a model\\u2019s behavior. Code for this work is available at [1].'\",\"374\":null,\"375\":null,\"376\":null,\"377\":null,\"378\":\"'An underlying structure in several sampling-based methods for continuous multi-robot motion planning (MRMP) is the tensor roadmap (PR), which emerges from combining multiple PRM graphs constructed for the individual robots via a tensor product. We study the conditions under which the TR encodes a near-optimal solution for MRMP\\u2014satisfying these conditions implies near optimality for a variety of popular planners, including dRRT*, and the discrete methods M* and CBS when applied to the continuous domain. We develop the first finite-sample analysis of this kind, which specifies the number of samples, their deterministic distribution, and magnitude of the connection radii that should be used by each individual PRM graph, to guarantee near-optimality using the TR. This significantly improves upon a previous asymptotic analysis, wherein the number of samples tends to infinity. Our new finite sample-size analysis supports guaranteed high- quality solutions in practice within finite time. To achieve our new result, we first develop a sampling scheme, which we call the staggered grid, for finite-sample motion planning for individual robots, which requires significantly less samples than previous work. We then extend it to the much more involved MRMP setting which requires to account for interactions among multiple robots. Finally, we report on a few experiments that serve as a verification of our theoretical findings and raise interesting questions for further investigation.'\",\"379\":null,\"380\":null,\"381\":null,\"382\":null,\"383\":null,\"384\":null,\"385\":\"'In the field of visual ego-motion estimation for Micro Air Vehicles (MAVs), fast maneuvers stay challenging mainly because of the big visual disparity and motion blur. In the pursuit of higher robustness, we study convolutional neural networks (CNNs) that predict the relative pose between subsequent images from a fast-moving monocular camera facing a planar scene. Aided by the Inertial Measurement Unit (IMU), we mainly focus on translational motion. The networks we study have similar small model sizes (around 1.35MB) and high inference speeds (around 10 milliseconds on a mobile GPU). Images for training and testing have realistic motion blur. Departing from a network framework that iteratively warps the first image to match the second with cascaded network blocks, we study different network architectures and training strategies. Simulated datasets and a self-collected MAV flight dataset are used for evaluation. The proposed setup shows better accuracy over existing networks and traditional feature-point-based methods during fast maneuvers. Moreover, self-supervised learning outperforms supervised learning. Videos and open-sourced code are available at https:\\/\\/github. com\\/tudelft\\/PoseNet_Planar'\",\"386\":null,\"387\":\"'We propose novel solvers for estimating the egomotion of a calibrated camera mounted to a moving vehicle from a single affine correspondence via recovering special homographies. For the first, second and third classes of solvers, the sought plane is expected to be perpendicular to one of the camera axes. For the fourth class, the plane is orthogonal to the ground with unknown normal, e.g., it is a building facade. All methods are solved via a linear system with a small coefficient matrix, thus, being extremely efficient. Both the minimal and over-determined cases can be solved by the proposed solvers. They are tested on synthetic data and on publicly available real-world datasets. The novel methods are more accurate or comparable to the traditional algorithms and are faster when included in state-of-the-art robust estimators. The source code is publicly available[1].'\",\"388\":\"'Modeling and understanding the environment is an essential task for autonomous driving. In addition to the detection of objects, in complex traffic scenarios the motion of other road participants is of special interest. Therefore, we propose to use a recurrent neural network to predict a dynamic occupancy grid map, which divides the vehicle surrounding in cells, each containing the occupancy probability and a velocity estimate. During training, our network is fed with sequences of measurement grid maps, which encode the lidar measurements of a single time step. Due to the combination of convolutional and recurrent layers, our approach is capable to use spatial and temporal information for the robust detection of static and dynamic environment. In order to apply our approach with measurements from a moving ego-vehicle, we propose a method for ego-motion compensation that is applicable in neural network architectures with recurrent layers working on different resolutions. In our evaluations, we compare our approach with a state-of-the-art particle-based algorithm on a large publicly available dataset to demonstrate the improved accuracy of velocity estimates and the more robust separation of the environment in static and dynamic area. Additionally, we show that our proposed method for ego-motion compensation leads to comparable results in scenarios with stationary and with moving ego-vehicle.'\",\"389\":null,\"390\":null,\"391\":null,\"392\":null,\"393\":null,\"394\":null,\"395\":null,\"396\":null,\"397\":null,\"398\":null,\"399\":null,\"400\":null,\"401\":null,\"402\":null,\"403\":\"'Challenging manipulation tasks can be solved effectively by combining individual robot skills, which must be parameterized for the concrete physical environment and task at hand. This is time-consuming and difficult for human programmers, particularly for force-controlled skills. To this end, we present Shadow Program Inversion (SPI), a novel approach to infer optimal skill parameters directly from data. SPI leverages unsupervised learning to train an auxiliary differentiable program representation (\\\"shadow program\\\") and realizes parameter inference via gradient-based model inversion. Our method enables the use of efficient first-order optimizers to infer optimal parameters for originally non-differentiable skills, including many skill variants currently used in production. SPI zero-shot generalizes across task objectives, meaning that shadow programs do not need to be retrained to infer parameters for different task variants. We evaluate our methods on three different robots and skill frameworks in industrial and household scenarios. Code and examples are available at https:\\/\\/innolab.artiminds.com\\/icra2021.'\",\"404\":null,\"405\":null,\"406\":null,\"407\":null,\"408\":null,\"409\":null,\"410\":null,\"411\":null,\"412\":null,\"413\":null,\"414\":\"'With growing access to versatile robotics, it is beneficial for end users to be able to teach robots tasks without needing to code a control policy. One possibility is to...'\\n\\n'With growing access to versatile robotics, it is beneficial for end users to be able to teach robots tasks without needing to code a control policy. One possibility is to teach the robot through successful task executions. However, near-optimal demonstrations of a task can be difficult to provide and even successful demonstrations can fail to capture task aspects key to robust skill replication. Here, we propose a learning from demonstration (LfD) approach that enables learning of robust task definitions without the need for near-optimal demonstrations. We present a novel algorithmic framework for learning tasks based on the ergodic metric\\u2014a measure of information content in motion. Moreover, we make use of negative demonstrations\\u2014demonstrations of what not to do\\u2014and show that they can help compensate for imperfect demonstrations, reduce the number of demonstrations needed, and highlight crucial task elements improving robot performance. In a proof-of-concept example of cart-pole inversion, we show that negative demonstrations alone can be sufficient to successfully learn and recreate a skill. Through a human subject study with 24 participants, we show that consistently more information about a task can be captured from combined positive and negative (posneg) demonstrations than from the same amount of just positive demonstrations. Finally, we demonstrate our learning approach on simulated tasks of target reaching and table cleaning with a 7-DoF Franka arm. Our results point towards a future with robust, data-efficient LfD for novice users.'\",\"415\":null,\"416\":\"'When humans perform actions which involve sharing objects between one another, coordination and understanding are pivotal factors in a successful interaction. A core element in interaction situations is the need and the means of expressing intent. Intent can be communicated directly by comprehensive vocalized sentence or encoded as non-verbal cues through the body, head, or eye movements. Research in corticomuscular and intermuscular coherence in humans report the advantages of non-verbal communication in such interactions. For instance, the mirror neuron system found in humans (and other primates) may have the fundamental function of enabling the preparation of an appropriate complementary response to an observed action. It may explain how two individuals can become so attuned to cooperating in joint actions [1]. Synchronisation in motor coordination [2] is seen as a biological condition in order to improve efficiency and reliability in human-human interaction (HHI). Moreover, synchronisation between two agents is preferred, to two individuals systems, to achieve optimal motor control. Further research on psychology [3], cognition, and neuroscience [4], reinforces on the idea that social interaction adheres from synchronisation of lower-level elements [5].'\",\"417\":null,\"418\":null,\"419\":\"'The research and development of socially interactive robots is a complex challenge because of the wide variety of capabilities needed for effective social human-robot interactions (HRI). Many of these capabilities, including perception, dialog, and control, have state of the art methods and solutions, but combining those into a comprehensive and seamless interaction is still an open challenge. We describe HARMONI, a multi-modal, open-source tool for rapid social HRI development and deployment. HARMONI is centered around a ROS package for interaction development, including decision management and node orchestration. HARMONI systematically integrates with disparate functionalities needed to conduct a meaningful social human-robot interaction such as external cloud services, AI models, and modules for sensing, planning, and acting on a variety of platforms. HARMONI was applied to the QT robot platform and usability tests were conducted to evaluate the ease and speed of development and deployment. This paper describes the architecture and design of HARMONI and reports the results of a pilot study with novice users.'\\n\\n'Composing HARMONI: An Open-source Tool for Human and Robot Modular OpeN Interaction'\",\"420\":null,\"421\":null,\"422\":null,\"423\":null,\"424\":null,\"425\":null,\"426\":null,\"427\":null,\"428\":null,\"429\":null,\"430\":\"'GenGrid is a novel comprehensive open-source, distributed platform intended for conducting extensive swarm robotic experiments. The modular platform is designed to run sw...'\\n\\n'GenGrid is a novel comprehensive open-source, distributed platform intended for conducting extensive swarm robotic experiments. The modular platform is designed to run swarm robotics experiments that are compatible with different types of mobile robots ranging from Colias, Kilobot, and E-puck [1]\\u2013[4]. The platform offers programmable control over the experimental setup and its parameters and acts as a tool to collect swarm robot data, including localization, sensory feedback, messaging, and interaction. GenGrid is designed as a modular grid of attachable computing nodes that offers bidirectional communication between the robotic agent and grid nodes and within grids. The paper describes the hardware and software architecture design of the GenGrid system. Further, it discusses some common experimental studies covering multi-robot and swarm robotics to showcase the platform\\u2019s use. GenGrid of 25 homogeneous cells with identical sensing and communication characteristics with a footprint of 37.5 cm \\u00d7 37.5 cm, exhibits multiple capabilities with minimal resources. The open-source hardware platform is handy for running swarm experiments, including robot hopping based on multiple gradients, collective transport, shepherding, continuous pheromone deposition, and subsequent evaporation. The low-cost, modular, and open-source platform is significant in the swarm robotics research community, which is currently driven by commercial platforms that allow minimal modifications.'\",\"431\":null,\"432\":\"'State estimation, in particular estimation of the base position, orientation and velocity, plays a big role in the efficiency of legged robot stabilization. The estimation of the base state is particularly important because of its strong correlation with the underactuated dynamics, i.e. the evolution of center of mass and angular momentum. Yet this estimation is typically done in two phases, first estimating the base state, then reconstructing the center of mass from the robot model. The underactuated dynamics is indeed not properly observed, and any bias in the model would not be corrected from the sensors. While it has already been observed that force measurements make such a bias observable, these are often only used for a binary estimation of the contact state. In this paper, we propose to simultaneously estimate the base and the underactuation state by exploiting all measurements simultaneously. To this end, we propose several contributions to implement a complete state estimator using factor graphs. Contact forces altering the underactuated dynamics are pre-integrated using a novel adaptation of the IMU pre-integration method, which constitutes the principal contribution. IMU pre-integration is also used to estimate the positional motion of the base. Encoder measurements then participate to the estimation in two ways: by providing leg odometry displacements which contributes to the observability of IMU biases; and by relating the positional and centroidal states, thus connecting the whole graph and producing a tightly-coupled whole-body estimator. The validity of the approach is demonstrated on real data captured by the Solo12 quadruped robot.'\",\"433\":\"'Autonomous vehicles have to make safe and reliable decisions in highly dynamic and complex traffic environments. So far, the approaches proposed for tackling this challenge can be divided into three broad categories: rule-based systems, optimization-based (optimal control) systems and data-driven i.e machine learning-based systems. Rule-based systems [15], [24], [2], offer the advantage of greater control over their actions. However, defining a consistent set of rules that works in all possible situations under noisy observations is a difficult and error-prone procedure. Optimal control-based systems, like [29], [17], [3] and [4], which rely on constrained optimization and Model Predictive Control, encode the vehicle dynamics model directly in the planning module and are able to generate feasible and comfortable trajectories. They also typically offer sound solutions with mathematically backed up safety guarantees. However, due to the short optimization horizon, these approaches are not capable of making farsighted, globally optimal decisions [16]. Alternatively, purely machine learning-based approaches, while offering better generalization to unseen situations and learning from data, introduce safety concerns and reduce the transparency of the behavior of the system. Among machine learning methods, Reinforcement Learning (RL) has become increasingly popular in autonomous driving applications, due to the notable success in many application areas [14], [19], [20]. However, when applying RL to autonomous driving, an important challenge is to determine the level of control the agent should have over the vehicle. Most common are the discrete actions and continuous actions settings. In the discrete actions case (e.g. [1], [6], [31] and also [13], [7], [8], [9], [10]), the agent can choose from actions such as keep lane, lane-change to the left or lane-change to the right or fixed accelerating\\/decelerating steps. While the small and fixed action set leads to fast learning progress, the lane-change maneuvers are usually with fixed execution duration, resulting in a suboptimal, unnatural behavior in tight situations. On the other end of the spectrum, in the continuous actions setting, the agent learns to influence either the low-level steering wheel and gas\\/brake pedal control signals directly, or some kind of an abstraction thereof such as curvature or acceleration [27], [25], [26]. This endows the agent with maximal freedom of choice, but since the set of actions is infinite and the choices that the agent needs to make are typically very granular, it will need large amounts of good action sequences among the learning samples, slowing down learning considerably. In addition, the maximization of the action-value function in continuous off-policy RL methods is problematic. To address that, in [30], the maximization step is simplified by a learned proposal distribution which is used to sample the continuous action space. The approximate maximum is then taken as the maximizing action from the proposed sample-set.'\\n\\n'Well-established optimization-based methods can guarantee an optimal trajectory for a short optimization horizon, typically no longer than a few seconds. As a result, choosing the optimal trajectory for this short horizon may still result in a sub-optimal long-term solution. At the same time, the resulting short-term trajectories allow for effective, comfortable and provable safe maneuvers in a dynamic traffic environment. In this work, we address the question of how to ensure an optimal long-term driving strategy, while keeping the benefits of classical trajectory planning. We introduce a Reinforcement Learning based approach that coupled with a trajectory planner, learns an optimal long-term decision-making strategy for driving on highways. By online generating locally optimal maneuvers as actions, we balance between the infinite low-level continuous action space, and the limited flexibility of a fixed number of predefined standard lane-change actions. We evaluated our method on realistic scenarios in the open-source traffic simulator SUMO and were able to achieve better performance than the 4 benchmark approaches we compared against, including a random action selecting agent, greedy agent, high-level, discrete actions agent and an IDM-based SUMO-controlled agent.'\",\"434\":null,\"435\":null,\"436\":\"'Driving styles play a major role in the acceptance and use of autonomous vehicles. Yet, existing motion planning techniques can often only incorporate simple driving styles that are modeled by the developers of the planner and not tailored to the passenger. We present a new approach to encode human driving styles through the use of signal temporal logic and its robustness metrics. Specifically, we use a penalty structure that can be used in many motion planning frameworks, and calibrate its parameters to model different automated driving styles. We combine this penalty structure with a set of signal temporal logic formula, based on the Responsibility-Sensitive Safety model, to generate trajectories that we expected to correlate with three different driving styles: aggressive, neutral, and defensive. An online study showed that people perceived different parameterizations of the motion planner as unique driving styles, and that most people tend to prefer a more defensive automated driving style, which correlated to their self-reported driving style.'\",\"437\":null,\"438\":null,\"439\":null,\"440\":null,\"441\":null,\"442\":null,\"443\":null,\"444\":null,\"445\":null,\"446\":\"'For a dynamical system, safety is typically guaranteed by constraining the system states within a set defined a priori. A popular approach is to use control barrier functions (CBFs) that encode safety using a smooth function. However, typical constructions of the smooth function do not account for any notion of safety uncertainty for the system inside the safe set. Although, one can formulate uncertainty in the dynamics of the model in a CBF framework, observability of unmodeled dynamics is difficult, particularly in an online setting. Addressing these drawbacks, we present a novel formulation for synthesizing the CBF smooth function by taking into account safety uncertainty using online measurements of the system states. This uncertainty is encoded by computing the posterior variance using Gaussian processes conditioned on past measurement states. Our approach only requires observability of system states rather than the system dynamics. By incorporating safety uncertainty, the safe set can be dynamically expanded or compressed. This is achieved by computing a local safety map online at the present location and identifying samples with minimal safety exceeding the current safety limit. As more data is collected, the safety margin increases. Hence, these minimally safe exploratory samples can be used to expand the current safe set incrementally. We validate our approach experimentally by expanding an initial safe set, along x and y positions independently, for a quadrotor with safety. The experiment video can be seen at: https:\\/\\/youtu.be\\/9qvOf1UpRPw.'\",\"447\":null,\"448\":null,\"449\":\"'In this paper, we design a versatile multi-sensor aided inertial navigation system (MINS) that can efficiently fuse multi-modal measurements of IMU, camera, wheel encoder...'\\n\\n'In this paper, we design a versatile multi-sensor aided inertial navigation system (MINS) that can efficiently fuse multi-modal measurements of IMU, camera, wheel encoder, GPS, and 3D LiDAR along with online spatiotemporal sensor calibration. Building upon our prior work [1] \\u2013[3], in this work we primarily focus on efficient LiDAR integration in a sliding-window filtering fashion. As each 3D LiDAR scan contains a large volume of 3D points which poses great challenges for real-time performance, we advocate using plane patches, which contain the environmental structural information, extracted from the sparse LiDAR point cloud to update\\/calibrate the system efficiently. The proposed LiDAR plane patch processing algorithm (including extraction, data association, and update) is shown to be efficient and consistent. Both Extensive Monte-Carlo simulations and real-world datasets with large-scale urban driving scenarios have been used to verify the accuracy and consistency of the proposed MINS algorithm.'\\n\\n'Online localization is a fundamental prerequisite of autonomous vehicles. Many algorithms have been developed thus far to achieve a high-precision consistent 3D localization using different sensors. Multi-sensor fusion is often used to achieve this goal for a number of reasons including more reliable data outcomes, more coverage, applicability, and lower equipment cost though at a higher computation cost. Among all possible navigation sensors, IMU, camera, wheel encoder, GPS, and 3D LiDAR are appealing because of their sufficient information for 3D motion estimation and good accessibility to commercial products. While it appears to be straightforward in principle to fuse all of these sensors in order to achieve good localization performance, few work has shown to fuse sensors more than three types due to their different characteristics, increasing computation, asynchronicity, and calibration issues. Moreover, accurate online multi-sensor calibration is essential for optimal sensor fusion as it may change over time during navigation. As such, in this work, we develop an efficient multi-sensor aided inertial navigation system, MINS, an INS aided by multi-modal sensors including camera, wheel encoder, GPS, and 3D LiDAR with the online calibration of all involving sensors while taking into account their asynchronous nature, and achieving robust accurate real-time localization performance.'\",\"450\":null,\"451\":null,\"452\":null,\"453\":null,\"454\":null,\"455\":null,\"456\":null,\"457\":null,\"458\":\"'We explore how high-speed robot arm motions can dynamically manipulate ropes and cables to vault over obstacles, knock objects from pedestals, and weave between obstacles. In this paper, we propose a self-supervised learning framework that enables a UR5 robot to perform these three tasks. The framework finds a 3D apex point for the robot arm, which, together with a task-specific trajectory function, defines an arcing motion that dynamically manipulates the cable to perform a task with varying obstacle and target locations. The trajectory function computes minimum-jerk motions that are constrained to remain within joint limits and to travel through the 3D apex point by repeatedly solving quadratic programs to find the shortest and fastest feasible motion. We experiment with 5 physical cables with different thickness and mass and compare performance against two baselines in which a human chooses the apex point. Results suggest that a baseline with a fixed apex across the three tasks achieves respective success rates of 51.7 %, 36.7 %, and 15.0 %, and a baseline with human-specified, task-specific apex points achieves 66.7 %, 56.7 %, and 15.0 % success rate respectively, while the robot using the learned apex point can achieve success rates of 81.7 % in vaulting, 65.0 % in knocking, and 60.0 % in weaving. Code, data, and supplementary materials are available at https:\\/\\/sites.google.com\\/berkeley.edu\\/dynrope\\/home.'\",\"459\":\"https:\\/\\/github.com\\/DanielTakeshi\\/deformable-ravens\",\"460\":null,\"461\":null,\"462\":null,\"463\":null,\"464\":null,\"465\":null,\"466\":null,\"467\":null,\"468\":null,\"469\":null,\"470\":null,\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":null,\"476\":null,\"477\":\"'We investigate improving Monte Carlo Tree Search based solvers for Partially Observable Markov Decision Processes (POMDPs), when applied to adaptive sampling problems. We propose improvements in rollout allocation, the action exploration algorithm, and plan commitment. The first allocates a different number of rollouts depending on how many actions the agent has taken in an episode. We find that rollouts are more valuable after some initial information is gained about the environment. Thus, a linear increase in the number of rollouts, i.e. allocating a fixed number at each step, is not appropriate for adaptive sampling tasks. The second alters which actions the agent chooses to explore when building the planning tree. We find that by using knowledge of the number of rollouts allocated, the agent can more effectively choose actions to explore. The third improvement is in determining how many actions the agent should take from one plan. Typically, an agent will plan to take the first action from the planning tree and then call the planner again from the new state. Using statistical techniques, we show that it is possible to greatly reduce the number of rollouts by increasing the number of actions taken from a single planning tree without affecting the agent\\u2019s final reward. Finally, we demonstrate experimentally, on simulated and real aquatic data from an underwater robot, that these improvements can be combined, leading to better adaptive sampling. The code for this work is available at https:\\/\\/github.com\\/uscresl\\/AdaptiveSamplingPOMCP.'\",\"478\":null,\"479\":null,\"480\":null,\"481\":null,\"482\":null,\"483\":\"'Deep spatial autoencoders for visuomotor learning'\",\"484\":null,\"485\":null,\"486\":null,\"487\":null,\"488\":null,\"489\":null,\"490\":null,\"491\":null,\"492\":null,\"493\":null,\"494\":null,\"495\":null,\"496\":null,\"497\":null,\"498\":null,\"499\":null,\"500\":null,\"501\":null,\"502\":\"'No longer confined to factory-floor workcells repeating painstakingly hand-coded trajectories, robot arms today have been tasked with increasingly open-ended assignments such as \\\"put this shoe on the shelf\\\" or \\\"load the dish washer\\\", where the robots need to operate in unknown, unstructured environments potentially populated by humans. Naturally, the safety of such operations hinges upon the robot\\u2019s ability to reliably handle unplanned collisions between any part of itself and the environment.'\",\"503\":null,\"504\":null,\"505\":null,\"506\":null,\"507\":null,\"508\":null,\"509\":null,\"510\":null,\"511\":null,\"512\":null,\"513\":null,\"514\":null,\"515\":null,\"516\":null,\"517\":null,\"518\":null,\"519\":\"'We propose a methodology for robust, real-time place recognition using an imaging lidar, which yields image-quality high-resolution 3D point clouds. Utilizing the intensity readings of an imaging lidar, we project the point cloud and obtain an intensity image. ORB feature descriptors are extracted from the image and encoded into a bag-of-words vector. The vector, used to identify the point cloud, is inserted into a database that is maintained by DBoW for fast place recognition queries. The returned candidate is further validated by matching visual feature descriptors. To reject matching outliers, we apply PnP, which minimizes the reprojection error of visual features\\u2019 positions in Euclidean space with their correspondences in 2D image space, using RANSAC. Combining the advantages from both camera and lidar-based place recognition approaches, our method is truly rotation-invariant, and can tackle reverse revisiting and upside down revisiting. The proposed method is evaluated on datasets gathered from a variety of platforms over different scales and environments. Our implementation and datasets are available at https:\\/\\/git.io\\/image-lidar.'\",\"520\":null,\"521\":null,\"522\":null,\"523\":null,\"524\":null,\"525\":null,\"526\":null,\"527\":null,\"528\":null,\"529\":null,\"530\":null,\"531\":null,\"532\":null,\"533\":null,\"534\":null,\"535\":null,\"536\":null,\"537\":null,\"538\":null,\"539\":null,\"540\":null,\"541\":null,\"542\":null,\"543\":null,\"544\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34721939'\",\"545\":null,\"546\":null,\"547\":null,\"548\":\"'Using sensor data from multiple modalities presents an opportunity to encode redundant and complementary features that can be useful when one modality is corrupted or noi...'\\n\\n'Using sensor data from multiple modalities presents an opportunity to encode redundant and complementary features that can be useful when one modality is corrupted or noisy. Humans do this everyday, relying on touch and proprioceptive feedback in visually-challenging environments. However, robots might not always know when their sensors are corrupted, as even broken sensors can return valid values. In this work, we introduce the Crossmodal Compensation Model (CCM), which can detect corrupted sensor modalities and compensate for them. CMM is a representation model learned with self-supervision that leverages unimodal reconstruction loss for corruption detection. CCM then discards the corrupted modality and compensates for it with information from the remaining sensors. We show that CCM learns rich state representations that can be used for manipulation policies learned with reinforcement learning, even when input modalities are corrupted during policy rollout in ways not seen during training.'\",\"549\":null,\"550\":null,\"551\":null,\"552\":null,\"553\":null,\"554\":null,\"555\":null,\"556\":null,\"557\":null,\"558\":null,\"559\":null,\"560\":null,\"561\":null,\"562\":\"'Articulated body simulation is an indispensable component of robot motion planning and optimal control. Their governing dynamic equations, i.e. the recursive Newton- Euler\\u2019s equation [9], and discretization schemes have been studied for decades. However, efficient and accurate contact handling is still a challenging problem despite extensive recent studies [27], [34]. To predict robot motions under simultaneous Coulomb frictional contacts, the two most widely-used formulations are the linear-complementary problem (LCP) [29] and the maximal dissipation principle (MDP) [7]. From a computational perspective, LCP incurs an NP- hard problem while MDP identifies contact forces with the solution of a cheap-to-compute convex program. As reported by [8], MDP-based contact handler achieves the best stability and computational efficiency. Moreover, MDP can encode novel contact models as arbitrary convex wrench spaces, which enables learning contact models from data [33], [34].'\",\"563\":null,\"564\":null,\"565\":\"'The process of learning a manipulation task depends strongly on the action space used for exploration: posed in the incorrect action space, solving a task with reinforcement learning can be drastically inefficient. Additionally, similar tasks or instances of the same task family impose latent manifold constraints on the most effective action space: the task family can be best solved with actions in a manifold of the entire action space of the robot. Combining these insights we present LASER, a method to learn latent action spaces for efficient reinforcement learning. LASER factorizes the learning problem into two sub-problems, namely action space learning and policy learning in the new action space. It leverages data from similar manipulation task instances, either from an offline expert or online during policy learning, and learns from these trajectories a mapping from the original to a latent action space. LASER is trained as a variational encoder-decoder model to map raw actions into a disentangled latent action space while maintaining action reconstruction and latent space dynamic consistency. We evaluate LASER on two contact-rich robotic tasks in simulation, and analyze the benefit of policy learning in the generated latent action space. We show improved sample efficiency compared to the original action space from better alignment of the action space to the task space, as we observe with visualizations of the learned action space manifold. Additional details: pair.toronto.edu\\/laser'\",\"566\":null,\"567\":null,\"568\":null,\"569\":null,\"570\":null,\"571\":null,\"572\":null,\"573\":\"'Proposing grasp poses for novel objects is an essential component for any robot manipulation task. Planning six degrees of freedom (DoF) grasps with a single camera, however, is challenging due to the complex object shape, incomplete object information, and sensor noise. In this paper, we present a 6-DoF contrastive grasp proposal network (CGPN) to infer 6-DoF grasps from a single-view depth image. First, an image encoder is used to extract the feature map from the input depth image, after which 3-DoF grasp regions are proposed from the feature map with a rotated region proposal network. Feature vectors that within the proposed grasp regions are then extracted and refined to 6-DoF grasps. The proposed model is trained offline with synthetic grasp data. To improve the robustness in reality and bridge the simulation-to-real gap, we further introduce a contrastive learning module and variant image processing techniques during the training. CGPN can locate collision-free grasps of an object using a single-view depth image within 0.5 second. Experiments on a physical robot further demonstrate the effectiveness of the algorithm. The experimental videos are available at [1].'\",\"574\":null,\"575\":null,\"576\":null,\"577\":null,\"578\":null,\"579\":\"'Deep reinforcement learning (DRL) provides a promising way for learning navigation in complex autonomous driving scenarios. However, identifying the subtle cues that can indicate drastically different outcomes remains an open problem with designing autonomous systems that operate in human environments. In this work, we show that explicitly inferring the latent state and encoding spatial-temporal relationships in a reinforcement learning framework can help address this difficulty. We encode prior knowledge on the latent states of other drivers through a framework that combines the reinforcement learner with a supervised learner. In addition, we model the influence passing between different vehicles through graph neural networks (GNNs). The proposed framework significantly improves performance in the context of navigating T-intersections compared with state-of-the-art baseline approaches.'\",\"580\":null,\"581\":null,\"582\":null,\"583\":null,\"584\":null,\"585\":\"'Deep learning-based object pose estimators are often unreliable and overconfident especially when the input image is outside the training domain, for instance, with sim2real transfer. Efficient and robust uncertainty quantification (UQ) in pose estimators is critically needed in many robotic tasks. In this work, we propose a simple, efficient, and plug-and-play UQ method for 6-DoF object pose estimation. We ensemble 2\\u20133 pre-trained models with different neural network architectures and\\/or training data sources, and compute their average pair-wise disagreement against one another to obtain the uncertainty quantification. We propose four disagreement metrics, including a learned metric, and show that the average distance (ADD) is the best learning-free metric and it is only slightly worse than the learned metric, which requires labeled target data. Our method has several advantages compared to the prior art: 1) our method does not require any modification of the training process or the model inputs; and 2) it needs only one forward pass for each model. We evaluate the proposed UQ method on three tasks where our uncertainty quantification yields much stronger correlations with pose estimation errors than the baselines. Moreover, in a real robot grasping task, our method increases the grasping success rate from 35% to 90%. Video and code are available at https:\\/\\/sites.google.com\\/view\\/fastuq.'\",\"586\":null,\"587\":\"'We present an ensemble learning methodology that combines multiple existing robotic grasp synthesis algorithms and obtain a success rate that is significantly better than the individual algorithms. The methodology treats the grasping algorithms as \\\"experts\\\" providing grasp \\\"opinions\\\". An Ensemble Convolutional Neural Network (ECNN) is trained using a Mixture of Experts (MOE) model that integrates these opinions and determines the final grasping decision. The ECNN introduces minimal computational cost overhead, and the network can virtually run as fast as the slowest expert. We test this architecture using open-source algorithms in the literature by adopting GQCNN 4.0, GGCNN and a custom variation of GGCNN as experts and obtained a 6% increase in the grasp success on the Cornell Dataset compared to the best-performing individual algorithm. The performance of the method is also demonstrated using a Franka Emika Panda arm.'\",\"588\":null,\"589\":null,\"590\":null,\"591\":null,\"592\":null,\"593\":\"'Foot force sensors are widely used in humanoid robots for measuring GRFs and CoPs, which quantify the stability status of the system [1]. Many researchers and developers design planning and control algorithms for humanoid robots according to their foot sensory feedback in the studies of dynamic walking [2], self-balancing [3], push-and-recovery [4], etc. Additionally, many state estimation problems such as estimating center of mass (CoM) location [5] and external forces [6] in the dynamic motion of the humanoid robot require the integration of foot force data together with other in-body sensory data like an encoder or an IMU. Recent studies also show that high-precision foot force sensors can be used to identify the inertial properties of heavy objects for humanoid robot manipulation tasks [7].'\\n\\n'To function autonomously in the physical world, humanoid robots need high-fidelity sensing systems, especially for forces that cannot be easily modeled. Modeling forces in robot feet is particularly challenging due to static indeterminacy, thereby requiring direct sensing. Unfortunately, resolving forces in the feet of some smaller-sized humanoids is limited both by the quality of sensors and the current algorithms used to interpret the data. This paper presents light-weight, low-cost and open-source force-sensing shoes to improve force measurement for popular smaller-sized humanoid robots, and a method for calibrating the shoes. The shoes measure center of pressure (CoP) and normal ground reaction force (GRF). The calibration method enables each individual shoe to reach high measurement precision by applying known forces at different locations of the shoe and using a regularized least squares optimization to interpret sensor outputs. A NAO TM robot is used as our experimental platform. Experiments are conducted to compare the measurement performance between the shoes and the robot\\u2019s factory-installed force-sensing resistors (FSRs), and to evaluate the calibration method over these two sensing modules. Experimental results show that the shoes significantly improve CoP and GRF measurement precision compared to the robot\\u2019s built-in FSRs. Moreover, the developed calibration method improves the measurement performance for both our shoes and the built-in FSRs.'\",\"594\":null,\"595\":null,\"596\":null,\"597\":null,\"598\":null,\"599\":null,\"600\":null,\"601\":null,\"602\":null,\"603\":null,\"604\":null,\"605\":null,\"606\":null,\"607\":null,\"608\":null,\"609\":null,\"610\":null,\"611\":null,\"612\":null,\"613\":null,\"614\":null,\"615\":\"'We present a trajectory planning and control architecture for bipedal locomotion at a variety of speeds on a highly underactuated and compliant bipedal robot. A library of compliant walking trajectories are planned offline, and stored as compact arrays of polynomial coefficients for tracking online. The control implementation uses a floating-base inverse dynamics controller which generates dynamically consistent feedforward torques to realize walking using information obtained from the trajectory optimization. The effectiveness of the controller is demonstrated in simulation and on hardware for walking both indoors on flat terrain and over unplanned disturbances outdoors. Additionally, both the controller and optimization source code are made available on GitHub.'\",\"616\":null,\"617\":null,\"618\":null,\"619\":null,\"620\":null,\"621\":null,\"622\":null,\"623\":null,\"624\":\"'Semantic scene understanding is crucial for robust and safe autonomous navigation, particularly so in off-road environments. Recent deep learning advances for 3D semantic segmentation rely heavily on large sets of training data, however existing autonomy datasets either represent urban environments or lack multimodal off-road data. We fill this gap with RELLIS-3D, a multimodal dataset collected in an off-road environment, which contains annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A&M University, and presents challenges to existing algorithms related to class imbalance and environmental topography. Additionally, we evaluate the current state of the art deep learning semantic segmentation models on this dataset. Experimental results show that RELLIS-3D presents challenges for algorithms designed for segmentation in urban environments. This novel dataset provides the resources needed by researchers to continue to develop more advanced algorithms and investigate new research directions to enhance autonomous navigation in off-road environments. RELLIS-3D is available at https:\\/\\/github.com\\/unmannedlab\\/RELLIS-3D'\",\"625\":null,\"626\":null,\"627\":null,\"628\":null,\"629\":\"'Energy management is a critical aspect of risk assessment for Uncrewed Aerial Vehicle (UAV) flights, as a depleted battery during a flight brings almost guaranteed vehicle damage and a high risk of human injuries or property damage. Predicting the amount of energy a flight will consume is challenging as routing, weather, obstacles, and other factors affect the overall consumption. We develop a deep energy model for a UAV that uses Temporal Convolutional Networks to capture the time varying features while incorporating static contextual information. Our energy model is trained on a real world dataset and does not require segregating flights into regimes. We illustrate an improvement in power predictions by 29% on test flights when compared to a state-of-the-art analytical method. Using the energy model, we can predict the energy usage for a given trajectory and evaluate the risk of running out of battery during flight. We propose using Conditional Value-at-Risk (CVaR) as a metric for quantifying this risk. We show that CVaR captures the risk associated with worst-case energy consumption on a nominal path by transforming the output distribution of Monte Carlo forward simulations into a risk space. Computing the CVaR on the risk-space distribution provides a metric that can evaluate the overall risk of a flight before take-off. Our energy model and risk evaluation method can improve flight safety and evaluate the coverage area from a proposed takeoff location. The video and codebase are available at: [Video]a | [Code]b'\",\"630\":null,\"631\":null,\"632\":null,\"633\":null,\"634\":\"'Polyculture farming is a sustainable farming technique based on synergistic interactions between differing plant types that make them more resistant to diseases and pests and better able to retain water. Reduced uniformity can reduce use of pesticides, fertilizer, and water, but is more labor intensive and more challenging to automate. We describe a scaled physical testbed (1.5m\\u00d73.0m) that uses a high resolution camera and soil sensors to monitor polyculture plants to facilitate tuning of plant growth, companion effects, and irrigation parameters for a first-order garden simulator. We use this simulator to develop a novel seed placement algorithm that increases coverage and diversity, and a learned pruning policy. In simulation experiments, the seed placement algorithm yields 60% more coverage and 10% more diversity than random seed placement and the learned pruning policy runs 1000X faster than a procedural lookahead policy to achieve high leaf coverage and plant diversity on adversarial gardens that include plant species with diverse growth rates. These models and policies provide the groundwork for a fully-automated system under development. Code, datasets and supplementary material can be found at https:\\/\\/github.com\\/BerkeleyAutomation\\/AlphaGarden\\/.'\",\"635\":null,\"636\":null,\"637\":null,\"638\":null,\"639\":null,\"640\":null,\"641\":null,\"642\":\"https:\\/\\/github.com\\/yuqingd\\/sim2real2sim_rad\",\"643\":null,\"644\":null,\"645\":null,\"646\":null,\"647\":null,\"648\":null,\"649\":null,\"650\":null,\"651\":null,\"652\":null,\"653\":null,\"654\":null,\"655\":null,\"656\":null,\"657\":null,\"658\":\"'The paper proposes a multi-modal sensor fusion algorithm that fuses WiFi, IMU, and floorplan information to infer an accurate and dense location history in indoor environments. The algorithm uses 1) an inertial navigation algorithm to estimate a relative motion trajectory from IMU sensor data; 2) a WiFi-based localization API in industry to obtain positional constraints and geo-localize the trajectory; and 3) a convolutional neural network to refine the location history to be consistent with the floorplan. We have developed a data acquisition app to build a new dataset with WiFi, IMU, and floorplan data with ground-truth positions at 4 university buildings and 3 shopping malls. Our qualitative and quantitative evaluations demonstrate that the proposed system is able to produce twice as accurate and a few orders of magnitude denser location history than the current standard, while requiring minimal additional energy consumption. We will publicly share our code and models.'\",\"659\":null,\"660\":\"'While deep learning has dominated much of the computer vision literature in recent years, \\\"traditional\\\" filtering methods still perform better in localization problems that use one or more cameras, such as Visual Odometry (VO) and Visual Inertial Odometry (VIO). This is because the filters hard-code known nonlinear kinematic models that are difficult to learn from data, although deep neural networks are often used to learn feature representations of measured data. The Extended Kalman Filter (EKF) is the most common because it is so simple, even though complex non-linearities in the rotational dynamics and patent violation of the Gaussian assumption in the visual measurements remove all guarantees of convergence and accuracy. In practice, the EKF provides accurate state estimates and overconfident covariance estimates [1]. To improve the covariance estimates of VO problems, [2] and [3] learn time-dependent measurement noise covariances Q while [4] uses a deep convolutional network to correct and directly from images. On the other hand, [5], [6], [7], [8], [9], [10], [11] improve VIO covariance estimation by using filters specific to VIO. The accuracy of covariance estimates are evaluated by tabulating the percentage of timesteps where the estimation error is within the 1,2,3-\\u03c3 bounds dictated by the estimated covariance .'\",\"661\":null,\"662\":null,\"663\":null,\"664\":\"'In this work we present a simple end-to-end trainable machine learning system capable of realistically simulating driving experiences. This can be used for verification of self-driving system performance without relying on expensive and time-consuming road testing. In particular, we frame the simulation problem as a Markov Process, leveraging deep neural networks to model both state distribution and transition function. These are trainable directly from the existing raw observations without the need of any handcrafting in the form of plant or kinematic models. All that is needed is a dataset of historical traffic episodes. Our formulation allows the system to construct never seen scenes that unfold realistically reacting to the self-driving car\\u2019s behaviour. We train our system directly from 1,000 hours of driving logs and measure both realism, reactivity of the simulation as the two key properties of the simulation. At the same time we apply the method to evaluate performance of a recently proposed state-of-the-art ML planning system [1] trained from human driving logs. We discover this planning system is prone to previously unreported causal confusion issues that are difficult to test by non-reactive simulation. To the best of our knowledge, this is the first work that directly merges highly realistic data-driven simulations with a closed loop evaluation for self-driving vehicles. We make the data, code, and pre-trained models publicly available to further stimulate simulation development.'\",\"665\":null,\"666\":\"'An intelligent agent operating in the real-world must balance achieving its goal with maintaining the safety and comfort of not only itself, but also other participants within the surrounding scene. This requires jointly reasoning about the behavior of other actors while deciding its own actions as these two processes are inherently intertwined \\u2013 a vehicle will yield to us if we decide to proceed first at the intersection but will proceed first if we decide to yield. However, this is not captured in most self-driving pipelines, where planning follows prediction. In this paper we propose a novel data-driven, reactive planning objective which allows a self-driving vehicle to jointly reason about its own plans as well as how other actors will react to them. We formulate the problem as an energy-based deep structured model that is learned from observational data and encodes both the planning and prediction problems. Through simulations based on both real-world driving and synthetically generated dense traffic, we demonstrate that our reactive model outperforms a non-reactive variant in successfully completing highly complex maneuvers (lane merges\\/turns in traffic) faster, without trading off collision rate. Please see our supplementary document https:\\/\\/tinyurl.com\\/3nukpn5b for all additional details.'\",\"667\":null,\"668\":null,\"669\":\"'Programming dynamic, heterogeneous teams of robots to perform coordinated tasks is hard. Even simple tasks can require a daunting amount of logic. The program must include logic to handle the wide variety of scenarios that can occur while the program is executing. For example, the program must deal with the following questions: What robots are currently available for use? What are their capabilities? Where is each robot located? What is the geometry of their environment? Are they currently engaged in other tasks? Which robots can perform which parts of which tasks at the same time? What should be done when a robot is added to or deleted from the pool, or when a robot completes, or fails to complete, some part of a task? And so on. The essential logic of the program is soon overwhelmed by, and intertwined with, code to handle these matters.'\",\"670\":null,\"671\":null,\"672\":null,\"673\":null,\"674\":null,\"675\":null,\"676\":null,\"677\":null,\"678\":null,\"679\":null,\"680\":null,\"681\":null,\"682\":\"'Assisting surgeons with automation of surgical subtasks is challenging due to backlash, hysteresis, and variable tensioning in cable-driven robots. These issues are exacerbated as surgical instruments are changed during an operation. In this work, we propose a framework for automation of high- precision surgical subtasks by learning local, sample-efficient, accurate, closed-loop policies that use visual feedback instead of robot encoder estimates. This framework, which we call deep Intermittent Visual Servoing (IVS), switches to a learned visual servo policy for high-precision segments of repetitive surgical tasks while relying on a coarse open-loop policy for the segments where precision is not necessary. We train the policy using only 180 human demonstrations that are roughly 2 seconds each. Results on a da Vinci Research Kit suggest that combining the coarse policy with half a second of corrections from the learned policy during each high-precision segment improves the success rate on the Fundamentals of Laparoscopic Surgery peg transfer task from 72.9% to 99.2%, 31.3% to 99.2%, and 47.2% to 100.0% for 3 instruments with differing cable properties. In the contexts we studied, IVS attains the highest published success rates for automated surgical peg transfer and is significantly more reliable than previous techniques when instruments are changed. Supplementary material is available at https:\\/\\/tinyurl.com\\/ivs-icra.'\\n\\n'Laparoscopic surgical robots such as the da Vinci Research Kit (dVRK) [17] are challenging to accurately control using open-loop techniques because of the hysteresis, cable-stretch, and complex dynamics of their cable-driven joints [10], [33], [15]. Furthermore, encoders are typically located at the motors, far from the joints they control, making accurate state estimation challenging. Prior work addresses these issues by learning a model of robot dynamics from data [15], [39], [54] for accurate open-loop control or by learning control policies that directly command the robot to perform tasks [53]. However, these approaches tend to require many training samples, which can take a long time to collect on a physical robot. Additionally, learning a model of the robot\\u2019s dynamics requires accurate state estimation, which requires motion capturing techniques using fiducials [15], [39]. Also, the learned dynamics models can overfit to the specific cabling properties of individual instruments (see Section V-C). Because instrument changes are commonplace within and across surgeries, control strategies must be robust to these shifts in cabling properties.'\",\"683\":null,\"684\":null,\"685\":null,\"686\":null,\"687\":null,\"688\":null,\"689\":null,\"690\":null,\"691\":null,\"692\":null,\"693\":null,\"694\":null,\"695\":\"'In this paper, we address the task of interacting with dynamic environments where the changes in the environment are independent of the agent. We study this through the context of trapping a moving ball with a UR5 robotic arm. Our key contribution is an approach to utilize a static planner for dynamic tasks using a Dynamic Planning add-on; that is, if we can successfully solve a task with a static target, then our approach can solve the same task when the target is moving. Our approach has three key components: an off-the-shelf static planner, a trajectory forecasting network, and a network to predict robot\\u2019s estimated time of arrival at any location. We demonstrate the generalization of our approach across environments. More information and videos at https:\\/\\/mlevy2525.github.io\\/DynamicAddOn\\/.'\",\"696\":null,\"697\":null,\"698\":null,\"699\":\"'Fast autonomous motion in cluttered and unknown environments, such as forests, is highly dependent on low-latency obstacle avoidance strategies. In this context, this paper presents a motion planning strategy that relies on lattices for the fast computation of local paths that both avoid obstacles and follow a vector field that encodes the global robot task. Lattices are constructed in the sensor space and represent a set of search trees that can be quickly pruned in function of the detected obstacles. The remaining lattice trees are used to optimize a vector field-dependent functional, thus generating the best free local path that tracks the field. To illustrate the proposed approach, we present simulation and real-world experiments of a planar robot moving in a cluttered, forest-like environment.'\\n\\n'This work is based on three main pillars, namely: motion planning using vector fields, obstacle avoidance, and discretization of motion. Vector fields represent a simple way of providing the robot with a preferred direction of motion for each point in space. Vector fields can be easily computed with the use of artificial potential functions (APF) [16] or navigation functions [17]. However, using these techniques, which consider obstacles, the vector field would change every time the map of the environment changes. One strategy is then to construct the vector field ignoring the obstacles and solve obstacle avoidance in a lower level planner, as suggested by [14] and [9]. Thus, the vector field is used to encode the high-level specification of a task, which may be, for example, the periodic survey of a given curve [9]. Vector fields for curve circulation were proposed in [18].'\",\"700\":null,\"701\":null,\"702\":null,\"703\":null,\"704\":null,\"705\":\"'Several independent approaches exist for state estimation and control of multirotor unmanned aerial systems (UASs) that address specific and constrained operational conditions. This work presents a complete end-to-end pipeline that enables precise, aggressive and agile maneuvers for multirotor UASs under real and challenging outdoor environments. We leverage state-of-the-art optimal methods from the literature for trajectory planning and control, such that designing and executing dynamic paths is fast, robust and easy to customize for a particular application. The complete pipeline, built entirely using commercially available components, is made open-source and fully documented to facilitate adoption. We demonstrate its performance in a variety of operational settings, such as hovering at a spot under dynamic wind speeds of up to 5\\u2013 6m\\/s (12\\u201315mi\\/h) while staying within 12cm of 3D error. We also characterize its capabilities in flying high-speed trajectories outdoors, and enabling fast aerial docking with a moving target with planning and interception occurring in under 8s.'\",\"706\":null,\"707\":null,\"708\":null,\"709\":\"'CE-Net: Context Encoder Network for 2D Medical Image Segmentation'\",\"710\":null,\"711\":null,\"712\":null,\"713\":\"'Robust and accurate pose estimation in long-term localization is crucial to autonomous driving. In this paper, we dealt with absolute localization with a LiDAR feature map and multi-sensor measurements. We proposed a tightly-coupled fusion method with fixed-lag smoothing. A sliding window of recently maintained states is estimated by minimizing a joint cost function. This cost function includes residuals of global LiDAR registration and relative kinematic constraints from an IMU and wheel encoders. In addition, we enhance the robustness of our method by improving LiDAR registration. To achieve this goal, LiDAR feature maps with a hybrid of geometric and normal distribution features are constructed and exploited. The effectiveness of the proposed method is verified in several challenging test sequences over 200km. The experimental results demonstrate that the proposed method achieves accurate localization and high robustness in challenging scenarios even when the LiDAR observation is degraded.'\",\"714\":\"\\\"Modern LiDAR-SLAM (L-SLAM) systems have shown excellent results in large-scale, real-world scenarios. However, they commonly have a high latency due to the expensive data association and nonlinear optimization. This paper demonstrates that actively selecting a subset of features significantly improves both the accuracy and efficiency of an L-SLAM system. We formulate the feature selection as a combinatorial optimization problem under a cardinality constraint to preserve the information matrix's spectral attributes. The stochastic-greedy algorithm is applied to approximate the optimal results in real-time. To avoid ill-conditioned estimation, we also propose a general strategy to evaluate the environment's degeneracy and modify the feature number online. The proposed feature selector is integrated into a multi-LiDAR SLAM system. We validate this enhanced system with extensive experiments covering various scenarios on two sensor setups and computation platforms. We show that our approach exhibits low localization error and speedup compared to the state-of-the-art L-SLAM systems. To benefit the community, we have released the source code: https:\\/\\/ram-lab.com\\/file\\/site\\/m-loam.\\\"\",\"715\":null,\"716\":null,\"717\":null,\"718\":null,\"719\":null,\"720\":null,\"721\":null,\"722\":null,\"723\":null,\"724\":null,\"725\":null,\"726\":null,\"727\":null,\"728\":null,\"729\":\"'General object grasping is an important yet unsolved problem in the field of robotics. Most of the current methods either generate grasp poses with few DoF that fail to cover most of the success grasps, or only take the unstable depth image or point cloud as input which may lead to poor results in some cases. In this paper, we propose RGBD-Grasp, a pipeline that solves this problem by decoupling 7-DoF grasp detection into two sub-tasks where RGB and depth information are processed separately. In the first stage, an encoder-decoder like convolutional neural network Angle-View Net(AVN) is proposed to predict the SO(3) orientation of the gripper at every location of the image. Consequently, a Fast Analytic Searching(FAS) module calculates the opening width and the distance of the gripper to the grasp point. By decoupling the grasp detection problem and introducing the stable RGB modality, our pipeline alleviates the requirement for the high-quality depth image and is robust to depth sensor noise. We achieve state-of-the-art results on GraspNet-1Billion dataset compared with several baselines. Real robot experiments on a UR5 robot with an Intel Realsense camera and a Robotiq two-finger gripper show high success rates for both single object scenes and cluttered scenes. Our code and trained model are available at graspnet.net.'\",\"730\":null,\"731\":null,\"732\":null,\"733\":null,\"734\":\"'Principles of mechanical intelligence have been well developed in underactuated hands for grasping tasks, incorporating elastic and passive elements to generate self-adaptation for dealing with uncertainties [1]. Multiple hand designs have certainly been designed and implemented following these principles. Ma et al. [2] proposed an open source, low-cost, single-actuator, 3D-printed underactuated hand with four adaptive fingers. This hand shows the capability of grasping with compliant flexure joints, following ideas previously presented in [3]. An alternative to creating compliant underactuated hands is to use joints with locking mechanisms. Aukes et al. [4] proposed a hand design which is capable to lock individual joints. Then, by locking and unlocking, the hand can adopt grasp capabilities and configurations similar to a fully actuated hand. Moving away from traditional flexure joints, Bai and Rojas [5] presented a self-adaptive one-step 3D printed robotic gripper, where the joints are based on a teeth-guided compliant cross-four-bar linkage. This basic single-material, additive manufactured, underactuated hand increases the precision of robotic fingers by removing nonlinear characteristics of flexures. The Ocean One hand [6] is a tendon-driven robotic hand for deep-sea exploration. In this case, elastic finger joints and a spring transmission are leveraged to achieve a variety of adaptive power grasps. This last research is an example that shows why underactuated hands, with their simpler control and reduced hardware complexity, have become of great use for the robotics community and for diverse robotics applications.'\",\"735\":null,\"736\":\"'We present Pylot, a platform for autonomous vehicle (AV) research and development, built with the goal to allow researchers to study the effects of the latency and accuracy of their models and algorithms on the end-to-end driving behavior of an AV. This is achieved through a modular structure enabled by our high-performance dataflow system that represents AV software pipeline components (object detectors, motion planners, etc.) as a dataflow graph of operators which communicate on data streams using timestamped messages. Pylot readily interfaces with popular AV simulators like CARLA, and is easily deployable to real-world vehicles with minimal code changes.To reduce the burden of developing an entire pipeline for evaluating a single component, Pylot provides several state-of-the-art reference implementations for the various components of an AV pipeline. Using these reference implementations, a Pylot-based AV pipeline is able to drive a real vehicle, and attains a high score on the CARLA Autonomous Driving Challenge. We also present several case studies enabled by Pylot, including evidence of a need for context-dependent components, and per-component time allocation. Pylot is open source, with the code available at https:\\/\\/github.com\\/erdos-project\\/pylot.'\",\"737\":null,\"738\":null,\"739\":null,\"740\":null,\"741\":\"'Local planning is one of the key technologies for mobile robots to achieve full autonomy and has been widely investigated. To evaluate mobile robot local planning approaches in a unified and comprehensive way, a mobile robot local planning benchmark called MRPB 1.0 is newly proposed in this paper. The benchmark facilitates both motion planning researchers who want to compare the performance of a new local planner relative to many other state-of-the-art approaches as well as end users in the mobile robotics industry who want to select a local planner that performs best on some problems of interest. We elaborately design various simulation scenarios to challenge the applicability of local planners, including large-scale, partially unknown, and dynamic complex environments. Furthermore, three types of principled evaluation metrics are carefully designed to quantitatively evaluate the performance of local planners, wherein the safety, efficiency, and smoothness of motions are comprehensively considered. We present the application of the proposed benchmark in two popular open-source local planners to show the practicality of the benchmark. In addition, some insights and guidelines about the design and selection of local planners are also provided. The benchmark website [1] contains all data of the designed simulation scenarios, detailed descriptions of these scenarios, and example code.'\\n\\n'Stabilizing NMPC of wheeled mobile robots using open-source real-time software'\",\"742\":null,\"743\":null,\"744\":null,\"745\":null,\"746\":null,\"747\":null,\"748\":null,\"749\":null,\"750\":null,\"751\":null,\"752\":null,\"753\":null,\"754\":null,\"755\":null,\"756\":\"'The versatile nature of agile micro aerial vehicles (MAVs) poses fundamental challenges to the design of robust state estimation in various complex environments. Achieving high-quality performance in textureless scenes is one of the missing pieces in the puzzle. Previously proposed solutions either seek a remedy with visual loop closure or leverage RF localizability with inferior accuracy. None of them support accurate MAV state estimation in textureless scenes. This paper presents RFSift, a new state estimator that conquers the textureless challenge with RF-referenced monocular vision, achieving centimeter-level accuracy in textureless scenes. Our key observation is that RF and visual measurements are tied up with pose constraints. Mapping RF to feature quality and sift well-matched ones significantly improves accuracy. RFSift consists of 1) an RF-sifting algorithm that maps 3D UWB measurements to 2D visual features for sifting the best features; 2) an RF-visual-inertial sensor fusion algorithm that enables robust state estimation by leveraging multiple sensors with complementary advantages. We implement the prototype with off-the-shelf products and conduct large-scale experiments. The results demonstrate that RFSift is robust in textureless scenes, 10x more accurate than the state-of-the-art monocular vision system. The code of RFSift is available at https:\\/\\/github.com\\/weisgroup\\/RFSift.'\",\"757\":null,\"758\":null,\"759\":null,\"760\":null,\"761\":null,\"762\":null,\"763\":null,\"764\":null,\"765\":null,\"766\":null,\"767\":null,\"768\":null,\"769\":\"'Collaborative exploration in an unknown environment without external positioning under limited communication is an essential task for multi-robot applications. For inter-robot positioning, various Distributed Simultaneous Localization and Mapping (DSLAM) systems share the Place Recognition (PR) descriptors and sensor data to estimate the relative pose between robots and merge robots\\u2019 maps. As maps are constantly shared among robots in exploration, we design a map-based DSLAM framework, which only shares the submaps, eliminating the transfer of PR descriptors and sensor data. Our framework saves 30% of total communication traffic. For exploration, each robot is assigned to get much unknown information about environments with paying little travel cost. As the number of sampled points increases, the goal would change back and forth among sampled frontiers, leading to the downgrade in exploration efficiency and the overlap of trajectories. We propose an exploration strategy based on Multi-robot Multi-target Potential Field (MMPF), which can eliminate goal\\u2019s back-and-forth changes, boosting the exploration efficiency by 1.03 \\u00d7\\u223c1.62 \\u00d7 with 3 % \\u223c 40 % travel cost saved. Our SubMap-based Multi-robot Exploration method (SMMR-Explore) is evaluated on both Gazebo simulator and real robots. The simulator and the exploration framework are published as an open-source ROS project at https:\\/\\/github.com\\/efc-robot\\/SMMR-Explore.'\",\"770\":null,\"771\":null,\"772\":\"'Autonomous ultrasound (US) acquisition is an important yet challenging task, as it involves interpretation of the highly complex and variable images and their spatial relationships. In this work, we propose a deep reinforcement learning framework to autonomously control the 6-D pose of a virtual US probe based on real-time image feedback to navigate towards the standard scan planes under the restrictions in real-world US scans. Furthermore, we propose a confidence-based approach to encode the optimization of image quality in the learning process. We validate our method in a simulation environment built with real-world data collected in the US imaging of the spine. Experimental results demonstrate that our method can perform reproducible US probe navigation towards the standard scan plane with an accuracy of 4.91mm\\/4.65\\u00b0 in the intra-patient setting, and accomplish the task in the intra- and inter-patient settings with a success rate of 92% and 46%, respectively. The results also show that the introduction of image quality optimization in our method can effectively improve the navigation performance.'\",\"773\":null,\"774\":null,\"775\":null,\"776\":null,\"777\":null,\"778\":null,\"779\":null,\"780\":null,\"781\":null,\"782\":null,\"783\":\"'Differentiable simulators provide an avenue for closing the sim-to-real gap by enabling the use of efficient, gradient-based optimization algorithms to find the simulation parameters that best fit the observed sensor readings. Nonetheless, these analytical models can only predict the dynamical behavior of systems for which they have been designed. In this work, we study the augmentation of a novel differentiable rigid-body physics engine via neural networks that is able to learn nonlinear relationships between dynamic quantities and can thus model effects not accounted for in traditional simulators. Such augmentations require less data to train and generalize better compared to entirely data-driven models. Through extensive experiments, we demonstrate the ability of our hybrid simulator to learn complex dynamics involving frictional contacts from real data, as well as match known models of viscous friction, and present an approach for automatically discovering useful augmentations. We show that, besides benefiting dynamics modeling, inserting neural networks can accelerate model-based control architectures. We observe a ten-fold speedup when replacing the QP solver inside a model-predictive gait controller for quadruped robots with a neural network, allowing us to significantly improve control delays as we demonstrate in real-hardware experiments. We publish code, additional results and videos from our experiments on our project webpage at https:\\/\\/sites.google.com\\/usc.edu\\/neuralsim.'\",\"784\":\"'Although deep reinforcement learning (RL) has been successfully applied to a variety of robotic control tasks, it\\u2019s still challenging to apply it to real-world tasks, due to the poor sample efficiency. Attempting to overcome this shortcoming, several works focus on reusing the collected trajectory data during the training by decomposing them into a set of policy-irrelevant discrete transitions. However, their improvements are somewhat marginal since i) the amount of the transitions is usually small, and ii) the value assignment only happens in the joint states. To address these issues, this paper introduces a concise yet powerful method to construct Continuous Transition, which exploits the trajectory information by exploiting the potential transitions along the trajectory. Specifically, we propose to synthesize new transitions for training by linearly interpolating the consecutive transitions. To keep the constructed transitions authentic, we also develop a discriminator to guide the construction process automatically. Extensive experiments demonstrate that our proposed method achieves a significant improvement in sample efficiency on various complex continuous robotic control problems in MuJoCo and outperforms the advanced model-based \\/ model-free RL methods. The source code is available1.'\",\"785\":\"'Nowadays robot systems have been widely used in aviation, manufacturing, transportation, biology and many other areas. To enhance generality and scalability, a modern robot system uses various software programs to autonomously perform different kinds of tasks, such as map building, human- robot interaction and navigation. However, due to the risks of possible faults and errors (such as transient faults, configuration errors and code bugs), a robotic software program can inevitably crash in some cases, causing that the robot fails to perform the current task. This problem is especially serious for the robot systems working in unmanned environments (such as forests and deserts), because the developers cannot conveniently perform manual recovery. Thus, for robustness, the robot system should automatically and correctly recover the crashed software program to continue the failed task.'\",\"786\":null,\"787\":null,\"788\":null,\"789\":\"'AVs have received widespread attention from industry and academia in recent years. Highly accurate localization is an essential technology for AVs, because various modules, such as decision-making, planning and control, are heavily dependent on positioning. To achieve accurate localization, AVs are equipped with various sensors, such as GNSS, camera, LiDAR, IMU, wheel encoder, etc. Due to the expensive price of LiDAR, low-cost camera and IMU are more suitable for localization of commercial-level AVs.'\",\"790\":null,\"791\":null,\"792\":null,\"793\":null,\"794\":null,\"795\":null,\"796\":null,\"797\":\"'Depth completion aims to recover the dense depth map from sparse depth data and RGB image respectively. However, due to the huge difference between the multi-modal signal input, vanilla convolutional neural network and simple fusion strategy cannot extract features from sparse data and aggregate multi-modal information effectively. To tackle this problem, we design a novel network architecture that takes full advantage of multi-modal features for depth completion. An effective Pre-completion algorithm is first put forward to increase the density of the input depth map and to provide distribution priors. Moreover, to effectively fuse the image features and the depth features, we propose a multi-modal deep aggregation block that consists of multiple connection and aggregation pathways for deeper fusion. Furthermore, based on the intuition that semantic image features are beneficial for accurate contour, we introduce the deformable guided fusion layer to guide the generation of the dense depth map. The resulting architecture, called MDANet, outperforms all the stateof-the-art methods on the popular KITTI Depth Completion Benchmark, meanwhile with fewer parameters than recent methods. The code of this work will be available at https:\\/\\/github.com\\/USTC-Keyanjie\\/MDANet_ICRA2021.'\",\"798\":null,\"799\":null,\"800\":null,\"801\":null,\"802\":null,\"803\":null,\"804\":null,\"805\":null,\"806\":null,\"807\":null,\"808\":null,\"809\":\"'An Improved Magnetic Spot Navigation for Replacing the Barcode Navigation in Automated Guided Vehicles'\\n\\n'The barcode navigation based on QR (quick response) codes is widely employed in industrial logistics due to its accurate localization and flexible movement paths. However...'\\n\\n'The barcode navigation based on QR (quick response) codes is widely employed in industrial logistics due to its accurate localization and flexible movement paths. However, the regular repair of damaged barcodes and robot speed control when approaching the barcodes are required. In this study, we presented an improved magnetic spot navigation approach to replace the barcode navigation for automated guided vehicles (AGVs). The fusion of the high-precision magnetic tracking method and odometer based on AGV encoders can overcome the disadvantages of barcode navigation. The magnetic tracking approach provides the AGV pose relative to the nearest magnet spot, instead of the low-precision longitudinal and lateral measurement via a magnetic ruler. Besides, with the benefit of the adaptive weighted fusion algorithm, the distance between the adjacent barcode can be set from 500 to 1000 mm via magnetic spots. Experimental results show that the mean path accuracy and mean magnet spot localization accuracy of the improved magnetic spot navigation were 110 \\u00b1 30 mm and 14.5 \\u00b1 0.87 mm, respectively. The proposed approach provides a novel possibility for large-area and high-precision navigation in AGVs-based industrial logistics, especially for large outdoor scenarios.'\\n\\n'The two-dimensional (2D) barcode navigation based on QR (quick response) codes is widely employed in industrial logistics after the establishment of Amazon\\u2019s automated logistics warehouse [1]. KIVA robots, as a type of automated guided vehicles (AGVs), are guided by the fusion of inertial sensing and visual recognition of 2D codes, where the room map is equivalent to a large chessboard [2]. AGVs do not require a fixed route to reach all points. The primary advantages of barcode navigation include accurate localization, flexible movement paths, easy communication and control, no sound or light interference. Thus the efficiency of the logistics warehouse is significantly improved [3].'\",\"810\":null,\"811\":null,\"812\":null,\"813\":null,\"814\":\"'Although multi-person human pose estimation has made great progress in recent years, the challenges such as various scales of persons, occluded keypoints, and crowded backgrounds in complex scenes are still remained to be solved. In this paper, we propose a novel multi-level pose estimation network (MLPE) to learn multi-level features that can preserve both the strong semantic clues and spatial resolution for keypoint prediction and location. More specifically, a multi-level prediction network with a feature enhancement strategy is first proposed to learn multi-level features to achieve a good trade-off between the global context information and spatial resolution. We then build a high-resolution fine network to restore high spatial resolution information based on transposed convolutions to accurately locate the keypoints. We have conducted extensive experiments on the challenging MS COCO dataset, which has proved the effectiveness of our proposed method. Code \\u2020 and the experimental results are publicly online available for further research.'\",\"815\":null,\"816\":null,\"817\":null,\"818\":null,\"819\":null,\"820\":\"'Learning manipulation skills from observing human demonstration videos is a promising aspect for intelligent robotic systems. Recent advances in video to command provide an end-to-end approach to translate a video into robot plans. However, the general video captioning methods focus more on the understanding of the full frame, while they lack the consideration of the spatio-temporal features in videos. In this paper, we proposed the two-stream 2D\\/3D residual networks for robots to learn manipulation tasks from human demonstration videos. We integrate spatial features with 2D residual network and temporal features with 3D residual network as inputs for RNN layers. An encoder-decoder architecture is then used to encode the spatio-temporal features and sequentially generate the command words. Experimental results on an extended manipulation dataset show that our approach outperforms the state-of-the-art methods. Real-world experiments results on a Baxter robotic arm indicate that our method could produce more accurate commands from video demonstrations.'\",\"821\":null,\"822\":\"'Spatio-temporal information is key to resolve occlusion and depth ambiguity in 3D human pose estimation. Previous methods have focused on either temporal contexts or local-to-global architectures that embed fixed-length spatiotemporal information. To date, there have not been effective proposals to simultaneously and flexibly capture varying spatiotemporal sequences and effectively achieves real-time 3D human pose estimation. In this work, we improve the learning of kinematic constraints in the human skeleton: posture, local kinematic connections, and symmetry by modeling local and global spatial information via attention mechanisms. To adapt to single- and multi-frame estimation, the dilated temporal model is employed to process varying skeleton sequences. Also, importantly, we carefully design the interleaving of spatial semantics with temporal dependencies to achieve a synergistic effect. To this end, we propose a simple yet effective graph attention spatio-temporal convolutional network (GAST-Net) that comprises of interleaved temporal convolutional and graph attention blocks. Experiments on two challenging benchmark datasets (Human3.6M and HumanEva-I) and YouTube videos demonstrate that our approach effectively mitigates depth ambiguity and self-occlusion, generalizes to half upper body estimation, and achieves competitive performance on 2D-to-3D video pose estimation. Code, video, and supplementary information is available at: http:\\/\\/www.juanrojas.net\\/gast\\/'\",\"823\":null,\"824\":null,\"825\":null,\"826\":null,\"827\":null,\"828\":null,\"829\":\"'It is challenging to develop an online path planning algorithm for Ackermann-steering vehicles to find collision-free and kinematically-feasible paths, that is efficient for dense environments, adaptable to various environments, and suitable for environments with narrow passages. In this paper, we propose a kinematically constrained RRT-based path planning algorithm integrating with a trajectory parameter space (TP-space) with three novel improvements to meet the above requirements. In specific, we introduce a new way to choose candidate nodes to expand the tree for an RRT-based algorithm, which can significantly increase the success rate of the expansion and improve the efficiency of the algorithm. We also introduce a procedure to incrementally adjust the step size for the expansion, which enables the algorithm to automatically adapt to various environments. At last, we integrate rapidly-exploring random vines (RRV) with a TP-space to handle kinematic constraints and improve the performance of the algorithm to expand the tree through a narrow passage. We also prove that the algorithm is probabilistic complete and asymptotically near-optimal. An ablation study shows that all three improvements can notably improve the performance of the RRT-based path planning algorithm. We also evaluate the algorithm in various environments. The experimental results show that our algorithm achieves competitive performance compared with the state-of-the-art. The source code is available at https:\\/\\/github.com\\/PengJieb\\/fastbkrrt.'\",\"830\":null,\"831\":null,\"832\":\"'The surface electromyography (sEMG) signal-based human-machine interface (HMI) has been widely used for various scenarios of physical human-robot interaction. However, current HMIs based on bipolar myoelectric sensors are hindered by the limitations of global sEMG features, which are prone to variability and delay. In this letter, we define a HMI that takes advantage of the underlying neural information of spinal module activations from bipolar sEMG signals, inspired by recent findings of neural codes. Firstly, the spinal module activations are identified by the spiking trains of the muscle synergies extracted from bipolar sEMG signals. Secondly, we extract the information encoded in both firing rates and spike timings of the spinal module activation in a population coding manner, which follows the information encoding principle of neurons. Thirdly, we map the series of spinal module activations into gait phases, locomotion modes, joint moment and human identity in order to experimentally reveal the physiological information contained in the spinal module activations. The contained information and the benefit of our design are demonstrated and experimentally explained by the presented results and comparisons with the traditionally used global sEMG features. The proposed bipolar myoelectric sensor-enabled human-machine interface could contribute to various scenarios of physical human-robot interaction.'\",\"833\":null,\"834\":null,\"835\":null,\"836\":null,\"837\":null,\"838\":\"'Object detection in 3D with stereo cameras is an important problem in computer vision, and is particularly crucial in low-cost autonomous mobile robots without LiDARs. Nowadays, most of the best-performing frameworks for stereo 3D object detection are based on dense depth reconstruction from disparity estimation, making them extremely computationally expensive. To enable real-world deployments of vision detection with binocular images, we take a step back to gain insights from 2D image-based detection frameworks and enhance them with stereo features. We incorporate knowledge and the inference structure from real-time one-stage 2D\\/3D object detector and introduce a light-weight stereo matching module. Our proposed framework, YOLOStereo3D, is trained on one single GPU and runs at more than ten fps. It demonstrates performance comparable to state-of-the-art stereo 3D detection frameworks without usage of LiDAR data. The code will be published in https:\\/\\/github.com\\/Owen-Liuyuxuan\\/visualDet3D.'\",\"839\":null,\"840\":null,\"841\":null,\"842\":\"'An MR Safe Rotary Encoder Based on Eccentric Sheave and FBG Sensors'\\n\\n'MRI-guided robotic systems are emerging platforms for minimally invasive intervention because of high positioning accuracy and excellent tissue contrast. MR safe encoders...'\\n\\n'MRI-guided robotic systems are emerging platforms for minimally invasive intervention because of high positioning accuracy and excellent tissue contrast. MR safe encoders are critical components for closed-loop robotic control. This paper develops an MR safe absolute rotary encoder based on eccentric sheave and FBG sensors. The eccentric sheave transforms the rotational motion of the shaft to the bending deflection of the beam on which FBG sensors are integrated. A model is built by establishing the relationship of the kinematics of the sheave, the mechanical properties of the beam with unknown length, and the strain model of two Fiber Bragg Grating (FBG) sensors. A Pseudo-Rigid Body (PRB) 3R model is used to solve a set of constrained equations for accurate rotary encoding. A prototype is built to calibrate the parameters and validate the accuracy of the encoder and its MR compatibility. Results show that the maximum angular error is 1.6\\u00b0, and the RMS error is 0.46\\u00b0. MRI shows that no noticeable artifacts are observed, and the Signal to Noise Ratio (SNR) is not affected. The results demonstrate the potential of the proposed method for it to be integrated with MR safe robots with easy fabrication, compact structures, and continuous measurement.'\\n\\n'With the increasing sophistication and maturity of MRI in terms of both structural and functional assessment, its combined use for real-time image guided intervention with robotic assistance has drawn increased interest in recent years. Compared to Computed Tomography (CT) and ultrasound (US), Magnetic Resonance Imaging (MRI) can provide high-resolution soft tissue imaging, accurate ablation temperature monitoring, and intraoperative functional evaluation without the risk of ionizing radiation [1]. Recently, MRI-guided robotic systems have shown their ability to realize accurate robotic positioning with high-resolution image navigation [2], [3]. Pneumatic or hydraulic actuators [4]\\u2013[6] and optical sensors [7]\\u2013[10] working in the MR environment have been proposed. However, there are no suitable MR safe absolute rotary encoders available, which are essential for effective closed-loop control for MRI-guided robotic intervention.'\",\"843\":null,\"844\":null,\"845\":\"'Combining lidar in camera-based simultaneous localization and mapping (SLAM) is an effective method in improving overall accuracy, especially at outdoor large scale scenes. Recent development of low-cost lidars (e.g. Livox lidar) enable us to explore such SLAM systems with lower budget and higher performance. In this paper we propose CamVox by adapting Livox lidars into visual SLAM (ORB-SLAM2) by exploring the lidars\\u2019 unique features. Based on the unique scan pattern of Livox lidars, we propose an automatic lidar-camera calibration method that will work in uncontrolled scenes. The long depth detection range also benefit a more accurate mapping. Comparison of CamVox with visual SLAM (VINS-mono) and lidar SLAM (LOAM) are evaluated on the same dataset to demonstrate the performance. We open sourced our hardware, code and dataset on GitHub 1 .'\",\"846\":null,\"847\":null,\"848\":null,\"849\":null,\"850\":\"'Human motion prediction is essential in human-robot interaction. Current research mostly considers the joint dependencies but ignores the bone dependencies and their relationship in the human skeleton, thus limiting the prediction accuracy. To address this issue, we represent the human skeleton as a directed acyclic graph with joints as vertexes and bones as directed edges. Then, we propose a novel directed acyclic graph neural network (DA-GNN) that follows the encoder-decoder structure. The encoder is stacked by multiple encoder blocks, each of which includes a directed acyclic graph computational operator (DA-GCO) to update joint and bone attributes based on the relationship between joint and bone dependencies in the observed human states, and a temporal update operator (TUO) to update the temporal dynamics of joints and bones in the same observation. After progressively implementing the above update process, the encoder outputs the final update result, fed into the decoder. The decoder includes a directed acyclic graph-based gated recurrent unit (DAG-GRU) and a multi-layered perceptron (MLP) to predict future human states sequentially. To the best of our knowledge, this is the first time to introduce the relationship between bone and joint dependencies in human motion prediction. Our experimental evaluations on two datasets, CMU Mocap and Human 3.6m, prove that DA-GNN outperforms current models. Finally, we showcase the efficacy of DA-GNN in a realistic HRI scenario.'\",\"851\":null,\"852\":null,\"853\":\"'Exploiting Probabilistic Siamese Visual Tracking with a Conditional Variational Autoencoder'\\n\\n'Visual tracking is a fundamental capability for robots tasked with humans and environment interaction. However, state-of-the-art visual tracking methods are still prone to failures and are imprecise when applied to challenging stereos, and their results are generally confidence agonistic. These methods depend on an embedded deep learning model to provide deterministic features or regression maps. A deterministic output with low confidence can result in disastrous consequences and lacks evidence needed for subsequent operations. Moreover, training data ambiguities or noise in the observations (so-called data uncertainty) can also lead to inherent uncertainty. In this paper, we focus on exploiting probabilistic Siamese visual tracking with a conditional variational autoencoder (CVAE). First, we build a bridge between the Siamese architecture and the CVAE and propose a novel Bayesian visual tracking method. Second, the proposed method generates a complete probability distribution that enables the production of multiple plausible tracking outputs. Third, CVAE conditioned by ground truth data encodes a low-dimensional latent space and conducts noise-injection training to prevent overfitting. Our proposed tracking method outperformed the state-of-the-art trackers on the VOT2016, VOT2018 and TColor-128 datasets.'\",\"854\":null,\"855\":null,\"856\":null,\"857\":\"'Scene flow represents the motion of points in the 3D space, which is the counterpart of the optical flow that represents the motion of pixels in the 2D image. However, it is difficult to obtain the ground truth of scene flow in the real scenes, and recent studies are based on synthetic data for training. Therefore, how to train a scene flow network with unsupervised methods based on real-world data shows crucial significance. A novel unsupervised learning method for scene flow is proposed in this paper, which utilizes the images of two consecutive frames taken by monocular camera without the ground truth of scene flow for training. Our method realizes the goal that training scene flow network with real-world data, which bridges the gap between training data and test data and broadens the scope of available data for training. Unsupervised learning of scene flow in this paper mainly consists of two parts: (i) depth estimation and camera pose estimation, and (ii) scene flow estimation based on four different loss functions. Depth estimation and camera pose estimation obtain the depth maps and camera pose between two consecutive frames, which provide further information for the next scene flow estimation. After that, we used depth consistency loss, dynamic-static consistency loss, Chamfer loss, and Laplacian regularization loss to carry out unsupervised training of the scene flow network. To our knowledge, this is the first paper that realizes the unsupervised learning of 3D scene flow from monocular camera. The experiment results on KITTI show that our method for unsupervised learning of scene flow meets great performance compared to traditional methods Iterative Closest Point (ICP) and Fast Global Registration (FGR). The source code is available at: https:\\/\\/github.com\\/IRMVLab\\/3DUnMonoFlow.'\",\"858\":null,\"859\":null,\"860\":null,\"861\":null,\"862\":null,\"863\":\"'In this work, we present a lightweight, tightly-coupled deep depth network and visual-inertial odometry (VIO) system, which can provide accurate state estimates and dense depth maps of the immediate surroundings. Leveraging the proposed lightweight Conditional Variational Autoencoder (CVAE) for depth inference and encoding, we provide the network with previously marginalized sparse features from VIO to increase the accuracy of initial depth prediction and generalization capability. The compact representation of dense depth, termed depth code, can be updated jointly with navigation states in a sliding window estimator in order to provide the dense local scene geometry. We additionally propose a novel method to obtain the CVAE\\u2019s Jacobian which is shown to be more than an order of magnitude faster than previous works, and we additionally leverage First-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous works that rely on completely dense residuals, we propose to only provide sparse measurements to update the depth code and show through careful experimentation that our choice of sparse measurements and FEJs can still significantly improve the estimated depth maps. Our full system also exhibits state-of-the-art pose estimation accuracy, and we show that it can run in real-time with single-thread execution while utilizing GPU acceleration only for the network and code Jacobian.'\\n\\n'Accurate pose estimation and dense depth estimation are essential to a wide range of robotic applications such as obstacle avoidance and path planning. Recently, utilizing Conditional Variational Autoencoder (CVAE) networks as a differentiable manifold to compress dense depth maps into a far lower-dimensional subspace, has shown promising results [1] and even shown real-time performance when leveraging a desktop graphics card and incremental smoothing [2]. However, these methods only utilize camera information, and rely solely on the depth network to recover scale information from learned priors. Additionally, even the real-time method [2] still relies heavily on GPU acceleration to perform dense warping to extract whole-image photometric residuals, and to our knowledge, no work thus far has presented a method to efficiently calculate the network Jacobian.'\\n\\n'CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth'\",\"864\":null,\"865\":null,\"866\":null,\"867\":\"'Vehicle Re-identification (Re-ID) aims to retrieve all instances of query vehicle images present in an image pool. However viewpoint, illumination, and occlusion variations along with subtle differences between two unique images pose a significant challenge towards achieving an effective system. In this paper, we emphasize upon enhancing the performance of visual feature based ReID system by improving feature embedding quality and propose (1) an attention-guided hierarchical feature extractor (HFE) that leverages the structure of a backbone CNN to extract coarse and fine-grained features and (2) to train the proposed network within a hard negative adversarial framework that generates samples exhibiting extreme variations, encouraging the network to extract important distinguishing features across varying scales. To demonstrate the effectiveness of the proposed framework we use VERI-Wild, VRIC and Veri-776 datasets that exhibit extreme intra-class and minute inter-class differences and achieve state-of-the-art (SoTA) performance. Codes related to this paper are publicly available at https:\\/\\/github.com\\/PS06\\/VReID.'\",\"868\":\"'Voxel-based methods have been widely used in point cloud 3D object detection. These methods usually transform points into voxels while suffering from information loss during point cloud voxelization. To address this problem, we propose a novel one-stage Voxelization Information Compensation Network (VIC-Net), which has the ability of loss-free feature extraction. The whole framework consists of a point branch for geometry detail extraction and a voxel branch for efficient proposals generation. Firstly, PointNet++ is adopted to efficiently encode geometry structure features from the raw point clouds. Then based on the encoded point features, two Point2Voxel (P2V) feature fusion modules are proposed to fuse point features with a voxel backbone, including Local P2V and Multi-Scale P2V. The P2V modules respectively integrate local detail features and multi-scale semantic contexts into a sparse voxel backbone. Thirdly, an auxiliary reconstruction loss is employed on the point branch to explicitly guide the point backbone to be aware of real geometry structures. In addition, we extend VIC-Net to a two-stage approach, namely VIC-RCNN, which further utilizes the fine geometry features to refine object locations. Experiments on the KITTI dataset demonstrate that our proposed VIC-Net outperforms other onestage methods and our two-stage method VIC-RCNN achieves new state-of-the-art performance.'\",\"869\":null,\"870\":null,\"871\":null,\"872\":\"'Depth completion is an important task in computer vision and robotics applications, which aims at predicting accurate dense depth from a single RGB-LiDAR image. Convolutional neural networks (CNNs) have been widely used for depth completion to learn a mapping function from sparse to dense depth. However, recent methods do not exploit any 3D geometric cues during the inference stage and mainly rely on sophisticated CNN architectures. In this paper, we present a cascade and geometrically inspired learning framework for depth completion, consisting of three stages: view extrapolation, stereo matching, and depth refinement. The first stage extrapolates a virtual (right) view using a single RGB (left) and its LiDAR data. We then mimic the binocular stereo-matching, and as a result, explicitly encode geometric constraints during depth completion. This stage augments the final refinement process by providing additional geometric reasoning. We also introduce a distillation framework based on teacher-student strategy to effectively train our network. Knowledge from a teacher model privileged with real stereo pairs is transferred to the student through feature distillation. Experimental results on KITTI depth completion benchmark demonstrate that the proposed method is superior to state-of-the-art methods.'\",\"873\":\"'Image guided depth completion is the task of generating a dense depth map from a sparse depth map and a high quality image. In this task, how to fuse the color and depth modalities plays an important role in achieving good performance. This paper proposes a two-branch backbone that consists of a color-dominant branch and a depth-dominant branch to exploit and fuse two modalities thoroughly. More specifically, one branch inputs a color image and a sparse depth map to predict a dense depth map. The other branch takes as inputs the sparse depth map and the previously predicted depth map, and outputs a dense depth map as well. The depth maps predicted from two branches are complimentary to each other and therefore they are adaptively fused. In addition, we also propose a simple geometric convolutional layer to encode 3D geometric cues. The geometric encoded backbone conducts the fusion of different modalities at multiple stages, leading to good depth completion results. We further implement a dilated and accelerated CSPN++ to refine the fused depth map efficiently. The proposed full model ranks 1st in the KITTI depth completion online leaderboard at the time of submission. It also infers much faster than most of the top ranked methods. The code of this work is available at https:\\/\\/github.com\\/JUGGHM\\/PENet_ICRA2021.'\",\"874\":null,\"875\":null,\"876\":null,\"877\":null,\"878\":\"'Human activities are hugely restricted by COVID-19, recently. Robots that can conduct inter-floor navigation attract much public attention since they can substitute human workers to conduct the service work. However, current robots either depend on human assistance or elevator retrofitting, and fully autonomous inter-floor navigation is still not available. As the very first step of inter-floor navigation, elevator button segmentation and recognition hold an important position. Therefore, we release the first large-scale publicly available elevator panel dataset in this work, containing 3,718 panel images with 35,100 button labels, to facilitate more powerful algorithms on autonomous elevator operation. Together with the dataset, a number of deep learning based implementations for button segmentation and recognition are also released to benchmark future methods in the community. The dataset is available at https:\\/\\/github.com\\/zhudelong\\/elevator_button_recognition'\",\"879\":\"'Uncertainty estimation for point cloud semantic segmentation is to quantify the confidence degree for the predicted label of points, which is essential for decision-making tasks. This paper proposes a neighborhood spatial aggregation based method, NSA-MC dropout, to achieve efficient uncertainty estimation for point cloud semantic segmentation. Unlike the traditional uncertainty estimation method MC dropout de-pending on repeated inferences, our NSA-MC dropout achieves uncertainty estimation through one-time inference. Specifically, a space-dependent method is designed to sample the model many times by performing stochastic forward pass through the model just once, and it approximates the repeated inferences based sampling process in MC dropout. Besides, a neighborhood spatial aggregation module, called NSA, aggregates neighborhood probabilistic outputs for each point and works with space-dependent sampling to establish output distribution. Finally, we propose an uncertainty-aware framework NSA-MC dropout to capture the uncertainty of prediction results efficiently. Experimental results show that our method obtains comparable performance with MC dropout. More significantly, our NSA-MC dropout has little influence on the efficiency of semantic inference. It is much faster than MC dropout, and the inference time does not establish a coupling relation with the sampling times. Our code is available at https:\\/\\/github.com\\/chaoqi7\\/Uncertainty_Estimation_PCSS'\",\"880\":\"'Semantic Segmentation is a crucial component in the perception systems of many applications, such as robotics and autonomous driving that rely on accurate environmental perception and understanding. In literature, several approaches are introduced to attempt LiDAR semantic segmentation task, such as projection-based (range-view or birds-eye-view), and voxel-based approaches. However, they either abandon the valuable 3D topology and geometric relations and suffer from information loss introduced in the projection process or are inefficient. Therefore, there is a need for accurate models capable of processing the 3D driving-scene point cloud in 3D space. In this paper, we propose S3Net, a novel convolutional neural network for LiDAR point cloud semantic segmentation. It adopts an encoder-decoder backbone that consists of Sparse Intra-channel Attention Module (SIntraAM), and Sparse Inter-channel Attention Module (SInterAM) to emphasize the fine details of both within each feature map and among nearby feature maps. To extract the global contexts in deeper layers, we introduce Sparse Residual Tower based upon sparse convolution that suits varying sparsity of LiDAR point cloud. In addition, geo-aware anisotrophic loss is leveraged to emphasize the semantic boundaries and penalize the noise within each predicted regions, leading to a robust prediction. Our experimental results show that the proposed method leads to a large improvement (12%) compared to its baseline counterpart (MinkNet42 [1]) on SemanticKITTI [2] test set and achieves state-of-the-art mIoU accuracy of semantic segmentation approaches.'\",\"881\":\"'Recently, quadrotors are gaining significant attention in aerial transportation and delivery. In these scenarios, an accurate estimation of the external force is as essential as the six degree-of-freedom (DoF) pose since it is of vital importance for planning and control of the vehicle. To this end, we propose a tightly-coupled Visual-Inertial-Dynamics (VID) system that simultaneously estimates the external force applied to the quadrotor along with the six DoF pose. Our method builds on the state-of-the-art optimization-based Visual-Inertial system [1], with a novel deduction of the dynamics and external force factor extended from VIMO [2]. Utilizing the proposed dynamics and external force factor, our estimator robustly and accurately estimates the external force even when it varies widely. Moreover, since we explicitly consider the influence of the external force, when compared with VIMO [2] and VINS-Mono [1], our method shows comparable and superior pose accuracy, even when the external force ranges from neglectable to significant. The robustness and effectiveness of the proposed method are validated by extensive real-world experiments and application scenario simulation. We will release an open-source package of this method along with datasets with ground truth force measurements for the reference of the community.'\",\"882\":null,\"883\":\"'The learning-from-observation (LfO) framework aims to map human demonstrations to a robot to reduce programming effort. To this end, an LfO system encodes a human demonst...'\\n\\n'The learning-from-observation (LfO) framework aims to map human demonstrations to a robot to reduce programming effort. To this end, an LfO system encodes a human demonstration into a series of execution units for a robot, which are referred to as task models. Although previous research has proposed successful task-model encoders, there has been little discussion on how to guide a task-model encoder in a scene with spatio-temporal noises, such as cluttered objects or unrelated human body movements. Inspired by the function of verbal instructions guiding an observer\\u2019s visual attention, we propose a verbal focus-of-attention (FoA) system (i.e., spatiotemporal filters) to guide a task-model encoder. For object manipulation, the system first recognizes the name of a target object and its attributes from verbal instructions. The information serves as a where-to-look FoA filter to confine the areas in which the target object existed in the demonstration. The system then detects the timings of grasp and release that occurred in the filtered areas. The timings serve as a when-to-look FoA filter to confine the period of object manipulation. Finally, a task-model encoder recognizes the task models by employing the FoA filters. We demonstrate the robustness of the verbal FoA in attenuating spatio-temporal noises by comparing it with an existing action localization network. The contributions of this study are as follows: (1) to propose a verbal FoA for LfO, (2) to design an algorithm to calculate FoA filters from verbal input, and (3) to demonstrate the effectiveness of a verbal FoA in localizing an action by comparing it with a state-of-the-art vision system.'\",\"884\":null,\"885\":null,\"886\":null,\"887\":null,\"888\":null,\"889\":\"'Dense optical flow estimation plays a key role in many robotic vision tasks. In the past few years, with the advent of deep learning, we have witnessed great progress in optical flow estimation. However, current networks often consist of a large number of parameters and require heavy computation costs, largely hindering its application on low power-consumption devices such as mobile phones. In this paper, we tackle this challenge and design a lightweight model for fast and accurate optical flow prediction. Our proposed FastFlowNet follows the widely-used coarse-to-fine paradigm with following innovations. First, a new head enhanced pooling pyramid (HEPP) feature extractor is employed to intensify high-resolution pyramid features while reducing parameters. Second, we introduce a new center dense dilated correlation (CDDC) layer for constructing compact cost volume that can keep large search radius with reduced computation burden. Third, an efficient shuffle block decoder (SBD) is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on both synthetic Sintel data and real-world KITTI datasets demonstrate the effectiveness of the proposed approach, which needs only 1\\/10 computation of comparable networks to achieve on par accuracy. In particular, FastFlowNet only contains 1.37M parameters; and can execute at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images of resolution 1024 \\u00d7 436.Code is available at: https:\\/\\/git.io\\/fastflow'\\n\\n'Optical flow estimation is a fundamental task in computer vision and benefits a variety of downstream vision tasks, including target tracking [1], autonomous navigation [2], obstacle avoidance [3] and action-based human-robot interaction [4], [5]. Given two time-adjacent image frames, optical flow offers rich information for robotics to interact in complex dynamic environments, by estimating the projected 2D velocity field on the image plane, which is caused by relative 3D motion between an observer and a scene. However, extracting accurate motion information from RGB images is a complicated and computationally intensive task. Decades of research efforts have been spent on optical flow estimation. While traditional methods [6], [7], [8], [9] of optimizing an energy function with brightness constancy and spatial smoothness usually fail in large movement and illumination change cases, recent deep learning approaches significantly surpass them in both accuracy and speed thanks to large synthetic datasets and powerful GPUs. Existing Convolutional Neural Networks (CNNs) based architectures can be categorized into two classes: the encoder-decoder structure and the coarse-to-fine structure.'\",\"890\":null,\"891\":null,\"892\":null,\"893\":null,\"894\":null,\"895\":\"'Tactile perception on our fingers is a key sensory feedback that enables us to perceive and explore our world using our hands as probes, and is essential for efficient gripping and manipulation of objects. A tactile feedback system can therefore greatly improve the quality of life of individuals with partial or complete sensory loss like during stroke, or with artificial limbs after an amputation. However, most existing tactile texture feedback technologies suffer from two constraints. First, texture decoding and texture feedback have been traditionally examined separately and not as parts of the same problem, and second, texture information has been popularly fed back using sensory modality other than tactile itself. In this study, we propose a prototype on-line direct-texture decoding and feedback system in which the texture touched by a user is decoded using an accelerometer attached to the finger. The feedback is realized by rubbing the user\\u2019s skin with the actual material and the speed of the user swipes. The efficacy of the proposed system was tested in two user experiments with five test materials. The results and the corresponding hints for future improvements are discussed.'\",\"896\":null,\"897\":\"'The ability to estimate 3D object shape from a single image is vital to robotics and manufacturing. For instance, it enables iterative trial-and-error in simulated environments. In single-view reconstruction, implicit functions have demonstrated superior results over traditional methods. However, implicit functions suffer from the heavy computation of mesh extraction. This is due to the indirect mesh extraction, where the number of evaluation points grows cubically with resolution. On the other hand, reducing the resolution results in the discretization error of marching cubes (MC). In this work, we aim to perform efficient and accurate mesh extraction from implicit functions. The idea is to directly reconstruct the decision boundary of implicit functions as a mesh by reverse tracing from the output. It eliminates the need for evaluating massive points and error-prone MC. Consequently, we propose implementing an implicit function via a composite function of a flow and Binary-coded Input Neural Network (BCINN). The boundary of BCINN is easily identifiable, and the flow is invertible. Owing to these properties, the decision boundary of the composite function can be directly and efficiently reconstructed. In our experiments, we demonstrate that the proposed method significantly improves runtime\\/memory efficiency, with results comparable to those of existing methods. Specifically, our method enables real-time high-quality mesh inference from a single image.'\",\"898\":\"'Curb detection is an essential function of autonomous vehicles in urban areas. However, curbs are difficult to detect in complex urban environments in which many dynamic objects exist. Additionally, curbs appear in a variety of shapes and sizes. Previous studies have been based on the traditional pipeline, which consists of the extraction and aggregation of hand-crafted features that are then fed to classifiers. However, this sequential process is inefficient and designing the hand-crafted features is a complex process. Recently, this kind of process has been replaced by Deep Neural Networks (DNN), in which classifiers and features are learned from large-scale data. Very few works have exploited DNN for the curb detection problem. Most works use multi-modal sensor-based methods that combine images and accumulated 3D point clouds from LIDAR. However, these approaches require synchronization and calibration between sensors. In addition, they do not quantify the uncertainty of their predictions for autonomous system safety. In this paper, we present a two-stage DNN-based curb detection method that includes uncertainty quantification. An autoencoder-based network predicts the curbs, and then conditional neural processes rectify the predictions with uncertainty estimations. The experimental results show that our approach achieves high accuracy and recall in complex areas. We also constructed a large-scale dataset to create benchmarks consisting of approximately 5,224 scans with bird\\u2019s-eye view labels collected from urban areas. To the best of our knowledge, there are no public datasets for DNN-based curb detectors. The benchmarks and datasets are publicly available at https:\\/\\/github.com\\/YounghwaJung\\/curb_detection_DNN.'\",\"899\":\"'Accurate and fast inventory management algorithms are essential in the modern distribution industry. However, the configuration process of inventory management algorithms is very expensive, and the direct comprehensive management of inventory procedures is labor intensive and inaccurate. Therefore, in this paper, we propose an optical character recognition (OCR)-based inventory management algorithm to resolve these practical issues. The main purpose of our inventory management algorithm is to automatically inspect whether a list of items and the actual items match. To this end, our method consists of three steps, namely, text detection, text recognition, and text matching. In addition, to expand our algorithm to real-world applications, we propose adversarial training to ensure robustness against various damaged images, including corruption, blur, and inappropriate viewpoints. To train the network, we construct a new inventory management dataset (IMD) consisting of 10,000 sheets in real retail store environments. We verify our algorithm on the public dataset and our new IMD. As a result, we experimentally demonstrate that our method is not only robust against various damaged images but is also easily applicable in both large and small scale distributions stores at a low cost. Our code and dataset is available at https:\\/\\/blog.airlab.re.kr\\/Deform-and-Recover\\/.'\\n\\n'It is very important to construct a systematic and accurate inventory management algorithm in the modern distribution industry. If a suitable inventory management algorithm is not established, it is difficult to accurately determine the logistics inventory, which directly affects production management and sales. However, it is very expensive to establish an accurate and fast inventory management algorithm. For example, a logistics management algorithm relying on a laser-based barcode reader exhibits a very high initial cost, and a laser-based barcode reader cannot recognize multiple barcodes, so this approach is not preferred in small-scale stores. Therefore, we propose an optical character recognition (OCR)-based inventory management algorithm to resolve this practical problem. Since the OCR-based inventory management algorithm is a camera-based algorithm, it does not require a sensor such as a laser because it only needs RGB information, and in contrast to a laser-based barcode reader, it scans the entire logistics at once. However, despite the advantages of this RGB information-based algorithm, RGB information contains a major disadvantage in that it is highly sensitive to various damaged images in the real-world [1]. To apply OCR techniques using RGB information in our inventory management algorithm, we propose a dataset considering real-world image corruption. Our dataset is composed of data collected in real-world distribution stores and therefore includes various illumination conditions, viewpoint changes, and motion blur levels. Based on our new dataset, we develop an OCR-based inventory management algorithm consisting of three major steps. First, we perform text detection to localize text regions. The second step is to apply text recognition to the detected text regions. To ensure that the OCR recognition model is robust against image spatial transformation, we establish a deform-and-recover (DAR) learning technique, which adversarially deforms the network input and recovers it through spatial transformer networks (STNs) [2]. In addition, we develop an iteration-based augmentation to cover illumination changes and motion blur. Finally, as shown in Fig. 1, the last step of our inventory management algorithm finds matched character in the logistics with recognized characters in the target. To our knowledge, this work is the first attempt to construct a dataset for a real-world OCR-based inventory management algorithm, and the main contributions of this paper are summarized below.'\",\"900\":null,\"901\":null,\"902\":null,\"903\":null,\"904\":null,\"905\":null,\"906\":null,\"907\":null,\"908\":null,\"909\":\"'We present a fast, scalable, and accurate Simultaneous Localization and Mapping (SLAM) system that represents indoor scenes as a graph of objects. Leveraging the observation that artificial environments are structured and occupied by recognizable objects, we show that a compositional and scalable object mapping formulation is amenable to a robust SLAM solution for drift-free large-scale indoor reconstruction. To achieve this, we propose a novel semantically assisted data association strategy that results in unambiguous persistent object landmarks and a 2.5D compositional rendering method that enables reliable frame-to-model RGB-D tracking. Consequently, we deliver an optimized online implementation that can run at near frame rate with a single graphics card, and provide a comprehensive evaluation against state-of-the-art baselines. An open-source implementation will be provided at https:\\/\\/github.com\\/rpl-cmu\\/object-slam.'\",\"910\":null,\"911\":null,\"912\":null,\"913\":null,\"914\":\"\\\"Radio Frequency Identification (RFID) is a form of wireless communication that uses radio frequency (RF) waves to transmit data. A basic RFID system comprises a reader and a tag. The reader transmits an interrogating signal which powers the tag and induces a current through the circuit via electromagnetic induction. A chip (IC) in the tag encodes data in the received signal using either frequency or amplitude modulation [1] and transmits it back to the reader. In a chipless RFID tag, the electronic IC is replaced by passive electrical components. While the lack of active components in the tag eliminates the ability to decode or interpret any incoming data, the passive components still influence the wireless power transfer between the tag and the reader, as parametric changes in the passive components induce a change in the transmission power. Decades of research works have documented various sensing modalities that convert a change in RFID physical parameters into a change in electrical parameters. Hence, depending on the tag's interaction with the environment and the sensing modality used, it is possible to use these chipless tags as wireless sensors [2].\\\"\",\"915\":null,\"916\":null,\"917\":null,\"918\":null,\"919\":null,\"920\":null,\"921\":null,\"922\":null,\"923\":null,\"924\":null,\"925\":null,\"926\":null,\"927\":null,\"928\":null,\"929\":null,\"930\":null,\"931\":\"'This paper aims to tackle the problem of referring image segmentation, which is targeted at reasoning the region of interest referred by a query natural language sentence. One key issue to address the referring image segmentation is how to establish the cross-modal representation for encoding the two modalities, namely, the query sentence and the input image. Most existing methods are designed to concatenate the features from each modality or to gradually encode the cross-modal representation concerning each word\\u2019s effect. In contrast, our approach leverages the correlation between the two modalities for constructing the cross-modal representation. To make the resulting cross-modal representation more discriminative for the segmentation task, we propose a novel mechanism of language-driven attention to encode the cross-modal representation for reflecting the attention between every single visual element and the entire query sentence. The proposed mechanism, named as Language-Driven Attention (LDA), first decouples the cross-modal correlation to channel-attention and spatial-attention and then integrates the two attentions for obtaining the cross-modal representation. The channel attention and the spatial attention respectively reveal how sensitive each channel or each pixel of a particular feature map is with respect to the query sentence. With a proper fusion of the two kinds of feature attention, the proposed LDA model can effectively guide the generation of the final cross-modal representation. The resulting representation is further strengthened for capturing the multi-receptive-field and multi-level-semantic for the intended segmentation. We assess our referring image segmentation model on four public benchmark datasets, and the experimental results show that our model achieves state-of-the-art performance'\",\"932\":\"'Real-time semantic segmentation is a challenging task as both accuracy and inference speed need to be considered simultaneously. In real-world applications, it is usually achieved by deploying a deep neural network in modern GPU device. However, most of the work focused on real-time semantic segmentation is designed by significantly reducing computation complexity and model size. There are other factors that have a significant impact on inference speed are overlooked, especially when the network is running in modern GPU device. In this paper, we focus on designing a GPU-efficient network as backbone for real-time semantic segmentation. Dense connectivity can preserve and accumulate feature maps of multiple receptive fields and is therefore ideal for semantic segmentation. Therefore, we design a GPU-efficient network (DenseENet) with dense connectivity. The proposed DenseENet shows an obvious advantage in balancing accuracy and inference speed in modern GPU device. Specifically, on Cityscapes test set, DenseENet with a simple FCN decoder achieves 75.2% mIoU with 83.6 FPS for an input of 1024 \\u00d7 2048 resolution and 73.6% mIoU with 132 FPS for an input of 768 \\u00d7 1536 resolution on a single GTX 1080Ti card.'\",\"933\":null,\"934\":\"'Addressing on monocular visual odometry problem, this paper presents a novel end-to-end network for estimation of camera ego-motion. The network learns the latent space of optical flow (OF) and models sequential dynamics so that the motion estimation is constrained by the relations between sequential images. We compute the OF field of consecutive images and extract the latent OF representation in a self-encoding manner. A Recurrent Neural Network is then followed to examine the OF changes, i.e., to conduct sequential learning. The extracted sequential OF latent space is used to compute the regression of the 6-dimensional pose vector. Particularly, we separately train the encoder in an unsupervised manner. By this means, we avoid non-convergence during the training of the whole network and allow more generalized and effective feature representation. Substantial experiments have been conducted on KITTI and Malaga datasets, and the results demonstrate that our model outperforms most learning-based VO approaches.'\",\"935\":null,\"936\":null,\"937\":null,\"938\":\"'Parametric models that represent layout in terms of scene attributes are an attractive avenue for road scene understanding in autonomous navigation. Prior works that rely only on ground imagery are limited by the narrow field of view of the camera, occlusions and perspective foreshortening. In this paper, we demonstrate the effectiveness of using aerial imagery as an additional modality to overcome the above challenges. We propose a novel architecture, Unified, that combines features from both aerial and ground imagery to infer scene attributes. We quantitatively evaluate on the KITTI dataset and show that our Unified model outperforms prior works. Since this dataset is limited to road scenes close to the vehicle, we supplement the publicly available Argoverse dataset with scene attribute annotations and evaluate on far-away scenes. We show both quantitatively and qualitatively, the importance of aerial imagery in understanding road scenes, especially in regions farther away from the ego-vehicle. All code, models, and data, including scene attribute annotations on the Argoverse dataset along with collected and processed aerial imagery, are available at https:\\/\\/bit.ly\\/2QsKNeR.'\",\"939\":\"'This paper presents a Dynamic Vision Sensor (DVS) based system for reasoning about high-speed motion. As a representative scenario we consider a robot at rest, reacting to a small, fast approaching object at speeds higher than 15 m\\/s. Since conventional image sensors at typical frame rates observe such an object for only a few frames, estimating the underlying motion presents a considerable challenge for standard computer vision systems and algorithms. We present a method motivated by how animals such as insects solve this problem with their relatively simple vision systems.Our solution takes the event stream from a DVS and first encodes the temporal events with a set of causal exponential filters across multiple time scales. We couple these filters with a Convolutional Neural Network (CNN) to efficiently extract relevant spatiotemporal features. The combined network learns to output both the expected time to collision of the object, as well as the predicted collision point on a discretized polar grid. These critical estimates are computed with minimal delay by the network in order to react appropriately to the incoming object. We highlight our system\\u2019s results with a toy dart moving at 23.4 m\\/s with a 24.73\\u00b0 error in \\u03b8, 18.4 mm average discretized radius prediction error, and 25.03% median time to collision prediction error.'\",\"940\":null,\"941\":null,\"942\":null,\"943\":null,\"944\":null,\"945\":\"'The analysis of facial expression is a very complex and challenging problem. Most researches for automated Facial Expression Recognition (FER) are mainly based on deep learning networks, rarely considering data imbalance. This paper commits to addressing the long-tail distribution problems among large-scale datasets in wild. Inspired by the continual learning method, we reconstruct multi-subsets first by randomly selecting from head classes and up-sampling tail classes. A pre-trained backbone is then introduced to learn general weights in a repeatedly train-prune fashion. Hereafter, our approach creatively trains a new classifier based on union parameters previously preserved and achieves an outperformance without extra parameters added in, using the gradual-prune technique. The results show that the independent training of classifiers has been a contributing factor. We successfully conduct this experiment with several classic networks, prove its effectiveness in training a deep network on imbalanced dataset. In the face of the poor performance in current FER, we find that domain knowledge is somehow affecting the accuracy of recognition by further exploring the obstacles from the image itself.Code available at https:\\/\\/github.com\\/Epicghx\\/FER'\",\"946\":\"'Recognizing human emotion\\/expressions automatically is quite an expected ability for intelligent robotics, as it can promote better communication and cooperation with humans. Current deep-learning-based algorithms may achieve impressive performance in some lab-controlled environments, but they always fail to recognize the expressions accurately for the uncontrolled in-the-wild situation. Fortunately, facial action units (AU) describe subtle facial behaviors, and they can help distinguish uncertain and ambiguous expressions. In this work, we explore the correlations among the action units and facial expressions, and devise an AU-Expression Knowledge Constrained Representation Learning (AUE-CRL) framework to learn the AU representations without AU annotations and adaptively use representations to facilitate facial expression recognition. Specifically, it leverages AU-expression correlations to guide the learning of the AU classifiers, and thus it can obtain AU representations without incurring any AU annotations. Then, it introduces a knowledge-guided attention mechanism that mines useful AU representations under the constraint of AU-expression correlations. In this way, the framework can capture local discriminative and complementary features to enhance facial representation for facial expression recognition. We conduct experiments on the challenging uncontrolled datasets to demonstrate the superiority of the proposed framework over current state-of-the-art methods. Codes and trained models are available at https:\\/\\/github.com\\/HCPLab-SYSU\\/AUE-CRL.'\",\"947\":null,\"948\":null,\"949\":null,\"950\":\"'Automatic surgical gesture recognition is fundamentally important to enable intelligent cognitive assistance in robotic surgery. With recent advancement in robot-assisted minimally invasive surgery, rich information including surgical videos and robotic kinematics can be recorded, which provide complementary knowledge for understanding surgical gestures. However, existing methods either solely adopt uni-modal data or directly concatenate multi-modal representations, which can not sufficiently exploit the informative correlations inherent in visual and kinematics data to boost gesture recognition accuracies. In this regard, we propose a novel online approach of multi-modal relational graph network (i.e., MRG-Net) to dynamically integrate visual and kinematics information through interactive message propagation in the latent feature space. In specific, we first extract embeddings from video and kinematics sequences with temporal convolutional networks and LSTM units. Next, we identify multi-relations in these multi-modal embeddings and leverage them through a hierarchical relational graph learning module. The effectiveness of our method is demonstrated with state-of-the-art results on the public JIGSAWS dataset, outperforming current uni-modal and multi-modal methods on both suturing and knot typing tasks. Furthermore, we validated our method on in-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK) platforms in two centers, with consistent promising performance achieved. Our code and data are released at: https:\\/\\/www.cse.cuhk.edu.hk\\/~yhlong\\/mrgnet.html.'\",\"951\":null,\"952\":\"'Visual Place Recognition (VPR) is a crucial component for long-term mobile robot autonomy. In this paper, we exploit a coarse-to-fine paradigm to recognize places. In particular, we first select candidate frames for each query image, and then check the spatial geometric relationship between the query and its candidate frames to determine the final place match. In the coarse match stage, we employ the deep learning network to extract global features that encode semantic information of images, then by comparing the similarity between features to obtain a candidate list of the query place. In the fine match stage, we propose an effective and efficient feature matching algorithm for real-time geometrical verification of candidate places, termed as local affine preserving matching (LAP). Extensive experimental results demonstrate that our LAP can significantly promote the VPR performance, and the proposed overall VPR method can achieve much better performance over the current state-of-the-art approaches.'\",\"953\":null,\"954\":null,\"955\":null,\"956\":null,\"957\":\"'We propose simple yet effective improvements in point representations and local neighborhood graph construction within the general framework of graph neural networks (GNNs) for 3D point cloud processing. As a first contribution, we propose to augment the vertex representations with important local geometric information of the points, followed by nonlinear projection using a MLP. As a second contribution, we propose to improve the graph construction for GNNs for 3D point clouds. The existing methods work with a k-NN based approach for constructing the local neighborhood graph. We argue that it might lead to reduction in coverage in case of dense sampling by sensors in some regions of the scene. The proposed methods aims to counter such problems and improve coverage in such cases. As the traditional GNNs were designed to work with general graphs, where vertices may have no geometric interpretations, we see both our proposals as augmenting the general graphs to incorporate the geometric nature of 3D point clouds. While being simple, we demonstrate with multiple challenging benchmarks, with relatively clean CAD models, as well as with real world noisy scans, that the proposed method achieves state of the art results on benchmarks for 3D classification (ModelNet40) , part segmentation (ShapeNet) and semantic segmentation (Stanford 3D Indoor Scenes Dataset). We also show that the proposed network achieves faster training convergence, i.e. \\u223c 40% less epochs for classification. The project details are available at https:\\/\\/siddharthsrivastava.github.io\\/publication\\/geomgcnn\\/'\",\"958\":null,\"959\":null,\"960\":null,\"961\":null,\"962\":null,\"963\":null,\"964\":null,\"965\":null,\"966\":null,\"967\":null,\"968\":null,\"969\":null,\"970\":null,\"971\":null,\"972\":null,\"973\":null,\"974\":null,\"975\":\"'Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots'\\n\\n'Object slip perception is essential for mobile manipulation robots to perform manipulation tasks reliably in the dynamic real-world. Traditional approaches to robot arms\\u2019 slip perception use tactile or vision sensors. However, mobile robots still have to deal with noise in their sensor signals caused by the robot\\u2019s movement in a changing environment. To solve this problem, we present an anomaly detection method that utilizes multisensory data based on a deep autoencoder model. The proposed framework integrates heterogeneous data streams collected from various robot sensors, including RGB and depth cameras, a microphone, and a force-torque sensor. The integrated data is used to train a deep autoencoder to construct latent representations of the multisensory data that indicate the normal status. Anomalies can then be identified by error scores measured by the difference between the trained encoder\\u2019s latent values and the latent values of reconstructed input data. In order to evaluate the proposed framework, we conducted an experiment that mimics an object slip by a mobile service robot operating in a real-world environment with diverse household objects and different moving patterns. The experimental results verified that the proposed framework reliably detects anomalies in object slip situations despite various object types and robot behaviors, and visual and auditory noise in the environment.'\",\"976\":null,\"977\":null,\"978\":\"'This paper presents a navigation network based deep reinforcement learning framework for autonomous indoor robot exploration. The presented method features a pattern cognitive non-myopic exploration strategy that can better reflect universal preferences for structure. We propose the Extendable Navigation Network (ENN) to encode the partially observed high-dimensional indoor Euclidean space to a sparse graph representation. The robot\\u2019s motion is generated by a learned Q-network whose input is the ENN. The proposed framework is applied to a robot equipped with a 2D LIDAR sensor in the GAZEBO simulation where floor plans of real buildings are implemented. The experiments demonstrate the efficiency of the framework in terms of exploration time.'\",\"979\":\"'This paper addresses the localization of contacts of an unknown grasped rigid object with its environment, i.e., extrinsic to the robot. We explore the key role that distributed tactile sensing plays in localizing contacts external to the robot, in contrast to the role that aggregated force\\/torque measurements traditionally play in localizing contacts on the robot. When in contact with the environment, an object will move in accordance with the kinematic and possibly frictional constraints imposed by that contact. Small motions of the object, which are observable with tactile sensors, indirectly encode those constraints and the geometry that defines them.We formulate the extrinsic contact sensing problem as a constraint-based estimation problem. The estimation is subject to the kinematic constraints imposed by the tactile measurements of object motion, as well as the kinematic (e.g., non-penetration) and possibly frictional (e.g., sticking) constraints imposed by rigid-body mechanics. We validate the approach in simulation and with real experiments on the case studies of fixed point and line contacts.This paper discusses the theoretical basis for the value of distributed tactile sensing in contrast to aggregated force\\/torque measurements. It also provides an estimation framework for localizing environmental contacts with potential impact in contact-rich manipulation scenarios such as assembling or packing.'\",\"980\":null,\"981\":null,\"982\":null,\"983\":null,\"984\":null,\"985\":null,\"986\":null,\"987\":null,\"988\":\"'Trajectory generation is a fundamental topic in the robotics community that deals with the calculation of a time-optimal, smooth, jerk-limited, and accurate motion for a well-defined task. Manually programming and optimizing paths for complex robot systems is no longer viable when it comes to flexible production with small lot sizes and multiple robot manipulators, a common use case in small and medium-sized enterprises [1]. In order to quickly adapt to new processes, new paths have to be generated automatically by modern path planning algorithms that are able to calculate complex motions for multiple manipulators in a narrow space. As cycle times should be as short as possible, globally optimal path planning algorithms [2] present a considerable advantage over classical algorithms that are followed by a local optimization step. In order to avoid stopping at every waypoint in a path, supporting various forms of blending is a desired property in a trajectory generation algorithm to further increase the performance of a robot system. Kinodynamic path planning algorithms with velocity information however are proven to be PSPACE hard [3] and therefore lead to a large increase in computation time. Industrial robot controllers and open-source implementations commonly support blending via linear parabolic motions and cubic spline interpolation. Trajectories based on linear parabolic blending, that only limit the acceleration, suffer from infinite jerk around the blend waypoints [4]. Although cubic spline interpolation can improve the smoothness of a path by limiting the jerk, it can result in a more significant deviation of the straight-line movement. This is especially important when calculating trajectories for position-based solutions in robot path planning. Trajectory generation without explicit error bounds in the path deviation can lead to undesired behavior. Deviating too far from the collision-free solution path can result in collisions. Based on these observations, we present two different approaches to generate a trajectory for following multiple waypoints. They support an explicit upper bound in deviation and are jerk-limited around the blended waypoints. In our previous work [4], we consider the situation of performing an accurate motion for a robot manipulator by forcing the trajectory to precisely pass through all waypoints, which are either manually specified or generated via a path planning algorithm. In this work, we extend this to a more general application by considering blending around the waypoints. In contrast to most state-of-the-art blending algorithms, the jerk limitation is followed throughout the trajectory. In the same way as Haschke et al. [5] and Kr\\u00f6ger et al. [6], the trapezoidal acceleration profile is used to generate the trajectory between two consecutive waypoints. As stated in [4], the trapezoidal acceleration profile increases the optimization complexity while considering phase synchronization. In comparison to our previous work [4], we relaxed the objective function by introducing two additional weights to control the distribution of acceleration, deceleration, and cruising phases, which reduces the optimization complexity and shows a better performance from the perspective of straight-line movement. We continue to utilize the principle of model predictive control [4] for optimizing all waypoints by decomposing them into many consecutive waypoint batches and bridging each two adjacent batches with an overlapping waypoint. In addition to the optimization approach, we present another new approach that combines the trapezoidal acceleration model with a high-degree polynomial to perform a blending trajectory in joint and Cartesian space. Notably, quaternion interpolation is integrated and extended to a high degree polynomial, which considers the angular jerk and results in a smooth quaternion trajectory.'\",\"989\":null,\"990\":null,\"991\":null,\"992\":null,\"993\":null,\"994\":null,\"995\":null,\"996\":null,\"997\":null,\"998\":null,\"999\":null,\"1000\":null,\"1001\":null,\"1002\":null,\"1003\":null,\"1004\":null,\"1005\":null,\"1006\":\"'Choosing the correct number of PEs and SIMD lanes for each layer becomes a design problem of balancing the FPGA\\u2019s resources, the pipeline\\u2019s efficiency (throughput and latency), and potentially the choice of layers in the BNN (i.e. task-related accuracy). The number of resources on the FPGA is limited, especially in the context of low-power prosthetics, making these aspects important in planning the deployment with a HW-BNN codesign approach.'\",\"1007\":\"'Grasping unseen objects in unconstrained, cluttered environments is an essential skill for autonomous robotic manipulation. Despite recent progress in full 6-DoF grasp learning, existing approaches often consist of complex sequential pipelines that possess several potential failure points and run-times unsuitable for closed-loop grasping. Therefore, we propose an end-to-end network that efficiently generates a distribution of 6-DoF parallel-jaw grasps directly from a depth recording of a scene. Our novel grasp representation treats 3D points of the recorded point cloud as potential grasp contacts. By rooting the full 6-DoF grasp pose and width in the observed point cloud, we can reduce the dimensionality of our grasp representation to 4-DoF which greatly facilitates the learning process. Our class-agnostic approach is trained on 17 million simulated grasps and generalizes well to real world sensor data. In a robotic grasping study of unseen objects in structured clutter we achieve over 90% success rate, cutting the failure rate in half compared to a recent state-of-the-art method. Video of the real world experiments and code are available at https:\\/\\/research.nvidia.com\\/publication\\/2021-03_Contact-GraspNet%3A--Efficient.'\",\"1008\":\"'Each input image is scaled to the size of 300\\u00d7300 before being fed into the network. Meanwhile, the corresponding grasp labels are encoded for training and learning. Specifically, we use the form of sin(2\\u03b8) and cos(2\\u03b8) to represent the gripper\\u2019s angle and width to represent the opening and closing distance of the gripper. The center coordinate of the grasp box is obtained by searching for the position of the maximal grasp quality, where the pixel value of the corresponding area is set to 1, and the other pixels are set to 0. The proposed model is trained on an Nvidia RTX2080Ti GPU with an Adam optimizer where the batch size is set to 8, and the initial learning rate is set to 0.001. Moreover, our grasping algorithm is implemented by Pytorch 1.2.0.'\",\"1009\":\"'Several works use encoder\\/decoder based CNN architectures [23], [19] or dilated convolutions [24] for semantic segmentation. He et al. [25] performs the task of instance-specific semantic segmentation. In robotic vision, several methods [26], [27] predict semantic segmentation for unknown objects. Araki et al. [28] proposed a network for semantic segmentation and grasp detection for a suction cup using multi-task learning with a single deep neural network.'\",\"1010\":null,\"1011\":null,\"1012\":\"'Primitive: Software component that is specific to a certain model of device (or a certain implementation of an algorithm) and that facilitates the reuse of executable robot codes by exposing a uniform interface on all the devices (or algorithms) of the same type. Primitives are the building blocks of some hardware-abstraction layer, and they can be distinguished between:'\\n\\n'Skill: An executable robot code that exploits primitives to realize an action through a robot. This code may be the result of a hierarchical composition of other skills.'\",\"1013\":null,\"1014\":null,\"1015\":null,\"1016\":null,\"1017\":null,\"1018\":null,\"1019\":\"'The quadcopter trajectory tracking implementation is based on an open-source python simulator [31] and integrated with stable baselines [32] to create a custom quad- copter training environment.1 The experimental procedure can be broken into four steps. The low-level controllers are tuned for different operating conditions offering contrasting behaviour. The parameter bounds for the faults and disturbances and the reward function to train the agent are defined next. The agent is then trained using domain randomization on the bounded parameter space. Finally, the robustness improvement obtained through training is evaluated by comparing the performance of the low-level controllers on their own, a randomized high-level controller, and the presented hybrid approach after training.'\",\"1020\":\"'In adversarial scenarios like the one above, estimating risk is not a straightforward task. The risk of getting hit by a complex guided missile cannot be derived as a closed-form expression or hand-coded, instead it has to be learned from interaction with high fidelity simulation models. The risk measure we use is the future minimum separation between the aircraft and the missile, when the aircraft performs an optimal evasive maneuver, with large separations corresponding to lower risks. Thus we solve an RL problem to find such evasive maneuvers, using the smallest missile-aircraft distance measured over the entire missile trajectory as a reward, without any discounting. The value function of this RL problem then gives a measure of the agent\\u2019s vulnerability with respect to the adversarial aircraft. Finally, we use this value function as a Control Barrier Function (CBF) [1]\\u2013[3] when pursuing other objectives.'\\n\\n'The main contribution of this paper is that we show that a value function of an RL problem with reward only at the end of an episode, zero discounting, and a known system model, exhibits CBF properties. Thus we can create a CBF using RL as described above, in a way that makes the efficiency\\/risk trade-off transparent, in terms of allowing the operator to set and update the acceptable miss distance throughout the mission. Note that using RL to create the CBF in this way differs significantly from earlier combinations of RL and CBF, where a hand-coded CBF is used to guarantee safety while running the RL algorithm.'\",\"1021\":\"'Multi-object tracking (MOT) enables mobile robots to perform well-informed motion planning and navigation by localizing surrounding objects in 3D space and time. Existing methods rely on depth sensors (e.g., LiDAR) to detect and track targets in 3D space, but only up to a limited sensing range due to the sparsity of the signal. On the other hand, cameras provide a dense and rich visual signal that helps to localize even distant objects, but only in the image domain. In this paper, we propose EagerMOT, a simple tracking formulation that eagerly integrates all available object observations from both sensor modalities to obtain a well-informed interpretation of the scene dynamics. Using images, we can identify distant incoming objects, while depth estimates allow for precise trajectory localization as soon as objects are within the depth-sensing range. With EagerMOT, we achieve state-of-the-art results across several MOT tasks on the KITTI and NuScenes datasets. Our code is available at https:\\/\\/github.com\\/aleksandrkim61\\/EagerMOT'\",\"1022\":\"'Designing a Faster Region-based Convolutional Neural Networks (Faster R-CNN) algorithm to detect the surface properties of the environment that the robot is interacting with, using an integrated Wi-Fi camera, and optimization of the relevant code for implementation on an NVIDIA Jetson Xavier NX, as the processing unit. This arrangement provides the possibility of autonomous decision making on the suitable mode of attachment to the environment.'\",\"1023\":null,\"1024\":\"'A Fast Pose Estimation Method Based on New QR Code for Location of Indoor Mobile Robot'\",\"1025\":\"'Sample results. The upper row shows features and object tracks. Note how the network segmentations (framed in yellow) are not necessary in all frames. The lower row shows the masks propagated by DOT that encode the motion classification: in motion (color), static (black) and yet not observed (gray).'\",\"1026\":null,\"1027\":\"'Two representations of the surface points of a 4-fold symmetric box as color coded 3D-vectors (unwrapped): left plain object points, right the proposed closed symmetry loop or star representation. The right is continuous and respects symmetry, whereas the left does not.'\\n\\n'As the focus is on the output representation and the problem is rather simple, we use a canonical encoder-head architecture, details can be seen in the implementation. Our training dataset has images at \\u03c0\\/180 spaced angles, the test set at \\u03c0\\/900 spaced. We trained every CNN 11 times and report on the network with the median loss.'\",\"1028\":\"'Performance on test data in relation to available training data. Areas with available training data are coloured blue; for some parts of the state space no training data was provided intentionally in order to evaluate the influence. Performance is encoded in the smaller dots, with lighter colours corresponding to a higher degree of error.'\\n\\n'Performance on test data in relation to available training data. Areas with available training data are coloured blue. Performance is encoded in the smaller dots, with lighter colours corresponding to a higher degree of error.'\",\"1029\":\"'https:\\/\\/www.mucar3.de\\/icra2021-terrain-estimation'\\n\\n'Example for our static environment model consisting of a smooth terrain estimation (grid color coded by its height) and an obstacle map (red boxes). Note how the overhanging street sign is modeled in 2.5D (compared to the terrain map shown in Fig. 13a).'\",\"1030\":null,\"1031\":\"'The approach proposed in this paper has been implemented in C++ on an Intel Xeon CPU working at 3.70 GHz. Utilizing only a single core of the computer (without using GPU), the un-optimised code was able to achieve a runtime of 350 ms - 550 ms per frame while tracking deformations using the proposed approach, showing that it can be possible to achieve real-time performance at frame rate.'\",\"1032\":null,\"1033\":\"'https:\\/\\/youtu.be\\/dSJmoeVasI0'\\n\\n'The ability to simultaneously track and reconstruct multiple objects moving in the scene is of the utmost importance for robotic tasks such as autonomous navigation and interaction. Virtually all of the previous attempts to map multiple dynamic objects have evolved to store individual objects in separate reconstruction volumes and track the relative pose between them. While simple and intuitive, such formulation does not scale well with respect to the number of objects in the scene and introduces the need for an explicit occlusion handling strategy. In contrast, we propose a map representation that allows maintaining a single volume for the entire scene and all the objects therein. To this end, we introduce a novel multi-object TSDF formulation that can encode multiple object surfaces at any given location in the map. In a multiple dynamic object tracking and reconstruction scenario, our representation allows maintaining accurate reconstruction of surfaces even while they become temporarily occluded by other objects moving in their proximity. We evaluate the proposed TSDF++ formulation on a public synthetic dataset and demonstrate its ability to preserve reconstructions of occluded surfaces when compared to the standard TSDF map representation. Code is available at https:\\/\\/github.com\\/ethz-asl\\/tsdf-plusplus.'\",\"1034\":null,\"1035\":null,\"1036\":\"'Since ROS supports collaboration in robotics development, one can easily find a sufficient number of ROS packages for desired robotic applications. On the other hand, code reuse brings its own challenges. A developer of a package may very well know the behavior of each node, but a user may easily get confused while integrating the package to his robotic application. Neither a checking mechanism of ROS nodes for a desired behaviour nor a compatibility verification mechanism of ROS components with others in a robotic system is available. Not only the user but also the developer of a package might make logical mistakes while adding a new component to an existing system. In the best case, these mistakes can be caught during run-time tests. However, missed mistakes in a computational graph assembly [2] may result in serious failures or harm.'\\n\\n'The main contribution of this paper is to demonstrate formal verification of ROS based robotics software systems. For this purpose, we first formally encode the desired behaviour of each software component and then verify the correctness of these encoded properties by several means: 1) Checking computational graph consistency (idle publishers\\/subscribers, type consistency between publishers\\/subscribers, multiple publishers to the same topic etc...) 2) Building a new system by composing existing nodes 3) Selecting a compatible component for adding to a current system or 4) Testing if the system can follow a behaviour. We illustrate these methods on the robot Kobuki, extracting a model statically using HAROS or dynamically using runtime resources in ROS. We disclaim beforehand that the scope of this paper is formal verification only, and not the design of safety systems.'\\n\\n'In this section, we present encoding and verification of statically extracted models from source code. To this end, we use HAROS [22], [23] which is a tool to statically analyze ROS code and extract models from given launch files. The HAROS visualiser creates a graph for each extracted model. In this case, the components to be verified are the nodes of the extracted HAROS graph. They are encoded in linear logic to verify against desired properties, which describe how the system should be. If a given property can be proved, then it is verified that the behaviour is correct.'\\n\\n'We use the robot Kobuki\\u2019s source code to illustrate how to encode nodes in linear logic and then how to verify a property on the created model. Kobuki [24] is a mobile robot designed for the purpose of research and education, having a number of sensors and actuators. Its control system has three different components, from top to down level- kobuki_safety_controller which watches sensors such as bumper, cliff and wheel drop and acts accordingly; cmd_vel_mux which is a velocity command multiplexer for multiple incoming commands; and mobile base which is the mobile base for communicating with the robot. Another node, kobuki_keyop, provides the keyboard teleoperation command to the robot. The mobile_base is the central node which listens to incoming velocity commands and executes them. The cmd_vel_mux node assures that only one velocity command is relayed to the mobile base at a time.'\\n\\n'A. A Good Design Example for the Kobuki Robot\\\\nFig. 2 presents the Kobuki control system described above which is considered as a good design since its velocity muxer can handle multiple incoming velocity commands from multiple programs. We start with encoding all Kobuki nodes statically extracted with HAROS, and then proceed with proving a property. The HAROS model extracted is assumed to be implicitly accurate.\\\\nThe first node that we encode with linear logic is kobuki_safety controller, which subscribes to six topics-reset, disable, enable, wheel_drop, cliff and bumper; and publishes to two topics- safety_controller and cmd_vel. In Fig. 3, we present the HAROS graph for this node together with the input (subscriber) and output (publisher) topics with labels. We should note that to enable the controller for publishing, only one or none of these three topics can be valid - reset, disable, or enable. We encode this behavior as\\\\n(reset\\u2295disable\\u2295enable\\u22951),\\\\nView Source where the disjunction connectivity (\\u2295) is called external choice and allows only one of them to be valid. We use 1 to represent an empty resource, in the case when none of the other 3 resources are available. Together with other resources (topics), we encode the kobuki safety controller node as\\\\n(reset\\u2295disable\\u2295enable\\u22951)\\u2297wheel_drop\\u2297cliff\\u2297\\\\nbumper\\u22b8safety_controller\\u2297cmd_vel,\\\\nView Source where if all necessary subscriber topics, wheel_drop, cliff and bumper are provided, it can publish information to the topics, safety_controller and cmd_vel. These three subscriber topics are required since safety controller acts upon this information. If the bumper or cliff sensors are activated, the robot will go back and if the wheel drop sensor is activated, the controller will stop robot\\u2019s movement.\\\\nFig. 2:\\\\nA snapshot of the extracted graph with HAROS for a good design of the Kobuki robot. Solid green circles are topics, dashed green circles are conditional topics, and white circles are nodes.\\\\nShow All\\\\nFig. 3:\\\\nThe kobuki_safety_controller node with its publisher and subscriber topics. \\\"\\u0192\\\" here are unrecognised nodes namespaces, a shortcoming of HAROS.\\\\nShow All\\\\nSimilar to the kobuki_safety controller node, we present all node definitions in Fig. 4. For the sake of simplicity, we only show related resources of the mobile_base node.\\\\nWe now describe a property to illustrate the use of the linear logic encoding of nodes associated with the robot\\u2019s behaviour. Suppose we want to test the behavior of cmd_vel_mux node to see if it is working as we expect and relaying only one velocity to the mobile_base node. At a time, if the user gives a teleop command to the robot, the enable command is active and all sensors are sending information to the robot, the initial state of the this system can formally be encoded by the resource\\\\nteleop\\u2297enable\\u2297wheel_drop\\u2297cliff\\u2297bumper\\u2297led1\\u2208\\u0394,\\\\nView Source while the goal state can be encoded as\\\\nG=active\\u2297cmd_vel\\u2297wheel_drop\\u2297cliff\\u2297bumper,\\\\nView Source where safety controller publishes the velocity, multiplexer is active and once activated the mobile_base node updates all sensor values by publishing to related topics. In order to satisfy this goal, first the keyop node consumes the teleop resource and then creates new resources motor_power and keyboard_teleop. In parallel, the kobuki safety_controller node consumes the enable, wheel_drop, cliff and bumper resources and then creates the safety_controller and cmd_vel resources. Finally, the mobile_base node consumes the leftover resources and then creates new information for sensor resources. The proof tree corresponding to this example is given in Fig. 5. The proof starts with giving the node definitions, the initial state and the goal state by the judgment\\\\n\\u0393;\\u0394\\u21d2G.\\\\nView Source\\\\nWe should also note that, we shortly show the node definitions in \\u0393\\\\n\\u0393:={keyop,kobuki_safety_controller,cmd_vel_mux,mobile_base}\\\\n.\\\\nView Source\\\\nFig. 4:\\\\nEncoding of available nodes for a good design in the Kobuki domain within linear logic.\\\\nShow All'\\n\\n'The first node that we encode with linear logic is kobuki_safety controller, which subscribes to six topics-reset, disable, enable, wheel_drop, cliff and bumper; and publishes to two topics- safety_controller and cmd_vel. In Fig. 3, we present the HAROS graph for this node together with the input (subscriber) and output (publisher) topics with labels. We should note that to enable the controller for publishing, only one or none of these three topics can be valid - reset, disable, or enable. We encode this behavior as'\\n\\n'Robot Operating System (ROS) is an open-source middleware providing a peer-to-peer communication framework either with messages of topics using a publishing and subscribing mechanism or by services between nodes. Topics can also be seen as communication channels that helps connecting nodes together for communication. ROS provides a lot of flexibility for software development such as fast prototyping by easily creating and sharing packages, choosing many types of programming languages, and customizing publishing rate or waiting time of messages. On the other hand, this flexibility might lead to undesired system behaviour that will be hard to catch, since there exists no complete solution to formally verify a ROS system.'\",\"1037\":\"'https:\\/\\/github.com\\/rdelgadov\\/fuzztesting'\\n\\n'The behavior of a robot is typically expressed as a set of source code files written using a programming language. As for any software engineering activity, programming r...'\\n\\n'The behavior of a robot is typically expressed as a set of source code files written using a programming language. As for any software engineering activity, programming robotic behaviors is a complex and error-prone task. This paper propose a methodology that aims to reduce the cost of producing a reliable software describing a robotic behavior by automatically testing it.We employ a fuzz testing technique to stress software components with randomly generated data. By applying fuzz testing to a complex robotic-software, we identified errors related to the coding, the way data is handled, the logic of the robotic behavior, and the initialization of architectural components. Furthermore, a panel of experts acquainted with the analyzed behavior have highlighted the relevance and the significance of our findings. Our fuzzer operates on the SMACH and ROS frameworks and it is available under the MIT public open source license.'\\n\\n'Fuzz testing mixes random with automated data generation to produce a wide range of plausible values to test the software source code with. It has been shown that different fuzz techniques can find different types of bugs in the same software system [7]. There are some fuzzers that use finite state machines to model the software and apply fuzz tests to the model [8], [9]. This suggests that fuzz testing can be beneficial to a particular context, such as using state machines to define robotic behavior.'\\n\\n'To be able to generate appropriate userdata, it is crucial to restrict the space of the values accepted by a state. For example, assume a state expects an integer value as input. If our fuzzer provides a string character to that state, a type error will be inevitably produced. Since Python is a dynamically-typed language (as JavaScript and contrary to Java and C++), it is not possible to determine whether a variable accepts a string or a number by solely looking at the source code definition3. However, our fuzzer needs this crucial piece of information to generate random values of the appropriate type. Without such information, our fuzzer will identify trivial and non-relevant type errors instead of valuable bugs and errors in the robotic behavior logic.'\\n\\n'As for most robots developed in a University, large parts of the software source code is written by under- and postgraduate students during the development of their thesis. As such, it is crucial to rigorously and extensively test the robotic behavior to remain competitive. Note that the authors of this paper are not involved in the development of the robot nor part of the UChile Peppers team.'\\n\\n'S2: input description. Accepted input values must be adequately described and characterized in order to apply fuzz testing to a state or a state machine. This involves identifying (i) the names of the inputs, (ii) the type of the data used by the state code (using the state-monitoring technique), and (iii) whether it is external or internal to adequately tune the value generation by our fuzzer. The output of that step is a clear and unambiguous description of the input. The fuzzer is also correctly tuned to produce inputs properly structured by designing a grammar.'\\n\\n'Each participant received and evaluated three situations per errors. We indicated to the participants the exact location of an error in the source code, the input values our fuzzer generated, and the outcomes from the state execution. We also encourage the participant to reproduce the error.'\\n\\n'Syntax error. We designate as a syntax error a sequence of characters that cannot be interpreted by the Python interpreter. Syntax errors happen to be frequently produced by nonexperts since (i) programming environments for Python do a poor job at notifying practitioners about the presence of such errors, (ii) robot programmers usually do not have training in software engineering. A syntax error will occur when the interpreter is trying to execute the first instruction contained in the file with an error. Consider the following code snippet:'\\n\\n'This code contains a syntax error because a comma is missing between the string \\\"I am going left\\\" and \\\"right\\\". Our fuzzer identified one syntax error, which was not detected during the development and the test made in laboratory because it was fixed on the robot itself.'\\n\\n'Data handling error occurs when data are not properly handled by a state. In such a case, the error could either be in the calling state (i.e., data passed to another state is incorrectly defined), or in the called state (i.e., data provided as input is correct but incorrectly handled). Consider the following code snippet:'\\n\\n'In a previous version of the code, the variable userdata.listobjects was containing the two coordinates of a position in a two-dimensional space. After a new version of the code was produced, the variable now provides a label indicating a physical position (e.g., \\u2019Door\\u2019) instead of coordinates. It will return a tuple of chars (e.g., (\\\"D\\\",\\\"o\\\")) and will produce an error on the following states.'\\n\\n'Logic error. An incorrect conditional statement or loop control may lead to a logic error. During the execution, a logic error may be expressed by executing a wrong branch in a condition (e.g., the else branch is executed instead of the then branch). Consider the following code:'\\n\\n'This code contains a logic error because the variable text_confirmation points to the capitalized string \\u2019Yes\\u2019, while the first condition is expressed with lowercase \\u2019yes\\u2019. As a result, the branch return \\u2019aborted\\u2019 will be considered while obviously the first one should be considered.'\\n\\n'https:\\/\\/github.com\\/rdelgadov\\/fuzztesting'\",\"1038\":\"'The Plan Executive decides on the goals of the robot and the sequence of actions to perform to achieve these goals. Additionally, it uses data from the optical encoders in the joints of the robot to monitor action execution. It ensures that everything goes as planned and reacts to failures.'\\n\\n'To achieve scalability, the generalized plans are written with underspecified symbolic action descriptions. For example, in the code listing below, we command the robot to perform an underspecified action of type picking-up on some previously perceived object of type cup, whereby the goal of the action is to have the cup object in the hand:'\\n\\n'Task 2: Most of the parameters of the action in the code listing above are omitted from the description, which makes it very general. During execution, these missing parameters are inferred at runtime by querying the reasoning engines, taking into account the given situation. The resulting action description with inferred parameters is shown below:'\\n\\n'The contributions of this paper to the state of the art are our approach to addressing the challenges raised by the experiment and the implemented system (all the components are available as open source, see footnotes). In the remainder of this paper, we review related work, explain the integral components of our system and their interaction, and report on the experimental results and the lessons learned.'\",\"1039\":\"'https:\\/\\/github.com\\/craigiedon\\/ProbRobScene'\\n\\n'Our robot controller uses a standard open-loop planning pipeline. First, the robot takes the RGB-D image from the ceiling camera, and uses an off-the-shelf blob-detection algorithm to capture the location of the cube in world-space. Next, using a built-in inverse kinematics solver, it plans a trajectory to move the left-arm gripper just above the cube, grasps the cube, then drops it within reach of the right arm. The right arm then picks up the cube in a similar manner, and drops it into the right tray. The full code for language, sampler, wrapper, and controller is available at: https:\\/\\/github.com\\/craigiedon\\/ProbRobScene.'\\n\\n'Thanks to the members of the Edinburgh Robust Autonomy and Decisions Group for their feedback and support. In particular, we are grateful for Advaith Sai\\u2019s efforts to expand the range of examples available in the paper\\u2019s code repository.'\\n\\n'https:\\/\\/github.com\\/craigiedon\\/ProbRobScene'\",\"1040\":\"'While the communication is easy to realize and natively supports KUKA-specific data types, custom KRL code \\u2013 often running in a cyclic subprogram - must assign that data to variables, which can then be called from another program. Such a process works well for non-continuous projects [15] or for exchanging arbitrary data. The findings of Arbo et al. [16] relating to KUKAVARPROXY mostly also apply to EKI.'\",\"1041\":null,\"1042\":\"'Electromyography (EMG) is a technique employed for collecting biological signals that measure the summation of the muscular activation potentials of the human muscles. EMG activity can be used to decode the human motion and\\/or intention for the execution of robotic teleoperation tasks [7], [8]. EMG based interfaces provide numerous benefits over other interfaces such as being noninvasive and safe to use, as well as having the capability to measure the human effort. EMG based interfaces have also been developed using time delayed neural networks for online torque prediction [9] and by employing neuro-musculoskeletal models for movement intention prediction [10].'\\n\\n'To decode the user\\u2019s arm motion, the Random Forests (RF) regression methodology was selected [30]. This is is a supervised learning method for classification and regression based decoding using decision trees. It was first proposed by Tin Kam Ho of Bell Labs [31] using the random subspace method. The ensemble nature of RF renders it robust against overfitting. Furthermore, using the inherent property of RF, the importance of feature variables can be calculated (see [27] for details). Thus, important insights can be made regarding the muscular activation strategies that are employed by different subjects for the same tasks.'\\n\\n'Actual and decoded human motion trajectories based on data collected during the offline training phase. Subfigure a shows the motion in the y-axis, while subfigure b shows the motion in z-axis.'\\n\\n'Actual and decoded human motion trajectories during a real-time execution of the whiteboard cleaning task.'\\n\\n'This paper presented a shared control framework that enables the user to execute EMG based telemanipulation tasks with a robotic platform, employing a compliance controller. The EMG signals were utilized to decode the human arm motion using a Random Forest based regression model. The human wrist motion estimations were then used to control the pose of the robotic platform end-effector (cleaning tool). The accuracy of the learning model was evaluated during offline analysis (training phase). The correlation of the actual human motion and the decoded motion was 92.8%, while the NMSE accuracy was 73.4%. To compensate for errors in goal pose prediction and ensure appropriate contact force profiles, the system relies on an active compliance control scheme. The compliance controller ensures that any EMG based decoding inaccuracies will not drive the robot away from the cleaning plane, \\\"correcting\\\" the EMG based motion estimations so as the robot trajectories to be projected on the cleaning plane. The system was experimentally validated in a series of whiteboard cleaning tasks. The average percentage of the cleaning task completion was 90.6%. This percentage was calculated by analyzing the area of erased color markings on the whiteboard.'\\n\\n'Sequence of images presenting the EMG based shared control of the robotic in the execution of a whiteboard cleaning task. The user motion is decoded from EMG signals using a Random Forests regression model. The compliance control ensures that the EMG based decoding inaccuracies will not drive the robotic end-effector (cleaning tool) away from the whiteboard, \\\"correcting\\\" the EMG based motion estimations so as for the robot trajectories to be projected on the cleaning plane.'\\n\\n'A drawback of this framework is that it is only capable to decode a specific motion strategy of the user for which the decoding models have been developed. In our future work, we plan to: i) explore the effect of the number of EMG channels, ii) optimize electrode placement on the muscle sites, and iii) use external sensors such as Inertial Measurement Units (IMUs) along with the EMG signals to develop improved EMG based shared control frameworks.'\",\"1043\":null,\"1044\":\"'The solver is implemented in C++. The source code and a supplementary video are available for reference*.'\",\"1045\":\"'https:\\/\\/github.com\\/StanfordASL\\/mantrap'\\n\\n'https:\\/\\/github.com\\/StanfordASL\\/mantrap'\",\"1046\":null,\"1047\":null,\"1048\":null,\"1049\":\"'In order to further encode the impact of the climbing surface orientation on contact forces, friction constraints must be incorporated. Let us consider the contact force FCi \\u2208 \\u211d3 at the i-th contact point, with i := {l, r}. Being\\\\nF\\\\nn\\\\nCi\\\\n\\u2208\\\\nR\\\\n3\\\\nand\\\\nF\\\\nt\\\\nCi\\\\n\\u2208\\\\nR\\\\n3\\\\n, the normal and tangential component of the contact force, respectively, given by:'\",\"1050\":\"'The pseudo-code for calculating the rotation for a link to move from collision to collision-free is given in Alg. 1. While the details of the approach below uses the base link, the same approach applies for the leg link as well. The robot\\u2019s link have a vertical pair of points distributed across its length, shown in Fig. 5a, for the base link.'\\n\\n'Algorithm 1 describes the rotation for only a single pair of points. The pseudo-code in Alg. 2 describes how multiple pairs of points on a link is used to rotate the link out of collision. The rotation described in Alg. 1 is iterated for each collision pair points, starting from the pair nearest to the base frame and finishing at the edge of the link (line 6). The set of collision points are then further iterated until a collision-free pose is found or terminated after a number of iteration, ni (line 3). Both these bounded iterative approaches ensure that the entire link is collision free, as compared to using only a single pair of collision points for the entire link, and also accounts for scenarios where some collision points may be in a local minimums.'\",\"1051\":null,\"1052\":null,\"1053\":null,\"1054\":\"'https:\\/\\/github.com\\/TUI-NICR\\/ESANet'\\n\\n'In this paper, we propose an efficient and robust encoder- decoder-based semantic segmentation approach that can be embedded in complex systems for semantic scene analysis such as shown in Fig. 1. The segmentation output enriches the robot\\u2019s visual perception and facilitates subsequent processing steps by providing individual semantic masks. For our person perception [7], computations can be restricted to image regions segmented as person, instead of processing the entire image. Furthermore, the floor class indicates free space that can be used for inpainting invalid depth pixels as well as serves as additional information for avoiding even small obstacles below the laser. For mapping [8], we can include semantics and ignore image regions segmented as dynamic classes such as person.'\\n\\n'an efficient ResNet-based encoder that utilizes a modified basic block that is computationally less expensive while achieving higher accuracy'\\n\\n'a decoder that utilizes a novel learned upsampling'\\n\\n'Our code as well as the trained networks are publicly available at: https:\\/\\/github.com\\/TUI-NICR\\/ESANet'\\n\\n'Common network architectures for semantic segmentation follow an encoder-decoder design. The encoder extracts semantically rich features from the input and performs downsampling to reduce computational effort. The decoder restores the input resolution and, finally, assigns a semantic class to each input pixel.'\\n\\n'However, none of the aforementioned methods focus on efficient RGB-D segmentation for embedded hardware. Using deep encoders such as ResNets with 50, 101 or even 152 layers results in high inference times and, therefore, makes them inappropriate for deploying to mobile robots.'\\n\\n'Following SwiftNet and BiSeNet, our approach also uses a ResNet-based encoder. However, in order to further reduce inference time, we exchange the basic block in all ResNet layers with a more efficient block, based on factorized convolutions.'\\n\\n'A. Encoder'\\n\\n'The RGB and depth encoder both use a ResNet architecture [36] as backbone. For efficiency reasons, we do not replace strided convolutions by dilated convolutions as in PSPNet [33] or DeepLabv3 [37]. Thus, the resulting feature maps at the end of the encoder are 32 times smaller than the input image. For a trade-off between speed and accuracy, we use ResNet34 but also show results for ResNet18 and ResNet50. We replace the basic block in each layer of ResNet18 and ResNet34 with a spatially factorized version. More precisely, each 3\\u00d73 convolution is replaced by a 3\\u00d71 and a 1x3 convolution with a ReLU in-between. The so-called Non-Bottleneck-1D-Block (NBt1D) is depicted in Fig. 2 (violet) and was initially proposed in ERFNet [26] for another network architecture. In our experiments, we show that this block can also be used in ResNet and simultaneously reduces inference time and increases segmentation performance.'\\n\\n'At each of the five resolution stages in the encoders (see Fig. 2), depth features are fused into the RGB encoder. The features from both modalities are first reweighted with a Squeeze and Excitation (SE) module [38] and then summed element-wisely, as shown in Fig. 2 (light green). Using this channel attention mechanism, the model can learn which features of which modality to focus on and which to suppress, depending on the given input. In our experiments, we show that this fusion mechanism notably improves segmentation.'\\n\\n'D. Decoder'\\n\\n'Although being upscaled, the resulting feature maps still lack fine-grained details that were lost during downsampling in the encoders. Therefore, we design skip connections from encoder to decoder stages of the same resolution. To be precise, we take the fused RGB-D encoder feature maps, project them with a 1\\u00d71 convolution to the same number of channels used in the decoder, and add them to the decoder feature maps. Incorporating these skip connections results in more detailed semantic segmentations.'\\n\\n'Instead of calculating the training loss only at the final output scale, we add supervision to each decoder module. At each scale, a 1\\u00d71 convolution computes a segmentation of a smaller scale, which is supervised by the down-scaled ground truth segmentation.'\\n\\n'Comparison of RGB-D to RGB and depth networks (single encoder) and different backbones on NYUv2 test set.'\\n\\n\\\"Fig. 4 compares our RGB-D approach on NYUv2 to single-modality baselines for RGB and depth (single encoder) as well as evaluates different encoder backbones. As expected, neither processing depth data nor RGB data alone reach the segmentation performance of our proposed RGB-D network. Remarkably, the shallow ResNet18-based RGB-D network performs better than the much deeper ResNet50-based RGB network while still being faster. Moreover, replacing ResNet's basic block with Non-Bottleneck- 1D (NBt1D) block can further improve both segmentation and inference time. Note that ResNet50 incorporates bottle- neck blocks, which cannot be replaced the same way.\\\"\\n\\n'Ablation Study on NYUv2 test. Each color indicates modifying one aspect: purple: number of NBt1D blocks in decoder module, dark green: upsampling method, and gray: usage of specific network parts with\\\\nCM\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n: no context module,\\\\nSkip\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n: no encoder-decoder skip connections, and\\\\nSE\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n: no Squeeze-and-Excitation before fusing RGB and depth.'\\n\\n'As shown in purple, a shallow decoder similar to Swift- Net [30] is not as good as more complex decoders. Therefore, we gradually increased the number of additional NBt1D blocks in the decoder module. Apparently, a fixed number of three blocks in each decoder module performs better than a different number or a reversed layout of the encoder\\u2019s design.'\\n\\n'In dark green, different upsampling methods in the decoder are displayed. Although increasing inference time, the learned upsampling improves mIoU by 0.9. Moreover, as shown in Fig. 3, the obtained segmentation contains more fine-grained details compared to using bilinear interpolation. It further prevents gridding artifacts introduced by transposed convolutions as used in ACNet [11] or RedNet [10].'\\n\\n'In this paper, we have presented an efficient RGB-D segmentation approach, called ESANet, which is characterized by two enhanced ResNet-based encoders utilizing the Non-Bottleneck-1D block, an attention-based fusion for incorporating depth information, and a decoder utilizing a novel learned upsampling. On the indoor datasets NYUv2 and SUNRGB-D, our ESANet performs on par or even better while enabling much faster inference compared to other state-of-the-art methods. Thus, it is well suited for embedding in a complex system for scene analysis on mobile robots given limited hardware.'\\n\\n'https:\\/\\/github.com\\/TUI-NICR\\/ESANet'\\n\\n'For further details and other hyperparameters, we refer to our implementation available on GitHub.'\",\"1055\":\"'The main focus of this paper is to obtain a fast and accurate plane segmentation in an organized point cloud. To this end, we first show the results of applying our method in simulated environments where the ground-truth plane segmentation is known. For comparison, we also apply two other popular plane segmentation approaches available in the open source Point Cloud Library [17], namely, Sequential RANSAC [4], and Organized Multi-Plane Segmentation using Connected Components (CC) [16]. The simulated environments consist of five scenes constructed with basic geometric primitives like boxes, spheres, and polygons. These scenes are shown in Fig. 6. The segmentation accuracy and the computation times of all three methods in each scene are tabulated in Tab. I. We measure accuracy in terms of the Intersection over Union (IoU) of found planes with the ground-truth planes. Computation times were measured on an Intel i7 8700 3.2 GHz CPU by averaging over a thousand frames. The result of the experiment supports the increased accuracy and efficiency of our approach with respect to the PCL methods.'\",\"1056\":\"'Deep learning on 3D point clouds has drawn much attention, due to its large variety of applications in intelligent perception for automated and robotic systems. Unlike structured 2D images, it is challenging to extract features and implement convolutional networks over these unordered points. Although a number of previous works achieved high accuracies for point cloud recognition, they tend to process local point information in such a way that semantic information is not fully encoded. In this paper, we propose a deep neural network for 3D point cloud processing that utilizes effective feature aggregation methods emphasizing both generalizability and relevance. In particular, our method uses fixed-radius grouping for pooling layers and spherical kernel convolution for semantics mining. To address the issue of gradient degradation and memory consumption of a deep network, a parallel feature feed-forward mechanism and bottleneck layers are implemented to reduce the number of parameters. Experiments show that our algorithm achieves state-of-the-art results and competitive accuracy in both classification and part segmentation while maintaining an efficient architecture.'\\n\\n'A multi-scale local feature aggregation method is proposed. It can effectively encode semantics information of points ensuring both generalizability and high relevance, as well as maintaining permutation invariance for irregular points;'\\n\\n'However, these networks encode point features with only a single type of convolution operator. These operators might not be optimized or specialized for various receptive fields. Also the input might be \\u2018washed-out\\u2019 through these operators in a deep neural network. Instead, our proposed method constructs multi-level hierarchical network, utilizing two different operators for multi-scale local feature aggregation.'\\n\\n'Overview of the proposed architecture. The upper part shows the classification branch of the network, which consists of three semantics mining blocks, followed by a global average pooling layer to summarize the encoded feature. Two single-layer perceptrons (SLP) are used to output class scores. The lower part shows the network configuration for segmentation. Four semantics mining blocks are used paired with four feature propagation layers for point up-sampling. Rich semantic information is propagated to all input points, followed by a shared MLP for per-point classification. The numbers indicate the output feature-map size of each module in the format of N \\u00d7 C, where N is the number of points and C is the width of feature channels. L refers to the number of SM layers within the module.'\\n\\n'To extend this work, one direction is to impose a more general encoder for the inference on real-world data. This may involve dealing with occlusions and translation invariance of objects. Another direction is to apply this work to the subsequent tasks such as 3D point cloud retrieval and object detection in the autonomous driving context.'\",\"1057\":\"'Overall network architecture: VelocityNet applies the point feature encoder and U-Net encoder to multiple frames and uses our time-deformed contraction module to produce a single prediction for the latest frame. Orange arrows denote frame sequences passed between components. Our VelocityNet can leverage motion information that can either come from external sources or be predicted and integrated into the network model via a separate branch. In comparison, the TimeConvNet is using standard 3D convolutions as a contraction mechanism and the PointPillars baseline only processes a single frame and does not require a contraction module at all.'\\n\\n'Multi-task learning of motion and detection. Another possibility is to use a standard point cloud encoder and use these encoded feature maps not only inside the time-deformed convolutions for the object detection but implement a second network head that predicts the velocity maps. These maps are not only handed to the time-deformed convolutions but receive also explicit supervision through ground truth labels during training.'\\n\\n'Backbone architecture: The point feature encoder-processed point clouds are stacked along the temporal (vertical edge) dimension. First, the Down blocks produce 3 spatially (horizontal edge) downsampled tensors with increasing channel numbers. The Contract blocks (middle) consisting of td-convs remove the temporal dimension and leverage motion information. Finally, the Up blocks and concatenation produce the decoder output.'\\n\\n'Encoder. The first component of all our architectures is the point feature encoder (PFE) [12], [31] which takes raw point clouds as input and produces BEV feature maps. With a time-sequence of point clouds, there are two possibilities of how to use the PFE. The PP baseline [3] aggregates all point clouds into a single instance and produces a single BEV feature tensor from all points together. In order for PP to process motion every point is annotated with its relative measurement timestamp. For our intermediate TCN baseline and VN, we apply a weight-sharing PFE across all T input point clouds and let it produce T BEV feature maps which can be stacked along the first dimension to produce a 3D feature tensor.'\\n\\n'Encoder.'\\n\\n'Head. Following the concatenated features of the backbone, a simple linear layer (1x1 convolution) without any normalization or activation encodes the object predictions similar to [12]. For each of the 10 detection classes, we have 1 channel as detection logit, 3 channels for each the 3D location and size, 2 channels for the velocity prediction, and a regression value and classification logit for the object\\u2019s orientation.'\\n\\n'The TimeConvNet (TCN) features an additional contraction module between the encoder and decoder. It uses for all three feature map resolutions Contract(T =10, L=3, C) with the respective channel size of the incoming feature map.'\\n\\n'The VelocityNet (VN) architecture corresponds directly to TCN with all the 3D convolutions replaced by td-convs. When following the approach of learning the velocity maps in a multi-task setup or an end-to-end trainable fashion we employed a second network branch on top of the encoded feature maps as shown in Fig. 2. The additional stack of Contract blocks using 3D convolutions has the same number of layers as the detection backbone but only half the channel dimension in each layer. Together with the Up blocks and a separate network head, it regresses the velocities.'\",\"1058\":\"'Real-world missions require robots to detect objects in complex and changing environments. While deep learning methods for object detection are able to achieve a high level of performance, they can be unreliable when operating in environments that deviate from training conditions. However, by applying novelty detection techniques, we aim to build an architecture aware of when it cannot make reliable classifications, as well as identifying novel features\\/data. In this work, we have proposed and evaluated a system that assesses the competence of trained Convolutional Neural Networks (CNNs). This is achieved using three complementary introspection methods: (1) a Convolutional Variational Auto-Encoder (VAE), (2) a latent space Density-adjusted Distance Measure (DDM), and (3) a Spearman\\u2019s Rank Correlation (SRC) based approach. Finally these approaches are combined through a weighted sum, with weightings derived by maximising the correct attribution of novelty in an adversarial \\u2018meta-game\\u2019. Our experiments were conducted on real-world data from three datasets spread across two different domains: a planetary and an industrial setting. Results show that the proposed introspection methods are able to detect misclassifications and unknown classes indicative of novel features\\/data in both domains with up to 67% precision. Meanwhile classification results were either maintained or improved as a result.'\\n\\n'Variational Auto-Encoder (VAE)'\\n\\n'The encoder consists of convolutional layers with shapes (32 \\u00d7 7 \\u00d7 7), (128 \\u00d7 5 \\u00d7 5), (64\\u00d7 3 \\u00d7 3), (6 \\u00d7 3 \\u00d7 3) and identical layers inbetween each convolutional\\u00d7layer\\u00d7 as the CNN classifiers. This is again followed by flattening and passing through two dense layers except this time both have size double the number of latent dimensions in order to produce the mean and variance for each dimension. Once the mean and variance is calculated for a given image region, a latent vector representation is sampled from a multivariate Gaussian distribution, this is then passed into the decoder, which is defined symmetrical to the encoder. We employ a latent representation consisting of 768 dimensions, based upon the findings of [15]. In order to train the VAE and to assess the novelty of a region proposal, we use a negative ELBO loss that combines reconstruction errors in the image space as well as losses obtained from the VAE\\u2019s latent space:'\",\"1059\":null,\"1060\":\"'Deep learning is the essential building block of state-of-the-art person detectors in 2D range data. However, only a few annotated datasets are available for training and testing these deep networks, potentially limiting their performance when deployed in new environments or with different LiDAR models. We propose a method, which uses bounding boxes from an image-based detector (e.g. Faster R-CNN) on a calibrated camera to automatically generate training labels (called pseudo-labels) for 2D LiDAR-based person detectors. Through experiments on the JackRabbot dataset with two detector models, DROW3 and DR-SPAAM, we show that self-supervised detectors, trained or fine-tuned with pseudolabels, outperform detectors trained only on a different dataset. Combined with robust training techniques, the self-supervised detectors reach a performance close to the ones trained using manual annotations of the target dataset. Our method is an effective way to improve person detectors during deployment without any additional labeling effort, and we release our source code to support relevant robotic applications.'\\n\\n'We release our code, implemented in PyTorch with an easy-to-use detector ROS node, for robotic applications.1'\\n\\n'Person detections and the top-down view of the matching pseudo-labels (\\u2022) with surrounding LiDAR points (within a 0.5 m radius). The LiDAR points are overlaid in the detection images, where the color encodes measured distance with red being closest and white being farthest. The bottom two rows demonstrate some failure cases: heavy occlusion (columns 1-2); background distraction (column 3-4); sparse LiDAR points at far distance (column 5); and faulty calibration or synchronization between sensors (column6).'\\n\\n'Our method provides an effective way to bridge the domain gap between data encountered during training and during deployment. With our method, a mobile robot equipped with a 2D LiDAR-based person detector can fine-tune the detector during deployment, improving its performance with no additional labeling effort. With the released code, we expect our method will be useful for many robotic applications.'\",\"1061\":\"'The standard approach is to construct a cost function that encodes our two goals. It could take the form,'\",\"1062\":\"'https:\\/\\/youtu.be\\/pV4s7hzUgjc'\",\"1063\":null,\"1064\":null,\"1065\":\"'The mechanical stimulation of the scaffold is driven by a 12V DC microgear motor (Pololu) with a magnetic encoder placed at the back of the rotor. The motor has a 297.92:1 gear ratio which actuates the worm-gear mechanism. This actuation chain implies an important ratio r between one motor rotation (sensed by the encoder) and an actual rack translation. We can calculate that one step of the encoder senses a 0.00056mm moving of the clamps. From calibration we measure a 0.0006mm movement of the rack from one step in the encoder. The relative error of this ratio is\\\\ne=\\\\n|\\\\nr\\\\nexp\\\\n\\u2212\\\\nr\\\\nth\\\\n|\\\\nr\\\\nth\\\\n=9%\\\\n.'\\n\\n'Implementing two different controllers into the bioreactor demonstrates its versatility as a tool. The position control appeared to be more robust than the force control as was less noise in the encoder readings than there was for the force sensor. The force control, however, is a novel approach to induce mechanostimulation onto a CSS. Both control methods can be used to achieve stiffness-based control which may be a step closer to achieving optimum mechanostimulation for CCS where the force applied to the an evolving scaffold is truly constant throughout an experiment.'\",\"1066\":null,\"1067\":\"'https:\\/\\/youtu.be\\/Mp5HAMRCdAg'\\n\\n'Due to space limitations, interested readers are referred to the public code for parameter values used and the calculation of the required derivatives. A very low-stiffness k1:3 = [1],[1],[1]1e4 (N\\/m) and high-stiffness k1:3 = [1],[1],[1]1e6 coupling are compared with the optimized stiffnesses where k1:3 = [9.8,9.3,7.9]e5. A video is available at https:\\/\\/youtu.be\\/Mp5HAMRCdAg and the resulting state and control trajectories are seen in Figure 7. The iLQR feedback gains and feedforward trajectory vary more over time when the stiffness is higher, providing better performance from higher directed information. The optimized gains respect the performance limit of D = 300 while reducing DI. Similar trends occur in a door closing task, as seen in the attached video.'\",\"1068\":null,\"1069\":\"'This work proposes a RGB-D SLAM system specifically designed for structured environments and aimed at improved tracking and mapping accuracy by relying on geometric features that are extracted from the surrounding. Structured environments offer, in addition to points, also an abundance of geometrical features such as lines and planes, which we exploit to design both the tracking and mapping components of our SLAM system. For the tracking part, we explore geometric relationships between these features based on the assumption of a Manhattan World (MW). We propose a decoupling-refinement method based on points, lines, and planes, as well as the use of Manhattan relationships in an additional pose refinement module. For the mapping part, different levels of maps from sparse to dense are reconstructed at a low computational cost. We propose an instance-wise meshing strategy to build a dense map by meshing plane instances independently. The overall performance in terms of pose estimation and reconstruction is evaluated on public benchmarks and shows improved performance compared to state-of-the-art methods. The code is released at https:\\/\\/github.com\\/yanyan-li\\/PlanarSLAM.'\",\"1070\":null,\"1071\":null,\"1072\":\"'http:\\/\\/bit.ly\\/crowd_det_results'\\n\\n'http:\\/\\/bit.ly\\/crowd_det_results'\\n\\n'BiSeNet adopts a two-column network architecture consisting of two neural streams, namely, the Spatial Path and the Context Path. The Spatial Path is composed of a shallow CNN in order to learn high-resolution features that encode spatial information. In contrast, pre-trained state-of-the-art CNN architectures are utilized in the Context Path to encode high level semantic context information. Moreover, the features of each stage of the Context Path are refined using an Attention Refinement Module (ARM) to guide the learning process. As features from the Spatial and the Context Path encode different information, a Feature Fusion Module (FFM) was also utilized to effectively fuse the learned features. The final segmentation map is, then, obtained by upsampling the combined feature map to the output resolution. The loss function employed for training is the following one:'\",\"1073\":\"'CloudAAE: Augmented Autoencoder Based 6D Pose Estimation'\\n\\n'It is often desired to train 6D pose estimation systems on synthetic data because manual annotation is expensive. However, due to the large domain gap between the synthetic and real images, synthesizing color images is expensive. In contrast, this domain gap is considerably smaller and easier to fill for depth information. In this work, we present a system that regresses 6D object pose from depth information represented by point clouds, and a lightweight data synthesis pipeline that creates synthetic point cloud segments for training. We use an augmented autoencoder (AAE) for learning a latent code that encodes 6D object pose information for pose regression. The data synthesis pipeline only requires texture-less 3D object models and desired viewpoints, and it is cheap in terms of both time and hardware storage. Our data synthesis process is up to three orders of magnitude faster than commonly applied approaches that render RGB image data. We show the effectiveness of our system on the LineMOD, LineMOD Occlusion, and YCB Video datasets. The implementation of our system is available at: https:\\/\\/github.com\\/GeeeG\\/CloudAAE.'\\n\\n'In this work, we propose a point cloud based 6D pose estimator and a lightweight data synthesis pipeline. We propose to use an implicit approach during the pose inference stage. We achieve this by adapting the augmented autoencoder (AAE) proposed by Sundermeyer et al. [6] for a point cloud based system. By using an AAE, we are able to control which properties the latent code encodes and which properties are ignored [6]. This is achieved by applying augmentations to the input, and the encoding becomes invariant to those augmentations. We focus on learning a latent code that encodes the 6D pose information. The task of the AAE is to reconstruct a point cloud segment in the desired 6D pose. Moreover, this segment is noise and occlusion free. In this way, the latent code contains the necessary information for regressing the object pose. Using the latent code as the input, we use two separate networks for regressing 3D rotation and 3D translation, as suggested in [7], [8]. The on-line data synthesis pipeline requires a texture-less 3D object model and the desired viewpoint as the input. The computational cost is low.'\\n\\n'We present a new framework for regressing the 6D object pose from point cloud segments. An point cloud based augmented autoencoder is used to learn a latent code that encodes object pose information. This code is used for regressing the 6D object pose.'\\n\\n'Unsupervised learning of 6D pose. The augmented autoencoder is proposed in [6]. It is a variant of the Denoising Autoencoder [14]. The authors use 2D bounding boxes for translation estimation, and use the AAE for 3D rotation estimation. For 3D rotation, rather than explicitly mapping from an input image to a pose label, the AAE learns implicit codes of object orientations in a latent space. A codebook of latent codes is created off-line, and they use a nearest neighbor search to compare a test code within the codebook. This approach is improved by [15] by adding edge priors. Our 6D pose estimation pipeline adapts the AAE concept [6] for point clouds. There are three main differences between our approach and [6]. First, our approach does not require creating off-line codebooks. Second, we use the latent code from the AAE for both 3D translation and 3D rotation estimation. Third, they use 2D color image as input while ours uses point clouds.'\\n\\n'We propose a data synthesis pipeline and an augmented autoencoder (AAE) based 6D pose estimation system, referred to as CloudAAE. CloudAAE is a multi-class system and we use the same system to predict poses for objects from different classes. Figure 1 shows an overview of the proposed system. Given a known object represented by a set of points in the camera coordinate C, the aim of a 6D pose estimation system is to find the translation t and rotation R that describes the transformation from the object coordinate system O to C. The data synthesis pipeline creates a point cloud segment\\\\nP\\\\nC\\\\nocc\\\\n, and\\\\nP\\\\nC\\\\nocc\\\\nis normalized by removing its coordinate mean \\u03bcp before further steps. We denote the normalized\\\\nP\\\\nC\\\\nocc\\\\nas\\\\nP\\\\nN\\\\nocc\\\\n.'\\n\\n'By using the data synthesis pipeline, we apply random noise and occlusion to the input point cloud segment to add variance in the training samples. The AAE reconstructs a noise and occlusion free object segment at the desired 6D pose. Hence, the latent vector encodes the 6D object pose information, and it is used for 6D pose regression.'\\n\\n'In this section, we introduce our augmented autoencoder-based system CloudAAE, which computes the 6D pose of objects from point cloud data.'\\n\\n'CloudAAE contains an augmented autoencoder, and the 6D pose regressors. We adapted the idea of AAE proposed in [6] for point clouds, and use an adapted version of the dynamic graph CNN (DGCNN) [26] as the encoder. We additionally add two networks for regressing the 6D poses.'\\n\\n'Augmented autoencoder (AAE):'\\n\\n'Our encoder is an adapted version of DGCNN, which is a deep network processing unordered point sets. Assume\\\\nQ={\\\\nq\\\\ni\\\\n\\u2208\\\\nR\\\\np\\\\n\\u2223i=1,\\u2026,m}\\\\nis a point set.\\\\nR\\\\np\\\\ncan be a 3D space or an arbitrary feature space. For each point qi, a k-nearest neighbor graph is calculated. In all our experiments, we use k = 10. The graph contains directed edges (i, ji1), \\u2026 , (i, jik), in which qji1, \\u2026 , qjik are the k closest points to qi. For the edge eij, an edge feature qi, (qj - qi) T is calculated.'\\n\\n'6D pose regressors. Since the decoder is able to reconstruct the segment in the same 6D pose from the latent code, the latent code contains the object pose information. Hence, the latent code is used as the input to two networks for regressing 3D rotation and 3D translation, respectively. Each network contains three MLP layers with dimensions 512, 256, and 3, respectively.'\\n\\n'Point cloud based Augmented Autoencoder. The numbers of neurons in EC-MLP and MLP layers are indicated in parentheses. EC is short for EdgeConv. Dimensions of intermediate features are indicated without parentheses. Skip connections denote the concatenation of edge features. For the encoder, a 1024-dimensional feature vector for each point with its local neighbors is learned with shared weights. An average pooling layer aggregates the individual features into a 1024-dimensional latent code. Finally, three fully-connected layers output a noise and occlusion free point cloud segment.'\\n\\n'Augmented autoencoder (AAE).'\\n\\n'We compare our results to the state-of-the-art methods on public datasets. We also investigate the impact of the amount of synthetic data used for training, as well as different sizes for the latent code in AAE.'\\n\\n'We also investigate how the performance is impacted by latent code size. We train networks with different sizes for the latent code. All the networks are trained with the same synthetic data and trained until convergence. The results without ICP are shown in Table IV. The generalization ability of a network is the best when the latent code is of size 1024 and 2048. We pick 1024 for our system. We show a T- SNE [35] visualization of the latent code for the LineMOD dataset in Figure 4(b). The latent codes are well clustered for each class.'\\n\\n'Illustrations of ablation studies. (a) Amount of synthetic training data and performance accuracy (w\\/o ICP). (b) T-SNE visualization of latent code for the LineMOD dataset.'\\n\\n'TABLE IV Results (w\\/o ICP) with different latent code sizes on LM.'\\n\\n'We propose a point cloud based lightweight data synthesis pipeline and an augmented autoencoder based system for accurate and fast 6D pose estimation of known objects represented by point clouds. The data synthesis pipeline is low cost in terms of both time and hardware storage. Using the synthetic data created by our data synthesis pipeline, our pose estimation system achieves state-of-the-art performance among other synthetic trained methods on a public benchmark. Moreover, our cheap synthetic point cloud data can replace expensive render based synthetic data for training systems using depth for pose inference. Our lightweight data synthesis pipeline enables more agile deployment of object pose estimation systems in robotic applications.'\",\"1074\":\"'Fig. 1 (II) shows how our model is trained using (SM, r) pairs. Our RC-NMP model has a specific deep encoder-decoder neural network architecture built on top of Neural Processes [11]. The encoder layer of the RC-NMP learns a representation distribution of trajectory points (\\u03bcz,\\u03c3z) conditioned on the time and the corresponding reward. The decoder layer takes a sampled representation (z) with the reward and outputs the trajectory as a function of time. Fig. 1 (II) shows the training procedure using a hypothetical 1D scenario. At each training iteration, random sensorimotor time and value pairs (the green dots in the figure), named observation points (O), are sampled from a trajectory randomly chosen from the Replay Buffer. These (SM, t) points are processed by a parameter sharing encoder network and transformed into their corresponding latent space representations, and then merged into a common latent representation with the averaging operation. This latent representation space is modelled as a Gaussian distribution N(0,I) and is optimized using variational inference similar to \\u03b2-Variational Auto-Encoders (\\u03b2-VAE) [41] and Conditional Variational Auto-Encoders (CVAE) [42]. The loglikelihood of a trajectory is composed of the log-likelihoods of individual data points:\\\\nlog\\\\np\\\\n\\u03b8\\\\n(\\u03c4\\u2223t,r)=\\\\n\\u2211\\\\nt=1\\\\nT\\\\nlog\\\\np\\\\n\\u03b8\\\\n(\\\\nx\\\\nt\\\\n\\u2223t,r)\\\\n. Using the derivation in [43], this log-likelihood can be expressed as:'\\n\\n'The decoder network, i.e. Query Net, receives the latent representation, target reward, and time-points in order to generate the trajectory points. In order to intermix the representations of different individuals generated in the previous step, we applied a one-point crossover through temporal blending on the corresponding latent representations. For this, random pairs of latent representations (zi,zj) are selected for crossover; and a random time-point tk is selected from the t0 \\u2212 tn range. Then the query network is conditioned with zi for time-points t0 \\u2212 tk and with zj for time-points tk+1 \\u2212 tn; generating SM values for all time-points. This crossover operation is repeated for m random pairs, generating m\\/2 trajectories. Applying crossover symmetrically on the selected pairs, m total trajectories are obtained at the end of the crossover operation.'\",\"1075\":\"'We have introduces so far the off-policy objective, the context and the the trust region regularization. We can formulate the gradient estimation as a constrained optimization problem. To ensure the parameters consistency, we encode the categorical distribution as\\\\n\\u03c0=\\\\ne\\\\n\\u03b8\\\\ni\\\\n\\/\\\\n\\u2211\\\\nK\\\\ni=1\\\\ne\\\\n\\u03b8\\\\ni\\\\n, and the covariances \\u03a3k as positive-definite diagonal matrices. By combining (8) (10) and (11), we obtain'\\n\\n'Parameterized movement primitives have been extensively used for imitation learning of robotic tasks. However, the high-dimensionality of the parameter space hinders the improvement of such primitives in the reinforcement learning (RL) setting, especially for learning with physical robots. In this paper we propose a novel view on handling the demonstrated trajectories for acquiring low-dimensional, non-linear latent dynamics, using mixtures of probabilistic principal component analyzers (MPPCA) on the movements\\u2019 parameter space. Moreover, we introduce a new contextual off-policy RL algorithm, named LAtent-Movements Policy Optimization (LAMPO). LAMPO can provide gradient estimates from previous experience using self-normalized importance sampling, hence, making full use of samples collected in previous learning iterations. These advantages combined provide a complete framework for sample-efficient off-policy optimization of movement primitives for robot learning of high-dimensional manipulation skills. Our experimental results conducted both in simulation and on a real robot show that LAMPO provides sample-efficient policies against common approaches in literature. Code available at https:\\/\\/github.com\\/SamuelePolimi\\/lampo.'\",\"1076\":\"'Animals are capable of extreme agility, yet understanding their complex dynamics, which have ecological, biomechanical and evolutionary implications, remains challenging. Being able to study this incredible agility will be critical for the development of next-generation autonomous legged robots. In particular, the cheetah (acinonyx jubatus) is supremely fast and maneuverable, yet quantifying its wholebody 3D kinematic data during locomotion in the wild remains a challenge, even with new deep learning-based methods. In this work we present an extensive dataset of free-running cheetahs in the wild, called AcinoSet, that contains 119, 490 frames of multi-view synchronized high-speed video footage, camera calibration files and 7, 588 human-annotated frames. We utilize markerless animal pose estimation to provide 2D keypoints. Then, we use three methods that serve as strong baselines for 3D pose estimation tool development: traditional sparse bundle adjustment, an Extended Kalman Filter, and a trajectory optimization-based method we call Full Trajectory Estimation. The resulting 3D trajectories, human-checked 3D ground truth, and an interactive tool to inspect the data is also provided. We believe this dataset will be useful for a diverse range of fields such as ecology, neuroscience, robotics, biomechanics as well as computer vision. Code and data can be found at: https:\\/\\/github.com\\/African-Robotics-Unit\\/AcinoSet.'\",\"1077\":\"'Object pose estimation enables robots to understand and interact with their environments. Training with synthetic data is necessary in order to adapt to novel situations. Unfortunately, pose estimation under domain shift, i.e., training on synthetic data and testing in the real world, is challenging. Deep learning-based approaches currently perform best when using encoder-decoder networks but typically do not generalize to new scenarios with different scene characteristics. We argue that patch-based approaches, instead of encoder-decoder networks, are more suited for synthetic-to-real transfer because local to global object information is better represented. To that end, we present a novel approach based on a specialized feature pyramid network to compute multi-scale features for creating pose hypotheses on different feature map resolutions in parallel. Our single-shot pose estimation approach is evaluated on multiple standard datasets and outperforms the state of the art by up to \\u223c35 %. We also perform grasping experiments in the real world to demonstrate the advantage of using synthetic data to generalize to novel environments.'\\n\\n'Exploiting synthetic data is a popular direction in order to generalize to novel domains and to fully automate the training procedure. The best performing deep-learning approaches for single-shot object pose estimation employ encoder-decoder architectures [13], [16], [25], [26], [41]. These methods only focus on local changes in image space, which is detrimental for pose estimation under domain shift, i.e., synthetic and real. Alternatively, predicting from coalesced features at multiple spatial resolutions enables processing of local to global information in parallel and has shown remarkable success for object detection [6], [17], [18], [20], [21], [31]. We hypothesize that this is highly suitable for generalizing to novel domains and therefore for object pose estimation under domain shift.'\\n\\n'This paper proposes to use feature pyramids as the main building block for extracting meaningful hierarchical features for pose estimation in contrast to those generated by encoder-decoder networks. We show that making predictions from multi-resolution feature maps guides the network to learn robust pose estimation under occlusion and domain shift. Our approach PyraPose, shown in Figure 1, is based on a novel Pose Feature Pyramid Network (PFPN), specialized for pose estimation. Our design needs only one network (less than 43M parameters) per set of objects, which leads to fast inference time of \\u223c26 fps. In comparison to state-of-the-art methods also trained on synthetic data, we achieve up to \\u223c35% higher accuracy. Furthermore, real-world grasping experiments with a mobile manipulator demonstrate the full capability of the synthetic-to-real transfer.'\\n\\n'A large variety of approaches are applied for CNN-based pose estimation. The dominant strategy is to employ encoder-decoder architectures for dense hypotheses generation and subsequent pose estimation using the PnP algorithm [8], [10], [16], [24], [25], [26], [35], [41]. The majority of these approaches color the mesh model using uv-coordinates. The networks are trained to regress the vertex location in the object coordinate system, i.e., 2D-3D correspondences, that are the input to PnP. Alternatively to this, a family of approaches exists that is more closely related to object detection [13], [24], [27], [32]. These methods either predict a single or multiple 3D bounding box hypotheses [13], [24], [27], [32] as 2D-3D correspondences. Recent patch-based approaches such as [13] and [32] have fast inference time, however, have inferior performance to the encoder-decoder approaches. This performance gap occurs because these approaches do not coalesce features of different spatial resolutions and thus miss object scale information.'\\n\\n'We therefore address this shortcoming by making predictions from multi-scale features of image patches, which yields strong results under domain shift and improved performance over encoder-decoder approaches. We a) aggregate multi-scale features using a feature pyramid and b) can thus make predictions employing small head networks shared over all feature scales. This leads to richly encoded scale information, strong occlusion handling and fast inference.'\",\"1078\":\"'https:\\/\\/owncloud.fraunhofer.de\\/index.php\\/s\\/6QsRj5sSty7fI9c'\",\"1079\":\"'Upon visual inspection it can be noted that the created map is consistent with the environmental layout. Moreover, to facilitate a quantitative evaluation due to absence of external ground-truth, the relative pose estimates of the proposed methods are compared against those provided by a popular open-source LOAM [3] implementation2. The quantitative results are presented in Table I, with corresponding error plots shown in Figure 5. A very low difference can be observed between the pose estimates produced by the proposed approach and those provided by LOAM, hence demonstrating its suitability for real-world mapping applications.'\",\"1080\":null,\"1081\":\"'Autonomous agents interacting with the real world need to learn new concepts efficiently and reliably. This requires learning in a low-data regime, which is a highly challenging problem. We address this task by introducing a fast optimization-based meta-learning method for few-shot classification. It consists of an embedding network, providing a general representation of the image, and a base learner module. The latter learns a linear classifier during the inference through an unrolled optimization procedure. We design an inner learning objective composed of (i) a robust classification loss on the support set and (ii) an entropy loss, allowing transductive learning from unlabeled query samples. By employing an efficient initialization module and a Steepest Descent based optimization algorithm, our base learner predicts a powerful classifier within only a few iterations. Further, our strategy enables important aspects of the base learner objective to be learned during meta-training. To the best of our knowledge, this work is the first to integrate both induction and transduction into the base learner in an optimization-based meta-learning framework. We perform a comprehensive experimental analysis, demonstrating the speed and effectiveness of our approach on four few-shot classification datasets. The Code is available at https:\\/\\/github.com\\/4rdhendu\\/FIML.'\",\"1082\":\"'We employ point feature representations to encode the neighborhood\\u2019s geometrical properties, which provides an overall point density and pose invariant multi-value feature. The surface normal [28] is estimated by using PCA on the k-neighborhood. Furthermore, for each pair ps and qt with\\\\nq\\\\nt\\\\n\\u2208N(\\\\np\\\\ns\\\\n)\\\\n, Darboux frame at \\u3008ps,ns\\u3009 is defined as'\",\"1083\":\"'Recently, deep generative models have shown the capacity to learn complex data distributions, allowing OoD samples to be detected. This work includes Variational Autoencoders (VAEs) [17], Generative Adversarial Networks (GANs) [18] and Energy-Based Models (EBMs) [19].'\\n\\n'Additionally, many of these techniques are so specialised for their task, that they would require a separate encoder from the segmentation task. This results in significantly larger memory requirements for the perception system.'\\n\\n'Our approach solves both problems, leveraging a general OoD dataset and using the same encoder for both decoders.'\\n\\n'Our work also leverages an OoD dataset in order to perform OoD detection. However, it scales up the problem to pixel-wise OoD detection unlike many approaches for much simpler image-wise OoD detection. In doing so, we propose a novel contrastive loss function coupled with a data augmentation scheme designed to train an encoder suited to both segmentation and robust pixel-wise OoD detection. We show that these changes lead to an improvement in the network\\u2019s ability to perform selective segmentation on a dataset that contains data at (and beyond) the limit of the training distribution.'\\n\\n'The encoder,\\\\nE:\\\\nR\\\\n3\\u00d7H\\u00d7W\\\\n\\u2192\\\\nR\\\\nC\\u00d7\\\\nH\\\\n^\\\\n\\u00d7\\\\nW\\\\n^\\\\n, maps an image to a high-dimensional feature map, where, H,W and\\\\nH\\\\n^\\\\n,\\\\nW\\\\n^\\\\nare respectively the original and reduced spatial dimensions. The segmentation decoder,\\\\nD\\\\ns\\\\n:\\\\nR\\\\nC\\u00d7\\\\nH\\\\n^\\\\n\\u00d7\\\\nW\\\\n^\\\\n\\u2192\\\\nR\\\\nK\\u00d7H\\u00d7W\\\\n, produces the final segmentation map, xseg, where K is the number of classes, y. We view segmentation and OoD detection as distinct tasks and include a separate decoder for OoD detection,\\\\nD\\\\no\\\\n:\\\\nR\\\\nH\\\\n^\\\\nW\\\\n^\\\\n\\u00d7C\\\\n\\u2192\\\\nR\\\\n1\\u00d7H\\u00d7W\\\\n, which takes a reshaped feature map and maps it to an OoD prediction, xood.'\\n\\n'Extending a contrastive loss from image-wise classification to segmentation means the encoder yields a feature map with reduced spatial dimensions,\\\\nR\\\\n3\\u00d7H\\u00d7W\\\\n\\u2192\\\\nR\\\\nC\\u00d7\\\\nH\\\\n^\\\\n\\u00d7\\\\nW\\\\n^\\\\n, instead of a single feature vector per image, \\u211d3\\u00d7H\\u00d7W \\u2192 \\u211dC. In order to perform pixel-wise OoD detection, the feature map is reshaped to produce\\\\n(\\\\nH\\\\n^\\\\n\\u2217\\\\nW\\\\n^\\\\n)\\\\nvectors of shape, z \\u2208 \\u211dC.'\\n\\n'Our contrastive objective is designed to train an encoder that represents pixels from the training distribution with reliable separability from OoD data. We start with the objective of [23] and define two classes: in-distribution and OoD. Thus, we define\\\\nL\\\\nSupCon\\\\nas the total supervised contrastive loss over the batch B,|B| = N, as:'\\n\\n'As a result of using the feature map from a segmentation encoder,\\\\nE:\\\\nR\\\\n3\\u00d7H\\u00d7W\\\\n\\u2192\\\\nR\\\\nC\\u00d7\\\\nH\\\\n^\\\\n\\u00d7\\\\nW\\\\n^\\\\n, in a contrastive loss is that much smaller batch sizes are used than, for example, in [23] for image classification. This is due to each image producing\\\\n(\\\\nH\\\\n^\\\\n\\u2217\\\\nW\\\\n^\\\\n)\\\\ntimes more vectors for contrasting than in the case of image classification.'\\n\\n'In Stage 1, the encoder and segmentation decoder are trained by both the segmentation and contrastive objectives.'\\n\\n'Stage 2 updates the OoD decoder weights:'\\n\\n'On the OoD detection task, we use the best achieved average intersection over union (IoU) across the generated dataset as a performance metric. On the misclassification prediction task, we report segmentation performance in terms of the class mean intersection over union (mIoU) metric for a range of coverages. Coverage is defined as the fraction of pixels that are not considered OoD and thus considered to be close enough to the training distribution to segment accurately. Here, a certain coverage is reached by varying a threshold on the classification score from the OoD decoder.'\\n\\n'The OoD decoder, Do, and projection head, P, are multilayer perceptrons (MLPs) with three and two hidden layers respectively. Bilinear interpolation is employed in the OoD decoder to upsample to the same spatial dimensions as the input image. Both use batch normalisation and ReLU activation. The projected feature vector, z\\u2032, has dimensionality, z\\u2032 \\u2208 \\u211d64. It is important to note that z\\u2032 is only computed during training \\u2013 not inference.'\\n\\n'The baseline is a segmentation network with no additional decoder, trained on only in-distribution data and uses (1 \\u2212 max softmax score) as a measure of uncertainty, as presented in [9]. In the two tests that follow, different objectives and data augmentation schemes used to enhance the baseline performance are investigated. In terms of objectives, we compare the baseline and the proposed contrastive objective (OoDCon) to a KL divergence loss, which enforces a flat softmax response on OoD data at the end of the segmentation decoder, as seen in [33], and the BCE objective, performed on the OoD decoder [32]. As for data augmentation, we include both our proposed method (c.f. Sec. III-D, named DomainMix) and an adaptation of the Cutmix method [36] to pixel-wise labelling.'\",\"1084\":null,\"1085\":null,\"1086\":\"'https:\\/\\/github.com\\/janbruedigam\\/ConstrainedControl.jl'\\n\\n'https:\\/\\/github.com\\/janbruedigam\\/ConstrainedControl.jl'\",\"1087\":\"'The system of interest is a rotational axis drive, a positioning mechanism driven by a synchronous 3 phase permanent magnet AC motor equipped with encoders for position and speed tracking (see Fig. 1). Such systems are routinely used as components in the semiconductor industry, in biomedical engineering, and in photonics and solar technologies.'\\n\\n'We now demonstrate the proposed adaptive control algorithm on a rotational drive system. The system has an encoder resolution of \\u2206p = 1deg\\u00d710\\u22127 for the angular position, \\u2206v = 0.0004 RPM for the angular velocity and \\u2206T = 0.0008 Nm for the torque. The angular position of the system has no hardware limit. The limits of the angular velocity and the torque are vlim = 50 RPM and Tlim = 3.48 Nm, respectively. First, we show how the controller parameters adapt when the algorithm is explicitly informed about a change in the feed-forward gain Kff. We then demonstrate the performance when an external change occurs, corresponding to change in the rotational resistance, which can be estimated from the system\\u2019s data. The optimization ranges and the kernels hyperparameters are the same as in Section VI. The likelihood variance is adjusted separately for each GP model.'\",\"1088\":\"'The traffic manager software has been coded in python language, using the NetworkX [23] library to easily manage the graphs. All the graphs depend on the layout of the infrastructure, and, hence, they can be computed offline once the roadmap has been defined. The structure and the connectivity of these graphs is never changed during the traffic manager computations. These graphs are updated following the traffic status in terms of attributes and weights linked to the nodes and edges.'\",\"1089\":null,\"1090\":null,\"1091\":null,\"1092\":\"'Experimental setup: 1) Force sensor, 2) Linear encoder, 3) Static load, 4) A DC motor with angular incremental encoder, 5) Twisted strings'\",\"1093\":null,\"1094\":null,\"1095\":\"'PlaneSegNet Architecture: (1) The Encoder:'\",\"1096\":\"'Object segmentation is a key component in the visual system of a robot that performs tasks like grasping and object manipulation, especially in presence of occlusions. Like many other computer vision tasks, the adoption of deep architectures has made available algorithms that perform this task with remarkable performance. However, adoption of such algorithms in robotics is hampered by the fact that training requires large amount of computing time and it cannot be performed on-line.In this work, we propose a novel architecture for object segmentation, that overcomes this problem and provides comparable performance in a fraction of the time required by the state-of-the-art methods. Our approach is based on a pre-trained Mask R-CNN, in which various layers have been replaced with a set of classifiers and regressors that are retrained for a new task. We employ an efficient Kernel-based method that allows for fast training on large scale problems. Our approach is validated on the YCB-Video dataset which is widely adopted in the computer vision and robotics community, demonstrating that we can achieve and even surpass performance of the state-of-the-art, with a significant reduction (~6\\u00d7) of the training time.The code to reproduce the experiments is publicly available on GitHub 1 .'\",\"1097\":null,\"1098\":\"'Panoptic segmentation is the recently introduced task that tackles semantic segmentation and instance segmentation jointly [18]. In this paper, we present an extension of SemanticKITTI [1], a large-scale dataset providing dense point-wise semantic labels for all sequences of the KITTI Odometry Benchmark [10]. This extension enables training and evaluation of LiDAR-based panoptic segmentation. We provide the data and discuss the processing steps needed to enrich a given semantic annotation with temporally consistent instance information, i.e., instance information that supplements the semantic labels and identifies the same instance over sequences of LiDAR point clouds. Additionally, we present two strong baselines that combine state-of-the-art LiDAR-based semantic segmentation approaches with a state-of-the-art detector enriching the segmentation with instance information and that allow other researchers to compare their approaches against. We believe that our extension of SemanticKITTI with strong baselines enables the creation of novel algorithms for LiDAR-based panoptic segmentation as much as it has for the original semantic segmentation and semantic scene completion tasks. Data, code, and an online evaluation service using a hidden test set are publicly available at http:\\/\\/semantic-kitti.org.'\\n\\n'We present an extension of the SemanticKITTI dataset that enables the community to evaluate and benchmark panoptic segmentation approaches using data generated by an automotive LiDAR. We provide the data, code, as well as, an online platform for evaluation using a hidden test set. Additionally, we provide two panoptic segmentation baselines that are built from a combination of state-of-the-art semantic segmentation approaches and a 3D object detector. The goal of this dataset paper is to propel the research on LiDAR-based panoptic segmentation, since it is an important task that will become more relevant, and provide a platform for easy benchmarking of such approaches.'\",\"1099\":null,\"1100\":null,\"1101\":null,\"1102\":\"'https:\\/\\/arxiv.org\\/abs\\/2008.12687v3'\\n\\n'https:\\/\\/bit.ly\\/3gyccU6'\",\"1103\":null,\"1104\":\"'To accurately predict future positions of different agents in traffic scenarios is crucial for safely deploying intelligent autonomous systems in the real-world environment. However, it remains a challenge due to the behavior of a target agent being affected by other agents dynamically and there being more than one socially possible paths the agent could take. In this paper, we propose a novel framework, named Dynamic Context Encoder Network (DCENet). In our framework, first, the spatial context between agents is explored by using self-attention architectures. Then, the two-stream encoders are trained to learn temporal context between steps by taking the respective observed trajectories and the extracted dynamic spatial context as input. The spatial-temporal context is encoded into a latent space using a Conditional Variational Auto-Encoder (CVAE) module. Finally, a set of future trajectories for each agent is predicted conditioned on the learned spatial-temporal context by sampling from the latent space, repeatedly. DCENet is evaluated on one of the most popular challenging benchmarks for trajectory forecasting Trajnet and reports a new state-of-the-art performance. It also demonstrates superior performance evaluated on the benchmark inD for mixed traffic at intersections. A series of ablation studies is conducted to validate the effectiveness of each proposed module. Our code is available at https:\\/\\/github.com\\/wtliao\\/DCENet.'\\n\\n'Predicting multiple future trajectories (the most-likely one indicated by dash line over multiple ones indicated by shadow area) of a target agent (in red) conditioned on its observed movement (solid line) with the consideration of its interactions between neighboring agents (in blue) in mixed traffic. Interaction is learned through a sequence of dynamic maps at each step over the time axis and three layers are dedicated to capturing position, orientation and speed information (indicated by color-coded rectangles) using the self-attention structure.'\\n\\n'The pipeline for the proposed method. The Encoder Y and Encoder X are identical in structure.'\\n\\n'C. Encoder Network'\\n\\n'The spatial-temporal context from both the observation time and prediction time are encoded by Encoder X and Y, respectively. Both encoders have the same two-stream structure: both streams consist of stacked self-attention layers; as illustrated in Fig. 2 one stream is followed by a global average pooling (GApool), while the other one is followed by an LSTM module. The upper stream is trained to learn motion information from the observed trajectory, whose input is the locations vector of the observed trajectory of the target agent\\\\nX\\\\ni\\\\n={\\\\nx\\\\n1\\\\ni\\\\n,\\u22ef,\\\\nx\\\\nT\\\\ni\\\\n}\\u2208\\\\nR\\\\nT\\u00d72\\\\n. The lower stream is trained to explore dynamic interactions among agents from the dynamic maps noted as DM = {O,S,P} \\u2208 \\u211dT\\u00d7H\\u00d7W\\u00d73 (discussed in Sec. III-B). For simplicity, we take the upper stream for illustration. To get a sparse high dimensional representation, Xi is first passed to a 1D convolution layer (Conv1D) and a fully connected (FC) layer. Each of them is followed by a ReLU non-linear activation. We denote this operation as \\u03c0(Xi). A self-attention layer takes as input the Query (Q), Key (K) and Value (V) and outputs a weighted sum of the value vectors. The weight assigned to each value is calculated as the dot-product of the query with the corresponding key:'\\n\\n'Then the GApool is used to extract the temporal dependencies between steps by taking as input the output of the self-attention module and output an encoded representation.'\\n\\n'The lower stream that exploits the dynamic interactions among agents works in the same way but the spatial dependencies among agents are encoded by the hidden states of an LSTM. Finally, the outputs of these two streams are connected and passed to a FC layer for fusion as the encoded information that includes dynamic spatial-temporal context.'\\n\\n'Our method is CVAE-based and predicts multiple trajectories by repeatedly sampling from a learned latent space conditioned on the encoded information. The CVAE is an extension of the VAE [43] by introducing a condition to control the output [2]. Given a set of samples (X,Y)=((X1,Y1), \\u22ef ,(Xm,Ym)), it jointly learns a recognition model q\\u03d5(z|Y, X) of a variational approximation of the true posterior p\\u03b8(z|Y, X) and a generation model p\\u03b8(Y|X, z) for predicting the output Y conditioned on the input X. z are the stochastic latent variables, \\u03d5 and \\u03b8 are the respective recognition and generative parameters. The goal is to maximize the Conditional Log-Likelihood:\\\\nlog\\\\np\\\\n\\u03b8\\\\n(Y\\u2223X)=log\\\\n\\u2211\\\\nz\\\\np\\\\n\\u03b8\\\\n(Y,z\\u2223X)=log\\\\n(\\\\n\\u2211\\\\nz\\\\nq\\\\n\\u03d5\\\\n(z\\u2223X,Y)\\\\np\\\\n\\u03b8\\\\n(Y\\u2223X,z)\\\\np\\\\n\\u03b8\\\\n(z\\u2223X)\\\\nq\\\\n\\u03d5\\\\n(z\\u2223X,Y)\\\\n)\\\\n. According to Jensen\\u2019s inequality [44], the evidence lower bound can be obtained:'\\n\\n'Test: In the test phase, the ground truth of future trajectory is no more available and its pathway is removed (color coded in green in Fig. 2). A latent variable z is sampled from the prior distribution\\\\nN(0,I)\\\\nand concatenated with the observation encodings that serve as the condition for the following trained decoder, so that the decoder can predict a trajectory. To predict multiple trajectories, this process (sampling and decoding) is repeated multiple times.'\\n\\n'The implementation details of training and testing our methods can be found in our code repository.'\\n\\n'First, by comparing to the Baseline, both DCENet w\\/o DMs and Ind-TF had much better results, and DCENet w\\/o DMs was slightly better in the average score and FDE but a little inferior in ADE than Ind-TF. Considering both models only use observed trajectories as input, it indicates that our method (self-attention + LSTM encoder\\/decoder) explored a better spatial-temporal context than Transformer. Furthermore, Ind-TF utilizes BERT, a heavily stacked Transformer structure and must be pre-trained on an external large-scale dataset, while DCENet does not require it. The results of DCENet w\\/o DMs indicates that its superior performance is not because we used more information (dynamic maps).'\\n\\n'Third, interestingly, Trans. En&De that adopts the Transformer encoder and decoder in our framework did not achieve improved performance compared to DCENet. This phenomenon indicates that our self-attention + LSTM encoder\\/decoder structure explored better dynamic context between agents than Transformer encoder\\/decoder in terms of trajectory prediction. The superior performance of DCENet w\\/o DMs against Ind-TF has also confirmed that.'\\n\\n'In this paper, we proposed a novel framework DCENet for multi-path trajectory prediction for heterogeneous agents in various real-world traffic scenarios. We decompose the learning of dynamic spatial-temporal context into exploiting the dynamic spatial context between agents using self-attention and the LSTM encoder and learning temporal context between steps with the following self-attention and global average pooling. The spatial-temporal context is encoded into a latent space using a CVAE module. Finally, a set of future trajectories for each agent is predicted conditioned on the spatial-temporal context using the trained CVAE module. DCENet was evaluated on the Trajnet challenge benchmark and achieved the new state-of-the-art performance on the leader-board. Its superior performance on the inD benchmark further validated its efficacy and generalization ability. The ablation studies justified the impact of each module in DCENet. In the future, we are interested in extending the method for learning the impact from environment\\/static context, e.g., space layout and scene deployment, to further enhance the performance of trajectory prediction.'\",\"1105\":\"'Foot interfaces have been part of surgical operating rooms for many years, albeit not for performing manipulative tasks. Most medical foot interfaces consist of switches composed of one or a few push buttons, used typically, in on\\/off mode or to clutch. This can also be used for diathermy to enable an electrically heated probe via two (color-coded) foot switches. These are placed in front of the surgeon, who operates them with a single foot while being in a standing position [6]. Tele-robotic surgery consoles can use up to seven switches simultaneously for multiple non concurrent functions (e.g., instrument toggling, camera focus, electrosurgery, clutch, etc) [7]. Similarly, robotic endoscope holders like ViKy, AESOP, RoboLens and HIWIN normally require from six to eight foot switches on the floor to control only three degrees of freedom in positioning [8], [9].'\",\"1106\":null,\"1107\":null,\"1108\":\"'The da Vinci Research Kit (dVRK) was introduced in 2012 to provide an affordable, open-source platform for research in robotic minimally-invasive surgery. It provides access to all levels of control but, until now, has relied on an analog controller for the motor current, which cannot easily be customized to improve performance. This paper aims to implement the low-level control digitally and to improve the overall control performance. To enable model-based controller design, the system is first identified using measurements provided by the encoders and internal electronics. The digital current controller is then implemented on the existing field programmable gate array (FPGA). Experiments demonstrate that the new digital current controller yields superior performance compared to the original analog design. In addition, the identified system model is used to design an improved position controller that is also implemented on the FPGA and provides better trajectory tracking than the position controller currently implemented on the control PC. The comparison between simulation and measurement, for both the current and position control, verifies the validity of the model based on the system identification, enabling utilization for future adaptations. The improved low-level control enlarges the possibilities for more accurate operation and the achieved digital implementation enables researchers worldwide to easily adapt the low-level control in future versions of the dVRK.'\\n\\n'This paper presents the digital implementation of the low-level control and the improvement of its performance compared to the implemented analog control loop as well as the digital implementation of the position control on the FPGA. Previously, the FPGA was primarily used for hardware interfaces, low-level safety checks, and encoder-based position and velocity measurements [8]. The digital controllers are demonstrated on the PSM, but the developed open-source solution can be easily extended to the MTM and potentially other robotic arms.'\\n\\n'The da Vinci Research Kit (dVRK) was introduced in 2012 to provide an affordable, open-source platform for research in robotic minimally-invasive surgery. It provides acc...'\\n\\n'In addition, a position controller is designed in simulation, using the identified system model, to obtain higher performance (faster rise times) through the use of nonlinear behaviors, such as control signal saturation, that are difficult to analyze mathematically. The position controller is implemented on the FPGA, as an outer loop around the digital current controller, and is fine-tuned experimentally. It provides improved trajectory tracking performance when compared to the existing position controller. The contributions to the research community lie in the identified system models (for the PSM motors), the digital current and position control implemented in the open-source FPGA firmware, and especially in the systematic approach to modeling and control system design that can be applied to other dVRK components, such as the MTM and ECM, as well as to other robots. We are currently integrating the implemented position and current controllers in the open-source dVRK software stack, so that these improvements can be made available to the community.'\",\"1109\":null,\"1110\":null,\"1111\":null,\"1112\":null,\"1113\":null,\"1114\":\"'Vision based distance measurement system using two-dimensional barcode for mobile robot'\",\"1115\":null,\"1116\":null,\"1117\":null,\"1118\":null,\"1119\":null,\"1120\":null,\"1121\":\"\\\"This paper focuses on visual semantic navigation, the task of producing actions for an active agent to navigate to a specified target object category in an unknown environment. To complete this task, the algorithm should simultaneously locate and navigate to an instance of the category. In comparison to the traditional point goal navigation, this task requires the agent to have a stronger contextual prior to indoor environments. We introduce SSCNav, an algorithm that explicitly models scene priors using a confidence-aware semantic scene completion module to complete the scene and guide the agent's navigation planning. Given a partial observation of the environment, SSC-Nav first infers a complete scene representation with semantic labels for the unobserved scene together with a confidence map associated with its own prediction. Then, a policy network infers the action from the scene completion result and confidence map. Our experiments demonstrate that the proposed scene completion module improves the efficiency of the downstream navigation policies. Code and data: https:\\/\\/sscnav.cs.columbia.edu\\/\\\"\\n\\n'Our primary contribution is SSCNav, a framework that applies semantic scene completion with confidence estimation for the task of object goal visual semantic navigation. By leveraging the contextual priors of the typical indoor environment, our algorithm is able to infer a scene representation beyond its partial observation. We demonstrate that completed scene representation with confidence estimation enables more efficient planning policies for the downstream navigation task. Code and data will be available online.'\",\"1122\":null,\"1123\":\"'https:\\/\\/github.com\\/facebookresearch\\/pytouch'\\n\\n'Instantiating PyTouch with DIGIT sensor to detect touch and slip. PyTouch reduces the amount of code required to do advanced tasks such as these.'\\n\\n'As the field of touch sensing evolves, a need arises for advanced touch processing functionalities capable of efficiently extracting information from raw sensor measurements. Prior work provided some of these functionalities, but typically ad-hoc \\u2013 for single sensors, single applications, and not necessarily with reusable code (e.g., open-source). To better support and grow the tactile sensing community, we believe that is necessary the creation of a software library which brings together advancements in the touch processing field through a common interface. To address this need, we introduce PyTouch, a unified library for touch processing. Our goal with this library is to enable academics, researchers and experimenters to rapidly scale, modify and deploy new machine learning models for touch sensing. In this paper, we detailed the design choices and the corresponding software architecture of this library. PyTouch is designed with opensource at the core, enabling tactile processing \\\"as-a-service\\\" through distributed pre-trained models, and a modular and expandable library architecture. We demonstrate several of the feature of PyTouch on touch detection, slip and object pose estimation experiments using 3 different tactile sensors, and we show that a unified tactile touch library provides benefit to rapid experimentation in robotic tactile processing. We open-source PyTouch at https:\\/\\/github.com\\/facebookresearch\\/pytouch, and aim in continuing the evolution of this library to further the field of touch processing.'\\n\\n'With the increased availability of rich tactile sensors, there is an an equally proportional need for open-source and integrated software capable of efficiently and effec...'\\n\\n'With the increased availability of rich tactile sensors, there is an an equally proportional need for open-source and integrated software capable of efficiently and effectively processing raw touch measurements into high-level signals that can be used for control and decision-making. In this paper, we present PyTouch \\u2013 the first machine learning library dedicated to the processing of touch sensing signals. PyTouch, is designed to be modular, easy-to-use and provides state-of-the-art touch processing capabilities as a service with the goal of unifying the tactile sensing community by providing a library for building scalable, proven, and performance-validated modules over which applications and research can be built upon. We evaluate PyTouch on real-world data from several tactile sensors on touch processing tasks such as touch detection, slip and object pose estimations. PyTouch is open-sourced at https:\\/\\/github.com\\/facebookresearch\\/pytouch.'\\n\\n'A fundamental challenge in robotics is to process raw sensor measurements into high-level features that can be used for control and decision-making. For the sense of vision, the field of computer vision has been dedicated to study provide an algorithmic and programmatic method of understanding images and videos. In this field, open-source libraries such as PyTorch [1], CAFFE [2], and OpenCV [3] have enabled the acceleration of research and collectively brought together commonly used techniques in each perspective domain by providing unified interfaces, algorithms, and platforms. PyTorch and CAFFE have enabled researchers to develop and scale neural networks by reducing the amount of ground work required for implementing algorithms such as backpropagation or convolution functions. Furthermore, OpenCV provided benefits such as a collection of commonly used, tested and optimized functions. These frameworks, libraries and platforms all have an underlying goal in enabling further developments in their respective fields through abstracting lower level processes into building blocks for further applications.'\\n\\n'To address this challenge we introduce PyTouch \\u2013 the first open-source library for touch processing that enables the machine learning and the robotics community to process raw touch data from tactile sensors through abstractions which focus on the experiment instead of the low level details of elementary concepts. PyTouch is designed to be a high-performance, modular, and easy to use library aiming to provide touch processing functionalities \\\"as a service\\\" through a pre-trained model collection capable of kickstarting initial experiments to allowing end applications using new or variations of existing sensors the ability to apply transfer learning in levering the performance of the library. The software library modularizes a set of commonly used tactile-processing functions valuable for various down-stream tasks, such as tactile manipulation, slip detection, object recognition based on touch, etc. Furthermore, the library aims to standardize the way touch based experiments are designed in reducing the amount of individual software developed for one off experiments by using the PyTouch library as a foundation which can be expanded upon for future experimental and research applications.'\\n\\n'Frameworks which are applicable to the domain of tactile touch sensing are far and few between or not available through open-source channels. Observing public repositories and open-source networks, we see very little open-source work pertaining to building a tactile processing framework. Although there is work in isolated instances which provide insight into specific problems such as interfacing with robotic skin and providing interfaces for applications [8], [9], there are no works which brings commonly used paradigms within the field of tactile touch sensing together or that provide general purpose tools. Previous attempts include studying friction models [10], physical property understanding [11], [12], [13], in-hand manipulation, determining physical manipulation characteristics and grasp success. [14] demonstrates individual capabilities related to tactile touch which allow for robotic learning in providing given forces to an object, determining density and texture through stirring actions, and various types of in hand or arm rotation of objects. Other work such as [15] learns robotic grasp success through an end-to-end action-conditional model based off of raw tactile sensor input data. [16] introduces TactileGCN which estimates grasp stability through a graph convolutional network. PyTouch aims at being a general purpose and open-source solution that combines useful computational tools and capabilities into a single framework and thereby proposes an agnostic library for tactile sensors regardless of their hardware or sensing modalities.'\\n\\n'PyTouch aims to provide the same function in the domain of tactile touch processing through the introduction of an open-source library in which proven, tested and performance validated modules allow for scaling touch based applications, research and experimentation. This work aims to set forth a need for the unification of tactile processing based software which not only strengthens the field but also decreases the barrier to entry in designing applications and research which utilize tactile touch. As such, the main goals of PyTouch are to provide a library that is powered by pre-trained models through the \\\"as a service\\\" paradigm, enables transfer learning through the PyTouch for extending models for new hardware sensors, and deploys joint models which work across a variety of vision based tactile sensors. By centralizing touch processing tasks into a software library and through the models provided through PyTouch, this provides a path forward for sensor generalization. This is important due to the variance in produced hardware, image transfer layers and mechanical construction of lab-built sensors.'\\n\\n'https:\\/\\/github.com\\/facebookresearch\\/pytouch'\",\"1124\":\"'https:\\/\\/sites.google.com\\/view\\/contingency-planning\\/home'\",\"1125\":\"'Taking a sequence of depth images as input, ScrewNet first extracts features from the depth images using ResNet, passes them through an LSTM layer to encode their sequential information, and then uses MLP to predict a sequence of screw displacements having a shared screw axis'\",\"1126\":\"'https:\\/\\/rb.gy\\/i6uoke.'\\n\\n'We design the action representation with two key considerations in mind. First, we want to align the action representation with the state representation. Second, we need to preserve information about past action sequences. Thus, we propose to encode the robot\\u2019s trajectory sequence as two 2-channel images with the same dimensions as our top-down visibility map. Say we are interested in the action sequence from t=0 up to a future time t=ti. The first image is a visitation map Ft=0:ti denoting how many times each point on the map will be visited by the robot. This is achieved through the second alpha-channel where darker color represents more frequent visitation. The second image is a time-encoded trajectory map Tt=0:ti denoting the traversal order. Similar to F, the darker the color in T, the later the position will be traversed. Both F and T are necessary for a complete action representation to avoid the ambiguity brought by the trajectory intersection. An example of a training data pair is shown in Fig. 3.'\\n\\n'Network Architecture Our VPT-TOB model is a fully convolutional encoder-decoder network [7]. The encoder is a 8-layer fully convolutional network which takes in the concatenated visual observation and action embedding as input with size 128 \\u00d7 128. The decoder network comprises four transposed convolutional layers where each of them is followed by another 2D convolution and a transposed convolution for high-resolution output [43] with size 128\\u00d7128.'\",\"1127\":\"'Transform prediction network: Once we have a trained feature encoder, we use that encoder (with weights fixed) in the transform prediction network to map input image pairs {It-k, It} into keypoint feature vectors {ft-k, ft}. This is followed by a fully-connected regression network that predicts a relative 2D transformation\\\\nT\\\\nnet\\\\nt\\u2212k,t\\\\nbetween times {t-k, t} (Fig. 2). To make the same transform prediction network work across object classes, we also pass in a one-hot class label vector\\\\nc\\u2208{0,1\\\\n}\\\\nN\\\\nc\\\\n. We expand the feature inputs via an outer product with the class vector, ft-k \\u2297 c, ft \\u2297 c, and pass this expanded input to the fully-connected layers. The label vector c corresponds to a category class of the object shape, we assume we know this for the object being pushed.'\",\"1128\":null,\"1129\":\"'Deep neural networks have been used with great success in modeling interactions between humans in a data-driven manner. A recurrent neural network (RNN) has been widely used to encode the history of pedestrians\\u2019 motion [6], [1], [2], [15], [16]. RNNs use a pooling mechanism to extract interaction information by aggregating the encoded motion features of different individuals. However, such hand-crafted or arbitrary aggregation methods may hinder the acquisition of important representations for future trajectory generation. To address this limitation, a graph neural network has been introduced in this domain. The inherent node-edge topology of graphs helps to model more intuitive and natural interactions between humans [3], [17], [13], [7]. Rather than pooling the information, these approaches find interactions between agents, corresponding to updating edges. However, they are prone to error accumulation over the time horizon because of their structural dependency on recurrent units [3], [17], limited in modeling interactions in space and in time [13], or do not properly capture the importance of interactions in a spatio-temporal space [7]. In contrast, our aim in this work is to propose an approach that can overcome such limitations using graph neural networks [18] with multi-attention in space and in time.'\\n\\n'Social-STAGE Framework: Given input scenario with agents\\u2019 (shown in different colors) past trajectories (shown in dotted line), we perform ST-Graph Convolutions and multi-attention mechanism for encoding meaningful interactions. The encoded features are decoded into multi-modal trajectories (for all agents) and corresponding probabilities to rank each mode. The opacity of each future trajectory prediction is changed to show the corresponding probability in the final output.'\",\"1130\":null,\"1131\":null,\"1132\":null,\"1133\":null,\"1134\":\"'https:\\/\\/github.com\\/RoboticExplorationLab\\/altro-mpc-icra2021.'\\n\\n'In practice, most MPC problems are formulated as convex optimization problems, since convex solvers are available that can guarantee convergence to a globally optimal solution, or provide a certificate of infeasibility if a solution does not exist. A variety of numerical techniques for solving such problems have been developed over the past several decades, and many high-quality solver implementations\\u2014both open-source and proprietary\\u2014are available. In the control community, a particular emphasis has been placed on high-performance solvers for quadratic programs (QPs) that are suited to real-time use on embedded computing hardware, including active-set [4], [5], interior-point [6]\\u2013[8], and alternating direction method of multipliers (ADMM) [9] methods. There has also been at least one interior-point solver for second-order cone programs (SOCPs) developed for embedded applications [10].'\\n\\n'An open-source solver implementation in Julia that delivers state-of-the-art performance for convex MPC problems with a convenient interface for defining trajectory optimization problems.'\\n\\n'We have presented ALTRO-C, a conic augmented Lagrangian method for solving model-predictive control problems with general convex conic constraints. The method was implemented by modifying the open-source ALTRO solver and demonstrated on several benchmark control problems. ALTRO-C fully exploits the structure of trajectory optimization problems, achieves fast convergence to moderate tolerances, and offers good warm-starting capabilities, making it ideal for MPC applications. Comparisons to several QP and SOCP solvers show that our algorithm delivers state-of-the-art performance on small-to-medium-size MPC problems and is suitable for many real-time control applications. Additionally, unlike other specialized conic solvers, ALTRO-C can be readily applied to nonlinear and non-convex problems.'\\n\\n'https:\\/\\/github.com\\/RoboticExplorationLab\\/altro-mpc-icra2021.'\",\"1135\":null,\"1136\":\"'Qualitative results are here and source code is here.'\",\"1137\":null,\"1138\":\"'In this paper, we present a novel framework, referred to as - DESERTS - which uses a learning-enabled closed-loop system to overcome the challenges associated with telesurgery in austere conditions. The goal is to deploy a robotic agent on a forward operating basis, with a surgeon operating it from a safe location. The surgeon\\u2019s movements are encoded into unit instructions (surgemes), which are then sent to a robot, thereby limiting the dependence on the quality and reliability of the network.'\\n\\n'The proposed framework aims to tackle the problem of delays in teleoperated robotic surgery by using a virtual representation of the robot as a bridge between the surgeon and the physical robot. The surgeon operates using a simulator, instead of the live video stream. The simulated system at the server\\u2019s side recognizes and delivers the surgical instructions to the remote robot, while the robot sends back pose information of objects and tools, thereby eliminating the need to encode video frames. This exchange of high-level commands is efficient and allows to semi-autonomously execute surgical maneuvers in a safe manner. The feedback module displays the location of the remote robot and relevant elements in the surgical environment. This information is shown alpha-blended in the simulator side to enable situational awareness. The complete pipeline is shown in Figure 1.'\",\"1139\":null,\"1140\":null,\"1141\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/34840856'\",\"1142\":null,\"1143\":null,\"1144\":null,\"1145\":\"'It is common that the expert demonstrations are collected by drivers with different personalities and in different regions. Particularly, in terms of the decision-making task, different driving styles show different preferences of vehicle merging gaps, safety distance, maximum\\/minimum accelerations, etc., resulting in different data distributions in the states and actions. The driving tasks in our study are considered drawn from such a distribution of different driving styles. As most off-the-shelf simulators have not provided the functionality of conveniently simulating diverse driving behaviors, we develop a new platform with oracle modules to simulate expert driving in multiple styles. The demonstrations include various behaviors, such as yielding behavior where the ego vehicle waits for a safety gap to merge in, overtaking behavior where the ego vehicle accelerates to merge to a gap in front of it on the adjacent lane, and aborting behavior where the ego vehicle aborts the lane changing when it detects potential collisions measured by a safety margin. In this study, we take the conservative and neutral driving styles as the meta-training tasks and the challenging style (aggressive driving) as the meta-testing task to check whether the generalized model really learns the encoded structure across tasks or just performs an average over tasks.'\\n\\n'Vehicle kinematic performance in maximum acceleration (left graph), minimum acceleration (middle graph), and maximum speed (right graph). The distinguishable curves in the three graphs indicate that the agent has learned different driving characteristics. Furthermore, the aggressive style (in red) shows higher values in maximum acceleration and maximum speed, and lower negative values in minimum acceleration, indicating that the meta-training procedure indeed encodes the features of different driving behaviors rather than just performing an average over them.'\\n\\n'Figure 3 shows the driving performance which is represented by three metrics. The rolling-out steps in the left graph and decision-making steps in the middle graph show that the agent takes a long time to make lane changes in early episodes, but leans to make stable performance with fewer steps after trained for hundreds of episodes. The distinguishable curves in these two graphs represent different styles, indicating that the meta-training procedure does encode the characteristics of different tasks instead of just simply averaging over tasks. Furthermore, the online testing curves in red show that the task execution duration for aggressive drivers is shorter than that of conservative and neutral style drivers which is in line with our expectation.'\\n\\n'Figure 4 displays the vehicle kinematic performance of the driving styles in three metrics, i.e. the maximum acceleration, minimum acceleration, and maximum speed. The distinguishable curves indicate that the agent does learn different driving characteristics of different styles, with different value ranges for their corresponding styles. More importantly, the aggressive style shows higher values in maximum acceleration and maximum speed, and lower values (in negative) in minimum acceleration. This further verifies that the trained model indeed encodes the features of different driving behaviors instead of just performing an average over them.'\",\"1146\":\"'https:\\/\\/tinyurl.com\\/y5l8s2ua'\",\"1147\":null,\"1148\":\"'The public evaluation code from [4] was used to obtain our results in Table I. The ORB-SLAM results (from [4]) were obtained by running ORB-SLAM [16] only on the 5-frame snippets (ORB-SLAM short), and by running ORBSLAM on the full sequence and chopping the full estimated trajectory into 5-frame snippets (ORB-SLAM full).'\",\"1149\":\"'We address the problem of image registration to a compressed 3D map. While this is most often performed by comparing LiDAR scans to the point cloud based map, it depends on an expensive LiDAR sensor at run time and the large point cloud based map creates overhead in data storage and transmission. Recently, efforts have been underway to replace the expensive LiDAR sensor with cheaper cameras and perform 2D-3D localization. In contrast to the previous work that learns relative pose by comparing projected depth and camera images, we propose HyperMap, a paradigm shift from online depth map feature extraction to offline 3D map feature computation for the 2D-3D camera registration task through end-to-end training. In the proposed pipeline, we first perform offline 3D sparse convolution to extract and compress the voxelwise hypercolumn features for the whole map. Then at run-time, we project and decode the compressed map features to the rough initial camera pose to form a virtual feature image. A Convolutional Neural Network (CNN) is then used to predict the relative pose between the camera image and the virtual feature image. In addition, we propose an efficient occlusion handling layer, specifically designed for large point clouds, to remove occluded points in projection. Our experiments on synthetic and real datasets show that, by moving the feature computation load offline and compressing, we reduced map size by 87\\u221294% while maintaining comparable or better accuracy.'\\n\\n'Instead of the general 2D binary code, we perform clustering on the learned map features and only store the per-voxel centroid index as the map feature. This is more compact than the binary code used in [25]. Also, the 2D to 3D registration problem we solve is more complicated and challenging than the 2D to 2D registration setting in [25] since it requires backpropagation through projection.'\\n\\n'The compressed map features are projected and decoded to form a virtual feature image using the camera intrinsics and the given initial pose. However, because of the nature of sparse point clouds, the occluded points may appear in the virtual feature image if not handled. To remove the occluded points, we design a maxpooling pyramid inspired by the point cloud occlusion filtering described in [3]. Our occlusion handling layer is very efficient and is suitable for large-scale point clouds since it only utilizes the max pooling layers.'\",\"1150\":null,\"1151\":\"'The stack branch is designed to consist of repeated encoder-decoder architecture with intermediate output. This allows the network for re-evaluation and reassessment of initial estimates and helps to maintain precise local information while considering and then reconsidering the overall coherence of the features. With such stacked hourglass architecture, our network performance has been improved compared to baseline as shown in the ablation study in Section IV-D.'\",\"1152\":\"'The infrastructure is available at: \\/git@github.com: hildebrandt\\u2013carl\\/MixedRealityTesting.git.'\",\"1153\":\"'The success of deep reinforcement learning (RL) and imitation learning (IL) in vision-based robotic manipulation typically hinges on the expense of large scale data collection. With simulation, data to train a policy can be collected efficiently at scale, but the visual gap between sim and real makes deployment in the real world difficult. We introduce RetinaGAN, a generative adversarial network (GAN) approach to adapt simulated images to realistic ones with object-detection consistency. RetinaGAN is trained in an unsupervised manner without task loss dependencies, and preserves general object structure and texture in adapted images. We evaluate our method on three real world tasks: grasping, pushing, and door opening. RetinaGAN improves upon the performance of prior sim-to-real methods for RL-based object instance grasping and continues to be effective even in the limited data regime. When applied to a pushing task in a similar visual domain, RetinaGAN demonstrates transfer with no additional real data requirements. We also show our method bridges the visual gap for a novel door opening task using imitation learning in a new visual domain. Visit the project website at retinagan.github.io'\",\"1154\":\"'The architecture of the proposed model, which consists of 4 feature encoders (left) and prediction heads (right) for 4 modalities, and 1 fusion module (middle) for merging representations of different modalities.'\\n\\n'a) Feature Encoders:'\\n\\n'For auxiliary task prediction heads, we directly reconstruct the next frame. Transposed convolutional layers are employed in each decoder, and the fused map is upsampled to be in the same dimension as the original input. For the visual prediction head (Figure 2b), we use the idea of pixel transformation proposed in [2], [33], and perform two tasks instead of reconstructing the image directly. The first task is learning the pixel transformation parameters for each grouped object. The second task is performing an instance segmentation task that aims to group pixels by object. There are two branches for the visual prediction head. In the object motion capture branch, a motion prediction module called convolutional dynamic neural advection (CDNA) is used [2]. The CDNA function computes new pixel values by applying multiple normalized convolution kernels to previous frames. CDNA is an object-centric motion prediction module, and as it is indicated in [2], the intuition behind it is that pixels form the same rigid entity move together. This module is expressed in the following equation:'\",\"1155\":null,\"1156\":\"'A flat brushless DC motor (UTO-52, Celera Motion, MA, USA) equipped with a high precision incremental encoder (163840 ppr, CE400, Celera Motion, MA, USA), producing a continuous torque rating of 0.353 N\\u2022m. The transmission ratio of the harmonic gearbox (CSD-20-100-2A-GR-SP674, Harmonic Drive, MA, USA) with the maximum average load toque 34N.m is 100:1, resulting in an output torque of approximately 34N\\u2022m, and a peak torque 57 N\\u2022m. Each joint weighs only 1 kg. The entire assembly of the actuator and the transmission is designed to weight 1 kg and it can provide a joint velocity up to 300 deg\\/s, which is sufficient to support fast walking activities. We use the world\\u2019s smallest platinum solo twitter (G-SOLTWIR50\\/100EE1H, Elmo, Israel) as the motor drive with a maximum power of 1000W.'\",\"1157\":null,\"1158\":\"'Hand rehabilitation exoskeletons utilize various control strategies to optimize system operation and improve patient interaction with the exoskeleton. Tendon-driven exoskeletons often employ position control using feedback signals from motor encoders, which may require an extensive calibration process and is generally open-loop [19], [20]. Recently, several studies have examined tendon tension sensing as a form of force control in tendon-driven exoskeletons [21] - [23]. However, full integration of tendon tension sensors and intermediate-level force control such as impedance\\/admittance control has not yet been actively implemented into an exoskeleton system. Such an approach has the potential to provide various virtual grasping dynamics, which is applicable to rehabilitation procedures. This strategy also compensates for the disadvantages of position-based control, such as tendon slacking and inefficient calibration.'\",\"1159\":\"'The linear motion of the slider block was converted to rotary motion at the elbow joint through the slider-crank mechanism. The coupler connected the slider block via a pin joint to the side wall of the timing belt pulley, operating as a crank. Since the crank resided on the timing belt pulley, the crank torque was transmitted from the electric motor through the timing belt drive and bevel gear set. A 14-bit absolute rotary encoder (AS5048, ams, Austria) was installed to measure the elbow joint angle. Due to space constraints, the rotary encoder measured the angular position of the timing belt shaft instead of measuring the angular position of the elbow joint shaft directly. The torque of the electric motor was controlled to move the slider-crank mechanism to regulate spring deflection. By controlling the spring deflection, it was possible to control the joint torque at the elbow, which can be calculated from the following equation:'\\n\\n'The overall spring stiffness and estimated torque from spring deflection were determined. The lower arm of the training simulator was fixed at 90\\u00ba of elbow flexion, and a torque sensor (TQM301-45N, Omega Engineering Inc., USA) was attached to the joint to measure the joint torque generated by the motor. A series of desired spring deflection values were commanded to the system, and the corresponding torque sensor and linear encoder readings of the slider block displacement were recorded. The estimated joint torque based on spring deflection was calculated from the torque sensor reading, and the measured spring deflection was obtained from the linear encoder reading. Then, the linear relationship between the estimated joint torque and measured spring deflection was derived. Since the\\\\ncos(\\\\nsin\\\\n\\u22121\\\\n(\\\\nx\\\\ns\\\\n\\u2216\\\\nl\\\\nc\\\\n))\\\\nterm in Eqn. (1) was very close to 1 for the range of spring deflections that were used in the experiment, the slope of the torque vs. spring deflection plot was estimated to be kslc. Therefore, the overall effective spring stiffness was calculated by dividing the slope of the linear plot by lc.'\\n\\n'The training simulator was actuated by the electric motor to replicate the various resistance torque levels caused by different UPDRS levels of lead-pipe rigidity behavior. The motor was controlled to track a reference torque commanded by the rigidity controller which generated the reference torque based on a proposed rigidity model. The controller was implemented on a microcontroller (TI F28379D, Texas Instrument, USA) and programmed using MATLAB\\/ Simulink Embedded Coder (MATLAB 2020a, Mathworks, USA).'\",\"1160\":\"'https:\\/\\/youtu.be\\/qjSoSmnvFR8'\",\"1161\":null,\"1162\":\"'In this work, we present a per-instant pose optimization method that can generate configurations that achieve specified pose or motion objectives as best as possible over a sequence of solutions, while also simultaneously avoiding collisions with static or dynamic obstacles in the environment. We cast our method as a weighted sum non-linear constrained optimization-based IK problem where each term in the objective function encodes a particular pose objective. We demonstrate how to effectively incorporate environment collision avoidance as a single term in this multi-objective, optimization-based IK structure, and provide solutions for how to spatially represent and organize external environments such that data can be efficiently passed to a real-time, performance-critical optimization loop. We demonstrate the effectiveness of our method by comparing it to various state-of-the-art methods in a testbed of simulation experiments and discuss the implications of our work based on our results.'\\n\\n'We cast our method as a non-linear constrained optimization-based IK problem. The objective function is a weighted sum, where each term in the sum encodes a particular motion objective. The weights on the terms in the objective function set importances of the various terms and allows the optimization solver to relax certain objectives in favor of other, more important, terms in the case of competing objectives. In particular, our method favors motion feasibility objectives, such as avoiding collisions, over other objectives, such as matching end-effector pose goals. Throughout this work, we overview the structure of this overall optimization framework and highlight how this or similar frameworks extend well to the case of environment collision avoidance.'\\n\\n'Our method includes seven objective terms and one constraint in the non-linear optimization structure above by default. The default objective terms encode: (1) end-effector position goal matching; (2) end-effector orientation goal matching; (3) minimized joint velocity; (4) minimized joint acceleration; (5) minimized joint jerk; (6) self-collision avoidance; and (7) environment collision avoidance. The one constraint is designed to avoid kinematic singularities. We model objectives 1\\u20136 and the constraint based on prior work [4]. Derivative information about velocities, accelerations, and jerks are approximated using finite differencing over a short history of prior poses. We provide details on how we structure and pass spatial information into the environment collision avoidance objective in the following section. We note that this overall structure is modular and any of the above objectives or constraints can be removed and any additional objectives or constraints can be accommodated.'\\n\\n'In order to discourage the robot from colliding with the environment, it is necessary to define what it means for the robot to be \\\"close\\\" to an obstacle. We encode a distance to a collision state using the following cost function:'\",\"1163\":null,\"1164\":null,\"1165\":null,\"1166\":\"'https:\\/\\/github.com\\/joaomcm\\/Optimized-UVDisinfection.'\\n\\n'Comparison of a standard stationary mobile robot (left) against an optimized motion (right). Robot carries a tower light. Surfaces are color coded by UV fluence received, with red indicating 0J\\/m2 and green indicating 280J\\/m2 or higher. A stationary light cannot to disinfect much of the environment after 30 minutes, while a mobile robot following our optimally computed trajectory (in orange) achieves near complete coverage. (Best seen in color)'\\n\\n'https:\\/\\/github.com\\/joaomcm\\/Optimized-UVDisinfection.'\",\"1167\":null,\"1168\":null,\"1169\":null,\"1170\":null,\"1171\":null,\"1172\":null,\"1173\":\"'https:\\/\\/youtu.be\\/KVkNUKgViSg'\",\"1174\":\"'https:\\/\\/youtu.be\\/ppabRiPBi6o'\",\"1175\":null,\"1176\":null,\"1177\":null,\"1178\":\"'This section presents the simulation results of A-NMPC for trajectory tracking of different sizes of vessels with large changes in the payload. We use the open-source ACADO toolkit [20] to implement A-NMPC. All simulations were performed in MATLAB R2019b installed on a desktop computer with Intel Core i9-7900X and 64 GB RAM. The sampling time is fixed at 0.1 s in the simulation.'\",\"1179\":\"'The condensed approach for the QP formulation is faster at least than sparse direct transcription (for horizons between 10-20 with 120-240 variables), mainly because we avoid the equality constraints, but other algorithms (such as splines, direct collocation, or other solvers) may perform better computationally. The hardware implementation was built over the base MIT MiniCheetah code. The MPC algorithm is run on a quad-core Intel Atom CPU with 4 GB RAM, at a fixed update frequency of approximately 38.5Hz. The QP is reliably solved on-board within this allotted time. The solving time is \\u2248 3ms for common locomotion (horizon: 10) and \\u2248 7ms for jumps (horizon: 16).'\",\"1180\":\"'github.com\\/Cafolkes\\/koopman-learning-and-control'\",\"1181\":null,\"1182\":\"'In this paper, we consider a multi-robot trajectory planning problem motivated by pesticide spraying tasks in an agricultural field. We consider the problem of using quad-rotors to spray an appropriate amount of pesticide to the infected regions in the field. While this is basically an optimal coverage problem that requires visiting all the infected regions in the shortest time, additional constraints emerge due to the limited amount of pesticide that can be carried by the quad-rotors, which requires the quad-rotors to visit certain pesticide tanks for refilling. The objective is to minimize the total energy consumed by quad-rotors, which is considered as total amount of time spent in traversing between the infected regions and pesticide tanks, rotating in place to change direction of flight, and time spent in spraying and refilling. We present two methods, namely, multiple traveling salesman (MTS) based and clustering based de-compositional approach, respectively. The MTS based method consists of constructing a weighted graph whose weights capture the costs, such that finding a set of paths in the graph each corresponding to a quad-rotor provides a time-optimal trajectory that can be implemented in the field. The path finding problem for the graph can in turn be encoded into a mixed integer linear programming problem, for which efficient tools exist, inspite of being an NP-complete problem [26]. In the clustering based method, first, we decompose the infected regions into k clusters using a clustering algorithm, where we place one quad-rotor at the center of each cluster. Then, we independently solve the time-optimal trajectory generation problem for each quad-rotor using the MTS based method.'\",\"1183\":null,\"1184\":\"'Autonomous navigation in crowded, complex urban environments requires interacting with other agents on the road. A common solution to this problem is to use a prediction model to guess the likely future actions of other agents. While this is reasonable, it leads to overly conservative plans because it does not explicitly model the mutual influence of the actions of interacting agents. This paper builds a reinforcement learning-based method named MIDAS where an Ego agent learns to affect the control actions of other cars in urban driving scenarios. MIDAS uses an attention mechanism to handle an arbitrary number of other agents and includes a \\\"driver-type\\\" parameter to learn a single policy that works across different planning objectives. We build a simulation environment that enables diverse interaction experiments with a large number of agents and develop methods for quantitatively studying the safety, efficiency, and interaction among vehicles. MIDAS is validated using extensive experiments and we show that it (i) can work across different road geometries, (ii) results in an adaptive Ego policy that can be tuned easily to satisfy different performance criteria, such as aggressive or cautious driving, (iii) is robust to changes in the driving policies of external agents, and (iv) is safer and more efficient than existing approaches to interaction-aware decision-making. Code available here.'\",\"1185\":null,\"1186\":null,\"1187\":null,\"1188\":null,\"1189\":null,\"1190\":null,\"1191\":null,\"1192\":\"'Main Results: We propose a new formulation of local, optimization-based motion planning for UAV-trajectories with robustness and asymptotic optimality guarantee. Our method represents the UAV-trajectory using smooth curves, encoded as piecewise B\\u00e9zier curves. For each B\\u00e9zier piece, we formulate the collision-free constraints between its control polygon and the environmental geometry. We show that these constraints can be formulated as a summation of distance functions between geometric primitives (e.g. pointtriangle and edge-edge pairs). Each distance function is differentiable when restricted to the feasible domain and the restriction can be achieved using a primal interior point method (P-IPM).'\",\"1193\":null,\"1194\":null,\"1195\":null,\"1196\":null,\"1197\":\"'Visual navigation for autonomous agents is a core task in the fields of computer vision and robotics. Learning-based methods, such as deep reinforcement learning, have the potential to outperform the classical solutions developed for this task; however, they come at a significantly increased computational load. Through this work, we design a novel approach that focuses on performing better or comparable to the existing learning-based solutions but under a clear time\\/computational budget. To this end, we propose a method to encode vital scene semantics such as traversable paths, unexplored areas, and observed scene objects\\u2013alongside raw visual streams such as RGB, depth, and semantic segmentation masks\\u2014into a semantically informed, top-down egocentric map representation. Further, to enable the effective use of this information, we introduce a novel 2-D map attention mechanism, based on the successful multi-layer Transformer networks. We conduct experiments on 3-D reconstructed indoor PointGoal visual navigation and demonstrate the effectiveness of our approach. We show that by using our novel attention schema and auxiliary rewards to better utilize scene semantics, we outperform multiple baselines trained with only raw inputs or implicit semantic information while operating with an 80% decrease in the agent\\u2019s experience.'\\n\\n'Information about traversable and non-traversable space is extremely useful to support path planning in autonomous agents; however, our intuition is that a large amount of this information, particularly for short-term action decisions, is encoded in depth image already, as evidenced by the strong performance of agents with only a depth sensor [30]. Although exploration and map-building allow the learned policy to better support back-tracking and movement to unexplored areas, we propose that discriminating different objects and free space is more useful for long-term decision making and reward prediction, particularly in large environments where an agent must traverse several rooms to achieve its goal.'\\n\\n'An overview of proposed map Transformer module. The input semantic map (color-coded by class) is on the left and, the encoded positional indices (with colors becoming darker closer to the agent) is on the right.'\",\"1198\":\"'We showed that meta-learning on an embedding space produced by a visual encoder demonstrates promising results at the adaptation of the navigation agents in the new sensory configurations. We effectively apply our method to two different adaptation scenarios, object color and sensor height changing problems, in the target-driven visual navigation task. Our method requires only a handful of shots from the target environments and outperforms the baseline generalization methods described in the paper.'\",\"1199\":\"'Multi-Modal Cross Attention Encoder.'\\n\\n'Multi-Modal Attention Decoder.'\",\"1200\":\"'Predicting agents\\u2019 future trajectories plays a crucial role in modern AI systems, yet it is challenging due to intricate interactions exhibited in multi-agent systems, especially when it comes to collision avoidance. To address this challenge, we propose to learn congestion patterns as contextual cues explicitly and devise a novel \\\"Sense\\u2013Learn\\u2013Reason\\u2013Predict\\\" framework by exploiting advantages of three different doctrines of thought, which yields the following desirable benefits: (i) Representing congestion as contextual cues via latent factors subsumes the concept of social force commonly used in physics- based approaches and implicitly encodes the distance as a cost, similar to the way a planning-based method models the environment. (ii) By decomposing the learning phases into two stages, a \\\"student\\\" can learn contextual cues from a \\\"teacher\\\" while generating collision-free trajectories. To make the framework computationally tractable, we formulate it as an optimization problem and derive an upper bound by leveraging the variational parametrization. In experiments, we demonstrate that the proposed model is able to generate collision- free trajectory predictions in a synthetic dataset designed for collision avoidance evaluation and remains competitive on the commonly used NGSIM US-101 highway dataset. Source code and dataset tools can be accessed via Github.'\\n\\n'The proposed method offers three unique advantages over prior methods. First, we represent the contextual cues by congestion via graph-based generative learning, wherein the node of the graph is the agent, and the edge of the graph is a measurement of the distance between two agents. Such a rep-resentation subsumes the Social Force (SF) commonly used in physics-based approaches; it not only models pair-wise SF but also provides a holistic view through graph modeling. Moreover, it implicitly encodes the (relative) distance as the cost, similar to how a planning-based method models the environment. Specifically, we use a Gaussian Mixture Model (GMM) to summarize the congestion patterns to (i) properly accounted for various modes in the congestion patterns, and (ii) reduce parameter space for the training process while maintaining a high task performance.'\\n\\n'The proposed architecture for congestion-aware multi-agent trajectory prediction. The teacher model (top) is composed of the frame-wise graph construction module, and the Graph Convolution Network (GCN)-VAE graph encoder and decoder. The learned latents are passed to a GMM and used to unsupervisedly learn the multi-modal congestion patterns. The student model (bottom) makes prediction based on the observed trajectories. It follows the encoder-pooling-decoder design and uses the CPM module to match the teacher\\u2019s congestion patterns. The loss terms L1 and L2 are defined in Eq. (6) and Eq. (9), respectively.'\\n\\n'The final model recruits an encoder\\u2013pooling\\u2013decoder network design and is therefore compatible with many existing trajectory prediction architectures (e.g., [28], [37]). In the experiments, we show the superiority of the proposed method in collision-free trajectory prediction in a new synthetic dataset designed to evaluate collision avoidance. Furthermore, the model remains competitive on the typical benchmark of the NGSIM US-101 highway dataset [38].'\\n\\n'Generative Learning: We leverage Variational Auto- Encoder (VAE) [73] to unsupervisedly learn the latent congestion pattern z in the graphs. Specifically, we follow the graph VAE approach proposed in GCN [74] where both the encoder and decoder are instantiated as graph convolutional layers. The objective is to optimize the reconstructed graph representation At while regularizing the latent distribution.'\\n\\n'Ablation Study: We also conduct an ablation study to verify the efficiency of the proposed approach on collision- free trajectory prediction. Specifically, we show that our approach is compatible with other encoder-pooling-decoder architectures. By swapping the current architecture design to S-LSTM, we further improve S-LSTM\\u2019s performance on both datasets. We compare whether directly enforcing the student model to match the latent congestion features could be better than distributional modeling and find that building a multi-modal distribution on congestion patterns can significantly improve performance. We hypothesize that this is because the GMM can account for the various modes in congestion patterns. Finally, we search for the hyperparameter on the number of mixtures. We notice that, coherent to the assumption on congestion pattern learning, the model achieves the best performance when the hidden mixture number equals that of the ground-truth. Please refer to the supplementary video for details of the ablation study.'\",\"1201\":null,\"1202\":\"'We proposed a new instance of joint MOT approach that simultaneously optimizes object detection and data association modules. Moreover, to model spatial-temporal object relations, we used GNNs to learn more discriminative features that benefit both detection and data association. Through extensive experiments, we showed effectiveness of GNNs under the joint MOT framework and achieved state- of-the-art performance on the MOT challenges. Our code will be released so that future follow-up work can easily build on top of our method to advance the state-of-the-art.'\",\"1203\":\"'https:\\/\\/github.com\\/facebookresearch\\/droidlet'\\n\\n'https:\\/\\/github.com\\/facebookresearch\\/droidlet'\\n\\n'https:\\/\\/bit.ly\\/3jMGhkn'\\n\\n'One approach that has shown promise is to build virtual agents in simulations. Simulations allow researchers to explore self-supervised and self-directed learning agents that have access to large data, and so build on recent advances in ML that have demonstrated great success in the large-data setting. However, despite the expanding sophistication of simulated environments, there is still the danger that agents only learn what humans encode into the simulation.'\\n\\n'In recent years, there have been significant advances in building end-to-end Machine Learning (ML) systems that learn at scale. But most of these systems are: (a) isolated (perception, speech, or language only); (b) trained on static datasets. On the other hand, in the field of robotics, large-scale learning has always been difficult. Supervision is hard to gather and real world physical interactions are expensive.In this work we introduce and open-source droidlet, a modular, heterogeneous agent architecture and platform. It allows us to exploit both large-scale static datasets in perception and language and sophisticated heuristics often used in robotics; and provides tools for interactive annotation. Furthermore, it brings together perception, language and action onto one platform, providing a path towards agents that learn from the richness of real world interactions.'\\n\\n'In this work we introduce and open-source the droidlet platform and droidlet agent (an agent instantiation that forms the core of the platform). These allow a robot to meaningfully interact with the world and people in the world using current technology, but also provide a substrate for exploring new learning methods. The platform and agent provide'\\n\\n'https:\\/\\/github.com\\/facebookresearch\\/droidlet'\",\"1204\":\"'Note that it is not an issue that the network was trained on subsets of lidar frames, because, as it is carefully explained in the KPConv paper, the effective receptive field in the latest stage of the network does not exceed a few meters. We also want to highlight that the network is not optimized yet for real-time and can only process 2 frames per second in our setup. This is not a problem as we work in simulation for the time being. However, we are well aware of the bottlenecks in the implementation and have optimization in mind for real-world experiments in future works. In addition to optimizing the code, a trade-off between performance and computing speed can be set with the parameter dlin. It is possible to use a larger subsampling size to reach real-time, at the cost of loss in prediction accuracy.'\",\"1205\":\"'Our test platform was a Clearpath Husky A200 unmanned ground vehicle (UGV), a skid-steer robot with a maximum speed of 1 m\\/s. Husky was equipped with wheel encoders and a calibrated triple-axis LORD Microstrain 3DM-GX5-25 IMU comprising a magnetometer, gyroscope and accelerometer. Images were collected with an 8 MP monocamera mounted on Husky at a height of 0.5 to 1.2 m. To reduce light reflection from reflective classes e.g. gravel, the camera was set at about 0\\u25e6 to 10\\u25e6 from the horizontal towards the ground. Outdoors, a Swift GPS unit was used with real-time kinematic (RTK) corrections. The GPS, wheel encoder and IMU sensor data are fused using an extended Kalman Filter hosted on a Intel i7 Lenovo computer which runs the Robot Operating System (ROS) on Ubuntu for interacting with Husky. A Vicon motion camera tracker system was used for localization during indoor testing.'\",\"1206\":\"'https:\\/\\/github.com\\/chrisjtan\\/RWS'\\n\\n'https:\\/\\/github.com\\/chrisjtan\\/RWS'\",\"1207\":null,\"1208\":\"'Reward Machines (RMs) decompose tasks into finite-state machines that encode reward functions for MDPs [4]. An RM is characterized by a set of internal abstract states forming a graph, which the agent navigates through different modes of operation (see Figure 1b). Abstract state transitions indicate different sub-policies, each associated with an individual reward function. The graph structure of RMs can be exploited toward more sample-efficient RL (see [4], [15], [16]). The abstract states of an RM do not need to be directly correlated with the progression of one specific task. This property makes them practical for real robots, where various simple hardware signals\\u2014e.g., end effector states, gripper opening, suction flow readings, proximity sensor readings, joint encoders, etc.\\u2014 can be used as features to construct abstract states (see Figure 1a).'\\n\\n'We construct reward machines from demonstrations with the help of hard-coded feature detectors. On a real robot, these features can be obtained by handcrafted heuristics with sensory input, or training CNN\\u2019s to detect the presence or pose of task-specific objects. Given a robot equipped with RGB-D cameras and simple force sensors, it is feasible to engineer feature detectors that detect whether the end effector is holding a block, the color of such block, and whether a block of certain color is on the table or in the fixture. The value of state features can be used to solve these tasks at an abstract level\\u2014e.g., if one block is not on their designated plate, then pick up such block; if the end effector is holding a block, then place it on their designated plate. Additional details on the environment setup on our website.1'\",\"1209\":\"'Most similar to our work, context-conditioned reinforcement learning with imagined goals (CCRIG) learns a conditional variational auto-encoder (CVAE) [32] that generates goals conditional on the current scene [23]. CCRIG was able to learn pushing skills that generalized mainly to object color and partially to object geometry. Our work differs from CCRIG in a number of ways. First, we learn expressive generative models that are able to generate goals in scenes with significantly more visual diversity. Second, we learn a diverse set of skills (e.g., grasping, drawer opening) that require the goal generation to understand affordances of the environment. Finally, we show that we can use off-policy RL on prior experience, in addition to fine-tuning further on a single specific task to learn new skills.'\",\"1210\":null,\"1211\":\"'Tactile sensing has seen a rapid adoption with the advent of vision-based tactile sensors. Vision-based tactile sensors provide high resolution, compact and inexpensive data to perform precise in-hand manipulation and human-robot interaction. However, the simulation of tactile sensors is still a challenge. In this paper, we built the first fully general optical tactile simulation system for a GelSight sensor using physics based rendering techniques. We propose physically accurate light models and show in-depth analysis of individual components of our simulation pipeline. Our system outperforms previous simulation techniques qualitatively and quantitative on image similarity metrics. Our code and experimental data is open-sourced at\\\\nproject page\\\\nproject page.'\\n\\n'github'\",\"1212\":\"'https:\\/\\/github.com\\/edwinolson\\/axle'\\n\\n'The mix of dense matrix operations needed to implement a solution are fairly limited and operate on matrices of fixed size, which makes implementation with highly-optimized codes (e.g. using SIMD instructions) possible.'\\n\\n'We also show how the same code can be used to perform trajectory interpolation. The interpolations are almost identical to the trajectory fitting operations, except that the unary potentials also observe heading (and thus are 3 \\u00d7 1 measurements), and that only the first and last node have the unary potentials. It is thus up to the optimization framework to \\\"fill in\\\" the rest.'\\n\\n'https:\\/\\/github.com\\/edwinolson\\/axle'\",\"1213\":null,\"1214\":null,\"1215\":\"'An alternative coming from differential geometry is to encode constraints not with forces, but with metrics. For example, correctly designed Riemannian metrics [16] defined on the robot configuration manifold have been proposed to curve the manifold to prevent constraint violation, not due to forces pushing the robot away but due to the space stretching infinitely in the direction of constraints [17], [18]. Such a reliance on curvature rather than competing potential functions may also eliminate traps due to potential function local minima [19].'\\n\\n'We provide a class of constraint-enforcing tasks en-coded solely via simple, analytical Riemannian metrics that stretch the space, rather than via traditional barrier function potentials, eliminating potential function local minima.'\\n\\n'We demonstrate PBDS policy behavior in numerical experiments and at 300-500 Hz on a 7-DoF arm, and we provide a fast open-source Julia library called PBDS.jl.'\",\"1216\":null,\"1217\":null,\"1218\":null,\"1219\":null,\"1220\":null,\"1221\":\"'We hypothesize that the V network will be sensitive to changes in viewpoint, but not to tool changes, while the S network will be sensitive to changes in joint configurations and tool, but not to changes in material. By combining both state and visual information, we expect the V+S network to match the performance of the best performing type of network over all unseen conditions. While recurrent networks can encode temporal information that should capture the effects of viscoelastic tissue properties on manipulation we reason that by incorporating the velocity estimates into a single time frame input, we will likely encode part of this information. We also reason that surgical movements usually occur on slow timescales that mitigate the contribution of viscoelasticity, such that they are negligible with respect to network prediction error. These factors lead us to predict that the accuracy of our network will be comparable to that of the recurrent network, but with faster computation times, due to its reliance on single time frame inputs, that make it more suited for real-time use cases.'\\n\\n'The authors thank Negin Heravi for her ideas on CNN analysis, Diego Dall\\u2019Alba for providing the configuration files for the Maryland tool, and Arturo Marban for providing his image processing and TensorFlow RNN code. Compute was provided by the Stanford Research Computing Center.'\",\"1222\":null,\"1223\":\"'Optimal control problems are often written with running, or time-additive, cost functions. That is, the objective of interest is typically a sum of time-varying functions over a fixed time horizon. Although this structure is reasonably general and easily amenable to both locally optimal and globally contractive optimal control methods, not all scenarios of interest can be expressed with a running cost. For example, in problems which encode properties like collision-avoidance (Fig. 1), a time-additive objective can indicate safety even for an unsafe trajectory. Encoding these types of requirements with time-additive costs requires the introduction of (typically nonconvex) inequality constraints, which complicate solution methods. On the other hand, by considering a maximum-over-time objective structure we can accurately assess the safety of trajectories without introducing explicit constraints. Similarly, a minimum-over-time structure naturally expresses constraint satisfaction at any time, rather than for all time. Aside from reducing problem complexity, these extremum-over-time formulations are also easily amenable to minimally-invasive control designs in which a nominal motion planner and tracking controller are used until a monitor detects potential constraint violation.'\",\"1224\":\"'https:\\/\\/youtu.be\\/lPpMVfBzZH0'\",\"1225\":\"'Though useful, the curvilinear nature of deformation precludes traditional sensing methods such as encoders. Methods that are specific to sense continuum deformation such as Fiber Bragg Grating [7] and electromagnetic sensors are effective, but interfere with the felxibility of the SCA or are altered by environmental disturbances [8]. Alternatively, vision-based sensing [9]\\u2013[24] has gained prominence as it is noninvasive and easy to implement. Vision-based sensing methods seek to reconstruct the 3D shape of the SCA from images obtained from a camera placed in close proximity to the arm.'\",\"1226\":\"'https:\\/\\/github.com\\/charm-lab\\/Vine_Simulator'\\n\\n'https:\\/\\/github.com\\/charm-lab\\/Vine_Simulator'\",\"1227\":null,\"1228\":null,\"1229\":null,\"1230\":null,\"1231\":\"'This paper aims to mitigate straggler effects in synchronous distributed learning for multi-agent reinforcement learning (MARL) problems. Stragglers arise frequently in a distributed learning system, due to the existence of various system disturbances such as slow-downs or failures of compute nodes and communication bottlenecks. To resolve this issue, we propose a coded distributed learning framework, which speeds up the training of MARL algorithms in the presence of stragglers, while maintaining the same accuracy as the centralized approach. As an illustration, a coded distributed version of the multi-agent deep deterministic policy gradient (MADDPG) algorithm is developed and evaluated. Different coding schemes, including maximum distance separable (MDS) code, random sparse code, replication-based code, and regular low density parity check (LDPC) code are also investigated. Simulations in several multi-robot problems demonstrate the promising performance of the proposed framework.'\\n\\n'In this section, we introduce a coded distributed learning framework that can be incorporated with any general policy gradient method to solve the aforementioned MARL problem efficiently in the presence of stragglers. Before we describe this framework, we first overview a distributed learning framework for MARL without coding.'\\n\\n'A. Uncoded Distributed Learning for MARL'\\n\\n'Illustration of uncoded distributed learning for MARL.'\\n\\n'In an uncoded distributed learning system, each learner updates the parameters for a single agent. In other words, different learners are responsible for different agents and only M out of the N learners are used. The assignment matrix CUncoded \\u2208 \\u211dN\\u00d7M has entries:'\\n\\n'Uncoded'\\n\\n'In this paper, we focus on synchronous learning systems. Therefore, the central controller does not update the agent policies until all updated parameters have been received. The learning efficiency is thus bounded by the slowest learner at each training iteration. Any node or link failure may affect the whole task. In the following subsection, we introduce a coded distributed learning framework to address this problem.'\\n\\n'In this coded learning framework, the central controller will be able to recover all updated parameters, denoted as\\\\n\\u03b8\\\\n\\u2032\\\\n=\\\\n[\\\\n\\u03b8\\\\n\\u2032\\\\nT\\\\n1\\\\n,\\u2026,\\\\n\\u03b8\\\\nT\\\\nM\\\\n]\\\\nT\\\\nwith results received from only a subset of the learners. Particularly, let\\\\nI={j\\u2223\\\\ny\\\\n\\u2032\\\\nj\\\\n is received }\\\\nrepresent the set of learners whose results are received by the central controller by a certain time. Also let\\\\nC\\\\nI\\\\n\\u2208\\\\nR\\\\n|I|\\u00d7M\\\\nbe a submatrix of C formed by the j-th rows of C,\\\\n\\u2200j\\u2208I\\\\n. Then \\u03b8\\u02b9 can be recovered when\\\\nrank(\\\\nC\\\\nI\\\\n)=M\\\\nvia:'\\n\\n'An MDS code [41] specifies the assignment matrix such that any M rows have full rank, by using, e.g., a Vandermonde matrix [42] as follows'\\n\\n'A random sparse code [40] enables a sparser assignment matrix CRandom with (j,i)th entry randomly generated from a Gaussian distribution\\\\nN(0,1)\\\\nwith probability pm, i.e.,\\\\nP(\\\\nc\\\\nj,i\\\\n=\\u03b5)=\\\\np\\\\nm\\\\n, where\\\\n\\u03b5\\u223cN(0,1)\\\\n. Otherwise, cj,i = 0 with\\\\nP(\\\\nc\\\\nj,i\\\\n=0)=1\\u2212\\\\np\\\\nm\\\\n. Note that, by choosing an appropriate pm, we can control the sparsity of the assignment matrix CRandom.'\\n\\n'In this section, we apply the coded distributed learning framework to enhance the training efficiency of MADDPG [3] and its resilience to stragglers. Unlike the general formulation of MARL described in Sec. II, MADDPG adopts a deterministic policy \\u03c0i(si) for each agent i \\u2208 [M], which only depends on the local state si. The value function\\\\nQ\\\\n\\u03c0\\\\ni\\\\n(s,a)\\\\nof agent i still depends on the joint state s and joint action a. Moreover, for each agent i, four neural networks are used to approximate its policy \\u03c0i(si;\\u03b8p,i), value function\\\\nQ\\\\n\\u03c0\\\\ni\\\\n(s,a;\\\\n\\u03b8\\\\nq,i\\\\n)\\\\n, target policy\\\\n\\u03c0\\\\n^\\\\ni\\\\n(\\\\ns\\\\ni\\\\n;\\\\n\\u03b8\\\\n^\\\\np,i\\\\n)\\\\n, and target value function\\\\nQ\\\\n\\u03c0\\\\n^\\\\ni\\\\n(s,a;\\\\n\\u03b8\\\\n^\\\\nq,i\\\\n)\\\\n, respectively.\\\\n\\u03c0\\\\n^\\\\n=(\\\\n\\u03c0\\\\n^\\\\n1\\\\n,\\\\n\\u03c0\\\\n^\\\\n2\\\\n,\\u2026,\\\\n\\u03c0\\\\n^\\\\nM\\\\n)\\\\ndenotes the concatenated target policy. Therefore,\\\\n\\u03b8\\\\ni\\\\n=[\\\\n\\u03b8\\\\np,i\\\\n,\\\\n\\u03b8\\\\nq,i\\\\n,\\\\n\\u03b8\\\\n^\\\\np,i\\\\n,\\\\n\\u03b8\\\\n^\\\\nq,i\\\\n]\\\\n. To optimize these parameters using the coded distributed learning framework, a central controller and multiple learners need to be implemented, whose interactions are described in detail as follows.'\\n\\n'To demonstrate the effectiveness of the proposed coded distributed learning framework, we compare the coded distributed MADDPG with the original centralized MADDPG. The total number of agents is set to M = 8. In the competitive environments, the number of adversary agents is set to K = 4. The cumulative rewards of all agents averaged over 250 training iterations for each environment is shown in Fig. 3(a)- 3(d). As we can see, in all environments, the coded distributed MADDPG is able to generate the same quality of policies as the original MADDPG and converges within the same number of iterations.'\\n\\n'To evaluate the efficiency of the proposed coded distributed learning framework, we compare the training time of the coded distributed MADDPG with the uncoded distributed MADDPG. Different coding schemes, including the replication-based code, MDS code, random sparse code and regular LDPC are evaluated.'\\n\\n'When the straggler effect is relatively large, we can observe that the performance of the uncoded scheme degrades significantly and achieves worse performance than most coding schemes. Furthermore, its performance remains stable as the number of stragglers increases. This is because each straggler is delayed by the same amount of time, ts, such that each iteration is always delayed by ts no matter how many stragglers are present. In contrast, the coding schemes are generally more robust to stragglers, except when the number of stragglers exceeds the limit that the coding schemes can tolerate.'\\n\\n'Next, we analyze the performance of different coding schemes. We can observe that the MDS code (orange bars) is very robust to stragglers when the number of stragglers k does not exceed the maximum tolerable number N \\u2212 M. However, when k > N \\u2212M, the MDS performance degrades significantly, as shown in Fig. 4(c)- 4(d) and Fig. 5(c)- 5(d). Furthermore, we can observe that when k < N\\u2212M and when the straggler effect is relatively large, MDS outperforms the uncoded scheme, as well as the replication-based and regular LDPC codes, as shown in Fig. 4(b)- 4(d) and Fig. 5(b)- 5(d). However, when the straggler effect is relatively small, the MDS code has the worst performance, as shown in Fig. 4(a) and 5(a), due to the high computational redundancies it introduces through the dense assignment matrix.'\\n\\n'The random sparse code (green bars) shows a similar performance to the MDS code. This is because when its parameter pm takes a large value (pm = 0.8 in our experiments), the assignment matrix generated by this type of code has a similar density as the one generated by the MDS code.'\\n\\n'Finally, both the replication-based code (red bars) and regular LDPC code (purple bars) are more affected by an increase in the stragglers, as their assignment matrices are sparser. However, they achieve better performance than MDS and random sparse codes when the straggler effect is small, as shown in Fig. 4(a) and 5(a), and when there are many stragglers, as shown in Fig. 4(c)- 4(d) and Fig. 5(c)- 5(d).'\\n\\n'The above studies suggest guidelines for selecting an appropriate coding scheme in different scenarios. If the straggler effect is small, uncoded, replication-based or a regular LDPC scheme should be preferred. If the straggler effect is relatively large but the number of stragglers is small, the regular LDPC or replication-based schemes are suggested. Finally, if the straggler effect is large and many stragglers are present, the MDS code or a random sparse code with pm set to a large value should be chosen.'\\n\\n'This paper introduces a coded distributed learning framework for MARL, which improves the training efficiency of policy gradient algorithms in the presence of stragglers while not degrading the accuracy. We applied the proposed framework to a coded distributed version of MADDPG, a startof-the-art MARL algorithm. Simulations on several multirobot problems demonstrate the high training efficiency of the coded distributed MADDPG, compared with the traditional uncoded distributed learning approach.'\\n\\n'The results also show that the coded distributed MADDPG generates policies of the same quality as the original centralized MADDPG and converges within the same number of iterations. Furthermore, we investigated different coding schemes including replication-based, MDS, random sparse, and regular LDPC codes. Simulation results show that the MDS and random sparse codes can tolerate more stragglers but introduce larger computation overhead. Additionally, the replication-based and regular LDPC codes produce less overhead but are more susceptible to stragglers.'\\n\\n'Coded Distributed Learning for MARL'\\n\\n'Coded Distributed MADDPG'\\n\\n'B. Coded Distributed Learning for MARL'\\n\\n'1) Replication-based Code'\\n\\n'2) MDS Code'\\n\\n'3) Random Sparse Code'\\n\\n'4) Regular LDPC Code'\\n\\n'Algorithm 1 Coded Distributed MADDPG'\",\"1232\":\"'We investigate the extent to which the physical activation mechanism and the mapping paradigm of an interface explain differences in usage characteristics. To that end, we perform a pilot study using three common interfaces employed by powered wheelchair users\\u2014namely, a joystick, headarray, and sip-n-puff (SNP). We evaluate the operation of these interfaces on two open-source computer game tasks designed to assess trajectory and command following performance [5], [20]. To understand the effect of how the signal is altered through the interface, in addition to the most common mappings, we also remap the joystick and headarray to match the constraints of the SNP interface. This is the only direction of remapping possible because the joystick and headarray are higher dimensional than the SNP. The results indicate that even though the physical mechanism of providing input is different, when the control mappings are similar, the usage characteristics are normalized across the interfaces (Figure 1). Moreover, under the constrained mappings the performance characteristics under joystick and headarray interfaces suffer, which motivates the need for an interface-aware assistance system that will compensate for the degradation in overall human-robot team performance.'\",\"1233\":\"'We similarly should expect diverse adroitness from our robots. Yet, recent successes in algorithms which can codesign robots over morphology and control have been typically catered to singlular task specifications. For example, an algorithm may be able to co-design robots for forward running speed, energy efficient gaits, or climbing rough terrains, but not all three skills at once. In order to computationally develop robots capable of composite tasks rather than single repetitious motions, we require algorithms that simultaneously optimize over collections of requisite skills.'\",\"1234\":null,\"1235\":null,\"1236\":null,\"1237\":\"'In this section, we look at the problem of designing a constrained 3R serial robot. One of the motivations behind this study is to arrive at customized prosthetic solutions for patients with traumatic partial digit amputations as well as for designing a humanoid robotic hand. Each finger has three phalanges, namely, the proximal, the middle and the distal phalanx. Among these, the distal phalanx is codependent and it moves in a coordinated manner with the middle phalanx. Thus, it must be possible to design a mechanically constrained finger with 2 DOF, instead of 3. The optimization framework followed is analogous to the earlier design problem with some key differences which are explained briefly in the following.'\\n\\n'For the design of a humanoid finger, we obtained the required data by recording a video of the flexion\\/extension kinematics of first author\\u2019s left index finger with markers placed near the joints. Data processing was done in an open source video analysis tool, Tracker [20]. We used regression analysis on the noisy data to fit a quadratic function profile. Then, 21 equally spaced positions were chosen in the range of \\u03bc to form the design specification as shown in Fig. 8. We executed the parameter homotopy run consisting of 24 homotopy paths and obtained 24 successful endpoints for the target system. Six of the 24 solutions were found to correspond to physical mechanisms as reported in Table II. Four of them were saddle points and the remaining two were local minimum, Design #6 being the global minimum.'\",\"1238\":null,\"1239\":null,\"1240\":\"'https:\\/\\/youtu.be\\/imeC-Eri_nM'\\n\\n'The result in Proposition 1 allows us to encode the execution of tasks as a constraint, similarly to the undisturbed system as'\\n\\n'Note that accounting for the disturbance shrinks the size of the set of control inputs that can satisfy the inequality. Moreover, in cases where the disturbance is large, it may be the case that the robots cannot satisfy the inequality while respecting their control limits. This indicates that the disturbance is too large for the robot to handle and as such the robot is incapable of executing the task. To encode this, we limit the magnitude of the learned disturbance. Specifically, we enforce\\\\n\\u2225\\\\n\\u03c8\\\\nij\\\\n(x)\\u2225\\\\n\\u221e\\\\n\\u2264\\\\nd\\\\nmax\\\\n,\\u2200j\\u2208{1,\\u2026,p}\\\\n, where dmax is a user-defined threshold. The latter allows the robots to only execute tasks for which they can overcome the disturbance, while also accounting for disturbances that the robots cannot overcome through the adaptive specialization law as discussed in the next subsection. Regarding the incurred computational complexity, since the term\\\\nmin\\u2207\\\\nh\\\\nm\\\\n(x\\\\n)\\\\n\\u22a4\\\\n\\u03a8(x)\\\\ndoes not depend on the control input u, the additional cost is O(Np), where N is the number of robots, and p is the number of points forming the convex hull of the disturbance.'\\n\\n'High-level diagram of the proposed framework. The main optimization program solved is denoted by the blue box, where the task execution is encoded using the RCBFs obtained from the data-driven models (in our case GPs), and specializations from the adaptive specialization update. The specialization update is computed using the disturbance from the last step to approximate the nominal progress, along with the current state to estimate the actual progress.'\",\"1241\":\"'https:\\/\\/youtu.be\\/NjJ1F3E7UKs'\\n\\n'For the experiments we used small differential drive robots (designed in-house) as the convoy. The positions of the convoy robots and the quadrotors are available from the motion capture system comprising of eight Vicon MX T40 cameras. The control implementation is decentralized in the sense that the control code for each convoy robot and each quadrotor is run as a separate ROS Node. The experimental setup is illustrated in Fig. 3. The plots of Experiment 2 given in Fig. 5. Ideally for perfect tracking of elliptical orbit \\u03b3A = 1 (given by (10)), and for exact formation separation, the separation error Ds = 0 (given by (22)) for each agent. We thus consider these as the performance metrics for evaluating our strategy. In Experiment 2 the convoy moves on a path having sharp turns (shown in Fig. 4) with varying speeds, halts, reversals, etc. to ensure collision free motion. The aerial agents are tasked to track an orbit that is repeatedly changing its shape and orientation. From Fig. 5, we see that after the formation is achieved, despite some initial transients, the aerial agents faithfully track the desired orbit with \\u03b3A close to 1, as they monitor the moving convoy. We also see from Fig. 5 that the Ds remains close to 0, indicating that the aerial agents achieve and maintain the desired formation. A similar explanation holds for the plots of the remaining simulations and experiments which are shown in the supplementary video and can be found at the web link: https:\\/\\/youtu.be\\/NjJ1F3E7UKs'\",\"1242\":null,\"1243\":\"'https:\\/\\/spatial-intention-maps.cs.princeton.edu\\/'\\n\\n'https:\\/\\/spatial-intention-maps.cs.princeton.edu\\/'\\n\\n'Overview. Our system uses decentralized, asynchronous execution with communicating agents. In the figure, robot 1 is currently choosing a new action. It receives the intentions (represented as path waypoints) of the other agents, which are then encoded into a spatial map and used as input to the fully convolutional DQN policy network.'\\n\\n'Agents in our setup use decentralized, asynchronous execution, which means that whenever an agent is choosing a new action, all of the other agents in the environment will be in motion, executing their most recently selected action. We encode these in-progress actions spatially as rasterized paths in a spatial intention map. Intended paths are encoded using a linear ramp function, with a value of 1 at the executing agent\\u2019s current location, and dropping off linearly along the path (see Fig. 2 for an example). A lower value at a point on a path indicates a longer period of time before the executing agent will reach that point. This information enables more fine-grained reasoning about time and distance (e.g., the agent over there intends to come here, but is still quite a distance away).'\\n\\n'Encoding of spatial intentions. Our method encodes intended paths (with linearly ramped values) in a 2D map. Here we investigate four alternative encodings: (i) a binary encoding of the intention map where the linear ramp of values is replaced with a binaryon-path off-path map, (ii) a line encoding that replaces the path encoding with a simple straight line between each agent to its intended target location (which may not necessarily reflect the agent\\u2019s true trajectory), (iii) a circle encoding, where a circle marks each agent\\u2019s target location (this variant does not associate each agent with its target location), and (iv) an encoding that decomposes the circle intention map into multiple channels (\\\"spatial intention channels\\\" in Tab. III), one per robot, sorted in order from closest robot to furthest (thus allowing a way to associate agents with intentions). Results in Tab. III show that the binary and line variants generally perform on par with our method, while both circle variants are worse overall (but still better than without intention maps). These results suggest that providing any spatial encoding of intention is highly beneficial to multi-agent coordination, as long as there is a clear visual association between agents and their intentions.'\\n\\n'Predicting spatial intention maps. We further explore whether agents can learn to predict useful intention maps, either with heuristics or deep networks. We explore three possible methods: (i) predicted intention, which involves training an additional fully convolutional network to predict intention maps from the state representation so that they can be used during execution with no communication, (ii) history maps, where instead of encoding intention, the recent trajectory history of other agents are encoded into a map (assumes robots can track each other\\u2019s poses without communicating), and (iii) a combination of (i) and (ii), where an additional network is trained to predict the intention map from a state representation augmented with the history map. Results in Tab. III show that in isolation, (i) and (ii) do not improve performance as much as spatial intention maps do. However, we find that the last variant (iii) which combines history maps with predicted intention achieves performance (Tab. III) almost on par with explicit communication using spatial intention maps. This result is significant in that it provides a method for robot teams to leverage the benefits of spatial intention maps to coordinate without explicit communication, albeit at the cost of extra computation.'\",\"1244\":null,\"1245\":null,\"1246\":\"'In this paper, we introduce a novel spatiotemporal graph filtering approach that integrates graph learning and model-based estimation in a principled fashion to perform multi-view sensor fusion for collaborative object localization. For each view, we represent each observation as a graph, with the nodes to encode the locations of the detected objects and the edges to encode the spatial relationship of the objects. We also explicitly model the uncertainty in the observations of object locations. To explicitly model the time dimension, we represent a history of observations obtained from a view as spatiotemporal graphs. When multiple views are available, using their spatiotemporal graph representations, we formulate collaborative localization as a multi-view sensor fusion problem. Our method integrates spatiotemporal graph learning (that models the spatiotemporal relationship of the objects) and model-based state estimation (that estimates locations in a Bayesion fashion and explicitly models uncertainty in both observations and estimations) to address the formulated multi-view sensor fusion problem.'\\n\\n'We introduce a new representation for state estimation based on spatiotemporal graph neural networks, which is able to not only encode complex spatiotemporal relationships of the objects but also be readily integrated with model-based state estimation.'\\n\\n'First, we design a LSTM-based module to encode the temporal motion of each object as follows:'\\n\\n'We construct each observation acquired by each view as a graph with node attributes generated by 3D locations. The edges are generated by fully connection given the 3D locations. The LSTM-based encoder \\u03d5 and decoder \\u03c8 only contains one layer with We having the dimension of 3 \\u00d732 and Wd having the dimension of 64 \\u00d7 3. The dimension of the hidden state m is set to 32. The graph attention network consists of two layers, with Wa set to 32 \\u00d7 16 in the first layer and 16\\u00d7 32 in the second layer. Initially, the state x is set to a all zero matrix, the state estimation uncertainty P is set to a diagonal matrix with the diagonal values set to 10000, the measurement uncertainty R is set to 1000, and the process uncertainty Q is set to 2000. We use ADMM [45] as the optimization method in all experiments.'\",\"1247\":\"'https:\\/\\/github.com\\/MARSLab-UMN\\/DeepMultiviewDepth'\\n\\n'1) Encoder Module'\\n\\n'The output of the depth decoder (a modified Panoptic Feature Pyramid Network [31])\\\\nd\\\\n^\\\\nfrom the initial step of DRN may contain erroneous estimates. The IRM seeks to update the deep feature h such that the difference between the estimated depth\\\\nd\\\\n^\\\\nand the initial depth estimate\\\\nd\\\\n\\u00af\\\\nbecomes smaller for pixels with high con dent values\\\\nc\\\\n\\u00af\\\\n. This can be formulated as a weighted least-squares problem, in which the weights\\\\nw\\\\n^\\\\ni\\\\nare computed from the deep feature h via a weight decoder\\\\nW\\\\nwith the learned parameters \\u03b3:'\\n\\n'https:\\/\\/github.com\\/MARSLab-UMN\\/DeepMultiviewDepth'\",\"1248\":\"'We propose a novel learning-based method for multi-view stereo (MVS) depth estimation capable of recovering depth from images taken from known, but unconstrained, views. Existing MVS methods extract features from each image independently before projecting them onto a set of planes at candidate depths to compute matching costs. By projecting features after extraction, networks must learn rotation and scale invariant representations even though the relative poses of the cameras are known. In our approach, we compensate for viewpoint changes directly in the extraction layers, allowing the network to learn features that are projected by construction and reducing the need for rotation and scale invariance.Compensating for viewpoint changes naively, however, can be computationally expensive as the feature layers must either be applied multiple times (once per depth hypothesis), or replaced by 3D convolutions. We overcome this limitation in two ways. First, we only compute our matching cost volume at a coarse image scale before upsampling and refining the outputs. Second, we incrementally compute our projected features such that the bulk of the layers need only be executed a single time across all depth hypotheses. The combination of these two techniques allows our method to perform competitively with the state-of-the-art, while being significantly faster. We call our method MultiViewStereoNet and release our source code publicly for the benefit of the robotics community.'\\n\\n'The combination of these two techniques allows our method to achieve reconstruction accuracy comparable to the state-of-the-art, while being significantly more efficient. We call our method MultiViewStereoNet and release our source code publicly for the benefit of the robotics community.'\",\"1249\":null,\"1250\":null,\"1251\":null,\"1252\":\"'Primitive-based receding horizon planning that encodes good seamanship principles for multi-vessel scenarios.'\",\"1253\":\"'https:\\/\\/github.com\\/utexas-bwi\\/scavenger_hunt_api\\/blob\\/master\\/PseudoCode.pdf'\\n\\n'In this section we present seven scavenger hunt-solving algorithms: one RL algorithm (DQN), one exhaustive search algorithm (Bayesian), three heuristic algorithms (Proximity, Probability, and Probability-Proximity), and two bounding algorithms (Offline Optimal, and Salesman). Complete pseudocode for all the algorithms is available at: https:\\/\\/github.com\\/utexas-bwi\\/scavenger_hunt_api\\/blob\\/master\\/PseudoCode.pdf'\\n\\n'https:\\/\\/github.com\\/utexas-bwi\\/scavenger_hunt_api\\/blob\\/master\\/PseudoCode.pdf'\",\"1254\":\"'A representative result produced by our method in a complex, 3D environment including indoor and outdoor scenes. The vehicle autonomously explores over 1.4km in 20 minutes. The trajectory spans 16.0m in elevation, which is color-coded by height in the figure. More experiment details are in Section V.'\\n\\n'In experiments, we evaluate the method on a real robot in various indoor and outdoor environments. We compare to state-of-the-art methods in a cluttered indoor environment, a large indoor environment where the robot travels close to a kilometer. Comparison shows that our method covers spaces twice as fast as the start-of-the-art and at the same time using runtime less than one-fifth of the start-of-the-art. Our simulation environment and source-code are publicly available1 and experiment results are in a video2.'\\n\\n'An example exploration process with real data. The figure uses the same color code as Fig. 2. The white points show lidar scan data, with which the method extracts the uncovered surfaces (red points). The yellow dots are the viewpoint candidates, from which, the method samples the viewpoints (orange dots) to cover the red points.'\",\"1255\":null,\"1256\":\"'Latent Space Bayesian Optimization. Latent space BO tries to reduce the dimension of the optimization problem using representation learning methods, such as variational auto-encoder (VAE) [40], [41], and typically domain knowledge is combined with VAE to achieve a better performance [42]. In contrast, we introduce auxiliary supervision of grasping success labels to expedite task-oriented representation learning, and use empirical simulated grasping trials with the latent-space Bayesian optimizer for data-driven hand design.'\\n\\n'Schematic diagram of our LABO model. We learn a representation learning module, consisting of the encoder, decoder, and predictor to cast raw morphology and control parameters \\u03b8 into a latent representation z. Then we perform Bayesian optimization in the latent space. We rely on simulated grasp trials to evaluate the generated hand morphology and control on success metrics p.'\\n\\n'Directly optimizing in the raw parameter space suffers from the curse of dimensionality. To reduce the problem complexity and encourage knowledge transfer among different hand designs, we develop an encoder-decoder architecture to learn a latent space of design parameters, and use auxiliary losses to facilitate the representations to capture task-oriented information.'\\n\\n'Denote the encoder network as \\u03d5E and decoder network as \\u03d5D. The encoder network casts raw parameters \\u03b8 into a probabilistic lower-dimensional latent space characterized by the mean \\u00b5 and the standard deviation \\u03c3, where we use the reparameterization trick [45] to draw a sample in the latent space\\\\nz=\\u03bc+\\u03c3\\u2299\\u03b5,where\\u03b5\\u223cN(0,I)\\\\n. To ensure that the latent space representation is constricted within a bounded region for the ease of the subsequent optimization, we further take a sigmoid function on\\\\nz:f=sigmoid(z)=\\\\n1\\\\n1+\\\\ne\\\\n\\u2212z\\\\nso that\\\\nf\\u2208\\\\nR\\\\nd\\\\n\\u2032\\\\n\\u22650\\\\n,where\\\\nd\\\\n\\u2032\\\\n\\u226ad\\\\n. The decoder reconstructs the original input from the latent representation:'\\n\\n'Pretrain the Representation Learning Module. Before the optimization process starts, we pretrain the latent space by drawing random design parameters and train the encoder and decoder with the VAE objective [45]. The pretraining loss function can be expressed as'\\n\\n'Evolution in nature illustrates that the creatures\\u2019 biological structure and their sensorimotor skills adapt to the environmental changes for survival. Likewise, the ability to morph and acquire new skills can facilitate an embodied agent to solve tasks of varying complexities. In this work, we introduce a data-driven approach where effective hand designs naturally emerge for the purpose of grasping diverse objects. Jointly optimizing morphology and control imposes computational challenges since it requires constant evaluation of a black-box function that measures the performance of a combination of embodiment and behavior. We develop a novel Bayesian Optimization algorithm that efficiently co-designs the morphology and grasping skills through learned latent-space representations. We design the grasping tasks based on a taxonomy of human grasp types: power grasp, pinch grasp, and lateral grasp. Through experimentation and comparative study, we demonstrate that our approach discovers robust and cost-efficient hand morphologies for grasping novel objects. Additional videos and results at https:\\/\\/xinleipan.github.io\\/emergent_morphology'\",\"1257\":\"'https:\\/\\/sites.google.com\\/view\\/robotic-manip-task-axes-ctrlrs'\",\"1258\":null,\"1259\":\"'Humans leverage the dynamics of the environment and their own bodies to accomplish challenging tasks such as grasping an object while walking past it or pushing off a wall to turn a corner. Such tasks often involve switching dynamics as the robot makes and breaks contact. Learning these dynamics is a challenging problem and prone to model inaccuracies, especially near contact regions. In this work, we present a framework for learning composite dynamical behaviors from expert demonstrations. We learn a switching linear dynamical model with contacts encoded in switching conditions as a close approximation of our system dynamics. We then use discrete-time LQR as the differentiable policy class for data-efficient learning of control to develop a control strategy that operates over multiple dynamical modes and takes into account discontinuities due to contact. In addition to predicting interactions with the environment, our policy effectively reacts to inaccurate predictions such as unanticipated contacts. Through simulation and real world experiments, we demonstrate generalization of learned behaviors to different scenarios and robustness to model inaccuracies during execution.'\",\"1260\":\"'https:\\/\\/youtu.be\\/FidiVw0_yfs'\",\"1261\":null,\"1262\":null,\"1263\":null,\"1264\":null,\"1265\":null,\"1266\":\"'https:\\/\\/penst.at\\/Nl4Wl'\",\"1267\":\"'https:\\/\\/github.com\\/zwbgood6\\/deform'\\n\\n'We propose a framework for deformable linear object prediction. Prediction of deformable objects (e.g., rope) is challenging due to their non-linear dynamics and infinite-dimensional configuration spaces. By mapping the dynamics from a non-linear space to a linear space, we can use the good properties of linear dynamics for easier learning and more efficient prediction. We learn a locally linear, action-conditioned dynamics model that can be used to predict future latent states. Then, we decode the predicted latent state into the predicted state. We also apply a sampling-based optimization algorithm to select the optimal control action. We empirically demonstrate that our approach can predict the rope state accurately up to ten steps into the future and that our algorithm can find the optimal action given an initial state and a goal state.'\\n\\n'To solve the prediction problem, we propose a prediction framework that consists of a state autoencoder model, an action autoencoder model, and a dynamics model. The state autoencoder model is based on convolutional neural networks. Rope dynamics are non-linear and are very complicated to learn directly. Unlike non-linear dynamics, linear dynamics are easier to learn and more efficient for state predictions. Thus, we consider mapping the non-linear space into a linear space, predicting future states in the linear space, and decoding back to the non-linear space from the linear space. To do so, we first use learned encoders to encode both the state and the action into the latent state and the latent action. Then, we run the dynamics model to get the state matrix and control matrix for the linear dynamics in the latent space. After that, we get a predicted next latent state by using locally linear latent dynamics based on the current latent state, latent action, state matrix, and control matrix. Finally, we decode the predicted next latent state into a predicted next state. Our experiments show that the prediction framework can accurately predict states up to ten timesteps in the future.'\\n\\n'Instead of using a physics-based simulation to model the objects, we design an encoding network to extract the latent states from rope images. Then we learn the dynamical system in the latent space. Using the dynamics in the latent space, we can predict the future latent states. After that, the model decodes the future latent states into rope images. We elaborate on each step as following.'\\n\\n'Encoder Models. The state encoder model is a map \\u03d5e :\\\\nX\\u2192G\\\\n, the action encoder model is a map\\\\n\\u03c6\\\\ne\\\\n:U\\u2192A\\\\n. We then have'\\n\\n'Encoder Models.'\\n\\n'Encoder models encode the state x into the latent state g and the action u into the latent action a. For the state x (i.e., raw image), it is high-dimensional and redundant, so we want to reduce the state size and use the smaller latent state as a representation. For the action u (i.e., grasping positions, gripper moving length, and gripper moving angle), the raw action usually has a very non-linear effect on the state, so we lift the actions to a higher dimension and make them easier to linearize in the latent space.'\\n\\n'Decoder Model. The state decoder model is a map\\\\n\\u03d5\\\\nd\\\\n:G\\u2192X\\\\n, the action decoder model is a map\\\\n\\u03c6\\\\nd\\\\n:A\\u2192U\\\\n. We then have'\\n\\n'Decoder Model.'\\n\\n'Paired with encoder models, decoder models decode the latent state g into the reconstructed state\\\\nx\\\\n^\\\\nand the latent action a into the reconstructed action \\u00fb.'\\n\\n'Loss Function. We consider minimizing the losses for state autoencoder model, action autoencoder model, and dynamics model. The loss of state autoencoder model is'\\n\\n'Diagram for prediction framework. We take a part of the diagram from time step 0 to 1 as an example. The red arrows show the state autoencoder model. We use a state encoder \\u03d5e to encode a state x(0) into a latent state g(0) and then use a state decoder \\u03c6d to decode the latent state g(0) into a reconstructed state\\\\nx\\\\n^\\\\n(0)\\\\n. The orange arrows show the action autoencoder model. We use an action encoder \\u03c6e to encode an action u(0) into a latent action a(0) and then use an action decoder \\u03c6d to decode the latent action u(0) into a reconstructed action \\u00fb(0). The green arrows show the rope dynamics. Given the state x(0) and the action u(0), we can have next state x(1). The blue arrows show the latent dynamics. Given the latent state g(0), the latent action a(0), state matrix K(0), and control matrix L(0), we can get the next latent state g(1) based on locally linear latent dynamics.'\\n\\n'Training Autoencoders.'\\n\\n'There are two methods for model training: (i) training autoencoder models first and then training dynamics model, and (ii) training autoencoder models and dynamics model together. We discuss the model training order and our preference in Section IV ablation study.'\\n\\n'To wrap up, after training the autoencoders and the dynamics model using the training set, we start to make predictions for new states and actions in the test set. The detailed architectures and hyperparameters are available in the code https:\\/\\/github.com\\/zwbgood6\\/deform.'\\n\\n'Order for Training Models. We have autoencoder models for state and action and the dynamics model. The order to train models is important to get good performance. We use two methods: (i) training autoencoder models first and then training dynamics model, (ii) training autoencoder models and dynamics model together. We demonstrate that method (i) can provide better prediction and control results, as shown in Figure 3, while method (ii) reconstructs blurry rope images, and we cannot find a clear rope shape. The reason is that we can get a good latent state estimation when training autoencoder models. After having a good latent state estimation, we then train the dynamics model to get good system dynamics. For method (ii), training all models together to optimize all losses is more difficult to get a good prediction result. It is because we need to find the optimal neural network weights to get accurate latent state, latent action, state matrix, and control matrix at the same time.'\\n\\n'Whether Using Action Autoencoder. In the prediction framework, we use the autoencoder to lift the action space to a higher dimensional space so it can work linearly with the latent state in the latent space. We also try not to use the autoencoder in the prediction framework, and we cannot achieve good prediction results due to the non-linearity of the impact of the four-element action.'\\n\\n'Whether Using Action Autoencoder.'\\n\\n'This paper presents a prediction framework for rope prediction. Our method first maps a non-linear space into a locally linear space using the encoders. Then, we get the state matrix and control matrix for linear dynamics by running the dynamics model. In the next step, we make a prediction based on the locally linear latent dynamics. Finally, we decode the predicted latent state into the predicted state in the non-linear space. We also propose a sampling-based optimization algorithm to select the optimal control action to move the rope from an initial state to a goal state. The experimental results demonstrate that our method can accurately predict the rope images to ten steps in the future. The experiments also show that our sampling-based optimization algorithm can find the optimal actions to relocate the rope. By comparing sampled actions using our method and actions using the baseline, we demonstrate that our method achieves better performance.'\\n\\n'https:\\/\\/github.com\\/zwbgood6\\/deform'\",\"1268\":null,\"1269\":\"'We present a dense-indirect SLAM system using external dense optical flows as input. We extend the recent probabilistic visual odometry model VOLDOR [1], by incorporating the use of geometric priors to 1) robustly bootstrap estimation from monocular capture, while 2) seamlessly supporting stereo and\\/or RGB-D input imagery. Our customized back-end tightly couples our intermediate geometric estimates with an adaptive priority scheme managing the connectivity of an incremental pose graph. We leverage recent advances in dense optical flow methods to achieve accurate and robust camera pose estimates, while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [https:\\/\\/github.com\\/htkseason\\/VOLDOR] operates online at around 15 FPS on a single GTX1080Ti GPU.'\",\"1270\":null,\"1271\":\"'We present CLIPPER (Consistent LInking, Pruning, and Pairwise Error Rectification), a framework for robust data association in the presence of noise and outliers. We formulate the problem in a graph-theoretic framework using the notion of geometric consistency. State-of-the-art techniques that use this framework utilize either combinatorial optimization techniques that do not scale well to large-sized problems, or use heuristic approximations that yield low accuracy in high-noise, high-outlier regimes. In contrast, CLIPPER uses a relaxation of the combinatorial problem and returns solutions that are guaranteed to correspond to the optima of the original problem. Low time complexity is achieved with an efficient projected gradient ascent approach. Experiments indicate that CLIPPER maintains a consistently low runtime of 15 ms where exact methods can require up to 24 s at their peak, even on small-sized problems with 200 associations. When evaluated on noisy point cloud registration problems, CLIPPER achieves 100% precision and 98% recall in 90% outlier regimes while competing algorithms begin degrading by 70% outliers. In an instance of associating noisy points of the Stanford Bunny with 990 outlier associations and only 10 inlier associations, CLIPPER successfully returns 8 inlier associations with 100% precision in 138 ms. Code is available at https:\\/\\/mit-acl.github.io\\/clipper.'\",\"1272\":\"'The transmission is composed of a brushless DC motor (Maxon, EC 60 Flat, 100 Watt) and two-stage gearing system involving a planetary gearbox (Gysin, GPL042) and Capstan drive. The motor can generate 0.3 Nm of continuous torque, features an optical incremental encoder, and operates in current mode with sinusoidal commutation by the driver (Maxon, EPOS2). The transmission ratios of the gearbox and Capstan drive are 12.25:1 and 9.85:1, respectively, producing a combined reduction ratio of 120:1.'\",\"1273\":null,\"1274\":\"'Experiments for this system were conducted on a live system in conjunction with the AUV Lab at MIT Sea Grant College. 60 minutes of data was collected along the Charles River from the MIT sailing pavilion towards the the Boston Harbor and back in October 2020 during fair-weather, daylight conditions. A total of 67 obstacles were observed, consisting of a variety of small-to medium-sized boats and static, free-standing obstacles such as pillars. The sensor platform was a Boston Whaler R\\/V Philos, with a sensor payload consisting of a Simrad Broadband 4G Radar, three forward-facing visible light cameras (FLIR Blackfly, BFLY-PGE13E4C-CS, 1280x1024, 12fps) covering 140 degree horizontal FoV, a Velodyne VLP-16 LIDAR, and a SGB Systems Ellipse-D Dual Antenna RTK INS for GPS\\/IMU. The perception code was executed on a PC with an Intel i7-8700 3.2 GHz processor, 32GB of RAM, and a Nvidia GTX 1060 video card with 6GB VRAM, running Ubuntu 16.04. The machine learning model (trained on MS-COCO) for YOLOv4 [6] was downloaded from the author\\u2019s website. WaSR [7] was not utilized due to insufficient hardware resources, but was shown to be effective in segmenting obstacle pixels in the camera image when executed offline against logged data.'\\n\\n'This paper describes a set of software modules and algorithms for maritime object detection and tracking. The approach described here is designed to work in conjunction with various sensors from a maritime surface vessel (e.g. marine RADAR, LIDAR, camera). The described system identifies obstacles from the input sensors, estimates their state, and fuses the obstacle data into a consolidated report. The system is verified using experiments conducted on a live system and successfully demonstrates the ability to detect and track obstacles up to 450m away while operating at 7 fps. The software is open source and available at https:\\/\\/github.com\\/uml-marine-robotics\\/asv_perception.'\",\"1275\":\"'https:\\/\\/github.com\\/brian-h-wang\\/pseudolidar-tree-detection'\\n\\n'We validate our perception system on a test sequence of stereo point clouds captured in a forest-like environment, demonstrating accurate tree detection and mapping at ranges up to 7 meters away from the camera, where stereo depth noise becomes significant. To further promote creation of data sets and obstacle detectors for forests and other challenging environments for robotic navigation, our code for point cloud labeling, pseudo-lidar object detection, and tree mapping, as well as our collected data set, are publicly available at https:\\/\\/github.com\\/brian-h-wang\\/pseudolidar-tree-detection.'\\n\\n'We present a method for detecting and mapping trees in noisy stereo camera point clouds, using a learned 3D object detector. Inspired by recent advancements in 3-D object detection using a pseudo-lidar representation for stereo data, we train a PointRCNN detector to recognize trees in forest-like environments. We generate detector training data with a novel automatic labeling process that clusters a fused global point cloud. This process annotates large stereo point cloud training data sets with minimal user supervision, and unlike previous pseudo-lidar detection pipelines, requires no 3D ground truth from other sensors such as lidar. Our mapping system additionally uses a Kalman filter to associate detections and consistently estimate the positions and sizes of trees. We collect a data set for tree detection consisting of 8680 stereo point clouds, and validate our method on an outdoors test sequence. Our results demonstrate robust tree recognition in noisy stereo data at ranges of up to 7 meters, on 720p resolution images from a Stereolabs ZED 2 camera. Code and data are available at https:\\/\\/github.com\\/brian-h-wang\\/pseudolidar-tree-detection.'\\n\\n'https:\\/\\/github.com\\/brian-h-wang\\/pseudolidar-tree-detection'\",\"1276\":\"'As a lightweight solution, we implement a tiny encoder-decoder network architecture for each iteration of the ADMM updates. Different from the traditional proximal operators which only consider the RGB image or single data modulation as prior knowledge, our implementation introduces an early-fusion architecture. This is achieved by concatenating the noisy or sparse depth input and corresponding RGB image. The encoder of the network consists of three \\\"bottleneck\\\" blocks as the Resnet model [15] and three up-Projection blocks detailed in [6]. According to previous experimental experience, skip connections usually pass the gradients better and improve training efficiency of the neural network, so we add two skip connections between the peer-to-peer bottleneck block and up-Projection block. In order to further improve the quality of the neural network-based proximal operator G, we applied adversarial training techniques. Compared to traditional supervised training, adversarial training results in a more natural and accurate depth map, given a noisy depth input. In this case, a classifier or discriminator D is pretrained to distinguish a ground truth depth map from the dataset or a fake depth map generated by the proximal operator, applying the cross-entropy loss. The proximal operator G is also pretrained using the L-2 norm as the loss function. During the adversarial process, the generator, which is also the proximal operator, tries to trick the discriminator D by generating a depth map similar to the ground truth, while the discriminator tries its best to classify the true depth map and generated depth map. We jointly train both the discriminator D and the proximal operator G to ensure they compete with each other. In both outdoor and indoor scenarios, the ground truth depth map contains vacant pixels due to sensor failure or sparsity of the ground truth. To avoid the discriminator\\u2019s learning greedy features from these data, we masked out the invalid regions in the noisy depth map according to the ground truth depth map before feeding into the discriminator.'\\n\\n'In this paper, we formulate the sensor fusion-based depth completion problem as a Linear Inverse Problem (LIP). Instead of applying a traditional hand-crafted signal prior, we propose a multiple-modulation proximal operator with an encoder-decoder architecture. To further improve the smoothness of the proximal operator, we apply an adversarial training technique. The proposed method both guarantees data consistency and achieves smooth and realistic depth prediction, which outperforms previous methods. As a fast-converging iterative approach, our method allows a lightweight solution which has fewer parameters compared to other state-of-the-art methods. Future work will further reduce the number of parameters by training a single proximal operator to fit all iterations of the LIP. We will also extend the depth completion results to benefit other perception tasks, including object detection, shape completion and 3D tracking.'\",\"1277\":null,\"1278\":\"'https:\\/\\/github.com\\/antoniopradom\\/FwdKinNeckBrace.'\\n\\n'Due to the high back-drivability of this robot, it can be used to measure the volitional head-neck movement of an individual. In this case, the servomotors do not output torque, but record angular displacements of the base joints through encoders. To compute the head orientation through these encoder data, forward kinematics is also needed. This approach was also used to provide visual feedback to the user [30], [31].'\\n\\n'The second approach to solve the forward kinematics is to integrate the angular rates of the end-effector. In this approach, the initial configuration of the robot, the active joint velocities, and the Jacobian of the robot are needed to map the joint velocities to the angular rates of the endeffector. The orientation of the end-effector at the next time step is then numerically integrated and the configuration of the robot is updated accordingly using the inverse kinematics. This approach relies on the condition of the Jacobian matrix. When it is ill-conditioned, the solver may fail to produce reliable solutions. Additionally, the joint velocities are obtained from the encoder data and are subject to noise from the sensors and the numerical operation.'\\n\\n'Other approaches like adding redundant sensors, such as an inertial measurement unit (IMU) on the end-effector or encoders on the passive joints, may mitigate the computational challenge. However, these approaches compromise the portability of the robot and increases its cost.'\\n\\n'In the presented application, noise can arise from encoder measurements and the deviation of brace parameters due to part deflection and joint misalignment. Adding realistic noise to the simulated training dataset (domain randomization) may improve learning and produce a more robust prediction [35]. However, determining an optimal distribution of noise may not be easy. Instead, this paper demonstrates that by carefully creating a training dataset using mathematical modeling to train a NN, we can achieve comparable performance to fully data-driven approaches. This approach was tested with a small dataset of human subjects and still needs to be evaluated with a larger dataset. For example, the NN models\\u2019 performances could be improved by adding a large number of subjects to the training dataset, while traditional mathematical models\\u2019 performances are fixed after validation. Adding subject data to the training dataset is a costly and time consuming process, so the CD model can be used until sufficient in-vivo data is collected. Although, determining the number of subjects, if any, needs to be studied.'\\n\\n'https:\\/\\/github.com\\/antoniopradom\\/FwdKinNeckBrace.'\",\"1279\":null,\"1280\":\"'In order to encode the current cycle time \\u03d5 and the cycle offset parameters \\u03b8left,\\u03b8right, we condition the policies on two clock inputs:'\\n\\n'To encode information about the start and end times of the phases into the state, we derive a vector of ratios from the sequence of phase timings; each ratio represents the proportion out of the total period that a phase occupies.'\",\"1281\":\"'https:\\/\\/www.youtube.com\\/watch?v=LZcBN9zgtXg&t=11s'\",\"1282\":\"'https:\\/\\/github.com\\/husha1993\\/FWBO'\\n\\n'https:\\/\\/github.com\\/husha1993\\/FWBO'\",\"1283\":null,\"1284\":null,\"1285\":null,\"1286\":null,\"1287\":null,\"1288\":\"'We use Blender 2.8, an open-source simulation and rendering engine [3] released in mid-2019, to both create large synthetic RGB training datasets and model the fabric dynamics for simulated experiments using its in-built fabric solver based on [29], [30]. We simulate T-shirts and square fabrics, each of which we model as a polygonal mesh made up of 729 vertices, a square number we experimentally tuned to trade-off between fine-grained deformations and reasonable simulation speed. See Figure 2 for an illustration. Each vertex on the mesh has a global coordinate which we can query directly through Blender\\u2019s API, allowing for easily available ground truth information about various locations on the mesh and their pixel counterparts. We can also simulate finer-grained manipulation of the mesh including grasps, pulls, and folds. See the supplement for further details on how we perform manipulation and experiments in simulation.'\",\"1289\":null,\"1290\":null,\"1291\":null,\"1292\":null,\"1293\":null,\"1294\":null,\"1295\":\"'In our framework, the ego agent (Player 1) operates under the assumption that all other agents are momentarily distracted. To encode this \\\"imagined\\\" scenario, the ego agent imagines the overall time horizon [0, T] as divided into two sub-intervals, the adversarial interval [0, Tadv] and cooperative interval [Tadv, T], with 0 < Tadv < T. During the adversarial interval, the ego agent imagines the other agents to be \\\"momentarily distracted,\\\" and desires to act defensively. This phenomenon is modeled using an adversarial running cost\\\\ng\\\\nadv,i\\\\n:\\\\nR\\\\nn\\\\n\\u00d7\\\\nR\\\\nNm\\\\n\\u2192R\\\\nfor each\\\\ni\\u2208{2,\\u22ef,N}\\\\n. On the other hand, during the cooperative interval, the ego agent supposes that the other agents have reverted to their \\\"normal\\\" or \\\"cooperative\\\" manner, and thus proceeds to select control signals for the remainder of the time horizon in a less conservative manner. This behavior is captured using a cooperative running cost\\\\ng\\\\ncoop,i\\\\n:\\\\nR\\\\nn\\\\n\\u00d7\\\\nR\\\\nNm\\\\n\\u2192R\\\\nfor each\\\\ni\\u2208{2,\\u22ef,N}.\\\\nIn other words, the running cost of each non-ego agent gi can be piecewisely defined as follows:'\\n\\n'Here, the \\\"proximity\\\" constraint is enforced for only the ego agent, to force the ego to bear responsibility for satisfying joint state constraints which encode his or her own safety (e.g. non-collision). In addition, all agents must satisfy individual constraints that encode reasonable conduct in traffic (e.g., staying within a range of speeds). All constraints are enforced over the entire time horizon [0, T]. For all tests, we use a time horizon T = 15 s and discretize time (following [21] and [25]) at 0.1 s intervals.'\\n\\n'In this example, the ego car is traveling North on a straight road when it encounters another car traveling South. Since the road has a lane in each direction, \\\"ideally\\\" the ego vehicle would not deviate too far from its lane or speed. However, to drive more defensively, the ego vehicle should plan as though the oncoming Southbound car were to act noncooperatively. Our method encodes precisely this type of defensive planning. Fig. 2 shows the planned trajectories that emerge for increasing Tadv. As shown, the ego vehicle (bottom) imagines more aggressive maneuvers for itself and the oncoming car (top) as Tadv increases. Note, however, that these are merely imagined trajectories and that (a) the ego vehicle can always choose to follow this trajectory only for an initial period of time, and recompute its trajectory thereafter, and (b) the oncoming vehicle will make its own decisions and will not generally follow this \\\"partially adversarial\\\" trajectory. We solve each of these problems (with fixed Tadv) in under 0.5 s.'\\n\\n'The traffic simulations in this work are solved approximately to local feedback Nash equilibria in real time using ILQGames, a recently developed, open-source C++ based game solver algorithm introduced in [21]. ILQGames iteratively solves linear-quadratic games, obtained by linearizing dynamics and quadraticizing costs, and incurs computational complexity that is cubic in the number of players [21]. As discussed above, we must also account for both equality and inequality constraints on the game trajectory. While we note that [21] does not address constrained Nash games, here we incorporate constraints using augmented Lagrangian methods [26]. For a more detailed discussion of constraint-handling in feedback Nash games, please refer to [27]. Note that although other game solvers, such as ALGAMES [22] and Iterative Best Response algorithms [20], can likewise handle constraints, their applications are restricted to open-loop games. A thorough treatment of constraints in games can be found in [28].'\",\"1296\":null,\"1297\":null,\"1298\":null,\"1299\":\"'Here, we present a novel method to encode spatiotemporal longitudinal ultrasound features of the rectus femoris (RF) and vastus intermedius (VI) muscles within a task-invariant learning paradigm for continuous estimation of steady-state and transient ambulation kinematics. We hypothesize that spatiotemporal ultrasound features of the proximal muscles can be used for task-invariant learning of the knee joint kinematics with an accuracy that is comparable to a task-specific learning paradigm.'\\n\\n'This study demonstrates the feasibility of an ultrasound-based approach for the continuous task-invariant learning of steady-state and transient ambulation using able-bodied subjects. Future work needs to focus on translating the same paradigm to the target populations for lower-limb assistive devices, i.e. populations with mobility-related pathology or limb-loss. While our approach demonstrated useful for continuous transient ambulation, there are still some common volitional activities such as sit-to-stand, non-weight-bearing, and unstructured movements that need to be incorporated within a task-invariant learning paradigm for a truly \\\"free form\\\" control scheme. The positive effect of incorporating the temporal features on the accuracy of task-invariant learning suggests that the sequence-to-sequence prediction models might prove particularly useful to encode the spatiotemporal features of ultrasound data for the task-invariant learning of continuous ambulation. Furthermore, subject-specific models were trained for this study. While it has been shown difficult to rely solely on neuromuscular signals [15], [28] for a subject-independent approach, it may be feasible to combine the temporal ultrasound features with anatomically normalized intensity features to obtain the desired performance based on a subject-independent approach. Due to the high between-subject variability of the neuromuscular signals, partially subject-independent schemes with a larger sample size might be more feasible for task-invariant learning of continuous volitional ambulation.'\",\"1300\":\"'Cross-sectional view of the haptic interface assembly with key elements labeled. The Bowden cable from the harness terminates on a linear carriage which constrains its motion to a single degree of freedom. A cable transmission converts between the linear motion and force of the carriage and rotary motion and torque of the attached brushless DC (BLDC) motor. An encoder fixed to the motor shaft measures rotation of the shaft. A custom tension sensor measures the forces applied to the participant through the Bowden cable from the torque output of the motor.'\",\"1301\":null,\"1302\":null,\"1303\":\"'https:\\/\\/github.com\\/anassinator\\/ilqr'\\n\\n'https:\\/\\/github.com\\/openai\\/baselines'\\n\\n'A final consideration is that for contact models learned from data, it is prudent to limit the input, i.e. depth and orientation, to stay close to the range of collected data, because learned models have questionable extrapolation capability. We encode this constraint, along with the robot joint position and velocity limits, in\\\\nX\\\\n. In addition, we have the joint torque limits\\\\nu[\\u22c5]\\u2208U\\\\n.'\\n\\n'Next, we compare with 2 base-lines: standard trajectory optimization (iLQR) and reinforcement learning (RL). We use the open source implementation of the iLQR algorithm [17] at https:\\/\\/github.com\\/anassinator\\/ilqr. For RL, we tested open source implementation of the PPO [25] and TRPO [26] algorithms at https:\\/\\/github.com\\/openai\\/baselines. Default algorithm settings were used.'\\n\\n'https:\\/\\/github.com\\/anassinator\\/ilqr'\\n\\n'https:\\/\\/github.com\\/openai\\/baselines'\",\"1304\":null,\"1305\":null,\"1306\":\"'The Hessian matrix encodes the second derivatives of the objective function and determines the convergence properties of the optimizer. Specifically, a Hessian and its eigenvalues represent the local curvature of the objective function. This eigenstructure of the optimization problem is critical in determining the convergence characteristics and the minimization performance. During the descent towards the minimum, the step size is bounded by the largest eigenvalue of the Hessian, but the rate of convergence is determined by the smallest. Hence, a particular metric of interest is the condition number of the Hessian, which is defined as the ratio of the largest eigenvalue to the smallest. An optimization problem with a large condition number is known to be \\u2018ill-conditioned\\u2019 and causes poor convergence properties, because the function contours are stretched out and the gradient-based descent steps tend to point towards non-optimal directions. Other elements in the problem configuration can also have adverse effects, such as when the gradients are ambiguous around certain values (see Figure 1), or too small, resulting in an ineffective step size.'\\n\\n'The open sourced implementations of the Loco and Fast planners were utilized, which contain a BFGS solver for loco planner, L-BFGS and SLSQP versions for Fast planner. We test the adversarial attack scheme in two environments which refer to as sparse (3a), with a small number of obstacles; and dense with a larger obstacle field (3d). In both cases, the target robot spawns randomly at one edge of the map and is commanded to plan a path to the opposite side, while the adversary is spawned at a random location. For both planners, we test two configurations: default, where the weights in the cost function were kept at their default values; and another as conservative, where the weight on the collision cost was increased. We provide an Euclidean signed distance field [34] map representation to the planners, and we use a simple repulsion method to avoid the adversary colliding with the obstacles. The adversary and the target are both in motion, with the rest of the map assumed to be static.'\",\"1307\":\"'Semantic segmentation of point clouds is a key component of scene understanding for robotics and autonomous driving. In this paper, we introduce TORNADO-Net - a neural network for 3D LiDAR point cloud semantic segmentation. We incorporate a multi-view (bird-eye and range) projection feature extraction with an encoder-decoder ResNet architecture with a novel diamond context block. Current projection-based methods do not take into account that neighboring points usually belong to the same class. To better utilize this local neighbourhood information and reduce noisy predictions, we introduce a combination of Total Variation, Lov\\u00e1sz-Softmax, and Weighted Cross-Entropy losses. We also take advantage of the fact that the LiDAR data encompasses 360 \\u25e6 field of view and use circular padding. We demonstrate state-of-the-art results on the SemanticKITTI dataset and also provide thorough quantitative evaluations and ablation results.'\\n\\n'In this paper, a novel and intuitive Neural Network (NN) architecture is introduced to solve the problem of LiDAR semantic segmentation benefiting from information extraction in different views. Although the encoder-decoder model processes range image similar to [5], the pillar-projection-learning module (PPL) learns and extracts information in the Bird\\u2019s Eye View (BEV). This series of different projections as shown in Fig. 1, help the model to learn features that are otherwise difficult to extract. Results and ablation studies on the SemanticKITTI [1] dataset benchmark show the effectiveness of the proposed Convolutional Neural Network (CNN) model. The details of the new architecture are explained in the subsections below.'\\n\\n'Given a high resolution input, most encoder-decoder NNs reduce the input size to half immediately after only a few layers of 2D convolutions. This reduction in spatial size limits the network capability to produce features suitable for the fine grained data like LiDAR or point cloud. SalsaNext [5] showed that further processing the input can result in better overall accuracy of the network. Diamond block is an attempt to further improve upon the idea of contextual block before an encoder-decoder, using different 2D kernel convolutions.'\\n\\n'3) Encoder-Decoder'\\n\\n'After the raw point cloud is processed by PPL and DCB, the feature tensor is fed to an encoder-decoder CNN similar to what is proposed in [10]. The architecture of the proposed encoder-decoder is illustrated in Figure 1. The input to the network is the spherical projection of the extracted features from PPL+DCB in section III-A.2. The encoder-decoder part of TORNADONet is built upon the base SalsaNet model [10] which follows the standard encoder-decoder architecture with a bottleneck compression rate of 16. As opposed to the original implementation of SalsaNet with series of ResNet blocks [22], we use the blocks introduced in [5] with dilations in the convolutions both on the decoder and encoder parts of the network.'\\n\\n'Semantic segmentation has been a subject of interest in many fields, such as autonomous driving. While other deep learning techniques are promising on the LiDAR semantic segmentation task, they either require complex training schemes, or are real-time but not as accurate. We proposed a novel deep neural network, TORNADO-Net that leverages a multi-view (bird-eye and range) projection feature extraction and an encoder-decoder ResNet architecture with a novel diamond context block. Moreover, TV loss was introduced along with Lov\\u00e1sz-Softmax, and WCE loss to efficiently train the network. We evaluated the proposed method on the SemanticKITTI benchmark and were able to achieve near state-of-the-art results on its published leaderboard, outperforming most previous methods.'\",\"1308\":\"'Autonomous driving vehicles and robotic systems rely on accurate perception of their surroundings. Scene understanding is one of the crucial components of perception modules. Among all available sensors, LiDARs are one of the essential sensing modalities of autonomous driving systems due to their active sensing nature with high resolution of sensor readings. Accurate and fast semantic segmentation methods are needed to fully utilize LiDAR sensors for scene understanding. In this paper, we present Lite-HDSeg, a novel real-time convolutional neural network for semantic segmentation of full 3D LiDAR point clouds. Lite-HDSeg can achieve the best accuracy vs. computational complexity trade-off in SemanticKITTI bench-mark and is designed on the basis of a new encoder-decoder architecture with light-weight harmonic dense convolutions as its core. Moreover, we introduce ICM, an improved global contextual module to capture multi-scale contextual features, and MCSPN, a multi-class Spatial Propagation Network to further refine the semantic boundaries. Our experimental results show that the proposed method outperforms state-of- the-art semantic segmentation approaches which can run real-time, thus is suitable for robotic and autonomous driving applications.'\\n\\n'A new and improved encoder-decoder CNN model on a new encoder named Lite-HDSeg, a residual network decoder, a multi-class SPN and a modified boundary loss which is trained end-to-end using the spherical data representation (range-image). Lite-HDSeg surpasses state-of-the-art methods in accuracy vs. runtime tradeoff on a large-scale public benchmark, SemanticKITTI (shown in Fig. 1);'\\n\\n'The proposed network structure is based on a) multi-scale convolutional learning module, Inception-like Context Module (ICM), to extract multi-scale contextual features from range-image to be processed by the encoder-decoder network, b) lite version of HarDNet [28] as the encoder with less dense connections to achieve better performance (see Fig. 4b), c) CAM module in the encoder to collect nearby context from neighborhood feature maps, d) Multi-class Convolutional Spatial Propagation Network (MCSPN) before the last layer of convolutions in the encoder-decoder CNN to refine the masks of each class for the segmentation task and e) a dedicated boundary loss in addition to cross entropy and Lovasz loss loss for the segmentation task of LiDAR point cloud data. The combination of all above-mentioned contributions demonstrate that segmentation results can substantially be improved over the range map with a large margin.'\\n\\n'We present a multi-scale convolutional learning module, Inception-like Context Module (ICM), to extract multi-scale contextual features from range-image to be processed by the encoder-decoder network targeted at semantic segmentation. The proposed context feature extractor consists of concatenation of several multi-scale convolution layers with residual connections and larger receptive fields to extract rich global information which is essential in learning complex correlations between classes with different size. This yields to gather the spatial fine-grained information along with global context information. A detailed visual description with numerical values for channels, kernels and dilation ratios can be found in Fig. 3a.'\\n\\n'B. Lite-HDSeg Encoder'\\n\\n'Our proposed Lite-HDSeg uses a series of simplified HD blocks for the encoder by removing some of the skip connections, while keeping the critical connections (Enc1-Enc5). This is due to the fact that the dense skip connections introduce many redundant and repeated channels. This systematic pruning in the skip connections reduces the computational complexity and improves memory efficiency, which are the cornerstone of real-time applications. In the proposed Light HD Block (LHD), each convolution layer fi takes a direct input from at most\\\\n\\u230a\\\\nlog(i)\\\\n5\\\\n\\u230b\\\\nnumber of previous layers, and these input layers are apart from layers with base 5, i.e.:'\\n\\n'D. Decoder'\\n\\n'The proposed decoder is based on residual blocks (Res- block). The proposed residual block is a combination of conv-blocks in a specific order and increased kernel size as shown in Fig. 3b with a skip connection from the encoder with the same spatial size. The Resblock in Fig. 3b along with an up-sampling layer is used four times (Dec4, Dec3, Dec2 and Dec1) to generate an output with the spatial size of the input range-image. The final layer in the decoder is a convolutional layer to generate output with the depth size C13.'\\n\\n'Comparison of prediction (left) and error map (right) of the proposed method vs. SalsaNext on SemanticKITTI validation set (sequence 08). Color codes are: road side-walk parking car bicyclist pole vegetation terrain trunk building other-structure other-object. C. Ablation Study'\\n\\n'TABLE II: Ablative Analysis evaluated on SemanticKITTI dataset validation (seq 08). Our proposed backbone is the encoder-decoder introduced in Sections III-B and III-D.'\\n\\n'In this paper, we presented Lite-HDSeg, a novel real-time CNN model for semantic segmentation of a full 3D LiDAR point cloud. Lite-HDSeg has a new encoder-decoder architecture based on light-weight harmonic dense convolutions and residual blocks with a novel contextual module called ICM and Multi-class SPN. We trained our network with boundary loss to emphasize the semantic boundaries. To show the performance of the proposed method, Lite-HDSeg was evaluated on the public benchmark, SemanticKITTI. Experiments show that the proposed method outperforms all real-time state-of-the-art semantic segmentation approaches in terms of accuracy (mIoU), while being less complex. More specifically, Lite-HDSeg introduces a novel design with accuracy improvements from each of the blocks, i.e., the new context module (ICM), the new encoder-decoder, MCSPN and boundary loss, to benefit not only semantic segmentation task, but also instance segmentation and object detection.'\",\"1309\":null,\"1310\":null,\"1311\":\"'We present a method for computing exact reachable sets for deep neural networks with rectified linear unit (ReLU) activation. Our method is well-suited for use in rigorous safety analysis of robotic perception and control systems with deep neural network components. Our algorithm can compute both forward and backward reachable sets for a ReLU network iterated over multiple time steps, as would be found in a perception-action loop in a robotic system. Our algorithm is unique in that it builds the reachable sets by incrementally enumerating polyhedral cells in the input space, rather than iterating layer-by-layer through the network as in other methods. If an unsafe cell is found, our algorithm can return this result without completing the full reachability computation, thus giving an anytime property that accelerates safety verification. In addition, our method requires less memory during execution compared to existing methods where memory can be a limiting factor. We demonstrate our algorithm on safety verification of the ACAS Xu aircraft advisory system. We find unsafe actions many times faster than the fastest existing method and certify no unsafe actions exist in about twice the time of the existing method. We also compute forward and backward reachable sets for a learned model of pendulum dynamics over a 50 time step horizon in 87s on a laptop computer. Algorithm source code: https:\\/\\/github.com\\/StanfordMSL\\/Neural-Network-Reach.'\",\"1312\":null,\"1313\":\"'https:\\/\\/youtu.be\\/B9j8LVIs384'\",\"1314\":\"'https:\\/\\/www.nestlab.net\\/doku.php\\/papers:mrs_fl_dataset'\\n\\n'We apply the proposed approaches to a trajectory prediction problem and make our open-source federated dataset available for the research community;'\\n\\n'To the best of our knowledge, this paper is the first to provide an open-source federated dataset of swarm motion with communication graph information. We generated multiple synthetic datasets of swarm motion across four distinct behaviors using the ARGoS simulator [23].'\",\"1315\":null,\"1316\":null,\"1317\":\"'https:\\/\\/github.com\\/rutgers-arc-lab\\/3d_coverage'\\n\\n'https:\\/\\/github.com\\/rutgers-arc-lab\\/3d_coverage'\",\"1318\":null,\"1319\":\"'https:\\/\\/www.github.com\\/4estlaine\\/gfne'\\n\\n'Game-theoretic motion planners instead model the interaction with other agents directly, by handling planning and prediction jointly. That is, the intentions of all agents in the scene are encoded as optimization problems that they are each trying to solve, and an equilibrium solution for the collection of optimization problems is found. This equilibrium consists of a set of interacting trajectories of all the agents, which can be used as predictions for non-ego agents, and as a motion plan for the ego agent.'\\n\\n'https:\\/\\/www.github.com\\/4estlaine\\/gfne'\",\"1320\":\"\\\"The approximation algorithm was coded in Python 3.7 and the computations were run on a Dell Inspiron (4 Core Intel i7-8565U processor @ 1.80 GHz, 16 GB RAM). Three sets of instances were created for testing. Each instance consists of a network that was randomly generated on a 50 x 50 grid. For the large problem instances (|V| = 50), nodes were connected by an edge if their Euclidean distance was less than 20 units. For smaller instances (|V| = 25), nodes were connected by an edge if their Euclidean distance was less than 30 units. In both cases, additional edges were added if the generated network was not originally connected. For each instance, the cardinal robot's unimpeded travel time on each edge of the network was chosen to be a random integer between 5 and 20. The number of impeded edges for each instance was set in advance and the impeded edges were randomly selected among all edges available. The service time of each impeded edge was chosen to be a random integer between 1 and 5. The cardinal robot's origin was chosen to be the node closest to (0, 0). The support robot\\u2019s origin was chosen to be the node closest to (0, 50). The cardinal robot\\u2019s destination was chosen to be the node closest to (50, 50). This was done for each instance in an attempt to avoid trivial solutions corresponding to the cardinal robot using a single edge to reach the destination.\\\"\\n\\n'It should be noted that while the run time of the algorithm has been recorded for each instance, the code used to run the approximation algorithm was not optimized. In particular, the portion of the code corresponding to Step 4 likely added a significant amount of time to the total run time. This is because the shortest path algorithm used to find shortest paths between pairs of boundary nodes was Dijkstra\\u2019s algorithm and the algorithm was not terminated early once all boundary nodes had been reached.'\",\"1321\":\"'Mobile 3D printing. The accuracy of traditional gantry-based 3D printing relies on counting the steps of a stepper motor\\u2019s output. For mobile 3D printing, the biggest challenge is how to localize a robot, because the accuracy and reliability of the wheel encoder and Inertial measurement unit (IMU) cannot provide satisfactory position feedback. Therefore, some pioneers have explored different types of localization methods for mobile 3D printing systems.'\\n\\n'The robot arm assembly consisted of two actuated DoFs. The proximal DoF is a revolute joint whose rotation axis is perpendicular to the top surface. The distal DoF is a prismatic joint that is orthogonal to the proximal DoF. Each joint is driven by a Robotis Dynamixel XM430-W350 smart servo motor. Each servo motor integrated a motor controller, network communication and wheel encoder. The robot arm\\u2019s main structure has three separate ABS 3D printed parts, linked by aluminum hole pattern beams at the top and bottom. An optional circle track cart helps reduce the horizontal vibration of the robot arm when the arm length is extended in the radial direction. The repeating 3.5-mm holes on the aluminum beam provide a flexible arm length in the radial direction. The structure of the vertical prismatic joint is designed with one 8-mm lead screw and one 6-mm round shafting support rail fixed by a pillow block flange bearing. The printer head holder achieved vertical movement by connecting with the lead screw nut. A Hotend kit (Lerdge BP6 with 8-mm nozzle) is installed on the end of the printer head holder. To reduce sliding friction and better support the weight of the print head, an additional ball caster wheel is added to the bottom of the arm block.'\",\"1322\":null,\"1323\":\"'Animations of motion planning problems, code to reproduce numerical results of this section, and additional numerical experiments can be found in the GitHub repository [1].'\\n\\n'GitHub'\",\"1324\":null,\"1325\":null,\"1326\":\"'The data is collected in an open-source simulation environment, \\u2018AI2thor\\u2019 [9]. In this simulator, a robot is equipped with an RGBD camera whose pose can be varied. To simplify the data collection process and minimize the human labeling work, we collect training data using two cameras as follows. The two cameras are both fixed on the robot, with relative poses described in Sec. III. At each timestep, we move the robot to a random reachable position in the room, and use the two cameras to take two depth observations simultaneously. We build the two local (raw) occupancy maps Olow and Ohigh from the camera measurements. Then the combined map O* can be obtained using Eq. 1. We use Olow as the network input, and O* to supervise the training.'\",\"1327\":\"'Trajectory prediction is one of the most critical tasks in an autonomous system, allowing SDVs to safely navigate in an uncertain real-world environment. Given the historical observations, the trajectory prediction module predicts future positions of the given actors, which can then be consumed by the motion planning module of the SDV system. The current state-of-the-art approaches rely on the deep neural network models for this task. They commonly use an encoder network to generate embeddings from the historical observations (e.g., by using recurrent layers) and a decoder network to generate future trajectories as well as their uncertainties (e.g., [8], [9], [10], [11], [12], [13], [14], [15]). In the cases when there is map information available, a common approach is to rasterize the map polygons (e.g., lanes and other drivable surfaces) into a multi-channel bird\\u2019s-eye view (BEV) image, and use a convolutional neural network to extract scene context features from the BEV image which can then be fused with the embeddings pertaining to the historical observation [5], [6], [10], [11], [16], [17], [18].'\",\"1328\":null,\"1329\":\"'Encoding Target Pedestrian and Ego-Vehicle Dynamics. To directly capture information pertaining to target pedestrian and ego-vehicle dynamics, we also encode a vector of pedestrian features,\\\\np\\\\n\\u20d7 \\\\nt\\\\n= [px,t, py,t, pvx,t, pvy,t], and a vector of ego-vehicle features,\\\\ne\\\\n\\u20d7 \\\\nt\\\\n=[\\\\ne\\\\nx,t\\\\n,\\\\ne\\\\ny,t\\\\n,\\\\ne\\\\nvx,t\\\\n,\\\\ne\\\\nvy,t\\\\n]\\\\n, at each frame ft. Here, x and y are the locations of the pedestrian and ego-vehicle and vx and vy are the velocities in the global bird\\u2019s-eye-view reference frame. Velocity is calculated as\\\\n[vx,vy]=(\\\\nl\\\\n(i)\\\\nt\\\\n\\u2212\\\\nl\\\\n(i)\\\\nt\\u22121\\\\n)\\\\n, for t = 2\\u2026T where i is the target pedestrian or ego-vehicle. At t = 1, we set [vx,vy] = [0,0]. We also multiply the velocity by 1000 to scale it to an order of magnitude that is similar to the global x and y coordinates.'\\n\\n'Stacked with Fusion GRU (SF-GRU) [25]. This pedestrian action prediction model has a multi-level recurrent architecture that encodes and infuses five modalities of data gradually \\u2013 pedestrians\\u2019 appearance, surrounding context, and poses, 2D bounding box coordinates, and the ego-vehicle speed. We modify this to use 3D global coordinates instead of 2D bounding boxes.'\\n\\n'One of the most crucial yet challenging tasks for autonomous vehicles in urban environments is predicting the future behaviour of nearby pedestrians, especially at points of crossing. Predicting behaviour depends on many social and environmental factors, particularly interactions between road users. Capturing such interactions requires a global view of the scene and dynamics of the road users in three-dimensional space. This information, however, is missing from the current pedestrian behaviour benchmark datasets. Motivated by these challenges, we propose 1) a novel graph-based model for predicting pedestrian crossing action. Our method models pedestrians\\u2019 interactions with nearby road users through clustering and relative importance weighting of interactions using features obtained from the bird\\u2019s-eye-view. 2) We introduce a new dataset that provides 3D bounding box and pedestrian behavioural annotations for the existing nuScenes dataset. On the new data, our approach achieves state-of-the-art performance by improving on various metrics by more than 15% in comparison to existing methods. The dataset is available at https:\\/\\/github.com\\/huawei-noah\\/datasets\\/PePScenes.'\",\"1330\":null,\"1331\":\"'E. Pseudo-code'\\n\\n'The pseudo-code in Algorithm 1 demonstrates all the steps of hotspot identification. The UAV starting from an initial position v(0) = ai(0) is given a budget B and initializes GP with zero mean. At every sensing location k, the UAV updates the GP mean and variance functions (Lines 8 and 11) by using measurements collected up to and including the previous sensing locations (Line 16). To compute the objective function (Line 14), we calculate the average mean and the average variance (Lines 12 and 13) as the number of test points is not 1. If the budget B assigned to the UAV is exhausted (Line 2), the algorithm terminates and the UAV finds xALG from learned GP for the USV to sample from this location.'\",\"1332\":\"'Each bounding box and patch is encoded to form a latent representation. The latent representation is passed through a fully-connected neural network to generate the latent encoding at the next time step. The predicted encodings are decoded to get the predicted future location of the bounding box,\\\\nx\\\\n^\\\\nt+1\\\\ndyn\\\\nand its patch,\\\\np\\\\n^\\\\nt+1\\\\ndyn\\\\n.'\",\"1333\":\"'Extensive efforts have been made to improve the generalization ability of Reinforcement Learning (RL) methods via domain randomization and data augmentation. However, as more factors of variation are introduced during training, optimization becomes increasingly challenging, and empirically may result in lower sample efficiency and unstable training. Instead of learning policies directly from augmented data, we propose SOft Data Augmentation (SODA), a method that decouples augmentation from policy learning. Specifically, SODA imposes a soft constraint on the encoder that aims to maximize the mutual information between latent representations of augmented and non-augmented data, while the RL optimization process uses strictly non-augmented data. Empirical evaluations are performed on diverse tasks from DeepMind Control suite as well as a robotic manipulation task, and we find SODA to significantly advance sample efficiency, generalization, and stability in training over state-of-the-art vision-based RL methods.1'\\n\\n'We perform visual representation learning by projecting augmented observations into a compact latent space using the shared encoder and a learned projection, and similarly projecting non-augmented observations using a moving average of the learned network. The SODA objective is then to learn a mapping from latent features of the augmented observation to those of the original observation. With strong and varied data augmentation, SODA learns to ignore factors of variation that are irrelevant to the RL task, which greatly reduces observational overfitting [10]. Our visual representation learning task is a self-supervised learning task. Our approach is related to recent work on joint learning with self-supervision and RL [18]\\u2013[20], where two tasks are trained jointly on the same augmented observations. However, our method is fundamentally different. We decouple the training data flow by using non-augmented data for RL and using augmented data only for representation learning. Besides, instead of learning invariance by contrasting two augmented instances of the same image to a batch of negative samples [19], [21], [22], SODA learns to map augmented images to their non-augmented counterparts in latent space, without the need for negative samples. Because we strictly use data augmentation for representation learning and instead impose a soft constraint on the shared encoder through joint training, we call it soft data augmentation.'\\n\\n'SODA is implemented as a self-supervised auxiliary task that shares a common encoder f with an RL policy. For a given policy network parameterized by a collection of parameters \\u03b8, we split the network and corresponding parameters sequentially into an encoder f\\u03b8 and a policy \\u03c0\\u03b8 such that a = \\u03c0\\u03b8(f\\u03b8(o)) outputs a distribution over actions (and any other algorithm-specific values) for an input observation o. SODA then consists of the following three components: the shared encoder f\\u03b8, a projection g\\u03b8, and a prediction head h\\u03b8. We additionally consider an architecturally identical encoder f\\u03c8 and projection g\\u03c8 where \\u03c8 denotes a collection of parameters separate from \\u03b8. We denote f\\u03b8 as the online encoder and f\\u03c8 as the target encoder, and similarly for projections g\\u03b8, g\\u03c8. We then define the parameters \\u03c8 as an exponential moving average (EMA) of \\u03b8, and update \\u03c8 with every iteration of SODA using the update rule,'\\n\\n'SODA reformulates the problem of generalization as a representational consistency learning problem: the encoder f should learn to map different views of the same underlying state to similar feature vectors. This encourages the encoder to learn features that are shared across views, e.g. physical quantities and object interactions, and discard information that yields no predictive power such as background, lighting, and high-frequency noise. Given an observation o and a data augmentation t, we seek to encode o and o\\u2032 = t(o) into compact feature vectors z\\u2032, z\\u22c6\\u2208\\u211dK, respectively, by learned mappings f\\u03b8, g\\u03b8 and f\\u03c8, g\\u03c8 such that the mutual information I between o and o\\u2032 is maximally preserved. The mutual information between z\\u22c6 and z\\u2032 is then given by'\",\"1334\":null,\"1335\":\"'A ScanNet point cloud and the reconstruction results based on the four initialization methods. (b)-(e) are reconstructed from level 2 HGMMs with 64 Gaussian distributions. Different distributions are color coded and the reconstruction PSNR are noted in the brackets. Excessive outliers in the reconstructed point cloud are marked with red ellipses. We evaluate the effect of the different initialization methods on the reconstruction PSNR and their scale invariance property. Our experiments show that clustering-based initializations are beneficial for HGMMs.'\\n\\n'Our training and testing data is from two commonly used open-source point cloud datasets, namely the Stanford 3D Scanning Repository [21] and ScanNet [22]. Three of the'\",\"1336\":\"'This figure describes our overall network architecture for the pedestrian localization experiment. It consists of an encoder network used to learn a latent embedding and measurement covariance matrix, a state transition network used to learn a dynamics model and process covariance matrix and a decoder network used to decode the latent embedding to estimated depth along with capturing aleatoric and epistemic uncertainty.'\\n\\n'Encoder Network'\\n\\n'The encoder network receives a 24 \\u00d7 24 \\u00d7 1 grayscale image in the pendulum experiments or a 1600\\u00d7900 \\u00d7 3 RGB image for the pedestrian localization experiments to produce a latent embedding representing and an associated observation noise covariance matrix. In the pendulum experiment, the encoder network consists of a 5 \\u00d7 5 convolutional layer and a 3 \\u00d7 3 convolutional layer. This is followed by a fully connected layer to produce a feature vector x_ini,t. For the pedestrian localization experiments, similar to [19], we first extract PifPaf [23] features that represent a 17 \\u00d7 2 dimensional keypoint vector (xi,t, yi,t) for each pedestrian i along with a confidence score. They keypoint vector and confidence scores are concatenated to produce feature vector x_ini,t. In both experiments, we pass the feature vector x_ini,t to two fully connected layers to generate a latent embedding wi,t for each pedestrian along with a covariance matrix Ri,t that represents the observation noise. The output features of Linear1 and Linear2 are both 30 dimensions.'\\n\\n'Decoder Network'\\n\\n'The decoder network has two purposes: (1) receive the posteriori estimate of the latent embedding and covariance matrix to decode an estimated state, (2) to calculate the associated aleatoric and epistemic uncertainties of the state estimate. Aleatoric uncertainty is computed using the mean variance estimation (MVE) [24] technique where the decoder produces two outputs that represent the mean and variance of a Normal distribution that are sampled during inference. To calculate the epistemic uncertainty, we use a stochastic dropout technique [16] where several dropout layers were added to the decoder. During inference, multiple passes of the neural network generate a sample population from which the mean and variance can be calculated. The decoder network can be described as follows:'\\n\\n'To measure robustness to out-of-distribution noise, during evaluation, we set the maximum noise threshold to 75% to simulate out-of-distribution noise. We compare against two baselines. The first baseline, referred to as no dynamics, removes any aspects of a learned dynamical model. Here we map the outputs of the encoder network, wt and Rt directly to the decoder network and train end-to-end in a similar fashion. Our second baseline replaces the Kalman filter elements (e.g., State Transition Network, Kalman Filter Prediction, and Kalman Filter Update steps) with a vanilla LSTM. As demonstrated in Fig. 3, we show significant improvement to state estimation in both the in and out-of-distribution noise profile. Further, the recursive filter demonstrates better robustness to predicting confidence intervals.'\",\"1337\":\"https:\\/\\/github.com\\/r-pad\\/zephyr\",\"1338\":\"'Both geometric and semantic information of the search space are imperative for a good plan. We encode those properties in a weighted colored graph (geometric information ...'\\n\\n'Both geometric and semantic information of the search space are imperative for a good plan. We encode those properties in a weighted colored graph (geometric information in terms of edge weight and semantic information in terms of edge and vertex color) and propose a generalized A\\u2217 to find the shortest path among the set of paths with minimal inclusion of low-ranked color edges. We prove the completeness and optimality of this Class-Ordered A\\u2217 (COA\\u2217 ) algorithm with respect to the hereto defined notion of optimality. The utility of COA\\u2217 is numerically validated in a ternary graph with feasible, infeasible, and unknown vertices and edges for the cases of a 2D mobile robot, a 3D robotic arm, and a 5D robotic arm with limited sensing capabilities. We compare the results of COA\\u2217 to that of the regular A\\u2217 algorithm, the latter of which finds a shortest path regardless of the semantic information, and we show that the COA\\u2217 dominates the A\\u2217 solution in terms of finding less uncertain paths.'\\n\\n'We have introduced the notion of optimality on a weighted colored graph, which encodes both geometric and semantic information of the search space. We present a new search algorithm, the Class-Ordered A\\u2217 (COA\\u2217) to find a globally optimal path in a weighted colored graph by incrementally and lazily building an optimal search tree using a heuristic, and we proved the completeness and optimality of the algorithm. The optimal path of COA\\u2217 is the shortest path among the set of paths with minimal inferior class edges. In addition, COA\\u2217 monotonically finds a better path when the underlying graph has a strictly better class of vertices and edges. Finally, COA\\u2217 was numerically evaluated on a ternary graph with feasible, unknown, and infeasible classes against the standard A\\u2217 algorithm, which finds a shortest path regardless of uncertainty. The results of these numerical experiments confirmed the superiority of COA\\u2217 in terms of finding safer plans when the search space is partially known.'\",\"1339\":null,\"1340\":null,\"1341\":null,\"1342\":\"'https:\\/\\/sites.google.com\\/view\\/hamr-icra-2021'\",\"1343\":\"'Additional calibration is required to set the radio local oscillator frequency to properly transmit 802.15.4 packets at 2.405 GHz for channel 11 as well as receive packets at 2.410 GHz on 802.15.4 channel 12. This additional calibration is needed as the optical programming phase is not accurate enough to calibrate the local oscillator. To calibrate, SC\\u00b5M sweeps across tuning settings for each of the three 5-bit capacitive DACs used to set the frequency for the LC local oscillator (coarse, mid, and fine settings). The LC codes are swept until a packet is properly transmitted on channel 11 to an RX OpenMote. Next, the LC codes are again swept until SC\\u00b5M properly receives a packet on channel 12 from an TX OpenMote (Figure 2). The TX and RX LC configuration codes are then fixed and SC\\u00b5M is now properly calibrated to transmit and receive.'\",\"1344\":null,\"1345\":null,\"1346\":\"'The soft robot was actuated using a pneumatic controller unit based on an open source hardware platform [31]. Here the control action is equivalent to a torque applied at the posterior of the segment [26]. An external compressor supplied compressed air to the actuation unit at a constant pressure of 20 psi. The air pressure in the soft segment was regulated by a pulse-width modulation (PWM) at 100 Hz. The actuation signals, converted using the torque-to-PWM mapping explained subsequently, were serially transmitted to the control board (Arduino Mega). At a given time instance, depending on the sign of the torque, only one compartment out of the two in the segment was actuated. A positive torque commanded compartment A of the segment, while a negative torque commanded compartment B.'\",\"1347\":null,\"1348\":null,\"1349\":\"'System architecture for reconstructing a functionally equivalent scene. (A) Per-frame segmentation and cross-frame data fusion produce (a) a 3D volumetric panoptic map with fine-grained semantics and geometry, served as the input for (B) physical common sense reasoning that matches, aligns, and replaces segmented object meshes with functionally equivalent CAD alternatives. Specifically, (b) by geometric similarity, a ranking-based matching algorithm selects a shortlist of CAD candidates, followed by an optimization-based process that finds a proper transformation and scaling between the CAD candidates and object mesh. A global physical violation check is further applied to finalize CAD replacements to ensure physical plausibility. (C) This CAD augmented scene can be seamlessly imported to existing simulators; (c) contact graph encodes the kinematic relations among the objects in the scene as the planning space for a robot.'\",\"1350\":\"'In contrast to the aforementioned work, we propose to define information gain in terms of persistent topological features modeled as rewards. The key insight of this effort is that the incorporation of topological attributes can yield unique knowledge with respect to the structure of point cloud data which is not obtainable from other metrics. Our work makes the following contributions: (i) a fully automated online deep reinforcement learning approach for achieving the NBV on streaming 3D point cloud data; (ii) a novel and generalizable topology-based information gain metric; (iii) the public release of a point cloud dataset for seven exclusive objects in various labeled positions, an open-source CAD design for a custom robot manipulator, and open-source software for the transformation, union, and registration of point clouds given their viewpoint coordinates.'\",\"1351\":\"'https:\\/\\/github.com\\/UMich-CURLY\\/unified_cvo'\\n\\n'This paper reports on a novel nonparametric rigid point cloud registration framework, Semantic Continuous Visual Odometry (CVO), that jointly integrates geometric and semantic measurements such as color or semantic labels into the alignment process and does not require explicit data association. The point clouds are represented as nonparametric functions in a reproducible kernel Hilbert space. The alignment problem is formulated as maximizing the inner product between two functions, essentially a sum of weighted kernels, each of which exploits the local geometric and semantic features. As a result of the continuous models, analytical gradients can be computed, and a local solution can be obtained by optimization over the rigid body transformation group. Besides, we present a new point cloud alignment metric that is intrinsic to the proposed framework and takes into account geometric and semantic information. The evaluations using publicly available stereo and RGB-D datasets show that the proposed method outperforms state-of-the-art outdoor and indoor frame-to-frame registration methods. An open-source GPU implementation is also provided.'\\n\\n'We developed a nonparametric registration framework that integrates geometric and semantic measurements and does not require explicit data association. The proposed approach can utilize the extra visual and semantic information from modern range sensors while not being restricted by pairwise data correspondences. The novel hierarchical distributed representation of features via the tensor product representation provides a mathematically sound and systematic way of incorporating semantic knowledge into the point cloud model. The evaluations using publicly available stereo and RGB-D datasets show that the proposed method outperforms state-of-the-art outdoor and indoor frame-to-frame registration methods. We also provided an open-source GPU implementation.'\\n\\n'An open source GPU implementation available at [31]: https:\\/\\/github.com\\/UMich-CURLY\\/unified_cvo'\\n\\n'https:\\/\\/github.com\\/UMich-CURLY\\/unified_cvo'\",\"1352\":\"'https:\\/\\/boxixia.github.io\\/Flexipod'\\n\\n'The hardware design, code, and videos are available at (https:\\/\\/boxixia.github.io\\/Flexipod).'\\n\\n'This paper presents an open-source untethered quadrupedal soft robot platform for dynamic locomotion (e.g., high-speed running and backflipping). The robot is mostly soft...'\\n\\n\\\"This paper presents an open-source untethered quadrupedal soft robot platform for dynamic locomotion (e.g., high-speed running and backflipping). The robot is mostly soft (80 vol.%) while driven by four geared servo motors. The robot\\u2019s soft body and soft legs were 3D printed with gyroid infill using a flexible material, enabling it to conform to the environment and passively stabilize during locomotion in multi-terrain environments. In addition, we simulated the robot in a real-time soft body simulation. With tuned gaits in simulation, the real robot can locomote at a speed of 0.9 m\\/s (2.5 body length\\/second), substantially faster than most untethered legged soft robots published to date. We hope this platform, along with its verified simulator, can catalyze agile soft robots' development.\\\"\\n\\n'Finally, free-form manufacturing of soft robots has been challenging. Compared to subtractive manufacturing, 3D printing enables rapid prototyping for objects with complex structures and functionalities. 3D printing enables designs to be open-sourced and fabricated with little specialized tooling. For example, robotic platforms such as the \\\"Poppy\\\" humanoid robot [27] and the \\\"Open Dynamic Robot Initiative\\\" quadruped [28] used 3D printed rigid parts extensively. Advances in soft and flexible 3D printing have opened new possibilities for fabricating soft robot directly. Many 3D printed soft actuators, sensors, and control circuits have been proposed [29]\\u2013[32]. While some of the technologies are still nascent, materials such as Thermoplastic polyurethanes (TPU) are widely available and can be printed with an off-the-shelf fused deposition modeling (FDM) 3D printer. By varying the infill density, an FDM printer can fabricate lattices with varying meta-material stiffness.'\\n\\n'In this paper, we present an open-source soft quadrupedal robot platform (named Flexipod) for researching soft dynamic locomotion. As shown in Fig. 1, this robot combines the merit of both soft material and rigid BLDC actuators. The Flexipod can be easily manufactured using a desktop 3D printer and offthe-shelf flexible TPU material. The soft body and legs are compliant and can absorb impact during locomotion, thereby protecting the motors and electronics. Driven by BLDC servo motors, the Flexipod can dynamically locomote at 0.9 m\\/s (2.5 BL\\/s) and perform backflips. Locomotion in various terrains was tested, as shown in the following section. Besides, we developed and verified a real-time soft body simulation based on Titan [24], which enabled rapid design improvements and gait tuning.'\\n\\n'https:\\/\\/boxixia.github.io\\/Flexipod'\",\"1353\":null,\"1354\":null,\"1355\":\"'https:\\/\\/www.youtube.com\\/watch?v=K86zXKdobTU'\",\"1356\":null,\"1357\":\"'Commanding mobile robot movement based on natural language processing with RNN encoder\\\\xaddecoder'\",\"1358\":\"'Actions. The action space consists of a gripper pose displacement, and an open\\/close command. The gripper pose displacement is a difference between the current pose and the desired pose in Cartesian space, encoded as translation T \\u2208 R3, and vertical rotation encoded via a sine-cosine encoding R \\u2208 R2. A gripper command (whether open or closed) is encoded as one-hot vector [gripper_command] \\u2208 0, 1. The full action is defined as at = (T, R, gripper_command).'\",\"1359\":\"'For the dynamics model, we train a 3-D convolution neural network that takes in the contingency-aware observation (voxels), action and last three position changes. The voxels contingent to end effector are encoded using three 3d convolution with kernel size 3 and stride 2. Channels of these 3d conv layers are 16, 32, 64, respectively. A 64-unit FC layer is connected to the flattened features after convolution. The action is encoded with one-hot vector connected to a 64-unit FC layer. The last three \\u03b4 positions are also encoded with a 64-unit FC layer. The three encoded features are concatenated and go through a 128-unit hidden FC layer and output predicted change in position. All intermediate layers use ReLu as activation.'\",\"1360\":\"'The rest of this section will explore how each method performed on each track. These methods were compared over various real and simulated datasets including the TORCS open source driving simulator [26] dataset and the KITTI dataset [27]. For the TORCS dataset, we used the baseline test set collected by [28]. All hardware experiments were conducted using our 1\\/5 scale autonomous vehicle test plat-form (Figure 4a) [29].'\",\"1361\":\"'The Euler angle position of the exoskeleton\\u2019s cuff can be tracked using onboard encoder feedback. However, in prior works it was shown that encoder feedback for this prototype is only accurate to within 1\\u00b0, due to tolerancing during fabrication [16]. Therefore, to achieve a higher accuracy in the position data needed to estimate shoulder impedance, a 3D motion capture system (Bonita 10 System, Vicon, UK) was added to track the position of the robot and\\/or human subjects.'\",\"1362\":null,\"1363\":null,\"1364\":\"'A pseudocode view of the Strobe meta-algorithm can be seen in Algorithm 3. At a high level, the algorithm alternates between optimizing over blue and red pods in parallel over some maximum number of epochs or until the optimization converges subject to some specified tolerance. The optimize input can be any optimization algorithm that is compatible with the underlying objectives and constraints.'\",\"1365\":null,\"1366\":null,\"1367\":\"'TABLE III: Comparison to state-of-the-art real-time methods on YouTube VIS. We use our sub-training and subvalidation splits for YouTube VIS and perform joint training with COCO using a 1:1 data sampling ratio. (Box AP is not evaluated in the authors\\u2019 code base of SOLOv2.)'\\n\\n'To encode pixel motion, FeatFlowNet takes as input C3 features from the ResNet backbone. As shown in Table VIc, we choose to reduce the channels to 1\\/4 before it enters FeatFlowNet as the AP only drops slightly while being much faster. If we further decrease it to 1\\/8, the FPS does not increase by a large margin, and flow pre-training does not converge well. As shown in Table VId, accurate flow maps are crucial for transforming features across frames. Notably, our FeatFlowNet is equally effective for mask prediction as FlowNetS [15], while being faster as it reuses C3 features for pixel motion estimation (whereas FlowNetS computes flow starting from raw RGB pixels).'\\n\\n'We propose YolactEdge, the first competitive instance segmentation approach that runs on small edge devices at real-time speeds. Specifically, YolactEdge runs at up to 30.8 FPS on a Jetson AGX Xavier (and 172.7 FPS on an RTX 2080 Ti) with a ResNet-101 backbone on 550x550 resolution images. To achieve this, we make two improvements to the state-of-the-art image-based real-time method YOLACT [1]: (1) applying TensorRT optimization while carefully trading off speed and accuracy, and (2) a novel feature warping module to exploit temporal redundancy in videos. Experiments on the YouTube VIS and MS COCO datasets demonstrate that YolactEdge produces a 3-5x speed up over existing real-time methods while producing competitive mask and box detection accuracy. We also conduct ablation studies to dissect our design choices and modules. Code and models are available at https:\\/\\/github.com\\/haotian-liu\\/yolact_edge.'\",\"1368\":\"'Several enhancements were made to push the performance of semantic segmentation higher by making improvements to encoder and decoder in FCNs. Dilated residual convolutions [26], Feature pyramid networks [1], [27], Spatial pyramid pooling [28] etc. are examples of improvements made to encoder while U-Net [29], Densely connected CRFs [26], [30] are examples of improvements made to decoder. We use a combination of feature pyramid networks and a light weight asymmetric decoder presented by Kirillov et al. [1] to learn semantic segmentation.'\",\"1369\":\"'Multi-Motion Segmentation with a monocular event camera on an EV-IMO dataset sequence. Top Row: The event frames are color-coded by cluster membership. The corresponding grayscale frames are shown in the bottom row. Bounding boxes on the images are color coded with respect to the objects for reference. Note that grayscale images are not used for computation and are provided for visualization purposes only. All the images in this paper are best viewed in color.'\\n\\n'Qualitative Evaluation of our method on three datasets. Top two rows: EV-IMO dataset, Bottom two rows: MOD dataset. Insets show the corresponding grayscale\\/RGB images for reference. The cluster membership is color coded where gray color indicates background cluster. Bounding boxes on the images are color coded with respect to the objects for reference.'\\n\\n'Segmentation of moving objects in dynamic scenes is a key process in scene understanding for navigation tasks. Classical cameras suffer from motion blur in such scenarios rendering them effete. On the contrary, event cameras, because of their high temporal resolution and lack of motion blur, are tailor-made for this problem. We present an approach for monocular multi-motion segmentation, which combines bottom-up feature tracking and top-down motion compensation into a unified pipeline, which is the first of its kind to our knowledge. Using the events within a time-interval, our method segments the scene into multiple motions by splitting and merging. We further speed up our method by using the concept of motion propagation and cluster keyslices.The approach was successfully evaluated on both challenging real-world and synthetic scenarios from the EV-IMO, EED, and MOD datasets and outperformed the state-of-the-art detection rate by 12%, achieving a new state-of-the-art average detection rate of 81.06%, 94.2% and 82.35% on the aforementioned datasets. To enable further research and systematic evaluation of multi-motion segmentation, we present and open-source a new dataset\\/benchmark called MOD++, which includes challenging sequences and extensive data stratification in-terms of camera and object motion, velocity magnitudes, direction, and rotational speeds.'\\n\\n'New open-source multi-motion segmentation dataset and benchmark MOD++ including extensive motion stratification and including challenging collision\\/exploding sequences.'\\n\\n'A comprehensive qualitative and quantitative evaluation is provided on three challenging event motion segmentation datasets, namely, EV-IMO, EED and MOD showcasing the robustness of our approach. Our method outperforms the previous state-of-the-art approaches by upto \\u223c12% detection, thereby achieving the new state-of-the-art on the three aforementioned datasets. To accelerate further research in this area, we present and open-source a new benchmark dataset MOD++ which includes challenging scenes such as cube breaking and a mug getting shot by a bullet along with extensive data stratification in-terms of camera and object motion, velocity magnitudes, direction and rotational speeds. We achieve 73.21% detection rate on MOD++ which is 2 to 3\\u00d7 higher than the state-of-the-art methods.'\",\"1370\":null,\"1371\":null,\"1372\":null,\"1373\":null,\"1374\":null,\"1375\":null,\"1376\":null,\"1377\":null,\"1378\":null,\"1379\":null,\"1380\":null,\"1381\":null,\"1382\":null,\"1383\":\"'The test environment consisted of a machine equipped with an Intel XEON E5-2650 CPU, 256GB of RAM, and Ubuntu 16.04. The heuristic was coded in Python 3.5.'\",\"1384\":null,\"1385\":\"'https:\\/\\/bit.ly\\/3mJbeb3'\\n\\n'https:\\/\\/bit.ly\\/3mJbeb3'\\n\\n'Pseudocode for the overall approach is shown in Alg. 1. The approach starts by constructing the roadmaps and initializing the motion tree. The core loop is driven by two methods: (i) selecting a group \\u0393v from the motion-tree partition, and (ii) expanding the motion tree\\\\nT\\\\nalong the roadmap paths associated with \\u0393v. This process is repeated until a solution is found or a runtime limit is reached.'\\n\\n\\\"Pseudocode for the group expansion is shown in Alg. 1:b. The group expansion starts by selecting a node v' \\u2208 \\u0393v at random from which to expand\\\\nT\\\\n(Alg. 1:b1). The robots are considered one at a time in some random order \\u03c01,\\u2026, \\u03c0n. The objective for robot \\u03c0i is to reach\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(1),\\u2026,\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(end) in succession while avoiding collisions with the obstacles and the trajectories\\\\n\\u03b6\\\\n\\u03c0\\\\n1\\\\n,\\u2026,\\\\n\\u03b6\\\\n\\u03c0\\\\ni\\u22121\\\\nalready planned for robots \\u03c01,\\u2026, \\u03c0i\\u20131 (Alg. 1:b3-b19). Initially, the target index j is set to 1, and is incremented when\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(j)\\\\nis reached. The group expansion keeps track of the states that should reach\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(j)\\\\n, denoted by\\\\nX\\\\n. Initially,\\\\nX={\\\\nv\\\\n\\u2032\\\\n.\\\\ns\\\\n\\u03c0\\\\ni\\\\n}\\\\n(Alg. 1:b4). At each iteration, a target point is sampled near\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(j)\\\\n(Alg. 1:b6). A state is then selected at random from\\\\nX\\\\n(Alg. 1:b7), and a PID controller is employed to steer robot \\u03c0i from the selected state toward the target point (Alg. 1:b8-b10). The new state snew obtained after each integration step is checked for collisions with the obstacles and robots \\u03c01,\\u2026, \\u03c0i\\u20131, which move along the previously planned trajectories\\\\n\\u03b6\\\\n\\u03c0\\\\n1\\\\n,\\u2026,\\\\n\\u03b6\\\\n\\u03c0\\\\ni\\u22121\\\\n(Alg. 1:b11). If a collision is found, the PID controller is stopped. In that case, the group expansion for robot \\u03c0i continues with the next iteration, i.e., goes back to Alg. 1:b6 to select a state from\\\\nX\\\\n, sample a point near\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(j)\\\\n, and apply a PID controller to steer the robot from the selected state toward the sampled point. If snew is not in collision and it does not reach\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(j)\\\\n, then snew is added to\\\\nX\\\\n. If snew reaches\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(j)\\\\n(Alg. 1:b14), then\\\\n\\u03c3\\\\n\\u03c0\\\\ni\\\\n(j+1)\\\\nbecomes the next target. In that case,\\\\nX\\\\nis reset to contain just snew.\\\"\"},\"Stars\":{\"0\":-1,\"1\":-1,\"2\":10,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":3,\"26\":-1,\"27\":4,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":2,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":38,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":91,\"91\":22,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":4,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":10,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":117,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":116,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":39,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":47,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":261,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":3,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":35,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":18,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":0,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":280,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":0,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":204,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":23,\"879\":3,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":7,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":39,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":8,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":248,\"1022\":-1,\"1023\":-1,\"1024\":-1,\"1025\":-1,\"1026\":-1,\"1027\":-1,\"1028\":-1,\"1029\":-1,\"1030\":-1,\"1031\":-1,\"1032\":-1,\"1033\":121,\"1034\":-1,\"1035\":-1,\"1036\":-1,\"1037\":-1,\"1038\":-1,\"1039\":4,\"1040\":-1,\"1041\":-1,\"1042\":-1,\"1043\":-1,\"1044\":-1,\"1045\":6,\"1046\":-1,\"1047\":-1,\"1048\":-1,\"1049\":-1,\"1050\":-1,\"1051\":-1,\"1052\":-1,\"1053\":-1,\"1054\":31,\"1055\":-1,\"1056\":-1,\"1057\":-1,\"1058\":-1,\"1059\":-1,\"1060\":-1,\"1061\":-1,\"1062\":-1,\"1063\":-1,\"1064\":-1,\"1065\":-1,\"1066\":-1,\"1067\":-1,\"1068\":-1,\"1069\":238,\"1070\":-1,\"1071\":-1,\"1072\":-1,\"1073\":34,\"1074\":-1,\"1075\":4,\"1076\":42,\"1077\":-1,\"1078\":-1,\"1079\":-1,\"1080\":-1,\"1081\":5,\"1082\":-1,\"1083\":-1,\"1084\":-1,\"1085\":-1,\"1086\":3,\"1087\":-1,\"1088\":-1,\"1089\":-1,\"1090\":-1,\"1091\":-1,\"1092\":-1,\"1093\":-1,\"1094\":-1,\"1095\":-1,\"1096\":-1,\"1097\":-1,\"1098\":-1,\"1099\":-1,\"1100\":-1,\"1101\":-1,\"1102\":-1,\"1103\":-1,\"1104\":9,\"1105\":-1,\"1106\":-1,\"1107\":-1,\"1108\":-1,\"1109\":-1,\"1110\":-1,\"1111\":-1,\"1112\":-1,\"1113\":-1,\"1114\":-1,\"1115\":-1,\"1116\":-1,\"1117\":-1,\"1118\":-1,\"1119\":-1,\"1120\":-1,\"1121\":-1,\"1122\":-1,\"1123\":27,\"1124\":-1,\"1125\":-1,\"1126\":-1,\"1127\":-1,\"1128\":-1,\"1129\":-1,\"1130\":-1,\"1131\":-1,\"1132\":-1,\"1133\":-1,\"1134\":2,\"1135\":-1,\"1136\":-1,\"1137\":-1,\"1138\":-1,\"1139\":-1,\"1140\":-1,\"1141\":-1,\"1142\":-1,\"1143\":-1,\"1144\":-1,\"1145\":-1,\"1146\":-1,\"1147\":-1,\"1148\":-1,\"1149\":-1,\"1150\":-1,\"1151\":-1,\"1152\":-1,\"1153\":-1,\"1154\":-1,\"1155\":-1,\"1156\":-1,\"1157\":-1,\"1158\":-1,\"1159\":-1,\"1160\":-1,\"1161\":-1,\"1162\":-1,\"1163\":-1,\"1164\":-1,\"1165\":-1,\"1166\":-1,\"1167\":-1,\"1168\":-1,\"1169\":-1,\"1170\":-1,\"1171\":-1,\"1172\":-1,\"1173\":-1,\"1174\":-1,\"1175\":-1,\"1176\":-1,\"1177\":-1,\"1178\":-1,\"1179\":-1,\"1180\":2,\"1181\":-1,\"1182\":-1,\"1183\":-1,\"1184\":-1,\"1185\":-1,\"1186\":-1,\"1187\":-1,\"1188\":-1,\"1189\":-1,\"1190\":-1,\"1191\":-1,\"1192\":-1,\"1193\":-1,\"1194\":-1,\"1195\":-1,\"1196\":-1,\"1197\":-1,\"1198\":-1,\"1199\":-1,\"1200\":-1,\"1201\":-1,\"1202\":-1,\"1203\":79,\"1204\":-1,\"1205\":-1,\"1206\":1,\"1207\":-1,\"1208\":-1,\"1209\":-1,\"1210\":-1,\"1211\":-1,\"1212\":1,\"1213\":-1,\"1214\":-1,\"1215\":-1,\"1216\":-1,\"1217\":-1,\"1218\":-1,\"1219\":-1,\"1220\":-1,\"1221\":-1,\"1222\":-1,\"1223\":-1,\"1224\":-1,\"1225\":-1,\"1226\":2,\"1227\":-1,\"1228\":-1,\"1229\":-1,\"1230\":-1,\"1231\":-1,\"1232\":-1,\"1233\":-1,\"1234\":-1,\"1235\":-1,\"1236\":-1,\"1237\":-1,\"1238\":-1,\"1239\":-1,\"1240\":-1,\"1241\":-1,\"1242\":-1,\"1243\":-1,\"1244\":-1,\"1245\":-1,\"1246\":-1,\"1247\":6,\"1248\":-1,\"1249\":-1,\"1250\":-1,\"1251\":-1,\"1252\":-1,\"1253\":2,\"1254\":-1,\"1255\":-1,\"1256\":-1,\"1257\":-1,\"1258\":-1,\"1259\":-1,\"1260\":-1,\"1261\":-1,\"1262\":-1,\"1263\":-1,\"1264\":-1,\"1265\":-1,\"1266\":-1,\"1267\":4,\"1268\":-1,\"1269\":302,\"1270\":-1,\"1271\":-1,\"1272\":-1,\"1273\":-1,\"1274\":22,\"1275\":1,\"1276\":-1,\"1277\":-1,\"1278\":0,\"1279\":-1,\"1280\":-1,\"1281\":-1,\"1282\":1,\"1283\":-1,\"1284\":-1,\"1285\":-1,\"1286\":-1,\"1287\":-1,\"1288\":-1,\"1289\":-1,\"1290\":-1,\"1291\":-1,\"1292\":-1,\"1293\":-1,\"1294\":-1,\"1295\":-1,\"1296\":-1,\"1297\":-1,\"1298\":-1,\"1299\":-1,\"1300\":-1,\"1301\":-1,\"1302\":-1,\"1303\":45,\"1304\":-1,\"1305\":-1,\"1306\":-1,\"1307\":-1,\"1308\":-1,\"1309\":-1,\"1310\":-1,\"1311\":3,\"1312\":-1,\"1313\":-1,\"1314\":-1,\"1315\":-1,\"1316\":-1,\"1317\":2,\"1318\":-1,\"1319\":-1,\"1320\":-1,\"1321\":-1,\"1322\":-1,\"1323\":-1,\"1324\":-1,\"1325\":-1,\"1326\":-1,\"1327\":-1,\"1328\":-1,\"1329\":-1,\"1330\":-1,\"1331\":-1,\"1332\":-1,\"1333\":-1,\"1334\":-1,\"1335\":-1,\"1336\":-1,\"1337\":18,\"1338\":-1,\"1339\":-1,\"1340\":-1,\"1341\":-1,\"1342\":-1,\"1343\":-1,\"1344\":-1,\"1345\":-1,\"1346\":-1,\"1347\":-1,\"1348\":-1,\"1349\":-1,\"1350\":-1,\"1351\":10,\"1352\":-1,\"1353\":-1,\"1354\":-1,\"1355\":-1,\"1356\":-1,\"1357\":-1,\"1358\":-1,\"1359\":-1,\"1360\":-1,\"1361\":-1,\"1362\":-1,\"1363\":-1,\"1364\":-1,\"1365\":-1,\"1366\":-1,\"1367\":1094,\"1368\":-1,\"1369\":-1,\"1370\":-1,\"1371\":-1,\"1372\":-1,\"1373\":-1,\"1374\":-1,\"1375\":-1,\"1376\":-1,\"1377\":-1,\"1378\":-1,\"1379\":-1,\"1380\":-1,\"1381\":-1,\"1382\":-1,\"1383\":-1,\"1384\":-1,\"1385\":-1},\"Forks\":{\"0\":-1,\"1\":-1,\"2\":12,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":5,\"26\":-1,\"27\":12,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":9,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":12,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":11,\"91\":6,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":2,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":2,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":25,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":31,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":8,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":11,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":85,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":2,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":12,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":3,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":52,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":36,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":7,\"879\":1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":7,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":3,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":58,\"1022\":-1,\"1023\":-1,\"1024\":-1,\"1025\":-1,\"1026\":-1,\"1027\":-1,\"1028\":-1,\"1029\":-1,\"1030\":-1,\"1031\":-1,\"1032\":-1,\"1033\":20,\"1034\":-1,\"1035\":-1,\"1036\":-1,\"1037\":-1,\"1038\":-1,\"1039\":4,\"1040\":-1,\"1041\":-1,\"1042\":-1,\"1043\":-1,\"1044\":-1,\"1045\":4,\"1046\":-1,\"1047\":-1,\"1048\":-1,\"1049\":-1,\"1050\":-1,\"1051\":-1,\"1052\":-1,\"1053\":-1,\"1054\":143,\"1055\":-1,\"1056\":-1,\"1057\":-1,\"1058\":-1,\"1059\":-1,\"1060\":-1,\"1061\":-1,\"1062\":-1,\"1063\":-1,\"1064\":-1,\"1065\":-1,\"1066\":-1,\"1067\":-1,\"1068\":-1,\"1069\":61,\"1070\":-1,\"1071\":-1,\"1072\":-1,\"1073\":8,\"1074\":-1,\"1075\":1,\"1076\":9,\"1077\":-1,\"1078\":-1,\"1079\":-1,\"1080\":-1,\"1081\":1,\"1082\":-1,\"1083\":-1,\"1084\":-1,\"1085\":-1,\"1086\":8,\"1087\":-1,\"1088\":-1,\"1089\":-1,\"1090\":-1,\"1091\":-1,\"1092\":-1,\"1093\":-1,\"1094\":-1,\"1095\":-1,\"1096\":-1,\"1097\":-1,\"1098\":-1,\"1099\":-1,\"1100\":-1,\"1101\":-1,\"1102\":-1,\"1103\":-1,\"1104\":14,\"1105\":-1,\"1106\":-1,\"1107\":-1,\"1108\":-1,\"1109\":-1,\"1110\":-1,\"1111\":-1,\"1112\":-1,\"1113\":-1,\"1114\":-1,\"1115\":-1,\"1116\":-1,\"1117\":-1,\"1118\":-1,\"1119\":-1,\"1120\":-1,\"1121\":-1,\"1122\":-1,\"1123\":171,\"1124\":-1,\"1125\":-1,\"1126\":-1,\"1127\":-1,\"1128\":-1,\"1129\":-1,\"1130\":-1,\"1131\":-1,\"1132\":-1,\"1133\":-1,\"1134\":3,\"1135\":-1,\"1136\":-1,\"1137\":-1,\"1138\":-1,\"1139\":-1,\"1140\":-1,\"1141\":-1,\"1142\":-1,\"1143\":-1,\"1144\":-1,\"1145\":-1,\"1146\":-1,\"1147\":-1,\"1148\":-1,\"1149\":-1,\"1150\":-1,\"1151\":-1,\"1152\":-1,\"1153\":-1,\"1154\":-1,\"1155\":-1,\"1156\":-1,\"1157\":-1,\"1158\":-1,\"1159\":-1,\"1160\":-1,\"1161\":-1,\"1162\":-1,\"1163\":-1,\"1164\":-1,\"1165\":-1,\"1166\":-1,\"1167\":-1,\"1168\":-1,\"1169\":-1,\"1170\":-1,\"1171\":-1,\"1172\":-1,\"1173\":-1,\"1174\":-1,\"1175\":-1,\"1176\":-1,\"1177\":-1,\"1178\":-1,\"1179\":-1,\"1180\":9,\"1181\":-1,\"1182\":-1,\"1183\":-1,\"1184\":-1,\"1185\":-1,\"1186\":-1,\"1187\":-1,\"1188\":-1,\"1189\":-1,\"1190\":-1,\"1191\":-1,\"1192\":-1,\"1193\":-1,\"1194\":-1,\"1195\":-1,\"1196\":-1,\"1197\":-1,\"1198\":-1,\"1199\":-1,\"1200\":-1,\"1201\":-1,\"1202\":-1,\"1203\":766,\"1204\":-1,\"1205\":-1,\"1206\":3,\"1207\":-1,\"1208\":-1,\"1209\":-1,\"1210\":-1,\"1211\":-1,\"1212\":2,\"1213\":-1,\"1214\":-1,\"1215\":-1,\"1216\":-1,\"1217\":-1,\"1218\":-1,\"1219\":-1,\"1220\":-1,\"1221\":-1,\"1222\":-1,\"1223\":-1,\"1224\":-1,\"1225\":-1,\"1226\":0,\"1227\":-1,\"1228\":-1,\"1229\":-1,\"1230\":-1,\"1231\":-1,\"1232\":-1,\"1233\":-1,\"1234\":-1,\"1235\":-1,\"1236\":-1,\"1237\":-1,\"1238\":-1,\"1239\":-1,\"1240\":-1,\"1241\":-1,\"1242\":-1,\"1243\":-1,\"1244\":-1,\"1245\":-1,\"1246\":-1,\"1247\":43,\"1248\":-1,\"1249\":-1,\"1250\":-1,\"1251\":-1,\"1252\":-1,\"1253\":4,\"1254\":-1,\"1255\":-1,\"1256\":-1,\"1257\":-1,\"1258\":-1,\"1259\":-1,\"1260\":-1,\"1261\":-1,\"1262\":-1,\"1263\":-1,\"1264\":-1,\"1265\":-1,\"1266\":-1,\"1267\":6,\"1268\":-1,\"1269\":41,\"1270\":-1,\"1271\":-1,\"1272\":-1,\"1273\":-1,\"1274\":11,\"1275\":7,\"1276\":-1,\"1277\":-1,\"1278\":1,\"1279\":-1,\"1280\":-1,\"1281\":-1,\"1282\":0,\"1283\":-1,\"1284\":-1,\"1285\":-1,\"1286\":-1,\"1287\":-1,\"1288\":-1,\"1289\":-1,\"1290\":-1,\"1291\":-1,\"1292\":-1,\"1293\":-1,\"1294\":-1,\"1295\":-1,\"1296\":-1,\"1297\":-1,\"1298\":-1,\"1299\":-1,\"1300\":-1,\"1301\":-1,\"1302\":-1,\"1303\":227,\"1304\":-1,\"1305\":-1,\"1306\":-1,\"1307\":-1,\"1308\":-1,\"1309\":-1,\"1310\":-1,\"1311\":2,\"1312\":-1,\"1313\":-1,\"1314\":-1,\"1315\":-1,\"1316\":-1,\"1317\":1,\"1318\":-1,\"1319\":-1,\"1320\":-1,\"1321\":-1,\"1322\":-1,\"1323\":-1,\"1324\":-1,\"1325\":-1,\"1326\":-1,\"1327\":-1,\"1328\":-1,\"1329\":-1,\"1330\":-1,\"1331\":-1,\"1332\":-1,\"1333\":-1,\"1334\":-1,\"1335\":-1,\"1336\":-1,\"1337\":2,\"1338\":-1,\"1339\":-1,\"1340\":-1,\"1341\":-1,\"1342\":-1,\"1343\":-1,\"1344\":-1,\"1345\":-1,\"1346\":-1,\"1347\":-1,\"1348\":-1,\"1349\":-1,\"1350\":-1,\"1351\":57,\"1352\":-1,\"1353\":-1,\"1354\":-1,\"1355\":-1,\"1356\":-1,\"1357\":-1,\"1358\":-1,\"1359\":-1,\"1360\":-1,\"1361\":-1,\"1362\":-1,\"1363\":-1,\"1364\":-1,\"1365\":-1,\"1366\":-1,\"1367\":236,\"1368\":-1,\"1369\":-1,\"1370\":-1,\"1371\":-1,\"1372\":-1,\"1373\":-1,\"1374\":-1,\"1375\":-1,\"1376\":-1,\"1377\":-1,\"1378\":-1,\"1379\":-1,\"1380\":-1,\"1381\":-1,\"1382\":-1,\"1383\":-1,\"1384\":-1,\"1385\":-1},\"Citations\":{\"0\":0,\"1\":2,\"2\":0,\"3\":4,\"4\":0,\"5\":0,\"6\":0,\"7\":0,\"8\":0,\"9\":1,\"10\":0,\"11\":0,\"12\":0,\"13\":1,\"14\":0,\"15\":1,\"16\":4,\"17\":0,\"18\":0,\"19\":0,\"20\":0,\"21\":1,\"22\":1,\"23\":0,\"24\":0,\"25\":3,\"26\":25,\"27\":0,\"28\":0,\"29\":0,\"30\":0,\"31\":0,\"32\":0,\"33\":0,\"34\":0,\"35\":0,\"36\":0,\"37\":1,\"38\":2,\"39\":0,\"40\":5,\"41\":0,\"42\":0,\"43\":0,\"44\":0,\"45\":0,\"46\":0,\"47\":2,\"48\":2,\"49\":0,\"50\":1,\"51\":0,\"52\":5,\"53\":1,\"54\":0,\"55\":4,\"56\":1,\"57\":1,\"58\":1,\"59\":2,\"60\":0,\"61\":1,\"62\":6,\"63\":0,\"64\":3,\"65\":0,\"66\":5,\"67\":0,\"68\":2,\"69\":8,\"70\":0,\"71\":1,\"72\":4,\"73\":0,\"74\":1,\"75\":9,\"76\":0,\"77\":0,\"78\":0,\"79\":0,\"80\":0,\"81\":1,\"82\":1,\"83\":2,\"84\":5,\"85\":0,\"86\":1,\"87\":2,\"88\":1,\"89\":2,\"90\":12,\"91\":0,\"92\":0,\"93\":4,\"94\":1,\"95\":1,\"96\":0,\"97\":0,\"98\":3,\"99\":2,\"100\":3,\"101\":0,\"102\":2,\"103\":1,\"104\":2,\"105\":1,\"106\":2,\"107\":12,\"108\":1,\"109\":6,\"110\":4,\"111\":0,\"112\":1,\"113\":0,\"114\":0,\"115\":0,\"116\":0,\"117\":5,\"118\":8,\"119\":0,\"120\":0,\"121\":0,\"122\":9,\"123\":0,\"124\":0,\"125\":0,\"126\":2,\"127\":7,\"128\":1,\"129\":4,\"130\":3,\"131\":0,\"132\":0,\"133\":1,\"134\":0,\"135\":4,\"136\":7,\"137\":4,\"138\":0,\"139\":1,\"140\":1,\"141\":0,\"142\":0,\"143\":2,\"144\":0,\"145\":1,\"146\":0,\"147\":1,\"148\":0,\"149\":1,\"150\":0,\"151\":1,\"152\":0,\"153\":0,\"154\":4,\"155\":4,\"156\":1,\"157\":4,\"158\":8,\"159\":1,\"160\":0,\"161\":4,\"162\":24,\"163\":1,\"164\":0,\"165\":0,\"166\":4,\"167\":0,\"168\":0,\"169\":1,\"170\":2,\"171\":2,\"172\":0,\"173\":0,\"174\":1,\"175\":0,\"176\":1,\"177\":4,\"178\":0,\"179\":18,\"180\":0,\"181\":0,\"182\":7,\"183\":11,\"184\":5,\"185\":1,\"186\":0,\"187\":3,\"188\":5,\"189\":0,\"190\":9,\"191\":1,\"192\":3,\"193\":20,\"194\":5,\"195\":2,\"196\":13,\"197\":5,\"198\":0,\"199\":0,\"200\":6,\"201\":3,\"202\":2,\"203\":0,\"204\":2,\"205\":3,\"206\":1,\"207\":13,\"208\":14,\"209\":2,\"210\":0,\"211\":1,\"212\":2,\"213\":2,\"214\":1,\"215\":0,\"216\":2,\"217\":3,\"218\":3,\"219\":1,\"220\":3,\"221\":0,\"222\":0,\"223\":4,\"224\":3,\"225\":0,\"226\":6,\"227\":0,\"228\":6,\"229\":2,\"230\":0,\"231\":1,\"232\":5,\"233\":1,\"234\":1,\"235\":1,\"236\":2,\"237\":2,\"238\":1,\"239\":10,\"240\":0,\"241\":1,\"242\":0,\"243\":5,\"244\":1,\"245\":20,\"246\":2,\"247\":0,\"248\":2,\"249\":4,\"250\":2,\"251\":1,\"252\":3,\"253\":0,\"254\":6,\"255\":0,\"256\":0,\"257\":0,\"258\":4,\"259\":1,\"260\":2,\"261\":5,\"262\":2,\"263\":0,\"264\":5,\"265\":2,\"266\":4,\"267\":6,\"268\":0,\"269\":8,\"270\":8,\"271\":5,\"272\":1,\"273\":3,\"274\":1,\"275\":0,\"276\":3,\"277\":2,\"278\":3,\"279\":0,\"280\":1,\"281\":0,\"282\":5,\"283\":1,\"284\":0,\"285\":1,\"286\":2,\"287\":0,\"288\":0,\"289\":0,\"290\":2,\"291\":3,\"292\":1,\"293\":1,\"294\":1,\"295\":2,\"296\":2,\"297\":0,\"298\":1,\"299\":0,\"300\":6,\"301\":3,\"302\":4,\"303\":12,\"304\":13,\"305\":2,\"306\":1,\"307\":1,\"308\":2,\"309\":0,\"310\":0,\"311\":1,\"312\":0,\"313\":11,\"314\":5,\"315\":0,\"316\":9,\"317\":0,\"318\":0,\"319\":2,\"320\":1,\"321\":0,\"322\":0,\"323\":0,\"324\":10,\"325\":1,\"326\":1,\"327\":4,\"328\":0,\"329\":12,\"330\":16,\"331\":2,\"332\":0,\"333\":2,\"334\":0,\"335\":5,\"336\":5,\"337\":18,\"338\":1,\"339\":2,\"340\":0,\"341\":2,\"342\":2,\"343\":1,\"344\":4,\"345\":1,\"346\":2,\"347\":0,\"348\":5,\"349\":1,\"350\":0,\"351\":1,\"352\":1,\"353\":0,\"354\":1,\"355\":1,\"356\":2,\"357\":1,\"358\":0,\"359\":4,\"360\":3,\"361\":3,\"362\":5,\"363\":2,\"364\":4,\"365\":0,\"366\":1,\"367\":0,\"368\":0,\"369\":0,\"370\":5,\"371\":1,\"372\":0,\"373\":1,\"374\":7,\"375\":7,\"376\":2,\"377\":1,\"378\":4,\"379\":0,\"380\":10,\"381\":0,\"382\":7,\"383\":8,\"384\":0,\"385\":0,\"386\":0,\"387\":0,\"388\":4,\"389\":0,\"390\":3,\"391\":4,\"392\":1,\"393\":0,\"394\":10,\"395\":1,\"396\":11,\"397\":1,\"398\":5,\"399\":2,\"400\":0,\"401\":6,\"402\":13,\"403\":2,\"404\":1,\"405\":0,\"406\":3,\"407\":0,\"408\":8,\"409\":0,\"410\":0,\"411\":7,\"412\":7,\"413\":2,\"414\":2,\"415\":0,\"416\":3,\"417\":6,\"418\":2,\"419\":0,\"420\":0,\"421\":0,\"422\":3,\"423\":2,\"424\":1,\"425\":0,\"426\":0,\"427\":0,\"428\":0,\"429\":0,\"430\":0,\"431\":4,\"432\":2,\"433\":2,\"434\":4,\"435\":20,\"436\":3,\"437\":2,\"438\":0,\"439\":2,\"440\":1,\"441\":0,\"442\":0,\"443\":0,\"444\":0,\"445\":1,\"446\":2,\"447\":15,\"448\":1,\"449\":3,\"450\":7,\"451\":0,\"452\":0,\"453\":0,\"454\":1,\"455\":1,\"456\":2,\"457\":2,\"458\":5,\"459\":35,\"460\":7,\"461\":37,\"462\":3,\"463\":0,\"464\":13,\"465\":8,\"466\":2,\"467\":0,\"468\":1,\"469\":3,\"470\":1,\"471\":1,\"472\":1,\"473\":0,\"474\":1,\"475\":1,\"476\":1,\"477\":2,\"478\":0,\"479\":1,\"480\":5,\"481\":0,\"482\":1,\"483\":1,\"484\":1,\"485\":0,\"486\":0,\"487\":4,\"488\":3,\"489\":2,\"490\":2,\"491\":2,\"492\":2,\"493\":2,\"494\":6,\"495\":0,\"496\":1,\"497\":2,\"498\":0,\"499\":1,\"500\":0,\"501\":7,\"502\":3,\"503\":9,\"504\":25,\"505\":1,\"506\":22,\"507\":3,\"508\":1,\"509\":5,\"510\":43,\"511\":26,\"512\":3,\"513\":3,\"514\":0,\"515\":1,\"516\":5,\"517\":0,\"518\":9,\"519\":7,\"520\":4,\"521\":1,\"522\":19,\"523\":1,\"524\":18,\"525\":10,\"526\":3,\"527\":1,\"528\":2,\"529\":4,\"530\":17,\"531\":1,\"532\":1,\"533\":3,\"534\":1,\"535\":0,\"536\":4,\"537\":2,\"538\":3,\"539\":0,\"540\":0,\"541\":0,\"542\":2,\"543\":0,\"544\":1,\"545\":1,\"546\":0,\"547\":7,\"548\":9,\"549\":0,\"550\":12,\"551\":0,\"552\":0,\"553\":8,\"554\":5,\"555\":1,\"556\":0,\"557\":0,\"558\":7,\"559\":0,\"560\":4,\"561\":0,\"562\":0,\"563\":4,\"564\":12,\"565\":9,\"566\":0,\"567\":1,\"568\":2,\"569\":2,\"570\":6,\"571\":4,\"572\":8,\"573\":7,\"574\":9,\"575\":21,\"576\":8,\"577\":30,\"578\":5,\"579\":17,\"580\":2,\"581\":1,\"582\":0,\"583\":1,\"584\":0,\"585\":3,\"586\":3,\"587\":1,\"588\":6,\"589\":17,\"590\":13,\"591\":7,\"592\":13,\"593\":-1,\"594\":0,\"595\":0,\"596\":0,\"597\":5,\"598\":2,\"599\":1,\"600\":0,\"601\":5,\"602\":12,\"603\":0,\"604\":0,\"605\":0,\"606\":18,\"607\":3,\"608\":4,\"609\":4,\"610\":2,\"611\":2,\"612\":1,\"613\":8,\"614\":1,\"615\":10,\"616\":2,\"617\":5,\"618\":12,\"619\":25,\"620\":0,\"621\":8,\"622\":4,\"623\":4,\"624\":30,\"625\":1,\"626\":7,\"627\":11,\"628\":3,\"629\":4,\"630\":1,\"631\":0,\"632\":1,\"633\":0,\"634\":4,\"635\":0,\"636\":0,\"637\":0,\"638\":1,\"639\":22,\"640\":15,\"641\":3,\"642\":18,\"643\":5,\"644\":13,\"645\":1,\"646\":3,\"647\":0,\"648\":22,\"649\":18,\"650\":12,\"651\":16,\"652\":15,\"653\":0,\"654\":13,\"655\":5,\"656\":6,\"657\":5,\"658\":3,\"659\":32,\"660\":0,\"661\":3,\"662\":1,\"663\":1,\"664\":18,\"665\":1,\"666\":7,\"667\":7,\"668\":0,\"669\":0,\"670\":1,\"671\":3,\"672\":2,\"673\":10,\"674\":5,\"675\":39,\"676\":0,\"677\":1,\"678\":0,\"679\":2,\"680\":0,\"681\":0,\"682\":14,\"683\":0,\"684\":9,\"685\":0,\"686\":2,\"687\":0,\"688\":8,\"689\":1,\"690\":5,\"691\":5,\"692\":1,\"693\":8,\"694\":0,\"695\":0,\"696\":6,\"697\":3,\"698\":0,\"699\":1,\"700\":3,\"701\":16,\"702\":0,\"703\":5,\"704\":1,\"705\":1,\"706\":0,\"707\":0,\"708\":1,\"709\":0,\"710\":8,\"711\":1,\"712\":0,\"713\":2,\"714\":5,\"715\":1,\"716\":3,\"717\":17,\"718\":0,\"719\":1,\"720\":0,\"721\":0,\"722\":0,\"723\":0,\"724\":0,\"725\":3,\"726\":0,\"727\":0,\"728\":2,\"729\":7,\"730\":0,\"731\":6,\"732\":0,\"733\":0,\"734\":0,\"735\":1,\"736\":11,\"737\":0,\"738\":0,\"739\":0,\"740\":1,\"741\":4,\"742\":0,\"743\":7,\"744\":3,\"745\":2,\"746\":1,\"747\":-1,\"748\":0,\"749\":1,\"750\":1,\"751\":0,\"752\":1,\"753\":3,\"754\":0,\"755\":1,\"756\":1,\"757\":1,\"758\":0,\"759\":0,\"760\":0,\"761\":0,\"762\":1,\"763\":0,\"764\":0,\"765\":0,\"766\":0,\"767\":6,\"768\":0,\"769\":2,\"770\":9,\"771\":0,\"772\":11,\"773\":1,\"774\":1,\"775\":0,\"776\":0,\"777\":0,\"778\":0,\"779\":3,\"780\":0,\"781\":1,\"782\":1,\"783\":50,\"784\":1,\"785\":0,\"786\":0,\"787\":1,\"788\":12,\"789\":0,\"790\":2,\"791\":0,\"792\":5,\"793\":1,\"794\":5,\"795\":0,\"796\":0,\"797\":0,\"798\":5,\"799\":0,\"800\":0,\"801\":0,\"802\":0,\"803\":0,\"804\":3,\"805\":0,\"806\":1,\"807\":1,\"808\":1,\"809\":0,\"810\":4,\"811\":3,\"812\":7,\"813\":1,\"814\":0,\"815\":0,\"816\":0,\"817\":0,\"818\":2,\"819\":0,\"820\":1,\"821\":1,\"822\":8,\"823\":0,\"824\":1,\"825\":0,\"826\":4,\"827\":9,\"828\":2,\"829\":0,\"830\":0,\"831\":2,\"832\":0,\"833\":2,\"834\":0,\"835\":3,\"836\":0,\"837\":0,\"838\":16,\"839\":1,\"840\":0,\"841\":16,\"842\":0,\"843\":5,\"844\":1,\"845\":12,\"846\":2,\"847\":0,\"848\":2,\"849\":1,\"850\":1,\"851\":8,\"852\":1,\"853\":0,\"854\":0,\"855\":0,\"856\":0,\"857\":1,\"858\":0,\"859\":5,\"860\":0,\"861\":1,\"862\":0,\"863\":14,\"864\":0,\"865\":4,\"866\":1,\"867\":1,\"868\":2,\"869\":7,\"870\":1,\"871\":0,\"872\":1,\"873\":40,\"874\":19,\"875\":0,\"876\":2,\"877\":0,\"878\":2,\"879\":1,\"880\":6,\"881\":7,\"882\":10,\"883\":2,\"884\":0,\"885\":0,\"886\":1,\"887\":1,\"888\":1,\"889\":5,\"890\":1,\"891\":1,\"892\":1,\"893\":0,\"894\":0,\"895\":0,\"896\":4,\"897\":0,\"898\":2,\"899\":0,\"900\":4,\"901\":4,\"902\":2,\"903\":1,\"904\":0,\"905\":0,\"906\":1,\"907\":8,\"908\":12,\"909\":1,\"910\":1,\"911\":0,\"912\":0,\"913\":0,\"914\":0,\"915\":0,\"916\":0,\"917\":1,\"918\":0,\"919\":8,\"920\":1,\"921\":5,\"922\":0,\"923\":2,\"924\":7,\"925\":1,\"926\":0,\"927\":39,\"928\":1,\"929\":6,\"930\":1,\"931\":0,\"932\":0,\"933\":2,\"934\":1,\"935\":11,\"936\":4,\"937\":2,\"938\":0,\"939\":0,\"940\":0,\"941\":0,\"942\":0,\"943\":6,\"944\":0,\"945\":0,\"946\":4,\"947\":1,\"948\":1,\"949\":0,\"950\":7,\"951\":3,\"952\":0,\"953\":1,\"954\":0,\"955\":0,\"956\":0,\"957\":2,\"958\":4,\"959\":0,\"960\":0,\"961\":0,\"962\":0,\"963\":1,\"964\":0,\"965\":0,\"966\":1,\"967\":0,\"968\":0,\"969\":6,\"970\":11,\"971\":1,\"972\":0,\"973\":0,\"974\":2,\"975\":0,\"976\":5,\"977\":0,\"978\":0,\"979\":9,\"980\":3,\"981\":1,\"982\":0,\"983\":0,\"984\":1,\"985\":3,\"986\":3,\"987\":0,\"988\":0,\"989\":2,\"990\":0,\"991\":5,\"992\":3,\"993\":0,\"994\":0,\"995\":3,\"996\":0,\"997\":0,\"998\":0,\"999\":3,\"1000\":0,\"1001\":0,\"1002\":4,\"1003\":0,\"1004\":3,\"1005\":0,\"1006\":1,\"1007\":36,\"1008\":2,\"1009\":6,\"1010\":9,\"1011\":6,\"1012\":0,\"1013\":1,\"1014\":4,\"1015\":0,\"1016\":1,\"1017\":1,\"1018\":1,\"1019\":0,\"1020\":1,\"1021\":29,\"1022\":1,\"1023\":0,\"1024\":3,\"1025\":7,\"1026\":0,\"1027\":1,\"1028\":2,\"1029\":0,\"1030\":0,\"1031\":0,\"1032\":2,\"1033\":1,\"1034\":0,\"1035\":-1,\"1036\":1,\"1037\":0,\"1038\":6,\"1039\":2,\"1040\":0,\"1041\":3,\"1042\":1,\"1043\":1,\"1044\":2,\"1045\":5,\"1046\":2,\"1047\":10,\"1048\":1,\"1049\":1,\"1050\":0,\"1051\":1,\"1052\":1,\"1053\":9,\"1054\":27,\"1055\":2,\"1056\":0,\"1057\":0,\"1058\":0,\"1059\":0,\"1060\":5,\"1061\":3,\"1062\":5,\"1063\":1,\"1064\":1,\"1065\":0,\"1066\":1,\"1067\":1,\"1068\":2,\"1069\":12,\"1070\":0,\"1071\":2,\"1072\":3,\"1073\":9,\"1074\":2,\"1075\":2,\"1076\":7,\"1077\":4,\"1078\":2,\"1079\":7,\"1080\":3,\"1081\":3,\"1082\":0,\"1083\":2,\"1084\":0,\"1085\":4,\"1086\":3,\"1087\":7,\"1088\":2,\"1089\":0,\"1090\":0,\"1091\":2,\"1092\":0,\"1093\":0,\"1094\":3,\"1095\":3,\"1096\":5,\"1097\":5,\"1098\":20,\"1099\":0,\"1100\":0,\"1101\":3,\"1102\":10,\"1103\":1,\"1104\":8,\"1105\":0,\"1106\":0,\"1107\":1,\"1108\":0,\"1109\":0,\"1110\":1,\"1111\":8,\"1112\":22,\"1113\":5,\"1114\":5,\"1115\":1,\"1116\":2,\"1117\":3,\"1118\":0,\"1119\":4,\"1120\":1,\"1121\":14,\"1122\":0,\"1123\":3,\"1124\":9,\"1125\":28,\"1126\":2,\"1127\":10,\"1128\":7,\"1129\":1,\"1130\":0,\"1131\":0,\"1132\":0,\"1133\":0,\"1134\":6,\"1135\":0,\"1136\":1,\"1137\":6,\"1138\":2,\"1139\":1,\"1140\":0,\"1141\":1,\"1142\":0,\"1143\":0,\"1144\":4,\"1145\":1,\"1146\":0,\"1147\":0,\"1148\":0,\"1149\":0,\"1150\":19,\"1151\":1,\"1152\":1,\"1153\":22,\"1154\":0,\"1155\":1,\"1156\":7,\"1157\":0,\"1158\":0,\"1159\":0,\"1160\":1,\"1161\":6,\"1162\":2,\"1163\":10,\"1164\":9,\"1165\":4,\"1166\":1,\"1167\":17,\"1168\":2,\"1169\":0,\"1170\":0,\"1171\":4,\"1172\":2,\"1173\":2,\"1174\":0,\"1175\":0,\"1176\":1,\"1177\":11,\"1178\":0,\"1179\":1,\"1180\":4,\"1181\":2,\"1182\":0,\"1183\":5,\"1184\":0,\"1185\":11,\"1186\":0,\"1187\":2,\"1188\":0,\"1189\":0,\"1190\":0,\"1191\":1,\"1192\":5,\"1193\":1,\"1194\":2,\"1195\":0,\"1196\":20,\"1197\":8,\"1198\":3,\"1199\":12,\"1200\":5,\"1201\":4,\"1202\":47,\"1203\":2,\"1204\":6,\"1205\":3,\"1206\":1,\"1207\":6,\"1208\":6,\"1209\":10,\"1210\":17,\"1211\":6,\"1212\":0,\"1213\":2,\"1214\":2,\"1215\":2,\"1216\":0,\"1217\":4,\"1218\":1,\"1219\":0,\"1220\":5,\"1221\":5,\"1222\":0,\"1223\":2,\"1224\":9,\"1225\":1,\"1226\":2,\"1227\":0,\"1228\":0,\"1229\":1,\"1230\":1,\"1231\":1,\"1232\":2,\"1233\":9,\"1234\":0,\"1235\":0,\"1236\":0,\"1237\":2,\"1238\":2,\"1239\":1,\"1240\":3,\"1241\":0,\"1242\":1,\"1243\":5,\"1244\":4,\"1245\":1,\"1246\":2,\"1247\":7,\"1248\":1,\"1249\":0,\"1250\":5,\"1251\":4,\"1252\":0,\"1253\":1,\"1254\":4,\"1255\":0,\"1256\":7,\"1257\":1,\"1258\":0,\"1259\":0,\"1260\":0,\"1261\":0,\"1262\":2,\"1263\":2,\"1264\":1,\"1265\":0,\"1266\":1,\"1267\":3,\"1268\":1,\"1269\":3,\"1270\":25,\"1271\":7,\"1272\":0,\"1273\":2,\"1274\":2,\"1275\":0,\"1276\":0,\"1277\":0,\"1278\":1,\"1279\":4,\"1280\":27,\"1281\":16,\"1282\":3,\"1283\":2,\"1284\":0,\"1285\":1,\"1286\":0,\"1287\":6,\"1288\":25,\"1289\":2,\"1290\":2,\"1291\":0,\"1292\":1,\"1293\":1,\"1294\":0,\"1295\":4,\"1296\":2,\"1297\":9,\"1298\":2,\"1299\":0,\"1300\":1,\"1301\":7,\"1302\":0,\"1303\":2,\"1304\":1,\"1305\":1,\"1306\":3,\"1307\":31,\"1308\":13,\"1309\":0,\"1310\":0,\"1311\":12,\"1312\":1,\"1313\":3,\"1314\":4,\"1315\":4,\"1316\":1,\"1317\":0,\"1318\":1,\"1319\":5,\"1320\":1,\"1321\":0,\"1322\":3,\"1323\":4,\"1324\":0,\"1325\":1,\"1326\":2,\"1327\":1,\"1328\":3,\"1329\":4,\"1330\":8,\"1331\":2,\"1332\":1,\"1333\":34,\"1334\":9,\"1335\":0,\"1336\":0,\"1337\":3,\"1338\":2,\"1339\":0,\"1340\":0,\"1341\":1,\"1342\":1,\"1343\":1,\"1344\":5,\"1345\":0,\"1346\":0,\"1347\":0,\"1348\":5,\"1349\":5,\"1350\":3,\"1351\":3,\"1352\":1,\"1353\":1,\"1354\":0,\"1355\":5,\"1356\":12,\"1357\":6,\"1358\":4,\"1359\":0,\"1360\":2,\"1361\":0,\"1362\":0,\"1363\":3,\"1364\":0,\"1365\":1,\"1366\":0,\"1367\":9,\"1368\":1,\"1369\":13,\"1370\":1,\"1371\":0,\"1372\":2,\"1373\":1,\"1374\":3,\"1375\":0,\"1376\":1,\"1377\":0,\"1378\":3,\"1379\":0,\"1380\":2,\"1381\":2,\"1382\":3,\"1383\":0,\"1384\":2,\"1385\":0}}"