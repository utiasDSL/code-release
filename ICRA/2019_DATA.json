"{\"Conference\":{\"0\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"2\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"3\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"4\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"5\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"6\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"7\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"8\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"9\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"10\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"11\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"12\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"13\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"14\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"15\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"16\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"17\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"18\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"19\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"20\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"21\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"22\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"23\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"24\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"25\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"26\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"27\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"28\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"29\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"30\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"31\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"32\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"33\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"34\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"35\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"36\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"37\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"38\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"39\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"40\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"41\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"42\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"43\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"44\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"45\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"46\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"47\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"48\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"49\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"50\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"51\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"52\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"53\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"54\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"55\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"56\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"57\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"58\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"59\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"60\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"61\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"62\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"63\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"64\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"65\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"66\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"67\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"68\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"69\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"70\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"71\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"72\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"73\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"74\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"75\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"76\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"77\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"78\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"79\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"80\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"81\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"82\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"83\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"84\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"85\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"86\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"87\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"88\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"89\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"90\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"91\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"92\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"93\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"94\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"95\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"96\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"97\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"98\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"99\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"100\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"101\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"102\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"103\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"104\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"105\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"106\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"107\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"108\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"109\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"110\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"111\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"112\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"113\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"114\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"115\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"116\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"117\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"118\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"119\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"120\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"121\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"122\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"123\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"124\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"125\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"126\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"127\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"128\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"129\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"130\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"131\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"132\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"133\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"134\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"135\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"136\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"137\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"138\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"139\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"140\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"141\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"142\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"143\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"144\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"145\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"146\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"147\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"148\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"149\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"150\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"151\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"152\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"153\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"154\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"155\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"156\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"157\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"158\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"159\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"160\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"161\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"162\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"163\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"164\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"165\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"166\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"167\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"168\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"169\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"170\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"171\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"172\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"173\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"174\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"175\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"176\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"177\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"178\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"179\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"180\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"181\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"182\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"183\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"184\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"185\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"186\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"187\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"188\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"189\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"190\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"191\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"192\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"193\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"194\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"195\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"196\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"197\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"198\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"199\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"200\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"201\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"202\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"203\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"204\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"205\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"206\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"207\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"208\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"209\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"210\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"211\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"212\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"213\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"214\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"215\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"216\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"217\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"218\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"219\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"220\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"221\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"222\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"223\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"224\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"225\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"226\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"227\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"228\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"229\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"230\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"231\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"232\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"233\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"234\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"235\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"236\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"237\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"238\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"239\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"240\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"241\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"242\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"243\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"244\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"245\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"246\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"247\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"248\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"249\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"250\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"251\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"252\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"253\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"254\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"255\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"256\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"257\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"258\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"259\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"260\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"261\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"262\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"263\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"264\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"265\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"266\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"267\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"268\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"269\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"270\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"271\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"272\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"273\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"274\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"275\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"276\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"277\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"278\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"279\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"280\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"281\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"282\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"283\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"284\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"285\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"286\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"287\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"288\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"289\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"290\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"291\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"292\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"293\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"294\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"295\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"296\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"297\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"298\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"299\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"300\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"301\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"302\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"303\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"304\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"305\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"306\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"307\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"308\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"309\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"310\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"311\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"312\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"313\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"314\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"315\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"316\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"317\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"318\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"319\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"320\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"321\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"322\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"323\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"324\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"325\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"326\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"327\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"328\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"329\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"330\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"331\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"332\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"333\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"334\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"335\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"336\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"337\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"338\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"339\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"340\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"341\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"342\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"343\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"344\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"345\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"346\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"347\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"348\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"349\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"350\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"351\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"352\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"353\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"354\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"355\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"356\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"357\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"358\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"359\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"360\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"361\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"362\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"363\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"364\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"365\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"366\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"367\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"368\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"369\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"370\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"371\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"372\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"373\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"374\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"375\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"376\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"377\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"378\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"379\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"380\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"381\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"382\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"383\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"384\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"385\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"386\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"387\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"388\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"389\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"390\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"391\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"392\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"393\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"394\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"395\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"396\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"397\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"398\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"399\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"400\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"401\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"402\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"403\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"404\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"405\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"406\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"407\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"408\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"409\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"410\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"411\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"412\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"413\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"414\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"415\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"416\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"417\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"418\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"419\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"420\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"421\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"422\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"423\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"424\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"425\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"426\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"427\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"428\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"429\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"430\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"431\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"432\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"433\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"434\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"435\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"436\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"437\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"438\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"439\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"440\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"441\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"442\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"443\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"444\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"445\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"446\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"447\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"448\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"449\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"450\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"451\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"452\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"453\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"454\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"455\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"456\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"457\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"458\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"459\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"460\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"461\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"462\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"463\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"464\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"465\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"466\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"467\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"468\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"469\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"470\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"471\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"472\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"473\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"474\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"475\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"476\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"477\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"478\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"479\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"480\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"481\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"482\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"483\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"484\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"485\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"486\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"487\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"488\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"489\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"490\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"491\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"492\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"493\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"494\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"495\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"496\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"497\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"498\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"499\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"500\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"501\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"502\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"503\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"504\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"505\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"506\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"507\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"508\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"509\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"510\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"511\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"512\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"513\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"514\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"515\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"516\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"517\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"518\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"519\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"520\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"521\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"522\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"523\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"524\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"525\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"526\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"527\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"528\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"529\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"530\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"531\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"532\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"533\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"534\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"535\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"536\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"537\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"538\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"539\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"540\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"541\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"542\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"543\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"544\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"545\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"546\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"547\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"548\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"549\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"550\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"551\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"552\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"553\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"554\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"555\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"556\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"557\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"558\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"559\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"560\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"561\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"562\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"563\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"564\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"565\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"566\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"567\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"568\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"569\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"570\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"571\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"572\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"573\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"574\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"575\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"576\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"577\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"578\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"579\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"580\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"581\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"582\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"583\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"584\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"585\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"586\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"587\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"588\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"589\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"590\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"591\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"592\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"593\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"594\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"595\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"596\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"597\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"598\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"599\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"600\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"601\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"602\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"603\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"604\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"605\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"606\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"607\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"608\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"609\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"610\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"611\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"612\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"613\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"614\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"615\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"616\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"617\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"618\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"619\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"620\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"621\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"622\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"623\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"624\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"625\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"626\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"627\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"628\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"629\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"630\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"631\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"632\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"633\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"634\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"635\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"636\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"637\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"638\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"639\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"640\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"641\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"642\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"643\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"644\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"645\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"646\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"647\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"648\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"649\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"650\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"651\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"652\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"653\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"654\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"655\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"656\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"657\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"658\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"659\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"660\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"661\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"662\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"663\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"664\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"665\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"666\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"667\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"668\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"669\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"670\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"671\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"672\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"673\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"674\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"675\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"676\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"677\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"678\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"679\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"680\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"681\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"682\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"683\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"684\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"685\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"686\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"687\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"688\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"689\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"690\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"691\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"692\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"693\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"694\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"695\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"696\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"697\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"698\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"699\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"700\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"701\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"702\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"703\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"704\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"705\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"706\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"707\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"708\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"709\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"710\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"711\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"712\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"713\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"714\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"715\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"716\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"717\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"718\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"719\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"720\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"721\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"722\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"723\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"724\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"725\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"726\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"727\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"728\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"729\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"730\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"731\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"732\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"733\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"734\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"735\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"736\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"737\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"738\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"739\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"740\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"741\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"742\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"743\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"744\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"745\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"746\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"747\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"748\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"749\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"750\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"751\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"752\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"753\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"754\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"755\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"756\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"757\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"758\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"759\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"760\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"761\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"762\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"763\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"764\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"765\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"766\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"767\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"768\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"769\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"770\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"771\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"772\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"773\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"774\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"775\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"776\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"777\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"778\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"779\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"780\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"781\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"782\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"783\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"784\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"785\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"786\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"787\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"788\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"789\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"790\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"791\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"792\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"793\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"794\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"795\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"796\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"797\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"798\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"799\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"800\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"801\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"802\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"803\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"804\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"805\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"806\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"807\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"808\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"809\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"810\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"811\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"812\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"813\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"814\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"815\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"816\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"817\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"818\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"819\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"820\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"821\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"822\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"823\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"824\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"825\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"826\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"827\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"828\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"829\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"830\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"831\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"832\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"833\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"834\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"835\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"836\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"837\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"838\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"839\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"840\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"841\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"842\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"843\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"844\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"845\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"846\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"847\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"848\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"849\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"850\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"851\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"852\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"853\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"854\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"855\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"856\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"857\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"858\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"859\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"860\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"861\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"862\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"863\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"864\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"865\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"866\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"867\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"868\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"869\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"870\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"871\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"872\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"873\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"874\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"875\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"876\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"877\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"878\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"879\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"880\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"881\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"882\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"883\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"884\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"885\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"886\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"887\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"888\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"889\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"890\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"891\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"892\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"893\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"894\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"895\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"896\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"897\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"898\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"899\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"900\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"901\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"902\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"903\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"904\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"905\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"906\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"907\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"908\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"909\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"910\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"911\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"912\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"913\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"914\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"915\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"916\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"917\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"918\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"919\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"920\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"921\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"922\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"923\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"924\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"925\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"926\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"927\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"928\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"929\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"930\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"931\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"932\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"933\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"934\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"935\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"936\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"937\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"938\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"939\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"940\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"941\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"942\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"943\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"944\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"945\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"946\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"947\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"948\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"949\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"950\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"951\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"952\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"953\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"954\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"955\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"956\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"957\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"958\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"959\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"960\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"961\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"962\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"963\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"964\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"965\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"966\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"967\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"968\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"969\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"970\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"971\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"972\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"973\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"974\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"975\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"976\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"977\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"978\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"979\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"980\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"981\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"982\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"983\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"984\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"985\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"986\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"987\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"988\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"989\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"990\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"991\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"992\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"993\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"994\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"995\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"996\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"997\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"998\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"999\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1000\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1001\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1002\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1003\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1004\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1005\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1006\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1007\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1008\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1009\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1010\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1011\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1012\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1013\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1014\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1015\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1016\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1017\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1018\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1019\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1020\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"1021\":\"2019 International Conference on Robotics and Automation (ICRA)\"},\"Year\":{\"0\":\"Date of Conference: 20-24 May 2019\",\"1\":\"Date of Conference: 20-24 May 2019\",\"2\":\"Date of Conference: 20-24 May 2019\",\"3\":\"Date of Conference: 20-24 May 2019\",\"4\":\"Date of Conference: 20-24 May 2019\",\"5\":\"Date of Conference: 20-24 May 2019\",\"6\":\"Date of Conference: 20-24 May 2019\",\"7\":\"Date of Conference: 20-24 May 2019\",\"8\":\"Date of Conference: 20-24 May 2019\",\"9\":\"Date of Conference: 20-24 May 2019\",\"10\":\"Date of Conference: 20-24 May 2019\",\"11\":\"Date of Conference: 20-24 May 2019\",\"12\":\"Date of Conference: 20-24 May 2019\",\"13\":\"Date of Conference: 20-24 May 2019\",\"14\":\"Date of Conference: 20-24 May 2019\",\"15\":\"Date of Conference: 20-24 May 2019\",\"16\":\"Date of Conference: 20-24 May 2019\",\"17\":\"Date of Conference: 20-24 May 2019\",\"18\":\"Date of Conference: 20-24 May 2019\",\"19\":\"Date of Conference: 20-24 May 2019\",\"20\":\"Date of Conference: 20-24 May 2019\",\"21\":\"Date of Conference: 20-24 May 2019\",\"22\":\"Date of Conference: 20-24 May 2019\",\"23\":\"Date of Conference: 20-24 May 2019\",\"24\":\"Date of Conference: 20-24 May 2019\",\"25\":\"Date of Conference: 20-24 May 2019\",\"26\":\"Date of Conference: 20-24 May 2019\",\"27\":\"Date of Conference: 20-24 May 2019\",\"28\":\"Date of Conference: 20-24 May 2019\",\"29\":\"Date of Conference: 20-24 May 2019\",\"30\":\"Date of Conference: 20-24 May 2019\",\"31\":\"Date of Conference: 20-24 May 2019\",\"32\":\"Date of Conference: 20-24 May 2019\",\"33\":\"Date of Conference: 20-24 May 2019\",\"34\":\"Date of Conference: 20-24 May 2019\",\"35\":\"Date of Conference: 20-24 May 2019\",\"36\":\"Date of Conference: 20-24 May 2019\",\"37\":\"Date of Conference: 20-24 May 2019\",\"38\":\"Date of Conference: 20-24 May 2019\",\"39\":\"Date of Conference: 20-24 May 2019\",\"40\":\"Date of Conference: 20-24 May 2019\",\"41\":\"Date of Conference: 20-24 May 2019\",\"42\":\"Date of Conference: 20-24 May 2019\",\"43\":\"Date of Conference: 20-24 May 2019\",\"44\":\"Date of Conference: 20-24 May 2019\",\"45\":\"Date of Conference: 20-24 May 2019\",\"46\":\"Date of Conference: 20-24 May 2019\",\"47\":\"Date of Conference: 20-24 May 2019\",\"48\":\"Date of Conference: 20-24 May 2019\",\"49\":\"Date of Conference: 20-24 May 2019\",\"50\":\"Date of Conference: 20-24 May 2019\",\"51\":\"Date of Conference: 20-24 May 2019\",\"52\":\"Date of Conference: 20-24 May 2019\",\"53\":\"Date of Conference: 20-24 May 2019\",\"54\":\"Date of Conference: 20-24 May 2019\",\"55\":\"Date of Conference: 20-24 May 2019\",\"56\":\"Date of Conference: 20-24 May 2019\",\"57\":\"Date of Conference: 20-24 May 2019\",\"58\":\"Date of Conference: 20-24 May 2019\",\"59\":\"Date of Conference: 20-24 May 2019\",\"60\":\"Date of Conference: 20-24 May 2019\",\"61\":\"Date of Conference: 20-24 May 2019\",\"62\":\"Date of Conference: 20-24 May 2019\",\"63\":\"Date of Conference: 20-24 May 2019\",\"64\":\"Date of Conference: 20-24 May 2019\",\"65\":\"Date of Conference: 20-24 May 2019\",\"66\":\"Date of Conference: 20-24 May 2019\",\"67\":\"Date of Conference: 20-24 May 2019\",\"68\":\"Date of Conference: 20-24 May 2019\",\"69\":\"Date of Conference: 20-24 May 2019\",\"70\":\"Date of Conference: 20-24 May 2019\",\"71\":\"Date of Conference: 20-24 May 2019\",\"72\":\"Date of Conference: 20-24 May 2019\",\"73\":\"Date of Conference: 20-24 May 2019\",\"74\":\"Date of Conference: 20-24 May 2019\",\"75\":\"Date of Conference: 20-24 May 2019\",\"76\":\"Date of Conference: 20-24 May 2019\",\"77\":\"Date of Conference: 20-24 May 2019\",\"78\":\"Date of Conference: 20-24 May 2019\",\"79\":\"Date of Conference: 20-24 May 2019\",\"80\":\"Date of Conference: 20-24 May 2019\",\"81\":\"Date of Conference: 20-24 May 2019\",\"82\":\"Date of Conference: 20-24 May 2019\",\"83\":\"Date of Conference: 20-24 May 2019\",\"84\":\"Date of Conference: 20-24 May 2019\",\"85\":\"Date of Conference: 20-24 May 2019\",\"86\":\"Date of Conference: 20-24 May 2019\",\"87\":\"Date of Conference: 20-24 May 2019\",\"88\":\"Date of Conference: 20-24 May 2019\",\"89\":\"Date of Conference: 20-24 May 2019\",\"90\":\"Date of Conference: 20-24 May 2019\",\"91\":\"Date of Conference: 20-24 May 2019\",\"92\":\"Date of Conference: 20-24 May 2019\",\"93\":\"Date of Conference: 20-24 May 2019\",\"94\":\"Date of Conference: 20-24 May 2019\",\"95\":\"Date of Conference: 20-24 May 2019\",\"96\":\"Date of Conference: 20-24 May 2019\",\"97\":\"Date of Conference: 20-24 May 2019\",\"98\":\"Date of Conference: 20-24 May 2019\",\"99\":\"Date of Conference: 20-24 May 2019\",\"100\":\"Date of Conference: 20-24 May 2019\",\"101\":\"Date of Conference: 20-24 May 2019\",\"102\":\"Date of Conference: 20-24 May 2019\",\"103\":\"Date of Conference: 20-24 May 2019\",\"104\":\"Date of Conference: 20-24 May 2019\",\"105\":\"Date of Conference: 20-24 May 2019\",\"106\":\"Date of Conference: 20-24 May 2019\",\"107\":\"Date of Conference: 20-24 May 2019\",\"108\":\"Date of Conference: 20-24 May 2019\",\"109\":\"Date of Conference: 20-24 May 2019\",\"110\":\"Date of Conference: 20-24 May 2019\",\"111\":\"Date of Conference: 20-24 May 2019\",\"112\":\"Date of Conference: 20-24 May 2019\",\"113\":\"Date of Conference: 20-24 May 2019\",\"114\":\"Date of Conference: 20-24 May 2019\",\"115\":\"Date of Conference: 20-24 May 2019\",\"116\":\"Date of Conference: 20-24 May 2019\",\"117\":\"Date of Conference: 20-24 May 2019\",\"118\":\"Date of Conference: 20-24 May 2019\",\"119\":\"Date of Conference: 20-24 May 2019\",\"120\":\"Date of Conference: 20-24 May 2019\",\"121\":\"Date of Conference: 20-24 May 2019\",\"122\":\"Date of Conference: 20-24 May 2019\",\"123\":\"Date of Conference: 20-24 May 2019\",\"124\":\"Date of Conference: 20-24 May 2019\",\"125\":\"Date of Conference: 20-24 May 2019\",\"126\":\"Date of Conference: 20-24 May 2019\",\"127\":\"Date of Conference: 20-24 May 2019\",\"128\":\"Date of Conference: 20-24 May 2019\",\"129\":\"Date of Conference: 20-24 May 2019\",\"130\":\"Date of Conference: 20-24 May 2019\",\"131\":\"Date of Conference: 20-24 May 2019\",\"132\":\"Date of Conference: 20-24 May 2019\",\"133\":\"Date of Conference: 20-24 May 2019\",\"134\":\"Date of Conference: 20-24 May 2019\",\"135\":\"Date of Conference: 20-24 May 2019\",\"136\":\"Date of Conference: 20-24 May 2019\",\"137\":\"Date of Conference: 20-24 May 2019\",\"138\":\"Date of Conference: 20-24 May 2019\",\"139\":\"Date of Conference: 20-24 May 2019\",\"140\":\"Date of Conference: 20-24 May 2019\",\"141\":\"Date of Conference: 20-24 May 2019\",\"142\":\"Date of Conference: 20-24 May 2019\",\"143\":\"Date of Conference: 20-24 May 2019\",\"144\":\"Date of Conference: 20-24 May 2019\",\"145\":\"Date of Conference: 20-24 May 2019\",\"146\":\"Date of Conference: 20-24 May 2019\",\"147\":\"Date of Conference: 20-24 May 2019\",\"148\":\"Date of Conference: 20-24 May 2019\",\"149\":\"Date of Conference: 20-24 May 2019\",\"150\":\"Date of Conference: 20-24 May 2019\",\"151\":\"Date of Conference: 20-24 May 2019\",\"152\":\"Date of Conference: 20-24 May 2019\",\"153\":\"Date of Conference: 20-24 May 2019\",\"154\":\"Date of Conference: 20-24 May 2019\",\"155\":\"Date of Conference: 20-24 May 2019\",\"156\":\"Date of Conference: 20-24 May 2019\",\"157\":\"Date of Conference: 20-24 May 2019\",\"158\":\"Date of Conference: 20-24 May 2019\",\"159\":\"Date of Conference: 20-24 May 2019\",\"160\":\"Date of Conference: 20-24 May 2019\",\"161\":\"Date of Conference: 20-24 May 2019\",\"162\":\"Date of Conference: 20-24 May 2019\",\"163\":\"Date of Conference: 20-24 May 2019\",\"164\":\"Date of Conference: 20-24 May 2019\",\"165\":\"Date of Conference: 20-24 May 2019\",\"166\":\"Date of Conference: 20-24 May 2019\",\"167\":\"Date of Conference: 20-24 May 2019\",\"168\":\"Date of Conference: 20-24 May 2019\",\"169\":\"Date of Conference: 20-24 May 2019\",\"170\":\"Date of Conference: 20-24 May 2019\",\"171\":\"Date of Conference: 20-24 May 2019\",\"172\":\"Date of Conference: 20-24 May 2019\",\"173\":\"Date of Conference: 20-24 May 2019\",\"174\":\"Date of Conference: 20-24 May 2019\",\"175\":\"Date of Conference: 20-24 May 2019\",\"176\":\"Date of Conference: 20-24 May 2019\",\"177\":\"Date of Conference: 20-24 May 2019\",\"178\":\"Date of Conference: 20-24 May 2019\",\"179\":\"Date of Conference: 20-24 May 2019\",\"180\":\"Date of Conference: 20-24 May 2019\",\"181\":\"Date of Conference: 20-24 May 2019\",\"182\":\"Date of Conference: 20-24 May 2019\",\"183\":\"Date of Conference: 20-24 May 2019\",\"184\":\"Date of Conference: 20-24 May 2019\",\"185\":\"Date of Conference: 20-24 May 2019\",\"186\":\"Date of Conference: 20-24 May 2019\",\"187\":\"Date of Conference: 20-24 May 2019\",\"188\":\"Date of Conference: 20-24 May 2019\",\"189\":\"Date of Conference: 20-24 May 2019\",\"190\":\"Date of Conference: 20-24 May 2019\",\"191\":\"Date of Conference: 20-24 May 2019\",\"192\":\"Date of Conference: 20-24 May 2019\",\"193\":\"Date of Conference: 20-24 May 2019\",\"194\":\"Date of Conference: 20-24 May 2019\",\"195\":\"Date of Conference: 20-24 May 2019\",\"196\":\"Date of Conference: 20-24 May 2019\",\"197\":\"Date of Conference: 20-24 May 2019\",\"198\":\"Date of Conference: 20-24 May 2019\",\"199\":\"Date of Conference: 20-24 May 2019\",\"200\":\"Date of Conference: 20-24 May 2019\",\"201\":\"Date of Conference: 20-24 May 2019\",\"202\":\"Date of Conference: 20-24 May 2019\",\"203\":\"Date of Conference: 20-24 May 2019\",\"204\":\"Date of Conference: 20-24 May 2019\",\"205\":\"Date of Conference: 20-24 May 2019\",\"206\":\"Date of Conference: 20-24 May 2019\",\"207\":\"Date of Conference: 20-24 May 2019\",\"208\":\"Date of Conference: 20-24 May 2019\",\"209\":\"Date of Conference: 20-24 May 2019\",\"210\":\"Date of Conference: 20-24 May 2019\",\"211\":\"Date of Conference: 20-24 May 2019\",\"212\":\"Date of Conference: 20-24 May 2019\",\"213\":\"Date of Conference: 20-24 May 2019\",\"214\":\"Date of Conference: 20-24 May 2019\",\"215\":\"Date of Conference: 20-24 May 2019\",\"216\":\"Date of Conference: 20-24 May 2019\",\"217\":\"Date of Conference: 20-24 May 2019\",\"218\":\"Date of Conference: 20-24 May 2019\",\"219\":\"Date of Conference: 20-24 May 2019\",\"220\":\"Date of Conference: 20-24 May 2019\",\"221\":\"Date of Conference: 20-24 May 2019\",\"222\":\"Date of Conference: 20-24 May 2019\",\"223\":\"Date of Conference: 20-24 May 2019\",\"224\":\"Date of Conference: 20-24 May 2019\",\"225\":\"Date of Conference: 20-24 May 2019\",\"226\":\"Date of Conference: 20-24 May 2019\",\"227\":\"Date of Conference: 20-24 May 2019\",\"228\":\"Date of Conference: 20-24 May 2019\",\"229\":\"Date of Conference: 20-24 May 2019\",\"230\":\"Date of Conference: 20-24 May 2019\",\"231\":\"Date of Conference: 20-24 May 2019\",\"232\":\"Date of Conference: 20-24 May 2019\",\"233\":\"Date of Conference: 20-24 May 2019\",\"234\":\"Date of Conference: 20-24 May 2019\",\"235\":\"Date of Conference: 20-24 May 2019\",\"236\":\"Date of Conference: 20-24 May 2019\",\"237\":\"Date of Conference: 20-24 May 2019\",\"238\":\"Date of Conference: 20-24 May 2019\",\"239\":\"Date of Conference: 20-24 May 2019\",\"240\":\"Date of Conference: 20-24 May 2019\",\"241\":\"Date of Conference: 20-24 May 2019\",\"242\":\"Date of Conference: 20-24 May 2019\",\"243\":\"Date of Conference: 20-24 May 2019\",\"244\":\"Date of Conference: 20-24 May 2019\",\"245\":\"Date of Conference: 20-24 May 2019\",\"246\":\"Date of Conference: 20-24 May 2019\",\"247\":\"Date of Conference: 20-24 May 2019\",\"248\":\"Date of Conference: 20-24 May 2019\",\"249\":\"Date of Conference: 20-24 May 2019\",\"250\":\"Date of Conference: 20-24 May 2019\",\"251\":\"Date of Conference: 20-24 May 2019\",\"252\":\"Date of Conference: 20-24 May 2019\",\"253\":\"Date of Conference: 20-24 May 2019\",\"254\":\"Date of Conference: 20-24 May 2019\",\"255\":\"Date of Conference: 20-24 May 2019\",\"256\":\"Date of Conference: 20-24 May 2019\",\"257\":\"Date of Conference: 20-24 May 2019\",\"258\":\"Date of Conference: 20-24 May 2019\",\"259\":\"Date of Conference: 20-24 May 2019\",\"260\":\"Date of Conference: 20-24 May 2019\",\"261\":\"Date of Conference: 20-24 May 2019\",\"262\":\"Date of Conference: 20-24 May 2019\",\"263\":\"Date of Conference: 20-24 May 2019\",\"264\":\"Date of Conference: 20-24 May 2019\",\"265\":\"Date of Conference: 20-24 May 2019\",\"266\":\"Date of Conference: 20-24 May 2019\",\"267\":\"Date of Conference: 20-24 May 2019\",\"268\":\"Date of Conference: 20-24 May 2019\",\"269\":\"Date of Conference: 20-24 May 2019\",\"270\":\"Date of Conference: 20-24 May 2019\",\"271\":\"Date of Conference: 20-24 May 2019\",\"272\":\"Date of Conference: 20-24 May 2019\",\"273\":\"Date of Conference: 20-24 May 2019\",\"274\":\"Date of Conference: 20-24 May 2019\",\"275\":\"Date of Conference: 20-24 May 2019\",\"276\":\"Date of Conference: 20-24 May 2019\",\"277\":\"Date of Conference: 20-24 May 2019\",\"278\":\"Date of Conference: 20-24 May 2019\",\"279\":\"Date of Conference: 20-24 May 2019\",\"280\":\"Date of Conference: 20-24 May 2019\",\"281\":\"Date of Conference: 20-24 May 2019\",\"282\":\"Date of Conference: 20-24 May 2019\",\"283\":\"Date of Conference: 20-24 May 2019\",\"284\":\"Date of Conference: 20-24 May 2019\",\"285\":\"Date of Conference: 20-24 May 2019\",\"286\":\"Date of Conference: 20-24 May 2019\",\"287\":\"Date of Conference: 20-24 May 2019\",\"288\":\"Date of Conference: 20-24 May 2019\",\"289\":\"Date of Conference: 20-24 May 2019\",\"290\":\"Date of Conference: 20-24 May 2019\",\"291\":\"Date of Conference: 20-24 May 2019\",\"292\":\"Date of Conference: 20-24 May 2019\",\"293\":\"Date of Conference: 20-24 May 2019\",\"294\":\"Date of Conference: 20-24 May 2019\",\"295\":\"Date of Conference: 20-24 May 2019\",\"296\":\"Date of Conference: 20-24 May 2019\",\"297\":\"Date of Conference: 20-24 May 2019\",\"298\":\"Date of Conference: 20-24 May 2019\",\"299\":\"Date of Conference: 20-24 May 2019\",\"300\":\"Date of Conference: 20-24 May 2019\",\"301\":\"Date of Conference: 20-24 May 2019\",\"302\":\"Date of Conference: 20-24 May 2019\",\"303\":\"Date of Conference: 20-24 May 2019\",\"304\":\"Date of Conference: 20-24 May 2019\",\"305\":\"Date of Conference: 20-24 May 2019\",\"306\":\"Date of Conference: 20-24 May 2019\",\"307\":\"Date of Conference: 20-24 May 2019\",\"308\":\"Date of Conference: 20-24 May 2019\",\"309\":\"Date of Conference: 20-24 May 2019\",\"310\":\"Date of Conference: 20-24 May 2019\",\"311\":\"Date of Conference: 20-24 May 2019\",\"312\":\"Date of Conference: 20-24 May 2019\",\"313\":\"Date of Conference: 20-24 May 2019\",\"314\":\"Date of Conference: 20-24 May 2019\",\"315\":\"Date of Conference: 20-24 May 2019\",\"316\":\"Date of Conference: 20-24 May 2019\",\"317\":\"Date of Conference: 20-24 May 2019\",\"318\":\"Date of Conference: 20-24 May 2019\",\"319\":\"Date of Conference: 20-24 May 2019\",\"320\":\"Date of Conference: 20-24 May 2019\",\"321\":\"Date of Conference: 20-24 May 2019\",\"322\":\"Date of Conference: 20-24 May 2019\",\"323\":\"Date of Conference: 20-24 May 2019\",\"324\":\"Date of Conference: 20-24 May 2019\",\"325\":\"Date of Conference: 20-24 May 2019\",\"326\":\"Date of Conference: 20-24 May 2019\",\"327\":\"Date of Conference: 20-24 May 2019\",\"328\":\"Date of Conference: 20-24 May 2019\",\"329\":\"Date of Conference: 20-24 May 2019\",\"330\":\"Date of Conference: 20-24 May 2019\",\"331\":\"Date of Conference: 20-24 May 2019\",\"332\":\"Date of Conference: 20-24 May 2019\",\"333\":\"Date of Conference: 20-24 May 2019\",\"334\":\"Date of Conference: 20-24 May 2019\",\"335\":\"Date of Conference: 20-24 May 2019\",\"336\":\"Date of Conference: 20-24 May 2019\",\"337\":\"Date of Conference: 20-24 May 2019\",\"338\":\"Date of Conference: 20-24 May 2019\",\"339\":\"Date of Conference: 20-24 May 2019\",\"340\":\"Date of Conference: 20-24 May 2019\",\"341\":\"Date of Conference: 20-24 May 2019\",\"342\":\"Date of Conference: 20-24 May 2019\",\"343\":\"Date of Conference: 20-24 May 2019\",\"344\":\"Date of Conference: 20-24 May 2019\",\"345\":\"Date of Conference: 20-24 May 2019\",\"346\":\"Date of Conference: 20-24 May 2019\",\"347\":\"Date of Conference: 20-24 May 2019\",\"348\":\"Date of Conference: 20-24 May 2019\",\"349\":\"Date of Conference: 20-24 May 2019\",\"350\":\"Date of Conference: 20-24 May 2019\",\"351\":\"Date of Conference: 20-24 May 2019\",\"352\":\"Date of Conference: 20-24 May 2019\",\"353\":\"Date of Conference: 20-24 May 2019\",\"354\":\"Date of Conference: 20-24 May 2019\",\"355\":\"Date of Conference: 20-24 May 2019\",\"356\":\"Date of Conference: 20-24 May 2019\",\"357\":\"Date of Conference: 20-24 May 2019\",\"358\":\"Date of Conference: 20-24 May 2019\",\"359\":\"Date of Conference: 20-24 May 2019\",\"360\":\"Date of Conference: 20-24 May 2019\",\"361\":\"Date of Conference: 20-24 May 2019\",\"362\":\"Date of Conference: 20-24 May 2019\",\"363\":\"Date of Conference: 20-24 May 2019\",\"364\":\"Date of Conference: 20-24 May 2019\",\"365\":\"Date of Conference: 20-24 May 2019\",\"366\":\"Date of Conference: 20-24 May 2019\",\"367\":\"Date of Conference: 20-24 May 2019\",\"368\":\"Date of Conference: 20-24 May 2019\",\"369\":\"Date of Conference: 20-24 May 2019\",\"370\":\"Date of Conference: 20-24 May 2019\",\"371\":\"Date of Conference: 20-24 May 2019\",\"372\":\"Date of Conference: 20-24 May 2019\",\"373\":\"Date of Conference: 20-24 May 2019\",\"374\":\"Date of Conference: 20-24 May 2019\",\"375\":\"Date of Conference: 20-24 May 2019\",\"376\":\"Date of Conference: 20-24 May 2019\",\"377\":\"Date of Conference: 20-24 May 2019\",\"378\":\"Date of Conference: 20-24 May 2019\",\"379\":\"Date of Conference: 20-24 May 2019\",\"380\":\"Date of Conference: 20-24 May 2019\",\"381\":\"Date of Conference: 20-24 May 2019\",\"382\":\"Date of Conference: 20-24 May 2019\",\"383\":\"Date of Conference: 20-24 May 2019\",\"384\":\"Date of Conference: 20-24 May 2019\",\"385\":\"Date of Conference: 20-24 May 2019\",\"386\":\"Date of Conference: 20-24 May 2019\",\"387\":\"Date of Conference: 20-24 May 2019\",\"388\":\"Date of Conference: 20-24 May 2019\",\"389\":\"Date of Conference: 20-24 May 2019\",\"390\":\"Date of Conference: 20-24 May 2019\",\"391\":\"Date of Conference: 20-24 May 2019\",\"392\":\"Date of Conference: 20-24 May 2019\",\"393\":\"Date of Conference: 20-24 May 2019\",\"394\":\"Date of Conference: 20-24 May 2019\",\"395\":\"Date of Conference: 20-24 May 2019\",\"396\":\"Date of Conference: 20-24 May 2019\",\"397\":\"Date of Conference: 20-24 May 2019\",\"398\":\"Date of Conference: 20-24 May 2019\",\"399\":\"Date of Conference: 20-24 May 2019\",\"400\":\"Date of Conference: 20-24 May 2019\",\"401\":\"Date of Conference: 20-24 May 2019\",\"402\":\"Date of Conference: 20-24 May 2019\",\"403\":\"Date of Conference: 20-24 May 2019\",\"404\":\"Date of Conference: 20-24 May 2019\",\"405\":\"Date of Conference: 20-24 May 2019\",\"406\":\"Date of Conference: 20-24 May 2019\",\"407\":\"Date of Conference: 20-24 May 2019\",\"408\":\"Date of Conference: 20-24 May 2019\",\"409\":\"Date of Conference: 20-24 May 2019\",\"410\":\"Date of Conference: 20-24 May 2019\",\"411\":\"Date of Conference: 20-24 May 2019\",\"412\":\"Date of Conference: 20-24 May 2019\",\"413\":\"Date of Conference: 20-24 May 2019\",\"414\":\"Date of Conference: 20-24 May 2019\",\"415\":\"Date of Conference: 20-24 May 2019\",\"416\":\"Date of Conference: 20-24 May 2019\",\"417\":\"Date of Conference: 20-24 May 2019\",\"418\":\"Date of Conference: 20-24 May 2019\",\"419\":\"Date of Conference: 20-24 May 2019\",\"420\":\"Date of Conference: 20-24 May 2019\",\"421\":\"Date of Conference: 20-24 May 2019\",\"422\":\"Date of Conference: 20-24 May 2019\",\"423\":\"Date of Conference: 20-24 May 2019\",\"424\":\"Date of Conference: 20-24 May 2019\",\"425\":\"Date of Conference: 20-24 May 2019\",\"426\":\"Date of Conference: 20-24 May 2019\",\"427\":\"Date of Conference: 20-24 May 2019\",\"428\":\"Date of Conference: 20-24 May 2019\",\"429\":\"Date of Conference: 20-24 May 2019\",\"430\":\"Date of Conference: 20-24 May 2019\",\"431\":\"Date of Conference: 20-24 May 2019\",\"432\":\"Date of Conference: 20-24 May 2019\",\"433\":\"Date of Conference: 20-24 May 2019\",\"434\":\"Date of Conference: 20-24 May 2019\",\"435\":\"Date of Conference: 20-24 May 2019\",\"436\":\"Date of Conference: 20-24 May 2019\",\"437\":\"Date of Conference: 20-24 May 2019\",\"438\":\"Date of Conference: 20-24 May 2019\",\"439\":\"Date of Conference: 20-24 May 2019\",\"440\":\"Date of Conference: 20-24 May 2019\",\"441\":\"Date of Conference: 20-24 May 2019\",\"442\":\"Date of Conference: 20-24 May 2019\",\"443\":\"Date of Conference: 20-24 May 2019\",\"444\":\"Date of Conference: 20-24 May 2019\",\"445\":\"Date of Conference: 20-24 May 2019\",\"446\":\"Date of Conference: 20-24 May 2019\",\"447\":\"Date of Conference: 20-24 May 2019\",\"448\":\"Date of Conference: 20-24 May 2019\",\"449\":\"Date of Conference: 20-24 May 2019\",\"450\":\"Date of Conference: 20-24 May 2019\",\"451\":\"Date of Conference: 20-24 May 2019\",\"452\":\"Date of Conference: 20-24 May 2019\",\"453\":\"Date of Conference: 20-24 May 2019\",\"454\":\"Date of Conference: 20-24 May 2019\",\"455\":\"Date of Conference: 20-24 May 2019\",\"456\":\"Date of Conference: 20-24 May 2019\",\"457\":\"Date of Conference: 20-24 May 2019\",\"458\":\"Date of Conference: 20-24 May 2019\",\"459\":\"Date of Conference: 20-24 May 2019\",\"460\":\"Date of Conference: 20-24 May 2019\",\"461\":\"Date of Conference: 20-24 May 2019\",\"462\":\"Date of Conference: 20-24 May 2019\",\"463\":\"Date of Conference: 20-24 May 2019\",\"464\":\"Date of Conference: 20-24 May 2019\",\"465\":\"Date of Conference: 20-24 May 2019\",\"466\":\"Date of Conference: 20-24 May 2019\",\"467\":\"Date of Conference: 20-24 May 2019\",\"468\":\"Date of Conference: 20-24 May 2019\",\"469\":\"Date of Conference: 20-24 May 2019\",\"470\":\"Date of Conference: 20-24 May 2019\",\"471\":\"Date of Conference: 20-24 May 2019\",\"472\":\"Date of Conference: 20-24 May 2019\",\"473\":\"Date of Conference: 20-24 May 2019\",\"474\":\"Date of Conference: 20-24 May 2019\",\"475\":\"Date of Conference: 20-24 May 2019\",\"476\":\"Date of Conference: 20-24 May 2019\",\"477\":\"Date of Conference: 20-24 May 2019\",\"478\":\"Date of Conference: 20-24 May 2019\",\"479\":\"Date of Conference: 20-24 May 2019\",\"480\":\"Date of Conference: 20-24 May 2019\",\"481\":\"Date of Conference: 20-24 May 2019\",\"482\":\"Date of Conference: 20-24 May 2019\",\"483\":\"Date of Conference: 20-24 May 2019\",\"484\":\"Date of Conference: 20-24 May 2019\",\"485\":\"Date of Conference: 20-24 May 2019\",\"486\":\"Date of Conference: 20-24 May 2019\",\"487\":\"Date of Conference: 20-24 May 2019\",\"488\":\"Date of Conference: 20-24 May 2019\",\"489\":\"Date of Conference: 20-24 May 2019\",\"490\":\"Date of Conference: 20-24 May 2019\",\"491\":\"Date of Conference: 20-24 May 2019\",\"492\":\"Date of Conference: 20-24 May 2019\",\"493\":\"Date of Conference: 20-24 May 2019\",\"494\":\"Date of Conference: 20-24 May 2019\",\"495\":\"Date of Conference: 20-24 May 2019\",\"496\":\"Date of Conference: 20-24 May 2019\",\"497\":\"Date of Conference: 20-24 May 2019\",\"498\":\"Date of Conference: 20-24 May 2019\",\"499\":\"Date of Conference: 20-24 May 2019\",\"500\":\"Date of Conference: 20-24 May 2019\",\"501\":\"Date of Conference: 20-24 May 2019\",\"502\":\"Date of Conference: 20-24 May 2019\",\"503\":\"Date of Conference: 20-24 May 2019\",\"504\":\"Date of Conference: 20-24 May 2019\",\"505\":\"Date of Conference: 20-24 May 2019\",\"506\":\"Date of Conference: 20-24 May 2019\",\"507\":\"Date of Conference: 20-24 May 2019\",\"508\":\"Date of Conference: 20-24 May 2019\",\"509\":\"Date of Conference: 20-24 May 2019\",\"510\":\"Date of Conference: 20-24 May 2019\",\"511\":\"Date of Conference: 20-24 May 2019\",\"512\":\"Date of Conference: 20-24 May 2019\",\"513\":\"Date of Conference: 20-24 May 2019\",\"514\":\"Date of Conference: 20-24 May 2019\",\"515\":\"Date of Conference: 20-24 May 2019\",\"516\":\"Date of Conference: 20-24 May 2019\",\"517\":\"Date of Conference: 20-24 May 2019\",\"518\":\"Date of Conference: 20-24 May 2019\",\"519\":\"Date of Conference: 20-24 May 2019\",\"520\":\"Date of Conference: 20-24 May 2019\",\"521\":\"Date of Conference: 20-24 May 2019\",\"522\":\"Date of Conference: 20-24 May 2019\",\"523\":\"Date of Conference: 20-24 May 2019\",\"524\":\"Date of Conference: 20-24 May 2019\",\"525\":\"Date of Conference: 20-24 May 2019\",\"526\":\"Date of Conference: 20-24 May 2019\",\"527\":\"Date of Conference: 20-24 May 2019\",\"528\":\"Date of Conference: 20-24 May 2019\",\"529\":\"Date of Conference: 20-24 May 2019\",\"530\":\"Date of Conference: 20-24 May 2019\",\"531\":\"Date of Conference: 20-24 May 2019\",\"532\":\"Date of Conference: 20-24 May 2019\",\"533\":\"Date of Conference: 20-24 May 2019\",\"534\":\"Date of Conference: 20-24 May 2019\",\"535\":\"Date of Conference: 20-24 May 2019\",\"536\":\"Date of Conference: 20-24 May 2019\",\"537\":\"Date of Conference: 20-24 May 2019\",\"538\":\"Date of Conference: 20-24 May 2019\",\"539\":\"Date of Conference: 20-24 May 2019\",\"540\":\"Date of Conference: 20-24 May 2019\",\"541\":\"Date of Conference: 20-24 May 2019\",\"542\":\"Date of Conference: 20-24 May 2019\",\"543\":\"Date of Conference: 20-24 May 2019\",\"544\":\"Date of Conference: 20-24 May 2019\",\"545\":\"Date of Conference: 20-24 May 2019\",\"546\":\"Date of Conference: 20-24 May 2019\",\"547\":\"Date of Conference: 20-24 May 2019\",\"548\":\"Date of Conference: 20-24 May 2019\",\"549\":\"Date of Conference: 20-24 May 2019\",\"550\":\"Date of Conference: 20-24 May 2019\",\"551\":\"Date of Conference: 20-24 May 2019\",\"552\":\"Date of Conference: 20-24 May 2019\",\"553\":\"Date of Conference: 20-24 May 2019\",\"554\":\"Date of Conference: 20-24 May 2019\",\"555\":\"Date of Conference: 20-24 May 2019\",\"556\":\"Date of Conference: 20-24 May 2019\",\"557\":\"Date of Conference: 20-24 May 2019\",\"558\":\"Date of Conference: 20-24 May 2019\",\"559\":\"Date of Conference: 20-24 May 2019\",\"560\":\"Date of Conference: 20-24 May 2019\",\"561\":\"Date of Conference: 20-24 May 2019\",\"562\":\"Date of Conference: 20-24 May 2019\",\"563\":\"Date of Conference: 20-24 May 2019\",\"564\":\"Date of Conference: 20-24 May 2019\",\"565\":\"Date of Conference: 20-24 May 2019\",\"566\":\"Date of Conference: 20-24 May 2019\",\"567\":\"Date of Conference: 20-24 May 2019\",\"568\":\"Date of Conference: 20-24 May 2019\",\"569\":\"Date of Conference: 20-24 May 2019\",\"570\":\"Date of Conference: 20-24 May 2019\",\"571\":\"Date of Conference: 20-24 May 2019\",\"572\":\"Date of Conference: 20-24 May 2019\",\"573\":\"Date of Conference: 20-24 May 2019\",\"574\":\"Date of Conference: 20-24 May 2019\",\"575\":\"Date of Conference: 20-24 May 2019\",\"576\":\"Date of Conference: 20-24 May 2019\",\"577\":\"Date of Conference: 20-24 May 2019\",\"578\":\"Date of Conference: 20-24 May 2019\",\"579\":\"Date of Conference: 20-24 May 2019\",\"580\":\"Date of Conference: 20-24 May 2019\",\"581\":\"Date of Conference: 20-24 May 2019\",\"582\":\"Date of Conference: 20-24 May 2019\",\"583\":\"Date of Conference: 20-24 May 2019\",\"584\":\"Date of Conference: 20-24 May 2019\",\"585\":\"Date of Conference: 20-24 May 2019\",\"586\":\"Date of Conference: 20-24 May 2019\",\"587\":\"Date of Conference: 20-24 May 2019\",\"588\":\"Date of Conference: 20-24 May 2019\",\"589\":\"Date of Conference: 20-24 May 2019\",\"590\":\"Date of Conference: 20-24 May 2019\",\"591\":\"Date of Conference: 20-24 May 2019\",\"592\":\"Date of Conference: 20-24 May 2019\",\"593\":\"Date of Conference: 20-24 May 2019\",\"594\":\"Date of Conference: 20-24 May 2019\",\"595\":\"Date of Conference: 20-24 May 2019\",\"596\":\"Date of Conference: 20-24 May 2019\",\"597\":\"Date of Conference: 20-24 May 2019\",\"598\":\"Date of Conference: 20-24 May 2019\",\"599\":\"Date of Conference: 20-24 May 2019\",\"600\":\"Date of Conference: 20-24 May 2019\",\"601\":\"Date of Conference: 20-24 May 2019\",\"602\":\"Date of Conference: 20-24 May 2019\",\"603\":\"Date of Conference: 20-24 May 2019\",\"604\":\"Date of Conference: 20-24 May 2019\",\"605\":\"Date of Conference: 20-24 May 2019\",\"606\":\"Date of Conference: 20-24 May 2019\",\"607\":\"Date of Conference: 20-24 May 2019\",\"608\":\"Date of Conference: 20-24 May 2019\",\"609\":\"Date of Conference: 20-24 May 2019\",\"610\":\"Date of Conference: 20-24 May 2019\",\"611\":\"Date of Conference: 20-24 May 2019\",\"612\":\"Date of Conference: 20-24 May 2019\",\"613\":\"Date of Conference: 20-24 May 2019\",\"614\":\"Date of Conference: 20-24 May 2019\",\"615\":\"Date of Conference: 20-24 May 2019\",\"616\":\"Date of Conference: 20-24 May 2019\",\"617\":\"Date of Conference: 20-24 May 2019\",\"618\":\"Date of Conference: 20-24 May 2019\",\"619\":\"Date of Conference: 20-24 May 2019\",\"620\":\"Date of Conference: 20-24 May 2019\",\"621\":\"Date of Conference: 20-24 May 2019\",\"622\":\"Date of Conference: 20-24 May 2019\",\"623\":\"Date of Conference: 20-24 May 2019\",\"624\":\"Date of Conference: 20-24 May 2019\",\"625\":\"Date of Conference: 20-24 May 2019\",\"626\":\"Date of Conference: 20-24 May 2019\",\"627\":\"Date of Conference: 20-24 May 2019\",\"628\":\"Date of Conference: 20-24 May 2019\",\"629\":\"Date of Conference: 20-24 May 2019\",\"630\":\"Date of Conference: 20-24 May 2019\",\"631\":\"Date of Conference: 20-24 May 2019\",\"632\":\"Date of Conference: 20-24 May 2019\",\"633\":\"Date of Conference: 20-24 May 2019\",\"634\":\"Date of Conference: 20-24 May 2019\",\"635\":\"Date of Conference: 20-24 May 2019\",\"636\":\"Date of Conference: 20-24 May 2019\",\"637\":\"Date of Conference: 20-24 May 2019\",\"638\":\"Date of Conference: 20-24 May 2019\",\"639\":\"Date of Conference: 20-24 May 2019\",\"640\":\"Date of Conference: 20-24 May 2019\",\"641\":\"Date of Conference: 20-24 May 2019\",\"642\":\"Date of Conference: 20-24 May 2019\",\"643\":\"Date of Conference: 20-24 May 2019\",\"644\":\"Date of Conference: 20-24 May 2019\",\"645\":\"Date of Conference: 20-24 May 2019\",\"646\":\"Date of Conference: 20-24 May 2019\",\"647\":\"Date of Conference: 20-24 May 2019\",\"648\":\"Date of Conference: 20-24 May 2019\",\"649\":\"Date of Conference: 20-24 May 2019\",\"650\":\"Date of Conference: 20-24 May 2019\",\"651\":\"Date of Conference: 20-24 May 2019\",\"652\":\"Date of Conference: 20-24 May 2019\",\"653\":\"Date of Conference: 20-24 May 2019\",\"654\":\"Date of Conference: 20-24 May 2019\",\"655\":\"Date of Conference: 20-24 May 2019\",\"656\":\"Date of Conference: 20-24 May 2019\",\"657\":\"Date of Conference: 20-24 May 2019\",\"658\":\"Date of Conference: 20-24 May 2019\",\"659\":\"Date of Conference: 20-24 May 2019\",\"660\":\"Date of Conference: 20-24 May 2019\",\"661\":\"Date of Conference: 20-24 May 2019\",\"662\":\"Date of Conference: 20-24 May 2019\",\"663\":\"Date of Conference: 20-24 May 2019\",\"664\":\"Date of Conference: 20-24 May 2019\",\"665\":\"Date of Conference: 20-24 May 2019\",\"666\":\"Date of Conference: 20-24 May 2019\",\"667\":\"Date of Conference: 20-24 May 2019\",\"668\":\"Date of Conference: 20-24 May 2019\",\"669\":\"Date of Conference: 20-24 May 2019\",\"670\":\"Date of Conference: 20-24 May 2019\",\"671\":\"Date of Conference: 20-24 May 2019\",\"672\":\"Date of Conference: 20-24 May 2019\",\"673\":\"Date of Conference: 20-24 May 2019\",\"674\":\"Date of Conference: 20-24 May 2019\",\"675\":\"Date of Conference: 20-24 May 2019\",\"676\":\"Date of Conference: 20-24 May 2019\",\"677\":\"Date of Conference: 20-24 May 2019\",\"678\":\"Date of Conference: 20-24 May 2019\",\"679\":\"Date of Conference: 20-24 May 2019\",\"680\":\"Date of Conference: 20-24 May 2019\",\"681\":\"Date of Conference: 20-24 May 2019\",\"682\":\"Date of Conference: 20-24 May 2019\",\"683\":\"Date of Conference: 20-24 May 2019\",\"684\":\"Date of Conference: 20-24 May 2019\",\"685\":\"Date of Conference: 20-24 May 2019\",\"686\":\"Date of Conference: 20-24 May 2019\",\"687\":\"Date of Conference: 20-24 May 2019\",\"688\":\"Date of Conference: 20-24 May 2019\",\"689\":\"Date of Conference: 20-24 May 2019\",\"690\":\"Date of Conference: 20-24 May 2019\",\"691\":\"Date of Conference: 20-24 May 2019\",\"692\":\"Date of Conference: 20-24 May 2019\",\"693\":\"Date of Conference: 20-24 May 2019\",\"694\":\"Date of Conference: 20-24 May 2019\",\"695\":\"Date of Conference: 20-24 May 2019\",\"696\":\"Date of Conference: 20-24 May 2019\",\"697\":\"Date of Conference: 20-24 May 2019\",\"698\":\"Date of Conference: 20-24 May 2019\",\"699\":\"Date of Conference: 20-24 May 2019\",\"700\":\"Date of Conference: 20-24 May 2019\",\"701\":\"Date of Conference: 20-24 May 2019\",\"702\":\"Date of Conference: 20-24 May 2019\",\"703\":\"Date of Conference: 20-24 May 2019\",\"704\":\"Date of Conference: 20-24 May 2019\",\"705\":\"Date of Conference: 20-24 May 2019\",\"706\":\"Date of Conference: 20-24 May 2019\",\"707\":\"Date of Conference: 20-24 May 2019\",\"708\":\"Date of Conference: 20-24 May 2019\",\"709\":\"Date of Conference: 20-24 May 2019\",\"710\":\"Date of Conference: 20-24 May 2019\",\"711\":\"Date of Conference: 20-24 May 2019\",\"712\":\"Date of Conference: 20-24 May 2019\",\"713\":\"Date of Conference: 20-24 May 2019\",\"714\":\"Date of Conference: 20-24 May 2019\",\"715\":\"Date of Conference: 20-24 May 2019\",\"716\":\"Date of Conference: 20-24 May 2019\",\"717\":\"Date of Conference: 20-24 May 2019\",\"718\":\"Date of Conference: 20-24 May 2019\",\"719\":\"Date of Conference: 20-24 May 2019\",\"720\":\"Date of Conference: 20-24 May 2019\",\"721\":\"Date of Conference: 20-24 May 2019\",\"722\":\"Date of Conference: 20-24 May 2019\",\"723\":\"Date of Conference: 20-24 May 2019\",\"724\":\"Date of Conference: 20-24 May 2019\",\"725\":\"Date of Conference: 20-24 May 2019\",\"726\":\"Date of Conference: 20-24 May 2019\",\"727\":\"Date of Conference: 20-24 May 2019\",\"728\":\"Date of Conference: 20-24 May 2019\",\"729\":\"Date of Conference: 20-24 May 2019\",\"730\":\"Date of Conference: 20-24 May 2019\",\"731\":\"Date of Conference: 20-24 May 2019\",\"732\":\"Date of Conference: 20-24 May 2019\",\"733\":\"Date of Conference: 20-24 May 2019\",\"734\":\"Date of Conference: 20-24 May 2019\",\"735\":\"Date of Conference: 20-24 May 2019\",\"736\":\"Date of Conference: 20-24 May 2019\",\"737\":\"Date of Conference: 20-24 May 2019\",\"738\":\"Date of Conference: 20-24 May 2019\",\"739\":\"Date of Conference: 20-24 May 2019\",\"740\":\"Date of Conference: 20-24 May 2019\",\"741\":\"Date of Conference: 20-24 May 2019\",\"742\":\"Date of Conference: 20-24 May 2019\",\"743\":\"Date of Conference: 20-24 May 2019\",\"744\":\"Date of Conference: 20-24 May 2019\",\"745\":\"Date of Conference: 20-24 May 2019\",\"746\":\"Date of Conference: 20-24 May 2019\",\"747\":\"Date of Conference: 20-24 May 2019\",\"748\":\"Date of Conference: 20-24 May 2019\",\"749\":\"Date of Conference: 20-24 May 2019\",\"750\":\"Date of Conference: 20-24 May 2019\",\"751\":\"Date of Conference: 20-24 May 2019\",\"752\":\"Date of Conference: 20-24 May 2019\",\"753\":\"Date of Conference: 20-24 May 2019\",\"754\":\"Date of Conference: 20-24 May 2019\",\"755\":\"Date of Conference: 20-24 May 2019\",\"756\":\"Date of Conference: 20-24 May 2019\",\"757\":\"Date of Conference: 20-24 May 2019\",\"758\":\"Date of Conference: 20-24 May 2019\",\"759\":\"Date of Conference: 20-24 May 2019\",\"760\":\"Date of Conference: 20-24 May 2019\",\"761\":\"Date of Conference: 20-24 May 2019\",\"762\":\"Date of Conference: 20-24 May 2019\",\"763\":\"Date of Conference: 20-24 May 2019\",\"764\":\"Date of Conference: 20-24 May 2019\",\"765\":\"Date of Conference: 20-24 May 2019\",\"766\":\"Date of Conference: 20-24 May 2019\",\"767\":\"Date of Conference: 20-24 May 2019\",\"768\":\"Date of Conference: 20-24 May 2019\",\"769\":\"Date of Conference: 20-24 May 2019\",\"770\":\"Date of Conference: 20-24 May 2019\",\"771\":\"Date of Conference: 20-24 May 2019\",\"772\":\"Date of Conference: 20-24 May 2019\",\"773\":\"Date of Conference: 20-24 May 2019\",\"774\":\"Date of Conference: 20-24 May 2019\",\"775\":\"Date of Conference: 20-24 May 2019\",\"776\":\"Date of Conference: 20-24 May 2019\",\"777\":\"Date of Conference: 20-24 May 2019\",\"778\":\"Date of Conference: 20-24 May 2019\",\"779\":\"Date of Conference: 20-24 May 2019\",\"780\":\"Date of Conference: 20-24 May 2019\",\"781\":\"Date of Conference: 20-24 May 2019\",\"782\":\"Date of Conference: 20-24 May 2019\",\"783\":\"Date of Conference: 20-24 May 2019\",\"784\":\"Date of Conference: 20-24 May 2019\",\"785\":\"Date of Conference: 20-24 May 2019\",\"786\":\"Date of Conference: 20-24 May 2019\",\"787\":\"Date of Conference: 20-24 May 2019\",\"788\":\"Date of Conference: 20-24 May 2019\",\"789\":\"Date of Conference: 20-24 May 2019\",\"790\":\"Date of Conference: 20-24 May 2019\",\"791\":\"Date of Conference: 20-24 May 2019\",\"792\":\"Date of Conference: 20-24 May 2019\",\"793\":\"Date of Conference: 20-24 May 2019\",\"794\":\"Date of Conference: 20-24 May 2019\",\"795\":\"Date of Conference: 20-24 May 2019\",\"796\":\"Date of Conference: 20-24 May 2019\",\"797\":\"Date of Conference: 20-24 May 2019\",\"798\":\"Date of Conference: 20-24 May 2019\",\"799\":\"Date of Conference: 20-24 May 2019\",\"800\":\"Date of Conference: 20-24 May 2019\",\"801\":\"Date of Conference: 20-24 May 2019\",\"802\":\"Date of Conference: 20-24 May 2019\",\"803\":\"Date of Conference: 20-24 May 2019\",\"804\":\"Date of Conference: 20-24 May 2019\",\"805\":\"Date of Conference: 20-24 May 2019\",\"806\":\"Date of Conference: 20-24 May 2019\",\"807\":\"Date of Conference: 20-24 May 2019\",\"808\":\"Date of Conference: 20-24 May 2019\",\"809\":\"Date of Conference: 20-24 May 2019\",\"810\":\"Date of Conference: 20-24 May 2019\",\"811\":\"Date of Conference: 20-24 May 2019\",\"812\":\"Date of Conference: 20-24 May 2019\",\"813\":\"Date of Conference: 20-24 May 2019\",\"814\":\"Date of Conference: 20-24 May 2019\",\"815\":\"Date of Conference: 20-24 May 2019\",\"816\":\"Date of Conference: 20-24 May 2019\",\"817\":\"Date of Conference: 20-24 May 2019\",\"818\":\"Date of Conference: 20-24 May 2019\",\"819\":\"Date of Conference: 20-24 May 2019\",\"820\":\"Date of Conference: 20-24 May 2019\",\"821\":\"Date of Conference: 20-24 May 2019\",\"822\":\"Date of Conference: 20-24 May 2019\",\"823\":\"Date of Conference: 20-24 May 2019\",\"824\":\"Date of Conference: 20-24 May 2019\",\"825\":\"Date of Conference: 20-24 May 2019\",\"826\":\"Date of Conference: 20-24 May 2019\",\"827\":\"Date of Conference: 20-24 May 2019\",\"828\":\"Date of Conference: 20-24 May 2019\",\"829\":\"Date of Conference: 20-24 May 2019\",\"830\":\"Date of Conference: 20-24 May 2019\",\"831\":\"Date of Conference: 20-24 May 2019\",\"832\":\"Date of Conference: 20-24 May 2019\",\"833\":\"Date of Conference: 20-24 May 2019\",\"834\":\"Date of Conference: 20-24 May 2019\",\"835\":\"Date of Conference: 20-24 May 2019\",\"836\":\"Date of Conference: 20-24 May 2019\",\"837\":\"Date of Conference: 20-24 May 2019\",\"838\":\"Date of Conference: 20-24 May 2019\",\"839\":\"Date of Conference: 20-24 May 2019\",\"840\":\"Date of Conference: 20-24 May 2019\",\"841\":\"Date of Conference: 20-24 May 2019\",\"842\":\"Date of Conference: 20-24 May 2019\",\"843\":\"Date of Conference: 20-24 May 2019\",\"844\":\"Date of Conference: 20-24 May 2019\",\"845\":\"Date of Conference: 20-24 May 2019\",\"846\":\"Date of Conference: 20-24 May 2019\",\"847\":\"Date of Conference: 20-24 May 2019\",\"848\":\"Date of Conference: 20-24 May 2019\",\"849\":\"Date of Conference: 20-24 May 2019\",\"850\":\"Date of Conference: 20-24 May 2019\",\"851\":\"Date of Conference: 20-24 May 2019\",\"852\":\"Date of Conference: 20-24 May 2019\",\"853\":\"Date of Conference: 20-24 May 2019\",\"854\":\"Date of Conference: 20-24 May 2019\",\"855\":\"Date of Conference: 20-24 May 2019\",\"856\":\"Date of Conference: 20-24 May 2019\",\"857\":\"Date of Conference: 20-24 May 2019\",\"858\":\"Date of Conference: 20-24 May 2019\",\"859\":\"Date of Conference: 20-24 May 2019\",\"860\":\"Date of Conference: 20-24 May 2019\",\"861\":\"Date of Conference: 20-24 May 2019\",\"862\":\"Date of Conference: 20-24 May 2019\",\"863\":\"Date of Conference: 20-24 May 2019\",\"864\":\"Date of Conference: 20-24 May 2019\",\"865\":\"Date of Conference: 20-24 May 2019\",\"866\":\"Date of Conference: 20-24 May 2019\",\"867\":\"Date of Conference: 20-24 May 2019\",\"868\":\"Date of Conference: 20-24 May 2019\",\"869\":\"Date of Conference: 20-24 May 2019\",\"870\":\"Date of Conference: 20-24 May 2019\",\"871\":\"Date of Conference: 20-24 May 2019\",\"872\":\"Date of Conference: 20-24 May 2019\",\"873\":\"Date of Conference: 20-24 May 2019\",\"874\":\"Date of Conference: 20-24 May 2019\",\"875\":\"Date of Conference: 20-24 May 2019\",\"876\":\"Date of Conference: 20-24 May 2019\",\"877\":\"Date of Conference: 20-24 May 2019\",\"878\":\"Date of Conference: 20-24 May 2019\",\"879\":\"Date of Conference: 20-24 May 2019\",\"880\":\"Date of Conference: 20-24 May 2019\",\"881\":\"Date of Conference: 20-24 May 2019\",\"882\":\"Date of Conference: 20-24 May 2019\",\"883\":\"Date of Conference: 20-24 May 2019\",\"884\":\"Date of Conference: 20-24 May 2019\",\"885\":\"Date of Conference: 20-24 May 2019\",\"886\":\"Date of Conference: 20-24 May 2019\",\"887\":\"Date of Conference: 20-24 May 2019\",\"888\":\"Date of Conference: 20-24 May 2019\",\"889\":\"Date of Conference: 20-24 May 2019\",\"890\":\"Date of Conference: 20-24 May 2019\",\"891\":\"Date of Conference: 20-24 May 2019\",\"892\":\"Date of Conference: 20-24 May 2019\",\"893\":\"Date of Conference: 20-24 May 2019\",\"894\":\"Date of Conference: 20-24 May 2019\",\"895\":\"Date of Conference: 20-24 May 2019\",\"896\":\"Date of Conference: 20-24 May 2019\",\"897\":\"Date of Conference: 20-24 May 2019\",\"898\":\"Date of Conference: 20-24 May 2019\",\"899\":\"Date of Conference: 20-24 May 2019\",\"900\":\"Date of Conference: 20-24 May 2019\",\"901\":\"Date of Conference: 20-24 May 2019\",\"902\":\"Date of Conference: 20-24 May 2019\",\"903\":\"Date of Conference: 20-24 May 2019\",\"904\":\"Date of Conference: 20-24 May 2019\",\"905\":\"Date of Conference: 20-24 May 2019\",\"906\":\"Date of Conference: 20-24 May 2019\",\"907\":\"Date of Conference: 20-24 May 2019\",\"908\":\"Date of Conference: 20-24 May 2019\",\"909\":\"Date of Conference: 20-24 May 2019\",\"910\":\"Date of Conference: 20-24 May 2019\",\"911\":\"Date of Conference: 20-24 May 2019\",\"912\":\"Date of Conference: 20-24 May 2019\",\"913\":\"Date of Conference: 20-24 May 2019\",\"914\":\"Date of Conference: 20-24 May 2019\",\"915\":\"Date of Conference: 20-24 May 2019\",\"916\":\"Date of Conference: 20-24 May 2019\",\"917\":\"Date of Conference: 20-24 May 2019\",\"918\":\"Date of Conference: 20-24 May 2019\",\"919\":\"Date of Conference: 20-24 May 2019\",\"920\":\"Date of Conference: 20-24 May 2019\",\"921\":\"Date of Conference: 20-24 May 2019\",\"922\":\"Date of Conference: 20-24 May 2019\",\"923\":\"Date of Conference: 20-24 May 2019\",\"924\":\"Date of Conference: 20-24 May 2019\",\"925\":\"Date of Conference: 20-24 May 2019\",\"926\":\"Date of Conference: 20-24 May 2019\",\"927\":\"Date of Conference: 20-24 May 2019\",\"928\":\"Date of Conference: 20-24 May 2019\",\"929\":\"Date of Conference: 20-24 May 2019\",\"930\":\"Date of Conference: 20-24 May 2019\",\"931\":\"Date of Conference: 20-24 May 2019\",\"932\":\"Date of Conference: 20-24 May 2019\",\"933\":\"Date of Conference: 20-24 May 2019\",\"934\":\"Date of Conference: 20-24 May 2019\",\"935\":\"Date of Conference: 20-24 May 2019\",\"936\":\"Date of Conference: 20-24 May 2019\",\"937\":\"Date of Conference: 20-24 May 2019\",\"938\":\"Date of Conference: 20-24 May 2019\",\"939\":\"Date of Conference: 20-24 May 2019\",\"940\":\"Date of Conference: 20-24 May 2019\",\"941\":\"Date of Conference: 20-24 May 2019\",\"942\":\"Date of Conference: 20-24 May 2019\",\"943\":\"Date of Conference: 20-24 May 2019\",\"944\":\"Date of Conference: 20-24 May 2019\",\"945\":\"Date of Conference: 20-24 May 2019\",\"946\":\"Date of Conference: 20-24 May 2019\",\"947\":\"Date of Conference: 20-24 May 2019\",\"948\":\"Date of Conference: 20-24 May 2019\",\"949\":\"Date of Conference: 20-24 May 2019\",\"950\":\"Date of Conference: 20-24 May 2019\",\"951\":\"Date of Conference: 20-24 May 2019\",\"952\":\"Date of Conference: 20-24 May 2019\",\"953\":\"Date of Conference: 20-24 May 2019\",\"954\":\"Date of Conference: 20-24 May 2019\",\"955\":\"Date of Conference: 20-24 May 2019\",\"956\":\"Date of Conference: 20-24 May 2019\",\"957\":\"Date of Conference: 20-24 May 2019\",\"958\":\"Date of Conference: 20-24 May 2019\",\"959\":\"Date of Conference: 20-24 May 2019\",\"960\":\"Date of Conference: 20-24 May 2019\",\"961\":\"Date of Conference: 20-24 May 2019\",\"962\":\"Date of Conference: 20-24 May 2019\",\"963\":\"Date of Conference: 20-24 May 2019\",\"964\":\"Date of Conference: 20-24 May 2019\",\"965\":\"Date of Conference: 20-24 May 2019\",\"966\":\"Date of Conference: 20-24 May 2019\",\"967\":\"Date of Conference: 20-24 May 2019\",\"968\":\"Date of Conference: 20-24 May 2019\",\"969\":\"Date of Conference: 20-24 May 2019\",\"970\":\"Date of Conference: 20-24 May 2019\",\"971\":\"Date of Conference: 20-24 May 2019\",\"972\":\"Date of Conference: 20-24 May 2019\",\"973\":\"Date of Conference: 20-24 May 2019\",\"974\":\"Date of Conference: 20-24 May 2019\",\"975\":\"Date of Conference: 20-24 May 2019\",\"976\":\"Date of Conference: 20-24 May 2019\",\"977\":\"Date of Conference: 20-24 May 2019\",\"978\":\"Date of Conference: 20-24 May 2019\",\"979\":\"Date of Conference: 20-24 May 2019\",\"980\":\"Date of Conference: 20-24 May 2019\",\"981\":\"Date of Conference: 20-24 May 2019\",\"982\":\"Date of Conference: 20-24 May 2019\",\"983\":\"Date of Conference: 20-24 May 2019\",\"984\":\"Date of Conference: 20-24 May 2019\",\"985\":\"Date of Conference: 20-24 May 2019\",\"986\":\"Date of Conference: 20-24 May 2019\",\"987\":\"Date of Conference: 20-24 May 2019\",\"988\":\"Date of Conference: 20-24 May 2019\",\"989\":\"Date of Conference: 20-24 May 2019\",\"990\":\"Date of Conference: 20-24 May 2019\",\"991\":\"Date of Conference: 20-24 May 2019\",\"992\":\"Date of Conference: 20-24 May 2019\",\"993\":\"Date of Conference: 20-24 May 2019\",\"994\":\"Date of Conference: 20-24 May 2019\",\"995\":\"Date of Conference: 20-24 May 2019\",\"996\":\"Date of Conference: 20-24 May 2019\",\"997\":\"Date of Conference: 20-24 May 2019\",\"998\":\"Date of Conference: 20-24 May 2019\",\"999\":\"Date of Conference: 20-24 May 2019\",\"1000\":\"Date of Conference: 20-24 May 2019\",\"1001\":\"Date of Conference: 20-24 May 2019\",\"1002\":\"Date of Conference: 20-24 May 2019\",\"1003\":\"Date of Conference: 20-24 May 2019\",\"1004\":\"Date of Conference: 20-24 May 2019\",\"1005\":\"Date of Conference: 20-24 May 2019\",\"1006\":\"Date of Conference: 20-24 May 2019\",\"1007\":\"Date of Conference: 20-24 May 2019\",\"1008\":\"Date of Conference: 20-24 May 2019\",\"1009\":\"Date of Conference: 20-24 May 2019\",\"1010\":\"Date of Conference: 20-24 May 2019\",\"1011\":\"Date of Conference: 20-24 May 2019\",\"1012\":\"Date of Conference: 20-24 May 2019\",\"1013\":\"Date of Conference: 20-24 May 2019\",\"1014\":\"Date of Conference: 20-24 May 2019\",\"1015\":\"Date of Conference: 20-24 May 2019\",\"1016\":\"Date of Conference: 20-24 May 2019\",\"1017\":\"Date of Conference: 20-24 May 2019\",\"1018\":\"Date of Conference: 20-24 May 2019\",\"1019\":\"Date of Conference: 20-24 May 2019\",\"1020\":\"Date of Conference: 20-24 May 2019\",\"1021\":\"Date of Conference: 20-24 May 2019\"},\"Paper Title\":{\"0\":\"Trajectory-based Probabilistic Policy Gradient for Learning Locomotion Behaviors\",\"1\":\"Learning Motion Planning Policies in Uncertain Environments through Repeated Task Executions\",\"2\":\"BaRC: Backward Reachability Curriculum for Robotic Reinforcement Learning\",\"3\":\"Active Sampling based Safe Identification of Dynamical Systems using Extreme Learning Machines and Barrier Certificates\",\"4\":\"Navigating Dynamically Unknown Environments Leveraging Past Experience\",\"5\":\"VPE: Variational Policy Embedding for Transfer Reinforcement Learning\",\"6\":\"Automatic Labeled LiDAR Data Generation based on Precise Human Model\",\"7\":\"Video Object Segmentation using Teacher-Student Adaptation in a Human Robot Interaction (HRI) Setting\",\"8\":\"Morphology-Specific Convolutional Neural Networks for Tactile Object Recognition with a Multi-Fingered Hand\",\"9\":\"A Maximum Likelihood Approach to Extract Finite Planes from 3-D Laser Scans\",\"10\":\"Designing Worm-inspired Neural Networks for Interpretable Robotic Control\",\"11\":\"Acting Is Seeing: Navigating Tight Space Using Flapping Wings\",\"12\":\"Design and Characterization of a Novel Robotic Surface for Application to Compressed Physical Environments\",\"13\":\"Learning Extreme Hummingbird Maneuvers on Flapping Wing Robots\",\"14\":\"FMD Stereo SLAM: Fusing MVG and Direct Formulation Towards Accurate and Fast Stereo SLAM\",\"15\":\"ScalableFusion: High-resolution Mesh-based Real-time 3D Reconstruction\",\"16\":\"GEN-SLAM: Generative Modeling for Monocular Simultaneous Localization and Mapping\",\"17\":\"RESLAM: A real-time robust edge-based SLAM system\",\"18\":\"On-line 3D active pose-graph SLAM based on key poses using graph topology and sub-maps\",\"19\":\"Modeling and Planning Manipulation in Dynamic Environments\",\"20\":\"Efficient Obstacle Rearrangement for Object Manipulation Tasks in Cluttered Environments\",\"21\":\"MoveIt! Task Constructor for Task-Level Motion Planning\",\"22\":\"Exploiting Environment Contacts of Serial Manipulators\",\"23\":\"optimization-Based Human-in-the-Loop Manipulation Using Joint Space Polytopes\",\"24\":\"Large-Scale Multi-Object Rearrangement\",\"25\":\"Manipulation Using Microrobot Driven by Optothermally Generated Surface Bubble\",\"26\":\"Compound micromachines powered by acoustic streaming\",\"27\":\"ChevBot \\u2013 An Untethered Microrobot Powered by Laser for Microfactory Applications\",\"28\":\"Capillary Ionic Transistor and Precise Transport Control for Nano Manipulation\",\"29\":\"Resolved Viscoelasticity Control Considering Singularity for Knee-stretched Walking of a Humanoid\",\"30\":\"Versatile Reactive Bipedal Locomotion Planning Through Hierarchical Optimization\",\"31\":\"Using Deep Reinforcement Learning to Learn High-Level Policies on the ATRIAS Biped\",\"32\":\"Unsupervised Gait Phase Estimation for Humanoid Robot Walking\",\"33\":\"Stair Climbing Stabilization of the HRP-4 Humanoid Robot using Whole-body Admittance Control\",\"34\":\"Reinforcement Learning Meets Hybrid Zero Dynamics: A Case Study for RABBIT\",\"35\":\"Learning Wheel Odometry and IMU Errors for Localization\",\"36\":\"Radar-only ego-motion estimation in difficult settings via graph matching\",\"37\":\"Recursive Integrity Monitoring for Mobile Robot Localization Safety\",\"38\":\"Four-Wheeled Dead-Reckoning Model Calibration using RTS Smoothing\",\"39\":\"A Multi-Domain Feature Learning Method for Visual Place Recognition\",\"40\":\"Event-based, Direct Camera Tracking from a Photometric 3D Map using Nonlinear Optimization\",\"41\":\"Linear Heterogeneous Reconfiguration of Cubic Modular Robots via Simultaneous Tunneling and Permutation\",\"42\":\"Autonomous Sheet Pile Driving Robots for Soil Stabilization\",\"43\":\"ModQuad-Vi: A Vision-Based Self-Assembling Modular Quadrotor\",\"44\":\"Robotic endoscopy system (easyEndo) with a robotic arm mountable on a conventional endoscope\",\"45\":\"Design and Fabrication of Transformable Head Structures for Endoscopic Catheters\",\"46\":\"A Rolling-Tip Flexible Instrument for Minimally Invasive Surgery\",\"47\":\"A Novel Laser Scalpel System for Computer-assisted Laser Surgery\",\"48\":\"Intent-Uncertainty-Aware Grasp Planning for Robust Robot Assistance in Telemanipulation\",\"49\":\"Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network\",\"50\":\"An energy-shared two-layer approach for multi-master-multi-slave bilateral teleoperation systems\",\"51\":\"Passive Task-Prioritized Shared-Control Teleoperation with Haptic Guidance\",\"52\":\"Quasi-Direct Drive for Low-Cost Compliant Robotic Manipulation\",\"53\":\"Augmented Reality Predictive Displays to Help Mitigate the Effects of Delayed Telesurgery\",\"54\":\"Stability Optimization of Two-Fingered Anthropomorphic Hands for Precision Grasping with a Single Actuator\",\"55\":\"A new Approach for an Adaptive Linear Quadratic Regulated Motion Cueing Algorithm for an 8 DoF Full Motion Driving Simulator\",\"56\":\"Singularity of Cable-Driven Parallel Robot With Sagging Cables: Preliminary Investigation\",\"57\":\"A defect identification approach of operations for the driving element of multi-duty parallel manipulators\",\"58\":\"Active Damping of Parallel Robots Driven by Flexible Cables Using Cold-Gas Thrusters\",\"59\":\"Exploiting Human and Robot Muscle Synergies for Human-in-the-loop Optimization of EMG-based Assistive Strategies\",\"60\":\"Development of a Low Inertia Parallel Actuated Shoulder Exoskeleton Robot for the Characterization of Neuromuscular Property during Static Posture and Dynamic Movement\",\"61\":\"Effort Estimation in Robot-aided Training with a Neural Network\",\"62\":\"Characterizing Architectures of Soft Pneumatic Actuators for a Cable-Driven Shoulder Exoskeleton\",\"63\":\"Design and Implementation of a Two-DOF Robotic System with an Adjustable Force Limiting Mechanism for Ankle Rehabilitation\",\"64\":\"Rorg: Service Robot Software Management with Linux Containers\",\"65\":\"CartesI\\/O: A ROS Based Real-Time Capable Cartesian Control Framework\",\"66\":\"Synthesis of Real-Time Observers from Past-Time Linear Temporal Logic and Timed Specification\",\"67\":\"Julia for robotics: simulation and real-time control in a high-level programming language\",\"68\":\"Motion Planning Templates: A Motion Planning Framework for Robots with Low-power CPUs\",\"69\":\"Distortion-free Robotic Surface-drawing using Conformal Mapping\",\"70\":\"Automated Cell Patterning System with a Microchip using Dielectrophoresis\",\"71\":\"Mobile Robotic Painting of Texture\",\"72\":\"Detecting Invasive Insects with Unmanned Aerial Vehicles\",\"73\":\"Robust Object-based SLAM for High-speed Autonomous Navigation\",\"74\":\"A Fault Diagnosis Framework for MAVLink-Enabled UAVs Using Structural Analysis\",\"75\":\"Real-Time Minimum Snap Trajectory Generation for Quadcopters: Algorithm Speed-up Through Machine Learning\",\"76\":\"Beauty and the Beast: Optimal Methods Meet Learning for Drone Racing\",\"77\":\"Detection and Reconstruction of Wires Using Cameras for Aircraft Safety Systems\",\"78\":\"Pose and Posture Estimation of Aerial Skeleton Systems for Outdoor Flying\",\"79\":\"Flight Testing Boustrophedon Coverage Path Planning for Fixed Wing UAVs in Wind\",\"80\":\"Obstacle-aware Adaptive Informative Path Planning for UAV-based Target Search\",\"81\":\"Real-Time Planning with Multi-Fidelity Models for Agile Flights in Unknown Environments\",\"82\":\"Efficient Trajectory Planning for High Speed Flight in Unknown Environments\",\"83\":\"Priority Maps for Surveillance and Intervention of Wildfires and other Spreading Processes\",\"84\":\"A Practical Approach to Insertion with Variable Socket Position Using Deep Reinforcement Learning\",\"85\":\"Uncertainty-Aware Data Aggregation for Deep Imitation Learning\",\"86\":\"Uncertainty Aware Learning from Demonstrations in Multiple Contexts using Bayesian Neural Networks\",\"87\":\"Learning From Demonstration in the Wild\",\"88\":\"A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies\",\"89\":\"Simulating Emergent Properties of Human Driving Behavior Using Multi-Agent Reward Augmented Imitation Learning\",\"90\":\"A Supervised Approach to Predicting Noise in Depth Images\",\"91\":\"Quantum Computation in Robotic Science and Applications\",\"92\":\"A Learning Framework for High Precision Industrial Assembly\",\"93\":\"Manipulation by Feel: Touch-Based Control with Deep Predictive Models\",\"94\":\"3D Printed Soft Pneumatic Actuators with Intent Sensing for Hand Rehabilitative Exoskeletons\",\"95\":\"Gaze-based, Context-aware Robotic System for Assisted Reaching and Grasping\",\"96\":\"Ways to Learn a Therapist\\u2019s Patient-specific Intervention: Robotics-vs Telerobotics-mediated Hands-on Teaching\",\"97\":\"Development of a Novel Force Sensing System to Measure the Ground Reaction Force of Rats with Complete Spinal Cord Injury\",\"98\":\"A Four-magnet System for 2D Wireless Open-Loop Control of Microrobots\",\"99\":\"Nitinol living hinges for millimeter-sized robots and medical devices\",\"100\":\"Tetherless Mobile Micro-Surgical Scissors Using Magnetic Actuation\",\"101\":\"A Large-Deflection FBG Bending Sensor for SMA Bending Modules for Steerable Surgical Robots\",\"102\":\"Towards Learning Abstract Representations for Locomotion Planning in High-dimensional State Spaces\",\"103\":\"Fast Stochastic Functional Path Planning in Occupancy Maps\",\"104\":\"A Scalable Framework For Real-Time Multi-Robot, Multi-Human Collision Avoidance\",\"105\":\"Lazy Evaluation of Goal Specifications Guided by Motion Planning\",\"106\":\"Reconfigurable Motion Planning and Control in Obstacle Cluttered Environments under Timed Temporal Tasks\",\"107\":\"Door opening and traversal with an industrial cartesian impedance controlled mobile robot\",\"108\":\"An Algorithm for Odor Source Localization based on Source Term Estimation\",\"109\":\"Neural Network Pile Loading Controller Trained by Demonstration\",\"110\":\"Dynamic Manipulation of Gear Ratio and Ride Height for a Novel Compliant Wheel using Pneumatic Actuators\",\"111\":\"Feasible coordination of multiple homogeneous or heterogeneous mobile vehicles with various constraints\",\"112\":\"Turn-minimizing multirobot coverage\",\"113\":\"Efficient Kinodynamic Multi-Robot Replanning in Known Workspaces\",\"114\":\"Cannot avoid penalty? Let\\u2019s minimize\",\"115\":\"Mixed-Granularity Human-Swarm Interaction\",\"116\":\"Flexible collaborative transportation by a team of rotorcraft\",\"117\":\"Dynamically-consistent Generalized Hierarchical Control\",\"118\":\"Robotic Joint Control System based on Analogue Spiking Neural Networks and SMA Actuators\",\"119\":\"Model Reference Adaptive Control of a Two-Wheeled Mobile Robot\",\"120\":\"A Robust Tracking Controller for Robot Manipulators: Embedding Internal Model of Disturbances\",\"121\":\"Receding horizon estimation and control with structured noise blocking for mobile robot slip compensation\",\"122\":\"Decoupled Control of Position and \\/ or Force of Tendon Driven Fingers\",\"123\":\"Reconfigurable Network for Efficient Inferencing in Autonomous Vehicles\",\"124\":\"Fast Radar Motion Estimation with a Learnt Focus of Attention using Weak Supervision\",\"125\":\"Learned Map Prediction for Enhanced Mobile Robot Exploration\",\"126\":\"Propagation Networks for Model-Based Control Under Partial Observation\",\"127\":\"Interactive Trajectory Prediction for Autonomous Driving via Recurrent Meta Induction Neural Network\",\"128\":\"Studies on Positioning Manipulators Actuated by Solid Media Transmissions\",\"129\":\"Exploiting Bistability for High Force Density Reflexive Gripping\",\"130\":\"Compliant Bistable Gripper for Aerial Perching and Grasping\",\"131\":\"MH-iSAM2: Multi-hypothesis iSAM using Bayes Tree and Hypo-tree\",\"132\":\"Improving Keypoint Matching Using a Landmark-Based Image Representation\",\"133\":\"Fast and Robust Initialization for Visual-Inertial SLAM\",\"134\":\"Accurate Direct Visual-Laser Odometry with Explicit Occlusion Handling and Plane Detection\",\"135\":\"Efficient Constellation-Based Map-Merging for Semantic SLAM\",\"136\":\"Learning Robust Manipulation Strategies with Multimodal State Transition Models and Recovery Heuristics\",\"137\":\"Adaptive Critic Based Optimal Kinematic Control for a Robot Manipulator\",\"138\":\"Manipulability Optimization Control of a Serial Redundant Robot for Robot-assisted Minimally Invasive Surgery\",\"139\":\"Accounting for Part Pose Estimation Uncertainties during Trajectory Generation for Part Pick-Up Using Mobile Manipulators\",\"140\":\"Adapting Everyday Manipulation Skills to Varied Scenarios\",\"141\":\"Feedback Control and 3D Motion of Heterogeneous Janus Particles\",\"142\":\"Data-Driven Gait Segmentation for Walking Assistance in a Lower-Limb Assistive Device\",\"143\":\"Closed-loop MPC with Dense Visual SLAM - Stability through Reactive Stepping\",\"144\":\"Safe 3D Bipedal Walking through Linear MPC with 3D Capturability\",\"145\":\"Feedback motion planning of legged robots by composing orbital Lyapunov functions using rapidly-exploring random trees\",\"146\":\"Online Walking Pattern Generation for Humanoid Robot with Compliant Motion Control\",\"147\":\"A Kalman Filter-Based Algorithm for Simultaneous Time Synchronization and Localization in UWB Networks\",\"148\":\"eRTIS: A Fully Embedded Real Time 3D Imaging Sonar Sensor for Robotic Applications\",\"149\":\"LookUP: Vision-Only Real-Time Precise Underground Localisation for Autonomous Mining Vehicles\",\"150\":\"Analysis of Robust Functions for Registration Algorithms\",\"151\":\"CoLo: A Performance Evaluation System for Multi-robot Cooperative Localization Algorithms\",\"152\":\"A cane-based low cost sensor to implement attention mechanisms in telecare robots\",\"153\":\"A Deployable Soft Robotic Arm with Stiffness Modulation for Assistive Living Applications\",\"154\":\"Development of a Novel Gait Rehabilitation Device with Hip Interaction and a Single DOF Mechanism\",\"155\":\"Differentially-Clutched Series Elastic Actuator for Robot-Aided Musculoskeletal Rehabilitation\",\"156\":\"A Novel Robotic Suturing System for Flexible Endoscopic Surgery\",\"157\":\"A Noninvasive Approach to Recovering the Lost Force Feedback for a Robotic-Assisted Insertable Laparoscopic Surgical Camera\",\"158\":\"Development of a Multi-level Stiffness Soft Robotic Module with Force Haptic Feedback for Endoscopic Applications\",\"159\":\"Feasibility Study of Robotic Needles with a Rotational Tip-Joint and Notch Patterns\",\"160\":\"Autonomous Laparoscopic Robotic Suturing with a Novel Actuated Suturing Tool and 3D Endoscope\",\"161\":\"Active Contraints for Tool-Shaft Collision Avoidance in Minimally Invasive Surgery\",\"162\":\"Energy Budget Transaction Protocol for Distributed Robotic Systems\",\"163\":\"Tele-Echography using a Two-Layer Teleoperation Algorithm with Energy Scaling\",\"164\":\"EMG-Controlled Non-Anthropomorphic Hand Teleoperation Using a Continuous Teleoperation Subspace\",\"165\":\"Enhancing the Force Transparency of Time Domain Passivity Approach: Observer-Based Gradient Controller\",\"166\":\"Motion Scaling Solutions for Improved Performance in High Delay Surgical Teleoperation\",\"167\":\"Robust object grasping in clutter via singulation\",\"168\":\"Towards an Integrated Autonomous Data-Driven Grasping System with a Mobile Manipulator\",\"169\":\"Design Principles and Optimization of a Planar Underactuated Hand for Caging Grasps\",\"170\":\"Mechanical Search: Multi-Step Retrieval of a Target Object Occluded by Clutter\",\"171\":\"Transferring Grasp Configurations using Active Learning and Local Replanning\",\"172\":\"Kinematic Analysis of a 4-DOF Parallel Mechanism with Large Translational and Orientational Workspace\",\"173\":\"Kinematically Redundant (6+3)-dof Hybrid Parallel Robot with Large orientational Workspace and Remotely Operated Gripper\",\"174\":\"Modeling Variable Curvature Parallel Continuum Robots Using Euler Curves\",\"175\":\"Variable Damping Control of the Robotic Ankle Joint to Improve Trade-off between Performance and Stability\",\"176\":\"An Autonomous Exoskeleton for Ankle Plantarflexion Assistance\",\"177\":\"Analytic Collision Risk Calculation for Autonomous Vehicle Navigation\",\"178\":\"A new approach to local navigation for autonomous driving vehicles based on the curvature velocity method\",\"179\":\"Goal-Driven Navigation for Non-holonomic Multi-Robot System by Learning Collision\",\"180\":\"Efficient Exact Collision Detection between Ellipsoids and Superquadrics via Closed-form Minkowski Sums\",\"181\":\"Positioning Uncertainty Reduction of Magnetically Guided Actuation on Planar Surfaces\",\"182\":\"Robot Localization Based on Aerial Images for Precision Agriculture Tasks in Crop Fields\",\"183\":\"Visual Appearance Analysis of Forest Scenes for Monocular SLAM\",\"184\":\"An Approach for Semantic Segmentation of Tree-like Vegetation\",\"185\":\"Thermal Image Based Navigation System for Skid-Steering Mobile Robots in Sugarcane Crops\",\"186\":\"Dynamic Obstacles Detection for Robotic Soil Explorations\",\"187\":\"Non-Destructive Robotic Assessment of Mango Ripeness via Multi-Point Soft Haptics\",\"188\":\"UAV Pose Estimation using Cross-view Geolocalization with Satellite Imagery\",\"189\":\"The Open Vision Computer: An Integrated Sensing and Compute System for Mobile Robots\",\"190\":\"RaD-VIO: Rangefinder-aided Downward Visual-Inertial Odometry\",\"191\":\"Learning to Capture a Film-Look Video with a Camera Drone\",\"192\":\"Design and Experiments for MultI-Section-Transformable (MIST)-UAV\",\"193\":\"Online Estimation of Geometric and Inertia Parameters for Multirotor Aerial Vehicles\",\"194\":\"External Wrench Estimation for Multilink Aerial Robot by Center of Mass Estimator Based on Distributed IMU System\",\"195\":\"A Novel Development of Robots with Cooperative Strategy for Long-term and Close-proximity Autonomous Transmission-line Inspection\",\"196\":\"Hunting Drones with Other Drones: Tracking a Moving Radio Target\",\"197\":\"Design and Testing of a New Cell Microinjector with Embedded Soft Force Sensor\",\"198\":\"Energy optimization for a Robust and Flexible Interaction Control\",\"199\":\"Design of Versatile and Low-Cost Shaft Sensor for Health Monitoring\",\"200\":\"Robust Execution of Contact-Rich Motion Plans by Hybrid Force-Velocity Control\",\"201\":\"Endoscope Force Generation and Intrinsic Sensing with Environmental Scaffolding\",\"202\":\"On The Combination of Gamification and Crowd Computation in Industrial Automation and Robotics Applications\",\"203\":\"A New Overloading Fatigue Model for Ergonomic Risk Assessment with Application to Human-Robot Collaboration\",\"204\":\"Human-inspired balance model to account for foot-beam interaction mechanics\",\"205\":\"Real-time Robot-assisted Ergonomics\",\"206\":\"A Fog Robotic System for Dynamic Visual Servoing\",\"207\":\"Approximate Probabilistic Security for Networked Multi-Robot Systems\",\"208\":\"A Decentralized Heterogeneous Control Strategy for a Class of Infinitesimally Shape-Similar Formations\",\"209\":\"Asynchronous Network Formation in Unknown Unbounded Environments\",\"210\":\"Switching Topology for Resilient Consensus using Wi-Fi Signals\",\"211\":\"Multi-Vehicle Trajectory optimisation On Road Networks\",\"212\":\"Disturbance Compensation Based Control for an Indoor Blimp Robot\",\"213\":\"Informed Information Theoretic Model Predictive Control\",\"214\":\"A Generic Optimization Based Cartesian Controller for Robotic Mobile Manipulation\",\"215\":\"Task-Driven Estimation and Control via Information Bottlenecks\",\"216\":\"Feasibility Analysis For Constrained Model Predictive Control Based Motion Cueing Algorithm\",\"217\":\"Robustness to Out-of-Distribution Inputs via Task-Aware Generative Uncertainty\",\"218\":\"Multimodal Trajectory Predictions for Autonomous Driving using Deep Convolutional Networks\",\"219\":\"Classifying Pedestrian Actions In Advance Using Predicted Video Of Urban Driving Scenes\",\"220\":\"Lightweight Contrast Modeling for Attention-Aware Visual Localization\",\"221\":\"Learning to Write Anywhere with Spatial Transformer Image-to-Motion Encoder-Decoder Networks\",\"222\":\"Motion Planning Networks\",\"223\":\"Improving Data Efficiency of Self-supervised Learning for Robotic Grasping\",\"224\":\"Online Object and Task Learning via Human Robot Interaction\",\"225\":\"Dynamic Manipulation of Flexible Objects with Torque Sequence Using a Deep Neural Network\",\"226\":\"Color-Coded Fiber-Optic Tactile Sensor for an Elastomeric Robot Skin\",\"227\":\"Reinforcement Learning in Topology-based Representation for Human Body Movement with Whole Arm Manipulation\",\"228\":\"Demonstration-Guided Deep Reinforcement Learning of Control Policies for Dexterous Human-Robot Interaction\",\"229\":\"Team-Based Robot Righting via Pushing and Shell Design\",\"230\":\"Deformation-based shape control with a multirobot system\",\"231\":\"One-to-many bipartite matching based coalition formation for multi-robot task allocation\",\"232\":\"Coordinated multi-robot planning while preserving individual privacy\",\"233\":\"Multi-Agent Synchronization Using Online Model-Free Action Dependent Dual Heuristic Dynamic Programming Approach\",\"234\":\"Robust Area Coverage with Connectivity Maintenance\",\"235\":\"Living with a Mobile Companion Robot in your Own Apartment - Final Implementation and Results of a 20-Weeks Field Study with 20 Seniors\",\"236\":\"Enabling Identity-Aware Tracking via Fusion of Visual and Inertial Features\",\"237\":\"Inverse Reinforcement Learning of Interaction Dynamics from Demonstrations\",\"238\":\"Investigating Design Elements of Companion Robots for Older Adults\",\"239\":\"Development of Informative Path Planning for Inspection of the Hanford Tank Farm\",\"240\":\"A Fuzzy Based Accessibility Model for Disaster Environment\",\"241\":\"Learning to Predict the Wind for Safe Aerial Vehicle Planning\",\"242\":\"Distributed Radiation Field Estimation and Informative Path Planning for Nuclear Environment Characterization\",\"243\":\"Visual recognition in the wild by sampling deep similarity functions\",\"244\":\"Evaluating Merging Strategies for Sampling-based Uncertainty Techniques in Object Detection\",\"245\":\"Training a Binary Weight Object Detector by Knowledge Transfer for Autonomous Driving\",\"246\":\"Visual SLAM: Why Bundle Adjust?\",\"247\":\"Illumination Robust Monocular Direct Visual Odometry for Outdoor Environment Mapping\",\"248\":\"A Comparison of CNN-Based and Hand-Crafted Keypoint Descriptors\",\"249\":\"Environment Driven Underwater Camera-IMU Calibration for Monocular Visual-Inertial SLAM\",\"250\":\"Leveraging Structural Regularity of Atlanta World for Monocular SLAM\",\"251\":\"Multimodal Semantic SLAM with Probabilistic Data Association\",\"252\":\"Improving Incremental Planning Performance through Overlapping Replanning and Execution\",\"253\":\"Multimodal Policy Search using Overlapping Mixtures of Sparse Gaussian Process Prior\",\"254\":\"Online adaptation of uncertain models using neural network priors and partially observable planning\",\"255\":\"Contact-Implicit Trajectory Optimization Based on a Variable Smooth Contact Model and Successive Convexification\",\"256\":\"Energy Gradient-Based Graphs for Planning Within-Hand Caging Manipulation\",\"257\":\"A new robot skating on water surface intimating water striders based on flexible driving mechanism\",\"258\":\"Performance Metrics for a Robotic Actuation System using Static and Mobile Electromagnets\",\"259\":\"Yaw Torque Authority for a Flapping-Wing Micro-Aerial Vehicle\",\"260\":\"Data-efficient Learning of Morphology and Controller for a Microrobot\",\"261\":\"Retrieval of magnetic medical microrobots from the bloodstream\",\"262\":\"Experiments with Human-inspired Behaviors in a Humanoid Robot: Quasi-static Balancing using Toe-off Motion and Stretched Knees\",\"263\":\"Prediction Maps for Real-Time 3D Footstep Planning in Dynamic Environments\",\"264\":\"See and Be Seen \\u2013 Rapid and Likeable High-Definition Camera-Eye for Anthropomorphic Robots\",\"265\":\"Generalized Orientation Learning in Robot Task Space\",\"266\":\"ATLAS FaST: Fast and Simple Scheduled TDOA for Reliable Ultra-Wideband Localization\",\"267\":\"HD Map Change Detection with a Boosted Particle Filter\",\"268\":\"Non-parametric Error Modeling for Ultra-wideband Localization Networks\",\"269\":\"Self-Supervised Incremental Learning for Sound Source Localization in Complex Indoor Environment\",\"270\":\"Automated Models of Human Everyday Activity based on Game and Virtual Reality Technology\",\"271\":\"Development of a strain gauge based disturbance estimation and compensation technique for a wheeled inverted pendulum robot\",\"272\":\"Spatio-temporal representation for long-term anticipation of human presence in service robotics\",\"273\":\"Object Transfer Point Estimation for Fluent Human-Robot Handovers\",\"274\":\"Controlling AeroBot: Development of a Motion Planner for an Actively Articulated Wheeled Humanoid Robot\",\"275\":\"User Centric Device Registration for Streamlined Workflows in Surgical Navigation Systems\",\"276\":\"Compliant four degree-of-freedom manipulator with locally deformable elastic elements for minimally invasive surgery\",\"277\":\"Safe teleoperation of a laparoscope holder with dynamic precision but low stiffness\",\"278\":\"Real-time Teleoperation of Flexible Beveled-tip Needle Insertion using Haptic Force Feedback and 3D Ultrasound Guidance\",\"279\":\"End-User Robot Programming Using Mixed Reality\",\"280\":\"Control of Delayed Bilateral Teleoperation System for Robotic Tele-Echography\",\"281\":\"A Unified Framework for the Teleoperation of Surgical Robots in Constrained Workspaces\",\"282\":\"High-Speed Ring Insertion by Dynamic Observable Contact Hand\",\"283\":\"Learning To Grasp Under Uncertainty Using POMDPs\",\"284\":\"Soft Hands with Embodied Constraints: The Soft ScoopGripper\",\"285\":\"A Simple Electric Soft Robotic Gripper with High-Deformation Haptic Feedback\",\"286\":\"Using Geometric Features to Represent Near-Contact Behavior in Robotic Grasping\",\"287\":\"A GPU Based Parallel Genetic Algorithm for the Orientation Optimization Problem in 3D Printing\",\"288\":\"Where Should We Place LiDARs on the Autonomous Vehicle? - An Optimal Design Approach\",\"289\":\"A Robotic Cell for Multi-Resolution Additive Manufacturing\",\"290\":\"Multimodal Bin Picking System with Compliant Tactile Sensor Arrays for Flexible Part Handling\",\"291\":\"Offline Policy Iteration Based Reinforcement Learning Controller for Online Robotic Knee Prosthesis Parameter Tuning\",\"292\":\"Consolidated control framework to control a powered transfemoral prosthesis over inclined terrain conditions\",\"293\":\"Estimating Loads Along Elastic Rods\",\"294\":\"Oriented Point Sampling for Plane Detection in Unorganized Point Clouds\",\"295\":\"The Importance of Metric Learning for Robotic Vision: Open Set Recognition and Active Learning\",\"296\":\"Learning Discriminative Embeddings for Object Recognition on-the-fly\",\"297\":\"A Novel Multi-layer Framework for Tiny Obstacle Discovery\",\"298\":\"DSNet: Joint Learning for Scene Segmentation and Disparity Estimation\",\"299\":\"Spatial change detection using voxel classification by normal distributions transform\",\"300\":\"Set-based Inverse Kinematics Control of an Anthropomorphic Dual Arm Aerial Manipulator\",\"301\":\"Detection and Tracking of Small Objects in Sparse 3D Laser Range Data\",\"302\":\"GPS-Denied UAV Localization using Pre-existing Satellite Imagery\",\"303\":\"Adaptive View Planning for Aerial 3D Reconstruction\",\"304\":\"An Autonomous Loop-Closure Approach for Simultaneous Exploration and Coverage of Unknown Infrastructure Using MAVs\",\"305\":\"Unsupervised Learning of Assistive Camera Views by an Aerial Co-robot in Augmented Reality Multitasking Environments\",\"306\":\"Visual Coverage Control for Teams of Quadcopters via Control Barrier Functions\",\"307\":\"Robot Co-design: Beyond the Monotone Case\",\"308\":\"Multi-Vehicle Close Enough Orienteering Problem with B\\u00e9zier Curves for Multi-Rotor Aerial Vehicles\",\"309\":\"Critically fast pick-and-place with suction cups\",\"310\":\"Robust Link Position Tracking Control for Robot Manipulators with Series Elastic Actuators Using Time-delay Estimation\",\"311\":\"A Simple but Robust Impedance Controller for Series Elastic Actuators\",\"312\":\"Robotic Cutting: Mechanics and Control of Knife Motion\",\"313\":\"A constrained control-planning strategy for redundant manipulators\",\"314\":\"Reinforcement Learning on Variable Impedance Controller for High-Precision Robotic Assembly\",\"315\":\"A Compliant and Precise Pneumatic Rotary Drive Using Pneumatic Artificial Muscles in a Swash Plate Design\",\"316\":\"Passivity based Control of Antagonistic Tendon-Driven Mechanism\",\"317\":\"Exact Modal Characterization of the Non Conservative Non Linear Radial Mass Spring System\",\"318\":\"Body Lift and Drag for a Legged Millirobot in Compliant Beam Environment\",\"319\":\"Tightly Coupled 3D Lidar Inertial Odometry and Mapping\",\"320\":\"Expectation-Maximization for Adaptive Mixture Models in Graph Optimization\",\"321\":\"Multi-Camera Visual-Inertial Navigation with Online Intrinsic and Extrinsic Calibration\",\"322\":\"Joint Inference of Kinematic and Force Trajectories with Visuo-Tactile Sensing\",\"323\":\"Every Hop is an Opportunity: Quickly Classifying and Adapting to Terrain During Targeted Hopping\",\"324\":\"Semiparametrical Gaussian Processes Learning of Forward Dynamical Models for Navigating in a Circular Maze\",\"325\":\"Semantic Predictive Control for Explainable and Efficient Policy Learning\",\"326\":\"Adaptive Variance for Changing Sparse-Reward Environments\",\"327\":\"Combining Physical Simulators and Object-Based Networks for Control\",\"328\":\"Using Data-Driven Domain Randomization to Transfer Robust Control Policies to Mobile Robots\",\"329\":\"Coordinating multi-robot systems through environment partitioning for adaptive informative sampling\",\"330\":\"A Fleet of Miniature Cars for Experiments in Cooperative Driving\",\"331\":\"Multi-robot Informative Path Planning with Continuous Connectivity Constraints\",\"332\":\"Sensor Coverage Control Using Robots Constrained to a Curve\",\"333\":\"Coverage of an Environment Using Energy-Constrained Unmanned Aerial Vehicles\",\"334\":\"Point Cloud Compression for 3D LiDAR Sensor using Recurrent Neural Network with Residual Blocks\",\"335\":\"Depth Completion with Deep Geometry and Context Guidance\",\"336\":\"Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera\",\"337\":\"In-hand Object Scanning via RGB-D Video Segmentation\",\"338\":\"Multi-Modal Generative Models for Learning Epistemic Active Sensing\",\"339\":\"Decentralized Formation Coordination of Multiple Quadcopters under Communication Constraints\",\"340\":\"Online Plan Repair in Multi-robot Coordination with Disturbances\",\"341\":\"Integrated Mapping and Path Planning for Very Large-Scale Robotic (VLSR) Systems\",\"342\":\"Methodology of Designing Multi-agent Robot Control Systems Utilising Hierarchical Petri Nets\",\"343\":\"Interaction-Aware Multi-Agent Reinforcement Learning for Mobile Agents with Individual Goals\",\"344\":\"Coverage Control for Multiple Event Types with Heterogeneous Robots\",\"345\":\"Active Perception in Adversarial Scenarios using Maximum Entropy Deep Reinforcement Learning\",\"346\":\"A Competitive Algorithm for Online Multi-Robot Exploration of a Translating Plume\",\"347\":\"Online Estimation of Ocean Current from Sparse GPS Data for Underwater Vehicles\",\"348\":\"Working towards Adaptive Sensing for Terrain-aided Navigation\",\"349\":\"Non-Gaussian SLAM utilizing Synthetic Aperture Sonar\",\"350\":\"Easily Deployable Underwater Acoustic Navigation System for Multi-Vehicle Environmental Sampling Applications\",\"351\":\"Underwater Terrain Reconstruction from Forward-Looking Sonar Imagery\",\"352\":\"Detect in RGB, Optimize in Edge: Accurate 6D Pose Estimation for Texture-less Industrial Parts\",\"353\":\"POSEAMM: A Unified Framework for Solving Pose Problems using an Alternating Minimization Method\",\"354\":\"Learning Object Localization and 6D Pose Estimation from Simulation and Weakly Labeled Real Images\",\"355\":\"STAMPEDE: A Discrete-Optimization Method for Solving Pathwise-Inverse Kinematics\",\"356\":\"Reconstructing Human Hand Pose and Configuration using a Fixed-Base Exoskeleton\",\"357\":\"Learning Pose Estimation for High-Precision Robotic Assembly Using Simulated Depth Images\",\"358\":\"Aided Inertial Navigation: Unified Feature Representations and Observability Analysis\",\"359\":\"A Linear-Complexity EKF for Visual-Inertial Navigation with Loop Closures\",\"360\":\"Sensor-Failure-Resilient Multi-IMU Visual-Inertial Navigation\",\"361\":\"Learning Monocular Visual Odometry through Geometry-Aware Curriculum Learning\",\"362\":\"Visual-Odometric Localization and Mapping for Ground Vehicles Using SE(2)-XYZ Constraints\",\"363\":\"Keyframe-based Direct Thermal\\u2013Inertial Odometry\",\"364\":\"On Parameter Estimation of Space Manipulator Systems with Flexible Joints Using the Energy Balance\",\"365\":\"Central Pattern Generators Control of Momentum Driven Compliant Structures\",\"366\":\"Contact-Event-Triggered Mode Estimation for Dynamic Rigid Body Impedance-Controlled Capture\",\"367\":\"Leveraging Contact Forces for Learning to Grasp\",\"368\":\"Learning Latent Space Dynamics for Tactile Servoing\",\"369\":\"PointNetGPD: Detecting Grasp Configurations from Point Sets\",\"370\":\"Learning Deep Visuomotor Policies for Dexterous Hand Manipulation\",\"371\":\"Learning to Identify Object Instances by Touch: Tactile Recognition via Multimodal Matching\",\"372\":\"Dexterous Manipulation with Deep Reinforcement Learning: Efficient, General, and Low-Cost\",\"373\":\"Inkjet Printable Actuators and Sensors for Soft-bodied Crawling Robots\",\"374\":\"Design and Evaluation of an Energy-Saving Drive for a Versatile Robotic Gripper\",\"375\":\"Generative Deformation: Procedural Perforation for Elastic Structures\",\"376\":\"Robotics Education and Research at Scale: A Remotely Accessible Robotics Development Platform\",\"377\":\"Automated Seedling Height Assessment for Tree Nurseries Using Point Cloud Processing\",\"378\":\"Adsorption Pad using Capillary Force for Uneven Surface\",\"379\":\"Effects of Foot Stiffness and Damping on Walking Robot Performance\",\"380\":\"Dynamic Walking on Slippery Surfaces : Demonstrating Stable Bipedal Gaits with Planned Ground Slippage\",\"381\":\"Torque and velocity controllers to perform jumps with a humanoid robot: theory and implementation on the iCub robot\",\"382\":\"Safe Adaptive Switching among Dynamical Movement Primitives: Application to 3D Limit-Cycle Walkers\",\"383\":\"Interactive Open-Ended Object, Affordance and Grasp Learning for Robotic Manipulation\",\"384\":\"A parallel low-impedance sensing approach for highly responsive physical human-robot interaction\",\"385\":\"Safe Human Robot Cooperation in Task Performed on the Shared Load\",\"386\":\"A Multi-modal Sensor Array for Safe Human-Robot Interaction and Mapping\",\"387\":\"Dynamic Primitives in Human Manipulation of Non-Rigid Objects\",\"388\":\"State Estimation in Contact-Rich Manipulation\",\"389\":\"Improved Proximity, Contact, and Force Sensing via Optimization of Elastomer-Air Interface Geometry\",\"390\":\"Improving Haptic Adjective Recognition with Unsupervised Feature Learning\",\"391\":\"Tactile Mapping and Localization from High-Resolution Tactile Imprints\",\"392\":\"Maintaining Grasps within Slipping Bounds by Monitoring Incipient Slip\",\"393\":\"Road Detection through CRF based LiDAR-Camera Fusion\",\"394\":\"Semantic mapping extension for OpenStreetMap applied to indoor robot navigation\",\"395\":\"Adaptive Probabilistic Vehicle Trajectory Prediction Through Physically Feasible Bayesian Recurrent Neural Network\",\"396\":\"Optimizing Vehicle Distributions and Fleet Sizes for Shared Mobility-on-Demand\",\"397\":\"Global Vision-Based Reconstruction of Three-Dimensional Road Surfaces Using Adaptive Extended Kalman Filter\",\"398\":\"Autonomous Tissue Manipulation via Surgical Robot Using Learning Based Model Predictive Control\",\"399\":\"Robotic Control of a Multi-Modal Rigid Endoscope Combining Optical Imaging with All-Optical Ultrasound\",\"400\":\"Enabling Technology for Safe Robot-Assisted Retinal Surgery: Early Warning for Unsafe Scleral Force\",\"401\":\"Robotic bronchoscopy drive mode of the Auris Monarch platform\",\"402\":\"Using comanipulation with active force feedback to undistort stiffness perception in laparoscopy\",\"403\":\"Stiffness-Tuneable Limb Segment with Flexible Spine for Malleable Robots\",\"404\":\"A Reconfigurable Variable Stiffness Manipulator by a Sliding Layer Mechanism\",\"405\":\"A Novel Variable Stiffness Actuator Based on Pneumatic Actuation and Supercoiled Polymer Artificial Muscles\",\"406\":\"A Novel Iterative Learning Model Predictive Control Method for Soft Bending Actuators\",\"407\":\"Design and Experimental Validation of a 2DOF Sidestick Powered by Hyper-Redundant Magnetorheological Actuators Providing Active Feedback\",\"408\":\"A Lightweight Force-Controllable Wearable Arm Based on Magnetorheological-Hydrostatic Actuators\",\"409\":\"Optical Force Sensing In Minimally Invasive Robotic Surgery\",\"410\":\"Mechanical Framework Design with Experimental Verification of a Wearable Exoskeleton Chair\",\"411\":\"KO-Fusion: Dense Visual SLAM with Tightly-Coupled Kinematic and Odometric Tracking\",\"412\":\"Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source\",\"413\":\"DeepFusion: Real-Time Dense 3D Reconstruction for Monocular SLAM using Single-View Depth and Gradient Predictions\",\"414\":\"Dynamic Hilbert Maps: Real-Time Occupancy Predictions in Changing Environments\",\"415\":\"Evaluating the Effectiveness of Perspective Aware Planning with Panoramas\",\"416\":\"Actively Improving Robot Navigation On Different Terrains Using Gaussian Process Mixture Models\",\"417\":\"Continuous Occupancy Map Fusion with Fast Bayesian Hilbert Maps\",\"418\":\"Fault-tolerant Flight Control of a VTOL Tailsitter UAV\",\"419\":\"Modeling and Control of a Passively-Coupled Tilt-Rotor Vertical Takeoff and Landing Aircraft\",\"420\":\"Power-Minimizing Control of a Variable-Pitch Propulsion System for Versatile Unmanned Aerial Vehicles\",\"421\":\"Rapid Inertial Reorientation of an Aerial Insect-sized Robot Using a Piezo-actuated Tail\",\"422\":\"Contact\\u2013based Navigation Path Planning for Aerial Robots\",\"423\":\"Cargo Transportation Strategy using T3-Multirotor UAV\",\"424\":\"Experimental Learning of a Lift-Maximizing Central Pattern Generator for a Flapping Robotic Wing\",\"425\":\"Toward Lateral Aerial Grasping & Manipulation Using Scalable Suction\",\"426\":\"Design and Implementation of Computer Vision based In-Row Weeding System\",\"427\":\"LSTM-based Network for Human Gait Stability Prediction in an Intelligent Robotic Rollator\",\"428\":\"Urban Swarms: A new approach for autonomous waste management\",\"429\":\"Automated Aortic Pressure Regulation in ex vivo Heart Perfusion\",\"430\":\"A Multi-Vehicle Trajectories Generator to Simulate Vehicle-to-Vehicle Encountering Scenarios\",\"431\":\"Deep n-Shot Transfer Learning for Tactile Material Classification with a Flexible Pressure-Sensitive Skin\",\"432\":\"Towards Effective Tactile Identification of Textures using a Hybrid Touch Approach\",\"433\":\"\\u201cTouching to See\\u201d and \\u201cSeeing to Feel\\u201d: Robotic Cross-modal Sensory Data Generation for Visual-Tactile Perception\",\"434\":\"Shear-invariant Sliding Contact Perception with a Soft Tactile Sensor\",\"435\":\"Soft tactile sensing: retrieving force, torque and contact point information from deformable surfaces\",\"436\":\"Miniaturization of multistage high dynamic range six-axis force sensor composed of resin material\",\"437\":\"Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots\",\"438\":\"The Doctor will See You Now: Could a Robot Be a medical Receptionist?\",\"439\":\"Designing a Personality-Driven Robot for a Human-Robot Interaction Scenario\",\"440\":\"How Shall I Drive? Interaction Modeling and Motion Planning towards Empathetic and Socially-Graceful Driving\",\"441\":\"Detection-by-Localization: Maintenance-Free Change Object Detector\",\"442\":\"Customized Object Recognition and Segmentation by One Shot Learning with Human Robot Interaction\",\"443\":\"SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data\",\"444\":\"Object Classification Based on Unsupervised Learned Multi-Modal Features For Overcoming Sensor Failures\",\"445\":\"SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud\",\"446\":\"RoPose-Real: Real World Dataset Acquisition for Data-Driven Industrial Robot Arm Pose Estimation\",\"447\":\"A Framework for Self-Training Perceptual Agents in Simulated Photorealistic Environments\",\"448\":\"Fast and Precise Detection of Object Grasping Positions with Eigenvalue Templates\",\"449\":\"Improved Coverage Path Planning Using a Virtual Sensor Footprint: a Case Study on Demining\",\"450\":\"Model-Based Estimation of the Gravity-Loaded Shape and Scene Depth for a Slim 3-Actuator Continuum Robot with Monocular Visual Feedback\",\"451\":\"Design of a Modular Continuum Robot Segment for use in a General Purpose Manipulator\",\"452\":\"Reshaping Particle Configurations by Collisions with Rigid Objects\",\"453\":\"Velocity Constrained Trajectory Generation for a Collinear Mecanum Wheeled Robot\",\"454\":\"Vibration Control for Manipulators on a Translationally Flexible Base\",\"455\":\"Gaussian Processes Model-Based Control of Underactuated Balance Robots\",\"456\":\"Analysis of 3D Position Control for a Multi-Agent System of Self-Propelled Agents Steered by a Shared, Global Control Input\",\"457\":\"A Heuristic for Task Allocation and Routing of Heterogeneous Robots while Minimizing Maximum Travel Cost\",\"458\":\"Solving Methods for Multi-Robot Missions Planning with Energy Capacity Consideration\",\"459\":\"Salty-A Domain Specific Language for GR(1) Specifications and Designs\",\"460\":\"Persistent Multi-Robot Mapping in an Uncertain Environment\",\"461\":\"A Fog Robotics Approach to Deep Robot Learning: Application to Object Recognition and Grasp Planning in Surface Decluttering\",\"462\":\"Streamlines for Motion Planning in Underwater Currents\",\"463\":\"A Distributed Predictive Control Approach for Cooperative Manipulation of Multiple Underwater Vehicle Manipulator Systems\",\"464\":\"Coordinated Control of a Reconfigurable Multi-Vessel Platform: Robust Control Approach\",\"465\":\"Ambient light based depth control of underwater robotic unit aMussel\",\"466\":\"A Unified Closed-Loop Motion Planning Approach For An I-AUV In Cluttered Environment With Localization Uncertainty\",\"467\":\"A bio-robotic remora disc with attachment and detachment capabilities for reversible underwater hitchhiking\",\"468\":\"Robot Communication Via Motion: Closing the Underwater Human-Robot Interaction Loop\",\"469\":\"Three-Dimensionally Maneuverable Robotic Fish Enabled by Servo Motor and Water Electrolyser\",\"470\":\"A Multimodal Aerial Underwater Vehicle with Extended Endurance and Capabilities\",\"471\":\"Design and Experiments of a Squid-Like Aquatic-Aerial Vehicle with Soft Morphing Fins and Arms\",\"472\":\"Nonlinear Orientation Controller for a Compliant Robotic Fish Based on Asymmetric Actuation\",\"473\":\"Project AutoVision: Localization and 3D Scene Perception for an Autonomous Vehicle with a Multi-Camera System\",\"474\":\"Improving the Robustness of Visual-Inertial Extended Kalman Filtering\",\"475\":\"Towards Fully Dense Direct Filter-Based Monocular Visual-Inertial Odometry\",\"476\":\"Enhancing V-SLAM Keyframe Selection with an Efficient ConvNet for Semantic Analysis\",\"477\":\"Unsupervised Learning of Monocular Depth and Ego-Motion Using Multiple Masks\",\"478\":\"Characterizing the Effects of Reduced Gravity on Rover Wheel-Soil Interactions using Computer Vision Techniques\",\"479\":\"Adaptive H\\u221e Controller for Precise Manoeuvring of a Space Robot\",\"480\":\"Belief Space Planning for Reducing Terrain Relative Localization Uncertainty in Noisy Elevation Maps\",\"481\":\"Soil Displacement Terramechanics for Wheel-Based Trenching with a Planetary Rover\",\"482\":\"Experimental Evaluation of Teleoperation Interfaces for Cutting of Satellite Insulation\",\"483\":\"OmniDRL: Robust Pedestrian Detection using Deep Reinforcement Learning on Omnidirectional Cameras\",\"484\":\"2D3D-Matchnet: Learning To Match Keypoints Across 2D Image And 3D Point Cloud\",\"485\":\"Teaching Robots To Draw\",\"486\":\"Learning Probabilistic Multi-Modal Actor Models for Vision-Based Robotic Grasping\",\"487\":\"Self-supervised Learning for Single View Depth and Surface Normal Estimation\",\"488\":\"Learning to Drive from Simulation without Real World Labels\",\"489\":\"Fabrication and Characterization of Muscle Rings Using Circular Mould and Rotary Electrical Stimulation for Bio-Syncretic Robots\",\"490\":\"Cell Injection Microrobot Development and Evaluation in Microfluidic Chip\",\"491\":\"Orienting Oocytes using Vibrations for In-Vitro Fertilization Procedures\",\"492\":\"Vision-Based Automated Sorting of C. Elegans on a Microfluidic Device\",\"493\":\"Asymmetric Local Metric Learning with PSD Constraint for Person Re-identification\",\"494\":\"Fast and Robust 3D Person Detector and Posture Estimator for Mobile Robotic Applications\",\"495\":\"Spatiotemporal and Kinetic Gait Analysis System Based on Multisensor Fusion of Laser Range Sensor and Instrumented Insoles\",\"496\":\"Part Segmentation for Highly Accurate Deformable Tracking in Occlusions via Fully Convolutional Neural Networks\",\"497\":\"Using Variable Natural Environment Brain-Computer Interface Stimuli for Real-time Humanoid Robot Navigation\",\"498\":\"Estimating the Localizability in Tunnel-like Environments using LiDAR and UWB\",\"499\":\"Global Localization with Object-Level Semantics and Topology\",\"500\":\"Look No Deeper: Recognizing Places from Opposing Viewpoints under Varying Scene Appearance using Single-View Depth Estimation\",\"501\":\"Exploiting Trademark Databases for Robotic Object Fetching\",\"502\":\"Object Detection Approach for Robot Grasp Detection\",\"503\":\"MetaGrasp: Data Efficient Grasping by Affordance Interpreter Network\",\"504\":\"Toward Fingertip Non-Contact Material Recognition and Near-Distance Ranging for Robotic Grasping\",\"505\":\"Video-based Prediction of Hand-grasp Preshaping with Application to Prosthesis Control\",\"506\":\"Reactive Walking Based on Upper-Body Manipulability: An application to Intention Detection and Reaction\",\"507\":\"A Self-Modulated Impedance Multimodal Interaction Framework for Human-Robot Collaboration\",\"508\":\"SMT-Based Control and Feedback for Social Navigation\",\"509\":\"Safe and Efficient High Dimensional Motion Planning in Space-Time with Time Parameterized Prediction\",\"510\":\"Fast Online Segmentation of Activities from Partial Trajectories\",\"511\":\"Laparoscopy instrument tracking for single view camera and skill assessment\",\"512\":\"OffsetNet: Deep Learning for Localization in the Lung using Rendered Images\",\"513\":\"Using Augmentation to Improve the Robustness to Rotation of Deep Learning Segmentation in Robotic-Assisted Surgical Data\",\"514\":\"Deep Learning based Motion Prediction for Exoskeleton Robot Control in Upper Limb Rehabilitation\",\"515\":\"Adaptive Gait Planning for Walking Assistance Lower Limb Exoskeletons in Slope Scenarios\",\"516\":\"A Data-Driven Predictive Model of Individual-Specific Effects of FES on Human Gait Dynamics\",\"517\":\"The (Sensorized) Hand is Quicker than the Eye: Restoring Grasping Speed and Confidence for Amputees with Tactile Reflexes\",\"518\":\"Development of A Soft Power Suit for Lower Back Assistance\",\"519\":\"A new soft fingertip based on electroactive hydrogels\",\"520\":\"Open Loop Position Control of Soft Continuum Arm Using Deep Reinforcement Learning\",\"521\":\"Fast Motion Planning for High-DOF Robot Systems Using Hierarchical System Identification\",\"522\":\"Resilient Task Planning and Execution for Reactive Soft Robots\",\"523\":\"Dynamic morphological computation through damping design of soft material robots: application to under-actuated grippers\",\"524\":\"Augmented Reality Assisted Instrument Insertion and Tool Manipulation for the First Assistant in Robotic Surgery\",\"525\":\"High-Fidelity Grasping in Virtual Reality using a Glove-based System\",\"526\":\"On the role of wearable haptics for force feedback in teleimpedance control for dual-arm robotic teleoperation\",\"527\":\"CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction\",\"528\":\"A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation\",\"529\":\"MID-Fusion: Octree-based Object-Level Multi-Instance Dynamic SLAM\",\"530\":\"Surfel-Based Dense RGB-D Reconstruction With Global And Local Consistency\",\"531\":\"A-SLAM: Human in-the-loop Augmented SLAM\",\"532\":\"Balance Map Analysis as a Measure of Walking Balance Based on Pendulum-Like Leg Movements\",\"533\":\"Non-parametric Imitation Learning of Robot Motor Skills\",\"534\":\"Dynamic Stepping on Unknown Obstacles With Upper-Body Compliance and Angular Momentum Damping From the Reaction Null-Space\",\"535\":\"Efficient Humanoid Contact Planning using Learned Centroidal Dynamics Prediction\",\"536\":\"Scalable Closed-Form Trajectories for Periodic and Non-Periodic Human-Like Walking\",\"537\":\"Flying STAR, a Hybrid Crawling and Flying Sprawl Tuned Robot\",\"538\":\"Autonomous Cooperative Flight of Rigidly Attached Quadcopters\",\"539\":\"Energy Optimal Control Allocation in a Redundantly Actuated Omnidirectional UAV\",\"540\":\"Development of SAM: cable-Suspended Aerial Manipulator\",\"541\":\"The Phoenix Drone: An Open-Source Dual-Rotor Tail-Sitter Platform for Research and Education\",\"542\":\"1-Actuator 3-DoF Manipulation Using an Underactuated Mechanism with Multiple Nonparallel and Viscoelastic Passive Joints\",\"543\":\"Spline Based Curve Path Following of Underactuated Snake Robots\",\"544\":\"High-Bandwidth Control of Twisted String Actuators\",\"545\":\"TREE: A Variable Topology, Branching Continuum Robot\",\"546\":\"Model Based In Situ Calibration with Temperature compensation of 6 axis Force Torque Sensors\",\"547\":\"Whole-Body Active Compliance Control for Humanoid Robots with Robot Skin\",\"548\":\"Internal Array Electrodes Improve the Spatial Resolution of Soft Tactile Sensors Based on Electrical Resistance Tomography\",\"549\":\"Dense Tactile Force Estimation using GelSlim and inverse FEM\",\"550\":\"Pose Graph optimization for Unsupervised Monocular Visual Odometry\",\"551\":\"Probably Unknown: Deep Inverse Sensor Modelling Radar\",\"552\":\"Uncertainty-Aware Occupancy Map Prediction Using Generative Networks for Robot Navigation\",\"553\":\"Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space\",\"554\":\"Autonomous Exploration, Reconstruction, and Surveillance of 3D Environments Aided by Deep Learning\",\"555\":\"GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks\",\"556\":\"Fast Instance and Semantic Segmentation Exploiting Local Connectivity, Metric Learning, and One-Shot Detection for Robotics\",\"557\":\"Adding Cues to Binary Feature Descriptors for Visual Place Recognition\",\"558\":\"Recursive Bayesian Classification for Perception of Evolving Targets using a Gaussian Toroid Prediction Model\",\"559\":\"Large-Scale Object Mining for Object Discovery from Unlabeled Video\",\"560\":\"Goal-oriented Object Importance Estimation in On-road Driving Videos\",\"561\":\"Priming Deep Pedestrian Detection with Geometric Context\",\"562\":\"The Robust Canadian Traveler Problem Applied to Robot Routing\",\"563\":\"Improved A-search guided tree construction for kinodynamic planning\",\"564\":\"Balancing Global Exploration and Local-connectivity Exploitation with Rapidly-exploring Random disjointed-Trees\",\"565\":\"Locomotion Planning through a Hybrid Bayesian Trajectory Optimization\",\"566\":\"Dynamic Channel: A Planning Framework for Crowd Navigation\",\"567\":\"Composition of Local Potential Functions with Reflection\",\"568\":\"Analyzing Electromagnetic Actuator based on Force Analysis\",\"569\":\"A Novel Robotic System for Finishing of Freeform Surfaces\",\"570\":\"Context-Dependent Compensation Scheme to Reduce Trajectory Execution Errors for Industrial Manipulators\",\"571\":\"Identifying Feasible Workpiece Placement with Respect to Redundant Manipulator for Complex Manufacturing Tasks\",\"572\":\"Geometric Search-Based Inverse Kinematics of 7-DoF Redundant Manipulator with Multiple Joint Offsets\",\"573\":\"Design and Formal Verification of a Safe Stop Supervisor for an Automated Vehicle\",\"574\":\"Optimization-Based Terrain Analysis and Path Planning in Unstructured Environments\",\"575\":\"Pedestrian Dominance Modeling for Socially-Aware Robot Navigation\",\"576\":\"Dynamic Traffic Scene Classification with Space-Time Coherence\",\"577\":\"Towards the Design of Robotic Drivers for Full-Scale Self-Driving Racing Cars\",\"578\":\"Model-free Online Motion Adaptation for Optimal Range and Endurance of Multicopters\",\"579\":\"Multi-view Reconstruction of Wires using a Catenary Model\",\"580\":\"Real-time Optimal Planning and Model Predictive Control of a Multi-rotor with a Suspended Load\",\"581\":\"Bioinspired Direct Visual Estimation of Attitude Rates with Very Low Resolution Images using Deep Networks\",\"582\":\"Automatic Real-time Anomaly Detection for Autonomous Aerial Vehicles\",\"583\":\"Learning ad-hoc Compact Representations from Salient Landmarks for Visual Place Recognition in Underwater Environments\",\"584\":\"Finding divers with SCUBANet\",\"585\":\"Robotic Detection of Marine Litter Using Deep Visual Detection Models\",\"586\":\"A Dual-Bladder Buoyancy Engine for a Cephalopod-Inspired AUV\",\"587\":\"Uncertainty-Aware Path Planning for Navigation on Road Networks Using Augmented MDPs\",\"588\":\"Real-time Model Based Path Planning for Wheeled Vehicles\",\"589\":\"Integrity Risk-Based Model Predictive Control for Mobile Robots\",\"590\":\"What lies in the shadows? Safe and computation-aware motion planning for autonomous vehicles using intent-aware dynamic shadow regions\",\"591\":\"Dynamic Risk Density for Autonomous Navigation in Cluttered Environments without Object Detection\",\"592\":\"Deep Local Trajectory Replanning and Control for Robot Navigation\",\"593\":\"Learning from Transferable Mechanics Models: Generalizable Online Mode Detection in Underactuated Dexterous Manipulation\",\"594\":\"CARA system Architecture - A Click and Assemble Robotic Assembly System\",\"595\":\"Tool Macgyvering: Tool Construction Using Geometric Reasoning\",\"596\":\"A Framework for Robot Manipulation: Skill Formalism, Meta Learning and Adaptive Control\",\"597\":\"Open-Loop Collective Assembly Using a Light Field to Power and Control a Phototaxic Mini-Robot Swarm\",\"598\":\"A Variational Observation Model of 3D Object for Probabilistic Semantic SLAM\",\"599\":\"Learning Action Representations for Self-supervised Visual Exploration\",\"600\":\"Plug-and-Play: Improve Depth Prediction via Sparse Data Propagation\",\"601\":\"DFNet: Semantic Segmentation on Panoramic Images with Dynamic Loss Weights and Residual Fusion Block\",\"602\":\"Anytime Stereo Image Depth Estimation on Mobile Devices\",\"603\":\"Improved Generalization of Heading Direction Estimation for Aerial Filming Using Semi-Supervised Regression\",\"604\":\"Graduated Fidelity Lattices for Motion Planning under Uncertainty\",\"605\":\"Non-Parametric Informed Exploration for Sampling-Based Motion Planning\",\"606\":\"Simulated Annealing-optimized Trajectory Planning within Non-Collision Nominal Intervals for Highway Autonomous Driving\",\"607\":\"On the Impact of Uncertainty for Path Planning\",\"608\":\"Localization with Sliding Window Factor Graphs on Third-Party Maps for Automated Driving\",\"609\":\"Night-to-Day Image Translation for Retrieval-based Localization\",\"610\":\"Accurate and Efficient Self-Localization on Roads using Basic Geometric Primitives\",\"611\":\"Efficient 2D-3D Matching for Multi-Camera Visual Localization\",\"612\":\"Localizing Discriminative Visual Landmarks for Place Recognition\",\"613\":\"Beyond Point Clouds: Fisher Information Field for Active Visual Localization\",\"614\":\"Deep Reinforcement Learning of Navigation in a Complex and Crowded Environment with a Limited Field of View\",\"615\":\"Sim-to-Real Transfer Learning using Robustified Controllers in Robotic Tasks involving Complex Dynamics\",\"616\":\"Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight\",\"617\":\"Crowd-Robot Interaction: Crowd-Aware Robot Navigation With Attention-Based Deep Reinforcement Learning\",\"618\":\"Residual Reinforcement Learning for Robot Control\",\"619\":\"A Reinforcement Learning Approach for Control of a Nature-Inspired Aerial Vehicle\",\"620\":\"Formal Policy Learning from Demonstrations for Reachability Properties\",\"621\":\"Formalized Task Characterization for Human-Robot Autonomy Allocation\",\"622\":\"DoS-Resilient Multi-Robot Temporal Logic Motion Planning\",\"623\":\"Task-Based Design of Ad-hoc Modular Manipulators\",\"624\":\"SweepNet: Wide-baseline Omnidirectional Depth Estimation\",\"625\":\"3D Surface Reconstruction Using A Two-Step Stereo Matching Method Assisted with Five Projected Patterns\",\"626\":\"Real-Time Dense Mapping for Self-Driving Vehicles using Fisheye Cameras\",\"627\":\"Tightly-Coupled Aided Inertial Navigation with Point and Plane Features\",\"628\":\"FastDepth: Fast Monocular Depth Estimation on Embedded Systems\",\"629\":\"Sliding Mode Momentum Observers for Estimation of External Torques and Joint Acceleration\",\"630\":\"Discrete Layer Jamming for Safe Co-Robots\",\"631\":\"Admittance Control for Human-Robot Interaction Using an Industrial Robot Equipped with a F\\/T Sensor\",\"632\":\"Effect of Mechanical Resistance on Cognitive Conflict in Physical Human-Robot Collaboration\",\"633\":\"Lifelong Learning for Heterogeneous Multi-Modal Tasks\",\"634\":\"Magnetic-Field-Inspired Navigation for Quadcopter Robot in Unknown Environments\",\"635\":\"Human-Care Rounds Robot with Contactless Breathing Measurement\",\"636\":\"An Improved Control-Oriented Modeling of the Magnetic Field\",\"637\":\"Efficient Micro Waveguide Coupling based on Microrobotic Positioning\",\"638\":\"Assembly of Multilayered Hepatic Lobule-like Vascular Network by using Heptapole Magnetic Tweezer\",\"639\":\"Sizing the aortic annulus with a robotised, commercially available soft balloon catheter: in vitro study on idealised phantoms\",\"640\":\"Miniature Robotic Tubes with Rotational Tip-Joints as a Medical Delivery Platform\",\"641\":\"Nonlinear System Identification of Soft Robot Dynamics Using Koopman Operator Theory\",\"642\":\"Data Driven Inverse Kinematics of Soft Robots using Local Models\",\"643\":\"Modal Dynamics and Analysis of a Vertical Stretch-Retractable Continuum Manipulator with Large Deflection\",\"644\":\"ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics\",\"645\":\"A Validated Physical Model For Real-Time Simulation of Soft Robotic Snakes\",\"646\":\"SpaceBok: A Dynamic Legged Robot for Space Exploration\",\"647\":\"Mini Cheetah: A Platform for Pushing the Limits of Dynamic Quadruped Control\",\"648\":\"Optimal Leg Sequencing for a Hexapod Subject to External Forces and Slopes\",\"649\":\"Stanford Doggo: An Open-Source, Quasi-Direct-Drive Quadruped\",\"650\":\"Workspace CPG with Body Pose Control for Stable, Directed Vision during Omnidirectional Locomotion\",\"651\":\"Robotic Forceps without Position Sensors using Visual SLAM\",\"652\":\"3D Keypoint Repeatability for Heterogeneous Multi-Robot SLAM\",\"653\":\"ROVO: Robust Omnidirectional Visual Odometry for Wide-baseline Wide-FOV Camera Systems\",\"654\":\"SLAMBench 3.0: Systematic Automated Reproducible Evaluation of SLAM Systems for Robot Vision Challenges and Scene Understanding\",\"655\":\"Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation\",\"656\":\"Efficient Integrity Monitoring for KF-based Localization\",\"657\":\"High-Precision Localization Using Ground Texture\",\"658\":\"IN2LAMA: INertial Lidar Localisation And MApping\",\"659\":\"Speeding Up Iterative Closest Point Using Stochastic Gradient Descent\",\"660\":\"Energy Tank-Based Wrench\\/Impedance Control of a Fully-Actuated Hexarotor: A Geometric Port-Hamiltonian Approach\",\"661\":\"Integral Backstepping Position Control for Quadrotors in Tunnel-Like Confined Environments\",\"662\":\"Control and Configuration Planning of an Aerial Cable Towed System\",\"663\":\"Adaptive Control of Aerobatic Quadrotor Maneuvers in the Presence of Propeller-Aerodynamic-Coefficient and Torque-Latency Time-Variations\",\"664\":\"Fast Terminal Sliding Mode Super Twisting Controller For Position And Altitude Tracking of the Quadrotor\",\"665\":\"Multirotor dynamics based online scale estimation for monocular SLAM\",\"666\":\"Real Time Dense Depth Estimation by Fusing Stereo with Sparse Depth Measurements\",\"667\":\"Vision-based Control of a Quadrotor in User Proximity: Mediated vs End-to-End Learning Approaches\",\"668\":\"Parity-Based Diagnosis in UAVs: Detectability and Robustness Analyses\",\"669\":\"Exploiting a Human-Aware World Model for Dynamic Task Allocation in Flexible Human-Robot Teams\",\"670\":\"Group Surfing: A Pedestrian-Based Approach to Sidewalk Robot Navigation\",\"671\":\"Dentronics: Review, First Concepts and Pilot Study of a New Application Domain for Collaborative Robots in Dental Assistance\",\"672\":\"Activity recognition in manufacturing: The roles of motion capture and sEMG+inertial wearables in detecting fine vs. gross motion\",\"673\":\"Optimal Proactive Path Planning for Collaborative Robots in Industrial Contexts\",\"674\":\"Build your own hybrid thermal\\/EO camera for autonomous vehicle\",\"675\":\"Redundant Perception and State Estimation for Reliable Autonomous Racing\",\"676\":\"UWB\\/LiDAR Fusion For Cooperative Range-Only SLAM\",\"677\":\"Localization and Tracking of Uncontrollable Underwater Agents: Particle Filter Based Fusion of On-Body IMUs and Stationary Cameras\",\"678\":\"Steering Co-centered and Co-directional Optical and Acoustic Beams with a Water-immersible MEMS Scanning Mirror for Underwater Ranging and Communication\",\"679\":\"A Simple Adaptive Tracker with Reminiscences\",\"680\":\"Learning-driven Coarse-to-Fine Articulated Robot Tracking\",\"681\":\"Diagonally-Decoupled Direct Visual Servoing\",\"682\":\"2D LiDAR Map Prediction via Estimating Motion Flow with GRU\",\"683\":\"Robot eye-hand coordination learning by watching human demonstrations: a task function approximation approach\",\"684\":\"Vision-Based Dynamic Control of Car-Like Mobile Robots\",\"685\":\"Uncertainty Estimation for Projecting Lidar Points onto Camera Images for Moving Platforms\",\"686\":\"Modeling and Analysis of Motion Data from Dynamically Positioned Vessels for Sea State Estimation\",\"687\":\"Visual Localization at Intersections with Digital Maps\",\"688\":\"Interaction-aware Multi-agent Tracking and Probabilistic Behavior Prediction via Adversarial Learning\",\"689\":\"Model Predictive Control of Ride-sharing Autonomous Mobility-on-Demand Systems\",\"690\":\"A Hierarchical Framework for Coordinating Large-Scale Robot Networks\",\"691\":\"EasyLabel: A Semi-Automatic Pixel-wise Object Annotation Tool for Creating Robotic RGB-D Datasets\",\"692\":\"BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving\",\"693\":\"A Benchmarking Framework for Systematic Evaluation of Robotic Pick-and-Place Systems in an Industrial Grocery Setting\",\"694\":\"Characterizing Visual Localization and Mapping Datasets\",\"695\":\"Quantifying the Reality Gap in Robotic Manipulation Tasks\",\"696\":\"Are We Ready for Autonomous Drone Racing? The UZH-FPV Drone Racing Dataset\",\"697\":\"Practical guide to solve the minimum-effort problem with geometric algorithms and B-Splines\",\"698\":\"Dynamics Consensus between Centroidal and Whole-Body Models for Locomotion of Legged Robots\",\"699\":\"Adaptive Bingham Distribution Based Filter for SE (3) Estimation\",\"700\":\"GuSTO: Guaranteed Sequential Trajectory optimization via Sequential Convex Programming\",\"701\":\"Efficient Computation of Feedback Control for Equality-Constrained LQR\",\"702\":\"Mitigating energy loss in a robot hopping on a physically emulated dissipative substrate\",\"703\":\"Energy Efficient Navigation for Running Legged Robots\",\"704\":\"Force-controllable Quadruped Robot System with Capacitive-type Joint Torque Sensor\",\"705\":\"Visual Diver Recognition for Underwater Human-Robot Collaboration\",\"706\":\"An Integrated Approach to Navigation and Control in Micro Underwater Robotics using Radio-Frequency Localization\",\"707\":\"Online Utility-Optimal Trajectory Design for Time-Varying Ocean Environments\",\"708\":\"Online Continuous Mapping using Gaussian Process Implicit Surfaces\",\"709\":\"Dense 3D Visual Mapping via Semantic Simplification\",\"710\":\"Predicting the Layout of Partially Observed Rooms from Grid Maps\",\"711\":\"Dense Surface Reconstruction from Monocular Vision and LiDAR\",\"712\":\"FSMI: Fast Computation of Shannon Mutual Information for Information-Theoretic Mapping\",\"713\":\"Real-time Scalable Dense Surfel Mapping\",\"714\":\"Inferring Compact Representations for Efficient Natural Language Understanding of Robot Instructions\",\"715\":\"Improving Grounded Natural Language Understanding through Human-Robot Dialog\",\"716\":\"Prospection: Interpretable plans from language by predicting the future\",\"717\":\"Flight, Camera, Action! Using Natural Language and Mixed Reality to Control a Drone\",\"718\":\"An Interactive Scene Generation Using Natural Language\",\"719\":\"Efficient Generation of Motion Plans from Attribute-Based Natural Language Instructions Using Dynamic Constraint Mapping\",\"720\":\"Safe and Fast Path Planning in Cluttered Environment using Contiguous Free-space Partitioning\",\"721\":\"Learning from Extrapolated Corrections\",\"722\":\"Merging Position and orientation Motion Primitives\",\"723\":\"Learning Haptic Exploration Schemes for Adaptive Task Execution\",\"724\":\"Learning Motion Trajectories from Phase Space Analysis of the Demonstration\",\"725\":\"I Can See Clearly Now: Image Restoration via De-Raining\",\"726\":\"Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs\",\"727\":\"Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations\",\"728\":\"Semantic Mapping for View-Invariant Relocalization\",\"729\":\"Automatic Targeting of Plant Cells via Cell Segmentation and Robust Scene-Adaptive Tracking\",\"730\":\"Real-Time Monocular Object-Model Aware Sparse SLAM\",\"731\":\"Probabilistic Projective Association and Semantic Guided Relocalization for Dense Reconstruction\",\"732\":\"MRS-VPR: a multi-resolution sampling based global visual place recognition method\",\"733\":\"Robust low-overlap 3-D point cloud registration for outlier rejection\",\"734\":\"Generalized Controllers in POMDP Decision-Making\",\"735\":\"Continuous Value Iteration (CVI) Reinforcement Learning and Imaginary Experience Replay (IER) For Learning Multi-Goal, Continuous Action and State Space Controllers\",\"736\":\"iX-BSP: Belief Space Planning through Incremental Expectation\",\"737\":\"What am I touching? Learning to classify terrain via haptic sensing\",\"738\":\"Multi-Object Search using Object-Oriented POMDPs\",\"739\":\"Depth Generation Network: Estimating Real World Depth from Stereo and Depth Images\",\"740\":\"Multi-Task Template Matching for Object Detection, Segmentation and Pose Estimation Using Depth Images\",\"741\":\"A Clustering Approach to Categorizing 7 Degree-of-Freedom Arm Motions during Activities of Daily Living\",\"742\":\"Factored Pose Estimation of Articulated Objects using Efficient Nonparametric Belief Propagation\",\"743\":\"Domain Randomization for Active Pose Estimation\",\"744\":\"GraspFusion: Realizing Complex Motion by Learning and Fusing Grasp Modalities with Instance Segmentation\",\"745\":\"Factored Contextual Policy Search with Bayesian optimization\",\"746\":\"Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data\",\"747\":\"Probabilistic Active Filtering for Object Search in Clutter\",\"748\":\"Robust 3D Object Classification by Combining Point Pair Features and Graph Convolution\",\"749\":\"Discrete Rotation Equivariance for Point Cloud Recognition\",\"750\":\"MVX-Net: Multimodal VoxelNet for 3D Object Detection\",\"751\":\"Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data\",\"752\":\"Multi-Modal Geometric Learning for Grasping and Manipulation\",\"753\":\"Panthera: Design of a Reconfigurable Pavement Sweeping Robot\",\"754\":\"Automatic Leg Regeneration for Robot Mobility Recovery\",\"755\":\"Geometric interpretation of the general POE model for a serial-link robot via conversion into D-H parameterization\",\"756\":\"Dynamic friction model with thermal and load dependency: modeling, compensation, and external force estimation\",\"757\":\"Echinoderm Inspired Variable Stiffness Soft Actuator with Connected Ossicle Structure\",\"758\":\"Controllability pre-verification of silicone soft robots based on finite-element method\",\"759\":\"A Vacuum-driven Origami \\u201cMagic-ball\\u201d Soft Gripper\",\"760\":\"Azimuthal Shear Deformation of a Novel Soft Fiber-reinforced Rotary Pneumatic Actuator\",\"761\":\"INFORA: A Novel Inflatable Origami-based Actuator\",\"762\":\"Dynamic Period-two Gait Generation in a Hexapod Robot based on the Fixed-point Motion of a Reduced-order Model\",\"763\":\"Realizing Learned Quadruped Locomotion Behaviors through Kinematic Motion Primitives\",\"764\":\"Single-shot Foothold Selection and Constraint Evaluation for Quadruped Locomotion\",\"765\":\"Optimized Jumping on the MIT Cheetah 3 Robot\",\"766\":\"Lift Your Leg: Mechanics of Running Through Fluids\",\"767\":\"Safely Probabilistically Complete Real-Time Planning and Exploration in Unknown Environments\",\"768\":\"Handling robot constraints within a Set-Based Multi-Task Priority Inverse Kinematics Framework\",\"769\":\"Compliant Limb Sensing and Control for Safe Human-Robot Interactions\",\"770\":\"Ascento: A Two-Wheeled Jumping Robot\",\"771\":\"Path Following Controller for Differentially Driven Planar Robots with Limited Torques and Uncertain and Changing Dynamics\",\"772\":\"Nonlinear Tire Cornering Stiffness Observer for a Double Steering Off-Road Mobile Robot\",\"773\":\"Hierarchical optimization for Whole-Body Control of Wheeled Inverted Pendulum Humanoids\",\"774\":\"An Actively Controlled Variable Stiffness Structure via Layer Jamming and Pneumatic Actuation\",\"775\":\"A Floating-Piston Hydrostatic Linear Actuator and Remote-Direct-Drive 2-DOF Gripper\",\"776\":\"3D Printed Ferrofluid Based Soft Actuators\",\"777\":\"Learning Primitive Skills for Mobile Robots\",\"778\":\"Coverage Path Planning in Belief Space\",\"779\":\"Continuous Control for High-Dimensional State Spaces: An Interactive Learning Approach\",\"780\":\"A Predictive Reward Function for Human-Like Driving Based on a Transition Model of Surrounding Environment\",\"781\":\"ADAPS: Autonomous Driving Via Principled Simulations\",\"782\":\"Planning Coordinated Event Observation for Structured Narratives\",\"783\":\"Algorithmic Resolution of Multiple Impacts in Nonsmooth Mechanical Systems with Switching Constraints\",\"784\":\"Rigid Body Motion Prediction with Planar Non-convex Contact Patch\",\"785\":\"A Data-driven Approach for Fast Simulation of Robot Locomotion on Granular Media\",\"786\":\"Controller Synthesis for Discrete-time Hybrid Polynomial Systems via Occupation Measures\",\"787\":\"Optimal Path Planning for \\u03c9-regular Objectives with Abstraction-Refinement\",\"788\":\"Sampling-Based Polytopic Trees for Approximate Optimal Control of Piecewise Affine Systems\",\"789\":\"A Classification-based Approach for Approximate Reachability\",\"790\":\"Improving drone localisation around wind turbines using monocular model-based tracking\",\"791\":\"Experimental Assessment of Plume Mapping using Point Measurements from Unmanned Vehicles\",\"792\":\"Online Deep Learning for Improved Trajectory Tracking of Unmanned Aerial Vehicles Using Expert Knowledge\",\"793\":\"Decentralized collaborative transport of fabrics using micro-UAVs\",\"794\":\"Precision Stationary Flight of a Robotic Hummingbird\",\"795\":\"Robust attitude estimation using an adaptive unscented Kalman filter\",\"796\":\"One-Shot Learning of Multi-Step Tasks from Observation via Activity Localization in Auxiliary Video\",\"797\":\"LVIS: Learning from Value Function Intervals for Contact-Aware Robot Controllers\",\"798\":\"Augmenting Action Model Learning by Non-Geometric Features\",\"799\":\"Skill Acquisition via Automated Multi-Coordinate Cost Balancing\",\"800\":\"Real-time Multisensory Affordance-based Control for Adaptive Object Manipulation\",\"801\":\"Learning Behavior Trees From Demonstration\",\"802\":\"Leveraging Temporal Reasoning for Policy Selection in Learning from Demonstration\",\"803\":\"Imitating Human Search Strategies for Assembly\",\"804\":\"Active Multi-Contact Continuous Tactile Exploration with Gaussian Process Differential Entropy\",\"805\":\"Learning Robust Manipulation Skills with Guided Policy Search via Generative Motor Reflexes\",\"806\":\"Incremental Learning of Spatial-Temporal Features in Human Motion Patterns with Mixture Model for Planning Motion of a Collaborative Robot in Assembly Lines\",\"807\":\"Learning Quickly to Plan Quickly Using Modular Meta-Learning\",\"808\":\"Deep Multi-Sensory Object Category Recognition Using Interactive Behavioral Exploration\",\"809\":\"Discontinuity-Sensitive Optimal Control Learning by Mixture of Experts\",\"810\":\"Wormhole Learning\",\"811\":\"Sharing the Load: Human-Robot Team Lifting Using Muscle Activity\",\"812\":\"Position control of medical cable-driven flexible instruments by combining machine learning and kinematic analysis\",\"813\":\"Online Learning for Proactive Obstacle Avoidance with Powered Transfemoral Prostheses\",\"814\":\"Passive Dynamic Object Locomotion by Rocking and Walking Manipulation\",\"815\":\"Autonomous Latching System for Robotic Boats\",\"816\":\"Streaming Scene Maps for Co-Robotic Exploration in Bandwidth Limited Environments\",\"817\":\"UWStereoNet: Unsupervised Learning for Depth Estimation and Color Correction of Underwater Stereo Imagery\",\"818\":\"Design and Parameter Optimization of a 3-PSR Parallel Mechanism for Replicating Wave and Boat Motion\",\"819\":\"A Framework for On-line Learning of Underwater Vehicles Dynamic Models\",\"820\":\"Incorporating End-to-End Speech Recognition Models for Sentiment Analysis\",\"821\":\"Improved Optical Flow for Gesture-based Human-robot Interaction\",\"822\":\"Decentralization of Multiagent Policies by Learning What to Communicate\",\"823\":\"Acquisition of Word-Object Associations from Human-Robot and Human-Human Dialogues\",\"824\":\"Robot Object Referencing through Legible Situated Projections\",\"825\":\"Security-Aware Synthesis of Human-UAV Protocols\",\"826\":\"Underwater Communication Using Full-Body Gestures and Optimal Variable-Length Prefix Codes\",\"827\":\"WISDOM: WIreless Sensing-assisted Distributed Online Mapping\",\"828\":\"Learning Recursive Bayesian Nonparametric Modeling of Moving Targets via Mobile Decentralized Sensors\",\"829\":\"UAV\\/UGV Autonomous Cooperation: UAV assists UGV to climb a cliff by attaching a tether\",\"830\":\"Distributed Motion Tomography for Reconstruction of Flow Fields\",\"831\":\"Who Takes What: Using RGB-D Camera and Inertial Sensor for Unmanned Monitor\",\"832\":\"Sound-Indicated Visual Object Detection for Robotic Exploration\",\"833\":\"HG-DAgger: Interactive Imitation Learning with Human Experts\",\"834\":\"Proximity Human-Robot Interaction Using Pointing Gestures and a Wrist-mounted IMU\",\"835\":\"Lidar Measurement Bias Estimation via Return Waveform Modelling in a Context of 3D Mapping\",\"836\":\"An Extrinsic Calibration Tool for Radar, Camera and Lidar\",\"837\":\"Compensation of measurement noise and bias in geometric attitude estimation\",\"838\":\"Hierarchical Depthwise Graph Convolutional Neural Network for 3D Semantic Segmentation of Point Clouds\",\"839\":\"CELLO-3D: Estimating the Covariance of ICP in the Real World\",\"840\":\"Low-latency Visual SLAM with Appearance-Enhanced Local Map Building\",\"841\":\"Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities\",\"842\":\"Unsupervised Out-of-context Action Understanding\",\"843\":\"Air-to-Ground Surveillance Using Predictive Pursuit\",\"844\":\"Online Planning for Target Object Search in Clutter under Partial Observability\",\"845\":\"Learning to Drive in a Day\",\"846\":\"Generating Adversarial Driving Scenarios in High-Fidelity Simulators\",\"847\":\"Data-Driven Contact Clustering for Robot Simulation\",\"848\":\"Pavilion: Bridging Photo-Realism and Robotics\",\"849\":\"A Real-Time Interactive Augmented Reality Depth Estimation Technique for Surgical Robotics\",\"850\":\"Force-based Heterogeneous Traffic Simulation for Autonomous Vehicle Testing\",\"851\":\"Dual Refinement Network for Single-Shot Object Detection\",\"852\":\"Distant Vehicle Detection Using Radar and Vision\",\"853\":\"Customizing Object Detectors for Indoor Robots\",\"854\":\"Semi Supervised Deep Quick Instance Detection and Segmentation\",\"855\":\"Mixed Frame-\\/Event-Driven Fast Pedestrian Detection\",\"856\":\"Real-Time Vehicle Detection from Short-range Aerial Image with Compressed MobileNet\",\"857\":\"Guaranteed Active Constraints Enforcement on Point Cloud-approximated Regions for Surgical Applications\",\"858\":\"Designing an Accurate and Customizable Epidural Anesthesia Haptic Simulator\",\"859\":\"Sleeve Pneumatic Artificial Muscles for Antagonistically Actuated Joints\",\"860\":\"Sensing Shear Forces During Food Manipulation: Resolving the Trade-Off Between Range and Sensitivity\",\"861\":\"Benchmarking Resilience of Artificial Hands\",\"862\":\"CHiMP: A Contact based Hilbert Map Planner\",\"863\":\"A Novel Reconfigurable Revolute Joint with Adjustable Stiffness\",\"864\":\"A novel force sensor with zero stiffness at contact transition based on optical line generation\",\"865\":\"Hydraulically-actuated compliant revolute joint for medical robotic systems based on multimaterial additive manufacturing\",\"866\":\"Model-Based On-line Estimation of Time-Varying Nonlinear Joint Stiffness on an e-Series Universal Robots Manipulator\",\"867\":\"A Rolling Flexure Mechanism for Progressive Stiffness Actuators\",\"868\":\"Locomotion Dynamics of a Miniature Wave-Like Robot, Modeling and Experiments\",\"869\":\"Fabric Soft Poly-Limbs for Physical Assistance of Daily Living Tasks\",\"870\":\"Design of a Soft Ankle-Foot Orthosis Exosuit for Foot Drop Assistance\",\"871\":\"A Depth Camera-Based Soft Fingertip Device for Contact Region Estimation and Perception-Action Coupling\",\"872\":\"A Pipe-Climbing Soft Robot\",\"873\":\"Generation of Stealth Walking Gait on Low-friction Road Surface\",\"874\":\"Support Surface Estimation for Legged Robots\",\"875\":\"ALMA - Articulated Locomotion and Manipulation for a Torque-Controllable Robot\",\"876\":\"Real-time Model Predictive Control for Versatile Dynamic Motions in Quadrupedal Robots\",\"877\":\"Scanning the Internet for ROS: A View of Security in Robotics Research\",\"878\":\"Risk Averse Robust Adversarial Reinforcement Learning\",\"879\":\"Bounded Collision Force by the Sobolev Norm\",\"880\":\"Liability, Ethics, and Culture-Aware Behavior Specification using Rulebooks\",\"881\":\"Early Failure Detection of Deep End-to-End Control Policy by Reinforcement Learning\",\"882\":\"Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning\",\"883\":\"Trajectory Planning for a Tractor with Multiple Trailers in Extremely Narrow Environments: A Unified Approach\",\"884\":\"A Friction-Based Kinematic Model for Skid-Steer Wheeled Mobile Robots\",\"885\":\"Turning a Corner with a Dubins Car\",\"886\":\"Modeling and state estimation of a Micro Ball-balancing Robot using a high yaw-rate dynamic model and an Extended Kalman Filter\",\"887\":\"Orientation-Aware Motion Planning in Complex Workspaces using Adaptive Harmonic Potential Fields\",\"888\":\"Energy-Aware Temporal Logic Motion Planning for Mobile Robots\",\"889\":\"Using Local Experiences for Global Motion Planning\",\"890\":\"DMP Based Trajectory Tracking for a Nonholonomic Mobile Robot With Automatic Goal Adaptation and Obstacle Avoidance\",\"891\":\"Predictive Collision Avoidance for the Dynamic Window Approach\",\"892\":\"Kinematic Constraints Based Bi-directional RRT (KB-RRT) with Parameterized Trajectories for Robot Path Planning in Cluttered Environment\",\"893\":\"Predicting Vehicle Behaviors Over An Extended Horizon Using Behavior Interaction Network\",\"894\":\"Multimodal Spatio-Temporal Information in End-to-End Networks for Automotive Steering Prediction\",\"895\":\"OVPC Mesh: 3D Free-space Representation for Local Ground Vehicle Navigation\",\"896\":\"Attention-based Lane Change Prediction\",\"897\":\"Safe Reinforcement Learning With Model Uncertainty Estimates\",\"898\":\"Using DP Towards A Shortest Path Problem-Related Application\",\"899\":\"Improving dual-arm assembly by master-slave compliance\",\"900\":\"Generation of Synchronized Configuration Space Trajectories of Multi-Robot Systems\",\"901\":\"REPLAB: A Reproducible Low-Cost Arm Benchmark for Robotic Learning\",\"902\":\"Stable Bin Packing of Non-convex 3D Objects with a Robot Manipulator\",\"903\":\"A Constraint Programming Approach to Simultaneous Task Allocation and Motion Scheduling for Industrial Dual-Arm Manipulation Tasks\",\"904\":\"Self-Supervised Surgical Tool Segmentation using Kinematic Information\",\"905\":\"Needle Localization for Robot-assisted Subretinal Injection based on Deep Learning\",\"906\":\"Robust Generalized Point Set Registration using Inhomogeneous Hybrid Mixture Models via Expectation Maximization\",\"907\":\"Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing\",\"908\":\"Towards 3D Path Planning from a Single 2D Fluoroscopic Image for Robot Assisted Fenestrated Endovascular Aortic Repair\",\"909\":\"Multi-View Picking: Next-best-view Reaching for Improved Grasping in Clutter\",\"910\":\"A Multi-Sensor Next-Best-View Framework for Geometric Model-Based Robotics Applications\",\"911\":\"Model-Free Optimal Estimation and Sensor Placement Framework for Elastic Kinematic Chain\",\"912\":\"Tree Search Techniques for Minimizing Detectability and Maximizing Visibility\",\"913\":\"Chance Constrained Motion Planning for High-Dimensional Robots\",\"914\":\"Complete and Near-Optimal Path Planning for Simultaneous Sensor-Based Inspection and Footprint Coverage in Robotic Crack Filling\",\"915\":\"Approximate Stability Analysis for Drystacked Structures\",\"916\":\"User-Guided Offline Synthesis of Robot Arm Motion from 6-DoF Paths\",\"917\":\"Visual Robot Task Planning\",\"918\":\"Towards Blended Reactive Planning and Acting using Behavior Trees\",\"919\":\"Visual Representations for Semantic Target Driven Navigation\",\"920\":\"Deep Object-Centric Policies for Autonomous Driving\",\"921\":\"Neural Autonomous Navigation with Riemannian Motion Policy\",\"922\":\"Two-Stage Transfer Learning for Heterogeneous Robot Detection and 3D Joint Position Estimation in a 2D Camera Image Using CNN\",\"923\":\"3D Control of Rotating Millimeter-Scale Swimmers Through Obstacles\",\"924\":\"Automatic Optical Coherence Tomography Imaging of Stationary and Moving Eyes with a Robotically-Aligned Scanner\",\"925\":\"Online Multilayered Motion Planning with Dynamic Constraints for Autonomous Underwater Vehicles\",\"926\":\"Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks\",\"927\":\"Deep Visuo-Tactile Learning: Estimation of Tactile Properties from Images\",\"928\":\"Variational End-to-End Navigation and Localization\",\"929\":\"Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience\",\"930\":\"Robotic Orientation Control of Deformable Cells\",\"931\":\"Drift-free Roll and Pitch Estimation for High-acceleration Hopping\",\"932\":\"Efficient Symbolic Reactive Synthesis for Finite-Horizon Tasks\",\"933\":\"Combined Task and Motion Planning under Partial Observability: An Optimization-Based Approach\",\"934\":\"Towards Robust Product Packing with a Minimalistic End-Effector\",\"935\":\"Gesture Recognition Via Flexible Capacitive Touch Electrodes\",\"936\":\"Robust Learning of Tactile Force Estimation through Robot Interaction\",\"937\":\"Soft Robotic Glove with Integrated Sensing for Intuitive Grasping Assistance Post Spinal Cord Injury\",\"938\":\"Shape Sensing of Variable Stiffness Soft Robots using Electrical Impedance Tomography\",\"939\":\"Adaptive Control of Sclera Force and Insertion Depth for Safe Robot-Assisted Retinal Surgery\",\"940\":\"Distributed Multi-Robot Formation Splitting and Merging in Dynamic Environments\",\"941\":\"Eagle Shoal: A new designed modular tactile sensing dexterous hand for domestic service robots\",\"942\":\"Learning Scene Geometry for Visual Localization in Challenging Conditions\",\"943\":\"Multi-Robot Region-of-Interest Reconstruction with Dec-MCTS\",\"944\":\"Design and Control of a Passively Morphing Quadcopter\",\"945\":\"Search-based 3D Planning and Trajectory Optimization for Safe Micro Aerial Vehicle Flight Under Sensor Visibility Constraints\",\"946\":\"LineRanger: Analysis and Field Testing of an Innovative Robot for Efficient Assessment of Bundled High-Voltage Powerlines\",\"947\":\"Adjustable Power Modulation For A Leg Mechanism Suitable For Running\",\"948\":\"Fast and In Sync: Periodic Swarm Patterns for Quadrotors\",\"949\":\"Transfer Learning for Surgical Task Segmentation\",\"950\":\"Bayesian Optimization of Soft Exosuits Using a Metabolic Estimator Stopping Process\",\"951\":\"Towards Semi-Autonomous and Soft-Robotics Enabled Upper-Limb Exoprosthetics: First Concepts and Robot-Based Emulation Prototype\",\"952\":\"A Miniature Suction-Gripper With Passive and Active Microneedle Arrays to Manipulate Peripheral Nerves\",\"953\":\"Robust 3D Distributed Formation Control With Collision Avoidance And Application To Multirotor Aerial Vehicles\",\"954\":\"Networked Operation of a UAV Using Gaussian Process-Based Delay Compensation and Model Predictive Control\",\"955\":\"Flappy Hummingbird: An Open Source Dynamic Simulation of Flapping Wing Robots and Animals\",\"956\":\"Visual Repetition Sampling for Robot Manipulation Planning\",\"957\":\"Contact-Driven Posture Behavior for Safe and Interactive Robot Operation\",\"958\":\"SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation\",\"959\":\"The Mechanics and Control of Leaning to Lift Heavy Objects with a Dynamically Stable Mobile Robot\",\"960\":\"Improving Underwater Obstacle Detection using Semantic Image Segmentation\",\"961\":\"RCM-SLAM: Visual localisation and mapping under remote centre of motion constraints\",\"962\":\"Comparing Physical and Simulated Performance of a Deterministic and a Bio-inspired Stochastic Foraging Strategy for Robot Swarms\",\"963\":\"WheeLeR: Wheel-Leg Reconfigurable Mechanism with Passive Gears for Mobile Robot Applications\",\"964\":\"Long-Term Occupancy Grid Prediction Using Recurrent Neural Networks\",\"965\":\"Guaranteed Globally Optimal Planar Pose Graph and Landmark SLAM via Sparse-Bounded Sums-of-Squares Programming\",\"966\":\"Trust Regions for Safe Sampling-Based Model Predictive Control\",\"967\":\"Removing Leaking Corners to Reduce Dimensionality in Hamilton-Jacobi Reachability\",\"968\":\"Control from the Cloud: Edge Computing, Services and Digital Shadow for Automation Technologies\",\"969\":\"Improving the Performance of Auxiliary Null Space Tasks via Time Scaling-Based Relaxation of the Primary Task\",\"970\":\"Shape Memory Structures-Automated Design of Monolithic Soft Robot Structures with Pre-defined End Poses\",\"971\":\"A Novel Rotating Beam Link for Variable Stiffness Robotic Arms\",\"972\":\"Mechanical Fourier Transform using an Array of Additively Manufactured Soft Whisker-like Sensors\",\"973\":\"Multi-Task Sensorization of Soft Actuators Using Prior Knowledge\",\"974\":\"Self-Modifying Morphology Experiments with DyRET: Dynamic Robot for Embodied Testing\",\"975\":\"Experimental Validation of High-Efficiency Hydraulic Direct-Drive System for a Biped Humanoid Robot\\u2014Comparison with Valve-Based Control System\",\"976\":\"Experimental Demonstration of High-Performance Robotic Balancing\",\"977\":\"OpenRoACH: A Durable Open-Source Hexapedal Platform with Onboard Robot Operating System (ROS)\",\"978\":\"Sensorless Force Control of Automated Grinding\\/Deburring Using an Adjustable force regulation mechanism\",\"979\":\"Constrained Feedback Control by Prioritized Multi-objective Optimization\",\"980\":\"Exploitation of Environment Support Contacts for Manipulation Effort Reduction of a Robot Arm\",\"981\":\"A Coordinate-based Approach for Static Balancing and Walking Control of Compliantly Actuated Legged Robots\",\"982\":\"Joint kinematic configuration influence on the passivity of an impedance-controlled robotic leg\",\"983\":\"Towards Robot Interaction Autonomy: Explore, Identify, and Interact\",\"984\":\"Personalized Online Learning of Whole-Body Motion Classes using Multiple Inertial Measurement Units\",\"985\":\"Knowledge is Never Enough: Towards Web Aided Deep Open World Recognition\",\"986\":\"Inferring Robot Morphology from Observation of Unscripted Movement\",\"987\":\"The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes\",\"988\":\"Joint Learning of Instance and Semantic Segmentation for Robotic Pick-and-Place with Heavy Occlusions in Clutter\",\"989\":\"Weakly Supervised Recognition of Surgical Gestures\",\"990\":\"Visual-Inertial Navigation: A Concise Review\",\"991\":\"Building a Winning Self-Driving Car in Six Months\",\"992\":\"Hierarchical Game-Theoretic Planning for Autonomous Vehicles\",\"993\":\"IceVisionSet: lossless video dataset collected on Russian winter roads with traffic sign annotations\",\"994\":\"Integrated UWB-Vision Approach for Autonomous Docking of UAVs in GPS-denied Environments\",\"995\":\"Online Vehicle Trajectory Prediction using Policy Anticipation Network and optimization-based Context Reasoning\",\"996\":\"A Flexible Low-Cost Biologically Inspired Sonar Sensor Platform for Robotic Applications\",\"997\":\"ClusterNav: Learning-Based Robust Navigation Operating in Cluttered Environments\",\"998\":\"Adaptive motor control and learning in a spiking neural network realised on a mixed-signal neuromorphic processor\",\"999\":\"Adaptive Genomic Evolution of Neural Network Topologies (AGENT) for State-to-Action Mapping in Autonomous Agents\",\"1000\":\"End to End Learning of a Multi-Layered Snn Based on R-Stdp for a Target Tracking Snake-Like Robot\",\"1001\":\"Improving collective decision accuracy via time-varying cross-inhibition\",\"1002\":\"A Motion Planning Scheme for Cooperative Loading Using Heterogeneous Robotic Agents\",\"1003\":\"Voluntary Retreat for Decentralized Interference Reduction in Robot Swarms\",\"1004\":\"Spatial Coverage Without Computation\",\"1005\":\"DeepSignals: Predicting Intent of Drivers Through Visual Signals\",\"1006\":\"Real-time Intent Prediction of Pedestrians for Autonomous Ground Vehicles via Spatio-Temporal DenseNet\",\"1007\":\"Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems\",\"1008\":\"Uncertainty-Aware Driver Trajectory Prediction at Urban Intersections\",\"1009\":\"Go with the Flow: Exploration and Mapping of Pedestrian Flow Patterns from Partial Observations\",\"1010\":\"Design and Analysis of A Miniature Two-Wheg Climbing Robot with Robust Internal and External Transitioning Capabilities\",\"1011\":\"Design and Implementation of CCRobot-II: a Palm-based Cable Climbing Robot for Cable-stayed Bridge Inspection\",\"1012\":\"Dynamic Modeling and Gait Analysis for Miniature Robots in the Absence of Foot Placement Control\",\"1013\":\"Memory Efficient Experience Replay for Streaming Learning\",\"1014\":\"RoboCSE: Robot Common Sense Embedding\",\"1015\":\"Neural Lander: Stable Drone Landing Control Using Learned Dynamics\",\"1016\":\"Distributional Deep Reinforcement Learning with a Mixture of Gaussians\",\"1017\":\"Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning\",\"1018\":\"DeltaMag: An Electromagnetic Manipulation System with Parallel Mobile Coils\",\"1019\":\"Surgical instrument segmentation for endoscopic vision with data fusion of cnn prediction and kinematic pose\",\"1020\":\"Pneumatically Actuated Deployable Tissue Distension Device for NOTES for Colon\",\"1021\":\"Steering a Multi-armed Robotic Sheath Using Eccentric Precurved Tubes\"},\"First and Last Author Affiliations\":{\"0\":null,\"1\":null,\"2\":null,\"3\":null,\"4\":null,\"5\":null,\"6\":null,\"7\":null,\"8\":null,\"9\":null,\"10\":null,\"11\":null,\"12\":null,\"13\":null,\"14\":null,\"15\":null,\"16\":null,\"17\":null,\"18\":null,\"19\":null,\"20\":null,\"21\":null,\"22\":null,\"23\":null,\"24\":null,\"25\":null,\"26\":null,\"27\":null,\"28\":null,\"29\":null,\"30\":null,\"31\":null,\"32\":null,\"33\":null,\"34\":null,\"35\":null,\"36\":null,\"37\":null,\"38\":null,\"39\":null,\"40\":null,\"41\":null,\"42\":null,\"43\":null,\"44\":null,\"45\":null,\"46\":null,\"47\":null,\"48\":null,\"49\":null,\"50\":null,\"51\":null,\"52\":null,\"53\":null,\"54\":null,\"55\":null,\"56\":null,\"57\":null,\"58\":null,\"59\":null,\"60\":null,\"61\":null,\"62\":null,\"63\":null,\"64\":null,\"65\":null,\"66\":null,\"67\":null,\"68\":null,\"69\":null,\"70\":null,\"71\":null,\"72\":null,\"73\":null,\"74\":null,\"75\":null,\"76\":null,\"77\":null,\"78\":null,\"79\":null,\"80\":null,\"81\":null,\"82\":null,\"83\":null,\"84\":null,\"85\":null,\"86\":null,\"87\":null,\"88\":null,\"89\":null,\"90\":null,\"91\":null,\"92\":null,\"93\":null,\"94\":null,\"95\":null,\"96\":null,\"97\":null,\"98\":null,\"99\":null,\"100\":null,\"101\":null,\"102\":null,\"103\":null,\"104\":null,\"105\":null,\"106\":null,\"107\":null,\"108\":null,\"109\":null,\"110\":null,\"111\":null,\"112\":null,\"113\":null,\"114\":null,\"115\":null,\"116\":null,\"117\":null,\"118\":null,\"119\":null,\"120\":null,\"121\":null,\"122\":null,\"123\":null,\"124\":null,\"125\":null,\"126\":null,\"127\":null,\"128\":null,\"129\":null,\"130\":null,\"131\":null,\"132\":null,\"133\":null,\"134\":null,\"135\":null,\"136\":null,\"137\":null,\"138\":null,\"139\":null,\"140\":null,\"141\":null,\"142\":null,\"143\":null,\"144\":null,\"145\":null,\"146\":null,\"147\":null,\"148\":null,\"149\":null,\"150\":null,\"151\":null,\"152\":null,\"153\":null,\"154\":null,\"155\":null,\"156\":null,\"157\":null,\"158\":null,\"159\":null,\"160\":null,\"161\":null,\"162\":null,\"163\":null,\"164\":null,\"165\":null,\"166\":null,\"167\":null,\"168\":null,\"169\":null,\"170\":null,\"171\":null,\"172\":null,\"173\":null,\"174\":null,\"175\":null,\"176\":null,\"177\":null,\"178\":null,\"179\":null,\"180\":null,\"181\":null,\"182\":null,\"183\":null,\"184\":null,\"185\":null,\"186\":null,\"187\":null,\"188\":null,\"189\":null,\"190\":null,\"191\":null,\"192\":null,\"193\":null,\"194\":null,\"195\":null,\"196\":null,\"197\":null,\"198\":null,\"199\":null,\"200\":null,\"201\":null,\"202\":null,\"203\":null,\"204\":null,\"205\":null,\"206\":null,\"207\":null,\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":null,\"213\":null,\"214\":null,\"215\":null,\"216\":null,\"217\":null,\"218\":null,\"219\":null,\"220\":null,\"221\":null,\"222\":null,\"223\":null,\"224\":null,\"225\":null,\"226\":null,\"227\":null,\"228\":null,\"229\":null,\"230\":null,\"231\":null,\"232\":null,\"233\":null,\"234\":null,\"235\":null,\"236\":null,\"237\":null,\"238\":null,\"239\":null,\"240\":null,\"241\":null,\"242\":null,\"243\":null,\"244\":null,\"245\":null,\"246\":null,\"247\":null,\"248\":null,\"249\":null,\"250\":null,\"251\":null,\"252\":null,\"253\":null,\"254\":null,\"255\":null,\"256\":null,\"257\":null,\"258\":null,\"259\":null,\"260\":null,\"261\":null,\"262\":null,\"263\":null,\"264\":null,\"265\":null,\"266\":null,\"267\":null,\"268\":null,\"269\":null,\"270\":null,\"271\":null,\"272\":null,\"273\":null,\"274\":null,\"275\":null,\"276\":null,\"277\":null,\"278\":null,\"279\":null,\"280\":null,\"281\":null,\"282\":null,\"283\":null,\"284\":null,\"285\":null,\"286\":null,\"287\":null,\"288\":null,\"289\":null,\"290\":null,\"291\":null,\"292\":null,\"293\":null,\"294\":null,\"295\":null,\"296\":null,\"297\":null,\"298\":null,\"299\":null,\"300\":null,\"301\":null,\"302\":null,\"303\":null,\"304\":null,\"305\":null,\"306\":null,\"307\":null,\"308\":null,\"309\":null,\"310\":null,\"311\":null,\"312\":null,\"313\":null,\"314\":null,\"315\":null,\"316\":null,\"317\":null,\"318\":null,\"319\":null,\"320\":null,\"321\":null,\"322\":null,\"323\":null,\"324\":null,\"325\":null,\"326\":null,\"327\":null,\"328\":null,\"329\":null,\"330\":null,\"331\":null,\"332\":null,\"333\":null,\"334\":null,\"335\":null,\"336\":null,\"337\":null,\"338\":null,\"339\":null,\"340\":null,\"341\":null,\"342\":null,\"343\":null,\"344\":null,\"345\":null,\"346\":null,\"347\":null,\"348\":null,\"349\":null,\"350\":null,\"351\":null,\"352\":null,\"353\":null,\"354\":null,\"355\":null,\"356\":null,\"357\":null,\"358\":null,\"359\":null,\"360\":null,\"361\":null,\"362\":null,\"363\":null,\"364\":null,\"365\":null,\"366\":null,\"367\":null,\"368\":null,\"369\":null,\"370\":null,\"371\":null,\"372\":null,\"373\":null,\"374\":null,\"375\":null,\"376\":null,\"377\":null,\"378\":null,\"379\":null,\"380\":null,\"381\":null,\"382\":null,\"383\":null,\"384\":null,\"385\":null,\"386\":null,\"387\":null,\"388\":null,\"389\":null,\"390\":null,\"391\":null,\"392\":null,\"393\":null,\"394\":null,\"395\":null,\"396\":null,\"397\":null,\"398\":null,\"399\":null,\"400\":null,\"401\":null,\"402\":null,\"403\":null,\"404\":null,\"405\":null,\"406\":null,\"407\":null,\"408\":null,\"409\":null,\"410\":null,\"411\":null,\"412\":null,\"413\":null,\"414\":null,\"415\":null,\"416\":null,\"417\":null,\"418\":null,\"419\":null,\"420\":null,\"421\":null,\"422\":null,\"423\":null,\"424\":null,\"425\":null,\"426\":null,\"427\":null,\"428\":null,\"429\":null,\"430\":null,\"431\":null,\"432\":null,\"433\":null,\"434\":null,\"435\":null,\"436\":null,\"437\":null,\"438\":null,\"439\":null,\"440\":null,\"441\":null,\"442\":null,\"443\":null,\"444\":null,\"445\":null,\"446\":null,\"447\":null,\"448\":null,\"449\":null,\"450\":null,\"451\":null,\"452\":null,\"453\":null,\"454\":null,\"455\":null,\"456\":null,\"457\":null,\"458\":null,\"459\":null,\"460\":null,\"461\":null,\"462\":null,\"463\":null,\"464\":null,\"465\":null,\"466\":null,\"467\":null,\"468\":null,\"469\":null,\"470\":null,\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":null,\"476\":null,\"477\":null,\"478\":null,\"479\":null,\"480\":null,\"481\":null,\"482\":null,\"483\":null,\"484\":null,\"485\":null,\"486\":null,\"487\":null,\"488\":null,\"489\":null,\"490\":null,\"491\":null,\"492\":null,\"493\":null,\"494\":null,\"495\":null,\"496\":null,\"497\":null,\"498\":null,\"499\":null,\"500\":null,\"501\":null,\"502\":null,\"503\":null,\"504\":null,\"505\":null,\"506\":null,\"507\":null,\"508\":null,\"509\":null,\"510\":null,\"511\":null,\"512\":null,\"513\":null,\"514\":null,\"515\":null,\"516\":null,\"517\":null,\"518\":null,\"519\":null,\"520\":null,\"521\":null,\"522\":null,\"523\":null,\"524\":null,\"525\":null,\"526\":null,\"527\":null,\"528\":null,\"529\":null,\"530\":null,\"531\":null,\"532\":null,\"533\":null,\"534\":null,\"535\":null,\"536\":null,\"537\":null,\"538\":null,\"539\":null,\"540\":null,\"541\":null,\"542\":null,\"543\":null,\"544\":null,\"545\":null,\"546\":null,\"547\":null,\"548\":null,\"549\":null,\"550\":null,\"551\":null,\"552\":null,\"553\":null,\"554\":null,\"555\":null,\"556\":null,\"557\":null,\"558\":null,\"559\":null,\"560\":null,\"561\":null,\"562\":null,\"563\":null,\"564\":null,\"565\":null,\"566\":null,\"567\":null,\"568\":null,\"569\":null,\"570\":null,\"571\":null,\"572\":null,\"573\":null,\"574\":null,\"575\":null,\"576\":null,\"577\":null,\"578\":null,\"579\":null,\"580\":null,\"581\":null,\"582\":null,\"583\":null,\"584\":null,\"585\":null,\"586\":null,\"587\":null,\"588\":null,\"589\":null,\"590\":null,\"591\":null,\"592\":null,\"593\":null,\"594\":null,\"595\":null,\"596\":null,\"597\":null,\"598\":null,\"599\":null,\"600\":null,\"601\":null,\"602\":null,\"603\":null,\"604\":null,\"605\":null,\"606\":null,\"607\":null,\"608\":null,\"609\":null,\"610\":null,\"611\":null,\"612\":null,\"613\":null,\"614\":null,\"615\":null,\"616\":null,\"617\":null,\"618\":null,\"619\":null,\"620\":null,\"621\":null,\"622\":null,\"623\":null,\"624\":null,\"625\":null,\"626\":null,\"627\":null,\"628\":null,\"629\":null,\"630\":null,\"631\":null,\"632\":null,\"633\":null,\"634\":null,\"635\":null,\"636\":null,\"637\":null,\"638\":null,\"639\":null,\"640\":null,\"641\":null,\"642\":null,\"643\":null,\"644\":null,\"645\":null,\"646\":null,\"647\":null,\"648\":null,\"649\":null,\"650\":null,\"651\":null,\"652\":null,\"653\":null,\"654\":null,\"655\":null,\"656\":null,\"657\":null,\"658\":null,\"659\":null,\"660\":null,\"661\":null,\"662\":null,\"663\":null,\"664\":null,\"665\":null,\"666\":null,\"667\":null,\"668\":null,\"669\":null,\"670\":null,\"671\":null,\"672\":null,\"673\":null,\"674\":null,\"675\":null,\"676\":null,\"677\":null,\"678\":null,\"679\":null,\"680\":null,\"681\":null,\"682\":null,\"683\":null,\"684\":null,\"685\":null,\"686\":null,\"687\":null,\"688\":null,\"689\":null,\"690\":null,\"691\":null,\"692\":null,\"693\":null,\"694\":null,\"695\":null,\"696\":null,\"697\":null,\"698\":null,\"699\":null,\"700\":null,\"701\":null,\"702\":null,\"703\":null,\"704\":null,\"705\":null,\"706\":null,\"707\":null,\"708\":null,\"709\":null,\"710\":null,\"711\":null,\"712\":null,\"713\":null,\"714\":null,\"715\":null,\"716\":null,\"717\":null,\"718\":null,\"719\":null,\"720\":null,\"721\":null,\"722\":null,\"723\":null,\"724\":null,\"725\":null,\"726\":null,\"727\":null,\"728\":null,\"729\":null,\"730\":null,\"731\":null,\"732\":null,\"733\":null,\"734\":null,\"735\":null,\"736\":null,\"737\":null,\"738\":null,\"739\":null,\"740\":null,\"741\":null,\"742\":null,\"743\":null,\"744\":null,\"745\":null,\"746\":null,\"747\":null,\"748\":null,\"749\":null,\"750\":null,\"751\":null,\"752\":null,\"753\":null,\"754\":null,\"755\":null,\"756\":null,\"757\":null,\"758\":null,\"759\":null,\"760\":null,\"761\":null,\"762\":null,\"763\":null,\"764\":null,\"765\":null,\"766\":null,\"767\":null,\"768\":null,\"769\":null,\"770\":null,\"771\":null,\"772\":null,\"773\":null,\"774\":null,\"775\":null,\"776\":null,\"777\":null,\"778\":null,\"779\":null,\"780\":null,\"781\":null,\"782\":null,\"783\":null,\"784\":null,\"785\":null,\"786\":null,\"787\":null,\"788\":null,\"789\":null,\"790\":null,\"791\":null,\"792\":null,\"793\":null,\"794\":null,\"795\":null,\"796\":null,\"797\":null,\"798\":null,\"799\":null,\"800\":null,\"801\":null,\"802\":null,\"803\":null,\"804\":null,\"805\":null,\"806\":null,\"807\":null,\"808\":null,\"809\":null,\"810\":null,\"811\":null,\"812\":null,\"813\":null,\"814\":null,\"815\":null,\"816\":null,\"817\":null,\"818\":null,\"819\":null,\"820\":null,\"821\":null,\"822\":null,\"823\":null,\"824\":null,\"825\":null,\"826\":null,\"827\":null,\"828\":null,\"829\":null,\"830\":null,\"831\":null,\"832\":null,\"833\":null,\"834\":null,\"835\":null,\"836\":null,\"837\":null,\"838\":null,\"839\":null,\"840\":null,\"841\":null,\"842\":null,\"843\":null,\"844\":null,\"845\":null,\"846\":null,\"847\":null,\"848\":null,\"849\":null,\"850\":null,\"851\":null,\"852\":null,\"853\":null,\"854\":null,\"855\":null,\"856\":null,\"857\":null,\"858\":null,\"859\":null,\"860\":null,\"861\":null,\"862\":null,\"863\":null,\"864\":null,\"865\":null,\"866\":null,\"867\":null,\"868\":null,\"869\":null,\"870\":null,\"871\":null,\"872\":null,\"873\":null,\"874\":null,\"875\":null,\"876\":null,\"877\":null,\"878\":null,\"879\":null,\"880\":null,\"881\":null,\"882\":null,\"883\":null,\"884\":null,\"885\":null,\"886\":null,\"887\":null,\"888\":null,\"889\":null,\"890\":null,\"891\":null,\"892\":null,\"893\":null,\"894\":null,\"895\":null,\"896\":null,\"897\":null,\"898\":null,\"899\":null,\"900\":null,\"901\":null,\"902\":null,\"903\":null,\"904\":null,\"905\":null,\"906\":null,\"907\":null,\"908\":null,\"909\":null,\"910\":null,\"911\":null,\"912\":null,\"913\":null,\"914\":null,\"915\":null,\"916\":null,\"917\":null,\"918\":null,\"919\":null,\"920\":null,\"921\":null,\"922\":null,\"923\":null,\"924\":null,\"925\":null,\"926\":null,\"927\":null,\"928\":null,\"929\":null,\"930\":null,\"931\":null,\"932\":null,\"933\":null,\"934\":null,\"935\":null,\"936\":null,\"937\":null,\"938\":null,\"939\":null,\"940\":null,\"941\":null,\"942\":null,\"943\":null,\"944\":null,\"945\":null,\"946\":null,\"947\":null,\"948\":null,\"949\":null,\"950\":null,\"951\":null,\"952\":null,\"953\":null,\"954\":null,\"955\":null,\"956\":null,\"957\":null,\"958\":null,\"959\":null,\"960\":null,\"961\":null,\"962\":null,\"963\":null,\"964\":null,\"965\":null,\"966\":null,\"967\":null,\"968\":null,\"969\":null,\"970\":null,\"971\":null,\"972\":null,\"973\":null,\"974\":null,\"975\":null,\"976\":null,\"977\":null,\"978\":null,\"979\":null,\"980\":null,\"981\":null,\"982\":null,\"983\":null,\"984\":null,\"985\":null,\"986\":null,\"987\":null,\"988\":null,\"989\":null,\"990\":null,\"991\":null,\"992\":null,\"993\":null,\"994\":null,\"995\":null,\"996\":null,\"997\":null,\"998\":null,\"999\":null,\"1000\":null,\"1001\":null,\"1002\":null,\"1003\":null,\"1004\":null,\"1005\":null,\"1006\":null,\"1007\":null,\"1008\":null,\"1009\":null,\"1010\":null,\"1011\":null,\"1012\":null,\"1013\":null,\"1014\":null,\"1015\":null,\"1016\":null,\"1017\":null,\"1018\":null,\"1019\":null,\"1020\":null,\"1021\":null},\"Keywords or Approach\":{\"0\":\"trajectory\\nlegged locomotion\\ntask analysis\\ngradient methods\\nstochastic processes\\ntraining\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nprobability\\nrobot programming\\nmoderate sample complexity\\ntrajectory-based probabilistic policy gradient\\ntrajectory-based reinforcement learning method\\ndeep latent policy gradient\\ndlpg\\npolicy function\\nprobability distribution\\ndeep latent variable model\\ncurriculum learning\\nlocomotion skills\\nsnapbot\\nfour-legged walking robot\",\"1\":\"task analysis\\nrobot sensing systems\\nplanning\\nnavigation\\nreinforcement learning\\nheuristic algorithms\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nlearned reactive planning problem\\nmotion planning policy learning\\nexecution cost\\nmotion policy\\nnavigation task\\nonline replanning\\nreactive algorithms\\ngoal location\\nrepeated task executions\\nuncertain environments\",\"2\":\"robots\\ntask analysis\\ntraining\\ncomputational modeling\\nheuristic algorithms\\ncomplexity theory\\napproximation algorithms\\nlearning (artificial intelligence)\\nmarkov processes\\noptimisation\\npath planning\\nbarc\\ninitial state distribution backwards\\nmodel-free rl algorithm\\ngoal-directed continuous control mdps\\ncurriculum strategy\\nrepresentative dynamic robotic learning problems\\ngoal-directed tasks\\nlearning signal\\nmodel-free policy optimization algorithm\\nbackward reachability curriculum\\ncurriculum generation techniques\\nrobotic reinforcement learning\\nmodel-free reinforcement learning\\nmodel-free algorithms\\nreward function\\nexploration strategies\",\"3\":\"robots\\ntrajectory\\nsafety\\nstability analysis\\nheuristic algorithms\\nneurons\\nconvergence\\ncyber-physical systems\\nfeedforward neural nets\\nfunction approximation\\ngaussian processes\\nlearning (artificial intelligence)\\nmanipulators\\nnonlinear dynamical systems\\noptimisation\\nrobot programming\\ndynamical system model\\nrobot learning applications\\nmodel learning method\\nelm learning\\ninvariance property\\ninvariant trajectories\\nbarrier certificates\\nparameter learning problem\\nactive sampling based safe identification\\nextreme learning machines\\ninfinite constraint problem\\nrobot arm\\nbarrier constraints\",\"4\":\"robot sensing systems\\ntrajectory\\nsociology\\nstatistics\\nplanning\\nadaptive control\\ncollision avoidance\\nmobile robots\\nnavigation\\nautonomous robot navigation\\nunknown dynamic obstacles\\nreal-time adaptive motion planner\\nrobot motion online\\nsensed environmental data\\nlimited sensing range\\nramp framework\\nprobabilistic model\\nunknown dynamic environment\\nsensing information\\nramp robot\\ndynamic environment changes\\nunknown ways\\nlearned probabilistic data\\nhilbert maps framework\\ndynamically unknown environment navigation\",\"5\":\"optimization\\ntraining\\ntask analysis\\nrobots\\nadaptation models\\nreinforcement learning\\nsupervised learning\\nlearning (artificial intelligence)\\nmarkov processes\\npendulums\\nvariational techniques\\nvariational policy embedding\\ntransfer reinforcement learning\\ncomplex problems\\ndeployment conditions\\ndata collection\\nsimulation training\\nq-function\\nmaster policy\\nlatent variables\\nlatent space\\nlow-dimensional space\\nsimulation-to-real transfer\\nreinforcement learning methods\\nmarkov decision processes\",\"6\":\"laser radar\\ndata models\\nthree-dimensional displays\\npipelines\\nlabeling\\nlegged locomotion\\nsolid modeling\\ndata analysis\\nimage motion analysis\\nimage recognition\\nlearning (artificial intelligence)\\nneural nets\\noptical radar\\nhuman recognition\\npoint clouds\\nground truth label\\nautomatic labeled data generation pipeline\\ndata generation environments\\nrealistic artificial data\\nautomatic labeled lidar data generation\\nprecise human model\\ndeep neural networks\",\"7\":\"task analysis\\nadaptation models\\nmotion segmentation\\neducation\\nobject segmentation\\nbenchmark testing\\ncomputer aided instruction\\nhuman-robot interaction\\nimage segmentation\\nlearning (artificial intelligence)\\nmanipulators\\nteaching\\nvideo signal processing\\nteacher-student learning paradigm\\ninteractive video object segmentation\\ngrasping affordances\\nchildren learning process\\nivos dataset\\nteaching signal\\nhuman teacher\\nunstructured environments\\nrobotics\\nincremental learning\\nlearning affordances\\nrobot manipulation\\nhuman robot interaction setting\\nteacher-student adaptation\\nadaptation method\\nmanipulation tasks\\nhri setup\\nappearance student network\\nappearance teacher network\",\"8\":\"tactile sensors\\nconvolution\\ntask analysis\\nobject recognition\\nconvolutional neural nets\\ndexterous manipulators\\nforce measurement\\nforce sensors\\nmorphology-specific convolutional neural network\\ndistributed tactile sensors\\nmultifingered hands\\nhigh-dimensional information\\ngrasping objects\\nabundant tactile information\\nms-cnn\\nallegro hand\\nuskin modules\\nconsecutive layers\\nfinger segment\\ntactile map\\nforce measurements\\njoint angle measurements\\nobject recognition rate\\ntriaxial force sensors\",\"9\":\"laser modes\\nthree-dimensional displays\\nmeasurement by laser beam\\nclustering algorithms\\nprobabilistic logic\\ncomputational modeling\\nmaximum likelihood estimation\\nfeature extraction\\nimage registration\\nimage segmentation\\nlaser ranging\\npattern clustering\\nray tracing\\nstereo image processing\\nmeasurement likelihood\\npoint-to-plane distance\\nray path information\\nmaximum likelihood approach\\nobject detection\\nmodel reconstruction\\nlaser odometry\\npoint cloud registration\\nrobotic systems\\nstrictly probabilistic method\\nagglomerative hierarchical clustering\\n3-d laser range scans\\nfinite plane extraction\",\"10\":\"neurons\\nbiological neural networks\\nrobot sensing systems\\nsynapses\\ncorrelation\\ncouplings\\nbrain\\nmanipulator dynamics\\nmobile robots\\nneurocontrollers\\nneurophysiology\\nnonlinear control systems\\nrecurrent neural nets\\nsearch problems\\nsupervised learning\\ntime-varying systems\\ninterpretable robotic control\\nnonlinear time-varying synaptic links\\nliquid time-constants dynamics\\nneuron-pair communication motifs\\ncompact neuronal network structures\\nsequential robotic tasks\\nsensory neurons\\nrecurrently-wired interneurons\\nmotor neurons\\ninterpretable dynamics\\nmobile arm robots\\nartificial neural network-based control agents\\nwiring structure\\nworm-inspired neural networks\\nliquid time-constant recurrent neural networks\\nnematode\\nc. elegans\\nsupervised-learning scheme\\nsearch-based algorithm\",\"11\":\"robot sensing systems\\ndc motors\\nnavigation\\nloading\\naerodynamics\\naerospace robotics\\nbiomimetics\\ncontrol system synthesis\\nfeedback\\nmobile robots\\npath planning\\nrobust control\\ntorque control\\nflapping-wing robot\\nwing loading feedback\\ninstantaneous wing loading\\nbio-inspired robotic flyers\\ntight space navigation\\npurdu hummingbird\\nflight stability\\nrobust controller design\",\"12\":\"prototypes\\nservice robots\\nsprings\\nsurface treatment\\nrobot kinematics\\nsurface waves\\nbiomechanics\\ndesign engineering\\nmobile robots\\ncompressed physical environments\\nrobot arms\\nrobot surface\\ncompliant surfaces\\nhabitable space\\nphysical space\\ntendon-driven robotic surface\\nherringbone pattern\\n3d-printed panels\",\"13\":\"aerodynamics\\nvehicle dynamics\\nuncertainty\\nrobots\\nadaptation models\\ntorque\\nactuators\\naerospace components\\naerospace robotics\\naircraft control\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nnonlinear control systems\\nposition control\\nrobot dynamics\\nrobot kinematics\\nstability\\nextreme aerobatic maneuvers\\nvisual stimulus\\n180-degree yaw turn\\nwingbeat frequency\\nflight control strategy\\nhybrid control policy\\nmodel-based nonlinear control\\nmodel-free reinforcement learning policy\\nhummingbird-like fast evasive maneuvers\\nextreme hummingbird maneuvers\\nflapping wing robots\\nbackward translation\\nposture stabilization\\nhummingbird robot\\nfrequency 40.0 hz\\ntime 0.2 s\",\"14\":\"simultaneous localization and mapping\\ncameras\\nthree-dimensional displays\\nfeature extraction\\nvisualization\\nrobot vision systems\\ntwo dimensional displays\\nmotion estimation\\npose estimation\\nslam (robots)\\nstereo image processing\\nkey-feature-based multiple view geometry\\nglobal map\\n3d structure\\nbundle adjustment\\nfast stereo slam\\ndirect formulation\\nlocal map\\nconstant motion model\\ndirect-based formulation\\nnovel stereo visual slam framework\\nfmd stereo slam\\nback-end process\\nstereo constraint\\nreprojection error minimization\",\"15\":\"surface reconstruction\\nimage color analysis\\nrobot sensing systems\\ngeometry\\nthree-dimensional displays\\nsurface texture\\nimage reconstruction\\ncameras\\nimage colour analysis\\nimage resolution\\nimage sensors\\nslam (robots)\\nsensor resolution\\ncolorization\\nmultiscale memory management process\\nhigh resolution global shutter camera\\nmemory management approach\\nhigh-resolution mesh-based real-time 3d reconstruction\\ndense 3d reconstructions\\nrobot applications\\ncolor resolution\\ndepth resolution\\ngeometrical fidelity\\nrgb-d based reconstructions\\nprimesense rgb-d camera\",\"16\":\"cameras\\nimage reconstruction\\nsimultaneous localization and mapping\\ndecoding\\ntraining\\ncollision avoidance\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nmobile robots\\npose estimation\\nrobot vision\\nslam (robots)\\ndepth estimation system\\ngen-slam\\ngenerative modeling\\ndeep learning based system\\nobstacle avoidance\\nmobile robot\\nconventional geometric slam\\nsingle camera\\ntopological map\\ncamera image\\ntopological location estimation\\nmonocular localization\\nmonocular simultaneous localization and mapping\",\"17\":\"image edge detection\\ncameras\\nsimultaneous localization and mapping\\noptimization\\nreal-time systems\\nmicrosoft windows\\nedge detection\\nimage colour analysis\\nimage representation\\nimage sensors\\nmotion estimation\\noptimisation\\npose estimation\\nrobot vision\\nslam (robots)\\ncamera intrinsics\\nsliding window\\nedge-based verification\\nreslam\\ncamera motions\\nrgbd sensors\\nsparse representation\\nslam pipeline\\nrobust edge-based slam system\",\"18\":\"simultaneous localization and mapping\\nmeasurement\\ntrajectory\\nplanning\\nthree-dimensional displays\\nuncertainty\\nautonomous aerial vehicles\\ncomputational complexity\\ngraph theory\\nmobile robots\\noptimisation\\npath planning\\nremotely operated vehicles\\nrobot vision\\nslam (robots)\\ngraph topology\\npose-graph simultaneous localization\\nthree-dimensional environments\\nd-optimality metrics\\nweighted node degree\\nt-optimality metric\\nsampling-based path\\ncontinuous-time trajectory optimization method\\nlarge-scale active slam problems\\nsubmap joining method\\nonline 3d active pose-graph slam\",\"19\":\"planning\\ntask analysis\\ncollision avoidance\\nmanipulator dynamics\\ngrippers\\nmobile robots\\npose estimation\\nrobot vision\\nsteering systems\\nkinodynamic manipulation planner\\ndynamic environments\\nrobot dynamics\\ntime-variant environments\\nmanipulation modeling\\nmanipulation planning\\nonline collision avoidance\\nobject pose estimation\\nsteering functions\",\"20\":\"histograms\\nplanning\\ntask analysis\\ngrasping\\nend effectors\\ncollision avoidance\\ncomputational complexity\\nmanipulators\\nmobile robots\\nnavigation\\nobject manipulation tasks\\ncluttered environments\\nrobotic manipulator\\nconstrained confined space\\ncollision-free path\\nobject rearrangement\\nnp-hard\\nservice domains\\ncollision avoidance scheme\\nmobile robot navigation\\nobject poses\\nobstacle rearrangement\\npolynomial time\",\"21\":\"planning\\ntask analysis\\nrobots\\ntrajectory\\ncontainers\\ngenerators\\ncollision avoidance\\nmanipulators\\npath planning\\nrobotic manipulation actions\\ntask constructor framework\\nblack-box planning stages\\ntask-level motion planning\\nrobotic manipulation framework\\nmoveit!\",\"22\":\"actuators\\nforce\\nfriction\\nkinematics\\nend effectors\\nforce control\\nmanipulators\\nenvironment contacts\\nserial manipulators\\nredundant serial manipulator\\nrobot configuration\\nend-effector forces\\nactuator\",\"23\":\"end effectors\\ntrajectory\\nkinematics\\ntask analysis\\ncollision avoidance\\naerospace electronics\\ngeometry\\nmanipulator kinematics\\nmobile robots\\noptimisation\\nmotion planner\\nhuman-in-the-loop manipulation\\noptimization\\nrobot operation\\ncartesian polyhedron\\nfast collision-free inverse kinematic\\njoint space polytopes\\nsingular configurations\\nconstrained manipulability polytopes\\noperator commands\",\"24\":\"planning\\ntask analysis\\ngrasping\\nrobots\\nannealing\\ntrajectory\\nmarkov processes\\niterative methods\\noptimisation\\nrobot vision\\nsearch problems\\nrobotic tabletop rearrangement system\\nhigh packing factor forces\\nsimulated pushing actions\\nvision system\\niterated local search technique\\nlarge-scale multiobject rearrangement\",\"25\":\"microstructure\\nsilicon\\ngold\\nsurface waves\\nlaser theory\\nphotothermal effects\\nbiomems\\nbubbles\\nhydrogels\\nmicrorobots\\noptothermal effects\\nmicroparticles\\noptothermally generated surface bubble\\nmanipulation technique\\nhydrogel microstructures\\nabsorptivity\\ntransmissivity\",\"26\":\"acoustics\\nrotors\\nbars\\noscillators\\nturbines\\nmicrochannels\\nacoustic streaming\\nenergy harvesting\\ngears\\nmachine tools\\nmanipulators\\nmicrofabrication\\nmicrofluidics\\nmicromachining\\npolymerisation\\npumps\\nin-situ polymerization process\\nacoustic energy harvesting\\nrobotic manipulation systems\\nmicrofluidic devices\\nmechanical power\\nmicroscale turbines\\nprogrammable projector\\nassembly steps\\nmachine components\\ncompound micromachines\",\"27\":\"laser modes\\nactuators\\nlaser beams\\nmeasurement by laser beam\\npower lasers\\nsilicon\\npredictive models\\ndisplacement measurement\\nindustrial robots\\nintegrated circuit manufacture\\nlaser beam applications\\nmicroactuators\\nmicromechanical devices\\nmicrorobots\\nsemiconductor technology\\nsilicon-on-insulator\\nchevbot\\nmicrofactory applications\\ndry environments\\nthermal microelectro mechanical actuator\\nlaser light\\nopto-thermal-mechanical energy conversion\\nopto-thermal simulation model\\nstatic displacement measurements\\ndynamic extension\\ndirectional locomotion\\nlaser power\\nactuator displacements\\nlocomotion velocity\\nsubmillimeter robot\\nlaser beam\\nuntethered microrobot\\nmicrorobot designs\\nsilicon on insulator wafer\",\"28\":\"logic gates\\nelectrodes\\ntransistors\\nnanobioscience\\nelectric potential\\nions\\nnanoscale devices\\ncapillarity\\nelemental semiconductors\\nnanoparticles\\nsilicon\\nsize measurement\\nsurface charging\\nvelocity measurement\\nnegative gate voltage\\npositive gate\\nnanodevice\\nsurface charge\\ncapillary interface\\nsolution interface\\nelectrical double layer\\nnanoparticle delivery\\nresistive pulse method\\ncit device\\ngate control ability\\ngate electrode\\nnanochannel\\nionic transport\\ncit\\ncapillary ionic transistor\\nprecise transport control\\nsi\\nnanofluidic\\ngate voltage control\\nsub 100nm\\nmicromanipulator\",\"29\":\"task analysis\\nknee\\nlegged locomotion\\nkinematics\\ntrajectory\\nfoot\\nhumanoid robots\\nmotion control\\nviscoelasticity\\ntask-space\\nmass viscoelasticity\\njoint-space viscoelasticity\\nrobust motion\\ncompliant motion\\nrvc method\\nkinematic singularity\\nknee joint torque\\nrvc capable\\nhumanoid\\nstable knee-stretched walking\\nknee-bent posture\\nresolved viscoelasticity control\",\"30\":\"legged locomotion\\noptimization\\ntrajectory\\nhumanoid robots\\nlinear programming\\ndynamics\\ncontrol nonlinearities\\nlinear systems\\nmotion control\\nnonlinear control systems\\noptimisation\\npath planning\\npendulums\\npredictive control\\nrobot dynamics\\nstep frequency\\nhierarchical optimization\\nangular momentum\\nnonlinear model predictive control\\nreactive bipedal locomotion planning\\nnonlinearities\\nwalking dynamics\\nstep time abilities\\nstep location adjustment\\ncenter of mass height variation\\nlinear inverted pendulum model\\nrobot gait generation\",\"31\":\"neural networks\\nhardware\\nlegged locomotion\\nreinforcement learning\\ntorso\\nfoot\\ncontrol system synthesis\\nlearning (artificial intelligence)\\nneurocontrollers\\ndeep reinforcement learning\\nexpert knowledge\\ndomain randomization\\nstable controllers\\nhigh-fidelity simulators\\nneural network policy\\natrias robot\\nbipedal robots\",\"32\":\"legged locomotion\\nhumanoid robots\\nrobot sensing systems\\nkinematics\\nunsupervised learning\\ndata acquisition\\ndata reduction\\npattern clustering\\nphase estimation\\nrobot dynamics\\nrobust control\\nstate estimation\\nunsupervised gait phase estimation\\nhumanoid robot walking\\ncontact detection\\nfeet contact status\\nproprioceptive sensing\\ninertial measurement unit\\ndimensionality reduction\\nfeature representation\\ngait phase dynamics\\njoint encoder\\nforce data\\ntorque data\\nclustering\\nrobustness\\nlegged robots\",\"33\":\"admittance\\nlegged locomotion\\nfoot\\nhumanoid robots\\nforce\\ntask analysis\\nend effectors\\nlinear systems\\nnonlinear control systems\\npendulums\\nquadratic programming\\nrobot dynamics\\nstability\\nstair climbing stabilization\\nhrp-4 humanoid robot\\nwhole-body admittance control\\nairbus manufacturing use-case demonstrator\\nquadratic programming-based wrench distribution\\nwhole-body admittance controller\\nwalking controller\\ndynamic stair climbing\\nwalking stabilization\\nlinear inverted pendulum tracking\\nend-effector\\ncom strategy\\ntracking performance\\nindustrial staircase\\nopen source software\\nsize 18.5 cm\",\"34\":\"legged locomotion\\nhip\\nrabbits\\nrobot kinematics\\ntraining\\ncomputational modeling\\nadaptive control\\ncontrol engineering computing\\ncontrol system synthesis\\nfeedback\\nlearning (artificial intelligence)\\nmobile robots\\npd control\\nrobust control\\nstability\\nfeedback controllers\\nbipedal robots\\nhigh-dimensional bipedal models\\nbipedal walking\\nbipedal control\\nwalking limit cycles\\nhzd framework\\npolicy learning\\nadaptive pd controller\\nstable control policy\\nrobust control policy\\nrl framework\\nrabbit robot model\\nreinforcement learning\\nlocal stability\\nhybrid zero dynamics\\nopenai gym\\nmujoco physics engine\",\"35\":\"wheels\\nkernel\\nmobile robots\\nsensors\\ngaussian processes\\ntraining\\ncameras\\ndistance measurement\\nimage filtering\\nkalman filters\\nlearning (artificial intelligence)\\nnonlinear filters\\npath planning\\nrobot vision\\nwheel odometry\\nimu errors\\nodometry techniques\\nautonomous robot navigation\\nrobust odometry system\\ncamera\\ndeep learning\\nvariational inference\\nobservation models\\nstate-space systems\\nground truth\\nwheel encoders\\nfiber optic gyro\\nekf\\nwheel speed sensors\\ninertial measurement unit\\nextended kalman filter\\ngaussian process\\nodometry estimation\\nkalman filter\",\"36\":\"robot sensing systems\\nazimuth\\nradar measurements\\nradar detection\\nfeature extraction\\nestimation\\ndistance measurement\\nglobal positioning system\\ngraph theory\\nimage matching\\nmotion estimation\\nobject detection\\nsensor fusion\\nspeckle noise\\nscan matching accuracy\\nvisual odometry\\nego-motion estimation\\nstable range objects\\nlong-range objects\\nvariable weather\\nlighting conditions\\nradar-only odometry pipeline\\nradar artifacts\\nkey point extraction\\ndata association\\ngraph matching optimization problem\",\"37\":\"monitoring\\nfeature extraction\\nsafety\\nfault detection\\nrobot sensing systems\\nkalman filters\\nfault diagnosis\\nmobile robots\\nnonlinear filters\\nconstant computation requirements\\nsequential chi-squared integrity monitoring methodology\\nextended kalman filter\\nmobile ground robots\\nopen-sky aviation applications\\nintegrity risk\\nmobile robot localization safety\\nrecursive integrity monitoring\\npreceding time window\",\"38\":\"wheels\\nsensors\\nsmoothing methods\\nglobal navigation satellite system\\ncalibration\\ndead reckoning\\nradio frequency\\nautomobiles\\ninertial navigation\\nkalman filters\\nmobile robots\\nnavigation\\npath planning\\nremotely operated vehicles\\nsensor fusion\\nstate estimation\\nrts smoothing\\nautonomous vehicles\\naccurate dead-reckoning system\\ncar-like vehicles\\ncomplementary sensors\\nredundant sensors\\nwheel encoders\\nyaw rate gyro\\nsteering wheel measurements\\nground truth\\nsmoothing scheme\\nsmoothed estimates\\nmodel parameters\\nexperimental vehicle\\npublic roads\\ndead-reckoning drift\\ncommonly used calibration method\\ndead reckoning system\\nfour-wheeled dead-reckoning model calibration\\ncomplex maneuvers\\npublic traffic\",\"39\":\"feature extraction\\nentropy\\nvisualization\\ntask analysis\\ndecoding\\nrobots\\nupper bound\\nimage recognition\\nlearning (artificial intelligence)\\nmobile robots\\nobject recognition\\nrobot vision\\ncomputer vision\\nrobotics applications\\nvpr methods\\nplace recognition performance\\nenvironmental factors\\nend-to-end conditional visual place recognition method\\nmultidomain feature learning method\\nfeature detaching module\\nenvironmental condition-related features\\nfeature learning pipeline\\nmultiseason nordland dataset\\nmultiweather gtav dataset\\nfeature robustness\\nvariant environmental conditions\",\"40\":\"cameras\\nrobot vision systems\\nthree-dimensional displays\\noptimization\\nimage reconstruction\\nimage sensors\\nmaximum likelihood estimation\\nmotion estimation\\nmotion measurement\\nnonlinear programming\\nphotometry\\npose estimation\\nasynchronous sensors\\nlow power consumption\\nphotometric 3d map\\nclassic dense 3d reconstruction algorithms\\nbioinspired vision sensors\\nvideo imaging\\noutput pixel-level intensity\\nevent-based direct camera tracking\\nnonlinear optimization\\nrobot localization\\nar-vr\\n6-dof pose tracking\\nmaximum-likelihood framework\\nevent camera motion estimation\",\"41\":\"tunneling\\ncomputer aided software engineering\\ncameras\\nrobot vision systems\\nrobot kinematics\\nlattices\\ncomputational complexity\\ncontrol engineering computing\\ndistributed control\\nevolutionary computation\\nmobile robots\\nmulti-robot systems\\nreconfigurable architectures\\nheterogeneous lattice modular robots\\nlinear operation time cost\\n2\\u00d72\\u00d72 cubic meta-module-based connected robot structure\\nheterogeneous modular robots\\nlinear heterogeneous reconfiguration\\ncubic modular robots\\nordinary heterogeneous reconfiguration\\nlinear homogeneous transformation\\nlinear heterogeneous permutation\\nsimultaneous tunneling and permutation\",\"42\":\"robots\\nsoil\\ntask analysis\\ndams\\nforce\\nactuators\\nautomation\\nconstruction equipment\\nconstruction industry\\nerosion\\nfoundations\\ngeotechnical engineering\\nhammers (machines)\\nmobile robots\\nstability\\nautonomous sheet pile\\nsoil stabilization\\nconstruction projects\\nenvironmental restoration projects\\nautonomous robot\\ncontinuous linear structures\\nvibratory hammer\\nhardware parameters\\nspray-based stabilizing agent\\nhydraulic erosion\",\"43\":\"robot kinematics\\nacceleration\\nrotors\\nforce\\ntrajectory\\nnavigation\\nautonomous aerial vehicles\\nindoor radio\\nmobile robots\\npath planning\\npose estimation\\nrobot vision\\nself-assembly\\nrelative pose estimation\\nlocal estimation\\nself-assembly process\\nexternal systems\\nmodquad-vi\\nflying modular robot\\nrobot design\\nvision-based docking method\\ndocking actions\\naerial modular system\\nvision-based self-assembling modular quadrotor\\ntemporary structures\\nindoor infrastructures\",\"44\":\"endoscopes\\nmanipulators\\ninstruments\\nmedical services\\ncameras\\ngears\\nbiological tissues\\nbiomechanics\\nmedical robotics\\ntissue traction\\nlesion\\nrubber band\\nrobotic arm\\nflexible endoscope\\nrobotic manipulations\\nintuitive hand-held controllers\\nsolo-endoscopy\\nconventional endoscope\\nrobotic endoscopy system\",\"45\":\"catheters\\nhead\\nmagnetic heads\\ntools\\nelectron tubes\\npolymers\\ncameras\\nbiomedical optical imaging\\nendoscopes\\nmedical image processing\\nmedical robotics\\ncamera module\\ntransformable catheter head structure\\nendoscopic catheter\\nlaser micromachining\\npolymer catheter\",\"46\":\"tendons\\ninstruments\\ngrippers\\nfasteners\\nsurgery\\nend effectors\\ndexterous manipulators\\nflexible manipulators\\nmedical robotics\\nmobile robots\\nprototypes\\nrolling-tip flexible instrument\\nhuman body\\nend-effector\\ninstrument prototype\\nin-place rolling motion\\nrolling-tip gripper\\nminimally invasive surgery\\nsnake-like robots\\n6 degrees-of-freedom\\ndexterity\",\"47\":\"three-dimensional displays\\nlasers\\ncameras\\ncalibration\\nsurgery\\nmeasurement by laser beam\\ncolor\\nbiomedical optical imaging\\nlaser applications in medicine\\nmedical computing\\nphantoms\\nradiation therapy\\nskin\\nplanar phantoms\\ncylindrical phantoms\\nroot-mean-square\\ncarbon dioxide laser scalpel\\nlaser scalpel system\\n3d extrinsic calibration method\\n3d triangulation sensor\\nsuperficial laser therapy applications\\ndermatological procedures\\nsurgical procedures\\ncomputer-assisted laser surgery\\nsensor system\\nautomated laser therapy\\nlaser coordinate system\\nrgb-d camera frame\",\"48\":\"task analysis\\nrobots\\nplanning\\nuncertainty\\nhandover\\ngrasping\\ndexterous manipulators\\ngrippers\\nmobile robots\\nmotion control\\npath planning\\ntelerobotics\\nintent-uncertainty-aware grasp planning\\nrobust robot assistance\\nrobot agent\\nmotion assistance\\ntarget approaching process\\nfine motion constraints\\nambiguous human motion\\nrobot hands\\nplanning techniques\\nhuman motion input\\nmultitask robot\\nintent-uncertainty-aware grasp planner\\nrobust grasp\\nambiguous human intent inference inputs\\nteleoperated robots\\nobject manipulation task\",\"49\":\"training\\npose estimation\\nthree-dimensional displays\\nneural networks\\nrobot sensing systems\\ndexterous manipulators\\nhuman-robot interaction\\nimage classification\\nlearning (artificial intelligence)\\nneurocontrollers\\nrobot vision\\ntelerobotics\\nteachnet\\nshadow dexterous hand\\nend-to-end deep neural network\\nintuitive vision-based teleoperation\\nmarkerless vision-based teleoperation\\ndexterous robotic hands\\nrobot joint angles\\nhuman hand\\nvisually similar robot hand\\nconsistency loss function\\nhuman hands\\nhuman-robot training set\\nlabeled depth images\\nsimulated depth images\\nshadow c6 robotic hand\\npairwise depth images\\nvision-based teleoperation method\",\"50\":\"computer architecture\\nsurgery\\nrobot kinematics\\nforce\\ndelays\\ncommunication channels\\ndelay systems\\nmedical robotics\\ntelerobotics\\nenergy-shared two-layer approach\\nmultimaster-multislave bilateral teleoperation systems\\ntwo-layer architecture\\nmultiarms systems\\ncommunication delay\\nenergy tank\\nsingle-master-single-slave two layer approach\\npassivity preservation\\nsurgical scenario\",\"51\":\"task analysis\\nhaptic interfaces\\njacobian matrices\\nmanipulators\\nrobot kinematics\\ncouplings\\ncontrol engineering computing\\ntelerobotics\\npassive task-prioritized shared-control teleoperation\\nrobot teleoperation\\nteleoperator capabilities shared-control methods\\npassive task-prioritized shared-control method\\nredundant robots\\ntask-prioritized control architecture\\nhaptic guidance techniques\\nshared-control framework\\nsemiautonomous telerobotic system safety\\nenergy-tanks passivity-based controller\\nsimulated slave robot\",\"52\":\"manipulators\\npayloads\\nbandwidth\\nrobot sensing systems\\ntask analysis\\nbelts\\ncontrol engineering computing\\nforce control\\nposition control\\ntelerobotics\\nuser interfaces\\nvirtual reality\\nquasidirect drive actuation\\nrobotic force-controlled manipulation\\ntelepresence\\nblue system\\nhuman environments\\nrobot training demonstrations\\n7 degree of freedom arm\\nposition-control bandwidth\\nvirtual reality based interface\\ncompliant robotic manipulation\\nmass 2.0 kg\\nfrequency 7.5 hz\\nsize 4.0 mm\",\"53\":\"delays\\nrendering (computer graphics)\\ncameras\\nstereo image processing\\nreal-time systems\\ntransforms\\nrobots\\naugmented reality\\nmedical computing\\nmedical robotics\\nsurgery\\ntelemedicine\\ntelerobotics\\nda vinci surgical system\\nsarpd\\naugmented reality predictive displays\\nstereoscopic ar predictive display\\npredictive displays\\nvisual feedback\\nteleoperated surgical robots\\nsurgical environment\\nremote operator\\nremote telesurgery\",\"54\":\"force\\ngrasping\\nactuators\\ntendons\\noptimization\\nstability criteria\\nbiomechanics\\ndexterous manipulators\\ngrippers\\nprosthetics\\nlink length ratios\\nanthropomorphic design parameters\\ngrasp planning applications\\noptimal configuration\\nheuristically evaluated optimal solutions\\npost-contact system work\\nupper limb prosthetic design\\npalm width\\njoint stiffness ratios\\ntransmission ratios\\npost-contact stability\\nconstrained optimization framework\\nsingle actuator\\nprecision grasping\\ntwo-fingered anthropomorphic hands\\nstability optimization\",\"55\":\"acceleration\\nswitches\\nvehicles\\nheuristic algorithms\\nvehicle dynamics\\nkinematics\\nlinear quadratic control\\nminimisation\\nmotion control\\nroad vehicles\\n8 dof full motion driving simulator\\nstuttgart driving simulator\\nstate-flow chart\\nkinematic vehicle movements\\nmotion driving simulator\\nadaptive motion cueing algorithm\\nadaptive linear quadratic regulated motion cueing algorithm\\nlinear quadratic error minimization\",\"56\":\"parallel robots\\nmathematical model\\nkinematics\\nzirconium\\nlegged locomotion\\nend effectors\\ncables (mechanical)\\nmanipulator kinematics\\ncable-driven parallel robot\\nsagging cables\\nsingularity analysis\\ncdpr\\nirvine model\\ncable model representation singularity\\nsingularity type\\nik singularity\\nparallel robot singularity\\ninverse kinematics\\nforward kinematics\\nrigid legs\\nparallel robot cable-driven parallel robot\\nsingularity\",\"57\":\"force\\nindexes\\nmachining\\nfasteners\\nmanipulator dynamics\\ndynamics\\nend effectors\\nforce control\\nindustrial manipulators\\ndefect identification approach\\ndriving element\\nmultiduty parallel manipulators\\nheavy-load\\n1pu+3ups parallel manipulator\\nmachining efficiency improvement\",\"58\":\"attitude control\\nvibrations\\nend effectors\\nvalves\\ndamping\\nbars\\ncables (mechanical)\\nflexible manipulators\\nmanipulator dynamics\\nsupersonic flow\\nplanar robot\\ncustom-built supersonic air thrusters\\nactive damping\\ncold-gas thrusters\\nflexible cable-driven parallel robots\\nindustry-standard pressure level\",\"59\":\"muscles\\nrobots\\noptimization\\nexoskeletons\\nelectromyography\\nbayes methods\\npneumatic systems\\nartificial limbs\\nbiomechanics\\nmedical robotics\\nmotion control\\noptimisation\\nrobot muscle synergies\\nemg-based assistive strategies\\nexoskeleton robot control\\nelectromyography-based assistive strategies\\nmultiple emg channels\\nmultidof robots\\noptimization process\\nhuman muscles\\npneumatic artificial muscle contractions\\nbayesian optimization method\\nhuman movements\\npams-driven upper-limb exoskeleton robot\\nhuman-in-theloop optimization\\nhuman-in-the-loop optimization approach\",\"60\":\"shoulder\\nexoskeletons\\nservomotors\\nrobots\\nneuromuscular\\nprototypes\\nimpedance\\nactuators\\nbiomechanics\\nhuman-robot interaction\\nmanipulator kinematics\\nmedical robotics\\npatient rehabilitation\\nlow inertia parallel actuated shoulder exoskeleton robot\\nneuromuscular property\\ndynamic movement\\nintrinsic mechanisms\\nreflexive mechanisms\\nvoluntary mechanism\\nshoulder control\\nupper limb performance\\nphysical human-robot interaction\\nspherical parallel manipulator\\ncontrol roll\\nparallel architecture\\nspeed requirement\\n4b-spm exoskeleton\\nneuromuscular mechanisms\\nshoulder joint\\nneuromuscular properties\",\"61\":\"robot sensing systems\\ntorque\\nshoulder\\nmuscles\\nexoskeletons\\ndynamics\\nbiomechanics\\nmedical robotics\\nmotion control\\nmuscle\\nneurocontrollers\\npatient rehabilitation\\npatient treatment\\nrobot dynamics\\nrobot kinematics\\nrobust control\\ntorque control\\nwearable robots\\neffort estimation\\nrobot-aided training\\nneural network\\nrobotic exoskeletons\\npost-stroke rehabilitation\\nsensorimotor impairments\\nvariable assistance\\nmovement execution\\nmovement quality\\nindividualized treatment\\nkinematic guidance\\nrobotic assistance\\nvoluntary effort\\nactive torques\\nunmodeled dynamics\\npassive neuromuscular properties\\ninvoluntary forces\\nmuscle activity\\nhigh resolution assessment tool\\ntherapy tasks\\nrobustness\\nharmony upper-body exoskeleton\\nmotor recovery evaluation\",\"62\":\"shoulder\\nactuators\\nforce\\nexoskeletons\\ncable shielding\\ntorque\\nprototypes\\nbiomechanics\\ncables (mechanical)\\nelastomers\\nmedical robotics\\npatient rehabilitation\\npneumatic actuators\\nwearable robots\\narchitectures characterization\\nfiber-reinforced elastomeric enclosures\\nfrees\\nshoulder flexion\\nforce-displacement curves\\ncable anchor movement\\nbowden cables\\nnested linear architecture\\npennate architectures\\ncable-driven shoulder exoskeleton\\nsoft pneumatic actuators\",\"63\":\"robots\\nwheels\\nforce\\ntorque\\nmedical treatment\\nsafety\\ntraining\\nend effectors\\nfriction\\nmanipulator dynamics\\nmedical robotics\\nmobile robots\\npatient rehabilitation\\npatient treatment\\nlower-limb rehabilitation robot\\nmechanical adjustment\\ndesign procedure\\nadjustable force limiting mechanism\\nlight-weight back-drivable inherently-safe robotic mechanism\\nankle rehabilitation therapies\\nankle module\\nmathematical modeling\\nexperimental validations\\ntwo-dof robotic system\\nfriction-based safety feature\",\"64\":\"containers\\nsoftware\\nlinux\\nservice robots\\ndata centers\\nmonitoring\\ncloud computing\\ncontrol engineering computing\\nmobile robots\\nobject-oriented programming\\nscheduling\\nlinux container-based scheme\\nmonitor software components\\nlinux containers\\nservice robot systems\\nresource constraints\\nprogrammable container management interface\\nresource time-sharing mechanism\\nrorg\\nresource contention\\nlong-term autonomous tour guide robot\\nrobot software system\\nsoftware processes\\nsoftware components\\ncomputer resources\\nservice robot software management\\nrobot operating system\",\"65\":\"task analysis\\nrobot kinematics\\nreal-time systems\\nlibraries\\nc++ languages\\nplanning\\ncontrol engineering computing\\nhumanoid robots\\nlegged locomotion\\nmotion control\\nrobot dynamics\\ntelerobotics\\nrobotics platforms\\nsimple auto-generated ros-based interface\\nmotion control frameworks\\nros moveit\\ncartesian trajectories\\nlocomotion tasks\\ncoman + humanoid robot\\nredundant robots\\nmotion tasks\\nmultilegged highly redundant robots\\ncartesian control framework\",\"66\":\"observers\\nmonitoring\\nreal-time systems\\nrobot sensing systems\\ntiming\\nsafety\\nfault tolerant control\\nmobile robots\\nspecification languages\\ntemporal logic\\ntimed specification\\nfault-tolerant architectures\\nautonomous robots\\nrobot developers\\nspecification language\\nreal-time evaluation\\nreal-time observers\\npast-time linear temporal logic\\npast-time ltl\\nsoftware components\\nmission patrolling\",\"67\":\"robots\\nlibraries\\nsoftware packages\\nc++ languages\\nproductivity\\nresource management\\ncomputer simulation\\ncontrol engineering computing\\nhigh level languages\\nhumanoid robots\\nprogramming languages\\nquadratic programming\\nrobot dynamics\\nrobot programming\\nreal-time control\\nhigh-level programming language\\nrobotics applications\\ntwo-language problem\\nperformance-sensitive components\\nhigh-level language\\nsoftware complexity\\njulia programming language\\nonline control\\njulia packages\\nboston dynamics atlas humanoid robot\\nquadratic-programming-based controller\",\"68\":\"planning\\nruntime\\nc++ languages\\nlibraries\\ndata structures\\nmobile robots\\ncontrol engineering computing\\ngraph theory\\nhumanoid robots\\npath planning\\nlow-power cpu\\ntemplate-based library\\ncompile-time polymorphism\\nrobot-specific motion planning code\\nrobot software\\nmotion planning problem\\ngeneral motion planning implementations\\nmotion planning graph\\ncompile-time algorithms\\nmotion planning scenarios\\nhumanoid robot\\n3d rigid-body motions\\nmotion planning framework\\nmpt\\nmotion planning templates\",\"69\":\"surface impedance\\nthree-dimensional displays\\nrobot kinematics\\ntwo dimensional displays\\nservice robots\\nsurface reconstruction\\ncomputational geometry\\ncomputer graphics\\nconformal mapping\\ndistance measurement\\nimage reconstruction\\nleast squares approximations\\nmanipulators\\nmobile robots\\nsolid modelling\\ndistortion-free robotic surface-drawing\\nrobotic pen-drawing system\\nunknown surface\\nrobotic system\\nseven-degree-of freedom manipulator\\ncontinuous surface\\nphysical canvas surface\\npoint-cloud estimation\\ndrawing surface\\n2d vector pen art\\nsurface parameterization\\nsquares conformal mapping\\ncomplicated pen drawings\\ngeneral surfaces\\nimpedance-control\\ndigital drawing\\n2d drawing\",\"70\":\"integrated circuits\\nsubstrates\\nelectrodes\\nmicroscopy\\nforce\\nlenses\\nglass\\nbioelectric phenomena\\nbiological techniques\\ncellular biophysics\\nelectrophoresis\\nlab-on-a-chip\\nmicroorganisms\\ndielectrophoresis\\nautomatic method\\n6-aminohexanoic acid\\nyeast cells\\ncell-printing microchip\\nlarge-scale cell patterns\\ncell-based assay\\npatterning cells\\nautomated cell patterning system\",\"71\":\"painting\\nrobots\\npaints\\nink\\natmospheric modeling\\nspraying\\nimage colour analysis\\nimage reconstruction\\nimage texture\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nmobile robotic painting\\nrobotic paint delivery systems\\nspray painting\\npainting tasks\\nrobotic paint commands\\ndeep learning approach\\nappearance reconstruction\",\"72\":\"insects\\ncameras\\nunmanned aerial vehicles\\nimage color analysis\\ndata acquisition\\npipelines\\nagriculture\\nautonomous aerial vehicles\\ncomputer vision\\nmobile robots\\npest control\\nrobot vision\\nmark-release-recapture technique\\ninvasive insect species migration patterns\\ncomputer vision algorithms\\ninvasive insects detection\",\"73\":\"ellipsoids\\nimage edge detection\\nsemantics\\nsimultaneous localization and mapping\\ncameras\\nshape\\nshape measurement\\nhelicopters\\nimage sequences\\nimage texture\\nmobile robots\\nobject detection\\npath planning\\nrobot vision\\nslam (robots)\\nroshan\\nobject-level mapping\\nellipsoid-based slam\\nobject surface\\nautonomous quadrotor\\nbounding box detections\\nmedian shape error\\nforward-moving camera sequence\\nplanar constraint\\nvehicle motions\\nsemantic knowledge\\nrobust object-based slam for high-speed autonomous navigation\",\"74\":\"mathematical model\\natmospheric modeling\\ngenerators\\nfault diagnosis\\ntelemetry\\nbatteries\\nunmanned aerial vehicles\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\nmavlink-enabled uavs\\nfixed-wing uavs\\nmavlink telemetry streams\\nresidual generators\\nobservable faults\\nfdi system\\nisolability analyses\\nreal-life telemetry log\\nuav crash\\nfault diagnosis framework\\nstructural analysis\\nmessage protocol\\nfault detection and isolation framework\",\"75\":\"trajectory\\nresource management\\nreal-time systems\\nneural networks\\nacceleration\\nquadratic programming\\nnonlinear optics\\ncomputational complexity\\ncontrol engineering computing\\ngradient methods\\nhelicopters\\niterative methods\\nlearning (artificial intelligence)\\nneurocontrollers\\ntrajectory control\\nhuman-machine interface\\ngradient descent method\\nsupervised neural network\\ncomputational time\\nmachine learning\\nquadcopter\\nreal-time minimum snap trajectory generation\\nsmart-tablet interface\",\"76\":\"logic gates\\ndrones\\nnavigation\\ntraining data\\ncurrent measurement\\nlayout\\nuncertainty\\nautonomous aerial vehicles\\ncollision avoidance\\nkalman filters\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\npredictive control\\nstate estimation\\nrobust flight\\npreviously-unseen race tracks\\noptimal methods\\nfast maneuvers\\nagile maneuvers\\ndynamic environments\\nimperfect sensing\\nstate estimation drift\\nhuman pilots\\nunseen track\\npractice runs\\nstate-of-the-art autonomous navigation algorithms\\nprecise metric map\\nunseen environment\\nprecise map\\nexpensive data collection\\nglobal track layout\\ncoarse gate locations\\nsingle demonstration flight\\nconvolutional network\\nclosest gates\\nextended kalman filter\\nmaximum-a-posteriori estimates\\nhigh-variance estimates\\npoor observability\\nvisible gates\\nestimated gate poses\\nmodel predictive control\\nagile flight\\nautonomous microaerial vehicles\\nautonomous drone racing\\niros 2018 autonomous drone race competition\",\"77\":\"wires\\nimage reconstruction\\ncameras\\ndetectors\\nimage resolution\\nagriculture\\nthree-dimensional displays\\naerospace computing\\naerospace safety\\naircraft\\nconvolutional neural nets\\nfeature extraction\\nhazards\\nimage segmentation\\nneural net architecture\\nobject detection\\nreal-time systems\\naircraft safety systems\\nfree hanging wires\\nwire obstacles\\nneural network architecture\\ndeep wire cnn\\nwire line segments\\nwire reconstruction\\nreal-time detections\\nwire hazards\\nwire detection\",\"78\":\"skeleton\\nestimation\\nkinematics\\nglobal navigation satellite system\\nrotors\\nkalman filters\\nforce\\npose estimation\\nsatellite navigation\\noutdoor flying\\nposture estimation framework\\nsystem modular\\ngnss module\\nekf estimates\\nthree-link aerial skeleton system\\ninertial measurement unit\\nextended kalman filtering\",\"79\":\"wind\\naircraft\\nrobot sensing systems\\nmathematical model\\natmospheric modeling\\npath planning\\npredictive models\\naerospace components\\nautonomous aerial vehicles\\nmobile robots\\nremotely operated vehicles\\nflight test results\\nmodel prediction\\nmultirotor uav\\nboustrophedon coverage path planning\\nflight path\\naerial surveys\\ncomplex concave agricultural fields\\nflight time\\nwind prediction model\\ncost function\\nwind field measurements\\nfixed wing uav\\naerial surveying\\ncoverage path planning\\nremote sensing\\nboustrophedon paths\\ntrochoids\",\"80\":\"planning\\nsearch problems\\nthree-dimensional displays\\noptimization\\ntrajectory\\nunmanned aerial vehicles\\nrobot sensing systems\\naircraft control\\nautonomous aerial vehicles\\ncollision avoidance\\ngaussian processes\\nmobile robots\\nobject detection\\nrobot vision\\ntarget occupancy\\nuav-based target search\\ngaussian process based model\\nflight time constraints\\nplanning strategy\\nobstacle-aware adaptive informative path planning algorithm\\ntarget detection\",\"81\":\"planning\\ntrajectory\\ncomputational modeling\\nvehicle dynamics\\nrobot sensing systems\\noptimization\\nautonomous aerial vehicles\\ncollision avoidance\\nmobile robots\\nsensors\\nlow-fidelity models\\nfast planner\\nplanning framework\\nagile flights\\nreplanning times\\ncluttered environments\\nmultifidelity models\\nautonomous navigation\\nreal-time localization\\nlightweight sensing\\nplanning methodologies\\nhierarchical planning architecture\\nlow-fidelity global planner\\nhigh-fidelity local planner\\nerratic behavior\\nunstable behavior\\nglobal plan\\nhigher-order dynamics\\nreal-time planning\\nsensor data\\ncollision check\\nuav\\ntime 5.0 ms to 40.0 ms\",\"82\":\"trajectory\\nplanning\\nsensors\\nthree-dimensional displays\\ncameras\\nvehicle dynamics\\ntracking\\naircraft control\\nautonomous aerial vehicles\\nclosed loop systems\\ncollision avoidance\\ninertial navigation\\nmobile robots\\nmotion control\\noptimal control\\npredictive control\\nsampling methods\\ntrajectory control\\nmotion planning\\nmotion capture systems\\nreceding horizon planning architecture\\nreactive obstacle avoidance\\nclosed-form trajectory generation method\\nspatial partitioning data structures\\nobstacle density\\nsampling-based motion planner\\nminimum-jerk trajectories\\nclosed-loop tracking\\nhigh-speed flight\\nautonomous quadrotor flights\\nurban environment\\ntrajectory planning\\nvisual-inertial navigation\\ndistance 22.0 km\",\"83\":\"mathematical model\\nsurveillance\\nstochastic processes\\nunmanned aerial vehicles\\npath planning\\nvehicle dynamics\\ncomputational modeling\\nautonomous aerial vehicles\\noptimisation\\nwildfires\\npriority maps\\nknowledge reward function\\ndynamic spreading processes\\nbushfire spreading dynamics\\nwildfire intervention\\nunmanned aerial vehicle\\noptimization framework\\nuav path planning\",\"84\":\"task analysis\\nrobots\\nsockets\\nvisualization\\ntraining\\nplugs\\nfeature extraction\\ncontrol engineering computing\\nindustrial robots\\nlearning (artificial intelligence)\\nneural nets\\nproduction engineering computing\\nvariable socket position\\nvisual control problem\\nmodel-based robotics community\\ntask geometry\\noff-the-shelf deep-rl algorithm\\nnarrow-clearance peg-insertion task\\ndeformable clip-insertion task\\ndeep reinforcement learning\\nhaptic control problem\",\"85\":\"uncertainty\\ndata aggregation\\ntask analysis\\nswitches\\nestimation\\ndata models\\nlearning (artificial intelligence)\\nmonte carlo methods\\nuncertain systems\\nuncertainty estimation method\\nuail\\nuncertainty-aware data aggregation\\ndeep imitation learning\\nstatistical uncertainties\\nautonomous agents\\ntask execution\\nsafety-critical domains\\nautonomous driving\\nuncertainty-aware imitation learning algorithm\\nend-to-end control systems\\nmonte carlo dropout\\ncontrol output\\nend-to-end systems\\ntraining data\\nprior data aggregation algorithms\\nsub-optimal states\\nsimulated driving tasks\",\"86\":\"uncertainty\\ntask analysis\\nneural networks\\ntraining\\nrobots\\nbayes methods\\nmeasurement uncertainty\\nbelief networks\\nintelligent robots\\nlearning (artificial intelligence)\\nlearning systems\\nneurocontrollers\\nbayesian neural networks\\nrobotic controllers\\nevaluation conditions\\nlearned controller\\ntesting conditions\\nhigh-dimensional simulated domains\\nreal robotic domains\\nuncertainty based solution\\nuncertainty aware learning\",\"87\":\"cameras\\ntrajectory\\nroads\\nthree-dimensional displays\\nsensors\\ntraining\\ntracking\\nlearning (artificial intelligence)\\nobject detection\\ntraffic engineering computing\\nvideo signal processing\\nuncalibrated camera\\nlearning from demonstration\\nvibe\\ntraffic intersection\\nknowledge expert\\nvideo to behaviour\\nnatural behaviour\\nreward function\\nhand-coding behaviour\\nwild\\nraw videos\\nnaturalistic behaviour\\nlfd\\nmonocular camera\\nsingle camera\\ntraffic scene\",\"88\":\"robots\\ndata models\\nplanning\\ntask analysis\\ntrajectory\\ntraining\\nnavigation\\ngradient methods\\nimage coding\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\ndata-efficient framework\\nsim-to-real transfer\\nnavigation policies\\neffective visuomotor policies\\nlearning-based system\\nmanual tuning\\nrobot operating\\ntraining process\\nleverage simulation\\noff-policy data\\ninitial image\\nlower dimensional latent state\\nplanner modules\\nmeta-learning strategy\\nadversarial domain transfer\\nsimulated environments\\nsimilarly distributed latent representation\\nfine tuning\\nencoder + planner\\nplanning performances\\nnavigation tasks\\nunlabelled random images\",\"89\":\"rails\\nconvergence\\nbiological system modeling\\ntrajectory\\ncomputational modeling\\nautonomous vehicles\\nbehavioural sciences computing\\nlearning (artificial intelligence)\\nmulti-agent systems\\ntraffic engineering computing\\nmultiagent settings\\nmultiagent imitation learning\\nhuman drivers\\nreward augmentation\\nimitation learning process\\nmultiagent reward augmented imitation learning\\ntraffic behaviors\\nimitation learning algorithms\\nhuman driving behavior modeling\\nprior knowledge specification\\nconvergence guarantees\\ndriving policies\",\"90\":\"cameras\\nimage reconstruction\\ndata models\\nnoise measurement\\nrobot vision systems\\nconvolutional neural nets\\nimage denoising\\nimage fusion\\nimage sensors\\nobject detection\\npose estimation\\nrobot vision\\nsupervised learning\\nsupervised approach\\nmodern robotic systems\\ndetailed sensor noise models\\nrobotic behavior\\nscene-dependent pixel-wise dropouts\\ndepth camera simulations\\ndata driven approach\\nconvolutional neural network\\nno-depth-return pixels\\nndp\\nground truth depth\\nnoisy depth image\\nresulting noise-free\\nnoise-free image\\ndepth sensor\\ncluttered scenes\\nuncorrupted depth images\\nnoise prediction\\nscenes reconstruction\\ncnn\\nunsupervised domain adaptation baselines\\nobject pose estimation\\nnoise-free depth image\\nlabel fusion dataset\",\"91\":\"robot sensing systems\\ncomputers\\noptimization\\nqubit\\nacceleration\\ncloud computing\\nintelligent robots\\nlearning (artificial intelligence)\\nquantum computing\\nrobot programming\\ncloud services\\nartificial intelligence\\nmachine learning\\nrobotic scientists\\nquantum mechanics\\nquantum computation\\npowerful robots\",\"92\":\"task analysis\\ntrajectory\\noptimization\\ndynamics\\nsupervised learning\\ncomputational modeling\\nspace exploration\\nassembling\\noptimisation\\nproduction engineering computing\\nlearning framework\\nhigh precision industrial assembly\\nreinforcement learning\\nautomatic assembly\\nassembly tasks\\ntrajectory optimization\\nactor-critic algorithm\",\"93\":\"task analysis\\npredictive models\\ntactile sensors\\nvideos\\ndexterous manipulators\\nneurocontrollers\\npredictive control\\nunsupervised learning\\nfeel\\ntouch-based control\\ndeep predictive models\\ntouch sensing\\ndexterous robotic manipulation\\ntactile sensing\\nnonprehensile manipulation\\ngeneral purpose control techniques\\naccurate physics models\\ntactile percepts\\nhigh-resolution tactile\\ndeep neural network dynamics models\\ndeep tactile mpc\\ntactile servoing\\nraw tactile sensor inputs\\ngelsight-style tactile sensor\\nlearned tactile predictive model\\nuser-specified configurations\\ngoal tactile reading\",\"94\":\"fingers\\nexoskeletons\\nactuators\\nrobots\\nthree-dimensional displays\\noptical sensors\\nbiomechanics\\nmedical robotics\\npatient rehabilitation\\npneumatic actuators\\nintent sensing\\nhand rehabilitative exoskeletons\\nfunctional motor skills\\nmotor recovery\\nsoft robotic exoskeletons\\ncomplex task-based rehabilitative exercises\\nbidirectional motion\\nsoft actuators\\nfinger flexion\\nnoninvasive intent detection\\nupper limb rehabilitative exoskeletons\\npassive finger extension\\nfold-based bidirectional 3d printed intent-sensing soft pneumatic actuator\",\"95\":\"three-dimensional displays\\ncameras\\nrobot kinematics\\nrobot vision systems\\ngrasping\\ngaze tracking\\ngrammars\\nhandicapped aids\\nhuman-robot interaction\\nmedical robotics\\nmobile robots\\npatient rehabilitation\\nhuman-in-the-loop assistive robotics\\nlow-level motion actions\\n3d gaze estimation\\ngrammars-based implementation\\ngaze-based\\ncontext-aware robotic system\\nassistive robotic systems\\nmovement disabilities\\nhuman user\\nmultimodal system\\nassisted reaching\\nassisted grasping\",\"96\":\"task analysis\\nimpedance\\nmedical treatment\\nrobot kinematics\\nrobot sensing systems\\nservice robots\\nmedical robotics\\npatient rehabilitation\\npatient treatment\\ntelerobotics\\ntherapists time\\nhealthcare resources\\nrehabilitation services\\nrobot-assisted rehabilitation\\neconomical solution\\nlfd\\nrobotic rehabilitation\\nlearning from demonstration\\ntelerobotic-mediated hands-on teaching\\ntelerobotic-mediated kinesthetic teaching\\ntmkt\\nrmkt\",\"97\":\"rats\\nforce\\nforce measurement\\nspinal cord\\nrobot sensing systems\\nbiomechanics\\nbiomedical measurement\\nforce sensors\\ninjuries\\nmedical disorders\\nmolecular biophysics\\nnanofibres\\nnanomedicine\\nneurophysiology\\npatient rehabilitation\\npatient treatment\\nproteins\\ntissue engineering\\nt9-t10 level\\nhuman observance\\nnanofiber scaffold\\nneurotrophin-3\\nright limb\\nforce sensing system\\nspinalized rats\\nforce detection\\neffective treatment method\\nspinal cord injury\\ncomplete spinal cord\\nspinal cord transection injury model\\nmotor function\\nrehabilitation enhanced recovery\\nrehabilitated rats\\nleft limb\\nground reaction force\",\"98\":\"actuators\\npermanent magnets\\nmagnetization\\nmagnetic forces\\nconvergence\\nforce\\ntrajectory\\nelectric actuators\\nmicrorobots\\nfour-magnet system\\n2d wireless open-loop control\\nuntethered microrobot\\nlocal maxima\\nmagnetic field magnitude\\nplanar workspace\\nconvergence point\\nmagnetic microrobots\\ninfluence zone\\nactuator orientation\\nactuator design\\nopen-loop guidance\",\"99\":\"fasteners\\nfabrication\\nrobots\\nlaser beam cutting\\nmicromachining\\nstrain\\ngeometry\\nbeam steering\\nbending\\nbiomedical materials\\ncameras\\nelasticity\\nendoscopes\\netching\\nhinges\\nlaser beam machining\\nmicrorobots\\nnickel alloys\\nprototypes\\ntitanium alloys\\nwater jet cutting\\nmillimeter-sized robots\\nmedical devices\\nhybrid manufacturing process\\nlaser micromaching\\nsuperelastic properties\\nroom-temperature mechanical etching procedure\\nthermal damage\\nbending stiffness model\\nprototype devices\\nsimple laser beam steering system\\nnitinol living hinges\\nabrasive jet micromaching\\nendoscopic camera wrist\",\"100\":\"blades\\nmagnetic flux\\ntools\\nforce\\ntorque\\nsprings\\nrobots\\nelasticity\\nmedical robotics\\nmicrorobots\\nsurgery\\nlaparoscopic probe\\ncurrent minimally-invasive surgical tools\\nwireless surgical scissors\\nuntethered surgical scissors\\nuntethered surgical tool\\nsuperelastic nitinol wire\\nexternal magnetic flux density\\n3d magnetic coil system\\nsharpened titanium sheets\\nmobile microrobotic device\\nproof-of-concept prototype\\nmagnetic actuation\\ntetherless mobile microsurgical scissors\\nsize 15.0 mm\",\"101\":\"robot sensing systems\\nfiber gratings\\nsubstrates\\nwires\\nstrain\\ngratings\\nbending\\nbiomedical measurement\\nbragg gratings\\nclosed loop systems\\nfibre optic sensors\\nmanipulators\\nmedical robotics\\nshape memory effects\\nsurgery\\nsteerable surgical robots\\ndisposable surgical robots\\nminimally invasive procedures\\nclosed-loop control\\nsuperelastic substrate\\nflexible adhesive\\nsensor-actuator assembly\\nsma actuation\\nintrinsic bending sensor\\nshape memory alloy bending modules\\nfiber bragg grating bending sensor\\nsma bending module\\nlarge-deflection fbg bending sensor\",\"102\":\"planning\\nrobots\\ncost function\\ntask analysis\\nsearch problems\\nacceleration\\ntuning\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nmobile robots\\nneurocontrollers\\npath planning\\nsearch-based planning\\nhybrid driving-stepping locomotion\\nlocomotion planning\\nhigh-dimensional state spaces\\nground robots\\nground structure\\nmovable body parts\\nrobot representation\",\"103\":\"planning\\noptimization\\ntrajectory\\nkernel\\nrobots\\nstochastic processes\\ncomputational complexity\\ngaussian processes\\noptimisation\\npath planning\\nsampling methods\\ntrajectory control\\noccupancy map\\nstochastic trajectory optimiser\\ngaussian process path representation\\ntrajectory optimisation\\nfast stochastic functional path planning\\npath planners\\nhighly expressive path representation\\nsampling-based planners\\nfully defined artificial potential field\\npartially observed model\\ncubic complexity\\nkernel approximation\\nstochastic sampling\\nsampling-based methods\",\"104\":\"planning\\ntrajectory\\nrobot sensing systems\\nrobot kinematics\\npredictive models\\nreal-time systems\\ncollision avoidance\\nhuman-robot interaction\\nmulti-robot systems\\nrobust control\\ntrajectory control\\nrobust motion planning\\nrobotics literature\\nrobot navigation\\nhigh-order system dynamics\\nconfidence-aware human motion predictions\\nsequential priority ordering\\ntrajectory planning\\nmultirobot multihuman collision avoidance\",\"105\":\"semantics\\nplanning\\nrobot kinematics\\ngrounding\\nautomobiles\\ncollaboration\\nmobile robots\\nmotion control\\npath planning\\nlazy evaluation\\ncollaborative environments\\nrobot motion commands\\ndelayed grounding\\nlazy variable grounding\\nmotion planning algorithm\\nsemantic interpretation\\ngoal specifications\\nreward-penalty strategy\",\"106\":\"clocks\\ntask analysis\\nplanning\\nnavigation\\nautomata\\ncollision avoidance\\nconvex programming\\nformal verification\\nmobile robots\\nmotion control\\npath planning\\ntemporal logic\\nhigh-level specification\\ntimed temporal logic formula\\nobstacle avoidance\\nmotion controller\\nsafe navigation\\ntransition system\\nstandard formal verification\\nconvex optimization techniques\\nreconfigurable motion planning\\nobstacle cluttered environments\\ntimed temporal tasks\\nrobot navigation\\nhybrid control strategy\\ntemporal specifications\\nagent motion abstraction\",\"107\":\"robot kinematics\\nimpedance\\ntask analysis\\nthree-dimensional displays\\nrobot sensing systems\\nend effectors\\nconvolutional neural nets\\nimage colour analysis\\nimage segmentation\\nmanipulator dynamics\\nmobile robots\\nrobot vision\\nhandle frame\\nmanipulator\\ninner controller loops\\ndoor opening\\nindustrial cartesian impedance\\nmobile robot\\nholistic approach\\ndoor model\\ndoor handle detection\\nconvolutional neural network-based architecture\\nhandle shapes\\ndetection rate\\ndoor plane\\ncontrol structure\",\"108\":\"robot sensing systems\\nestimation\\nnavigation\\nprobabilistic logic\\nheuristic algorithms\\nmobile robots\\nestimation theory\\nmarkov processes\\npath planning\\nwind tunnels\\nodor source localization\\nsource term estimation\\nairborne chemicals\\nmobile sensing systems\\nnavigation method\\npartially observable markov decision processes\\nwind tunnel\",\"109\":\"artificial neural networks\\nloading\\nrobot sensing systems\\nwheels\\ntraining\\nneurons\\nbackpropagation\\nbayes methods\\nconstruction equipment\\nfeedforward neural nets\\nindustrial robots\\nmobile robots\\nneurocontrollers\\nsensors\\nautomated pile loading\\nrobotic wheel loader\\ncontrol signals\\nhydrostatic driving pressure\\nboom control\\nsingle hidden layer\\ntraining data\\nbucket filling performance\\nheuristic automated controller\\nmanual human control\\ndemonstration approach\\nbayesian regularization backpropagation algorithm\\nend-to-end neural network controllers\\n3d laser scan\\nlevenberg-marquardt algorithm\",\"110\":\"wheels\\nmathematical model\\nmobile robots\\ntwo dimensional displays\\ngears\\nacceleration\\npneumatic actuators\\nroad vehicles\\nvehicle dynamics\\ndynamic manipulation\\ngear ratio\\nconfigurable wheel\\nvaried radius wheels\\npositional manipulation\\ncentre hub\\nvirtual wheels\\nphysical system\\nouter rim\\nfast control\\nvehicle ride height\\ncompliant wheel\\noff-road robotics\\nspace exploration\",\"111\":\"task analysis\\nkinematics\\nmathematical model\\nvehicle dynamics\\ntrajectory\\ntools\\nalgebra\\nmobile robots\\nmotion control\\nmulti-robot systems\\npath planning\\nnonholonomic motion constraints\\nholonomic coordination constraints\\ndifferential-algebraic equations\\nviability theory\\ncoordinated motion control\\nheterogeneous vehicle dynamics\\nmultivehicle coordination\\ncontrol schemes\\ncoordination control\\nheterogeneous mobile vehicles\",\"112\":\"robots\\ntools\\nturning\\nminimization\\nshape\\nmerging\\nplanning\\nminimisation\\nmobile robots\\nmulti-robot systems\\npath planning\\ntravelling salesman problems\\nturn-minimizing multirobot coverage\\nidentical robots\\nmission time\\nrobot\\nmultiple travelling salesperson problem\\ncoverage plans\\nrobotic vacuum\\nturn minimization\\ncoverage time\\npartitioning heuristic\\ncoverage plan\\nmultirobot coverage\",\"113\":\"planning\\ntrajectory\\nlibraries\\nlattices\\ncollision avoidance\\nrobot kinematics\\ngraph theory\\niterative methods\\nmobile robots\\nmulti-robot systems\\nreachability analysis\\nrobot dynamics\\ntrajectory control\\nknown workspaces\\nonline centralized kinodynamic multirobot replanning\\noffline state lattice reachability analysis\\nexplicit geometric graph\\nhigher-order derivatives\\ngeometric paths\\nkinodynamic multirobot replanning\\nsequential graph searches\\niterative refinement procedures\",\"114\":\"task analysis\\nrobots\\nresource management\\nschedules\\nreal-time systems\\nscheduling\\nmulti-robot systems\\noptimisation\\nprocessor scheduling\\nwarehousing\\nmultirobot systems\\nmultirobot task allocation\\ntask completion\\nmultiprocessor scheduling\\nminimum penalty scheduling\\nnear-optimal real-time task schedule\",\"115\":\"task analysis\\nrobot kinematics\\naugmented reality\\ncognitive science\\nnavigation\\nengines\\ncontrol engineering computing\\nhuman-robot interaction\\nmobile robots\\nmulti-robot systems\\nmixed-granularity human-swarm interaction\\naugmented reality human-swarm interface\\nenvironment-oriented modality\\nrobot swarm\\nrobot-oriented modality\\nenvironment-oriented interaction\\nrobot-oriented interaction\",\"116\":\"acceleration\\nshape\\nrobot kinematics\\ntracking\\npayloads\\ntransportation\\naerospace control\\ncontrol system synthesis\\nhelicopters\\nmobile robots\\nmotion control\\nmulti-robot systems\\nnonlinear control systems\\nposition control\\nrobot dynamics\\nsuspensions (mechanical components)\\nflexible collaborative transportation\\nsuspended payload\\nacceleration signals\\nmaximum payload\\nincremental nonlinear dynamic inversion controller\\ndistance-based formation-motion control algorithm\\nworst case conditions\\nopen-source autopilot\\nrotorcraft team\",\"117\":\"task analysis\\njacobian matrices\\nrobots\\ncouplings\\nmatrix decomposition\\ntransmission line matrix methods\\nmatlab\\nc++ language\\ncontrol engineering computing\\nmanipulator dynamics\\nmanipulator kinematics\\nredundant manipulators\\nrobot programming\\nsource code (software)\\ndynamically-consistent generalized hierarchical control\\nredundant robots\\nstrict prioritization schemes\\nghc\\nnullspace projection operator\\ndynamically-consistent stack-of-tasks hierarchies\\ndynghc\\nsoft prioritization schemes\\nc++ source code\",\"118\":\"conferences\\nautomation\\nbiocontrol\\nbiomechanics\\ndexterous manipulators\\nhumanoid robots\\nmedical robotics\\nmobile robots\\nmuscle\\nneurocontrollers\\nposition control\\nshape memory effects\\nrobotic joint control system\\nanalogue spiking neural networks\\nsma actuators\\nhuman hands\\ncomplex functions\\nmotor cortex\\nanthropomorphic hands\\nnatural muscle control\\nimproved control system\\nanalogue neural networks\\nbiological control mechanisms\\nnatural muscles\\nsingle-joint robotic arm\\nhuman elbow\\nartificial muscle\\nbiological plausibility\\nshape memory alloy wire\\ncontraction force\\nactuator wire\\nspiking frequency\\nelectronic neurons\\nexcitatory neurons\\ninhibitory neurons\\nartificial motor neurons\\nshape memory alloy actuator\\nspiking neural network\\narm mobile lever\\ncontrol method\\nrobotic arm junction\\nshape memory alloy actuators\\njoint rotation accuracy and precision\\ndisturbances\",\"119\":\"vehicle dynamics\\nadaptation models\\nmathematical model\\ncontrol systems\\nadaptive control\\nmobile robots\\ncontrol system synthesis\\nmodel reference adaptive control systems\\nnonlinear control systems\\npendulums\\ntwo-wheeled mobile robot\\ndynamically unstable system\\nenvironmental conditions\\nloading conditions\\nnonlinear controller\\nfixed parameter controllers\\nsingle-input multioutput nature\\nadaptive controller\\nsimo systems\\nhidden dynamic effects\\nmodel reference adaptive control\",\"120\":\"disturbance observers\\nmanipulators\\nuncertainty\\nstability analysis\\nclosed loop systems\\ntorque\\ncontrol system synthesis\\nnonlinear control systems\\nobservers\\nrobust control\\ntrajectory control\\nuncertain systems\\nrobust tracking controller\\nuncertain robot manipulators\\ndisturbance observer based controller\\ninternal model embedding\\nsinusoids frequencies\\n2-dof manipulator\\nclosed-loop system\",\"121\":\"estimation\\nadaptation models\\noptimization\\nwheels\\nmobile robots\\nparameter estimation\\nadaptive control\\ncompensation\\nmotion control\\noptimisation\\npredictive control\\nstatistical analysis\\noverlapping-block strategy\\nreceding horizon estimation\\nmobile robot slip compensation\\nfield robots\\nuncertain terrain conditions\\nautonomous navigation\\nonline estimation\\nwheel-terrain slip characteristics\\noff-road environments\\nconstrained estimation\\nreceding horizon control\\nadaptive optimisation-based control method\\nestimation horizon\\nstructured noise blocking\\ncontrol predictions\\ntracking trajectories\\nrhe\\nrhc\\nstructured blocking approach\\nstate estimation\",\"122\":\"tendons\\nforce\\ntorque\\nrobots\\ncouplings\\nposition control\\nrouting\\nactuators\\ndexterous manipulators\\nforce control\\nhumanoid robots\\nmotion control\\ntorque control\\nvectors\\ninner loop impedance controller\\njoint angle vector\\njoint torque vector\\ncartesian position\\nfinger endpoint\\ndlr david hand\\ndecoupled control\\ntendon driven fingers\\nunderactuated robotic hands\\ndlr awiwi ii hand\\ndavid robot\\njoint torques\\ngeneralized forces\\nmotor torques\",\"123\":\"feature extraction\\nsensors\\ntraining\\nautonomous vehicles\\ntask analysis\\ncomputational modeling\\ncameras\\nmobile robots\\nremotely operated vehicles\\nroad vehicles\\nrobot vision\\nreconfigurable network\\nmultiple perception sensors\\nsteering autonomous platforms\\ngating network\\nunmanned ground vehicle\\nugv\\nfeature extractors\\nexperts\",\"124\":\"azimuth\\nrobot sensing systems\\ntask analysis\\nlaser radar\\ntraining\\nlabeling\\ndistance measurement\\nimage filtering\\nimage fusion\\nimage segmentation\\nlearning (artificial intelligence)\\nmobile robots\\nmotion estimation\\npose estimation\\nradar imaging\\nrobot vision\\nfast radar motion estimation\\ndata association\\nradar odometry\\nweak supervision\\nfocus of attention policy\\nlearning algorithm\\nraw data prefiltering\\nimage segmentation network\\nradar processing time reduction\\nfield robots\\nconsistent motion estimation\\ncopious annotated measurements\\nwheel odometry\\nexternal ego-motion estimator\\nshort-term sensor coherence\\nmeasurement stream\\nscanning radar\\nradar\\nsensing\\nego-motion estimation\\nfield robotics\",\"125\":\"deep learning\\nrobot sensing systems\\ndecoding\\ngain measurement\\nnavigation\\ntraining\\ninformation theory\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nautonomous ground robot\\ngeometric heuristics\\nmobile robot exploration\\ngenerative neural network\\n2d maps\\nreinforcement learning\\nrobots behavior\",\"126\":\"engines\\ntask analysis\\nforce\\nrobots\\ncomputational modeling\\nadaptation models\\nlearning (artificial intelligence)\\nmobile robots\\noff-the-shelf physics engines\\ninteraction networks\\npairwise interactions\\npropagation networks\\ndifferentiable dynamics model\\nlearnable dynamics model\\npartially observable scenarios\\nmodel-based control\\ndeep reinforcement learning algorithms\\nrobot planning\\npropnet\",\"127\":\"trajectory\\nautomobiles\\nautonomous vehicles\\nestimation\\ngenerators\\ntask analysis\\nstochastic processes\\nestimation theory\\nlearning (artificial intelligence)\\nmobile robots\\nrecurrent neural nets\\nroad safety\\nroad traffic control\\ntraffic engineering computing\\nlane change trajectory\\nmeta-learning framework\\nautonomous driving data collection\\ninteractive trajectory prediction\\ninteractive driving\\nautonomous cars\\ndriving behavior estimation\\nrecurrent neural cell\\nrecurrent meta induction neural network\\nhuman-driven cars behaviors\\nconditional neural process\",\"128\":\"manipulators\\nkinematics\\nrobot kinematics\\nneedles\\nstrips\\nfasteners\\nactuators\\nclosed loop systems\\npi control\\nposition control\\ntelerobotics\\ntransmission lines\\npi controller\\nmaster-slave control\\npositioning manipulators\\nptfe tubing\\nclosed-loop position control\\npush-only bidirectional transmission\\nsmt\\nsolid-media transmission\\nfluidic transmission mechanisms\",\"129\":\"grippers\\nrobot sensing systems\\nsteel\\nstrain\\ngrasping\\ncontrol system synthesis\\nforce control\\nmobile robots\\npneumatic control equipment\\nvehicles\\nrobotic grasping\\nmobile vehicles\\nconventional manipulation\\ndynamic mobile robots\\nrobotic gripper\\nmobile platforms\\nreflexive activation\\nreflexive gripper\\nhigh force density reflexive gripping\\npneumatic design\",\"130\":\"grippers\\nelectron tubes\\nforce\\nunmanned aerial vehicles\\nsprings\\nmathematical model\\ngrasping\\naerospace control\\naerospace robotics\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\nmotion control\\nposition control\\ncompliant bistable gripper\\naerial perching\\naerial robots\\nonboard energy supply\\nflying robots\\nperching capability\\npower lines\\nmonitoring-related tasks\\nenergy-efficient perching mechanism\\npalm-size quadcopter\\naerial grasping\\nrobot weight\\nquadcopter perch\\ngrasp objects\",\"131\":\"simultaneous localization and mapping\\nzirconium\\nmaximum likelihood estimation\\noptimization\\ninference algorithms\\nrobustness\\nbayes methods\\ndata structures\\nmobile robots\\noptimisation\\nslam (robots)\\noriginal bayes tree\\nhypothesis pruning strategy\\nmultihypothesis isam\\nnonlinear incremental optimization algorithm\\nmh-isam2\\nsimultaneous localization and mapping problems\\nslam problems\\nhypo-tree\\nmultihypothesis inference\",\"132\":\"lighting\\nproposals\\nvisualization\\nstandards\\nfeature extraction\\nsimultaneous localization and mapping\\nimage representation\\nconvolutional neural nets\\nimage matching\\nlearning (artificial intelligence)\\nlandmark-based image representation\\nvisual loop closure verification\\nmultiview geometry\\nmvg\\ndeep learning\\nconvolutional neural network features\\nmatched landmark pairs\\nverification method\\nkeypoint matching method\\nconvnet features\",\"133\":\"cameras\\ngravity\\nsimultaneous localization and mapping\\nobservability\\nfeature extraction\\njacobian matrices\\naccelerometers\\ngyroscopes\\ninertial navigation\\nmobile robots\\nrobot vision\\nslam (robots)\\nvisual-inertial slam\\nvi-slam\\ngyroscope\\ninitialization method\\nvisual-inertial bundle adjustment\",\"134\":\"three-dimensional displays\\ncameras\\nmeasurement by laser beam\\nlasers\\nestimation\\nvisual odometry\\nmotion estimation\\ndistance measurement\\ncamera information\\nmobile platform\\ndirect laser-visual odometry approach building\\nphotometric image alignment\\ninformation usage\\nlaser scan\\nframe-to-frame motion estimate\\nplanar point\\nindividual point clouds\\ncorresponding pixel patches\\ncamera image\\nextracted planar image patches\\nnonplanar pixels\\npixel alignments\\nhigh estimation accuracy\\nclearpath husky platform\\ncompetitive estimation accuracy\\ncolored point clouds\\naccurate direct visual-laser odometry\\nexplicit occlusion handling\\nplane detection\",\"135\":\"simultaneous localization and mapping\\nsemantics\\ncovariance matrices\\nuncertainty\\ndetectors\\nsearch problems\\ncurrent measurement\\nslam (robots)\\ntree searching\\nlocal information\\nslam graph\\nexpensive recovery\\nsystem covariance matrix\\njoint compatibility\\nsearch space\\nclique-based pairwise compatibility\\nrobust object-based loop-closure\\nslam problems\\nsemantic slam\\ndata association\\nhigh-confidence loop-closure mechanism\\nobject-level slam\\nlandmark uncertainty\\nconstellation-based map-merging\\nbranch-and-bound max-cardinality search\",\"136\":\"task analysis\\nend effectors\\nrobustness\\ndata models\\nplanning\\nclustering algorithms\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmobile robots\\noptimisation\\nrobot programming\\ncontact dynamics\\nreinforcement learning\\nrecovery skills\\nmultiple contact state changes\\nmultimodal state transition model\\nrecovery heuristics\\ncontact-based manipulations\\nrobust manipulation strategies learning\\nskill selections\",\"137\":\"kinematics\\nmanipulators\\ntrajectory\\noptimal control\\ncost function\\nstability analysis\\nadaptive control\\nclosed loop systems\\ncontrol system synthesis\\nend effectors\\nlyapunov methods\\nmanipulator dynamics\\nmanipulator kinematics\\nmotion control\\nposition control\\nstability\\ntime-varying systems\\ntracking\\nrobot manipulator\\nrobot end effector position\\ntask space trajectory\\ndesired velocity profile\\nsingle network adaptive critic\\nforward kinematics\\ninput affine system\\ncritic weight update law\\ndesired optimal cost\\nclosed loop kinematic control\\noptimal regulation problem\\nsnac based kinematic control\\noptimal tracking problem\\ntracking error\\nreference trajectory\\noptimal control policy\\nkinematic control scheme\\nuniversal robot 10 manipulator\\nadaptive critic based optimal kinematic control\",\"138\":\"robots\\ntask analysis\\noptimization\\nsurgery\\naerospace electronics\\nredundancy\\ntrajectory\\ncompliance control\\nend effectors\\nmanipulator dynamics\\nmedical robotics\\nmotion control\\noptimisation\\nredundant manipulators\\nmanipulability optimization control\\nserial redundant robot\\nrobot-assisted minimally invasive surgery\\n7-dof robot manipulator\\nrcm constraint\\nhierarchical operational space formulation\\ncartesian compliance control\\nnull-space controller\\ncontrol components integration\\nremote center of motion\\nend-effector accuracy\",\"139\":\"grippers\\ngrasping\\ntrajectory\\nuncertainty\\nphysics\\nend effectors\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\npath planning\\npose estimation\\nposition control\\nprobability\\nmobile base trajectory\\nactive learning based approach\\noptimization-based framework\\ntime-optimal trajectories\\npart pose estimation uncertainties\\ntrajectory generation\\npart pick-up\\nmobile manipulators\\noperation time\\ngripper\",\"140\":\"tools\\ntask analysis\\nrobots\\nthree-dimensional displays\\ntrajectory\\ncontainers\\ndairy products\\ncontrol engineering computing\\nmanipulators\\nmobile robots\\nmotion control\\nrobot vision\\nservice robots\\npoint clouds\\ntarget object\\ntool-using manipulation skills\\nmotion trajectories\\nscraping material\\nrobot perception module\\npr2 robot\",\"141\":\"conferences\\nautomation\\nchemical engineering\\nclosed loop systems\\nfeedback\\nopen loop systems\\nposition control\\npropulsion\\nvectors\\n3d closed loop control system\\n3d motion\\nself-actuated particles\\nopen loop 3d control\\n2d feedback control\\ntriaxial approximate helmholtz coil system\\npropulsion vectors\\nheterogeneous chemically catalyzing janus particles\",\"142\":\"heuristic algorithms\\nmathematical model\\ndata models\\npredictive models\\nprediction algorithms\\nsystem dynamics\\nswitches\\nbiomechanics\\ngait analysis\\nlegged locomotion\\nmedical robotics\\nnonlinear control systems\\npatient rehabilitation\\npredictive control\\nrobot dynamics\\nrobot kinematics\\ndata-driven gait segmentation\\nwalking assistance\\nlower-limb assistive device\\nhybrid systems\\nbipedal walker\\nnonlinear dynamics\\nhybrid mode\\nreliable state sensing\\ndata-driven analysis\\ndata-driven dynamics identification\\nmodel predictive control\\nhybrid slip model\\ngait partitioning\\nhuman walking data\\nkinematics data\\nonline assistance\\npredefined gait structure\\nhealthy gaits\\npathological gaits\\nimpairment-specific rehabilitation strategies\",\"143\":\"simultaneous localization and mapping\\nfoot\\nlegged locomotion\\nhumanoid robots\\nlips\\napproximation theory\\nclosed loop systems\\nmobile robots\\nmotion control\\nnonlinear control systems\\npath planning\\npendulums\\nposition control\\npredictive control\\nslam (robots)\\nstability\\nfundamental capacity\\nexternal inputs\\nreference velocity\\nfootstep plans\\nreference motion\\nexternal disturbances\\nclosed-loop mpc scheme\\nproprioceptive sensors\\nimperfect open-loop control execution\\nhrp-4 humanoid robot\\nreactive stepping\\nwalking gaits\\nhumanoid locomotion\\nmodel predictive control\\npendulum\\ndense visual slam stability\",\"144\":\"legged locomotion\\ntrajectory\\ncollision avoidance\\nkinematics\\nthree-dimensional displays\\ndynamics\\ngait analysis\\nmanipulator dynamics\\nnonlinear control systems\\npredictive control\\ntrajectory control\\ncomplete kinematic feasibility guarantees\\ndynamic feasibility guarantees\\npiecewise horizontal ground\\nvertical motion\\nlinear constraints\\nnonlinear constraint\\nlinear mpc scheme\\nonline computation\\nreactive walking motions\\nphysical collaboration\\nfully adaptable height\\nadaptable step placement\\nsafe 3d bipedal walking\\n3d capturability constraint\",\"145\":\"limit-cycles\\nplanning\\nlegged locomotion\\nlyapunov methods\\nforce\\ntrajectory optimization\\nfeedback\\nlearning (artificial intelligence)\\nneurocontrollers\\npath planning\\nrandom processes\\nsampling methods\\ntrajectory control\\ntrees (mathematics)\\nfeedback motion planning\\nlegged robots\\norbital lyapunov functions\\nsampling-based framework\\npoincar\\u00e9 section\\nmultiple trajectory optimization problems\\nrapidly-exploring random tree algorithm\\nregions of attraction\\ndeep learning neural networks\",\"146\":\"legged locomotion\\ntrajectory\\nhumanoid robots\\nfoot\\nstability analysis\\nmotion control\\npendulums\\nposition control\\npredictive control\\nrobot dynamics\\nstability\\ndyros-jet\\ncom trajectory\\ncenter-of-mass\\nzero-moment point\\nlinear inverted pendulum model\\nsecond-order system\\npreview control\\ndynamics model\\nmotion controller\\nreal-time walking pattern generation\\nhumanoid robot walking\\nposition tracking performance\\nwalking stability\\ncompliant motion control\\nonline walking pattern generation\",\"147\":\"clocks\\nsynchronization\\nprotocols\\ndistance measurement\\nthermodynamics\\ntransceivers\\nindoor radio\\nkalman filters\\nmobile robots\\npath planning\\nposition control\\nradionavigation\\nsynchronisation\\ntime-of-arrival estimation\\nultra wideband communication\\nekf-based navigation system design\\nmobile robot positioning\\nkalman filter-based algorithm\\nuwb transceivers\\nultra-wideband wireless communication transceivers\\ntime difference of arrival\\ntdoa\\npassive uwb receivers\\nuwb networks\\nranging-based positioning systems\\nsimultaneous time synchronization-localization\\nsignal time-of-flight measurement\\ntime of arrival\\ntoa\\nmultiple uwb base station synchronization\",\"148\":\"robot sensing systems\\nthree-dimensional displays\\nmicrophone arrays\\ncomputer architecture\\nfield programmable gate arrays\\nimage sensors\\nmobile robots\\nrobot vision\\nsonar imaging\\nfully embedded real time 3d imaging sonar sensor\\nrobotic application\\nreflection strength\\nsensor systems\\ncrucial pieces\\nprocessing power\\nexternal computing device\\n3d sonar sensor\\n3d localization capabilities\\nfield-of-view\\n3d perception\\nharsh conditions\",\"149\":\"optical imaging\\ncameras\\noptical sensors\\nadaptive optics\\nheating systems\\noptical network units\\nreal-time systems\\nimage sensors\\nmining\\nmining industry\\nmobile robots\\nneural nets\\nrobot vision\\nautonomous underground mining vehicles\\nneural-network-based pixel sampling strategy\\nvisual based technique\\nreal-time accurate localisation system\\nrange sensor-based system\\nceiling-facing cameras\",\"150\":\"three-dimensional displays\\ncost function\\nminimization\\ntuning\\nrobot sensing systems\\ntwo dimensional displays\\nimage registration\\niterative methods\\nmobile robots\\nrobust functions\\nregistration algorithms\\nregistration accuracy\\nmobile robotic application\\niterative closest point algorithm\\nenvironment types\\noutlier filters\\nicp algorithm\\nm-estimators\\ntuning parameters\",\"151\":\"robots\\nautomation\\nread only memory\\ncooperative systems\\ndata analysis\\nlocation based services\\nmulti-robot systems\\nperformance evaluation\\nperformance evaluation system\\nphysical experiment\\ncolo-pe\\nsoftware analysis tool\\nalgorithm evaluation\\ncolo-at\\nmultirobot cooperative localization algorithms\\nmultirobot localization data collection\\nrobot location\",\"152\":\"robot sensing systems\\nlegged locomotion\\nshafts\\nforce sensors\\nmonitoring\\nstandards\\ngeriatrics\\nhealth care\\nmedical robotics\\ntelemedicine\\ntelerobotics\\nattention mechanisms\\ntelecare robots\\ntelepresence robots\\ncyclic checks\\ncaregivers\\ncga robots\\ncomprehensive geriatric assessment\\ncane-based low cost force sensor system\",\"153\":\"manipulators\\nmodulation\\npneumatic systems\\ntendons\\nsoft robotics\\nactuators\\nassisted living\\nbiomechanics\\nhandicapped aids\\nhuman-robot interaction\\nmanipulator kinematics\\nmedical robotics\\nmobile robots\\nmotion control\\npneumatic actuators\\nservice robots\\ndeployable soft robotic arm\\nassistive living applications\\nthree-tendon actuated continuum robot\\nelderly impaired individuals\\nphysically impaired individuals\\ndaily living\\nyoshimura pattern\\ncontrolled deployment\\nlength variation\\npneumatic stiffness mechanism\\nstiffness modulation approach\\nactuation system\\nassistive robots\",\"154\":\"trajectory\\nhip\\nlegged locomotion\\ntraining\\ncouplings\\ntiming\\nactuators\\ngait analysis\\nmedical robotics\\npatient rehabilitation\\ntrajectory control\\nhip interaction\\nsingle dof mechanism\\nlow-cost lower extremity gait rehabilitation device\\nsingle actuator\\nsingle dof 8-bar jansen mechanism\\nefficient walking mechanism\\nlegged robots\\nankle trajectory\\nhuman gait\\nlower limb\\ncustom designed seat-type weight support system\\ndonning-doffing action\\nweight-bearing\\neffective user-friendly training environment\\ngait training\",\"155\":\"actuators\\nbrakes\\ndc motors\\nsprings\\ngears\\nrobots\\nsafety\\nbiomechanics\\nclutches\\nhuman-robot interaction\\nmedical robotics\\nmotion control\\npatient rehabilitation\\nrobot dynamics\\nshafts\\nsprings (mechanical)\\ntorque control\\nrobot-aided rehabilitation\\nupper limb musculoskeletal injuries\\nlower limb musculoskeletal injuries\\nrobot-aided musculoskeletal rehabilitation\\nactuation modes\\ndifferentially-clutched series elastic actuator\\ncompliant nature\\ndc motor\\ntorsion spring\\nmagnetic particle brake\\ndifferential gear\\ntopology\\nfree motion\\nelastic motion\\nassistive motion\\nresistive motion\\nactuator dynamic model\\nreference torque\",\"156\":\"surgery\\nendoscopes\\nneedles\\ninstruments\\nmanipulators\\ntools\\nbiomedical optical imaging\\nmedical robotics\\nwound closure\\nrobotic suturing system\\nfive-degree-of-freedom robotic suturing instrument\\nsuturing tasks\\nflexible endoscopic surgery\",\"157\":\"cameras\\nforce\\nrobot vision systems\\nstators\\nactuators\\nforce measurement\\nbiological tissues\\nbiomedical optical imaging\\nmedical robotics\\nsurgery\\ncamera actuation\\nrotation camera behaviors\\nsimulated abdominal cavity\\nnoninvasive real-time camera-tissue interaction force measurement approach\\nabdominal wall tissue\\nrobotic-assisted camera control experiment\\nlaparoscopic imaging\\ntransabdominal magnetic coupling\\nminimally invasive surgery\\nconventional trocar-based laparoscopes\\ninsertable laparoscopic camera\\nrobotic-assisted insertable laparoscopic surgical camera\\nlost force feedback\\nnoninvasive approach\",\"158\":\"endoscopes\\nhaptic interfaces\\nforce\\nsurgery\\nsensors\\nphantoms\\ncontrol systems\\nfinite element analysis\\nmedical robotics\\n3d kinematics control\\nmaneuverability\\nendoscopic module\\nendoscopic surgical interventions\\nstructural stiffness\\nhaptic feedback control\\nforce sensing module\\nmultilevel stiffening mechanism\\nsoft robotics endoscope\\nsurgical manipulators\\nnatural orifice transluminal endoscopic surgeries\\nminimally invasive orifice transluminal endoscopic surgeries\\nsoft endoscopes\\nendoscopic applications\\nforce haptic feedback\\nmultilevel stiffness soft robotic module\",\"159\":\"needles\\nfasteners\\nelectron tubes\\ntendons\\nforce\\nrobots\\nplanning\\nindustrial robots\\nnickel alloys\\nrods (structures)\\nsprings (mechanical)\\ntitanium alloys\\nlaser machining\\ncosserat rod theory\\nspring model\\ndynamic region rrt\\nplanning algorithm\\ntissue reaction\\nmicrotools\\nembedded rotational tip joint\\nproximal notch patterns\\nsteerable needle\\nrobotic needles\",\"160\":\"tools\\nthree-dimensional displays\\nlaparoscopes\\nsurgery\\nrobot kinematics\\nimaging\\nbiological tissues\\nbiomedical optical imaging\\nendoscopes\\nmedical image processing\\nmedical robotics\\nstar\\nlaparoscopic suturing tool\\nsuture repositioning\\nsuture spacing\\nsuture pads\\nsuture planning strategy\\n3d imaging endoscope\\nsmart tissue anastomosis robot\\nautonomous laparoscopic robotic suturing system\\nsurgical tools\\nlaparoscopic surgery\\npatient recovery time\\ncollateral tissue damage\\nlaparoscopic surgical methods\\nopen surgical techniques\",\"161\":\"force\\ntools\\nsurgery\\nimage segmentation\\narteries\\nshafts\\nrobots\\nforce control\\nmedical robotics\\ntelerobotics\\ntool-shaft collision avoidance\\nclinical adoption\\nmaster-slave surgical systems\\nphysical separation\\nactive constraints\\nsensory information\\nsurgical robot operators\\nhaptic cues\\nvisual cues\\naudible cues\\nsurgical tool-clashing\\nelasto-plastic frictional force control\\nacs methods\\nteleoperated da vinci surgical system\\nacs assistance\\nteleoperation-based robotic-assisted minimally invasive surgery\\nminimally invasive partial nephrectomy\",\"162\":\"protocols\\nrobots\\ncommunication channels\\ntask analysis\\nreal-time systems\\nstability criteria\\nnetwork architecture\\nenergy management systems\\ntelecommunication power management\\nenergy transaction protocol\\ndistributed robotic system\\nsimulated unreliable communication channel\\nenergy budget transaction protocol\\nnecessary condition\\nenergy generating system\\nenergy-aware actuation\\nallocated energy budget\\nsystem stability\\nenergy monitoring\\naccidental energy generation\\nnaive communication strategy\",\"163\":\"delays\\nultrasonic imaging\\ndamping\\nforce\\nmanipulator dynamics\\nbiomedical ultrasonics\\nforce control\\nhaptic interfaces\\nmanipulators\\nmedical robotics\\nstability\\ntelerobotics\\ntime-varying systems\\nsonographer\\ncommunication delays\\npassivity-based bilateral teleoperation architecture\\nunknown time-varying delay\\ncontrol laws\\nslave manipulator\\nsignificant forces\\nwam barrett robot\\ntwo-layer teleoperation algorithm\\nenergy scaling\\nstable behavior\\ntouch haptic device\\ntele-echography system\\nultrasound procedures\",\"164\":\"electromyography\\naerospace electronics\\ngrippers\\nwrist\\nmuscles\\nteleoperators\\ndexterous manipulators\\nhuman-robot interaction\\nsensors\\nsignal processing\\ntelerobotics\\nemg-controlled nonanthropomorphic hand teleoperation\\ncontinuous teleoperation subspace\\nemg-driven teleoperation\\nemg sensors\\nemg signals\\npose space\\nforearm emg\\nrobot hand\\nemg teleoperation methods\\nnonanthropomorphic multidof robot hands\",\"165\":\"force\\ndelays\\ncommunication channels\\nhigh frequency\\nvibrations\\ntime-domain analysis\\nteleoperators\\nbang-bang control\\ncontrol system synthesis\\nfeedback\\nforce control\\ngradient methods\\nobservers\\nposition control\\nstability\\ntelerobotics\\ntime domain passivity approach\\ncommunication time delay\\nforce jittering\\nteleoperation setup\\nmaster device\\npassivity controller\\nvirtual mass-spring system\\nhigh frequency force vibrations\\nconservative passivity-based approaches\\nstable controller design\\nobserver-based gradient controller\\nforce transparency\\ntdpa\\nposition-force bilateral teleoperation system\\nsystem parameters\\ndelayed feedback force\",\"166\":\"delays\\ntask analysis\\nsurgery\\nhaptic interfaces\\nrobot sensing systems\\nmedical robotics\\ntelerobotics\\nhigh delay surgical teleoperation\\nrobotic teleoperation\\nsurgeon\\ntelerobotic surgery\\nsurgical workflow\\ncommercial surgical robotic systems\\nintuitive motion scaling solutions\\nteleoperated robotic systems\",\"167\":\"robots\\ntask analysis\\nimage segmentation\\nreinforcement learning\\nclutter\\nneural networks\\ncollision avoidance\\nfunction approximation\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nobject detection\\nrobot vision\\nrobust object grasping\\nsingulation\\ncluttered environment\\ncollision free grasp affordances\\noptimal push policies\\naction-value function approximation\\nrobot training\\ndeep neural network\",\"168\":\"uncertainty\\nmanipulators\\ngrasping\\nrobot sensing systems\\nplanning\\nthree-dimensional displays\\ndexterous manipulators\\ngrippers\\nimage reconstruction\\nimage registration\\nmobile robots\\npath planning\\npose estimation\\nrobot vision\\narbitrary object\\nscan overlap\\ngrasp analysis\\nautonomous data-driven grasping system\\nvolumetric next-best-view algorithm\\nnbv algorithm\\nforce closure\\nbase pose uncertainty\\nintegrated grasping system\\nmobile manipulator\",\"169\":\"tendons\\npulleys\\nmeasurement\\nsprings\\ngrasping\\ntorque\\nmathematical model\\ndesign engineering\\ndexterous manipulators\\nelastic constants\\ngrippers\\nsprings (mechanical)\\nplanar underactuated hand\\nunderactuated grippers\\npassive adaptability\\nfinger phalanx length\\ndesign parameters\\ncaging grasp performance\\nmechanical compliance\\nfree-swing motion\\njoint spring stiffness\\npulley radius\",\"170\":\"search problems\\ntask analysis\\ngrasping\\nimage segmentation\\nvisualization\\nrobot sensing systems\\nimage colour analysis\\nobject recognition\\nrobot vision\\nmechanical search\\ndistractor objects\\nrobots\\nrgbd perception system\\ntarget object multistep retrieval\",\"171\":\"shape\\ngrasping\\nsemantics\\nthree-dimensional displays\\nrobots\\nstability analysis\\nparticle swarm optimization\\ndexterous manipulators\\nimage segmentation\\nlearning (artificial intelligence)\\npath planning\\nrobot vision\\ngrasp configurations\\nactive learning\\nprior example objects\\nsimilar shapes\\ngeometric shape characteristics\\nsemantic shape characteristics\\ngrasp space\\nmodel parts\\ncorresponding grasps\\nlocal replanning\\npoint cloud\\nrobotic hands\",\"172\":\"actuators\\nkinematics\\njacobian matrices\\nredundancy\\nlegged locomotion\\nsolid modeling\\nmanipulator dynamics\\nmanipulator kinematics\\nkinematic analysis\\nparallel mechanism\\n3 translational dofs\\n1 rotational dof\\nparallel sliders\\nparallelogram linkages\\nkinematic equations\\ngeometric description\\ntranslational workspace\\norientational workspace\\nparallelogram\",\"173\":\"legged locomotion\\nkinematics\\nredundancy\\nsilicon\\nactuators\\nparallel robots\\ncad\\ngrippers\\nmanipulator kinematics\\norientational workspace\\ntype ii singularities\\nrobot actuators\\nkinematically redundant (6+3)-dof hybrid parallel robot\\nremotely operated gripper\\n3-[r(rr-rrr)sr] kinematically redundant 6+3-degree-of-freedom spatial hybrid parallel robot\\nrevolute actuators\\nconstraint conditions\\ncad model\\ncomputer animation\",\"174\":\"robots\\nspirals\\nkinematics\\nshape\\nmuscles\\nmathematical model\\ncomputational modeling\\nmanipulator kinematics\\neuler spiral models\\neuler spiral method\\neuler curves\\ncornu spirals\\nkinematic modeling\\nvariable curvature parallel continuum robots\\nclothoids\\nmckibben actuators\",\"175\":\"damping\\nstability analysis\\nrobot kinematics\\ntask analysis\\nexoskeletons\\nimpedance\\nbiomechanics\\ncontrol system synthesis\\nhuman-robot interaction\\nmedical robotics\\nmobile robots\\nrobot dynamics\\nstability\\nwearable robots\\nankle exoskeleton robot\\nhuman ankle damping\\nhuman-robot system\\nrobotic controller design\\nrobotic damping conditions\",\"176\":\"exoskeletons\\ntorque\\nbrushless dc motors\\nvoltage measurement\\nvoltage control\\nactuators\\nbiomechanics\\nfeedforward\\ngait analysis\\nlegged locomotion\\nmachine vector control\\nmedical robotics\\npatient rehabilitation\\nrobot kinematics\\ntorque control\\nankle assistance\\nautonomous platform\\ntorque control bandwidth\\nfield oriented control\\nfeed-forward controller\\nhigh efficiency transmission system\\nuntethered exoskeleton\\nankle plantarflexion assistance\\nautonomous exoskeleton platform\\nrobotics community\\nlower-limb exoskeletons\\nfrequency 17.5 hz\",\"177\":\"trajectory\\nintegral equations\\nautonomous vehicles\\nmonte carlo methods\\nuncertainty\\ncollision avoidance\\nmobile robots\\nnavigation\\nprobability\\nroad traffic control\\nroad vehicles\\nautonomous vehicle navigation\\nmonte carlo simulations\\nautonomous systems\\nself-driving car\\nfreie universit\\u00e4t berlin\\ncollision octagon\\nanalytic collision risk calculation\\ntrajectory prediction\\nground vehicle navigation\\nautonomous driving\\nplanning system\",\"178\":\"navigation\\nplanning\\nautomobiles\\nrobot sensing systems\\nvehicle dynamics\\ncomputer architecture\\ncollision avoidance\\nmobile robots\\nmotion control\\nroad traffic\\nlocal navigation\\nautonomous driving vehicles\\ncurvature velocity method\\ncar navigation system\\nroad path\\nhigh-level planning\\nlower-level reactive control\\nbeam curvature method\\npure pursuit method\\nlane obstacle avoidance\\nelderly people\",\"179\":\"robots\\ncollision avoidance\\ntraining\\nplanning\\ntask analysis\\nservers\\nheuristic algorithms\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\ngoal-driven navigation\\nnonholonomic multirobot system\\nlearning collision\\nreinforcement learning\\nmultirobot collision avoidance approach\\ntraining agent robots\\nagent robot\\ntrained policy\\nmultiple obstacle robots\\nrobot simulation\\nrobot experiment\",\"180\":\"ellipsoids\\ncollision avoidance\\nshape\\ntwo dimensional displays\\nthree-dimensional displays\\nrobot sensing systems\\ncomputational geometry\\nconvex programming\\nmesh generation\\nmobile robots\\nellipsoid\\nsuperquadric\\nclosed-form parametric expression\\ngeneral convex differentiable parametric surface\\nefficient exact collision detection\\nclosed-form minkowski sums\\ncomputer graphics\\nrobot motion planning\\nconvex polytopes\\ncollision detection scheme\\nn-dimensional euclidean space\\nprincipal kinematic formula\\npkf\\ngilbert-johnson-keerthi\\ngjk\\nalgebraic separation conditions\\nasc\\nmeshes\",\"181\":\"coils\\nrobots\\nforce\\npermanent magnets\\nwires\\nuncertainty\\ncollision avoidance\\nmagnetic actuators\\nmicrorobots\\nmobile robots\\nnumerical analysis\\nposition control\\nmagnetically guided actuation\\nminiature robots\\nplanar surfaces\\nrobot actuation\\npositioning uncertainty reduction\\nactuation superposition\\nlock-up field\",\"182\":\"agriculture\\nfeature extraction\\ncameras\\nsemantics\\nvisualization\\nrobot vision systems\\nautonomous aerial vehicles\\ncrops\\ndata visualisation\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nrobot localization\\naerial images\\nprecision agriculture tasks\\ncrop field environment\\nvisual aliasing\\nlocalization system\\naerial map\\nvisual ambiguity problem\\nautonomous robots\",\"183\":\"forestry\\nsimultaneous localization and mapping\\ncameras\\nvisualization\\nvegetation\\nfeature extraction\\nlighting\\nautonomous aerial vehicles\\nmobile robots\\npath planning\\nremotely operated vehicles\\nrobot vision\\nslam (robots)\\nmanaged forests\\ntree health\\nslam research\\nstructured human environments\\nunstructured forests\\nforest data\\nphotorealistic simulated forest\\nstraightforward forest terrain\\nforest scenes\\nnatural scenes\\nvisual appearance analysis\\ncheap energy efficient way\\nunmanned aerial vehicles\\nmonocular slam systems\\nslam systems\\nvisual appearance statistics\",\"184\":\"vegetation\\nimage segmentation\\nimage color analysis\\nsemantics\\nvegetation mapping\\ntraining\\nrobots\\nconvolutional neural nets\\nforestry\\nimage classification\\nimage colour analysis\\nimage fusion\\nlearning (artificial intelligence)\\nsemantic segmentation\\ntree-like vegetation\\npipeline\\nsingle rgb-d image\\ndeep network\\nmultiple convolutional neural network architectures\\ncolour data\\nasynchronous training approach\\n3-channel hha image\\nlate fusion architecture\\nsynthetic dataset\\nbroadleaf trees\\ntree species\",\"185\":\"agriculture\\nrobot kinematics\\nnavigation\\nmobile robots\\ncomputed tomography\\nimage color analysis\\nagricultural machinery\\ncrops\\nglobal positioning system\\ninfrared imaging\\npath planning\\nrobot vision\\nthermal image\\nnavigation system\\nskid-steering mobile robots\\nsugarcane crops\\nautonomous navigation\\nsugarcane plantations\\nordinary agricultural fields\\nsugarcane farms\\nrow crop tunnels\\nlow-cost skid-steering mobile robot\\nbioenergy farm\\ninfrared thermal imaging\\nlaser-based sensors\\nimage analysis\\nnavigation methodology\\nrobot swarm\\ntankette for intelligent bioenergy agriculture\",\"186\":\"soil\\nrobot sensing systems\\nnavigation\\nforce measurement\\nshafts\\nforce\\ncollision avoidance\\nfeedback\\nforce sensors\\nmobile robots\\ncomplex environment\\ndynamic obstacles detection\\n6-axis force torque sensor\\nplant-inspired robot\\nsoil exploration\\ncomplex environments\\ndynamic environments\\nrobotic soil explorations\",\"187\":\"grippers\\ntactile sensors\\nsprings\\nforce\\nskin\\nagricultural products\\nagricultural robots\\ncapacitive sensors\\nfood products\\nnondestructive testing\\nsensor arrays\\nstandards\\nwaste reduction\\nnondestructive robotic assessment\\nmango ripeness\\nmultipoint soft haptics\\ndestructive fruit ripeness estimation\\ncapacitive tactile sensor array\\nmango stiffness\\npenetrometer measurements\\nautomatic method\\nsurface areas\\nfresh product standards\\ncustom-made gripper\\nsimplified spring model\\nkeitt variety\",\"188\":\"satellites\\ngeology\\ncameras\\nfeature extraction\\ntraining\\nvisual odometry\\ngoogle\\nautonomous aerial vehicles\\ndistance measurement\\nkalman filters\\nneural nets\\npose estimation\\nunseen images\\ntrajectory estimation errors\\nuav pose estimation\\nimage-based cross-view geolocalization method\\ngeoreferenced satellite imagery\\nsiamese neural networks\\nuav camera\\nsatellite images\\ncrossview geolocalization\",\"189\":\"field programmable gate arrays\\nimage sensors\\nstreaming media\\nrobot sensing systems\\naircraft navigation\\nsensor arrays\\nautonomous aerial vehicles\\nhelicopters\\nmicrorobots\\nmobile robots\\npath planning\\nrobot vision\\nfalcon 250 quadrotor platform\\nvision guided autonomous drone flight\\nintegrated sensing\\nopen vision computer\\nnavigation operations\\noutdoor exploration\\novc system\\ncomputational performance\\npower consumption\\nrelatively small-scale flying platforms\",\"190\":\"cameras\\noptimization\\nvisualization\\nlaser beams\\nreliability\\nangular velocity\\nautonomous aerial vehicles\\nmobile robots\\nrobot vision\\ncommon local planarity assumption\\nframe-to-frame visual-inertial odometry algorithm\\ndownward facing cameras\\nrad-vio\\ncost function\\nrangefinder-aided downward visual-inertial odometry\\ncomplementary odometry algorithm\\nimu regularisation term\\nhomography based photometric cost\\nmicroaerial vehicles\",\"191\":\"cameras\\ndrones\\ntraining\\nskeleton\\nstreaming media\\ntwo dimensional displays\\nfeature extraction\\nautonomous aerial vehicles\\ncollision avoidance\\ndecision making\\nimage capture\\nimage sensors\\nimage sequences\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nobject tracking\\nrobot vision\\nvideo cameras\\nvideo signal processing\\nfilm-look video\\ncamera drone\\nintelligent drones\\nsmarter assistant tools\\nautonomous aerial filming\\ncamera motion planning\\ncinematic footages\\nfilm-look aerial footage\\ncamera position\\nautomatic filming onboard\\ndata-driven planning approach\\nimage composition\\ndata-driven learning-based approach\\nprofessional cameramans intention\\ndecision-making process\",\"192\":\"fasteners\\npropulsion\\nmathematical model\\nattitude control\\nbatteries\\nactuators\\nservomotors\\nautonomous aerial vehicles\\nmobile robots\\nmultiple sequential transformations\\nshape-shifting transformation\\nmist-uav\\nin-air transformation\\nmultirotor\\nmultisection-transformable-uav\\ntransformable vertical take off and landing uav\\ntransformable vtol uav\\ntail-sitter\\nfixed-wing operation\",\"193\":\"rotors\\nvehicle dynamics\\nestimation\\nquaternions\\nreal-time systems\\ntask analysis\\ntransportation\\nautonomous aerial vehicles\\nmobile robots\\nmulti-robot systems\\nnonlinear control systems\\nobservers\\nrobust control\\nmultirotor aerial vehicles\\nmoment of inertia\\ncenter of mass\\nnonlinear observability analysis\\nload transportation\\nmotor speed\\naerial vehicles\\nprecise control\\ninertia parameters\",\"194\":\"acceleration\\nestimation\\nrobot sensing systems\\ncollision avoidance\\ntask analysis\\naerospace robotics\\nmanipulators\\nmobile robots\\nmotion control\\nposition control\\nrobot dynamics\\nsensors\\nexternal wrench estimation\\nmass estimator\\ndistributed imu system\\naerial exploration\\nexternal force\\ntorque\\naerial manipulation tasks\\naerial multilink robot\\nonboard inertial measurement unit sensors\\ntwo-dimensional multilink aerial robot\\nexternal wrench estimator\\ncontact point\\nmotion primitives library\\nrobot dynamic models\\nacceleration data\",\"195\":\"inspection\\nthree-dimensional displays\\nunmanned aerial vehicles\\nmicroswitches\\ndelay effects\\nrobot sensing systems\\nautomatic optical inspection\\nautonomous aerial vehicles\\ncollision avoidance\\nlaser ranging\\nmobile robots\\npower overhead lines\\nrobot vision\\nsensors\\nservice robots\\nclose-proximity autonomous transmission-line inspection\\npower transmission lines inspection\\noverhead ground wire\\nogw\\nsensor data collection\\nunmanned aerial vehicle\\ngrabbing mechanism\\nmechanical structures\\nautonomous navigation\\n2d laser range finder\\nlrf\\ncbr platforms\\nartificially constructed ptls environment outdoors\\nhigh inspection accuracy\\nclose-proximity inspection\\nlong-term inspection\",\"196\":\"drones\\nantenna measurements\\ntelemetry\\ntarget tracking\\nantennas\\naircraft\\nradio frequency\\naerospace communication\\naerospace robotics\\nmobile radio\\nmobile robots\\npath planning\\nremotely operated vehicles\\nmoving radio target\\nunauthorized drone flights\\ncommodity radios\\ntelemetry radio emissions\\npassenger safety\\nbystander safety\",\"197\":\"force sensors\\npiezoresistance\\nforce\\nforce measurement\\nmicroinjection\\nprototypes\\ncellular biophysics\\nmedical control systems\\nembedded soft force sensor\\nmanual cell microinjection\\nposition-based robotic cell microinjection\\nforce-assisted robotic cell microinjection\\nsurvival rate\\npiezoresistive force sensor\\nsoft materials\\nsoft sensors\\nforce-sensing cell injector\",\"198\":\"admittance\\noptimization\\ndynamics\\nmanipulators\\ncollaboration\\nbuildings\\nhumanoid robots\\nhuman-robot interaction\\noptimisation\\nstability\\ntanks (containers)\\nadmittance controllers\\nenergy optimization problem\\ninteractive behavior\\nstable interaction\\nrobot interaction\\nadmittance control strategy\",\"199\":\"shafts\\ntorque\\nstrain\\nbridges\\nstrain measurement\\nrobot sensing systems\\ntorque measurement\\nbending\\ncondition monitoring\\npower transmission (mechanical)\\nprototypes\\nstrain sensors\\nvelocity measurement\\nvibration measurement\\ndesign engineering\\ntransportation\\nspeed measurement\\nbending measurement\\nstrain sensor\\nprototype\\nmechanical power transmission\\nrotating shafts\\nrobotic system\\nindustrial equipment\\npower generation system\\nhealth monitoring\\nlow-cost shaft sensor\",\"200\":\"robots\\nforce\\nforce control\\naerospace electronics\\ncollision avoidance\\nplanning\\nvelocity control\\ncontrol system synthesis\\nindustrial manipulators\\nmobile robots\\noptimisation\\npath planning\\ntrajectory control\\nrobust execution\\ncontact-rich motion plans\\ntrajectory hybrid servoing\\nhybrid force-velocity control actions\\ncontrol synthesis\\npositional errors\\noptimization problems\\ncontact-rich manipulation tasks\",\"201\":\"endoscopes\\ntendons\\nforce\\nshape\\nrobot sensing systems\\nactuators\\ncantilevers\\nforce control\\nmanipulator dynamics\\nmedical robotics\\nskin\\nsurgery\\ncantilevered configuration\\nenvironmental scaffolding\\ncontact forces\\ntip force estimation\\nactuation forces\\nskin wounds\\nlaparoscopic techniques\\nendoscopic surgery\\nintrinsic sensing\\nendoscope force generation\\nfinite element analysis\\nintrinsic force measurement\\nminimally invasive surgery\",\"202\":\"crowdsourcing\\ngames\\ntask analysis\\nservice robots\\nautomation\\ncollaboration\\ncognition\\ncomputer games\\nhuman computer interaction\\nmobile robots\\ncognitive versatility\\nindustrial automation\\nrobotics applications\\nhuman-machine collaboration\\ngamification\\nvideo games\\nhuman workers\\nautonomous intelligent systems\\ncrowd computation\",\"203\":\"fatigue\\ntorque\\ntask analysis\\nmuscles\\nload modeling\\ntools\\nrobots\\nbiomechanics\\nelectromyography\\nergonomics\\nhuman-robot interaction\\ninjuries\\nmedical disorders\\nmedical robotics\\noccupational health\\noccupational safety\\npatient monitoring\\nrisk analysis\\nwork-related musculoskeletal disorders\\nelectromyography analysis\\noverloading fatigue model\\nrisk factors\\nexcessive fatigue accumulation\\nhrc framework\\npainting task\\nfatigue ratio parameter\\njoint torque variations\\nrobot assistance\\nbody posture optimisation procedure\\nhuman-robot collaboration framework\\nlight payloads\\noverloading torque\\ncumulative effect\\nwhole-body fatigue model\\nhuman joints\\nsevere injuries\\nlocal muscle fatigue\\nlight-weight tools\\nmonotonous movements\\nwmsd\\nergonomic risk assessment\",\"204\":\"foot\\ntorque\\nrobot kinematics\\nmathematical model\\ntask analysis\\nlegged locomotion\\nnonlinear control systems\\npendulums\\nstability\\nhuman-inspired balance model\\nfoot-beam interaction mechanics\\nbipedal robots\\nmediolateral balance\\nfoot-beam interaction dynamics\\nhuman balancing behavior\\nhuman controller\\nwhole-body behavior\\nbalance controllers\\nfoot contact conditions\\nfoot-ground interaction dynamics\\ndouble inverted pendulum model\",\"205\":\"ergonomics\\nservice robots\\nrobot sensing systems\\nwrist\\nreal-time systems\\nhuman-robot interaction\\nimage colour analysis\\npose estimation\\nhuman-robot relative position\\nhuman ergonomics\\ncooperative robot movements\\nreal-time robot-assisted ergonomics\\nhuman user posture\\nrgb-d camera\",\"206\":\"cloud computing\\nservice robots\\nrobot sensing systems\\nlegged locomotion\\nvisualization\\ncontrol engineering computing\\ninternet\\nmobile robots\\nmotion control\\nmulti-robot systems\\nobject recognition\\nposition control\\nrobot vision\\ntelerobotics\\nvisual servoing\\ndynamic visual\\ncloud robotics\\nmultiple robots\\ncloud services\\nunlimited computation power\\nnetwork communication\\ndynamic compliant service robots\\nhuman compliant service robots\\ndynamic self-balancing robot\\ncloud teleoperation\\ncloud-based image based visual servoing module\\ncloud teleoperator\\nreal-time automation system\\ncloud-edge hybrid design\\ndynamic robotic control\\ndeep-learning recognition systems\\nself-balancing service robot\\nfog robotic object recognition system\",\"207\":\"security\\nobservers\\nrobot sensing systems\\nprobabilistic logic\\nboolean functions\\nprobability\\nbinary decision diagrams\\ncomputational complexity\\ncontrol system security\\ngraph theory\\nmulti-robot systems\\nnetworked control systems\\noptimisation\\nmrs\\napproximate probabilistic security\\nnetworked multirobot systems\\ncombinatorial optimization problem\\nlower bound estimate\\nexact probability\\noptimal subset\\nmultipoint optimization\\nleft invertiblility\\ndisjoint path sets\\nonline optimization\",\"208\":\"robot sensing systems\\nshape\\nrobot kinematics\\nself-assembly\\nmaintenance engineering\\ndecentralised control\\ngeometry\\nmobile robots\\nmulti-robot systems\\nposition control\\ndecentralized formation control strategy\\nassembled triangulation\\ninfinitesimally shape-similar formations\\nasymptotic controllers\\ndifferential-drive robots\\ndecentralized heterogeneous control strategy\\nsensing modalities\\nmultirobot team\\ninfinitesimal shape-similarity\",\"209\":\"robot kinematics\\ntask analysis\\npartitioning algorithms\\nvegetation\\nprotocols\\npeer-to-peer computing\\nmobile robots\\nmulti-robot systems\\npath planning\\ntrees (mathematics)\\nmobile multirobot system\\narbitrary robot deployments\\nrobot initial positions\\nrobot configuration\\nonline network formation problem\\nasynchronous network formation\\nunknown unbounded environments\\nonfp\\nbounded communication range\\ncompetitive ratio\\neuclidean minimum spanning tree\",\"210\":\"wireless communication\\nswitches\\ncommunication system security\\nwireless sensor networks\\nnetwork topology\\nsecurity\\ntopology\\ngraph theory\\nmulti-robot systems\\nprobability\\ntelecommunication network topology\\ntelecommunication security\\ntelecommunication switching\\nwireless channels\\nwireless lan\\nresilient consensus\\nphysical networks\\nmultiagent consensus\\nsybil attack\\nphysical properties\\nwireless transmissions\\nswitching signal\\nuntrustworthy agents\\narbitrary malicious node values\\ninitial topology\\nconnected topology\\nlegitimate agents\\nwi-fi signals\\nsocietal integration\\nmultirobot team security\\nuntrustworthy transmissions\\nswitching topology\\ntrue graph\",\"211\":\"roads\\ntrajectory\\nplanning\\nautomation\\noptimization\\niterative methods\\nmixed integer linear programming\\ninteger programming\\nlinear programming\\npath planning\\nmultivehicle trajectory optimisation\\nroad networks\\nplanning time-optimal trajectories\\nmultiple cooperative agents\\nstatic road network\\nvehicle interactions\\nnontrivial decisions\\ncomplex flow-on effects\\nglobally optimal time trajectory\\nminimum time trajectory\\nmilp\\ncomputational performance\\nbinary variables\\ncollision constraints\\nopen-pit mining scenario\\ngoal constraints\\nheuristic method\",\"212\":\"mathematical model\\natmospheric modeling\\nbiological system modeling\\noutput feedback\\nrobot sensing systems\\nrobot kinematics\\nautonomous aerial vehicles\\ncompensation\\ncontrol system synthesis\\nfeedback\\nmobile robots\\nnonlinear control systems\\nobservers\\nrobust control\\nuncertain systems\\nblimp disturbance compensation based controller\\nindoor blimp robot\\nrobust controller\\nhorizontal plane\\nslider-like nonlinear system\\nuncertain bounded disturbances\\noutput feedback controller\\ndisturbance evaluation\\nexogenous disturbances\\ncontrol scheme\\nconcrete blimp\\nhomogeneous differentiator\\nobserver\",\"213\":\"task analysis\\noptimal control\\ntrajectory\\ndecoding\\nrobots\\nplanning\\ncost function\\nnonlinear control systems\\nnonlinear dynamical systems\\noptimisation\\npredictive control\\nrobust control\\nsampling methods\\ninformed sampling distribution\\noptimized controls\\ninformed information theoretic model predictive control\\nuncertainties\\nhighly nonlinear dynamics\\ncontextual information\\ngenerative models\\ntrajectory control\\nsampling-based mpc methods\\nrobustness properties\\nconditional variational autoencoders\\nautonomous navigation domain\",\"214\":\"optimization\\nkinematics\\nmanipulators\\nrobot sensing systems\\nreal-time systems\\nplanning\\nangular velocity control\\nclosed loop systems\\nend effectors\\nmobile robots\\nmotion control\\noptimisation\\nprobability\\nrobot vision\\nservice robots\\nsequential phases\\nclosed loop perspective\\ngeneric optimization-based cartesian controller\\nmotion commands\\nrobotic system\\nmobile platform\\nvelocity space\\nend effector velocity\\njoint platform velocities\\nbase platform velocities\\nmobile service robot architecture\\ndomestic tasks\\nrobotic mobile manipulation\\nrandom arm configurations\",\"215\":\"task analysis\\nrobot sensing systems\\nrobustness\\noptimized production technology\\nestimation\\ninformation theory\\niterative methods\\nrobots\\nstate estimation\\ngeneral algorithmic framework\\ntask-driven estimation\\nstate representations\\ntask-driven representation\\ntask-relevant variables\\nperformant control policy\\nrobotic system control\\nprincipled algorithmic framework\\niterative algorithms\\ninformation bottleneck optimization problem\",\"216\":\"acceleration\\nforce\\nstability criteria\\ncost function\\nheuristic algorithms\\nclosed loop systems\\nmotion control\\noptimal control\\npredictive control\\nroad traffic control\\nstability\\nfeasibility issues\\nimplicit model predictive control-based motion cueing algorithms\\nprediction horizons\\ncontrol inputs\\nfeasibility analysis\\nconstrained model predictive control based motion cueing algorithm\\nconstrained optimal control\\nhigh performance driving simulator\",\"217\":\"uncertainty\\nrobots\\npredictive models\\nbayes methods\\nneural networks\\ntraining\\ncollision avoidance\\nbelief networks\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nneurocontrollers\\nrobust control\\nuncertainty-aware robotic perception\\nexplicit generative model\\nobservation distribution\\naction-conditioned collision prediction task\\nbayesian neural network techniques\\ntask-aware generative uncertainty\\ndeep learning\\nopen world\\nreal-world robotic systems\\nout-of-distribution observations\\nneural network predictions\\nrobotic perception\\napproximate bayesian approach\",\"218\":\"trajectory\\npredictive models\\nroads\\nhidden markov models\\ntask analysis\\nradar tracking\\nconvolutional neural nets\\ndriver information systems\\nfeature extraction\\nimage processing\\nmobile robots\\nprobability\\nroad accidents\\nroad safety\\nroad traffic\\nroad vehicles\\nmultimodal trajectory predictions\\nautonomous driving\\ndeep convolutional networks\\nrobotics\\nartificial intelligence communities\\nself-driving vehicles\\nsdvs\\nhuman drivers\\ntraffic behavior\\nsafe operations\\nautonomous vehicle\\nraster image\\nprobabilities\",\"219\":\"hidden markov models\\ndecoding\\npredictive models\\nkernel\\ntraining\\nautomobiles\\niterative decoding\\nfeature extraction\\ngraphics processing units\\nimage classification\\nimage representation\\nimage sequences\\nlearning (artificial intelligence)\\npedestrians\\ntraffic engineering computing\\nvideo signal processing\\nencoder-decoder network models\\npredicted video\\nurban driving scenes\\ntraffic scene\\npedestrian behaviour\\nurban pedestrian\\nbinary action classifier\\niterative transform\\nimage sequence\\ngpu\",\"220\":\"convolution\\nvisualization\\nobject detection\\nfeature extraction\\nrobots\\ntraining\\ntask analysis\\nintelligent robots\\nsalient object detection task\\nobject saliency details\\nlightweight refinement module\\nhigh-level visual contrast\\nsuperior lightweight network architecture\\ncomputational resource consumption\\ndetection performance\\nattention-aware visual objects\\nattention-aware visual localization\\nlightweight contrast modeling\",\"221\":\"trajectory\\nrobots\\ntransforms\\ntask analysis\\nwriting\\nneural networks\\ndecoding\\naffine transforms\\nhandwritten character recognition\\nhumanoid robots\\nimage coding\\nimage motion analysis\\nlearning (artificial intelligence)\\nrobot vision\\ndmp\\ndigit drawings\\nimage-to-motion encoder-decoder networks\\nconvolutional layers\\nhumanoid robot\\naffine transformed digits\\nfully differentiable overall network\\nspatial transformer\\nmotion trajectories\\ndigit images\\nhandwritten characters\",\"222\":\"planning\\nthree-dimensional displays\\nneural networks\\ntraining\\nmanipulators\\nencoding\\ncollision avoidance\\ncomputational complexity\\nmobile robots\\nmotion control\\nneurocontrollers\\nneural network\\ncollision-free paths\\nmotion planning networks\\nrobotics applications\\nself-driving cars\\n7 dof baxter robot manipulator\\nmpnet\",\"223\":\"grasping\\ngrippers\\nrobot kinematics\\nartificial neural networks\\ntask analysis\\ntraining\\nconvolutional neural nets\\nforce feedback\\nlearning (artificial intelligence)\\nmanipulators\\nrobot vision\\nfully-convolutional neural network\\ngripper parameters\\ninference performance\\nrandom grasp success rate\\ngrasp space\\nsystematic manner\\ntypical bin picking scenarios\\nself-supervised learning\\nrobotic grasping\\ndepth camera input\\ngripper force feedback\\nlearning algorithm\\ngeometric consistency\\nundistorted depth images\\ntask space\\ngrasp attempts\\ndata efficiency\\ntraining data\",\"224\":\"robots\\neducation\\nthree-dimensional displays\\ntask analysis\\nfeature extraction\\nhuman-robot interaction\\nforce control\\nlearning (artificial intelligence)\\nmotion control\\nneural nets\\ndeep learning based localization\\nrecognition system\\nintuitive user interface\\n3d motion task\\nhybrid force-vision control module\\ncompliant motion\\ntask learning\\nhuman robot interaction\\nkuka innovation award competition\\nhanover messe 2018\\nonline object learning\\nincremental object learning module\",\"225\":\"torque\\nmathematical model\\ndynamics\\noptical imaging\\nmanipulator dynamics\\nreal-time systems\\ncontrol engineering computing\\nflexible manipulators\\nmotion control\\nneural nets\\ntime series\\ntorque control\\ndynamic manipulation\\ntorque sequence\\ndeep neural network\\nacquisition method\\nflexible object motion equation model\\ntime-series joint torque command\",\"226\":\"optical sensors\\ntactile sensors\\nimage color analysis\\ncameras\\nrobots\\ncontact localization\\nrobotic perception system\\nforce sensing range\\ncolor-coded tactile sensor\\ntactile exploration\\nrobust tactile sensing\\ncolor-coded fiber-optic tactile sensor\\nelastomeric robot skin\\nartificial tactile skin\\ntransparent silicone rubber\\noff-the-shelf color camera\\ncamera pofs\",\"227\":\"robot kinematics\\nhumanoid robots\\nmanipulators\\nlaplace equations\\ntorso\\nshape\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmedical robotics\\npath planning\\npatient care\\nposition control\\ntopology-based representation\\nhuman body movement\\nbulky object\\nwam\\nmanipulation places\\nglobal properties\\nlocal contacts\\ngrasping\\nreinforcement learning problem\\nrobot behavior\\nhuman motion\\nrobot-human interaction\\ntopology-based coordinates\\ntorso positions\\nlearned policy\\nbody shapes\\ndynamic sea rescue scenario\\nunseen scenarios\\ndifferently-shaped humans\\nwhole arm manipulation\",\"228\":\"training\\nhumanoid robots\\ntask analysis\\nrobot kinematics\\nreinforcement learning\\nconvergence\\ndexterous manipulators\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmotion control\\nneural nets\\nhandshake\\nhand clap\\nfinger touch\\ntraining control policies\\nhuman-robot interactions\\nhand claps\\nhumanoid shadow dexterous hand\\nrobot arm\\nmultiobjective reward function\\nreward structure\\nmotion capture data\\nhuman-human interactions\\nhand interactions\\ndexterous human-robot interaction\\ndemonstration-guided deep reinforcement learning\",\"229\":\"force\\nfriction\\nshape\\nactuators\\nrobot kinematics\\npotential energy\\ncontrol system synthesis\\nmobile robots\\nmulti-robot systems\\nshells (structures)\\nteam-based robot righting\\nminimalist robot designs\\nspecialized escape actuator\\nemergency actuator\\nteammate pushing\\nvelociroach robots\\nrobot exterior hull\\nshell design\",\"230\":\"robot kinematics\\nshape\\nstrain\\ntask analysis\\ngeometry\\nshape control\\nfeedback\\ngradient methods\\nmanipulators\\nmobile robots\\nmulti-robot systems\\n2d space\\nuseful team behaviors\\ndeformation-based control framework\\nmanipulation task\\nresulting multirobot controller\\nrobot motions\\nfeedback loop\\nglobal measure\\ntypical goal\\ndeformable object\\napplication scenario\\nrobotic team\\nmultirobot system\\ndeformation-based shape control\",\"231\":\"task analysis\\nrobot kinematics\\nresource management\\nbipartite graph\\napproximation algorithms\\ntime complexity\\napproximation theory\\ncomputational complexity\\ngraph theory\\nmulti-robot systems\\nmultirobot task allocation\\nmultirobot coalition formation\\notmam problem\\nmultiple robots\\nrobot-task pairs\\nworst-case approximation ratio\\nworst-case time complexity\\nnp-hard problem\\none-to-many bipartite matching based coalition formation\",\"232\":\"robot kinematics\\ncollision avoidance\\nprotocols\\ntask analysis\\nprivacy\\nencryption\\ncryptography\\ndata privacy\\nmobile robots\\nmotion control\\nmulti-robot systems\\ncoordinated multirobot planning\\nindividual privacy\\nmultiple robots\\nprivacy-preserving rendezvous\\npersistent monitoring\\njoint plan\\ncollective motions\\ncollective task\\njoint-collision determination\\nsecure path intersection primitives\\nhomomorphic encryption\",\"233\":\"mathematical model\\nadaptation models\\noptimal control\\ndynamic programming\\nsynchronization\\ngames\\nneural networks\\nadaptive control\\napproximation theory\\ndiscrete time systems\\niterative methods\\nlearning systems\\nmulti-agent systems\\nneurocontrollers\\nnonlinear control systems\\naction dependent dual heuristic dynamic programming schemes\\nfast solution platforms\\nunknown models\\nuncertain dynamical models\\nonline model-free adaptive\\ndynamic graphical games\\napproximate the optimal value function\\nassociated model-free control strategy\\nmodel-free coupled bellman optimality equation\\nmultiagent synchronization\\nonline model-free action dependent dual heuristic dynamic programming approach\\napproximate dynamic programming platforms\\nagents interaction\\ncommunication graphs\\npolicy iteration process\",\"234\":\"robot kinematics\\nrobustness\\nmulti-robot systems\\nmaintenance engineering\\nrobot sensing systems\\ntask analysis\\ncomputational geometry\\nrobust swarm connectivity\\ncoverage task\\nkhepera iv robots\\ntri-objective control law\\npotential-based coverage\\nnetwork connectivity\\nrobust area coverage\\nconnectivity maintenance\\nrobotic swarm\\ncontrol laws\\nswarm robotics\\nglobally coordinated behavior\\nvoronoi tessellation\",\"235\":\"tactile sensors\\nthree-dimensional displays\\nlegged locomotion\\nnavigation\\ngeriatrics\\nhome automation\\nmobile robots\\nservice robots\\nmobile companion robot\\napartment\\ngerman research project sympartner\\nfunctional-emotional robot companion\\nmobile domestic robot companion\\nelderly people\\ndeveloped robot\\nsystem architecture\\nessential skills\\nfriendly home companion\\nlong-term field study\\nrobustness\\ndomestic operating conditions\\nsocial scientific questions\\nautonomous companion robots\\nsingle-person households\\nsenior households\\ntime 20 week\",\"236\":\"skeleton\\ncameras\\nrobot sensing systems\\ntarget tracking\\nrobot kinematics\\ncalibration\\nrobot vision\\nsensor fusion\\nsynchronisation\\nrobust pit\\ndata fusion technique\\nrgb-d camera\\nwearable inertial sensors\\ntime synchronization\\nrobotic platform\\nbiological feature\\nrecognition rate\\nvisual features\\ninertial features\\ncomputer vision\\nrobotic applications\\nrfid\\nenvironmental constraints\\nrecognition accuracy\\nidentity-aware tracking\\ndata fusion\\niot\\nperson identification\\ntracking\\nrobotics\\nwearable computing\",\"237\":\"robots\\ntask analysis\\neducation\\nreinforcement learning\\nuncertainty\\naircraft navigation\\nhuman-robot interaction\\ndecision theory\\nlearning (artificial intelligence)\\nmarkov processes\\ninverse reinforcement learning methods\\nhigh-level sequential tasks\\nhuman demonstrations\\npomdp policies\\ninteraction dynamics\\nhuman-robot interaction domain\\nstructured interactions\\npartially observable markov decision process\\nlearning from demonstration\\nirl algorithms\\nreward function learning\\nmdpirl algorithm\",\"238\":\"robots\\ndogs\\nanalysis of variance\\npediatrics\\npsychology\\nsynthetic fibers\\naging\\nage issues\\ndesign engineering\\nhealth care\\nhuman-robot interaction\\nmedical robotics\\nservice robots\\nolder adults\\nrobot design elements\\ndepression\\nphysical preferences\\npsychological health\\ndepressive symptoms\\ncompanion robots\",\"239\":\"robot sensing systems\\nkernel\\ninspection\\ntemperature measurement\\ngaussian processes\\nwelding\\nmobile robots\\noptimisation\\npath planning\\nrastering technique\\nhigh-level nuclear waste tanks\\nhanford facility\\nrobotic inspection\\nutility function\\ninformative path planning\\nhanford tank farm\\nstructural monitoring\\nstatic sensor networks\\npredetermined locations\\nbayesian optimization approach\",\"240\":\"indexes\\nangular velocity\\nfuzzy logic\\nrobot sensing systems\\nhistograms\\nmobile robots\\nangular velocity control\\ncollision avoidance\\ndisasters\\nfuzzy control\\nfuzzy reasoning\\nmotion control\\nnavigation\\nterrain accessibility index\\nrobots position\\nangular velocities\\nvfh algorithm\\ndisaster prone environment\\nfisvfh algorithm\\nfuzzy based accessibility model\\ndisaster environment\\nautonomous maneuvering\\nrobot estimate\\ntwo-output fuzzy inference system\\nsector accessibility index\\nfuzzy inference system vector field histogram method\\nobstacle distance\\nlinear velocities\\ntwo-input fuzzy inference system\",\"241\":\"wind forecasting\\nplanning\\ncomputational modeling\\nwind speed\\npredictive models\\ndata models\\nautonomous aerial vehicles\\nconvolutional neural nets\\nmobile robots\\npath planning\\nsampling methods\\nwind\\nsafe aerial vehicle planning\\nlocal wind\\nunmanned aerial vehicles\\nwind environment\\nrelatively low mass\\nhigh-resolution wind fields\\nterrain elevation model\\ndeep convolutional neural network\\nsampling-based planner\\nstrong wind scenarios\\nuav\\ninflow conditions\\nprediction error\\nwind estimation techniques\",\"242\":\"estimation\\ndetectors\\nrobot sensing systems\\npath planning\\nuncertainty\\narea measurement\\nglobal positioning system\\nmobile robots\\noptical radar\\nphotomultipliers\\nscintillation counters\\nsolid scintillation detectors\\nthallium\\ndistributed radiation field estimation\\ninformative path planning\\nnuclear environment characterization\\nautonomous estimation\\ndistributed nuclear radiation fields\\ngps-denied environments\\nsensing apparatus\\nradially placed thallium-doped cesium iodide\\nsilicon photomultipliers\\npulse counting circuitry\\nprovided readings\\nlidar-based localization\\nradiation intensity readings\\nimmediate field gradient\\nbelieved field intensity\\nlocal measurement\\nfield gradient co-estimation\\ninformative data gathering\\npath planning strategy\\nadmissible paths\\nautonomous exploration\\nsipm\",\"243\":\"training\\ncomputer aided instruction\\nvisualization\\nrobots\\nneural networks\\nlabeling\\ntask analysis\\nconvolutional neural nets\\nimage representation\\nlearning (artificial intelligence)\\nmobile robots\\nobject recognition\\npattern classification\\nprobability\\nprobabilities\\nclass variability\\nimages range\\ntraining exemplars\\nrelative similarities\\nclass predictions\\nclass labels\\ncnn\\ntest data\\ntraining data\\nrecognition system\\ntarget class\\ndeep convolutional neural networks\\nsupervised machine learning\\nautonomous robot\\ndeep similarity functions\\nvisual recognition\",\"244\":\"uncertainty\\nobject detection\\ndetectors\\nclustering methods\\nsemantics\\nmeasurement uncertainty\\nrobots\\nmerging\\nneural nets\\npattern clustering\\nsampling methods\\nstatistical analysis\\nmerging strategies\\nsampling-based uncertainty techniques\\nepistemic uncertainty\\ndeep neural networks\\naffinity-clustering combination\\nspatial uncertainty estimation\",\"245\":\"training\\ndetectors\\nneural networks\\nquantization (signal)\\nknowledge transfer\\ntask analysis\\nautonomous vehicles\\ncomputer vision\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nobject detection\\nroad traffic\\ntraffic engineering computing\\nbinary weight object detector\\nautonomous driving\\nenergy efficiency\\non-board object detection\\nobject detectors\\nlow-precision neural networks\\ncomputation requirements\\nmemory footprint\\nbinary weight neural networks\\nbwns\\nknowledge transfer method\\nfull-precision teacher network\\nmobilenet-based binary weight yolov2 detectors\\npedestrian\\ncyclist detection\\nkitti benchmark\\nmobilenet-yolo\\ndarknet-yolo\\ndeep convolutional neural network\\nword length 1.0 bit\\nmemory size 8.8 mbyte to 257.0 mbyte\\nmemory size 7.9 mbyte to 193.0 mbyte\",\"246\":\"simultaneous localization and mapping\\ncameras\\nestimation\\nbundle adjustment\\noptimization\\nvisualization\\nfeature extraction\\nimage sequences\\nmotion estimation\\noptimisation\\npose estimation\\nrobot vision\\nslam (robots)\\nvideo signal processing\\nfeature-based monocular slam\\ncamera orientation optimisation\\ncamera position estimation\\nquasiconvex formulation\\nkeyframe rate\\nslam optimisation\\nrotational motion\\nslow motion\\nslam algorithm\\n3d structure estimation\\ninput feature tracks\\n3d point cloud\\n3d map estimation\\n6dof camera trajectory estimation\\nvisual slam\",\"247\":\"lighting\\noptimization\\nrobustness\\nsimultaneous localization and mapping\\ncameras\\nmotion estimation\\nthree-dimensional displays\\ndistance measurement\\nimage colour analysis\\nmobile robots\\nrobot vision\\nslam (robots)\\nstereo image processing\\nillumination-robust direct monocular slam system\\nglobal lighting changes\\nlocal lighting changes\\nstereo slam systems\\ncamera motion\\nscene structure\\nhigh-precision motion estimation\\nillumination robust monocular direct visual odometry\\noutdoor environment\\nillumination invariant photometric costs\\nvision-based localization and mapping\\nrgb-d\\ndso system\\norbslam2 system\",\"248\":\"lighting\\ncomputational modeling\\ndetectors\\ndogs\\nsimultaneous localization and mapping\\nmeasurement\\nconvolutional neural nets\\nimage matching\\nneurocontrollers\\nrobot vision\\nslam (robots)\\npre-trained cnn descriptors\\nviewpoint changes\\nillumination changes\\nhand-crafted keypoint descriptors\\nkeypoint matching\\ncomputer vision\\nkeypoint description\\ntrained convolutional neural networks\\npre-trained cnns\\nhand-crafted descriptors\\nvisual simultaneous localization and mapping\\ncnn-based descriptors\\nslam\",\"249\":\"cameras\\ncalibration\\natmospheric modeling\\nsimultaneous localization and mapping\\ngeometry\\nmathematical model\\nautonomous underwater vehicles\\ninertial navigation\\nmobile robots\\nslam (robots)\\nvisual-inertial slam\\nshallow water\\ncalibration errors\\nenvironmental indexes\\nunderwater camera-inertial measurement unit\\nintrinsic parameters\\nextrinsic parameters\\nunderwater monocular vision systems\\nenvironment driven underwater camera-imu calibration\",\"250\":\"simultaneous localization and mapping\\nthree-dimensional displays\\ncameras\\nestimation\\noptimization\\nreliability\\nkalman filters\\nmaximum likelihood estimation\\npolynomials\\nslam (robots)\\nmultiple local atlanta frames\\nglobal atlanta frames\\nworld frame\\nstructural regularity\\nmonocular slam\\ncommon vertical axis\\nmultiple horizontal axes\\ncamera frame\",\"251\":\"simultaneous localization and mapping\\nsemantics\\nbelief propagation\\nmaximum likelihood estimation\\nmaximum likelihood detection\\nimage fusion\\nimage representation\\ninference mechanisms\\nmobile robots\\nobject detection\\npath planning\\nprobability\\nrobot vision\\nslam (robots)\\nnongaussian sensor model\\nmultimodal semantic slam\\nprobabilistic data association\\nrobot navigation\\nsemantic slam problem\\ndiscrete inference problem\\nobject class labels\\nmeasurement-landmark correspondences\\ncontinuous inference problem\\nrobot poses\\nobject detection systems\\nobject-based representations\\nobject locations\\nnongaussian inference problem\",\"252\":\"planning\\ndynamics\\nmobile robots\\ntrajectory optimization\\ncollision avoidance\\npath planning\\nrobots\\ntrajectory control\\nintegrated motion planning\\nchekov\\ntrajectory optimization problems\\nroadmap seed trajectories\\nmotion planning algorithms\\ncontrol information\\nincremental planning\",\"253\":\"task analysis\\ngrasping\\nkernel\\ngaussian processes\\nrobots\\ninference algorithms\\nprediction algorithms\\nbayes methods\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\nsearch problems\\nomsgps\\noptimal policies\\nreinforcement learning\\nmultimodal policy search algorithm\\noverlapping mixtures of sparse gaussian process\\nbayesian inference\\nobject grasping\\ntable-sweep tasks\",\"254\":\"adaptation models\\ntask analysis\\nphysics\\nplanning\\nrobots\\nengines\\nneural networks\\nmarkov processes\\nmobile robots\\nneural nets\\nplanning (artificial intelligence)\\nuncertain systems\\nonline adaptation\\nuncertain models\\nneural network priors\\npartially observable planning\\nmanipulation tasks\\nencode prior experiences\\nphysics engine\\nonline pomdp solver\\nobserved environments\\nprediction model\\ndomain complexity\",\"255\":\"robots\\nconvergence\\ntrajectory optimization\\ntask analysis\\nheuristic algorithms\\ngradient methods\\nlinear quadratic control\\nmanipulators\\nmobile robots\\noptimisation\\ntrajectory control\\ncito\\nvariable smooth contact model\\nvscm\\nsuccessive convexification\\ngradient-based optimization\\nnonprehensile manipulation tasks\\niterative linear quadratic regulator\\nphysically-consistent motions\\nscvx-based method\\nilqr-based method\\ncontact-implicit trajectory optimization method\\nrobot platform\",\"256\":\"actuators\\ncomputational modeling\\ngrippers\\nforce\\nplanning\\nenergy states\\ntrajectory\\ndexterous manipulators\\ngradient methods\\ngraph theory\\nmobile robots\\npath planning\\nhand-object configurations\\nyale t42 hand\\nstored energy profile\\nenergy gradient-based graph\\nenergy map\\nlow energy states\\nactuation input\\nunderactuated hands\\ncaging grasps\\nwithin-hand caging manipulation\",\"257\":\"legged locomotion\\nstrain\\nforce\\nsurface treatment\\ntrajectory\\nganglia\\ncantilevers\\nelasticity\\nhydrodynamics\\nmotion control\\nrobot dynamics\\nwater surface\\nflexible driven robot prototype\\nwater strider robot\\nflexible driving legs\\nflexible driving robot\\nbiological water striders\\nrobot skating\\nflexible materials\\nellipse-like spatial trajectories\\npin-linkage mechanism\\nmicroelement cantilever method\\nsimilarity analysis\\nhydrodynamic characteristic constants\",\"258\":\"magnetic resonance imaging\\nelectromagnets\\nrobots\\nmicromagnetics\\nelectromagnetics\\ncoils\\ntorque\\nelectromagnetic actuators\\nmedical robotics\\nmicrorobots\\nsurgery\\ntelerobotics\\nrobotic ema platform\\nophthalmic mis\\nmagnetic force\\ndexterity indexes\\nperformance metrics\\nrobotic actuation system\\nwireless magnetic microrobots\\nsmall-scale minimally invasive surgery\\nelectromagnetic actuation system\\nreliable medical applications\",\"259\":\"torque\\nactuators\\nharmonic analysis\\nforce\\npower harmonic filters\\nshape\\nresonant frequency\\naerodynamics\\naerospace components\\naircraft control\\ncontrol nonlinearities\\nmicrorobots\\nmobile robots\\npiezoelectric actuators\\nreliable torque\\ncontrollable yaw torque\\nyaw torque authority\\nflapping-wing microaerial vehicle\\nhigh-frequency wing\\nreliable yaw control authority\\nsplit-cycle flapping\\nflapping frequency\\nflapping signal\",\"260\":\"morphology\\noptimization\\nrobots\\nbayes methods\\nhardware\\nprocess control\\ntask analysis\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nlegged locomotion\\nmicrorobots\\noptimisation\\ndata-efficient learning\\nrobot design\\nhpc-bbo\\nhierarchical bayesian optimization process\\nmorphology configurations\\ncontroller learning process\\nhardware validation\\nhardware configurations design\\n6-legged microrobot\",\"261\":\"magnetic separation\\nmagnetic confinement\\nsaturation magnetization\\ncatheters\\nmagnetic cores\\nmicromagnetics\\nmagnetic particles\\nmedical robotics\\nmicrorobots\\npatient treatment\\nmagnetoresponsive agents\\ncapture efficiency\\nhuman body\\nuntethered magnetic microrobots\\nmagnetic medical microrobots\\nbody compartments\\nretrieval catheter\\nretrieval tools\\nmagnetic capture model\\nbloodstream\\nclinical translation\\nbody fluids\\nmagnetic microrobots retrieval\\nmagnetoresponsive microrobots\",\"262\":\"task analysis\\nknee\\nhumanoid robots\\nlegged locomotion\\nnull space\\nend effectors\\ngait analysis\\nhuman-inspired behaviors\\ntoe-off motion\\nstretched knees\\nlocomotion patterns\\nflat foot-ground contact\\nhuman gait\\nphysiological mechanisms\\ntoro dlr humanoid robot\\nquasistatic whole-body balancing\\nkinematic capabilities\\nquasistatic whole-body balancing controller\",\"263\":\"three-dimensional displays\\ntracking\\nreal-time systems\\nmobile robots\\ntask analysis\\nhumanoid robots\\ncollision avoidance\\nrobot vision\\nhumanoids\\nsmaller wheeled robots\\nplanar regions\\nsimple 2d occupancy map\\nenvironment representation\\nheight information\\nprediction maps\\nreal-time 3d footstep planning\\ndynamic obstacle detection\\ntime 10.0 ms\",\"264\":\"cameras\\nrobot vision systems\\neyelids\\nprototypes\\nacceleration\\nhumanoid robots\\neye\\nhuman-robot interaction\\ninteractive systems\\nmobile robots\\nrobot vision\\nintegrated high resolution camera\\nrobot eye\\nhuman eye movements\\nhigh-definition camera-eye\\nanthropomorphic robots\\nsocial robots\\nrobotic faces\\nanthropomorphic face\\nanthropomorphic robot head floka\",\"265\":\"quaternions\\ntrajectory\\nprobabilistic logic\\ntask analysis\\nrobots\\nangular velocity\\ntransforms\\ncontrol engineering computing\\nend effectors\\nhumanoid robots\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmobile robots\\nmotion control\\nposition control\\ngeneralized orientation learning\\nrobot task space\\nimitation learning\\nhuman skills\\njoint space\\nend-effector orientation\\narbitrary desired points\\nadapting learned orientation skills\\nlearning cartesian positions suffices\\nlearning multiple orientation trajectories\\nkernelized treatment\\ndynamic movement primitives\",\"266\":\"synchronization\\nclocks\\nmobile nodes\\nreliability\\nwireless communication\\nbatteries\\ndirection-of-arrival estimation\\nmobile robots\\noperating systems (computers)\\npublic domain software\\ntime-of-arrival estimation\\natlas fast\\nfast scheduled tdoa\\nsimple scheduled tdoa\\nultra-wideband localization\\nwireless localization\\naerial robot control\\nhigh precision personal safety tracking\\nrequired localization systems\\nmultiuser scalability\\nenergy efficiency\\nreal-time capabilities\\nreal-time localization\\nrobot operating system\\nopen source access\\nprecise robotic location estimation\\nscheduled time-difference of arrival channel access\",\"267\":\"roads\\nglobal navigation satellite system\\nmeasurement\\nvisualization\\nautomobiles\\nrobot sensing systems\\nfeature extraction\\nimage classification\\nlearning (artificial intelligence)\\nobject detection\\nparticle filtering (numerical methods)\\nprobability\\nroad vehicles\\ntraffic engineering computing\\nautomated driving\\nlandmark readings\\nprobability distribution\\nhd map change detection\\nboosted particle filter\\nchange detection algorithm\\nbackend-based stream processing pipeline\\nfloating car data\\nseries-production vehicles\\nautomotive high definition digital map\\ncrowd-based approach\",\"268\":\"antenna measurements\\ndelays\\nprobability density function\\nantennas\\noptimization\\ndensity measurement\\nhardware\\noptimisation\\nprobability\\nultra wideband antennas\\nwireless sensor networks\\nantenna delays\\nline-of-sight conditions\\nnonline-of-sight conditions\\nnonparametric error modeling\\nultra-wideband localization networks\\nnonparametric estimation\\nmeasurement probability densities\\nexplicit modeling\\nmultimodal errors\\nlinear estimation methods\\nsize 3.0 cm\\nsize 30.0 cm\",\"269\":\"feature extraction\\nrobots\\nacoustics\\npredictive models\\nindoor environment\\nmicrophones\\ndata models\\ndirection-of-arrival estimation\\ngeometry\\nlearning (artificial intelligence)\\nmicrophone arrays\\nmobile robots\\ngeometry features\\nself-supervision process\\nground truth label\\npre-collected data\\nhuman supervisions\\nexplicit gcc-phat features\\nsupervised incremental learning\\nsound source localization\\ncomplex indoor environment\\nincremental learning framework\\nhuman sound source\\nmicrophone array\\nmultiple rooms\\ntraining data\\nprediction model\\nincremental learning scheme\\nimplicit acoustic features\\ntraining samples\",\"270\":\"solid modeling\\ntask analysis\\nrobots\\nforce\\ngames\\nvirtual reality\\ncomputer games\\nknowledge acquisition\\nvirtual reality technology\\nhuman everyday manipulation activity\\nvirtual human living\\nworking environments\\nrecorded activity data\\nhuman manipulation activities\\nameva\\nautomated models of everyday activities\\nknowledge interpretation\\nknowledge processing system\\nknowrob\",\"271\":\"wheels\\nrobot sensing systems\\nstrain measurement\\ntask analysis\\nmobile robots\\naxles\\ncompensation\\nforce control\\nmotion control\\nnonlinear control systems\\npendulums\\nposition control\\nrobot dynamics\\nservice robots\\nstrain gauges\\nstrain gauge based disturbance estimation\\ncompensation technique\\nwheeled inverted pendulum robot\\ni-pentar\\nstrain gauge based sensor\\npurely estimation based control\\ninverted pendulum type assistant robot\",\"272\":\"hidden markov models\\npredictive models\\nmarket research\\nmobile robots\\nspectral analysis\\ntime series analysis\\npath planning\\nservice robots\\nspatiotemporal phenomena\\nspatio-temporal representation\\nlong-term anticipation\\nservice robotics\\nmobile autonomous robots\\nhuman populated environments\\nwrapped dimensions\\nperiodicities\\n2d spatial model\\nmultidimensional representation\\nmemory efficient spatio-temporal model\\nlong-term predictions\\nperiodic temporal patterns\",\"273\":\"handover\\nreceivers\\nrobot kinematics\\nestimation\\ndynamics\\nestimation theory\\nhumanoid robots\\nhuman-robot interaction\\nmobile robots\\nmotion control\\nposition control\\nrobot vision\\ncollaboration tasks\\noffline otp\\nhuman preferences\\ndynamic otp\\notp predictor\\nhumanoid nursing robot\\nhandover motion\\nreach-to-grasp response time\\nnatural human receiver\\nhuman-robot handovers\\nhuman-robot motion\\nobject transfer point estimation\\nrobot visible workspace\\nuser-adaptive reference frame\\ntime 3.1 s\",\"274\":\"trajectory\\nrobot kinematics\\nkinematics\\nstability analysis\\nlegged locomotion\\ncontrollability\\nhumanoid robots\\nmotion control\\npath planning\\nposition control\\nquadratic programming\\nstability\\nvelocity control\\nwheels\\nrobot morphologies\\nlegged robots\\nhumanlike form factor\\nwheeled systems\\nmotion planner\\nactively articulated wheeled humanoid robot\\ngap crossing\\ndynamically stable motion\\nrobots kinematics\\nsequential quadratic program algorithm\\npostural configuration adjustment requirements\\ndrive wheels\\nterrain adaptability\\nenergy consumption\\naerobot robot\\nstep climbing\\nnonlinear constraints\\nzero moment point\",\"275\":\"navigation\\nrobots\\ntracking\\nfixtures\\nmonitoring\\nmarket research\\nmedical services\\ncomputerised tomography\\nhealth care\\nimage registration\\nmedical image processing\\nmedical robotics\\nsurgery\\nuser centric device registration\\nregistration gesture\\npreoperative registration\\nmedical imaging modalities\\ndevice tracking\\nsurgical navigation systems\",\"276\":\"springs\\ninstruments\\nstrain\\nsurgery\\nmanipulators\\nmedical robotics\\nbending\\nfinite element analysis\\nneedles\\noptimisation\\nlocally deformable elastic elements\\nminimally invasive surgery\\nmis\\nsurgical robots\\nrobotic technology\\ncompliant four degree-of-freedom manipulator\\nmechanical parts\\nrobotic instruments\\noptimization method\\nfea\\nprototype implementation\",\"277\":\"laparoscopes\\nfriction\\nsurgery\\nrobots\\noptimized production technology\\nforce\\nkinematics\\nhaptic interfaces\\ninteractive devices\\nmedical robotics\\nposition control\\ntelerobotics\\nthree-term control\\nrigid laparoscope holders\\nhigh-gain pid position control\\ndynamic precision\\nundetected obstacles\\nstiff systems\\ncompliant behaviour\\nunknown friction\\ncompliant- scope holder\\n-precise laparo-scope holder\\nhigh backdrivability\\nintelligent pid position controller\\nlow pid gains\\nsatisfactory tracking precision\\nsafe teleoperation\\nlaparoscope holder\\nlow stiffness\\nminimally invasive surgery\\nmis\\nvisual feedback\\nhuman assistant\\nend-effector\\nrobotic assistant\\nlaparoscope displacements\\nmaster input interface\\nvoice control\\nslave level\",\"278\":\"needles\\ntask analysis\\nhaptic interfaces\\nrobots\\nsurgery\\ntrajectory\\nthree-dimensional displays\\nbiological tissues\\nbiomedical ultrasonics\\nforce feedback\\nmedical image processing\\nmedical robotics\\nphantoms\\ntelemedicine\\ntelerobotics\\ngelatin phantom\\nreal-time teleoperation\\n3d ultrasound guidance\\nflexible beveled-tip needle\\ntip trajectory\\nuser-controlled tasks\\nhaptic force feedback\\n3d ultrasound probe\\nreal-time visual feedback\\nhaptic interface\\nneedle tip\\nbeveledtip flexible needle steering\\nteleoperation framework\\ncontrol loop\\nrobotic systems\\nneedle insertion procedures\",\"279\":\"task analysis\\nvisualization\\ntwo dimensional displays\\nvirtual reality\\nprogramming\\nrobot sensing systems\\naugmented reality\\ndexterous manipulators\\nhelmet mounted displays\\nrobot programming\\nmr-hmd interface\\nend-user robot programming\\nimmersive 3d visualization\\nhand gestures\\nrobot motions\\nrobot arm\\nmixed reality head-mounted display\\npick-and-place programs\",\"280\":\"teleoperators\\nstability analysis\\nservice robots\\ninternet\\nreal-time systems\\nhaptic interfaces\\ncontrol system synthesis\\ndelays\\nmedical robotics\\nmobile robots\\nstability\\ntelerobotics\\nrobotic tele-echography\\ncontrollerfor bilateral robotic system\\ndelayed communications\\nidentical 6dof robotic devices\\nstewart-gough mechanisms\\ncontroller design\\nteleoperation setup\\nslave devices\\nidentical geometry\\nmeasurement arrays\\nindependent idof settings\\nintuitively shape teleoperator performance\\ncontrol method\",\"281\":\"robots\\nsurgery\\noptimization\\njacobian matrices\\nquaternions\\nkinematics\\nlaparoscopes\\ndexterous manipulators\\nforce feedback\\nmedical robotics\\noptimisation\\ntelerobotics\\nconstrained workspaces\\nrobot geometry\\nslave-side constrained optimization algorithm\\nrobotic systems\\nadult laparoscopy\\ninfant surgery\\nsurgical robots\\nrobot-aided surgery\\noperating rooms\\nrobotic tools\\nrobot control techniques\\npediatric surgery\\nmicrosurgery\\nnonredundant robots\\nteleoperation\\ndexterity\",\"282\":\"robots\\nshafts\\ntask analysis\\nshape\\nforce\\ngrippers\\nkinematics\\nassembling\\ndesign of experiments\\ndexterous manipulators\\nerror compensation\\nmasks\\nposition control\\nrobot hand\\nimpact reduction\\nposition-error compensation\\nobjects contact\\ndoc hand\\n6-degrees-of- freedom dynamic passivity\\nrobot system\\nhigh-speed precision assembly\\ndynamic observable contact hand\\nmultifingered hand\\nhigh speed ring insertion\\ntime 2.42 s\\ntime 2.58 s\",\"283\":\"uncertainty\\ngrippers\\ngrasping\\nplanning\\nsensors\\nshape\\ncomputational modeling\\nhumanoid robots\\nlearning (artificial intelligence)\\nmarkov processes\\nobject detection\\nrecurrent neural nets\\nrobust control\\nservice robots\\nuncertainty handling\\nvisual sensing\\npartially observable markov decision process\\ngrasp policy\\ndeep recurrent neural network\\nimitation learning\\nmodel-based pomdp planning\\ng3db object dataset\\nfar-field sensors\\nopen-loop grasp\\ntactile sensing\\nadaptive grasping\\nrobust object grasping strategy\",\"284\":\"robots\\ngrippers\\ntendons\\nactuators\\ngrasping\\nforce\\nfasteners\\ncontrol system synthesis\\nuncertain systems\\nsoft scoopgripper\\nunderactuation\\nrobust grasps\\nrobotic gripper design\\nmodular under actuated soft hand\\nobject grasping\",\"285\":\"grippers\\nstrain\\ncapacitive sensors\\nrobot sensing systems\\nservomotors\\npressure sensors\\nauxetics\\ndeformation\\nfeedback\\nhaptic interfaces\\ngrasping tasks\\nelectric soft robotic gripper\\nfabrication techniques\\nsensorized system\\nobject classification\\ngripper proprioception\\ncoupling deformable sensors\\nelectric motors\\nstructurally-compliant handed shearing auxetic structures\\nhigh-deformation strain\\ncomplex driving hardware\\nproprioceptive soft robotic grippers\\nproprioceptive feedback\\ntactile feedback\\nmanipulation tasks\\ncompliant robotic grippers\\nhigh-deformation haptic feedback\",\"286\":\"measurement\\nrobots\\ngrasping\\nsurface morphology\\ngeometry\\nmorphology\\nmachine learning algorithms\\ngrippers\\nlearning (artificial intelligence)\\nkinematic noise\\ncontact points\\nbinary grasp success classifier\\nhand morphologies\\nhand-object geometric relationships\\nnear-contact behavior\\nnear-contact stage\\nfeature representations\\nrobotic grasping\\ngeometric features\",\"287\":\"optimization\\nsilicon\\nsolid modeling\\ngraphics processing units\\ngenetic algorithms\\nthree-dimensional printing\\nparallel algorithms\\nproduction engineering computing\\nparallel genetic algorithm\\n3d printing\\nadditive manufacturing\\nga\\nsingle-objective optimization\\nbuilding time\\nmultiobjective optimization problem\\nmodel orientation problem\\norientation optimization problem\\ngpu\\norientation optimization\\nparallel computing\\ngenetic algorithm\",\"288\":\"laser radar\\nlaser beams\\ncost function\\nshape\\nthree-dimensional displays\\ncameras\\nant colony optimisation\\nevolutionary computation\\nminimax techniques\\nobject detection\\noptical radar\\nnondetectable subspace\\nmin-max optimization problem\\ncuboid-based approach\\nvsr-based measure\\nobject detection rate\\ntractable cost function computation\\nvsr measure\\ncost-effectiveness configuration\\noptimal design approach\\nautonomous vehicle manufacturers\\naccurate 3d views\\nprecise distance measures\\noptimal lidar configuration problem\\nutility maximization\\nartificial bee colony evolutionary algorithm\\nuncertain driving conditions\\nbio-inspired measure\",\"289\":\"tools\\nprinting\\nsurface treatment\\nmanipulators\\ncollision avoidance\\nrobot kinematics\\nextrusion\\nindustrial robots\\nnozzles\\nrapid prototyping (industrial)\\nsurface finishing\\nthree-dimensional printing\\ntrajectory control\\nplanar layers\\nrobot manipulators\\nrobotic cell\\nmultiresolution additive manufacturing\\nheated nozzle\\nsurface finish\\nfiber orientations\\nlayer-by-layer deposition\\nfused deposition model\\nnonplanar layers\\ndegrees of freedom\\ncollision-free trajectories\",\"290\":\"grippers\\ngrasping\\ntactile sensors\\npose estimation\\nmonitoring\\nsensor arrays\\ncontrol engineering computing\\nmanipulators\\nobject recognition\\nrobot vision\\ncompliant tactile sensor arrays\\nflexible part handling\\nrobot control architecture\\nvision sensors\\nreliable handling\\ntactile grasp validation system\\nin-hand object monitoring\\nindustrial grippers\\nmagnetic gripper\\nflexible vacuum gripper\\ncompliant custom tactile sensor array\\nspecific gripper\\nin-hand object recognition\\ntactile-based object\\ngrasp monitoring\\ntactile sensing\\nmultimodal bin picking system\\noff-the-shelf bin picking system\\nvision-based picking approaches\",\"291\":\"prosthetics\\nknee\\nimpedance\\nkinematics\\nreinforcement learning\\nlegged locomotion\\nbiomechanics\\nfinite state machines\\niterative methods\\nlearning systems\\nmedical robotics\\noptimal control\\noffline policy iteration\\nonline robotic knee prosthesis parameter\\noptimal controller\\npersonalized control\\noptimal control problems\\nhuman-prosthesis system\\nprototypic robotic prosthesis\\napproximate policy iteration algorithm\\nreinforcement learning-based control\\nnear-normal knee kinematics\\noffline learning\\nrobotic lower limb prosthesis\\nrl control\\ncontrol policy\\nimpedance control parameters\\nprosthesis control parameters\",\"292\":\"legged locomotion\\nprosthetics\\ntrajectory\\nimpedance\\nknee\\nthigh\\nkinematics\\nmedical control systems\\noptimisation\\npd control\\npd controller\\nampro ii\\ncubic bezier polynomials\\nproportional-derivative controller\\nimpedance parameters\\nimpedance control scheme\\ntrajectory tracking\\nsloped terrains\\ninclined terrain conditions\\nconsolidated control framework\\nterrain inclinations\\npowered transfemoral prosthesis\\nterminal swing phase\\nslope walking trajectories\\nhuman slope walking data\\noffline optimization problem\\ncompliant stance phase\",\"293\":\"force\\nrobot sensing systems\\nshape\\nload modeling\\noptimization\\nbending\\nelasticity\\ninverse problems\\nmanipulators\\nnonlinear programming\\nrods (structures)\\nelastic structures\\ndistributed loads\\nelastic rod\\nlarge-deflection cosserat-rod model\\nrod length\\ndouble-bend shapes\\nshape approximation\\nmechanics-based models\\nmechanics inverse problem\\nconstrained nonlinear optimization\",\"294\":\"three-dimensional displays\\nsun\\nsurface treatment\\nclustering algorithms\\noctrees\\nestimation\\nimage segmentation\\nimage reconstruction\\nimage sensors\\nobject detection\\nrobot vision\\nslam (robots)\\nsolid modelling\\nunorganized point clouds\\ncrucial pre-processing step\\npoint cloud segmentation\\norganized point clouds\\nplane hypotheses\\nunoriented points\\nefficient plane detection method\\nsemantic mapping\\nslam\\nplane detection methods\\nops\\noriented point sampling\",\"295\":\"training\\nrobots\\nlabeling\\nsemantics\\ntask analysis\\nextraterrestrial measurements\\nconvolutional neural nets\\nimage classification\\nlearning (artificial intelligence)\\nrobot vision\\nrobotic vision\\nrobotic problems\\ntraining set distribution\\nrobotic applications\\nreal-world robotic action\\ndeep metric learning classification system\\nopen set recognition problems\\nopen set active learning approach\\nactive learning problems\\ndeep neural network recognition systems\",\"296\":\"training\\nmeasurement\\ncomputational modeling\\nrobots\\ncomputer architecture\\nobject recognition\\nsupport vector machines\\nimage classification\\nlearning (artificial intelligence)\\nlightweight classifier\\ncomputer vision applications\\nreal-world images datasets\\ninference time\\nunseen objects\\ntrained model\\nsupervised triplet loss\\nseparable embeddings\\nhigh-end computational resources\\ncnns\\nobject recognition on-the-fly\\ndiscriminative embeddings\",\"297\":\"image edge detection\\nproposals\\nvisualization\\nroads\\ncameras\\nthree-dimensional displays\\ntwo dimensional displays\\ncollision avoidance\\nedge detection\\nfeature extraction\\nmobile robots\\nprobability\\nregression analysis\\nrobot vision\\ntiny obstacle discovery\\nmonocular image\\nobstacle-aware discovery method\\nmultilayer regions\\ntiny obstacle proposals\\nmultilayer framework\\nmissing contour recovery\\nvisual cues\\nobstacle-aware occlusion edge maps\\nproposals extraction\\nobstacle occupied probability map\\nobstacle-aware regressor\\nlost and found dataset\",\"298\":\"semantics\\nestimation\\ntask analysis\\nfeature extraction\\noptical imaging\\ntraining\\nthree-dimensional displays\\nimage coding\\nimage matching\\nimage segmentation\\nimage sequences\\nlearning (artificial intelligence)\\nsemantic features\\ndeep disparity features\\nsemantic labels\\nscene segmentation\\ndisparity estimation\\nscene semantics\\noptical flow estimation\\ndepth information\\ndense depth maps\\nimage frames\\ndeep semantic information\\ndisparity feature maps\\nindependent encoding modules\\nsemantic disparity information\\nmultitasking architecture dsnet\\nresnet encoding module\",\"299\":\"three-dimensional displays\\nmobile robots\\ncameras\\nreal-time systems\\nmeasurement by laser beam\\nsensors\\nimage classification\\nimage colour analysis\\nimage sensors\\nnormal distribution\\nobject detection\\noptical scanners\\nrobot vision\\nslam (robots)\\nstereo image processing\\ntransforms\\nvoxel classification\\nrobotic applications\\nmobile robot\\n3d laser scanner\\ngrid data\\nnd voxels\\nnormal distributions transform\\nspatial change detection\\nonboard rgb-d camera\\nstereo camera\\nreal-time range sensors\\nreal-time localization\",\"300\":\"task analysis\\nkinematics\\nend effectors\\njacobian matrices\\nrobot kinematics\\nautonomous aerial vehicles\\nhelicopters\\nmanipulator kinematics\\nmobile robots\\nmotion control\\nposition control\\nredundant manipulators\\ninverse kinematics control\\nanthropomorphic dual arm aerial manipulator\\nmultiple task-priority inverse kinematics algorithm\\ndual-arm aerial manipulator\\nequality constraints\\ninequality constraints\\nsingularity robust method\\nunderactuated aerial hexarotor vehicle\\nmanipulators\\nnull-space based behavioral control\",\"301\":\"three-dimensional displays\\nsensors\\ntarget tracking\\nobject tracking\\nreal-time systems\\nvehicle dynamics\\nheuristic algorithms\\nautonomous aerial vehicles\\ndata structures\\nimage segmentation\\nimage sensors\\nlaser ranging\\nmedian filters\\nmobile robots\\nobject detection\\nrobot vision\\nsolid modelling\\nautonomous behavior\\nmicroaerial vehicles\\nmultiobject tracking\\nlightweight sensors\\nsparse point clouds\\nvelodyne vlp-16 sensor\\nmav hardware\\nunlabeled data\\nsparse 3d laser range data\\nobjects detection\\ndata structure\",\"302\":\"satellites\\nglobal positioning system\\nunmanned aerial vehicles\\ntraining\\nmeters\\nimaging\\nbuildings\\nartificial satellites\\nautonomous aerial vehicles\\ncameras\\nconvolutional neural nets\\ndistance measurement\\nmobile robots\\nobject detection\\nrobot vision\\nflight location\\nuav imagery\\nimage capturing conditions\\nlocalization accuracy\\nadjacent uav frames\\nsatellite map\\ngps-denied flight\\naverage localization error\\ngps-denied uav localization\\nonboard gps system\\nnoisy gps signal\\nunreliable gps signal\\nmonocular rgb camera\\nconvolutional neural network representations\\nsatellite data\\nsatellite imagery\\ndistance 0.85 km\\ndistance 0.2 km\",\"303\":\"image reconstruction\\ntrajectory\\nthree-dimensional displays\\nplanning\\nimage resolution\\nfeature extraction\\ndrones\\nautonomous aerial vehicles\\noptimisation\\ntrajectory control\\naerial 3d reconstruction\\naerial vehicles\\nhigh quality reconstruction\\nadaptive view planning method\\ncoarse proxy\\nreconstruction error\\n3d free space\",\"304\":\"planning\\nthree-dimensional displays\\ntask analysis\\ninspection\\nsimultaneous localization and mapping\\nautonomous aerial vehicles\\nmicrorobots\\nmobile robots\\nspatial measurements\\nmav motions\\ncomplete exploration\\nautonomous loop-closure approach\\nsimultaneous exploration\\nunknown infrastructure\\nattractive means\\ncritical infrastructure\\nautonomous tasks\\nprecise spatial model\\noperational area\\nsensor measurements\\nautonomous inspection capabilities\\nautonomous mav exploration\\nunknown structure\\nspatial information\\nhigh-fidelity 3d model\\nlow-cost microaerial vehicles\",\"305\":\"visualization\\ntask analysis\\ncameras\\nrobot vision systems\\naugmented reality\\nautonomous aerial vehicles\\ncomputer displays\\nmobile robots\\nunsupervised learning\\nassistive camera views\\naugmented reality multitasking environments\\nassistive aerial robot\\ntask domain\\nhead motion\\nanisotropic spherical sensor\\nexpectation maximization solver\\ngaussians\\ndynamic coverage control law\\naugmented reality display\\nhuman operator\\nassistive robot\\nreflex time\\ntask completion time\\naerial co-robot\",\"306\":\"space missions\\nmonitoring\\nvisualization\\nrobot sensing systems\\ncameras\\ncomputational geometry\\ndistributed control\\ngradient methods\\nmobile robots\\nmulti-robot systems\\nposition control\\nvisual coverage control\\nquadcopters\\ncontrol barrier functions\\ncoverage control strategy\\nvisual sensors\\nlocational cost\\ncost function\\ndistributed control law\\ngradient ascent control law\",\"307\":\"robot sensing systems\\ndrones\\ntask analysis\\nhardware\\nlegged locomotion\\noptimization\\nautonomous aerial vehicles\\nmobile robots\\nmulti-robot systems\\noptimisation\\nsoftware aspects\\nrobotic platform\\nrobot design process\\nsmall-volumes low-cost applications\\ncomputational robot co-design problem\\nrobotic modules\\nbinary optimization formulation\\nco-design problems\\nautonomous drone racing platform\\nmultirobot system\\nmonotone case\\nminiaturized robotic hardware\\ninexpensive robots\\ndisposable robots\\nscientific discovery\\nconfined spaces\\nnanodrones\\ntask-specific robots clashes\\nhuman experts\\nsearch-and-rescue\",\"308\":\"trajectory\\nacceleration\\nadaptive arrays\\nplanning\\nunsupervised learning\\noptimization\\nsurveillance\\naircraft control\\nautonomous aerial vehicles\\nremotely operated vehicles\\ntravel cost\\ntravel budget\\nb\\u00e9zier curves\\nmultivehicle ceop\\nmultirotor aerial vehicles\\nmaximal velocity\\nacceleration limits\\nrewarding target locations\\nmultivehicle close enough orienteering problem\\nsurveillance planning\",\"309\":\"stability analysis\\nrobots\\nplanning\\nfriction\\nforce\\ncomputational modeling\\npipelines\\nlogistics\\nmanipulator dynamics\\nmanipulator kinematics\\nmechanical contact\\npath planning\\nstability\\ntime optimal control\\nsuction cup contacts\\nfast robotics pick-and-place\\ncontact stability constraint\\nfactory lines\\nobject transport\\nobject movement\\ncontact handling\\nkinodynamic constraint\\ntime-optimal parameterization\\ngeometric paths\\nphysical robot system\",\"310\":\"robot sensing systems\\nmanipulator dynamics\\nestimation\\nservice robots\\ntorque\\nclosed loop systems\\ndelay estimation\\ndelays\\nmanipulators\\nnonlinear dynamical systems\\nposition control\\nrobust control\\nvariable structure systems\\nclosed-loop stability\\nlink inertia information\\ndynamic coupling terms\\nterminal sliding mode control\\nmodified tde\\ncomplicated nonlinear dynamics terms\\nsea dynamics\\nsea-driven manipulator\\nrobust link position tracking control\\nrobot manipulator\\nseries elastic actuators\\ntime-delay estimation technique\\nconstant gain matrix\\ntde framework\",\"311\":\"impedance\\nbandwidth\\nend effectors\\nposition control\\nactuators\\ncontrol system synthesis\\ndelay estimation\\nelasticity\\nforce control\\nperturbation theory\\nrobust control\\nrobust impedance controller\\nseries elastic actuators\\nsingular perturbation theory\\nsp theory\\ntde technique\\ntime-delay estimation technique\\nnumerical analysis\",\"312\":\"force\\nfriction\\nstrain\\ntorque\\nrobot sensing systems\\nmanipulators\\nblades\\ndexterous manipulators\\nforce sensors\\nmobile robots\\nmotion control\\nposition control\\nmaterial toughness\\nblade-material friction\\nshape deformation\\nrobotic arm\\nseparate control strategy\\ncartesian space\\nforce constraints\\nsmooth motions\\nrobotic cutting\\nknife motion\\nmaterial fracture\\nsmooth knife movements\",\"313\":\"task analysis\\nmanipulators\\nmathematical model\\noptimal control\\nplanning\\nestimation\\nadaptive control\\ncontrol system synthesis\\nfuzzy control\\npath planning\\nredundant manipulators\\nconstrained control-planning strategy\\ninterconnected control-planning strategy\\nhigh-level planning components\\nadaptive control rule\\nmultibody robotic system\",\"314\":\"task analysis\\ngears\\naerospace electronics\\nrobots\\nreinforcement learning\\ntrajectory\\nneural networks\\nassembling\\ncontrol engineering computing\\nforce control\\nindustrial robots\\nlearning (artificial intelligence)\\nneural net architecture\\nposition control\\nrobotic assembly\\nwheels\\nhigh-precision robotic assembly\\nprecise robotic manipulation skills\\nindustrial settings\\nreinforcement learning methods\\nrl\\nperceived forces\\nhigh-precision tasks\\nproper operational space force controller\\nopen-source siemens robot learning challenge\\nprecise force-controlled behavior\\ndelicate force-controlled behavior\\nvariable impedance controller\",\"315\":\"shafts\\nforce\\nmuscles\\ntorque\\npneumatic systems\\nrobot sensing systems\\ndesign engineering\\nhuman-robot interaction\\nmuscle\\npneumatic actuators\\nposition control\\nmechanic design\\npneumatic control system\\nelectric control system\\ndrive unit\\nadjustable stiffness\\nstick-slip phenomenon\\nhigh precision positioning\\npneumatic rotary drive unit\\nhuman-robot collaboration\\narticulated robots\\nprecise rotary drive units\\ncompliant drive units\\nswash plate design\\npneumatic artificial muscles\",\"316\":\"tendons\\ngravity\\ntorque\\nasymptotic stability\\nrobots\\ncontrol systems\\ncontrol system synthesis\\nend effectors\\nforce control\\nmanipulator dynamics\\nnonlinear control systems\\ntorque control\\nantagonistic tendon-driven mechanism\\npassivity-based control law\\npassivity theorem\\ncomplex tendon-driven mechanism\\ncontrol strategy\\nimpedance control schemes\\ngravity compensation\\ninterconnected subsystems\",\"317\":\"manifolds\\nsprings\\nforce\\ndamping\\nsoft robotics\\ndynamics\\nelasticity\\nlinear systems\\nmodal analysis\\nnonlinear dynamical systems\\nrobot dynamics\\nshock absorbers\\nsprings (mechanical)\\nvibration control\\nexact modal characterization\\nlinear mechanical systems\\nnonlinear elastic robot\\nnonlinear normal modes\\nnonlinear oscillatory behaviors\\ndissipative effects\\nnonconservative nonlinear radial mass spring damper system\",\"318\":\"robot sensing systems\\nlegged locomotion\\ndrag\\nshape\\nforce measurement\\nbeams (structures)\\nforce sensors\\nmicrorobots\\nshells (structures)\\nlegged millirobots\\nbody lift\\nrobot locomotion\\nbody-beam forces\\nbody motion\\nlight-weight legged robots\\ndense terrains\\ndrag energy\\ndrag forces increase\\nnegative lift forces\\nvelociroach robotic platform\\ndensely cluttered environment\\ncompliant beams\\nhexapedal millirobot\\ninteraction forces\\nbody contact forces\\nterrain\\ngranular media\\nfoot traction forces\\ncompliant beam environment\",\"319\":\"laser radar\\nthree-dimensional displays\\nestimation\\nrobot sensing systems\\nfeature extraction\\noptimization\\ndistance measurement\\nimage fusion\\nmobile robots\\nmotion estimation\\noptical radar\\npose estimation\\nrobot vision\\nslam (robots)\\nfast motion conditions\\nego-motion estimation\\nmobile robotic applications\\nsensor fusion\\nstand-alone sensors\\ntightly coupled lidar-imu fusion method\\nimu measurements\\nlidarimu odometry\\nlidar measurement\\nrotation-constrained refinement algorithm\\nlio-mapping\\nsensor pair\\nimu update rate\\nlidar pose estimation\",\"320\":\"optimization\\nestimation\\nconvergence\\nglobal navigation satellite system\\nadaptation models\\nsensor fusion\\nrobot sensing systems\\nexpectation-maximisation algorithm\\ngaussian processes\\noptimisation\\nprobability\\nerror distribution\\nmultimodal gaussian mixture model\\nsensor fusion problem\\nexpectation-maximization\\nadaptive mixture algorithm\\nstatic parametrization\\ngraph optimization\\nnongaussian\\nmultimodal distributions\\nrobust cost functions\\nconvergence properties\\nrobust sensor fusion algorithms\",\"321\":\"cameras\\ncalibration\\nnavigation\\ncloning\\ninterpolation\\nestimation\\nvisualization\\nimage sensors\\ninertial navigation\\nkalman filters\\nextrinsic calibration\\nasynchronous cameras\\nstandard multistate constraint kalman filter framework\\nimu poses\\nsingle base camera\\nstate vector\\ncamera images\\ninertial measurements\\ntightly-coupled state estimation\\nonline sensor calibration\\nmc-vins algorithm\\nhigh-precision localization\\nmulticamera visual-inertial navigation\\nonline intrinsic calibration\\npose interpolation\\nhigh-fidelity localization\\nsensor configurations\",\"322\":\"force\\ntrajectory\\ntactile sensors\\nnoise measurement\\nestimation\\nbiomimetics\\nforce control\\nforce sensors\\ngraph theory\\ninference mechanisms\\nmanipulator kinematics\\noptimisation\\nprobability\\nrobot vision\\nsensor fusion\\nstate estimation\\ntrajectory control\\nbiomimetic tactile sensor\\nforce-torque sensor\\nprobabilistic inference\\nvision sensors\\nmultimodal sensor data\\nvisuo-tactile sensing\\nrobust state estimation\\nvision-based articulated model tracker\\nforce trajectories\\nkinematic trajectories\\nrobots\\nfactor graphs\\noptimization\\nmanipulation platforms\",\"323\":\"task analysis\\noptimal control\\nsolids\\nforce\\nrobot sensing systems\\nsolid modeling\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nlearning systems\\nmobile robots\\npattern classification\\nrobot programming\\nterrain properties\\nterrain-informed learning\\nlow shot learning\\ntargeted hopping\\ntask-relevant objectives\\nhopping robot\\ncontrol strategies\\njumping task\\nclosed-loop jumping\\nreal-world jumping data\\nterrain classification\\nonline learning experiments\",\"324\":\"computational modeling\\nheuristic algorithms\\nservomotors\\nnavigation\\ncameras\\ngaussian processes\\ndynamics\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\nregression analysis\\nrobot dynamics\\ntrajectory control\\ngaussian process regression\\nsemiparametrical gaussian processes learning\\nrobot learning\\nball trajectories\\nphysics first principles\\nmotion dynamics\\ndry friction\\nnonlinear effects\\ndegrees of freedom\\ncircular maze environment\\nforward dynamical models\",\"325\":\"semantics\\npredictive models\\nfeature extraction\\nvisualization\\ntask analysis\\npredictive control\\noptimization\\nimage motion analysis\\nimage representation\\nlearning (artificial intelligence)\\noptimisation\\nvisual explanation\\npolicy decisions\\nspc\\nfuture semantic segmentation\\nmultiscale feature maps\\nguidance model\\nmultiple simulation environments\\nmodel-based reinforcement\\ndata efficiency\\nshort time horizons\\nhuman-level performance\\ncomplex environments\\ndriving policy learning framework\\nfeature representations\\nsampling-based optimization\\nsemantic predictive control framework\",\"326\":\"robots\\ntask analysis\\nreinforcement learning\\ntraining\\nadaptation models\\nfriction\\nnavigation\\ngaussian processes\\nlearning (artificial intelligence)\\noptimisation\\nrobot programming\\nadaptive variance\\nsparse-reward environments\\noptimal exploration\\ngaussian-parameterized policy\",\"327\":\"physics\\nengines\\nanalytical models\\ntask analysis\\nrobots\\npredictive models\\nmathematical model\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nobject-based neural network\\ninteracting objects\\ncomplex control tasks\\nobject shapes\\nphysical simulators\\nphysics engine\\nrobot planning\\nreal-world control problems\\ncomplex contact dynamics\\nhybrid dynamics model\\nsimulator-augmented interaction networks\",\"328\":\"stochastic processes\\ndata models\\nvehicle dynamics\\noptimization\\nrobots\\nuncertainty\\ncomputational modeling\\nautomobiles\\ncollision avoidance\\nmobile robots\\nmotion control\\nprobability\\nrobust control\\ntrajectory control\\ndeep stochastic dynamics model\\n1\\/5 scale agile ground vehicle\\nrobust control policies\\nvehicle data\\ncollision probability\\ntrajectory tracking accuracy\\nstochasticity\\nsimple analytic car model\\nhigh quality stochastic dynamics model\\nrobot motion trajectories\\ndata-driven domain randomization\",\"329\":\"robot kinematics\\nrobot sensing systems\\nadaptation models\\nentropy\\ntask analysis\\ndata models\\nimage segmentation\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nsampling methods\\ncoordinating multirobot systems\\nenvironment partitioning\\nadaptive informative sampling\\nrobotic platforms\\ntime sensitive applications\\nsensor measurements\\nhighest expected information\\nmultiple robots\\nsystem partitions\\ninformation rate adaptive sampling approach\\nsimulation environment\\nregion segmentation approach\\nadaptive information gain rate tasking\\nna\\u00efve closest point approach\\nregion segmentation technique\\nreal world robots\",\"330\":\"automobiles\\ntrajectory\\ntraffic control\\ndc motors\\nservomotors\\nrobot kinematics\\nhuman factors\\nmobile robots\\nmotion control\\nsteering systems\\ntrajectory control\\nvehicle dynamics\\nunique experimental testbed\\ntrajectory planning\\nminiature robotic car\\ncambridge minicar\\nautonomous control strategies\\nphysical multilane setup\\nminiature highway\\nlarge-fleet experimental research\\ndriver models\\nmultilane road topographies\\nminiature ackermann-steering vehicles\",\"331\":\"robot sensing systems\\nbase stations\\npath planning\\ncollision avoidance\\nentropy\\nbipartite graph\\ngraph theory\\ninteger programming\\nmobile robots\\nmulti-robot systems\\ncollision-free informative locations\\ninformative paths\\nbase station performs\\nmultirobot system\\ninformation collection\\ncontinuous connectivity constraints\\nmultirobot informative path planning\\ntime 0.75 s\",\"332\":\"robot sensing systems\\noptimization\\ndensity functional theory\\nwires\\nenvironmental monitoring\\ntask analysis\\napproximation theory\\nconvex programming\\ngradient methods\\nmobile robots\\nsensors\\ntwo-dimensional domain\\nunconstrained coverage problem\\ngradient descent\\ndirect projection\\nunconstrained problem\\nsensor coverage control\\nrobots constrained\\nconstrained coverage control problem\\nlocational cost minimization\\nspatial allocation\\nconvex approximation\",\"333\":\"batteries\\nunmanned aerial vehicles\\nagriculture\\nrobot sensing systems\\nstrips\\nrouting\\napproximation theory\\nautonomous aerial vehicles\\ncomputational complexity\\nmobile robots\\npath planning\\ntravelling salesman problems\\nugv\\narea coverage problem\\nenergy-constrained unmanned aerial vehicle\\nunmanned ground vehicle\\nsymbiotic uav\\nnp-hard problem\\ngeneralized traveling salesperson problem\\nboustrophedon cells\\nlimited battery capacity\",\"334\":\"three-dimensional displays\\nimage coding\\nlaser radar\\ntwo dimensional displays\\nrobot sensing systems\\ndecoding\\nrecurrent neural networks\\ncomputational geometry\\ndata compression\\niterative methods\\nmobile robots\\noctrees\\noptical radar\\nrecurrent neural nets\\nslam (robots)\\nrecurrent neural network\\nresidual blocks\\ngeneric octree point cloud compression method\\npotential application scenarios\\ndecompressed point cloud data\\n3d lidar sensor\\nautonomous driving systems\\n3d lidar data\\nraw d formatted lidar data\\n2d formatted lidar data\",\"335\":\"three-dimensional displays\\ngeometry\\nfeature extraction\\nimage edge detection\\nreliability\\nlaser radar\\nconvolution\\ncomputer vision\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nobject detection\\nbilateral weight\\ndeep geometry\\ncontext guidance\\ngeometry network\\ncontext network\\nsingle encoder-decoder network\\ninitial propagated depth map\\nslanted surfaces\\nconvolutional neural network\\ncnn-based depth completions\\nlocal feature extraction\\nglobal feature extraction\",\"336\":\"laser radar\\ntraining\\ncolor\\nrobot sensing systems\\nthree-dimensional displays\\nconvolution\\nextraterrestrial measurements\\ncameras\\nimage colour analysis\\nimage fusion\\nimage sequences\\noptical radar\\nregression analysis\\nrobot vision\\nsemidense annotations\\nkitti depth completion benchmark\\nself-supervised sparse-to-dense\\nself-supervised depth completion\\ndense depth image\\nsparse depth measurements\\nirregularly spaced pattern\\nmultiple sensor modalities\\ndense level ground truth depth\\npixel-level ground truth depth\\nself-supervised training framework\\nsparse depth images\\ncolor images\",\"337\":\"three-dimensional displays\\npipelines\\nimage color analysis\\ncameras\\nsolid modeling\\nimage segmentation\\nproposals\\nimage colour analysis\\nimage reconstruction\\nobject detection\\nobject tracking\\nvideo signal processing\\nvideo-segmentation-based object tracking algorithm\\nhand object scanning\\nrgb-d video segmentation\\nin-hand manipulation\\nvideo camera\\nmultiple grasps\\nhousehold objects\\nin-hand object tracking\\nvideo tracking algorithms\\nrgb-d in-hand object manipulation dataset\\n3d object scanning\",\"338\":\"robot sensing systems\\nrobot kinematics\\ntraining\\nfeature extraction\\nreinforcement learning\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nneural nets\\nstatistical analysis\\nmultimodal deep generative models\\ncoordinated heterogeneous multiagent active sensing\\njoint latent representation\\nepistemic active sensing behavior\\nmultimodal variational auto encoder\\nsensor modalities\\nmultiagent deep reinforcement learning setup\\ndirect reward signal\\nevidence lower bound\",\"339\":\"splines (mathematics)\\nrobot kinematics\\ncollision avoidance\\nplanning\\noptimization\\nmobile ad hoc networks\\nautonomous aerial vehicles\\ndecentralised control\\nhelicopters\\ninteger programming\\nlinear programming\\nmobile robots\\nmulti-robot systems\\nnonlinear programming\\nrobot dynamics\\nrssi\\ntree searching\\noutdoor formation coordination\\nmultiple quadcopters\\ntime-optimal speed profile\\nminimum snap spline path\\nquadcopter kinematics\\nwireless communication connectivity\\ngeometric formations\\nmaximum separation distance\\nminimum viable received signal strength\\npath loss attenuation\\noutdoor flight test\\nformation type\\nrobin communication scheduling scheme\\ndecentralized formation coordination\\ncommunication constraints\\nrh-minlp\\nquadcopter dynamics\\nouter-approximation branch and bound solver\\nwarm-starting scheme\\nhardware-in-the-loop\\nhitl\\naverage radio packet loss statistics\\nround robin communication scheduling scheme\\nreceding horizon mixed-integer nonlinear program\",\"340\":\"robot kinematics\\ncollision avoidance\\ntrajectory\\nmaintenance engineering\\nsystem recovery\\ndelays\\ngaussian processes\\nmulti-robot systems\\npath planning\\nrmtrack control law\\ngaussian process\\ncoordination space obstacle\\ndisturbance probabilities\\nmultirobot coordination\\nonline plan repair\",\"341\":\"robot sensing systems\\nentropy\\nrobot kinematics\\ncollision avoidance\\npath planning\\ncontrol engineering computing\\nmobile robots\\nmulti-robot systems\\nregression analysis\\nsensor fusion\\ndecentralized approach\\nobstacle mapping\\ncontinuous probabilistic representation\\nmapping problem\\nbinary classification task\\nkernel logistic regression\\ndiscriminative classifier online\\nindividual robot maps\\npath planning algorithm\\nmaximum information value\\nobstacle avoidance\\nvlsr system\\nprior obstacle information\\nrobot paths\\nhilbert map fusion method\\nlarge-scale robotic systems\\nonboard range sensors\",\"342\":\"petri nets\\ntools\\ndata models\\nsystematics\\nautomation\\nrobot control\\nfinite state machines\\nmulti-agent systems\\nmulti-robot systems\\nmultiagent robot system layer\\nagent layer\\nsubsystem layer\\ncommunication layer\\nrobotic system\\nrobot controller code\\nmultiagent robot control systems\\nembodied agent\\ncooperating subsystems\\nhierarchical finite state machines\\nhierarchical petri nets\",\"343\":\"training\\nrobots\\ngames\\nmarkov processes\\nreinforcement learning\\nnavigation\\nautonomous vehicles\\ngradient methods\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\nrobot programming\\ninteraction-aware multiagent reinforcement learning\\nmobile agents\\nindividual goals\\noptimal policy\\ndecentralized learning\\nmobile robot navigation\\npolicy gradient algorithms\\nnonstationary policies\\ncurriculum-based strategy\\ninteractive policy learning\",\"344\":\"robot sensing systems\\ndensity functional theory\\ndistributed algorithms\\nlinear programming\\nsilicon\\nmobile robots\\nmulti-robot systems\\noptimisation\\ncontinuous environments\\ndiscrete environments\\nlocally optimal positions\\ndensity function\\ncoverage control\\nmultiple event types\\nheterogeneous robots\\nautonomous robots\\ntotal sensing quality\\nhomogeneous problem\",\"345\":\"uncertainty\\ngames\\nautonomous agents\\nreinforcement learning\\nnash equilibrium\\nplanning\\nadaptation models\\nbelief networks\\nentropy\\nlearning (artificial intelligence)\\nmulti-agent systems\\nneural nets\\nplanning (artificial intelligence)\\nstochastic processes\\npartial observability\\nadversary agent\\nautonomous agent\\nbelief space planning\\ngenerative adversary modeling\\nmaximum entropy reinforcement learning\\nstochastic belief space policy\\nunmodeled adversarial strategies\\nmaximum entropy deep reinforcement learning\\nactive perception problem\\npotentially adversarial behaviors\\nuncertainty modeling\\nstandard chance-constraint partially observable markov decision\",\"346\":\"robot kinematics\\ntwo dimensional displays\\napproximation algorithms\\nshape\\nunmanned aerial vehicles\\nbinary trees\\nautonomous aerial vehicles\\nmobile robots\\nmulti-robot systems\\nvelocity control\\narbitrary shape\\nplume shape\\nplume speed\\nrobot speed\\ntour\\naerial robots\\ntranslating plume\\nonline multirobot exploration\\ncompetitive algorithm\",\"347\":\"oceans\\nsea measurements\\ncurrent measurement\\nglobal positioning system\\ntrajectory\\nestimation\\nexpectation-maximisation algorithm\\ngaussian processes\\nmobile robots\\nposition control\\nregression analysis\\nunderwater vehicles\\nonline estimation\\ngaussian process-based expectation-maximisation algorithm\\ndead-reckoned position estimates\\nspecialised gp regression scheme\\nbest-fitting ocean\\nocean current field\\nunderwater robots\",\"348\":\"atmospheric measurements\\nsea measurements\\nparticle measurements\\nsonar navigation\\nsensors\\nsonar\\nadaptive signal processing\\naltimeters\\nautonomous underwater vehicles\\nkalman filters\\nmarine navigation\\nparticle filtering (numerical methods)\\nunderwater vehicles\\nvelocity measurement\\nterrain-aided navigation\\nadaptive sensing method\\npinging rate\\nlocalization accuracy\\ntan\\nsonar pinging interval\\nlocal seafloor topography\\nmodified teager kaiser energy operator\\npinging interval\\ndownward-looking sonar\\nautonomous underwater vehicle\\nparticle filter\\nbias velocity estimator\\nkalman filter\\ndepth variation\",\"349\":\"acoustics\\nsynthetic aperture sonar\\narray signal processing\\nsimultaneous localization and mapping\\napertures\\nreceivers\\nsonar navigation\\nacoustic signal processing\\ngraph theory\\nmarine navigation\\npose estimation\\nprobability\\nsensor fusion\\nslam (robots)\\nslam framework\\nbeacon position\\nacoustic measurements\\nfactor graph formulation\\nnongaussian slam\\nspatial resolution\\nnavigational measurements\\nunderwater missions\\nsas\\naccurate pose estimation\\nhydrophones acoustic data\\nempirical probability distribution\\nconventional beamformer\\nautonomous surface vehicle\",\"350\":\"acoustics\\nglobal navigation satellite system\\nacoustic measurements\\nreceivers\\nclocks\\nrobot sensing systems\\nautonomous underwater vehicles\\ninertial navigation\\nlakes\\nmobile robots\\npath planning\\nunderwater acoustic communication\\nclock synchronization\\nunderwater robotics\\ngnss\\nlake geneva\\nfeature tracking\\nadaptive sampling\\nunderwater environmental sensing\\nelectromagnetic waves\\nradio communication\\nsatellite based positioning\\naerial robotics\\nmultivehicle environmental sampling applications\\nacoustic navigation system\\nabsolute time information\\nauv position\\nunderwater acoustic positioning system\",\"351\":\"feature extraction\\nsonar measurements\\nsimultaneous localization and mapping\\nthree-dimensional displays\\ngaussian processes\\nimaging\\ngraph theory\\nimage reconstruction\\nmobile robots\\nremotely operated vehicles\\nslam (robots)\\nsonar imaging\\nterrain mapping\\nunderwater vehicles\\nterrain reconstruction\\nforward-looking sonar imagery\\nunderwater simultaneous localization\\nmultibeam imaging sonar\\n3d terrain mapping tasks\\nelevation angle information\\ndata association\\naccurate 3d mapping\\neuclidean space\\noptical flow\\nbearing-range images\\nsubsea terrain\\ngaussian process random field\\nterrain factors\\nfactor graph\\nterrain elevation estimate\\nvariable-elevation tank environment\\nsmooth height estimate\\nsonar images\\nchow-liu tree\\nextracted feature tracking\",\"352\":\"conferences\\nautomation\\nconvolutional neural nets\\nedge detection\\nimage colour analysis\\nmanipulators\\nobject recognition\\npose estimation\\nrobot vision\\ntexture-less industrial parts\\nrobotic bin-picking problem\\nindustrial applications\\nsingle rgb image\\ndiscriminative local appearance descriptors\\noptimization stage\\ncoarse initializations\\nsurface texture\\nedge image\\nedge optimization\\naccurate 6d object pose estimation\\n2d bounding box\\ntiny convolutional neural network\\nhypothesis-evaluation scheme\\nrobotic manipulation platform\",\"353\":\"cameras\\nlinear programming\\nthree-dimensional displays\\noptimization\\nminimization methods\\ncomputer vision\\nminimisation\\npose estimation\\nalternating minimization method\\ncamera models\\npose problem\\nobjective function\\nsimple minimization problem\\ndistinct pose problems\\nalternating minimization methods\\npose problems\",\"354\":\"feature extraction\\npose estimation\\ntraining\\nrobots\\nheating systems\\nsolid modeling\\ntask analysis\\nimage annotation\\nimage classification\\nimage segmentation\\nobject detection\\nrobot vision\\nunsupervised learning\\n6d pose estimation\\ncomputer vision models\\nobject localization\\nmultiple domain classifiers\\nsynthetic images\\nobject poses\\ntime-consuming annotations\\noccluded scenes\\ncluttered scenes\\nweak object detector\\ndeep learning approaches\\ncluttered environments\\nrobust robotic grasping\",\"355\":\"end effectors\\nkinematics\\nrobot motion\\nplanning\\ntrajectory\\ncollision avoidance\\ndynamic programming\\ngraph theory\\nmanipulator kinematics\\nmotion control\\nnonlinear programming\\nsearch problems\\ntrajectory control\\nadaptive sampling\\nsearch-space\\ninput end-effector trace\\nnonlinear trajectory-optimization approach\\ndiscrete-optimization method\\njoint-angles\\nrobot motion translation problem\\ndiscrete-space graph-search problem\\nnonlinear optimization\\ndynamic-programming algorithm\\nconfiguration space\\ndiversity sampling\\nrobot arm trajectories\\nstampede\\nend-effector path function\\npathwise-inverse kinematics\\nper-frame inverse kinematics\\ncollision-free robot motions\\n6-dof cartesian-space end-effector paths\",\"356\":\"exoskeletons\\nkinematics\\nthumb\\nrobot kinematics\\nmagnetic resonance imaging\\nmanipulators\\naugmented reality\\nbiomechanics\\nbiomedical mri\\nimage reconstruction\\nmedical image processing\\nmedical robotics\\ntelemedicine\\ntelerobotics\\nvirtual reality\\nteleoperation\\nforce-feedback\\nmotion capture\\nmri data\\nfixed-base hand exoskeleton\\njoint angles\\ndexterous haptic input device\\nfixed-base exoskeleton\\nhuman hand pose\",\"357\":\"conferences\\nautomation\\ncad\\ncameras\\nconvolutional neural nets\\nend effectors\\nindustrial manipulators\\nlearning (artificial intelligence)\\npose estimation\\nproduction engineering computing\\nrobot vision\\nrobotic assembly\\n3d cad models\\ndepth camera\\ndeep convolutional neural networks\\nindustrial robotic assembly tasks\\ndepth images\\npose estimation learning\\ntwo-stage pose estimation procedure\\nend-effector\",\"358\":\"observability\\nquaternions\\njacobian matrices\\nnoise measurement\\npollution measurement\\nthree-dimensional displays\\nq measurement\\ninertial navigation\\nmonte carlo methods\\nglobal yaw rotation\\nline features\\nclosest point parameterization\\nobservability properties\\nunified representations\\nclosest point form\\nunified parameterizations\\ncp representations\\nekf-based vision-aided ins\\ngeometrical features\\nunified feature representations\\naided inertial navigation systems\\nhomogeneous geometric features\\ngeneral aided ins\\nlinearized aided ins\\nminimal representation\\nobservability analysis\\n4d euclidean vector\\nanalytically-computed jacobians\\nmonte-carlo simulations\",\"359\":\"navigation\\nmicrosoft windows\\nstandards\\ncurrent measurement\\nreal-time systems\\ntrajectory\\nthree-dimensional displays\\ncomputational complexity\\ncorrelation methods\\ninertial navigation\\nkalman filters\\nmobile robots\\nnonlinear filters\\nrobot vision\\nslam (robots)\\nschmidt-msckf\\nreal-time visual-inertial navigation\\nbounded-error performance\\nrobotic applications\\nvisual-inertial localization\\nloop closure constraints\\nlong-term persistent navigation\\nschmidt-kalman formulation\\nmultistate constraint kalman filter framework\\nstate vector\\nlinear-complexity ekf\\ncross-correlations\\nnavigation states\\nperformance improvement\",\"360\":\"calibration\\ncameras\\nrobot sensing systems\\nnavigation\\ncovariance matrices\\nfuses\\nvisualization\\ninertial navigation\\nsensor fusion\\nsensor-failure-resilient multiimu visual-inertial navigation\\nreal-time multiimu visual-inertial navigation system\\nmultiple inertial measurement units\\nmi-vins formulation\\nauxiliary sensors\\nbase imu failure\\nsensor failure\\nconventional vins\\nmultiple imus\",\"361\":\"training\\ngeometry\\ncameras\\ntask analysis\\nestimation\\nfeature extraction\\noptical fiber networks\\ndistance measurement\\nimage sequences\\nlearning (artificial intelligence)\\npose estimation\\nrecurrent neural nets\\nregression analysis\\nwindowed composition layer\\ncl-vo\\nlearning monocular visual odometry\\noptical flow network\\ngeometry-aware objective function\\ncomplex geometry problems\\ngeometry-aware curriculum learning\",\"362\":\"visualization\\noptimization\\nland vehicles\\nperturbation methods\\nrobots\\njacobian matrices\\nsensors\\nfeature extraction\\ngraph theory\\nmobile robots\\nmotion control\\nobject detection\\npath planning\\nroad vehicles\\nrobot vision\\nground vehicle\\nodometric sensors\\nmonocular visual sensors\\nstochastic constraint\\nodometric measurement processing\\nvisual-odometric localization\\nmapping system\\nout-of se(2) motion perturbation\\nse(2)-xyz constraint\\nimage feature measurement\\npreintegration algorithm\\ngraph optimization structure\",\"363\":\"cameras\\nrobot vision systems\\nradiometry\\nestimation\\nunmanned aerial vehicles\\noptimization\\ndistance measurement\\nglobal positioning system\\ninertial navigation\\nmeasurement errors\\nmobile robots\\noptimisation\\npath planning\\nsensor fusion\\ntemperature measurement\\ntemperature sensors\\nkeyframe-based direct thermal-inertial odometry\\nthermal camera\\ninertial measurements\\nairborne obscurants\\nfog\\nsmoke\\noptimization based approach\\ninertial measurement errors\\ngps\\ndirect radiometric data fusion\\naerial robotic capabilities\\nvisually degraded environments\\nreprojection error minimisation\\n3d landmark error\\nindoor laboratory setting\\nunderground mine\",\"364\":\"mathematical model\\nspace vehicles\\nmanipulator dynamics\\nparameter estimation\\ndamping\\nestimation\\naerospace robotics\\nflexible manipulators\\nmobile robots\\nmotion control\\npath planning\\nmanipulators\\njoint flexibilities\\ntracking capabilities\\nsystem inertial parameters\\ndamping parameters\\nspace flexible-joint manipulator system\\nsystem full dynamics\\nspace manipulator systems\\nflexible joints\\nstiffness parameters\",\"365\":\"robots\\ntorque\\nthree-dimensional displays\\ncomplexity theory\\ngenerators\\naerospace electronics\\nspace exploration\\nactuators\\nlegged locomotion\\nmotion control\\noptimisation\\nrobot dynamics\\nco-evolved structures\\ncontrol-only optimized equivalent\\nmds\\ninertially actuated units\\ncompliant elements\\ncontrol method\\nbio-inspired concept\\ncompliance distribution\\nlocomotion performance\\npopulation based optimization techniques\\nmomentum driven compliant structures\\ncentral pattern generator control\\nhardware complexity\\ncpg\\nrough environment exploration\",\"366\":\"robot sensing systems\\nestimation\\ncollision avoidance\\nimpedance\\nrobot kinematics\\ncomputational modeling\\naerospace robotics\\nforce control\\nmobile robots\\nparticle filtering (numerical methods)\\nposition control\\nsensors\\ncontact-event-triggered mode estimation\\ndynamic rigid body impedance-controlled capture\\ncontact-event-triggered filter\\nimpedance control\\nparticle filter\\nsliding contact mode cases\\nforce-torque sensor\\nair bearing robotic system\\ntime 8.3 ms\\ntime 4.2 ms\",\"367\":\"grasping\\nrobot sensing systems\\nuncertainty\\nvisualization\\ngrippers\\nlearning (artificial intelligence)\\nmanipulators\\ngrasp acquisition\\nmodel-free deep reinforcement learning\\ncontrol policies\\ncontact sensing\\nrobust grasping\\nmultifingered hand\\ncomplex finger coordination\\nlearned policies\\ngrasping policies\\ncontact feedback\\nopen problem\\nrobotics research\\nnoisy observations\\nsensor feedback\\nvisual feedback\\ncontact forces\",\"368\":\"robot sensing systems\\nskin\\naerospace electronics\\ntwo dimensional displays\\nthree-dimensional displays\\ndexterous manipulators\\nfeedback\\nhaptic interfaces\\nlearning (artificial intelligence)\\nmobile robots\\ntactile sensors\\ntactile servoing\\ntactile sensing information\\ntactile skin geometry\\ntactile finger\\ndexterous robotic manipulation\\ntactile feedback capability\\nlatent space dynamics learning\\ncontact point tracking\\nmanifold learning\",\"369\":\"three-dimensional displays\\nrobot sensing systems\\nmeasurement\\ngrippers\\ngrasping\\nsolid modeling\\ngeometry\\ncomputational geometry\\nfeature extraction\\nimage colour analysis\\nlearning (artificial intelligence)\\npointnetgpd\\nend-to-end grasp evaluation\\nobject grasping\\n3d point cloud\\ngrasp configuration detection\\npoint sets\\nrobot grasp configurations\\ncomplex geometric structure\\ngripper\\n3d geometry information\\ndeep neural network\\nrgb-d camera\",\"370\":\"task analysis\\nvisualization\\ntraining\\ntactile sensors\\ndexterous manipulators\\nlearning (artificial intelligence)\\ntouch sensing information\\nexpert demonstration trajectories\\nhigh dimensional visual observations\\nmanipulation tasks\\nimitation learning\\non-board sensors\\nexternal tracking\\non-board sensing capabilities\\nin-hand manipulation\\nmultifingered dexterous hands\\ndexterous hand manipulation\\ndeep visuomotor policies\",\"371\":\"visualization\\ncameras\\ngrippers\\ntactile sensors\\ndata collection\\nimage recognition\\nobject recognition\\nvisual modality\\nglobal observation\\nrobotic manipulation\\nvisual object identification\\ntouch-based instance recognition\\nmultimodal recognition\\nvisual observation\\ntactile observation\\nmultimodal instance recognition problem\\ngelsight touch sensors\\nautonomous data collection procedure\\ntactile observations\\ntactile recognition\\nrobotic perception\",\"372\":\"task analysis\\nvalves\\nacceleration\\nreinforcement learning\\nrobot sensing systems\\nhardware\\ncontrol engineering computing\\ndexterous manipulators\\nlearning (artificial intelligence)\\nneural nets\\nrobot programming\\ndeep reinforcement learning\\nmultifingered hands\\ncontact-rich manipulation behavior\\nmodel-free deep rl algorithms\\ncomplex multifingered manipulation skills\\ndirect deep rl training\\nmodel-based control\\ndexterous manipulation\\ndexterous multifingered robotic hands\\ngeneral-purpose robotic manipulators\\nautonomous control\\ncomplex intermittent contact interactions\",\"373\":\"actuators\\nheating systems\\nink\\nsilver\\nrobot sensing systems\\nangular measurement\\nink jet printing\\nmobile robots\\nnanoparticles\\nnonlinear control systems\\nplastics\\ntemperature measurement\\ntemperature sensors\\nthin film sensors\\nall-printed paper caterpillar robots\\ninkjet printable actuator\\nrigid sensor-actuators\\nsoft-bodied crawling robot design\\nnanoparticle ink printing\\nflexible plastic film\\nbending sensors\\nthermal based actuators\\nhome-commodity inkjet printers\",\"374\":\"grippers\\nforce\\nmeasurement\\nrobots\\nenergy consumption\\ntask analysis\\nsprings\\nactuators\\ndexterous manipulators\\nenergy conservation\\nmotion control\\npath planning\\nenergy-saving drive\\nrobotic grippers\\nenergy-savings\\nenergy-neutral grippers\\nrobotic energy-efficient drive\\ngrip performance metric\\nnonbackdrivable mechanism\\nstatically balanced force amplifier\\nsbfa\\nnbdm\",\"375\":\"three-dimensional displays\\nstrain\\ndeformable models\\nstress\\npipelines\\nsolid modeling\\nprinters\\ndeformation\\nelasticity\\nfinite element analysis\\nsolid modelling\\nstress analysis\\nthree-dimensional printing\\ngenerative deformation\\nprocedural perforation\\nelastic structures\\nprocedural generation\\ncontrolling designing 3d printed deformable object behaviors\\ngenerative algorithms\\ncohesive process\\nvariable elasticity\\nautomated method\\nsimulated deformations\\ncohesive pipeline model\\nvolumetric structures\\nconsumer-level 3d printers\\nfinite element analysis metrics\\nelastic 3d prints\\ndesign objectives\\nelastic material behaviors\\nheterogeneous geometric structure\\n3d print deformations\\nautomated pipeline\\ndesign environment\\nperforated deformation models\\nheterogeneous lattice structures\\nautomated generation\\n3d print procedure\\nelastic material capabilities\",\"376\":\"service robots\\nrobot sensing systems\\nrobot learning\\nmobile robots\\ncollision avoidance\\nhardware\\ncontrol engineering computing\\neducational robots\\nlaboratories\\nlearning (artificial intelligence)\\nresearch and development\\ntelerobotics\\nremotely accessible robotics development platform\\nkuka robot learning lab\\nindustrial lightweight robots\",\"377\":\"three-dimensional displays\\nrobot sensing systems\\nmeasurement by laser beam\\ncontrol systems\\nvegetation\\nlaser radar\\ncomputer vision\\nforestry\\nheight measurement\\nimage sampling\\nsolid modelling\\nstereo image processing\\nseedling measurement process\\nscanning laser profilometer\\napplication specific point-cloud\\npoint-cloud generation methods\\n3d structured light sensing\\nlight intensity detection\\nmeasurement accuracy\\nmeasurement system\\ntree nurseries\\npoint cloud processing\\nautomated seedling height assessment system\\noffline identification\\nreport generation\\nseedling development process\\nproduction optimization purposes\\ndata samples\\nindustrial scale operations\\nmeasurement sample size\\nmeasurement resolution\\ncentre for agriculture and forestry development\\ncanada\\nnewfoundland and labrador\\nwooddale\",\"378\":\"adsorption\\nfabrication\\nforce\\nrobots\\nwires\\nbusiness\\nsurface treatment\\ncapillarity\\nmobile robots\\nseals (stoppers)\\nporous part\\ncapillary part\\ncapillary force\\nuneven surface\\nirregular surface object\\nsuper wet adsorption pad\\nwall climbing robot\\nswa pad\\nsalt reaching method\\nwater sealing\",\"379\":\"foot\\nlegged locomotion\\ndamping\\nrubber\\nsoft robotics\\nanimals\\nrobot dynamics\\nstability\\nfoot stiffness\\nrobot performance\\ndamping properties\\nsoft robotic feet\\nenergetic economy\\nbipedal robotic walking\\nhollow rubber\\ndamping values\\ndrop test rig\\nplanar bipedal robot ram\\nmechanical energy\\nwalking speeds\\nfoot properties\\nspherical feet\\nwalking instability\",\"380\":\"legged locomotion\\nfoot\\nfriction\\ndynamics\\nrough surfaces\\nsurface roughness\\nnonlinear control systems\\noptimisation\\nrobot dynamics\\nstability\\nstick-slip\\ntrajectory control\\nlubricated surface\\nrough no-slip surface\\nfoot slippage\\nslippery surfaces\\nstable bipedal gaits\\nplanned ground slippage\\ndynamic bipedal robot locomotion\\ntrajectory generation\\nnonlinear control\\nstabilization\\nlow-friction surfaces\\noutdoor terrains\\ntrajectory optimization\\nstick-slip transitions\\npoint foot contact\\ncoulomb's friction law\\nslippery walking gait\\namber-3m planar biped robot\\ndynamic walking\\nrobot stance foot\",\"381\":\"legged locomotion\\nhumanoid robots\\ntrajectory\\nangular velocity\\noptimization\\ntask analysis\\nangular velocity control\\nmotion control\\noptimisation\\ntorque control\\ntorque controller\\nvelocity controller\\nvelocity controllers\\nicub robot\\njumping\\nicub humanoid robot\\npredefined com trajectory\\ncentroidal angular momentum\",\"382\":\"switches\\nlimit-cycles\\nsafety\\ntask analysis\\nlegged locomotion\\ndynamics\\ngait analysis\\nhumanoid robots\\nmotion control\\npath planning\\nrobot dynamics\\nsafe adaptive switching\\ndynamical movement primitives\\nrobot motion plans\\n3d bipedal robot model\\ndynamic movement primitives\\nprimitive movements\\nlimit-cycle walking gait\\ncontrol-theoretic tools\",\"383\":\"task analysis\\nthree-dimensional displays\\neducation\\nobject detection\\nobject recognition\\nservice robots\\ndexterous manipulators\\nend effectors\\nhumanoid robots\\nlearning (artificial intelligence)\\nposition control\\nrobot vision\\nhuman-centric environments\\nend-effector positions\\naffordance category\\ngrasp configuration\\nbayesian approach\\nlearning recognition\\nobject categories\\ninstance-based approach\\nrobotic manipulation\\nobject perception\\ngrasp affordances\\ntraining data\\nbatch learning\\nend-effector orientations\",\"384\":\"robot sensing systems\\nimpedance\\nmanipulators\\nrobot kinematics\\ncollision avoidance\\ndexterous manipulators\\nend effectors\\nhuman-robot interaction\\nserial robotic arm\\nmacro-mini robot architecture\\ngeneral multidegree-of-freedom serial robot\\nfive-degree-of-freedom robotic arm\\nlow-impedance sensing approach\\nimpedance control scheme\",\"385\":\"force\\nrobot kinematics\\nmuscles\\ndynamics\\ntask analysis\\nservice robots\\ncollision avoidance\\nhuman-robot interaction\\nindustrial robots\\nmotion control\\nneurocontrollers\\nrobot dynamics\\nhuman-robot collaboration\\nsafety framework\\nforce myography data\\nhuman worker\\nhuman muscles\\nfmg signal\\nrobot motion\\nneural network\\nindustrial settings\",\"386\":\"robot sensing systems\\nforce\\nsensor arrays\\nhall effect\\nmultiplexing\\nelectric sensing devices\\nend effectors\\nforce sensors\\nhuman-robot interaction\\nmobile robots\\nmotion control\\nmultimodal sensor array\\nsafe human-robot interaction\\ntime-of-flight sensors\\naccidental contact detection\\ncontact localization\\nforce sensing\\nproximity sensing\\nproximity mapping\\ncollaborative continuum robot\\nbracing constraint\\nadmissible rolling motion\\nend effector\\ni2c communication network\\nrobot perception\\ncollaborative robots\\ncontinuum robots\\nbracing\\nmapping\",\"387\":\"task analysis\\nmathematical model\\noscillators\\ntrajectory\\nrobots\\npredictive models\\ncomputational modeling\\nbiomechanics\\nhuman-robot interaction\\nlearning systems\\nmanipulator dynamics\\nmotion control\\noptimisation\\npendulums\\nposition control\\nsloshing\\nhuman manipulation\\nnonrigid objects\\nliquid sloshing\\nhorizontal line\\nvirtual environment\\nhuman subjects\\nrobotic manipulandum\\nresidual oscillations\\nhumans simplified control\\nhuman movements\\ncontinuous optimization-based control\\ncontrol model\\nflexible objects\\nmotion profile\\nhuman profiles\\nrobot control\\ninput shaping model\\ncart-and-pendulum system\",\"388\":\"dynamics\\ncomputational modeling\\nmanipulators\\nbayes methods\\nestimation\\nprobabilistic logic\\nmanipulator dynamics\\nmulti-robot systems\\nrobot kinematics\\nrobot vision\\nstate estimation\\ntorque control\\ncomplex manipulation scenario\\nbayesian state estimator\\nnonprehensile manipulation\\nindustrial assembly\\nin-hand localization\\ncontact dynamics\\ntorque-based robot controller\\nmultiple robots\\narticulated objects\\nphysical robot\\nfreedom object\\nmultimodal distributions\",\"389\":\"robot sensing systems\\nreceivers\\nforce\\nforce measurement\\noptical sensors\\ndistance measurement\\nelastomers\\nforce sensors\\nmanipulators\\nobject detection\\noptimisation\\nforce sensing\\nelastomer-air interface geometry\\nrobot manipulation\\ncontact detection\\nsignal-to-noise ratio\\nelastomer-air boundary\\ndeformation measurement\\ncontact force measurement\\nproximity sensor design\\ncontact sensor design\\noptimization\\nsingle fingertip-mounted sensing system\\noptical time-of-flight range measurement modules\\nemitted light path control\",\"390\":\"haptic interfaces\\nfeature extraction\\ntask analysis\\ndictionaries\\nmatching pursuit algorithms\\nrobot sensing systems\\nimage classification\\niterative methods\\nunsupervised learning\\nunsupervised feature learning\\ndensely innervated skin\\nhaptics researchers\\nhaptic intelligence\\nconcrete tasks\\nobject recognition\\nfeature learning methods\\nhaptic adjectives\\ndiverse interactions\\nabstract binary classification tasks\\nspatio-temporal hierarchical matching pursuit\\nhaptic adjective recognition\",\"391\":\"shape\\nimage reconstruction\\ntactile sensors\\nthree-dimensional displays\\nestimation\\nhaptic interfaces\\nmanipulators\\nhigh-resolution tactile imprints\\nshape reconstruction\\nobject localization\\nvision-based tactile sensor\\nlocal shapes\\nreconstructed objects\\ntactile sensing\\ntactile feedback\\nonline object identification\",\"392\":\"force\\ntactile sensors\\ngrasping\\ncameras\\ndexterous manipulators\\nforce control\\ngrippers\\nslip\\ngrasped object\\nsensor surface\\n2d rigid-body motion\\nmotion field\\n2d planar rigid transformation\\ndense slip field\\nhighly deformable objects\\nslip feedback\\nmonitoring incipient slip\\nhigh-resolution vision-based tactile sensor\\ntactile imprints\\ndetection accuracy\\nfrequency 24.0 hz\",\"393\":\"laser radar\\nroads\\ncameras\\nthree-dimensional displays\\ntwo dimensional displays\\nimage segmentation\\ntransforms\\nconvolutional neural nets\\nimage colour analysis\\nimage fusion\\nimage sampling\\nobject detection\\noptical radar\\nkitti-road dataset\\ncrf based lidar-camera fusion\\nroad detection method\\ncolor information\\ncamera image domain\\ncrf fusion method\\nconditional random field framework\\nbinary road detection\\ndense road detection\\nlidar-camera calibration\\nheight-difference based scanning strategy\\nconvolutional network\",\"394\":\"robots\\nsemantics\\ndata models\\ngeometry\\nnavigation\\nindoor environment\\ntagging\\ngraph theory\\nmobile robots\\npath planning\\nopenstreetmap\\ngeometrical information\\nbasic indoor structures\\narchitectural principles\\napplication-specific knowledge\\ngraph-based map representation\\nhierarchical structure\\nsemantic mapping extension\\nindoor robot navigation\\ngrid-based motion planning algorithms\",\"395\":\"trajectory\\nadaptation models\\nprobabilistic logic\\nbayes methods\\nvehicle dynamics\\nvehicles\\npredictive models\\ngradient methods\\nlearning (artificial intelligence)\\nprobability\\nrecurrent neural nets\\nstochastic processes\\ntraffic engineering computing\\nbayesian recurrent neural network\\nprediction horizon\\ntarget human driver\\nnaturalistic car following data\\nmultimodal stochastic feedback gain\\nparticle-filter-based parameter adaptation algorithm\\nadopted gradient-based training method\\nembedded physical model\\ntrajectory distribution\\nbayesian-neural-network-based policy model\\nbayesian recurrent neural network model\\ndriving policy\\npredicted distribution\\nphysical feasibility\\nlong-term trajectory prediction\\nautonomous driving\\nrobust safety\\nadaptive probabilistic vehicle trajectory prediction\",\"396\":\"schedules\\ndelays\\nurban areas\\npublic transportation\\ncost function\\nautomation\\noptimisation\\nroad vehicles\\ntraffic engineering computing\\ntransportation\\noptimizing vehicle distributions\\nfleet sizes\\nurban transit\\nride-sharing\\nvehicle congestion\\nmultiple passengers\\nhistorical demand data\\nmod systems\\ntravel demand\\ntaxi demand\\nshared mobility-on-demand systems\\ntaxi requests\\ncity's transportation infrastructure\\nfour passenger vehicles\",\"397\":\"roads\\nsurface reconstruction\\ncameras\\nimage reconstruction\\nuncertainty\\nglobal positioning system\\nkalman filters\\nadaptive kalman filters\\ncomputer vision\\nimage filtering\\nnonlinear filters\\npose estimation\\nroad safety\\ntraffic engineering computing\\naekf\\nlocal road surface reconstruction techniques\\non-road test\\nglobal vision-based reconstruction\\nthree-dimensional road surfaces\\nvision-based technique\\nadaptive extended kalman filter\\nglobal camera pose estimation\\nreal-world global 3d road surface reconstruction\",\"398\":\"robots\\nheuristic algorithms\\nprediction algorithms\\nsurgery\\nneural networks\\ntask analysis\\naerospace electronics\\nbiological tissues\\nlearning (artificial intelligence)\\nmanipulators\\nmedical robotics\\nmobile robots\\npredictive control\\nrobot vision\\nautonomous tissue manipulation\\nsoft tissue\\nai learning\\nvision strategies\\nraven iv surgical robotic system\\npredictive control algorithms\\nreinforcement learning\\nrobotic tissue manipulation\\nlearning from demonstration\\nsimulation\\nautomation\\nmachine learning\\nartificial intelligence\\nai\\nraven surgical robot\",\"399\":\"endoscopes\\nultrasonic imaging\\noptical imaging\\noptical sensors\\ncameras\\nrobot sensing systems\\nbiomedical optical imaging\\nbiomedical ultrasonics\\nmedical robotics\\nphantoms\\nultrasonic transducers\\nspiral scan\\nplacenta phantom\\nwhite light stereo camera\\nrobotic multimodal endoscope\\nlow diameter endoscopes\\ndynamic environment\\ntechnically challenging surgery\\nfetoscopy\\nall-optical ultrasound\\nmultimodal rigid endoscope\\nrobotic control\\nsurface visualisations\\noptical ultrasound sensor\",\"400\":\"force\\nsurgery\\ntools\\nretina\\nrobot kinematics\\nrobot sensing systems\\ncalibration\\neye\\nforce sensors\\nmanipulators\\nmedical robotics\\nphantoms\\nmanipulation task\\nearly warning system\\nunsafe manipulation events\\nsafe robot-assisted retinal surgery\\nunsafe scleral force\\nretinal microsurgery\\nhigh surgical skill\\nmanipulation error\\nconstant contact\\nunexpected manipulation\\nextreme tool-sclera contact force\\nrobotic assistance\\nsurgeon\\npotential intra-operative danger\\nrobotic systems\\nimminent unsafe manipulation\\nsclera damage\\nforce-sensing tool\\nunsafe events\\nsteady hand eye robot\\nforce safety status\",\"401\":\"robot sensing systems\\nbronchoscopy\\nlung\\nmanipulators\\ncancer\\nbiomechanics\\ncomputerised tomography\\nendoscopes\\nmedical image processing\\nmedical robotics\\nphysiological models\\nsafety algorithms\\nrobotic bronchoscopy drive mode\\nauris monarch platform\\nlung cancer\\nten degree-of-freedom bronchoscope\\nthree degree-of-freedom user input\\npaired driving concept\\ntension monitoring\\nlung porcine models\",\"402\":\"tools\\nforce\\nrobot sensing systems\\nsurgery\\ninstruments\\nlaparoscopes\\nforce feedback\\nforce measurement\\nmedical computing\\nmedical robotics\\nlaparoscopic surgery\\nundistort stiffness perception\\ncomanipulation paradigm\\nfulcrum\\nlever effect\\nlaparoscopy\\nstiffness perception\\nactive force feedback\\npreliminary assessment experiment\\nlever ratio\\ntool tip\\nsurgeon\\nrobotic device\",\"403\":\"jamming\\nmanipulators\\nstrain\\nligaments\\nforce\\nservice robots\\nbending\\ndesign engineering\\nelasticity\\nfinite element analysis\\nflexible structures\\nhumanoid robots\\nmanipulator kinematics\\nmotion control\\nstructural engineering\\nmultimaterial spine-inspired flexible structure\\nstiffness-controllable layer-jamming-based robotic links\\nspine mechanism\\nrobotic link\\nlayer jamming\\nhollow structure\\nlight structure\\nflexible spine\\nlink utilising\\nlimb segments\\ngranular jamming\\nbending angles\\nstiffness-tuneable limb segment\\nmalleable robots\\nrobotic arms\\nstiffness-adjustable\\nbending segments\\nrevolute joints\\nmechanical architecture\\ndegrees of freedom\\nsuitable links\\nrobotic manipulators\\nreduced performance\\nstructural deformation\\ninner support structure\\nincreased stiffening performance\\nstiffening mechanisms\",\"404\":\"jamming\\nmanipulators\\ntendons\\nstrips\\nfriction\\nsoft robotics\\nactuators\\ncompliance control\\nelastic constants\\nhoneycomb structures\\nmanipulator dynamics\\nmechanical stability\\nmedical robotics\\nsurgery\\nsliding layer mechanism\\ninterlocking jamming layers\\nreconfigurable variable stiffness manipulator\\nsoft robots\\nvariable stiffness mechanism\\nstructural stability\\nmanipulator\\nlaparoscopic surgeries\",\"405\":\"muscles\\npneumatic actuators\\nsoft robotics\\nfabrication\\nstrain\\nbending\\ncontrol system synthesis\\nelastic constants\\nelectroactive polymer actuators\\npolymers\\nposition control\\npressure control\\nvariable structure systems\\nsupercoiled polymer artificial muscles\\nvariable stiffness soft actuator\\nsoft pneumatic actuation\\nmuscle-like supercoiled polymer actuation\\nsoft pneumatic actuator\\nscp actuation\\nsoft robot locomotion\\nsoft robot manipulation\\nstiffness tuning\\nscp artificial muscle tension\",\"406\":\"mathematical model\\npredictive models\\nsoft robotics\\nactuators\\niterative methods\\npredictive control\\ncomputational modeling\\nbending\\nelasticity\\niterative learning control\\nrobots\\nsoft bending actuators\\nsoft robots\\npseudorigid-body model\\nbending behavior\\nlearning curve\\nlearning process\\nsoft-elastic composite actuator\\niterative learning model predictive control method\\nsoft material robotics\\nmotion control\\nmodel learning for control\",\"407\":\"force\\ntorque\\nreliability\\naerospace control\\nelectromagnetics\\nactuators\\naircraft\\naerospace components\\nclosed loop systems\\nclutches\\ncontrol system synthesis\\nforce control\\nforce feedback\\nhaptic interfaces\\ninteractive devices\\nmagnetic actuators\\nmagnetorheology\\nman-machine systems\\njam-free design\\nactuation strategy\\ntendon driven 2dof mr powered manipulator\\nelectromechanical actuators\\nhuman controlled machines\\ntendon-driven 2-degree-of-freedom spherical gimbal\\nhyper-redundant mr clutches\\nclosed-loop force control\\nelectromagnetic actuators\\naerospace flight control\\nman-machine interaction\\nhaptic joysticks\\nactive feedback\\nhyper-redundant magnetorheological actuators\\n2dof sidestick\\nreliability requirements\\nsystem design\\nforce density\",\"408\":\"force\\nmanipulators\\nbandwidth\\nfriction\\nhydraulic systems\\ntask analysis\\nelectromagnetic actuators\\nforce control\\nhuman-robot interaction\\nmagnetorheology\\nmedical robotics\\nmotion control\\npatient rehabilitation\\nrobot dynamics\\nsupernumerary robotic limbs\\nwearable robots\\nhuman arms\\nlightweight srl\\nmr-hydrostatic actuation system\\nmagnetorheological-hydrostatic actuators\\nhuman environment\\nforce-control approaches\\nelbow joints\\nwearable robotic arm\\nforce-controllable srl\\nlow-friction hydrostatic transmission\\ninteraction forces\\nmechanical backdrivability\\nsize 25.0 nm\\nfrequency 25.0 hz\\nwearable robotic\\nlightweight\\nforce-control\\nmagnetorheological\\nhydrostatic transmission\\nhigh-bandwidth\\nbackdrivability\",\"409\":\"conferences\\nautomation\\nbeams (structures)\\nforce measurement\\nforce sensors\\nlight emitting diodes\\nmedical robotics\\noptical sensors\\nprototypes\\nshafts\\nsurgery\\ninvasive robotic surgery\\nbandwidth measurement\\ninfrared led-bicell pair\\n3d printed prototype\\nstructural dynamics\\nsensor limitations\\nflexible beam model\\ndifferential photocurrent\\ninstrument shaft\\noptical slit\\ndavinci endowrist instruments\\noptical force sensing\\nsurgical robotics: laparoscopy\\nforce and tactile sensing\\nhaptics and haptic interfaces\",\"410\":\"conferences\\nautomation\\nbending\\nbiomechanics\\nelectromyography\\nergonomics\\nfinite element analysis\\nmedical signal processing\\northotics\\nsolid modelling\\nhust-ec\\nvastus lateralis\\nvastus medialis\\nbiceps femoris\\nrectus femoris\\nmuscle activation\\nchair height\\nbending angles\\nchair angles\\nmatlab-based acquisition software\\nemg sensors\\nelectromyography test platform\\nfinite element analysis program\\nsolid models\\nprototype chair\\nwearable chair design\\nhuman-chair model\\nwearable exoskeleton chair\\nexoskeleton\\nwearable chair\\nemg\\nmechanism design\",\"411\":\"cameras\\nsimultaneous localization and mapping\\nrobot vision systems\\nkinematics\\nmanipulators\\ndistance measurement\\nimage colour analysis\\nimage fusion\\nimage texture\\nmanipulator kinematics\\nmobile robots\\nmotion estimation\\nrobot vision\\nslam (robots)\\nstereo image processing\\nko-fusion\\ndense visual slam methods\\nobserver\\nvisual information\\nslam systems\\ninertial measurements\\ndense rgb-d slam system\\nwheeled robot\\nkinematic data\\nodometric data\\nkinematic measurements\\ntightly-coupled kinematic\\nodometric tracking\\nmanipulator\\nodometry measurements\",\"412\":\"diffraction\\nray tracing\\nthree-dimensional displays\\ncomputational modeling\\nacoustic diffraction\\nrobots\\nacoustic signal processing\\nacoustic wave diffraction\\nacoustic wave propagation\\nparticle filtering (numerical methods)\\ngenerated acoustic rays\\nestimated source position\\nstatic nlos sound sources\\ndynamic nlos sound sources\\nactual source locations\\nstate-of-the-art localization method\\nnonline-of-sight source\\nsound localization algorithm\\nnonline-of-sight sound source\\nindoor environments\\ndiffraction properties\\nsound waves\\nbending effects\\nvirtual sound source\\nindoor scene\\ndiffraction acoustic rays\\nray tracing-based sound propagation\\ndiffraction-aware sound localization\\nutd\\nuniform theory of diffraction\\nwedge precomputing\\nreconstructed mesh\\nparticle filter\\nsize 7.0 m\\nsize 3.0 m\",\"413\":\"image reconstruction\\nuncertainty\\ncameras\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nreal-time systems\\nrobot vision systems\\ncomputerised instrumentation\\ngraphics processing units\\nimage fusion\\nlearning (artificial intelligence)\\nminimisation\\nneurocontrollers\\nphotometry\\nprobability\\nslam (robots)\\nstereo image processing\\ntarget tracking\\ndepth cameras\\ncnn\\ndeepfusion\\nsemidense multiview stereo algorithm\\ngradient predictions\\nmonocular slam\\nsingle-view depth\\nkeypoint-based maps\\ncamera tracking\\ndense 3d reconstructions\\nconvolutional neural network\\nsparse monocular simultaneous localisation and mapping systems\\nreal-time dense 3d reconstruction system\\nphotometric error minimization\\ngpu\",\"414\":\"vehicle dynamics\\npredictive models\\nuncertainty\\ndynamics\\nindexes\\nreal-time systems\\nclustering algorithms\\nhilbert spaces\\nmobile robots\\nremotely operated vehicles\\nreal-time occupancy predictions\\nstatic occupancy models\\ncontinuous occupancy map\\nhigh-dimensional feature space\\ndata-efficient model\\ncrowded unstructured outdoor environments\\ndynamic hilbert maps\\ntemporal dependencies\\n3d laser data\\n2d laser data\",\"415\":\"skeleton\\nmutual information\\ntask analysis\\ncameras\\nrobot vision systems\\nimage colour analysis\\nimage sensors\\nobject detection\\npath planning\\nrobot vision\\nslam (robots)\\nbinary coverage\\ngoal selection strategy\\nimage morphology\\nsearch space\\ncsqmi\\nperspective aware planning\\ninformation based exploration strategy\\nhigh resolution 3d maps\\nrgbd panoramas\\nangle enhanced occupancy grid\\nexploration strategy\\nmaximal cauchy-schwarz quadratic mutual information\\nlogging image control\",\"416\":\"robots\\nnavigation\\nvibrations\\nmixture models\\nenergy consumption\\ngaussian processes\\nplanning\\nmobile robots\\npath planning\\nrobot navigation\\noutdoor environments\\nplace-dependent model\\naerial image\\ngaussian process mixture model\",\"417\":\"bayes methods\\nrobots\\ncovariance matrices\\nmerging\\ntime complexity\\nreal-time systems\\ntraining\\nimage fusion\\nmobile robots\\nmulti-robot systems\\nslam (robots)\\nmultirobot hilbert map systems\\nindividual fast-bhms\\ndecentralised manner\\ncontinuous representation\\nrobot autonomy\\ntraditional occupancy grid maps\\ncontinuous nature\\ncontinuous occupancy map fusion\\nfused fast-bhms\\nglobal fast-bhm models\\nbayesian hilbert map models\\nfast bayesian hilbert maps\",\"418\":\"propellers\\nactuators\\naircraft\\naerospace electronics\\nforce\\nfault tolerance\\nfault tolerant systems\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nfault tolerant control\\nremotely operated vehicles\\nminimalistic actuation\\npossible actuator failures\\nlight-weight adaptations\\nnominal flight controller\\ntailsitter vtol aircraft\\nfault-tolerant flight control\\nlanding systems\\nmoving parts\\nvertical take-off and landing systems\",\"419\":\"aircraft\\nmathematical model\\natmospheric modeling\\naerodynamics\\naerospace control\\nrotors\\nangular velocity\\nactuators\\naerospace components\\naircraft control\\nattitude control\\nautonomous aerial vehicles\\ncascade control\\ncontrol system synthesis\\nhelicopters\\nhinges\\nmobile robots\\npropellers\\nrotors (mechanical)\\nthree-term control\\nvelocity control\\nquadrotor frame\\nfixed-wing aircraft\\nhover\\nforward flight\\ntilting actuators\\ncoupled dynamics\\naircraft frame\\ncascaded control architecture\\ncontrol design\\nforward velocity control\\npassively-coupled tilt-rotor vertical takeoff and landing aircraft\\ndifferential thrust\\ninner-loop attitude\\nheight control\\nconstrained lagrangian approach\\np-pid controllers\\nunactuated hinged mechanism\\nequations of motion\\nunmanned aerial vehicles\\nuavs\\nvertical takeoff and landing aircraft\\npassively-coupled tilt-rotor\\ndynamic modeling\\ncascade pid control\\npx4 autopilot\",\"420\":\"power demand\\npropellers\\nservomotors\\nunmanned aerial vehicles\\nbrushless dc motors\\naerospace propulsion\\nattitude control\\nautonomous aerial vehicles\\nmobile robots\\nmotion control\\npitch control (position)\\npower consumption\\npower control\\nrobot dynamics\\npropeller design\\npropeller-based propulsion mechanisms\\nvariable-pitch propeller\\nversatile uavs\\nquasisteady propulsive state\\npower-minimizing control\\nelectrical power consumption\\nversatile unmanned aerial vehicles\\nvariable-pitch propulsion system\",\"421\":\"actuators\\nrobots\\nanalytical models\\ntorque\\nfabrication\\naerodynamics\\nprototypes\\naerospace robotics\\nfeedforward\\ngears\\nmicrorobots\\nmobile robots\\nmotion control\\nopen loop systems\\npiezoelectric actuators\\nstability\\nrapid inertial reorientation\\naerial insect-sized robot\\nbio-inspired inertial tail\\ndc electric motor\\ngeared motor system\\npiezo-tail system\\nresonant system\\npiezo-driven inertial reorientation\\nopen-loop feedforward controller\\nrapid dynamic maneuvers\\npiezoactuator\\nmass 142.0 mg\",\"422\":\"navigation\\ntask analysis\\nunmanned aerial vehicles\\ninspection\\npath planning\\nmobile robots\\nautonomous aerial vehicles\\nremotely operated vehicles\\nrobot dynamics\\nrobot kinematics\\naerial robots\\nin-contact operation\\ncontact missions\\nflying robot\\nnavigation mode\\ncartwheel mode\\nnavigation modalities\\nin-contact navigation\\nspecialized contact mechanism\\ncontact-based navigation path planning\",\"423\":\"mathematical model\\nservomechanisms\\nattitude control\\ntorque\\ntransportation\\nradio frequency\\nautonomous aerial vehicles\\nfreight handling\\nhelicopters\\nmotion control\\ncargo transportation strategy\\nstable flight performance\\nconstant flight performance\\nfuselage part\\nrelative attitude control strategy\\nt3-multirotor uav platform\\nthrust generating part\\nservomechanism\\nmoment of inertia\",\"424\":\"kinematics\\nrobots\\noscillators\\ntrajectory\\nservomotors\\nheuristic algorithms\\ncomputational modeling\\naerodynamics\\naerospace components\\nautonomous aerial vehicles\\ncontrol engineering computing\\ngradient methods\\nlearning (artificial intelligence)\\nmotion control\\nmulti-agent systems\\nrobot dynamics\\nrobot kinematics\\nrobot programming\\nexperimental learning\\nlift-maximizing central pattern generator\\nflapping robotic wing\\npolicy gradient algorithm\\ndynamically scaled robotic wing\\nconstant reynolds number\\ncentral pattern generator model\\ncpg\\nmotion controller\\nrhythmic wing motion patterns\\nhalf-stroke symmetry constraint\\nlearning agent\\nrobotic learning\\nwing kinematic learning\",\"425\":\"grippers\\nforce\\nspirals\\npropulsion\\nelectron tubes\\nrobot sensing systems\\nmobile robots\\nself-adjusting systems\\nstability\\nscalable suction\\naerial robot\\nlateral physical work\\nground-based robots\\nfunctional work\\nlateral force\\nhovering vehicle\\nenvironmental forces\\nself-sealing suction cup\\nflight vehicle\\nphysical grasping demonstrations\\nsuction-based gripper\",\"426\":\"cameras\\nagriculture\\ndelays\\ncontrol systems\\nrobots\\nthree-dimensional displays\\ntracking\\nagricultural robots\\nagrochemicals\\ncrops\\nfeature extraction\\nmobile robots\\nrobot vision\\nspraying\\nsustainable development\\ncomputer vision\\nautonomous robotic weeding systems\\nprecision farming\\ncurrent dependency\\nherbicides\\npesticides\\nselective spraying\\nmechanical weed removal modules\\nenvironmental pollution\\nreal-time treatment\\nweeding control system\\nindeterminate classification delays\\nin-row weeding system design\\nsustainability\\nnonoverlapping multicamera system\\nterrain conditions\",\"427\":\"stability analysis\\nlegged locomotion\\nlaser stability\\nrobot sensing systems\\nsenior citizens\\nadaptive control\\ncontrol engineering computing\\ngait analysis\\ngeriatrics\\nhandicapped aids\\nimage colour analysis\\nintelligent robots\\nkalman filters\\nlaser ranging\\nlearning (artificial intelligence)\\nmedical robotics\\nmobile robots\\nnonlinear filters\\npose estimation\\nrobust control\\nstate estimation\\nlegs positions\\nukf\\ndeep learning\\nlaser range finder data\\naugmented gait state estimation\\nhuman gait stability predictor\\nuser-adaptive control architecture\\nunscented kalman filter\\nbody center of mass\\nlong short term memory networks\\nrobust predictions\\nencoder-decoder sequence\\nlrf data\\nnonwearable sensors\\nmultimodal rgb-d\\nelderly users\\nintelligent robotic rollator\\nlstm-based network\",\"428\":\"robots\\nurban areas\\nroads\\nwaste management\\nrfid tags\\nswarm robotics\\nbatteries\\ngeographic information systems\\nmobile robots\\nmulti-robot systems\\nnavigation\\npath planning\\nrefuse disposal\\nservice robots\\ngarbage collection scenarios\\nurban swarms\\necosystems\\nbio-inspired foraging methods\\nmultiplace foraging\\nreal-world gis data\\nrobot swarms\\nurban waste management system\\nstigmergy-based navigation\\nurban environment\\nswarm robotics system\\nautonomous waste management\",\"429\":\"heart\\nadaptation models\\nintegrated circuit modeling\\nmathematical model\\nbiochemistry\\npredictive models\\ntrajectory\\nadaptive control\\nblood vessels\\ncardiovascular system\\nhaemodynamics\\nphysiological models\\nautomated aortic pressure regulation\\nphysiological aerobic metabolism\\nex vivo heart perfusion\\naortic pressure regulation\\nisolated porcine heart\\nanimal hearts\\ncontrol parameters\\nadaptation algorithm\\nvirtual controller forms\\nnonlinear equivalent circuit fluid flow model\\nperfusion system\\naop regulation\\nadaptive controller\",\"430\":\"trajectory\\nmeasurement\\ndecoding\\ngenerators\\nbidirectional control\\ngenerative adversarial networks\\nstability analysis\\nmobile robots\\npath planning\\nposition control\\nremotely operated vehicles\\nroad traffic\\nroad vehicles\\nvehicle-to-vehicle encountering scenarios\\nautonomous vehicle development\\nmultivehicle trajectory generator\\nmtg\\nmultivehicle interaction scenarios\\ndriving encounter scenarios\\nmultibranch decoder\\nvehicle-to-vehicle encounters\\nautonomous vehicles\",\"431\":\"task analysis\\nrobot sensing systems\\nfeature extraction\\ntraining\\nskin\\nlearning systems\\nconvolutional neural nets\\nhaptic interfaces\\nlearning (artificial intelligence)\\npattern classification\\n1-shot learning\\nknowledge transfer\\ndeep n-shot transfer learning\\ntactile material classification\\nflexible pressure-sensitive skin\\nactive sensing tasks\\ndeep end-to-end transfer learning\\ndeep convolutional neural network\\nsuperhuman tactile classification performance\",\"432\":\"support vector machines\\nrobot sensing systems\\nstandards\\nmachine learning\\nrecurrent neural networks\\nconvolutional neural nets\\nfeature extraction\\nhumanoid robots\\nimage classification\\nimage texture\\nlearning (artificial intelligence)\\nneurocontrollers\\nrecurrent neural nets\\nrobot vision\\ntactile sensors\\nhybrid touch approach\\nhuman sense\\ninteracted objects\\nstandard sensory modalities\\nsliding movements\\ntactile-based texture classification\\nmachine-learning methods\\nsurface textures\\nhand-engineered features\\nrecurrent neural network layers\\nfeature representations\\ntactile data\\ntouch data\\ntactile identification\\nsense of touch\\ntouch movements\\nconvolutional neural network layers\\nicub platform\",\"433\":\"robot sensing systems\\nvisualization\\ntask analysis\\nimage color analysis\\nadaptation models\\ndata visualisation\\nhaptic interfaces\\nimage texture\\nmobile robots\\nneural nets\\nrobot vision\\ntouch (physiological)\\nvisual perception\\nrobotic cross-modal sensory data generation\\nvisual-tactile perception\\nvisual-tactile stimulus\\nunimodal visual perception\\nrobotic tasks\\ntexture perception\\nconditional generative adversarial networks\\npseudovisual images\\ntactile outputs\\nperception performance\\nsensory outputs\\nvitac dataset\",\"434\":\"principal component analysis\\ntraining\\ntactile sensors\\ntask analysis\\nfeature extraction\\nmanipulators\\nshear deformation\\ncontinuous contact data\\noutput path-dependent readings\\ncontact readings\\ncontinuous-contact tasks\\nsensor signal\\nshear-invariant perception method\\nsliding motion\\ncompliant tactile sensor\\ncontinuous tactile contact\\ncontour-following task\\nsoft tactile sensor\\nmanipulation tasks\\ntactile perception systems\\nshear-invariant sliding contact perception\",\"435\":\"robot sensing systems\\nforce\\nsurface impedance\\nmathematical model\\nstrain\\nforce measurement\\nforce sensors\\niterative methods\\nsurface topography measurement\\ntactile sensors\\ntorque measurement\\ncontact point information\\ngeometric surface description\\ncontact centroid\\nforce-deformation characteristics\\nforce-indentation behavior\\nintrinsic soft tactile sensing\\nits\\niterative procedure\\nsoft surface deformation characteristics\\nellipsoid silicone specimens\\nros-based toolbox\",\"436\":\"force sensors\\nstructural beams\\nforce\\nrobot sensing systems\\nstress\\nstrain\\ncreep\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npath planning\\nrobot vision\\nmedical robotics\\noptimisation\\nobject detection\\nposition control\\ncollision avoidance\",\"437\":\"robots\\nvideos\\ndecoding\\nnatural languages\\ntraining\\nrecurrent neural networks\\nprincipal component analysis\\ngesture recognition\\nhumanoid robots\\nhuman-robot interaction\\nknowledge based systems\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nspeech recognition\\ntext analysis\\nted talks\\nnao robot\\nspeech text understanding\\nend-to-end neural network model\\nlearning-based co-speech gesture generation\\nhuman labor\\nrule-based speech-gesture association\\nend-to-end learning\\nrobots learn social skills\",\"438\":\"educational robots\\nmedical services\\nservice robots\\nhead\\ntask analysis\\nsoftware\\nhuman computer interaction\\nhuman-robot interaction\\ninteractive systems\\nmedical administrative data processing\\nvisual perception\\npersonal friend\\ndoctor\\nmedical receptionist\\nrobotic system\\nclinic visit\\nwizard-of-oz study\\nfriendly receptionist\\npeople perceptions\",\"439\":\"robots\\ngames\\ntask analysis\\ngrasping\\nnatural languages\\nface\\nvisualization\\ncontrol engineering computing\\nconvolutional neural nets\\ngame theory\\nhumanoid robots\\nhuman-robot interaction\\npsychology\\nautonomous ai system\\ndice game scenario\\ncompetitive personality\\nhumanoid robot\\nuser interaction\\nturn-taking dice game\\nhri scenario\\nhuman-robot interaction scenario\\nconvolutional neural network\\nparticipants facial feedback\\nsocially engaged personality\\ngodspeed questionnaire\\nmind perception questionnaire\\npersonality-driven robot\",\"440\":\"planning\\ngames\\nvehicles\\nadaptation models\\ninference algorithms\\nestimation\\nloss measurement\\ngame theory\\nintelligent robots\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\npredictive control\\nremotely operated vehicles\\nav\\nhuman driver\\nsocial awareness\\npassive-aggressive motions\\ninteraction modeling\\nsocially-graceful driving\\nautonomous vehicles\\ntwo-player game\\nmodel predictive control\\nsocial gracefulness\\nintent inference\\nmotion planning\",\"441\":\"image segmentation\\ntask analysis\\nrobots\\ncomputational modeling\\nvisualization\\ndatabases\\nreal-time systems\\nfeature extraction\\nimage fusion\\nimage retrieval\\nobject detection\\nquery processing\\nrobot vision\\nobject-level subimages\\npixel-wise loc maps\\ndetection-by-localization scheme\\nranking function\\nranked list\\nranking based self-localization model\\nunsupervised rank fusion\\ncross-season change detection\\nmaintenance-free change object detector\\nlikelihood-of-change\\nobject-level change detection\\ngeneralized task\\nquery image\\nsubimagelevel pixel-wise loc maps\\npublicly available north campus long-term dataset\\npublicly available nclt dataset\\nmultimodal information retrieval\\nmmr\",\"442\":\"proposals\\nrobots\\nadaptation models\\ntraining data\\ndata models\\nobject recognition\\ntesting\\nbig data\\nhuman-robot interaction\\nimage segmentation\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\nhuman robot interaction\\nrobotic applications\\ndeep learning models\\nlabeled training data\\npre-defined big data\\nsegmentation method\\ntarget object\\ndata generation method\\nsegmentation model\\nlightweight segmentation net\\none shot learning\",\"443\":\"three-dimensional displays\\nlaser radar\\nsemantics\\nvehicle detection\\nfeature extraction\\nimage segmentation\\ntwo dimensional displays\\nimage colour analysis\\nobject detection\\noptical radar\\nseg-voxelnet\\nlidar data\\nrgb images\\nlidar point clouds\\nautonomous driving scenarios\\nsemantic segmentation technique\\n3d lidar point cloud based detection\\nimage semantic segmentation network\\nseg-net\\nimproved-voxelnet\\nsemantic segmentation map\\npoint cloud data\\nimage semantic feature\\nkitti 3d vehicle detection benchmark\",\"444\":\"three-dimensional displays\\nfeature extraction\\nrobot sensing systems\\ncomputer architecture\\ntraining\\ndecoding\\nconvolutional codes\\ndriver information systems\\nimage classification\\nimage fusion\\nunsupervised learning\\nunsupervised learned multimodal features\\nautonomous driving applications\\nroad users\\nroad side infrastructure\\nautonomous cars\\nclassification modules\\nunseen sensor noise\\nobject classification module\\ntotal sensor failure\\nunsupervised feature training\\nuni-modal classifiers training\\nmultimodal classifiers training\\nfeature space\\nsensor modalities\\ndecision module\\nunsupervised learned multi-modal features\",\"445\":\"three-dimensional displays\\nlaser radar\\ntraining\\nadaptation models\\ndata models\\npipelines\\nsensors\\nimage segmentation\\nobject detection\\noptical radar\\nrendering (computer graphics)\\nunsupervised learning\\nlidar point cloud\\ndeep-learning-based approaches\\npoint cloud segmentation\\nsqueezesetv2\\ndata collection\\ndomain-adaptation training pipeline\\ndomain adaptation pipeline\\nunsupervised domain adaptation\\nroad-object segmentation\\ndomain-adaptation methods\",\"446\":\"calibration\\nrobot kinematics\\ncameras\\nrobot vision systems\\ntools\\ncontrol engineering computing\\nconvolutional neural nets\\nindustrial manipulators\\nmanipulator kinematics\\npose estimation\\nproduction engineering computing\\nrobot system\\nworld robotic scenario\\nworld dataset acquisition\\ndata-driven industrial robot arm pose estimation\\nsmart sensory systems\\nindustrial robots\\nmobile platforms\\nconvolutional neural network architecture\\nindustrial robot arm system\\nautomatically annotated datasets\\nextracted pose information\\nropose-system\",\"447\":\"task analysis\\nrobots\\ntraining data\\nengines\\nvirtual environments\\ngames\\ndata models\\ncomputer games\\nlearning (artificial intelligence)\\nmobile robots\\nobject recognition\\nrobot vision\\nvirtual reality\\nself-training perceptual agents\\nsimulated photorealistic environments\\nhigh-performance perception\\nmobile robotic agents\\ngaming industry\\ngame engines\\nperceptual agent\\nvirtual environment\\ntask-specific object distribution\\ndescription language\\nlearning environments\\nsensory input\\nrobotic system\\nself-training perception\\nrobotic simulation\\nunreal engine\\nscenario description\",\"448\":\"eigenvalues and eigenfunctions\\ngrasping\\nconvolution\\nthree-dimensional displays\\ncollision avoidance\\nservice robots\\ndexterous manipulators\\ngrippers\\nindustrial robots\\nobject detection\\nposition control\\nsingular value decomposition\\nhand templates\\ntarget object\\noptimum grasping posture\\neigenvalue templates\\nparallel hands\\nthree-finger hands\\nobject grasping position detection\\nfast graspability evaluation\\neigenfunctions\\narbitrary parameters\",\"449\":\"robot sensing systems\\nmanipulators\\nmetals\\ndetectors\\ntrajectory\\nshape\\nmobile robots\\noptimisation\\npath planning\\nsensors\\nphysical sensor\\nvirtual footprint\\noptimization problem\\nenergy performances\\nvirtual sensor footprint\\ncoverage path planning problem\\nrobotic arm\\nlarger areas\\nplatform moves\\nlarger footprint\\nmetal detector\",\"450\":\"cameras\\nestimation\\nconferences\\nautomation\\nieee members\\nkinematics\\npredictive models\\nactuators\\nfeedback\\ngravity\\nimage filtering\\nkalman filters\\nmanipulator kinematics\\nmotion control\\nnonlinear filters\\nrobot vision\\nshape control\\nshape measurement\\nmodel-based estimation\\nslim 3-actuator continuum robot\\nseparately modeled segments\\nactuated segments\\ngravity-loaded shape estimation\\nscene depth estimation\\nmonocular visual feedback\\nrobot movements\\nmanipulation capabilities\\nconfined spaces\\nrobot control\\nconstant curvature\\nvariable curvature\\nspatial locations\\nrobot shape\\nmonocular camera\\nunscented kalman filters\\nukf\\nfeature depth estimation\\nrobot kinematics model\",\"451\":\"tendons\\nmanipulators\\nactuators\\nrouting\\nshafts\\nbending\\ncontrol system synthesis\\ndc motors\\nmotion control\\nposition control\\nthree-dimensional printing\\ntorsion\\nmodular continuum robot segment\\ngeneral purpose manipulator\\ntendon-driven continuum robot segment\\nmodular design\\ncontinuous flexible core\\nrigid interlocking vertebrae\\ntorsional movement\\nantagonistic tendon pairs\\nsingle geared dc motor\\nbulky actuation unit\\ntorsional rigidity\\nrigid vertebrae\\nmultimaterial 3d printing\",\"452\":\"shape\\nforce\\ntorque\\nshape control\\nmagnetic resonance imaging\\nmagnetoacoustic effects\\ncorrelation\\nactuators\\ncollision avoidance\\nmulti-robot systems\\nshear modulus\\nuniform global external field\\nmagnetic fields\\nworkspace obstacles\\nnavigation\\ndriving force\\nrectangular rigid body\\nparticle group\\ncircular workspace\\nmean variance configurations\",\"453\":\"trajectory\\nplanning\\nwheels\\nmobile robots\\ntransmission line matrix methods\\nacceleration\\npath planning\\ntrajectory control\\nvelocity constrained trajectory generation\\nunderactuated unstable aerial vehicles\\nlinear accelerations\\ntrajectory planner\\ntrajectory timing characteristics\\nomnidirectional balancing robot\\ncollinear mecanum wheeled robot\\nground based omnidirectional dynamically balancing robots\\ntrajectory optimisation methods\\ndifferentially flat model\\ncollinear mecanum drive\",\"454\":\"vibrations\\nmanipulator dynamics\\ntask analysis\\nrobot kinematics\\ndamping\\nflexible manipulators\\nlyapunov methods\\nnumerical analysis\\nsprings (mechanical)\\nstability\\nvibration control\\nmanipulators\\ntranslationally flexible base\\nfundamental oscillatory system\\nmass spring system\\ncontrol strategy couples\\nn-link manipulator\\nlinear translational stiffness\\nconditional stability argument\\nbase vibrations\\ninput transformation\\ncoordinate transformation\\nsemidefinite lyapunov functions\",\"455\":\"trajectory\\nmathematical model\\ncomputational modeling\\nrobot kinematics\\npredictive models\\ntrajectory tracking\\ncontrol system synthesis\\ngaussian processes\\nmobile robots\\npendulums\\npredictive control\\nrobot dynamics\\nrobust control\\ntrajectory control\\ncontrol design\\nunderactuated balance robot\\nmodel predictive control\\ngaussian process regression model\\nlearning-based control\\ngp model\\nrobustness\\nfuruta pendulum system\",\"456\":\"three-dimensional displays\\ntwo dimensional displays\\norbits\\nposition control\\nperturbation methods\\naerospace electronics\\nrobots\\nclosed loop systems\\ncontrollability\\nmatrix algebra\\nmulti-agent systems\\nmulti-robot systems\\nnonlinear control systems\\ncontrol laws\\nmultiagent system\\nself-propelled agents\\nshared control input\\nglobal control input\\n3d multiagent position control\\ncontrol inputs\\nrotation commands\\nrotation matrix\\ncontrollability results\\n2d case\",\"457\":\"robots\\ntask analysis\\nrouting\\nresource management\\ntraveling salesman problems\\napproximation algorithms\\nheuristic algorithms\\napproximation theory\\nminimax techniques\\npath planning\\ntravelling salesman problems\\ntask allocation\\nheterogeneous robots\\npath planning problem\\nmin-max objective\\nmin-max mdhtsp\\ntravel cost\\nmultiple depot heterogeneous traveling salesman problem\\nprimal-dual technique\",\"458\":\"robots\\niterative methods\\ntask analysis\\nlinear programming\\nplanning\\nresource management\\nrouting\\nautonomous aerial vehicles\\nmatrix algebra\\nminimisation\\nmulti-robot systems\\npath planning\\ndistance matrix\\nmultiphase heuristic\\ntwo-phase iterative heuristic\\nbranch-and-cut algorithm\\ndecomposition-based approximate methods\\nhigh quality solutions\\nheterogeneous vehicles\\nenergy capacity consideration\\nmultirobot missions planning\",\"459\":\"target tracking\\nrobot kinematics\\ntools\\nsoftware\\nunmanned aerial vehicles\\ncontrol systems\\nautonomous aerial vehicles\\ncontrol system synthesis\\nformal specification\\nmobile robots\\npath planning\\nprogram debugging\\nremotely operated vehicles\\nspecification languages\\nsanity checking\\nhigh-level specifications\\ndomain-specific language\\nspecification optimization\\nrobot controller design\\nspecification patterns\\nsalty domain specific language\\ngr(1) specifications\\ncorrect-by-construction synthesis approach\\ngeneralized reactivity(1) specifications\\nslugs synthesis tool\\nmultiple unmanned air vehicles\\nuav\",\"460\":\"robots\\nclustering algorithms\\nindexes\\nplanning\\ncomputational modeling\\nsensors\\nheuristic algorithms\\nmobile robots\\nmulti-agent systems\\nmulti-robot systems\\npath planning\\nprobability\\nmultiagent spatio-temporal states\\nworld model\\npersistent multirobot mapping\\nuncertain environment\\nconstrained energy capacities\\ntypical occupancy map approaches\\nstatic world\\noccupancy probability\\ngrid cells\\npromotes revisitation\\nunchanging areas\\nnaive planning\\ntractable subproblems\\ntractable computation time\",\"461\":\"cloud computing\\nrobot sensing systems\\ncomputational modeling\\nadaptation models\\nsecurity\\ndata models\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nobject recognition\\npath planning\\nrobot programming\\nrobot vision\\nstorage management\\nfog robotics\\nmobile robot\\ngrasp planning model\\nnonpublic synthetic images\\ndeep object recognition\\nnonprivate synthetic images\\ndeep models\\ncentralized cloud robotics model\\nsurface decluttering\\ndeep robot learning\",\"462\":\"aerospace electronics\\nplanning\\nunderwater vehicles\\ntwo dimensional displays\\noceans\\naustralia\\nlevel set\\nautonomous underwater vehicles\\nmotion control\\npath planning\\nreachability analysis\\nsampling methods\\nstream functions\\ncontrol space\\ncomplicated flows\\nunderwater currents\\nocean currents\\nreachability\\nsampling-based motion planning\",\"463\":\"task analysis\\nkinematics\\nrobot sensing systems\\njacobian matrices\\nend effectors\\nvehicle dynamics\\nautonomous underwater vehicles\\ncollision avoidance\\ndistributed control\\nfeedback\\nmanipulator dynamics\\nmanipulators\\nmobile robots\\nmotion control\\nmulti-robot systems\\nnonlinear control systems\\nposition control\\npredictive control\\ncontrol input saturations\\ncoupled dynamics\\nload sharing coefficients\\ndistributed nmpc\\nobject transportation\\nconstrained workspace\\nstatic obstacles\\nkinematic representation singularities\\njoint limits\\nmultiple underwater vehicle manipulator systems\\nnonlinear model predictive control approach\\ndistributed predictive control approach\\nuvms locally measurements\",\"464\":\"trajectory tracking\\nrobust control\\nfeedback control\\nattenuation\\nuncertainty\\nirrigation\\ncontrol system synthesis\\nfeedback\\nmarine engineering\\nposition control\\npropellers\\ncoordinated robust control scheme\\nreconfigurable multivessel platform\\nfeedback control system\\ncontrol variables\\ncontrol system design\\npropeller-driven vessels\\ndisturbance attenuation performance\\norientation tracking error\\nmaximum tracking error-to-disturbance ratio\",\"465\":\"robot sensing systems\\npressure sensors\\nbuoyancy\\npistons\\nsimulation\\nforce\\nautonomous underwater vehicles\\nmobile robots\\nunderwater robotic unit\\n1dof\\nambient light sensor\\namussel holding depth\\npressure sensor\\none degree-of-freedom\\nweather conditions\\nacoustic communication\",\"466\":\"trajectory\\nplanning\\nmanipulators\\nuncertainty\\noptimization\\naerospace electronics\\ntask analysis\\nautonomous underwater vehicles\\nclosed loop systems\\ncollision avoidance\\ncooperative systems\\nmobile robots\\nmotion control\\noptimisation\\nposition control\\nremotely operated vehicles\\ncluttered environment\\nlocalization uncertainty\\ntrajectory optimization problem\\noptimal trajectory\\ni-auv trajectories\\noptimization solvers\\nquasiquadratic optimization problems\\nnull space saturation controller\\ncluttered underwater environments\\noptimal collision-free\\nintervention autonomous underwater vehicle\\nlinear-quadratic-gaussian controller\\nbase trajectories\\nunified closed-loop motion planning approach\\nintervention auv\\nmotion planning\\nuncertainty minimization\",\"467\":\"lips\\nhydraulic systems\\nforce\\nsubstrates\\nprototypes\\nrough surfaces\\nsurface roughness\\nactuators\\nbiomimetics\\ncables (mechanical)\\nelastic constants\\nmobile robots\\nmotion control\\npolymers\\nunderwater vehicles\\nbio-robotic remora disc\\ndetachment capabilities\\nreversible underwater hitchhiking\\nremoras\\nadhesive discs\\nbiological disc\\nmultimaterial biomimetic disc\\nflexible cable-driven mechanism\\nsilicone soft lip\\ninternal pressure\\ndisc lamellae\\nattached carbon fiber spinules\\nambient underwater pressure\\nattachment and detachment\\nsoft robotics\\nunderwater adhesion\",\"468\":\"robots\\nsolids\\ntask analysis\\nunmanned underwater vehicles\\ncommunication systems\\nhuman-robot interaction\\nhardware\\ncontrol engineering computing\\nmobile robots\\nrobot communication\\nunderwater human-robot interaction loop\\ncolored lights\\nunderwater robots\\nrobot-to-human communication methods\\nbody language gestures\\ncommunication vector\",\"469\":\"robots\\nbuoyancy\\nthree-dimensional displays\\nforce\\ndynamics\\ntwo dimensional displays\\nsolid modeling\\nbiomechanics\\ncompressed air systems\\ndesign engineering\\nmobile robots\\nmotion control\\nopen loop systems\\npistons\\nservomotors\\ntanks (containers)\\nunderwater vehicles\\nservo motor\\nthree-dimensionally maneuverable robotic fish\\ndepth control mechanism\\ncompressed air tank\\non-board water electrolyzer\\ntwo-dimensionally planar motion\\nopen-loop control experiments\\nunderwater robot\\nasymmetric flapping motion\\ncaudal fin\\nforward velocity\\nturning rate\\nvelocity 0.13 m\\/s\\ntime 10.0 s\\ntime 5.5 s\\nsize 0.55 m\",\"470\":\"buoyancy\\nbladder\\nunderwater vehicles\\nprototypes\\nrotors\\nunmanned aerial vehicles\\nmathematical model\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\nautonomous underwater vehicles\\ndesign engineering\\nmotion control\\npneumatic systems\\nthree-term control\\nvehicle dynamics\\nmultimodal hybrid aerial underwater vehicle\\nmhauv\\ndesign concept\\nfixed-wing unmanned aerial vehicle\\nextended endurance\\nnewton-euler formalism\\nmultidomain simulation\\nunderwater glide test\\ndesign principles\\nproportional-integral-derivative\\nmultirotor\\nlightweight pneumatic buoyancy adjustment system\\nvehicle's physical parameters\\ngliding equilibrium points\\nvehicle's motion control\",\"471\":\"prototypes\\nforce\\ncavity resonators\\npropulsion\\nvalves\\nwind tunnels\\ndrag\\nactuators\\naircraft control\\nbiomimetics\\nhinges\\nlegged locomotion\\nmicrorobots\\nmobile robots\\nmotion control\\nnumerical analysis\\npneumatic control equipment\\npneumatic systems\\nrobot kinematics\\nsoft morphable structures\\nsoft morphing fins\\naquatic-aerial multimodal vehicle\\nconcept aircraft\\nnatural organisms\\nmultilocomotion\\nrigid link mechanisms\\npneumatically-driven soft fins\\nflying squid\\nlift force\\ndrag force\\nwind\\nwater tunnel\\nsquid-like aquatic-aerial vehicle\",\"472\":\"robot sensing systems\\nturning\\nposition control\\ntorque\\noscillators\\nkinematics\\nactuators\\nmobile robots\\nmotion control\\nnonlinear control systems\\nrobot dynamics\\nunderwater vehicles\\nnonlinear orientation controller\\ncompliant robotic fish\\nasymmetric actuation\\ncompliant fish-like robot\\nrigid tails\\nflexible tail\\nunderactuated robotic fish\\nasymmetric velocity profiles\\nskewed triangle waves\\nnonlinear control law\\nsimple actuation mechanism\\nunderwater observation\\nsinusoidal tail actuation\\nturning motion\",\"473\":\"cameras\\nsensors\\nthree-dimensional displays\\ncalibration\\nlaser radar\\nautonomous vehicles\\nlighting\\ngeometry\\nimage reconstruction\\nmobile robots\\npath planning\\nremotely operated vehicles\\nrobot vision\\nstereo image processing\\nvideo signal processing\\nsensor suite\\nautonomous vehicle\\nmulticamera system\\n3d scene perception capabilities\\nself-driving vehicle\\nautonomous navigation\\nurban environments\\nrural environments\\nexteroceptive sensors\\nautovision project\\nmultiview geometry\",\"474\":\"cameras\\nquaternions\\nestimation\\nkalman filters\\nmathematical model\\nrobustness\\nnavigation\\ndrag\\nimage filtering\\ninertial navigation\\nmonte carlo methods\\nnonlinear filters\\nobservability\\nstate estimation\\nconsistency problems\\nthree-fold improvement\\nlinear drag term\\nvelocity dynamics\\nestimation accuracy\\npartial-update formulation\\nlinearization errors\\npartially-observable states\\nsensor biases\\nnormally unobservable position\\nheading states\\nvisual-inertial state estimation problem\\nmonte carlo simulation experiment\\nvisual-inertial kalman filters\\nvisual-inertial extended kalman filtering\\nvisual-inertial navigation methods\\nglobal measurements\",\"475\":\"cameras\\nintegrated circuits\\nuncertainty\\ncomplexity theory\\nestimation\\ncovariance matrices\\njacobian matrices\\ndistance measurement\\ngradient methods\\nimage filtering\\nimage texture\\nmatrix inversion\\nmobile robots\\nrobot vision\\nsmoothing methods\\nlow-textured areas\\nsmooth gradients\\ncomplexity reduction methods\\ndirect filter-based monocular visual-inertial odometry\\ndirect filter-based visual-inertial odometry method\",\"476\":\"robots\\nsemantics\\ntask analysis\\ncomputer architecture\\nvisualization\\nimage segmentation\\nstandards\\nconvolutional neural nets\\nmicroprocessor chips\\nmobile robots\\nneural net architecture\\nobject detection\\nrobot vision\\nslam (robots)\\nvideo signal processing\\nimage quality\\nsemantic information\\nrobotic systems\\nv-slam keyframe selection\\nsemantic analysis\\nsemantic image analysis\\nvisual information\\nconvnet\\nvideo\\nvisual-slam\\ncnn architecture\\nonboard cpu\",\"477\":\"image reconstruction\\nestimation\\ncameras\\nunsupervised learning\\ntraining\\nsimultaneous localization and mapping\\nimage matching\\nimage sequences\\nmotion estimation\\nobject detection\\nvideo signal processing\\nmonocular depth\\nmultiple masks\\nunsupervised learning method\\ndepth estimation network\\nego-motion estimation network\\nprojection target imaging plane\\nfine masks\\nimage pixel mismatch\\nrepeated masking\\nkitti dataset\\nlow-quality uncalibrated bike video dataset\",\"478\":\"wheels\\nsoil\\ngravity\\nmoon\\naircraft\\nspace vehicles\\nearth\\ncomputer vision\\nmars\\nmobile robots\\nplanetary rovers\\nplanetary surfaces\\nlower gravity\\nsoil resistance\\nweaker soil bonding\\nrover mobility\\nreduced-mass rover\\nfull-mass rover\\nrover wheel-soil interactions\\ncomputer vision techniques\\nmartian soil simulant\\nrover-soil visualization technique\\nreduced gravity wheel-terrain interaction\\nexomars wheel prototype\\nsimulated martian gravity\\nwheel normal load\\nexomars space mission\",\"479\":\"space vehicles\\naerospace electronics\\nrobot kinematics\\nmanipulators\\nuncertainty\\norbits\\nadaptive control\\naerospace robotics\\nassembling\\ncontrol system synthesis\\nh\\u221e control\\nmirrors\\nmotion control\\nnonlinear control systems\\nposition control\\nrobust control\\nspace robot\\nprecise manoeuvring\\ncontrolled-floating mode\\nin-orbit telescope assembly\\nrobotic arm\\nslow manoeuvres\\nprecise manoeuvres\\norbital assembly missions\\nrobustness\\noptical mirrors\\nnonlinear h\\u221e controller\\nadaptive h\\u221e controller\",\"480\":\"uncertainty\\nplanning\\nnoise measurement\\nspace vehicles\\nnavigation\\nplanetary orbits\\nastronomical image processing\\ndigital elevation models\\nglobal positioning system\\nmobile robots\\npath planning\\nplanetary rovers\\nrobot vision\\nsolid modelling\\nterrain mapping\\nbelief space planning\\nnoisy map data\\nelevation data\\nlunar orbital imagery\\nterrain relative localization uncertainty\\nnoisy elevation\\naccurate global localization\\noperational risk\\ninitial exploration missions\\nglobal position\\nterrain relative navigation\\ntrn\\nplanetary rover-perspective images\\nabsolute positioning\\norbital data\\nterrain features\\ngps\\nsatellite orbital imagery\",\"481\":\"wheels\\nsoil\\ngeometry\\ndeformable models\\nmathematical model\\npredictive models\\nfinite element analysis\\naerospace robotics\\nmobile robots\\nplanetary rovers\\ntrenching\\nautonomous trenching\\nfront wheel\\nrear wheel\\ndeep trench\\nsingle wheel experiments\\ndriving strategy\\nclosed-form model\\ndigging operations\\nwheel actuators\\nplanetary exploration rovers\\nwheel-based trenching\\nsoil displacement terramechanics\",\"482\":\"satellites\\ncameras\\nvisualization\\nsolid modeling\\nrobot vision systems\\nthree-dimensional displays\\naerospace robotics\\nartificial satellites\\ncontrol engineering computing\\ndata visualisation\\nmedical robotics\\nrobot vision\\ntelerobotics\\nda vinci master console\\nconventional control interface\\nconventional visualization\\noperator performance\\ncutting task\\nconventional teleoperation interface\\nda vinci surgical robot\\nconventional camera-based visualization\\naugmented virtuality visualization\\ntrained nasa robot teleoperators\\nground-based experiments\\navailable cameras\\nround-trip telemetry delay\\nhuman operator\\nservicing operation\\nprotective thermal blanketing\\nsatellite insulation\\nteleoperation interfaces\\nexperimental evaluation\",\"483\":\"cameras\\nthree-dimensional displays\\ndistortion\\nimage segmentation\\nrobot vision systems\\nobject detection\\ncomputerised instrumentation\\nimage processing\\nlearning (artificial intelligence)\\nrobust pedestrian detection\\nomnidirectional cameras\\ncomputer vision\\nrobotics\\ndeep learning methods\\nomnidirectional imaging\\nomnidrl\\ndeep reinforcement learning\\n3d bounding boxes\",\"484\":\"three-dimensional displays\\ntwo dimensional displays\\npose estimation\\nvisualization\\ncameras\\ntraining\\ndetectors\\nfeature extraction\\nimage classification\\nimage matching\\nimage representation\\nimage retrieval\\nimage sensors\\nlearning (artificial intelligence)\\nobject recognition\\nsolid modelling\\nvisual databases\\nimage-based counterpart\\nvisual pose estimation\\n2d-3d image\\ncloud correspondences\\nend-to-end deep network architecture\\nquery image\\n3d point cloud reference map\\noxford 2d-3d patches dataset\\noxford robotcar dataset\\nground truth camera pose\",\"485\":\"automation\\nmachine-to-machine communications\\nhandwritten character recognition\\nmanipulators\\nnatural language processing\\nteaching\\njust-drawn handwritten characters\\nwriting utensil\\ntarget stroke\\ncontinuous drawing motion\\nhandcrafted rules\\npredefined paths\\nstroke-based drawing\\nteaching robots\\nmanipulator robots\\nline drawings\",\"486\":\"training\\ngrasping\\nrobots\\nneural networks\\ncomputational modeling\\npredictive models\\nprobability distribution\\ngaussian processes\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nrobot vision\\nstatistical distributions\\ninference time\\nprobabilistic multimodal actor models\\nvision-based robotic grasping\\nneural density model\\nneural network\\nnormalizing flows\\ncomplex probability distributions\\ngaussian mixture\\nconditional distribution\\n4 dimensional action space\",\"487\":\"training\\nestimation\\ngeometry\\ncameras\\nsensors\\nvisual odometry\\nneural networks\\nconvolutional neural nets\\nnatural scenes\\nstereo image processing\\nsupervised learning\\nsingle view depth\\nsurface normal estimation\\nself-supervised learning framework\\nsurface normals\\noutdoor scenes\\nfronto-parallel planes\\npiece-wise smooth depth\\nsurface orientation\\npiece-wise smooth normals\\ntrained normal network\\ndepth network\\nrealistic smooth normal assumption\\nself-supervised depth prediction network\\nconvolutional neural networks\\ndepth-normal consistency\",\"488\":\"aerospace electronics\\nimage reconstruction\\ntask analysis\\nsemantics\\ntraining\\nroads\\nmeasurement\\ncameras\\nclosed loop systems\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nlearning systems\\nmobile robots\\nroad vehicles\\nrobot vision\\ntraffic engineering computing\\ndomain transfer\\nsingle-camera control policy\\nsimulation control labels\\ndriving performance\\nrural roads\\nurban roads\\nmachine learning systems\\nsimulated environment\\nvision-based lane\\ndriving policy\\nrural road\\nimage-to-image translation\\nautonomous vehicle\\nopen-loop regression metric\",\"489\":\"muscles\\nelectrical stimulation\\nelectrodes\\nservice robots\\nskeleton\\nactuators\\nbiological tissues\\nbiomechanics\\ncardiology\\ncellular biophysics\\nelectromechanical actuators\\nmedical robotics\\nmuscle\\nphysiological models\\ntissue engineering\\ncircular mould\\nrotary electrical stimulation\\nbio-syncretic robots\\nhigh-quality muscle rings\\nliving biological systems\\nelectromechanical systems\\nnatural biological entities\\n3d skeletal muscles\\ncontraction force\\nelectrical pulses stimulation\\ncontrol property\\n3d muscle tissues\\nmuscle tissue engineering\",\"490\":\"permanent magnets\\nstrain\\nglass\\nrobots\\nmagnetic forces\\nmicrofluidics\\nbiomembranes\\nbiomems\\ncellular biophysics\\nlab-on-a-chip\\nmedical robotics\\nmicrorobots\\nmembrane\\ncell suction\\nelastic forces\\nejection forces\\nmicrorobot body\\nmammalian oocyte\\ncell injection microrobot development\\nmicrofluidic chip\\nmicronewton forces\\ncell nuclei\\nnozzle\",\"491\":\"vibrations\\ntransducers\\nautomation\\nsubspace constraints\\nglass\\ncameras\\nresonant frequency\\nbiomedical equipment\\nbiomems\\ncellular biophysics\\ngenetics\\nmedical robotics\\nmicromanipulators\\npatient diagnosis\\norienting oocytes\\nassisted reproductive technologies\\nsperm injection\\nmanual manipulation\\nerror procedure\\nskilled embryologists\\ndesired orientation\\nivf clinics\\nextensive changes\\nstandard equipment\\nsurface transducer\\npipette holder\\npipette tip axis\\nvibration burst\\npolar body detection algorithm\\nsystem cause rotation\\nmicropipettes\\nin-vitro fertilization procedures\\n2d motion\",\"492\":\"grippers\\nsorting\\nloading\\nvalves\\nsize measurement\\nimage processing\\nlength measurement\\ncellular biophysics\\nmicrofluidics\\nmultiworm loading\\nbody size measurement\\nsize-based sorting\\nvision-based algorithms\\ndouble-layer microfluidic device\\nvision-based worm detection\\nactive sorting\\npassive sorting mechanisms\\nconventional worm sorting\\nhigh-speed sorting\\nautomated speed sorting\\nvision-based microfluidic system\\nvision-based automated sorting\\nsequential loading\\ncomputer-controlled pneumatic valves\\nworm body length measurement\\nworm body width measurement\\nvision-based worm size measurement\\nworm biology\\nautomated on-chip worm manipulation\\nnematode worm c. elegans\",\"493\":\"measurement\\nlearning systems\\nmanifolds\\ndatabases\\nsymmetric matrices\\nmachine learning\\nmachine learning algorithms\\napproximation theory\\nimage recognition\\niterative methods\\nlearning (artificial intelligence)\\nmatrix algebra\\nvideo signal processing\\nvisual databases\\nasymmetric local metric learning\\npositive sample pairs\\nadaptive local metric learning method\\nsingle distance metric\\nsmooth metric matrix function\\nlinear combinations\\nlearning process\\npositive semidefinite constraint\\nperson reidentification\\nmetric learning\\npsd constraint\\nuci databases\\ngrid database\\nviper database\\ncuhk01 database\\nvideo monitor\\napproximation error bound\",\"494\":\"three-dimensional displays\\ndetectors\\nfeature extraction\\nrobots\\nhistograms\\ndeep learning\\ntask analysis\\ncomputer vision\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\npose estimation\\nhigh posture variance\\nreal-time detection rates\\n3d object detection domain\\nmobile application\\n3d point clouds\\nrobust 3d person detector\\nposture estimator\\nmobile robotic applications\\ncomputer vision domain\\nmobile robotics\\nstanding postures\\nsocially aware navigation\\ndeep learning techniques\\nkinect2 depth sensor\",\"495\":\"legged locomotion\\ntracking\\nacceleration\\nkalman filters\\ninstruments\\nforce\\nrobot sensing systems\\nbiomedical measurement\\nforce sensors\\ngait analysis\\nlaser applications in medicine\\noptical sensors\\nsensor fusion\\ndata association\\nkalman filter\\nfusion algorithm\\nin-shoe devices\\nspatiotemporal gait analysis system\\ndata association methods\\ntracked leg motions\\nmotion capture cameras\\ngait disorders\\nelderly patients\\nmovement function\\nhuman legs\\nlaser range sensor\\nwalking\\nmotion models\\ngait phases\\ngait events\\nmultisensor fusion algorithm\\ninstrumented insoles\\nkinetic gait analysis system\",\"496\":\"three-dimensional displays\\nsemantics\\ncomputational modeling\\nrobots\\nimage segmentation\\ncomputer architecture\\npose estimation\\nconvolutional neural nets\\nimage filtering\\nlearning (artificial intelligence)\\nobject tracking\\nrobot vision\\nstereo image processing\\nvisual perception\\ndirect pose estimation\\nmachine learning\\ndirect estimation techniques\\ngeometric tracking methods\\nrobotic applications\\nobserved point clouds\\nsegmentation maps\\nfast-fcn network architecture\\nconvolutional neural networks\",\"497\":\"navigation\\nelectroencephalography\\nreal-time systems\\nhumanoid robots\\nobject detection\\nheadphones\\nbrain-computer interfaces\\nconvolutional neural nets\\nmedical robotics\\nmedical signal processing\\nmobile robots\\nrobot vision\\ntelerobotics\\nvideo streaming\\nvisual evoked potentials\\nspecialised secondary cnn\\nteleoperation robot commands\\nssvep decoding model\\nreal-time humanoid robot navigation\\nhumanoid robot teleoperation\\nnatural indoor environment\\ndry-eeg technology\\ncortical waveforms\\non-board robot camera\\nonscreen object selection\\nvariable steady state visual evoked potential\\nreal-time video streaming\\nvariable natural environment\\nbrain-computer interface stimuli\\ndeep convolutional neural network\\ndry-electroencephalography based human cortical brain bio-signals decoding\\nvariable bci stimuli\\nnatural scene objects detection\\ndry-eeg enabled ssvep methodology\",\"498\":\"robot sensing systems\\nlaser radar\\nuncertainty\\nprobabilistic logic\\ndistance measurement\\nglobal positioning system\\ninspection\\nmobile robots\\noptical radar\\npath planning\\nprobability\\nrobot vision\\nsensor fusion\\ntunnels\\nultra wideband technology\\nuwb\\ninspection tasks\\nautonomous navigation technology\\nrobot localization techniques\\ngps-denied environments\\nonboard sensors\\ncameras\\nlidar\\nprobabilistic sensor fusion method\\ntunnel-like environments\\ndegeneration characterization model\\nultra-wideband ranging radio\",\"499\":\"semantics\\nthree-dimensional displays\\ntopology\\nsimultaneous localization and mapping\\nvisualization\\ncameras\\npose estimation\\nfeature extraction\\ngraph theory\\nimage matching\\nimage representation\\nstereo image processing\\nobject-level representation\\nsemantic object association\\nsemantic-level point alignment\\nobject-level semantics\\nappearance-based approach\\n3d dense semantics\\nsemantic graph\\nvision-based global localization\\nautonomous navigation\\nplace recognition\\n6-dof pose estimation\\nvisual feature matching\",\"500\":\"visualization\\nestimation\\nfeature extraction\\ncameras\\nindexes\\nrobots\\nmeasurement\\ncomputer vision\\nimage filtering\\nimage matching\\nimage representation\\nimage sensors\\nimage sequences\\nobject detection\\nkeypoint sequence\\nsingle query image\\ndepth-filtered keypoint sequences\\ncamera motion\\nvarying scene appearance\\nsingle-view depth estimation\\nfamiliar visual place\\nextreme environmental appearance change\\nfield-of-view vision\\ntemporal-aware visual place recognition system\\nextreme appearance-change visual place recognition problem\\nsequence-to-single frame matching\\ndepth-filtered keypoints\\ndepth estimation pipeline\",\"501\":\"databases\\ntrademarks\\nrobots\\ndetectors\\ntask analysis\\ntraining\\nshape\\nfeature extraction\\nimage sensors\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nobject detection\\nobject recognition\\nrobot vision\\nservice robots\\nshape recognition\\nsynthetic data\\nconvolutional neural network logo detector\\ndomain randomization\\nsoft drinks\\nlogo images\\nlarge-scale data\\nhousehold objects\\nrobotic object\\ntrademark databases\\nobject fetching\",\"502\":\"grasping\\nrobot kinematics\\ngrippers\\nneural networks\\nobject detection\\ntask analysis\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nrobot vision\\nrobot platform\\nobject detection approach\\nrobot grasp detection\\nrobot grasping problem\\nparallel gripper\\nimage data\\nend-to-end approach\\nrgb images\\ntransfer learning\\nadapted network\\nconvolutional neural network based based object detection architecture\",\"503\":\"grasping\\ntraining\\ndata collection\\ndata models\\ngrippers\\nrobots\\ndeep learning\\ndexterous manipulators\\nimage colour analysis\\ninference mechanisms\\nlearning (artificial intelligence)\\nrobot vision\\ndata efficient grasping\\ndata-driven approach\\ntraining data\\ngrasp training system\\nmodel inference\\nantipodal grasp rule\\naffordance map\\nungraspability\\ngrasp affordances\\npixel-level affordance interpreter network\\nquantitative experiments\\nreal-world grasp experiments\\nqualitative experiments\",\"504\":\"conferences\\nautomation\\ndisplacement measurement\\ngrippers\\nimage sensors\\nphotoacoustic effect\\nultrasonic imaging\\nultrasonic transducers\\npre-touch approaches\\nfingertip noncontact material recognition\\nconventional contact\\nacoustic bi-modal distance\\nnear-distance ranging\\nsensor design\\nnimble grasping\\nrobust grasping\\nmaterial type\\nrobotic fingers\\nsingle-element air-coupled transducers\\noptoacoustic effects\\npulse-echo ultrasound\\nlast-moment perception\\nrobot fingertip\\nrobotic grasping\\noptical bi-modal distance\",\"505\":\"cameras\\nprosthetics\\ndeep learning\\npredictive models\\ngrasping\\nobject recognition\\npattern recognition\\ndexterous manipulators\\nelectromyography\\nlearning (artificial intelligence)\\nmedical control systems\\nmedical signal processing\\northotics\\nvideo signal processing\\nautomatic prediction\\nhand prosthesis\\nvideo sequences\\nrgb-d video data\\northotic devices\\nprosthetic devices\\nsurface electromyography pattern recognition\\narbitrary objects\\nvideo-based technique\\ngrasp-type selection techniques\\nprosthesis control\\nhand-grasp\\nvideo-based prediction\",\"506\":\"legged locomotion\\nhumanoid robots\\nmanipulators\\nrobot sensing systems\\nrobot kinematics\\njacobian matrices\\nhuman-robot interaction\\nmotion control\\nposition control\\nrobot platforms\\nreactive walking\\nupper-body manipulability\\nintention detection\\nhuman robot interaction\\nhand-in-hand interaction scenario\\nimpedance controlled humanoid\\nvelocity transmission\\nrobot arms manipulation quality\\nappropriate directions\\nrobot manipulation ability\\ncoman + humanoid robot\\nmanipulation motion\\nhuman operator\\nreactive steps\\nhumanoid coman+ control\\nwalking pattern generators\",\"507\":\"task analysis\\nimpedance\\ncollaboration\\nrobot sensing systems\\npayloads\\nservice robots\\nactive disturbance rejection control\\nhumanoid robots\\nhuman-robot interaction\\nmanipulators\\nmobile robots\\nmotion control\\nunexpected interaction forces\\nonline self-tuning stiffness regulation principle\\nunexpected interaction loads\\nunnecessary motion commands\\nhuman generated motions\\nverbal interaction channel\\nhuman-robot collaboration task\\nself-modulated impedance multimodal interaction framework\\nhuman robot interaction\\nmanipulation manoeuvres\\nhumanoid robot coman +\",\"508\":\"navigation\\ncollision avoidance\\nrobot kinematics\\nlegged locomotion\\nmathematical model\\nsafety\\ncomputability\\nfeedback\\nhuman-robot interaction\\nmotion control\\nsmt-based control\\nsocial navigation\\nhri\\nsocially acceptable distance\\nrobot motion\\nhigh-level formal specifications\\nhuman behavior\\nformal methods\\nsatisfiability modulo theories\\nutility-based side-by-side navigation control\\nsmt formula\",\"509\":\"planning\\nrobots\\ntrajectory\\nprediction algorithms\\nheuristic algorithms\\ncollision avoidance\\nreal-time systems\\nhuman-robot interaction\\nmanipulators\\nmobile robots\\nprobability\\ntrajectory control\\nhuman-robot collaborative environments\\npre-planned path\\n6-joint manipulator\\nhuman hand\\nrobot trajectories\\nobstacle-avoidance strategies\\nmotion planning\\nlazy safe interval probabilistic roadmap\",\"510\":\"trajectory\\nhidden markov models\\nrobots\\ntask analysis\\ncomputational modeling\\nprediction algorithms\\npredictive models\\nassembling\\nimage segmentation\\nindustrial robots\\nmaximum likelihood estimation\\nmobile robots\\nparticle filtering (numerical methods)\\nrobot vision\\nfast online segmentation\\npartial trajectory\\nefficient plan\\nsafe plan\\nonline activity segmentation algorithm\\nhidden markov model\\nefficient particle-filtering approach\\nactivity sequence\\nonline search process\\ntask model information\\npartial order\\nhuman activity datasets\\nindustrial mobile robot\\nautomotive assembly task\",\"511\":\"instruments\\nlaparoscopes\\ntraining\\nestimation\\nkalman filters\\ntrajectory\\ncameras\\nmedical computing\\nsurgery\\nvisual tracking algorithm\\n3d reconstructed trajectories\\nlinear discriminant analysis\\nfrequency analysis\\nsimple colored tapes\\nstandard physical training box\\nextracted tool trajectories\\nsingle webcam camera\\nstandard laparoscopy training box\\nminimally invasive surgical skills\\nsingle view camera\\nlaparoscopy instrument tracking\",\"512\":\"lung\\ncomputed tomography\\ntraining\\nrobot sensing systems\\nreal-time systems\\ncameras\\nimaging phantoms\\nclosed loop systems\\nimage reconstruction\\nimage registration\\nmedical computing\\nmedical image processing\\nphantoms\\nrendering (computer graphics)\\nsurgery\\nbronchoscope\\nupdate rate\\naverage position error\\nconserved regions\\ntraining dataset\\nsimulated images\\nsimulated domains\\nconservative thresholds\\nrendered images\\nsurgical tools\\ndynamic anatomy\\ntortuous anatomy\\nreal-time localization\\npreoperative scan\\nhuman operators\\nclosed-loop control\\nautonomous agents\\ndeep learning architecture\\nrecorded camera images\\nlung phantom\\noffsetnet\\ntime 30.0 min\\nfrequency 47.0 hz\",\"513\":\"surgery\\ntask analysis\\ntraining\\nrobots\\nkinematics\\ndeep learning\\nrobustness\\ndata handling\\nlearning (artificial intelligence)\\nmedical computing\\nmedical robotics\\nrecurrent neural nets\\nrobot kinematics\\ntelerobotics\\njigsaws dataset\\ndata augmentation\\nkinematic data\\nsurgical data science\\ndeep learning segmentation\\nrobotic-assisted surgical data\\nrobotic-assisted minimally invasive surgery\\nautomated segmentation\\ndata-intensive segmentation algorithms\\nda vinci research kit\\nrecurrent neural network\\nsurgical robotics: laparoscopy\\ndeep learning in robotics and automation\\nrotation augmentation\\nnetwork generalization\",\"514\":\"manipulators\\nexoskeletons\\nsensors\\ndeep learning\\nmuscles\\ntraining\\nbiomechanics\\nelectromyography\\nlearning (artificial intelligence)\\nmedical robotics\\nmedical signal processing\\nmobile robots\\nmotion control\\npatient rehabilitation\\ntrajectory control\\nwearable robots\\ndeep learning based motion prediction model\\nhuman arm dynamics\\nsurface electromyography\\ndeep learning model\\nrobot arm\\nexoskeleton robot control\\nrobot-assisted training\\nmotion trajectory\\n8 degrees-of-freedom upper limb rehabilitation exoskeleton\\nntuh-ii\\nuser motion prediction\\nrat\\nwireless sensors\\nsemg\\n8dofs\",\"515\":\"legged locomotion\\nexoskeletons\\nfoot\\ntrajectory\\nadaptation models\\nforce\\nplanning\\nadaptive control\\ngait analysis\\nhumanoid robots\\nmedical robotics\\nmotion control\\npendulums\\nrobot dynamics\\nadaptive gait planning approach\\nlower-limb walking assistance exoskeletons\\nhuman-exoskeleton system\\nreference foot locations\\nadaptive gait trajectories\\nlevel ground walking\\nparaplegic patients\\nslope terrains\\nstepping locations\\ndynamic movement primitives\\n2d linear inverted pendulum model\\ndynamic gait generator\\nconventional capture point theory\\nadaptive gait planning\\nlower-limb exoskeleton\\nlipm\\nslope\",\"516\":\"superluminescent diodes\\nlegged locomotion\\nkinematics\\ntrajectory\\npredictive models\\niron\\ndata models\\nbioelectric phenomena\\ngait analysis\\nmedical robotics\\nmuscle\\npatient rehabilitation\\ngait coordination patterns\\nrobotic control strategies\\nfunctional electrical stimulation\\ngait cycle\\nswitched linear dynamical systems\\njoint angles\\nkinematic model\\nslds predictions\\nslds dynamics matrices\\ngait phase information\\njoint angle trajectories\\nslds models\\nnormal gait\\njoint angle kinematic data\\ndata-driven gait models\\nelectrical perturbations\\nmechanical perturbations\\ngait kinematics\\ngait rehabilitation robotics\\nhuman gait dynamics\\nindividual-specific effects\\ndata-driven predictive model\",\"517\":\"electromyography\\ngrasping\\nprosthetic hand\\ntask analysis\\ntactile sensors\\nmedical control systems\\nprosthetics\\ntouch (physiological)\\nmyoelectric prosthetic hand\\nclosing signals\\nopen-cell self-skinning polyurethane foam\\ntactile reflexes\\nuser confidence\\nsound side limb\\nrigid items\\nfragile items\\nunilateral myoelectric prosthesis users\\ninhibitory reflex controller\\ncontact signal\\nair pressure\\nexcessive forces\\nsimple tactile reflex\\nhuman hand dexterity\\nfragile objects\\nmyoelectric prosthetic hand users\\ngrasping speed\",\"518\":\"back\\nforce\\nmuscles\\nfabrics\\nthigh\\nhip\\nactuators\\nbending\\nbiomechanics\\nbone\\nkinematics\\nmedical control systems\\nmuscle\\nbiomechanical study\\nfabric construction\\ntwisted string actuators\\nmechanical stresses\\nlower back assistance\\nstatic forward bending\\ntrunk flexion\\nbending kinematics\\nstatic bending posture\\nrisk factor\\nmuscle activation\\nlightweight design\\ntensile forces\\nforce transmission\\ndynamic lifting\\nphysical load\\nrepetitive heavy lifting\\noccupational activities\\nlow back pain\\nsoft power suit\",\"519\":\"ions\\ngrasping\\nelectrodes\\nmathematical model\\nsoft robotics\\npolymers\\ncontrol system synthesis\\ndexterous manipulators\\nhydrogels\\nrobotic hands\\naqueous solutions\\nfingertip applications\\nstiffness\\nelectric fields\\nfingertip properties\\nelectroactive hydrogels\\nactive soft fingertip\",\"520\":\"manipulators\\nload modeling\\nmathematical model\\nnumerical models\\nstrain\\nbending\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nneural nets\\nnumerical analysis\\npneumatic actuators\\nposition control\\ntorsion\\nopen loop position control\\ndeep reinforcement learning\\nsoft robots\\nnonlinear spatial deformations\\ninherent actuation\\nsoft spatial continuum arm\\nunidirectional bending deformation\\nbidirectional torsional deformation\\ndeep-q learning\\ncontinuum arm prototype\\nexternal loading conditions\",\"521\":\"dynamics\\nplanning\\nheuristic algorithms\\nsoft robotics\\nfinite element analysis\\ncomputational modeling\\nactuators\\nelasticity\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmotion control\\npath planning\\nrobot dynamics\\nrobot kinematics\\nreinforcement-learning-based feedback control\\nunderwater swimming robot\\nline-actuated elastic robot arm\\noptimization-based motion planning\\nhierarchical adaptive grid\\nforward dynamics\\narticulated robots\\nsoft robots\\nhierarchical system identification\\nhigh-dof robot systems\\nfast motion planning\",\"522\":\"actuators\\nlegged locomotion\\nsoft robotics\\nrobot sensing systems\\ntask analysis\\ncontrol system synthesis\\ntemporal logic\\nphysical soft robot\\nreactive soft robots\\ncompliant materials\\nrigid bodied systems\\nsoft actuator fabrication methods\\nmultigait walking soft robots\\nsoft materials\\nresilient task planning\\nreactive controllers\\nformal synthesis\\nsensing-based abstraction\\nlinear temporal logic\",\"523\":\"damping\\nstrain\\noils\\nviscosity\\nrobots\\npneumatic systems\\nactuators\\nbending\\ndesign engineering\\ndexterous manipulators\\nelasticity\\ngranular materials\\ngrippers\\npneumatic actuators\\ngranular material\\nbending actuators\\ndynamic morphological computation\\ndamping design\\nsoft material robots\\nunder-actuated grippers\\nmultichamber pneumatic systems\\nmechanical parameters\\nstiffness system\\nviscous oil\\nimmersion\\ndeformation pattern\",\"524\":\"instruments\\nendoscopes\\nsurgery\\nrobot kinematics\\ntools\\ntask analysis\\naugmented reality\\nhelmet mounted displays\\nmean square error methods\\nmedical robotics\\nrobot vision\\ntelerobotics\\nhand-eye configurations\\nroot-mean-square path deviation\\nnovice assistants\\nhand-eye coordination\\ntool manipulation time\\nnavigation time\\nexperienced surgeons\\nhead-mounted display\\naugmented reality application\\narssist\\nhand-held tools\\nfa\\nrobotic-assisted laparoscopic surgery\\nfirst assistant\\naugmented reality assisted instrument insertion\",\"525\":\"haptic interfaces\\nrobot sensing systems\\nvibrations\\nreal-time systems\\nhardware\\ngeometry\\ncomputational geometry\\ncomputer simulation\\ndata gloves\\nhuman computer interaction\\nmanipulators\\nobject tracking\\nvirtual reality\\nphysics-based simulation\\nhaptic feedback\\nglove-based design\\ncollision geometry\\nvive tracker\\nhigh-fidelity grasping\\nvirtual objects\\ncaging-based approach\\nvirtual environments\\nvirtual object manipulation\\nhigh-fidelity hand\\nvr\\nreal-time stable grasps\\nhand localization\\nglove-based system\",\"526\":\"task analysis\\nforce feedback\\nforce\\nmanipulators\\nrobot sensing systems\\ndexterous manipulators\\nhaptic interfaces\\ntelerobotics\\nhaptic sensation\\nwearable haptic devices\\nwearable haptics\\nteleimpedance control\\ndual-arm robotic teleoperation\\nbox placement task\\nhigher mean interaction forces\\nwearable haptic feedback\\nsafely complete exploratory procedures\\ndeep sea exploration\\nhaptic interactions\\ndual arm robotic coordination\",\"527\":\"uncertainty\\ncameras\\nvisual odometry\\nfeature extraction\\nreliability\\nestimation\\nmotion estimation\\nprobability\\nslam (robots)\\nsvo mapping results\\nframe rate camera motion estimation\\nmap points\\ninitialized map point\\ndepth uncertainty\\nsingle-image depth prediction network\\nfeature location\\nprobabilistic mapping method\\ndirect pixel correspondence\\nsemidirect visual odometry\\nv-slam algorithms\\nvisual simultaneous localization\",\"528\":\"image segmentation\\ntask analysis\\nrobot sensing systems\\nmotion segmentation\\nfeature extraction\\nthree-dimensional displays\\nimage motion analysis\\nmobile robots\\npose estimation\\nrobot vision\\nslam (robots)\\nsegmentation algorithms\\nsemantic segmentation\\nrobotics\\nrefined 3d pose information\\nvision-based tasks\\nmutual improvement\\nunified framework\\ninstantaneous motion change handling\\nlong-term changes\\nsimultaneous localization and segmentation\",\"529\":\"cameras\\nsimultaneous localization and mapping\\ntracking\\nmotion segmentation\\nsemantics\\ndynamics\\nmeasurement uncertainty\\nimage colour analysis\\nimage motion analysis\\nimage segmentation\\nimage sequences\\nobject detection\\nobject tracking\\noctrees\\npose estimation\\nprobability\\nrobot vision\\nslam (robots)\\nvideo signal processing\\nrobustly track\\nforeground object probabilities\\nobject model\\nobject-level dynamic volumetric map\\ninstance segmentation part\\noctree-based object-level multiinstance dynamic slam\\nmultiinstance dynamic rgb-d slam system\\nrobust camera tracking\\ngeometric motion properties\\ngeometric motion information\\nobject-oriented tracking method\\ncamera pose estimation\\nsemantic motion properties\\nfrequency 2.0 hz to 3.0 hz\",\"530\":\"three-dimensional displays\\nsimultaneous localization and mapping\\ncameras\\npose estimation\\nimage reconstruction\\ngeometry\\ncomputer vision\\nimage colour analysis\\noptimisation\\nsurfel-based dense rgb-d reconstruction\\nlocal consistency\\nhigh surface reconstruction accuracy\\ndense mapping\\nvision communities\\nrobotics literature\\nrgb-d cameras\\ndense map\\ndepth input\\naccurate local pose estimation\\nlocally consistent model\\npose tracking\\noffline computer vision methods\\nstructure-from-motion\\nmultiview stereo\\nbatch optimization\\nglobal consistency\\nheavy computation loads\\nconsistent reconstruction\\noffline sfm pipeline\\nstrong global constraints\\noff-the-shelf slam systems\\nhigh local accuracy\\nfactor graph optimization\\naccurate camera\\ndense reconstruction\\ndense slam systems\\nsfm-mvs pipelines\",\"531\":\"simultaneous localization and mapping\\nnavigation\\ncollaboration\\nthree-dimensional displays\\ntask analysis\\naugmented reality\\nhuman-robot interaction\\nmobile robots\\npath planning\\nreal-time systems\\nslam (robots)\\ntelerobotics\\na-slam\\nmap editing\\nnavigation-forbidden areas\\nnavigation goals\\nslam algorithm\\noccupancy grid maps\\nhuman in-the-loop augmented slam\\nreal environment representation\\nmicrosoft hololens\\nrobot teleoperation\\npose correction\\nmap correction\\nar interface\",\"532\":\"legged locomotion\\ntrajectory\\norbits\\nmathematical model\\ncompass\\ncomputational modeling\\ncomputer simulation\\nlinearisation techniques\\nmotion control\\nnonlinear control systems\\npendulums\\nstate-space methods\\nbalance map analysis\\npendulum-like leg movements\\nswing legs\\ninverted pendulum\\nsimple pendulum\\nlinearization\\nnondimensionalization\\ncompass gait model\\nenergy ratio\\nphase difference\\nstance leg\\nswing leg\\norbital energy conservation\\nstep transition\\nbalance loss\\nstate space\\nreachability\\nwalking balance\",\"533\":\"trajectory\\nrobots\\ntask analysis\\nprobabilistic logic\\nkernel\\ndatabases\\ncorrelation\\nlearning (artificial intelligence)\\nmobile robots\\nnonparametric imitation learning\\nrobot motor skills\\nlearning capabilities\\nlearning approach\\nkernel treatment\\nhuman skills\\ncorrelation-adaptive imitation learning\\ncollaborative task\",\"534\":\"acceleration\\nrobots\\noptimization\\ndamping\\ndynamics\\ntask analysis\\ncollision avoidance\\nacceleration control\\nangular momentum\\nfeedback\\nlegged locomotion\\nmotion control\\nrobot dynamics\\nstability\\nshort time interval\\nreaction null-space\\nstepping time\\ngeneral whole-body controller\\nrelative angular acceleration control component\\nangular momentum damping\\ndynamic stepping\\nunknown obstacles\\nupper-body compliance\\nrobot steps\\nunknown height\\nrns\\niterative optimization\\nsimulated dynamic stepping\",\"535\":\"dynamics\\nplanning\\nend effectors\\nlegged locomotion\\nhumanoid robots\\noptimization\\ncomputational complexity\\nconvex programming\\ninteger programming\\nmobile robots\\nnavigation\\npath planning\\nrobot dynamics\\nlearned centroidal dynamics prediction\\nintermittent contact\\ncontact sequence\\nquasistatic balance criterion\\ndynamic motions\\nefficient mixed integer convex programming solvers\\ndynamic contact sequences\\nshort time horizon contact sequences\\ndynamic evolution\\nrobot centroidal momenta\\ndynamically robust contact sequences\\nsearch-based contact planner\\nhumanoid contact planning\",\"536\":\"legged locomotion\\ntrajectory\\npelvis\\nfoot\\nkinematics\\ntorso\\nknee\\ngait analysis\\nmedical control systems\\noptimal control\\ntrajectory control\\nhumanoid robots\\nlinear simplified model\\nnonperiodic walking\\nlower-limb trajectories\\nclosed-form trajectories\\ntorso style\\nbody mass\\ngait parameters\\nbody properties\\nwalking gaits\\nnumerical optimization\\ngeometric variables\\nkinematic conversion\\nstabilization\\ngait generation\\nfootstep locations\\noptimal time-projecting controller\",\"537\":\"mobile robots\\nwheels\\nservomotors\\npropellers\\ntorque\\nforce\\ncontrol engineering computing\\nhelicopters\\nlegged locomotion\\npath planning\\nthree-dimensional printing\\nflying star\\nhybrid crawling\\nreconfigurable hybrid\\nquadcopter robot\\nstar robots\\nsprawling mechanism\\nfstar robot\\nexperimental robot\\nflying modes\\nflying sprawl tuned robot\\nrunning modes\\n3d printed prototype\\ncrawling robot\\nflying robot\\nmechanical design\\nreconfigurable robot\\nsprawl tuning\",\"538\":\"propellers\\nestimation\\nacceleration\\nparameter estimation\\nadaptation models\\nforce\\nmeasurement units\\nadaptive control\\naircraft control\\nautonomous aerial vehicles\\nhelicopters\\nlearning (artificial intelligence)\\nquadcopter inertial measurement units\\nimu\\nplug and play assembly\\nreinforcement learning\\nquadcopters stable operation\\nautonomous flight\\ncontroller parameters\\nadaptive controller architecture\\nestimated physical attachment\\nshort online experiments\\nphysical structure\\nautomatic control\\nonline parameter estimation\\nrigidly attached quadcopters\\nautonomous cooperative flight\",\"539\":\"propellers\\nactuators\\natmospheric modeling\\naerodynamics\\npulse width modulation\\nresource management\\nautonomous aerial vehicles\\nconvex programming\\nmotion control\\noptimal control\\ntrajectory control\\nenergy optimal control allocation\\nredundantly actuated omnidirectional uav\\nactuation model\\ncontrol allocation strategy\\nredundantly-actuated multirotor unmanned aerial vehicle\\nomnicopter\\nactuation redundancy\\ninverse actuator model\\npropeller airflows\\nconvex constrained optimization problem\\npropellers thrusts\\npropeller thrust limits\\nunderactuated multirotors\\nmotion trajectories\",\"540\":\"manipulators\\nwinches\\nlegged locomotion\\npropellers\\ntask analysis\\nactuators\\naerospace robotics\\ncables (mechanical)\\ncollision avoidance\\ncontrol system synthesis\\nstability\\nsam\\nrotor blades\\nrobotic manipulator\\naerial carrier\\nactuation systems\\nsuspended aerial manipulator\\ncollision risk\\npropulsion units\",\"541\":\"open source software\\npropellers\\naerodynamics\\nvehicle dynamics\\nattitude control\\ndrones\\natmospheric modeling\\naerospace components\\naerospace robotics\\naircraft control\\nautonomous aerial vehicles\\neducational robots\\nhelicopters\\nmicrorobots\\nmobile robots\\nrotors\\neducational purposes\\ndesign methodology\\nopen-source phoenix reference design\\nsoftware design\\nphoenix drone\\nopen-source dual-rotor tail-sitter platform\\nopen-source tail-sitter microaerial vehicle platform\\ndual-rotor design\\nopen-source release\\ndesign documents\\nhigh-performance tail-sitter\\ntesting\\nopen-source materials\\nflight control\\nstate estimation\",\"542\":\"orbits\\nactuators\\nend effectors\\nlayout\\nswitches\\nmanipulator dynamics\\nmanipulator kinematics\\nmotion control\\nplates (structures)\\nposition control\\nviscoelasticity\\nsinusoidal displacement input\\norbital shape\\norbital direction\\ninput frequency\\nswitching frequency\\nmechanical parameters\\nthree-dof manipulation\\nplate orbital motions\\n1-actuator 3-dof manipulation\\nunderactuated mechanism\\nmultiple nonparallel\\nnonprehensile manipulation\\nplanar part\\nmanipulator\\nflat plate end effector\\nactive joint joints\\nmultiple passive viscoelastic joints\\njoint axes\\nviscoelastic passive joints\",\"543\":\"snake robots\\nfriction\\nturning\\nsplines (mathematics)\\ninterpolation\\nmathematical model\\nbiomimetics\\nmobile robots\\nmotion control\\npath planning\\ntime-varying systems\\nunderactuated snake robots\\nplanar underactuated bio-inspired snake robots\\ntime-varying line-of-sight guidance law\\ncubic spline interpolation path-planning method\\nsnake robot motion control\\n8-link custom-built snake robot\\nspline based curve path following\\nintegral controller\",\"544\":\"mathematical model\\nrobots\\nadaptation models\\njacobian matrices\\nadaptive control\\ntask analysis\\nactuators\\ncontrol system synthesis\\nfeedforward\\nparameter estimation\\nposition control\\ntwisted string behavior\\nadaptive control methodology\\ntsa-based systems\\nonline parameter estimation\\noutline adaptive estimation methods\\nvariable controller gain\\nadaptive control architecture\\nhigh-bandwidth control\\nadaptive control strategies\\ntsa control system\\nmechatronics\\ntwisted string actuators\\ntendon\\/wire mechanism\\nmotion control\\nlearning and adaptive systems\",\"545\":\"electron tubes\\ntendons\\nprototypes\\nmanipulators\\nmeters\\ninspection\\nadaptive systems\\ntree\\nvariable topology\\ncleaning operations\\nhard-to-reach environments\\nhybrid concentric-tube\\nfully retractable continuum branches\\nbranching continuum robot\\ninspection operations\\ntendon actuated continuum trunk core\",\"546\":\"temperature sensors\\ntemperature measurement\\ncalibration\\nrobot sensing systems\\nstrain measurement\\ncompensation\\nforce measurement\\nforce sensors\\nhumanoid robots\\ntorque measurement\\ntemperature compensation\\nsensor measurements\\nmodel based in situ calibration\\n6 axis force torque sensors method\\nstrain gauges\\nf-t measurement\\nhumanoid robot platform icub\\nforce torque sensing\\ncalibration and identification\",\"547\":\"skin\\nrobot sensing systems\\ntask analysis\\nlegged locomotion\\nhumanoid robots\\ncompliance control\\nhuman-robot interaction\\nmobile robots\\nmotion control\\nposition control\\ntactile sensors\\ncontrol systems\\ncontrol framework\\nskin cells\\nwhole-body active compliance control\\nmulticontact interactions\\nwhole-body control methods\\nhuman environments\\nbody active compliance control\\nposition-controlled stiff humanoid robot\\nmultiple control strategies\\nrobot body\\nfull-size humanoid robot\\nrobot skin\\nmultimodal tactile information\\ncompliant robots\\ntactile sensor information\\nphysical interactions\\nonline information\\ntouch sensing\",\"548\":\"electrodes\\ntactile sensors\\nconductivity\\nvoltage measurement\\nspatial resolution\\nfabrics\\ntomography\\nsoft tactile sensors\\nelectrical resistance tomography\\nunstructured environments\\nwhole-body tactile sensors\\ncomplex electrical wiring\\nsensing elements\\nreconstruction method\\nsensing region\\ncentral region\\nert approach\\noptimal pairwise current injection patterns\\nert system\\nelectrode pair\\nfabric-based soft tactile sensor\\nsensor-specific calibration\\nconstructed sensor\\ninternal array electrodes\\nfrequency 200.0 hz\",\"549\":\"force\\ntactile sensors\\nfinite element analysis\\ncameras\\nforce measurement\\nstrain\\ndeformation\\nimage sensors\\ntactile sensor\\ngelslim 2\\ninverse fem\\nkendama manipulations\\nforce field\\nreconstructed force distribution\\nmarker displacements\\ninverse finite element method\\ngel pad\\ncontact force distribution\\ndense tactile force estimation\",\"550\":\"optimization\\nvisual odometry\\nsimultaneous localization and mapping\\ncameras\\ntraining\\nneural networks\\nestimation\\ngraph theory\\nneural nets\\noptimisation\\npose estimation\\nunsupervised learning\\npose graph optimization\\nunsupervised monocular visual odometry\\nlabel-free leaning ability\\ndrift correction technique\\nlarge-scale odometry estimation\\nloop closure detection\\nhybrid vo system\\nneuralbundler\\ntemporal loss\\nspatial photometric loss\\nmultiview 6dof constraints\\ncycle consistency loss\\nglobal pose graph\\nlocal loop 6dof constraints\\nkitti odometry dataset\\nunsupervised monocular vo estimation\\nmonocular slam systems\",\"551\":\"uncertainty\\nrobot sensing systems\\nlaser radar\\ntraining\\nspaceborne radar\\nneural networks\\nimage segmentation\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\noptical radar\\nprobability\\nradar computing\\nradar imaging\\nautonomous vehicle applications\\nweather conditions\\nraw radar power returns\\nsensor noise\\nocclusion\\ninverse sensor model\\ngrid map\\ngrid cell\\nheteroscedastic uncertainty\\ndeep inverse sensor modelling radar\\nsensor observation\\nmodel formulation\\nstandard cfar filtering approaches\\ndynamic urban environment\\nworld occupancy\\nlidar\\npartial occupancy labels\\ndeep neural network\\noccupancy probabilities\",\"552\":\"neural networks\\nrobot sensing systems\\ntraining\\nmeasurement\\nnavigation\\nuncertainty\\nmobile robots\\nmulti-robot systems\\nneural net architecture\\npath planning\\nsensor field of view\\nsensor fov\\ngenerated hypotheses\\ninformation-theoretic exploration strategy\\ncombined map prediction\\nneural network architecture\\ncustom loss function\\ndeep neural networks\\nfuture robot motions\\nsensor data\\noccupancy map representations\\nbiological systems\\nsensor field\\nfuture motion\\nrobotic systems\\nrobot navigation\\ngenerative networks\\nuncertainty-aware occupancy map prediction\",\"553\":\"vehicle dynamics\\nsemantics\\ntask analysis\\nimage segmentation\\ndeep learning\\nimage reconstruction\\ntraining\\naugmented reality\\nconvolutional neural nets\\nimage classification\\nimage restoration\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nobject recognition\\nrobot vision\\nstatic structure\\nimage inpainting\\ndynamic-object-invariant space\\nvehicles\\npedestrians\\nplausible imagery\\nmulticlass semantic segmentation\\ninpainting methods\\ngenerative adversarial model\\nconvolutional network\\nvision-based robot localization\\nvisual place recognition\",\"554\":\"training\\nlevel set\\nsurveillance\\nthree-dimensional displays\\ntwo dimensional displays\\nsensors\\nconvolution\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\nneural nets\\nautonomous exploration\\ngreedy learning approach\\nsupervised learning approach\\nlevel set representation\\nconvolutional neural network\\nvisibility\\non-line computational cost\\ntopologically accurate maps\\ncomplex 3d environments\\nfrontier-based strategies\\npotential vantage points\\ndeep learning approaches\\nobstacle avoidance\\nlocal navigation\\nglobal exploration problem\\n3d urban environments\",\"555\":\"image reconstruction\\npose estimation\\ncameras\\ndeep learning\\ntraining\\nfeature extraction\\ngallium nitride\\nconvolutional neural nets\\ndistance measurement\\nimage colour analysis\\nimage motion analysis\\nimage sequences\\nunsupervised learning\\nunlabelled rgb image sequences\\ndeep convolutional generative adversarial networks\\nsingle-view depth generation network\\nunsupervised deep vo methods\\nunsupervised deep monocular visual odometry\\ndepth estimation\\nsupervised deep learning approaches\\nvisual odometry applications\\nunsupervised deep learning approaches\\nvo research\\ngenerative unsupervised learning framework\\nmultiview pose estimation\\n6-dof pose camera motion\",\"556\":\"semantics\\nimage segmentation\\ndecoding\\nfeature extraction\\ntask analysis\\nrobots\\nobject detection\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nneural net architecture\\nrobot vision\\nmetric learning\\none-shot detection\\nsemantic scene understanding\\nautonomous robots\\ndynamic environments\\ninstance segmentation\\nmultitask convolutional neural network architecture\\nobject instances\\nlocal connectivity\\nsemantic segmentation\",\"557\":\"hamming distance\\nsearch problems\\nquantization (signal)\\nvisualization\\nmeasurement\\nsimultaneous localization and mapping\\nimage retrieval\\nfeature extraction\\nbinary feature descriptors\\nvisual place recognition\\nmultidimensional continuous cues\\nfeature descriptor\\nbinary string\\ncontinuous cue\\nbinary descriptor types\",\"558\":\"predictive models\\nbayes methods\\nprobabilistic logic\\nprobability density function\\nestimation\\ndecision making\\nmathematical model\\nbelief networks\\nfeature extraction\\ngaussian processes\\nimage classification\\nreal-time systems\\nrecursive estimation\\nuncertainty handling\\ngaussian toroid prediction model\\nprobabilistic framework\\nrecursive bayesian estimation\\nperception-oriented context\\nrecursive bayesian classification scheme\\nhigh-dimensional belief spaces\\nevolving target classification\\nperception target evolution\\nrbc scheme\\nmultigaussian belief representation\\nreal-time analysis\\nobservational uncertainty\",\"559\":\"proposals\\ndata mining\\ndetectors\\nautomobiles\\nfeature extraction\\nstreaming media\\nrobots\\nimage representation\\nobject tracking\\npattern clustering\\ntraffic engineering computing\\nvideo signal processing\\nvideo streaming\\ngeneric object tracker\\nunlabeled video\\nunlabeled driving videos\\nraw video streams\\nobject distribution\\nobject discovery\\nobject tracks\\nobject categories\\nlarge scale object mining\\nrealistic automotive setting\\nfeature representations\\nclustering strategies\",\"560\":\"vehicles\\nfeature extraction\\nvideos\\nroads\\nvisualization\\ntask analysis\\nvehicle dynamics\\ndriver information systems\\nobject detection\\nroad traffic\\nroad vehicles\\nvideo signal processing\\non-road driving videos\\noie\\ndriving scene\\nvisual model\\nobject importance estimation\\nego-vehicles driver\\ndriving control\\nbinary brake prediction\",\"561\":\"cameras\\ndetectors\\nproposals\\nthree-dimensional displays\\ntwo dimensional displays\\ngeometry\\ncontext modeling\\ncomputational geometry\\nfeature extraction\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\npedestrians\\ndeep neural networks\\ndeep object detectors\\ngeometric context\\ndeep pedestrian detection\\ndnn detectors\\ndnn feature learning\",\"562\":\"approximation algorithms\\nrobot sensing systems\\nvisualization\\nsearch problems\\nheuristic algorithms\\nuncertainty\\ngraph theory\\nmobile robots\\nstochastic processes\\ntelecommunication network routing\\nworst-case cost\\nrobust canadian traveler problem applied\\nstochastic canadian traveler problem\\nctp\\nrobot route selection\\ntraversal policy\\npolicy cost\\nevaluation criteria\\napproximate algorithm\\ntraversal cost\\nrobot field trials\\nrctp framework\\nsub-optimal policy alternatives\\nminimum expected cost\\ndistance 5.0 km\",\"563\":\"robots\\npath planning\\naerospace electronics\\ncomplexity theory\\nprobabilistic logic\\nlattices\\nplanning\\ncollision avoidance\\nmobile robots\\nrobot dynamics\\nrobot kinematics\\nsearch problems\\ntrees (mathematics)\\na-search guided tree\\nnode selection\\nheuristic cost\\ncomputation efficiency\\nimproved agt\\ni-agt\\nnode expansion\\nprioritizing control actions\\nprioritizing nodes\\nbi-directional agt\\nbagt\\nkinodynamic planning\\nsecond tree encodes obstacles information\",\"564\":\"convergence\\nspace exploration\\nprobabilistic logic\\nplanning\\npath planning\\nproposals\\nmarkov processes\\nrobots\\nsampling methods\\nsearch problems\\ntrees (mathematics)\\nlocal-connectivity exploitation\\nsampling efficiency\\nsampling-based planners\\nincremental optimal multiquery planner\\nrrdt\\nmarkov chain random sampling\\nactive balancing\\nsampling-based motion planners\\nincremental planners\\nrapidly-exploring random disjointed-trees\",\"565\":\"optimization\\nschedules\\ntrajectory\\nkernel\\ntask analysis\\nrobots\\nbayes methods\\ncomputational complexity\\ngaussian processes\\nhumanoid robots\\nlearning (artificial intelligence)\\nlegged locomotion\\nmotion control\\nnonlinear programming\\noptimal control\\npath planning\\nlocomotion planning\\nlegged systems\\nsuitable contact schedules\\ncontact sequence\\nhybrid dynamical system\\nachievable motions\\noptimal control problem\\nmotion optimization\\nplans contacts\\ncontact schedule selection\\nhigh-level task descriptors\\nmotion planning nonlinear program\\nsingle-legged hopping\\ntask appropriate contact schedules\\nhybrid bayesian trajectory optimization\\nbayesian optimization\\ngaussian process model\\nbilevel optimization\",\"566\":\"robots\\ncollision avoidance\\nnavigation\\nplanning\\nheuristic algorithms\\ntrajectory\\nlogic gates\\ngraph theory\\nmobile robots\\npedestrians\\nsearch problems\\ncrowd navigation\\nreal-time navigation\\nrobotics\\nimminent collision avoidance\\npath planning problem\\ngraph-searching\\ntriangulation space\\nobstacle dynamics\\npublic pedestrian datasets\\ndynamic obstacle avoidance\\ndynamic channels\\nmotion planners\\npedestrian dynamics\\nmobile robot applications\",\"567\":\"robots\\ncollision avoidance\\nconvergence\\nplanning\\nnavigation\\ntrajectory\\npins\\nmobile robots\\nlocal potential functions\\nreflections\\ncollision capable robot platforms\\nreflection surfaces\\nreflection capable omnidirectional robot\\ncell decompositions\\nglobal convergence\\nomnipuck\",\"568\":\"mathematical model\\nactuators\\nmagnetic flux\\nforce\\nmagnetic forces\\nmagnetic circuits\\nembossing\\nelectric field effects\\nelectromagnetic actuators\\nhot working\\nimpact (mechanical)\\nmagnetic field effects\\nimpact hot embossing\\nforce analysis\\nmagnetic field\\nsystem equation\\nelectromagnetic actuator\\nelectrical field\\nmechanical system\\nsimulink analysis\\nhot embossing\\nlinear actuator\\nsystem modeling\\nequivalent magnetic circuit\",\"569\":\"robot sensing systems\\nrobot kinematics\\nsurface impedance\\nsurface treatment\\nsurface emitting lasers\\nend effectors\\nclosed loop systems\\nfixtures\\nforce sensors\\nindustrial manipulators\\nindustrial robots\\nmobile robots\\noptical scanners\\nsurface finishing\\nfreeform surfaces\\nconsistent surface quality\\nrobotic surface finishing system\\nfinishing tool\\nproximity laser sensor\\nsurface finishing process\\nsurface profile mesh\\nrobot closed-loop control system\\nrobot base coordinates\\nsurface finishing experiments\\nwooden surfaces\\nsurface finish\\nrobotic system\\nperception system\",\"570\":\"trajectory\\nservice robots\\nend effectors\\ntrajectory tracking\\nerror correction\\ncompensation\\nindustrial manipulators\\nmobile robots\\nmotion control\\nposition control\\nexecution error\\nend-effector loads\\ngeneral purpose automated compensation scheme\\ntrajectory errors\\nlearned compensation scheme\\ncontext-dependent compensation scheme\\nautomatically generated trajectories\\nrobot model\\nactuator errors\\nlow production volume applications\\nreduced trajectory execution errors\",\"571\":\"task analysis\\nrobot kinematics\\nend effectors\\ntools\\nindexes\\nindustrial manipulators\\nmachine tools\\nnonlinear programming\\nredundant manipulators\\nredundant manipulator\\ncomplex manufacturing task\\nrobot workspace\\ntask surfaces\\nnonlinear optimization problem\\ncomplex workpieces\\nfeasible workpiece placement\\nend-effector\\nconstraint violation functions\",\"572\":\"end effectors\\nkinematics\\nshoulder\\nwrist\\nelbow\\ngeometry\\niterative methods\\njacobian matrices\\nmanipulator kinematics\\nposition control\\nredundant manipulators\\ngeometric search-based inverse kinematics\\ngeometric method\\ninverse kinematics problems\\ninverse position kinematics\\nmanipulator jacobian\\nik algorithm\\nelbow joints\\ngeometry-based ik solvers\\nredundant baxter robot\",\"573\":\"trajectory\\nplanning\\nautomation\\nglobal positioning system\\nsoftware\\nswitches\\nroads\\nmobile robots\\nremotely operated vehicles\\nroad safety\\nroad vehicles\\nmodel-based approach\\nmodel checking\\ndemonstration vehicle\\nformal verification\\nsafe stop supervisor\\nautomated vehicle\\nautonomous vehicles\\npertinent planning\\ncontrol algorithms\\nmode switch\\nnominal planners\\nsafe fallback routine\\nsafe position\\nnominal operational conditions\\nsystem failure\\nmode switching\\nsafe stop trajectory planner\\nresearch concept vehicle\",\"574\":\"optimization\\npath planning\\nplanning\\ndata structures\\ncollision avoidance\\nnavigation\\nrobots\\ngraph theory\\nmobile robots\\noptimisation\\nposition control\\nremotely operated vehicles\\nterrain mapping\\ntrees (mathematics)\\nenvironment representation\\nterrain modeling\\ngraph edge expansions\\noptimization-based terrain analysis\\nunmanned ground vehicle\\nhierarchical model\\nlocal terrain map\\ngraph search algorithms\\nvertex positions\\ncompact data structure\\nspace-dividing tree\\nenvironment model\\nrough environments\\nreal-time optimization-based approach\\nunstructured environments\\nautonomous ground vehicle navigation\",\"575\":\"robots\\nnavigation\\ntrajectory\\npsychology\\ncomputational modeling\\nprediction algorithms\\ncollision avoidance\\nhuman-robot interaction\\nmobile robots\\nsocially-aware robot navigation\\ndominance characteristics\\npdm models\\nperceived dominance levels\\ndominance-based collision-avoidance\\npedestrian dominance model\\nrobot navigation\\nautonomous vehicle navigation\",\"576\":\"roads\\nmeteorology\\nvehicle dynamics\\nsemantics\\nheuristic algorithms\\ntask analysis\\ncameras\\nfeature extraction\\nimage classification\\nimage motion analysis\\nobject detection\\nroad safety\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\nvideo signal processing\\nspace-time variations\\ndynamic traffic scene classification\\nroad scenes\\nspace-time coherence\\nsan francisco bay area\\nsemantic context\\ntactical driver behavior understanding\\ndriving behavior detection\\nvehicle ego-motion\\ntime 80.0 hour\",\"577\":\"automobiles\\nplanning\\nreal-time systems\\ncomputer architecture\\noptimization\\nvehicle dynamics\\ncontrol system synthesis\\nmobile robots\\nnonlinear control systems\\npath planning\\npredictive control\\nfull-scale self-driving racing cars\\nautonomous vehicles\\ncontrol methods\\nautonomous racing cars\\nelectric full scale autonomous racing car\\ncontrol system architecture\\nlocalization methods\\nnonlinear model predictive control\\npre-planned racing line\\nrobotic driver design\",\"578\":\"power demand\\naerodynamics\\npayloads\\npropellers\\nrobots\\nbatteries\\nadaptation models\\nadaptive control\\nautonomous aerial vehicles\\nhelicopters\\nmotion control\\noptimal control\\npath planning\\nposition control\\nquadcopter\\non-board power measurement\\npower consumption\\nenergy-efficient loitering strategy\\nmodel-free online motion adaptation\\nextremum seeking control\\naerodynamic disturbances\",\"579\":\"wires\\nimage reconstruction\\nthree-dimensional displays\\ncameras\\ntransforms\\ncomputational modeling\\natmospheric modeling\\nautonomous aerial vehicles\\nextrapolation\\nimage segmentation\\ninspection\\nobject detection\\npower cables\\npower engineering computing\\nrobot vision\\nstereo image processing\\nmultiview reconstruction\\ncatenary model\\nuav community\\nwire avoidance capabilities\\npowerline corridor inspection\\nmultiview algorithm\\ncatenary curve\\npartial wire detections\\nbundle-adjustment approaches\\nbinarized wire segmentation images\\napproximate extrapolation\",\"580\":\"trajectory\\nreal-time systems\\nplanning\\nconvex functions\\nvehicle dynamics\\nnonlinear dynamical systems\\nload modeling\\naerospace robotics\\ncollision avoidance\\nconvex programming\\nhelicopters\\nmobile robots\\nnonlinear control systems\\npath planning\\npredictive control\\nhigh-dimensional nonlinear system\\ndifferential flatness property\\nnonconvex constraints\\nconvex optimization problem\\noptimal trajectory\\nsemifeasible trajectory\\nmodel predictive control\\nsuspended load\\ncontrol algorithms\\nmultirotor dynamics\\nreal-time trajectory generation\\ncollision-free trajectories\\ncollision-free trajectory\\ndynamic coupling\\nreal-time optimal planning\\nconcave obstacle-avoidance constraints\\nsequential linear quadratic solver\",\"581\":\"cameras\\nrobot sensing systems\\nvisualization\\nestimation\\nneural networks\\ncomputer vision\\nimage resolution\\nautonomous aerial vehicles\\ndata visualisation\\nimage sensors\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nrobot vision\\nlight source direction\\nartificial neural networks\\ndeep networks\\nbioinspired visual system sensor\\nlow resolution images\\nattitude rates\\nbioinspired direct visual estimation\\nsource code\\nclassical computer vision based method\\nlearning approach\\nlow resolution cameras\\ndrosophila's ocellar system\\nhardware setup\\nuav\\nunmanned aerial vehicles\\nangular rates\",\"582\":\"aircraft\\natmospheric modeling\\nfault detection\\nactuators\\nreliability\\ncomputational modeling\\nsafety\\naerospace components\\naerospace simulation\\naircraft testing\\nautonomous aerial vehicles\\nfault diagnosis\\nfault tolerant control\\nleast squares approximations\\nmobile robots\\nrecursive least squares method\\nanomaly detection method\\naircraft model\\nfault detection research\\nfixed-wing flights\\nground truth\\nmid-flight actuator failures\\nfault detection open dataset\\nautonomous aircraft\\ncorrelated input-output pairs\",\"583\":\"visualization\\nfeature extraction\\nimage color analysis\\ntraining\\nrobots\\ntask analysis\\nneural networks\\nconvolutional neural nets\\nimage coding\\nimage representation\\nmobile robots\\nobject recognition\\nrobot vision\\nslam (robots)\\nunsupervised learning\\nsalient landmarks\\nvisual place recognition\\nunderwater environments\\nvisual attention algorithm\\nhand-crafted local descriptors\\nad hoc descriptor generator\\nconvolutional autoencoder\\nad-hoc compact representations\\nseqslam\\nfab-map\\nsurf method\",\"584\":\"standards\\ntraining\\nobject detection\\nrobot sensing systems\\ncomputer vision\\nhuman-robot interaction\\nautonomous underwater vehicles\\nconvolutional neural nets\\ngesture recognition\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\ndiver-diver communication\\ndiver-robot communication\\nunderwater detection dataset\\nstandard diver gestures\\ndiver recognition\\ndiver body-head-hand localization\\ncnn-based approach\\nscubanet dataset\\nhuman-robot communication\\ndiver component recognition\\nrobot-diver communication\\nhuman operators\\ndivers finding\\nrf signal attenuation\\ngesture visual recognition\\nper-instance bounding boxes\\ncrowd sourcing\\ntransfer learning\\nweb-based interface\\ndataset\\nunderwater\\nrobotics\",\"585\":\"plastics\\ntraining\\noceans\\ndata models\\nobject detection\\nvisualization\\nbiological system modeling\\nautonomous underwater vehicles\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nmarine engineering\\nmarine pollution\\nmobile robots\\nneural net architecture\\nrobotic detection\\nmarine litter\\ndeep visual detection models\\ntrash deposits\\naquatic environments\\nmarine ecosystems\\nauv\\ndeep-learning algorithms\\nconvolutional neural network architectures\\ntrained networks\\nunderwater trash removal\",\"586\":\"buoyancy\\nengines\\nbladder\\nbackstepping\\nforce\\nvehicle dynamics\\npressure sensors\\nactuators\\nasymptotic stability\\nautonomous underwater vehicles\\nfeedback\\nflow sensors\\ngears\\nlyapunov methods\\nmobile robots\\nnonlinear control systems\\npressure measurement\\npumps\\nremotely operated vehicles\\nrobot dynamics\\ndual-bladder buoyancy engine\\ncephalopod-inspired auv\\nnonlinear depth\\nbackstepping depth\\npitch controller\\nflow-rate feedback\\ncustom flow sensor\\ndifferential pressure sensor\\n3d-printed attachment\\ndepth control capability\\nsingle-bladder buoyancy engine\\ndepth controller\\nautonomous underwater vehicle\",\"587\":\"uncertainty\\nroads\\nplanning\\nnavigation\\nmarkov processes\\nrobot sensing systems\\ndecision theory\\nmobile robots\\npath planning\\nprobability\\nstate estimation\\nuncertainty-aware path planning\\nroad networks\\naugmented mdps\\nprobabilistic algorithms\\nstate estimation problems\\nrobot\\ncomputationally expensive algorithms\\nuncertainty-augmented markov decision process\\nplanning approach\\nnavigation policies\\npartially observable markov decision process\\nnavigation problems\",\"588\":\"wheels\\npath planning\\nrobot sensing systems\\nplanning\\nmobile robots\\nreal-time systems\\nelectric vehicles\\nimage colour analysis\\nimage sensors\\npose estimation\\nroad safety\\nroad vehicles\\nrobot vision\\nsearch problems\\nmodel based traversability analysis method\\ncomplex environments\\nvehicles 3d pose\\nchassis collision\\nelevation map\\nreactive planning\\nsafe paths\\nwheeled mobile robots\\nreal world environment setups\\nreal-time model\\nreal-time path planning\\nsimulated world environment setups\\nwheeled vehicles\\nvehicle model\\nscoring function\\na*-like search strategy\\nrgb-d sensor\\nfrequency 30.0 hz\",\"589\":\"feature extraction\\nsafety\\ntechnological innovation\\npredictive models\\nmobile robots\\nnavigation\\npath planning\\npredictive control\\nrisk management\\nlocalization sensors\\nmission-critical situations\\nlocal nearest neighbor integrity risk evaluation methodology\\ndata association faults\\nlocalization safety\\ncontrol-input constraints\\nnavigation integrity risk\\nintegrity risk-based model predictive control\\nmpc\",\"590\":\"robot sensing systems\\nplanning\\nsafety\\nautomobiles\\nautonomous vehicles\\ntrajectory\\ninference mechanisms\\nmobile robots\\npath planning\\nroad safety\\nroad vehicles\\nsensor fusion\\nautonomous driving safety\\ninference planning\\npassive safety\\nsensor observations\\nintent-aware dynamic shadow regions\\ncomputation-aware motion planning\\ndriving behaviour\\nautonomous vehicle\",\"591\":\"navigation\\ncost function\\nwheelchairs\\ndynamics\\nvehicle dynamics\\nplanning\\nlevel set\\ncollision avoidance\\nhandicapped aids\\nobject detection\\npath planning\\ndynamic risk density\\ncongestion density\\noccupancy risk\\nvelocity fields\\nobject-based congestion cost\\ncluttered environments\\nautonomous navigation\\nobject tracking\\nautonomous wheelchair\",\"592\":\"navigation\\nrobot kinematics\\ntrajectory\\nplanning\\nrobot sensing systems\\nlaser radar\\ncollision avoidance\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nvelocity control\\nrobot navigation\\nhierarchical planning\\nmachine learning\\noptimal paths\\ndeep local trajectory planner\\nvelocity controller\\nmotion commands\\nattention mechanisms\\nnearby pedestrians\\nmap global plan information\\nsensor data\\nvelocity commands\\nhand-designed traditional navigation system\\ndeep local trajectory replanning\\nglobal planner\",\"593\":\"grippers\\ntransmission line matrix methods\\nfeature extraction\\njacobian matrices\\nkinematics\\nrobot sensing systems\\ncontrol engineering computing\\ndexterous manipulators\\nmanipulator dynamics\\nmanipulator kinematics\\nrandom forests\\nsupervised learning\\ntransferable mechanics models\\ngeneralizable online mode detection\\nunderactuated dexterous manipulation\\nmechanics-inspired framework\\nfingertip-based planar within-hand manipulation\\nunderactuated robotic gripper\\nhand-object system\\ngrasp matrix\\nmanipulability metrics\\nplanar manipulation modes\\nsupervised learning model\\ncontact curvatures\\nprediction transferability\\nvisual approach\\ngripper models\\nfinger jacobians\\nrandom forests classifier\",\"594\":\"robotic assembly\\nrobot sensing systems\\ntask analysis\\nplanning\\nsolid modeling\\ncad\\nproduct design\\ntolerance analysis\\nimplemented architecture capabilities\\ncara system architecture\\nclick and assemble robotic assembly system\\nindustrial product\\ntolerances\\nend to end robotic assembly premise\",\"595\":\"tools\\nrobots\\ntask analysis\\nfasteners\\npipelines\\ncognition\\ncomplexity theory\\nhand tools\\nmanipulators\\ntool construction\\ngeometric reasoning\\ntool macgyvering problem\\nsubstitution problems\\n7-dof robot arm\",\"596\":\"robots\\nimpedance\\ntask analysis\\ntrajectory\\nreinforcement learning\\ncomplexity theory\\nvisualization\\nadaptive control\\nlearning systems\\nmanipulators\\nadaptive impedance control\\nmeta parameter learning\\ncompatible skill specifications\\nabstract expert knowledge\\nquality evaluation metrics\\nadaptive impedance controller\\ncarefully defined skill formalism\\nmanipulation tasks\\nlearned tasks\\nlearning-based solution\\nlearning force-sensitive robot manipulation skills\\nsubmillimeter industrial tolerances\\ntime 20.0 min\",\"597\":\"robot kinematics\\nshape\\nrobot sensing systems\\nmobile robots\\ntask analysis\\ncollision avoidance\\nartificial life\\ncomputational geometry\\nmulti-robot systems\\nopen loop systems\\npath planning\\nopen-loop collective assembly\\ncollective construction\\ndynamic light field design strategies\\nassembled shapes\\npolygonal shapes\\nphototaxic minirobot swarm\\nnonconvex polygons\\nglobal light field\",\"598\":\"shape\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nobject oriented modeling\\nsolid modeling\\nsemantics\\nbayes methods\\ncameras\\nfeature extraction\\nimage colour analysis\\ninference mechanisms\\nobject detection\\npose estimation\\nprobability\\nslam (robots)\\nvariational techniques\\nprobabilistic observation model\\nbayesian inference\\nviewpoint-independent loop closure\\nvariational observation model\\nbayesian object observation model\\n3d object detection\\nprobabilistic semantic slam\\nsingle view projection\\nrgb monocamera\\nobject-oriented feature extraction\\n3d mapping\\nvolumetric 3d object shape information\\nvariational likelihood estimation\\nfeature estimation\\nloop detector\",\"599\":\"training\\nnavigation\\ntask analysis\\nvisualization\\nrobots\\npredictive models\\ncameras\\nimage representation\\nlearning (artificial intelligence)\\nlearning action\\nself-supervised visual exploration\\non-board camera\\ninitial state\\nself-supervised prediction network\\nintrinsic rewards\\ncurrent state-action pair\\nhigher dimensional representations\\nrepresentational power\\ntransition network\\nsparse extrinsic rewards\\ncamera view\\ninput actions\\naction representation module\",\"600\":\"estimation\\ntraining\\nlaser radar\\nimage reconstruction\\npredictive models\\nrobot sensing systems\\nimage colour analysis\\nimage segmentation\\nlearning (artificial intelligence)\\noptical radar\\npnp module updates\\nintermediate feature map\\nsparse data propagation\\nrgb image\\nsparse lidar points\\nplug-and-play module\\nsparse depths\\npretrained depth prediction model\\ndense depth map\",\"601\":\"image segmentation\\nsemantics\\nfeature extraction\\ndeep learning\\nconvolution\\ntraining\\nvehicle dynamics\\nimage fusion\\nneural nets\\nobject detection\\nroads\\ntraffic engineering computing\\nvisual perception\\nsight images\\ndfnet\\ndynamic loss weights\\nfusion layer\\nboundary information loss\\nsemantic segmentation\\nautomatic parking\\nlane markings\\nparking slots\\npavement information\\npixel multiplication\\npsv dataset\\nresidual fusion block\\nrfb\",\"602\":\"estimation\\nimage resolution\\nfeature extraction\\ncomputational modeling\\nthree-dimensional displays\\ncameras\\npredictive models\\nlearning (artificial intelligence)\\nmobile computing\\nstereo image processing\\nend-to-end learned approach\\ninference time\\nmobile devices\\nstereo depth estimation\\nmemory-constrained devices\\ndisparity prediction\\ndisparity maps\\ncomputational constraints\\nstereo image depth estimation\\nnvidia jetson tx2 module\\nanynet\",\"603\":\"task analysis\\ndrones\\ntraining\\nestimation\\ndata models\\ncameras\\nfeature extraction\\nautonomous aerial vehicles\\ncinematography\\nimage sequences\\nregression analysis\\nrobot vision\\nunsupervised learning\\nvideo signal processing\\nimproved generalization\\nsemisupervised regression\\nvisual input\\ndata distributions\\nsemisupervised algorithm\\ngeneralization ability\\nautonomous aerial filming\\nheading direction estimation problem\\ntemporal continuity\\nunsupervised signal\\ntesting performance\\nunlabeled sequences\\nperformance improvement\\nlabeled loss\\nunlabeled loss\\nmoving actor filming\",\"604\":\"uncertainty\\nplanning\\nrobots\\nlattices\\ncollision avoidance\\nshape\\nprobability density function\\nmobile robots\\npath planning\\nprobability\\nrobot shape\\nmotion models\\ngraduated fidelity lattices\\nmotion planning\\nstate lattice based approach\\nmobile robotics\\nmotion uncertainty\\nmultiresolution heuristic\\ncollision probability\",\"605\":\"kernel\\nplanning\\nheuristic algorithms\\nsampling methods\\nconvergence\\nsearch problems\\nbenchmark testing\\ncollision avoidance\\npath planning\\neffective sampling method\\ngood initial solution\\nnonparametric exploration technique\\nnonparametric informed exploration\\nsampling-based motion planning\\nsearch space\",\"606\":\"trajectory\\nroads\\nacceleration\\nvehicle dynamics\\nautonomous vehicles\\ndecision making\\nacceleration control\\ncollision avoidance\\nmobile robots\\npredictive control\\nroad traffic control\\nsimulated annealing\\ntime-varying systems\\ntrajectory control\\nsigmoid trajectory\\ncollision-free intervals\\nnominal conditions\\nvelocity-space representation\\nhighway autonomous driving\\nnear-optimal trajectory generation\\nhighways\\npredictive reference trajectory\\nfree evolution space\\npre-calculated set\\ncandidate trajectories\\ndecoupling path\\nvelocity optimizations\\nmulticriteria functions\\ndecision evaluation function\\ntrajectory generator\\nsimulated annealing approach\\nnoncollision nominal intervals\\nsimulated annealing-optimized trajectory planning\",\"607\":\"robot sensing systems\\nnavigation\\nuncertainty\\npath planning\\nestimation\\noptimized production technology\\ngraph theory\\nlearning (artificial intelligence)\\nmobile robots\\nprobability\\ntravelling salesman problems\\nplanning paths\\nuncertain edge\\nlearned classifier\\npartially-known environments\\nsimulation campaign\\nreal-world maps\\nplanning strategy\\ntraversability estimates\\ncanadian traveller problem\",\"608\":\"microsoft windows\\noptimization\\nsimultaneous localization and mapping\\nglobal navigation satellite system\\nlaser radar\\nmeasurement uncertainty\\noptimisation\\nparticle filtering (numerical methods)\\npose estimation\\nposition measurement\\nwindow factor graphs\\nthird-party maps\\nrobotic applications\\nwindow optimization\\nodometry measurements\\nfast vehicle localization\\naccurate vehicle localization\\nestimation problem\\nsliding window formulation\\nfactor graph\\nlandmark detections\\nautomated car\\nautomated driving applications\",\"609\":\"task analysis\\nvisualization\\ncameras\\nfeature extraction\\ntraining\\ngenerators\\ndata models\\napproximation theory\\nimage retrieval\\nlearning (artificial intelligence)\\nneural nets\\nobject detection\\npose estimation\\nrobot vision\\nslam (robots)\\nvisual databases\\ngeo-tagged images\\nnight-to-day image translation\\nretrieval-based localization\\nvisual localization\\nrobotics pipelines\\nimage retrieval techniques\\ndatabase image retrieval\\nneural models\\nvisual query photo\\ntodaygan\",\"610\":\"roads\\nautomobiles\\nfrequency modulation\\ndetectors\\nglobal navigation satellite system\\nfeature extraction\\noptimization\\ndistance measurement\\ngraph theory\\npath planning\\npose estimation\\nposition control\\nroad traffic control\\nlocalization update rate\\nbasic geometric primitives\\nbehavior generation\\ndistinctive signature\\nmap elements\\nlocalization framework\\nnext generation series cars\\nassociation measurement\\nodometry measurement\\nrobust pose graph optimization\\ntime 30.0 min\\nsize 10.0 cm\\nfrequency 50.0 hz\",\"611\":\"cameras\\nthree-dimensional displays\\npose estimation\\nvisualization\\nfeature extraction\\npipelines\\nreliability\\ndistance measurement\\nimage matching\\nmotion estimation\\nautonomous driving\\nmulticamera visual inertial localization algorithm\\nprioritized feature matching scheme\\nmulticamera systems\\nmonocular cameras\\nprioritization function\\nmulticamera setup\\nmatching efforts\\npose priors\\nlocalization system\\nmotion estimates\\nmulticamera visual inertial odometry pipeline\\nlarge scale environments\\npose estimation stages\\npre-built global 3d map\",\"612\":\"visualization\\nfeature extraction\\ntraining\\nimage recognition\\nbuildings\\ntask analysis\\nmeasurement\\ncomputer vision\\nconvolutional neural nets\\nimage representation\\nobject recognition\\nvisual place recognition\\nperceptual changes\\nrobust image representations\\nenvironmental changes\\nviewpoint changes\\nconvolutional neural networks\\nlandmark localization network\\nappearance changes\\nvegetations\\nsimilarity measurement\\ncnn\\ndiscriminative visual landmark localization\",\"613\":\"three-dimensional displays\\nplanning\\nvisualization\\ncameras\\nsimultaneous localization and mapping\\nmobile robots\\npath planning\\nrobot vision\\npoint clouds\\nfisher information field\\nactive visual localization\\nperception requirement\\nplanning stage\\nlocalization information\\nperception-aware planning\\n3d landmarks\\nsensor visibility\",\"614\":\"navigation\\nmobile robots\\nreinforcement learning\\nlaser radar\\nrobot sensing systems\\ncameras\\nlearning (artificial intelligence)\\nneural nets\\noptical radar\\npath planning\\nrobot vision\\ndeep reinforcement learning-based methods\\ndrl agents\\nlstm agent\\nlocal-map critic\\nlstm-lmc\\nwide fov\\nsingle depth camera\\nlidar devices\\ndrl method\\ndepth cameras\\ndynamics randomization technique\",\"615\":\"task analysis\\nrobots\\nphysics\\ngames\\nreinforcement learning\\ntraining\\ncomputational modeling\\nlearning (artificial intelligence)\\nrobot dynamics\\nrobust control\\nrobustified controller\\nrobotic tasks\\ncomplex dynamics\\nrobot tasks\\ndeep reinforcement learning\\nsimulated environment\\nlearned task\\nfine-tuning\\nsimulation parameters\\nnontrivial task\\nnonrobustified controller\\nsim-to-real transfer learning\\nrobustified controllers\",\"616\":\"data models\\nrobots\\ntask analysis\\npredictive models\\nneural networks\\nreinforcement learning\\ncollision avoidance\\naircraft control\\nautonomous aerial vehicles\\ndata analysis\\nhelicopters\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nvision-based autonomous flight\\nfragile scale quadrotors\\nsmall-scale quadrotors\\ncomplex physics\\nair currents\\nhybrid deep reinforcement learning algorithm\\ngeneralizable perception system\\nnanoaerial vehicle collision avoidance task\\nreal data\\nsimulated data\",\"617\":\"robots\\nnavigation\\nreinforcement learning\\nplanning\\ntask analysis\\nhuman-robot interaction\\nbiological system modeling\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\npedestrians\\ncrowded spaces\\nhuman-human interactions\\ndeep reinforcement learning framework\\ndense crowds\\nhuman dynamics\\ncrowd-aware robot navigation\\nattention-based deep reinforcement learning\\nrobot operations\\ncrowd-robot interaction\",\"618\":\"robots\\ntask analysis\\nfeedback control\\nreinforcement learning\\nmathematical model\\nmanufacturing\\nadaptive control\\ncontinuous systems\\ncontrol system synthesis\\nfeedback\\nfriction\\nindustrial robots\\nlearning (artificial intelligence)\\nlearning systems\\nmechanical contact\\nmotion control\\nrobot dynamics\\nfirst-order physical modeling\\nbrittle controllers\\ninaccurate controllers\\nreinforcement learning methods\\ncontinuous robot controllers\\ncontrol signals\\nrobot control problems\\nmodern manufacturing\\ncontrol design\\nfeedback control methods\\ncontrol policy\\nresidual reinforcement learning\\nrigid body equations of motion\\ncontacts\\nrobot learning\\nunstable objects\\nblock assembly task\",\"619\":\"training\\naerodynamics\\nneural networks\\nmathematical model\\nreinforcement learning\\ndrag\\nprototypes\\nautonomous aerial vehicles\\nfunction approximation\\ngradient methods\\nlearning (artificial intelligence)\\nneural nets\\nposition control\\nthree-term control\\nnature-inspired aerial vehicle\\nposition controller\\nuav\\nfixed-wing aircraft\\nneural network function approximators\\nreinforcement learning agent\\nlearned controller\\ndeep deterministic policy gradients\\npid controller\\nape-x distributed prioritized experience replay\\nmulti-rotors\\nunderactuated nature-inspired unmanned aerial vehicle\\nbody contrary\",\"620\":\"robots\\ntrajectory\\ntraining data\\npredictive models\\nreal-time systems\\nmeasurement\\nclosed loop systems\\nfeedback\\nlearning (artificial intelligence)\\npredictive control\\nreachability analysis\\nformal behavioral specifications\\ncounterexample-guided iterative loop\\nreceding horizon model-predictive controllers\\ndemonstrator actions\\nformally-verified policies\\nformal policy learning from demonstrations\\nreachability properties\\nstructured closed-loop policies\\nmpc\\ncost-to-go function\",\"621\":\"task analysis\\nkinematics\\ncomplexity theory\\nmanipulators\\nfeature extraction\\nservice robots\\ndesign of experiments\\nhuman-robot interaction\\nmulti-robot systems\\ntaguchi methods\\ntask characterization\\nhuman-robot autonomy allocation\\nhuman-robot teams\\nconjoint analysis\\nrotational features\\ntranslational features\\nautonomy assistance\\nrobotic arm\\nkinematic features\",\"622\":\"base stations\\nplanning\\ntrajectory\\nrobot kinematics\\njamming\\nradar\\nconvex programming\\nmobile robots\\nmulti-robot systems\\npath planning\\nrobot dynamics\\nrobust control\\ntemporal logic\\ndos-resilient multirobot temporal logic motion planning\\nmultirobot motion\\nlinear temporal logic specifications\\ndenial-of-service attacks\\nrobot trajectories\\ndos attacks\\ndos-free workspace\\ndos-resilient mission constraints\\ndos-resilient plans\\nsatisfiability modulo convex programming\",\"623\":\"task analysis\\nmanipulators\\ntrajectory\\nactuators\\ngenetic algorithms\\nkinematics\\ncontrol system synthesis\\ndemand robots\\nmodular robots\\ntask description\\ndegree-of-freedom modules\\ntask-based design\\nad-hoc modular manipulators\\ngenetic algorithm\",\"624\":\"cameras\\nlenses\\ncalibration\\nneural networks\\nthree-dimensional displays\\nrobot vision systems\\nimage reconstruction\\nimage sensors\\nneural nets\\nobject recognition\\nrobot vision\\nstereo image processing\\nmultiple sets\\nrectified images\\ndense omnidirectional depth map\\nrig global coordinate system\\nwarped images\\nfinal depth map\\naggregated cost volume\\ndeep neural network\\nconventional depth estimation methods\\nhighly accurate depth maps\\nwide-baseline omnidirectional depth estimation\\nomnidirectional depth sensing\\nconventional stereo systems\\nblind regions\\nnovel wide-baseline omnidirectional stereo algorithm\\ndense depth estimate\\nfisheye images\\ndeep convolutional neural network\\ncapture system\\nmultiple cameras\\nwide-baseline rig\\nultra-wide field\\nview lenses\\ncalibration algorithm\",\"625\":\"three-dimensional displays\\nimage reconstruction\\nsurface reconstruction\\ncameras\\nrobots\\npattern matching\\nrobustness\\ncorrelation methods\\nimage matching\\nimage resolution\\nrobot vision\\nsolid modelling\\nstereo image processing\\nthree-dimensional vision\\n3d surface reconstruction scheme\\nstereo matching pattern projection\\nstereo images\\nphase maps\\nphase-shifting patterns\\nimage acquisition time\\ncorrespondence refinement algorithm\\nhigh-resolution reconstruction\\nobject reconstruction\\ntwo-step stereo matching method\",\"626\":\"cameras\\nthree-dimensional displays\\nreal-time systems\\nimage resolution\\nvehicle dynamics\\nestimation\\nobject detection\\nimage capture\\nimage fusion\\nimage sensors\\nmobile robots\\nrobot vision\\nstereo image processing\\nvisual perception\\nfisheye cameras\\nreal-time dense geometric mapping algorithm\\npinhole cameras\\nvisual-inertial odometry\\nvisual localization\\nvision-only 3d scene perception\\ndepth map\\nreference camera\\nplane-sweeping stereo\\nfast object detection framework\\nyolov3\\nfisheye depth images\\ncomputer vision applications\\nangular resolution\\nimage resolutions\\nin-vehicle pc\\ntruncated signed distance function\\ntsdf volume\\n3d map\\nself-driving vehicles\",\"627\":\"feature extraction\\ncameras\\ncalibration\\nlaser radar\\njacobian matrices\\nsimultaneous localization and mapping\\nestimation\\nimage fusion\\nimage sensors\\ninertial navigation\\nmobile robots\\nmonte carlo methods\\nobject tracking\\nslam (robots)\\nplanar point features\\nnonplanar point features\\npoint-on-plane constraints\\neffective plane feature initialization algorithm\\ndepth sensor\\ngeneral sensor fusion framework\\npoint feature tracking\\nplane extraction\\ngeometrical structures\\nclosest point\\nplane parameterization\\nmonte-carlo simulations\\nvisual sensor\\ntightly-coupled aided inertial navigation system\\nfeature-based simultaneous localization and mapping\",\"628\":\"estimation\\ndecoding\\nneural networks\\nruntime\\nconvolution\\ncomplexity theory\\ntask analysis\\nautonomous aerial vehicles\\ncameras\\ncomputational complexity\\nembedded systems\\nestimation theory\\nimage colour analysis\\nimage segmentation\\nimage sensors\\nlearning (artificial intelligence)\\nmicrorobots\\nmobile robots\\nneural nets\\nobject detection\\nrobot vision\\nlow-latency decoder\\nnyu depth v2 dataset\\nreal-time monocular depth estimation\\ndeep neural network\\nembedded platform\\nmicroaerial vehicle\\nrobotic tasks\\nobstacle detection\\nsingle rgb image\\nmonocular cameras\\nlightweight encoder-decoder network architecture\\nfastdepth\\nfast monocular depth estimation\\ndepth sensing\\ndeep neural networks\",\"629\":\"observers\\nrobots\\ncollision avoidance\\ntorque\\nconvergence\\nnoise measurement\\nend effectors\\nhuman-robot interaction\\ntorque control\\nvariable structure systems\\nexternal torques\\njoint acceleration\\nexternal wrenches\\nrobot structure\\nmomentum dynamics\\nclassic momentum observer\\nreaction strategies\\nsliding mode momentum observers\\ncontrol loop\\nproprioceptive sensors\\nfirst-order filtered version\\nfinite-time convergence\",\"630\":\"clamps\\nrobots\\njamming\\nlaminates\\nforce\\nfriction\\nsafety\\nbeams (structures)\\nbending\\nelasticity\\nhuman-robot interaction\\nmanipulator dynamics\\nmechanical testing\\nmulti-robot systems\\npneumatic actuators\\nposition control\\nsafe co-robots\\nstiff robot arm\\nrobot systems\\nsafe interaction\\ntunable stiffness mechanism\\ndiscrete layer jamming mechanism\\nrobot link\\nmultiple clamps\\nsafer human-robot interaction\\ndiscrete layer jamming beam\\ninjury severity\\nclamping pressure\\npneumatic layer jamming\\npositioning performance\\npayload capacity\\ndynamic actuators\\nabs laminates\\naluminum clamps\\nstiffness tests\\nbending stiffness\\npressure 0.0 mpa to 1.0 mpa\",\"631\":\"robot sensing systems\\ncollision avoidance\\nservice robots\\ncollaboration\\nend effectors\\ncurrent measurement\\nforce sensors\\nhuman-robot interaction\\nindustrial manipulators\\nmanipulator kinematics\\nclosed control architecture\\nrobot dynamics\\nlow-level joint controllers\\nkinematic information\\nadmittance control law\\nwhole-body collision detection\\nkuka kr5 sixx r650 robot\\nindustrial robot\\nphysical human-robot interaction\\ntorque sensors\\nphri strategy\\nend-effector\\nforce\\/torque sensor\\nati f\\/t sensor\",\"632\":\"robots\\ntask analysis\\nimmune system\\nelectroencephalography\\nforce\\ncollaboration\\nbioelectric potentials\\ncognition\\nhuman-robot interaction\\nmedical signal processing\\ncognitive conflict\\nphrc\\npen\\nmechanical resistance\\nconflict level\\nhuman operator\\ndirect contact\\nintuitiveness\\nprediction error negativity\\nnegative deflection\\nevent related potential\\nphysical human-robot collaboration\",\"633\":\"task analysis\\ndictionaries\\nknowledge based systems\\nmachine learning\\nencoding\\noptimization\\nrobots\\nlearning (artificial intelligence)\\npattern classification\\nheterogeneous modalities\\nlearned classifier\\nmultimodal task\\nmultimodal lifelong learning framework\\nconsecutive multimodal learning tasks\\nmultimodal lifelong learning problem\\nheterogeneous multimodal tasks\\nheterogeneous multimodal fusion\\nmaterial recognition task\\nonline dictionary learning algorithm\",\"634\":\"collision avoidance\\nrobot sensing systems\\nnavigation\\nwires\\nforce\\nautonomous aerial vehicles\\ncontrol engineering computing\\nhelicopters\\nmobile robots\\nrobot vision\\nflying robots\\nlocal sensory information\\nquadcopter robot\\nmagnetic-field-inspired robot navigation\\nunder-actuated quad-copter\\narbitrary-shaped convex obstacles\\nmagnetic field interaction\\nreactive navigation algorithms\\nunknown environments\\nmotion commands\\nlocal minima configurations\\ndynamic model\\ncommercial asctec pelican microaerial vehicle\",\"635\":\"robot kinematics\\nmachine vision\\nmonitoring\\nmedical services\\ncameras\\nfrequency-domain analysis\\nbiomedical measurement\\nhandicapped aids\\nhealth care\\nhuman-robot interaction\\nmedical robotics\\nmobile robots\\npatient care\\npneumodynamics\\nrobot vision\\nphysical support\\ncare staffs\\nmedical facilities\\nvision-based contactless breathing measurement system\\nhuman detection\\nnursing facility\\nhuman measurement system\\nautonomous rounds robot\\nbreathing measurement system\\nhuman-care rounds robot\\nthree-dimensional shape\\nthermal information\\nlucia robot\\nbody postures\",\"636\":\"magnetic domains\\ncomputational modeling\\nmagnetic hysteresis\\nsoft magnetic materials\\nnumerical models\\nrobots\\nmagnetic cores\\nclosed loop systems\\ncoils\\ninterpolation\\nmagnetic fields\\nmicrorobots\\nmobile robots\\nmotion control\\ncontrol-oriented model\\ncoil\\nuntethered microscale mobile robotics\\nelliptic integral functions\\nmagnetically actuated microrobots\\nmap-based interpolation\\ncomputation time\\nclosed-loop control\\ndipole approximation\",\"637\":\"couplings\\noptical fibers\\noptical fiber sensors\\noptical distortion\\nfuzzy control\\nintegrated optics\\nmicro-optics\\nmicrorobots\\noptical fibre couplers\\noptical microscopy\\npath planning\\nfuzzy controller\\ndegrees of freedoms\\nmicrowaveguide coupling\\npath planning strategy\\nintegrated optical component\\noptical fiber\\nmicromanufacture field\\ntraditional manual method\\nmicrorobotic positioning system\\nlight intensity feedback\\ncommercial optoelectronic devices\\noptical devices\\ntime 40.0 s\",\"638\":\"steel\\nthree-dimensional displays\\noptical fiber networks\\nveins\\nmagnetic flux\\nmagnetic fields\\nmagnetic multilayers\\nbiomedical materials\\nblood\\nblood vessels\\ncellular biophysics\\nhydrogels\\nliver\\nmultilayers\\nrat liver cells\\nfibrin gel\\ncell viability\\ncellular structure\\n3d channel network\\nmagnetic tweezer\\ncentral veins\\nportal veins\\nhepatic lobule tissue\\nmagnetic hydrogel fibers\\nmultilayered channel system\\ncell-laden hydrogels\\nheptapole magnetic tweezer\\nmultilayered hepatic lobule-like vascular network\\nsteel rods\",\"639\":\"valves\\ncatheters\\nprosthetics\\nrobots\\nthree-dimensional displays\\ngeometry\\nmathematical model\\nblood vessels\\ncardiology\\ndiseases\\niterative methods\\nmedical robotics\\nphantoms\\nsurgery\\nintraballoon pressure\\nsoft balloon catheter\\nheart valve diseases\\nlinear regression\\niterative method\\npressure-volume data\\nidealised aortic phantoms\\nballoon free inflation\\ninflation device\\nrobotised valvuloplasty balloon catheter\\ncardiac electrical signal\\nprosthetic valve leakage\\nimplanted prosthetic valve\\naortic heart valve diseases\\nminimally invasive surgical technique\\ntranscatheter aortic valve implantation\\naortic annulus\",\"640\":\"fasteners\\njoints\\nrobots\\ninstruments\\nelectron tubes\\nkinematics\\ntendons\\ncontrollability\\nhinges\\nmedical robotics\\nmotion control\\nneedles\\nposition control\\nrobot kinematics\\nsurgery\\nminiature robotic tubes\\nrotational tip-joints\\nmedical delivery platform\\nmedical-needle-sized robotic tube\\ninstrument tip\\ntwo-axis laser micromachining\\ndirect tip controllability\\nhuman body\\nhinged instrument\\nflexure joints\\nshorter joint length\\ncompact articulation\\njoint strength\\nlateral directions\\nrobust delivery platform\\nhinge rotation\\ninstrumented prototype\\nlaser beam\\nfine angle control\\nkinematic model\\ntip motion control\",\"641\":\"soft robotics\\nnonlinear dynamical systems\\npredictive models\\nlinear regression\\ndata models\\ntuning\\nautoregressive processes\\nidentification\\nlearning (artificial intelligence)\\nmean square error methods\\nmobile robots\\nneural nets\\nnonlinear control systems\\npneumatic actuators\\nregression analysis\\nrobot dynamics\\nstate-space methods\\nsoft robot dynamics\\nkoopman operator theory\\nlarge-scale data collection\\nsystem identification method\\nlinear representation\\ninfinite-dimensional space\\nnonlinear system identification methods\\ndynamic model\\npneumatic soft robot arm\\nlinear state space model\\nnonlinear hammerstein-wiener model\\nneural network\\ntotal normalized-root-mean-square error\\nnrmse\",\"642\":\"shape\\nsoft robotics\\nsensors\\nkinematics\\ncalibration\\ncomputational modeling\\ncomputational complexity\\ndata visualisation\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot kinematics\\ndata driven inverse kinematics\\nsoft robot\\ncomputational approaches\\ndata quantity\\nmemory complexity\\ntime complexity\\nactuation system\\nvisual markers\\nmotion planning\\nmotion control\\nlearning cube environment\",\"643\":\"manipulator dynamics\\nanalytical models\\npotential energy\\nshape\\ndynamics\\nforce\\nbeams (structures)\\nbending\\ndynamic response\\nelasticity\\nintegral equations\\nvibrations\\nvertical stretch-retractable continuum manipulator\\nbackbone continuum manipulator\\ndeformation modal properties\\ntip horizontal load\\nelastic bending potential energy\\nfree dynamic response\\nmodal dynamic models\\nstatic model\\nvibration modal characteristics\\nequivalent-guided beam\\nfrequency properties\\nelliptic integral approach\",\"644\":\"graphics processing units\\ncomputational modeling\\nsoft robotics\\nthree-dimensional displays\\nplanning\\ninverse problems\\ndeformation\\nelasticity\\ngradient methods\\nleast squares approximations\\nmanipulator dynamics\\nmobile robots\\nmulti-robot systems\\noptimal control\\noptimisation\\npath planning\\nchainqueen\\nreal-time differentiable physical simulator\\nrobot planning\\ngradient-based optimization algorithms\\nmotion planning\\nrigid body simulators\\ndeformable objects\\nrigid body dynamics\\nlagrangian-eulerian physical simulator\\nmls-mpm\\nsoft robotic systems\\nforward simulation\\nbackward gradient computation\\nmoving least squares material point method\",\"645\":\"actuators\\ndeformable models\\nsoft robotics\\nstrain\\nfinite element analysis\\nmathematical model\\nbiomimetics\\nclosed loop systems\\ndeformation\\nlegged locomotion\\npneumatic actuators\\nrobot dynamics\\nconstraint-based dynamics model\\nmultiphysics environment\\nsoft robotic actuators\\nreal-time simulation\\nvalidated physical model\\nreal-time performance\\ndynamic locomotion open-loop control experiments\\nmultiple 1d actuators\\nsoft robotic snake\\ninternal pressure forces\\n1-dimensional pneumatic soft actuator\",\"646\":\"legged locomotion\\nhip\\nactuators\\ngravity\\ntorque\\nfoot\\naerospace robotics\\nforce control\\nmotion control\\nrobot dynamics\\nspacebok\\ndynamic legged robot\\nspace exploration\\nquadrupedal robot\\ndynamic legged locomotion\\nparallel elastic elements\\nhigh-torque brushless motors\\nwalking velocity\\nplanetary gear transmissions\\noptimized parallel motion\\njumping maneuvers\",\"647\":\"actuators\\nlegged locomotion\\ntorque\\nbandwidth\\nknee\\ntorque control\\nforce control\\nmotion control\\nnonlinear control systems\\noptimisation\\npredictive control\\nrobot dynamics\\nmini cheetah\\npowerful robot\\nmechanically robust quadruped robot\\ncontrol systems\\nlegged robots\\ncustom backdriveable modular actuators\\nhigh-bandwidth force control\\nhigh force density\\ndynamic trot\\ndynamic quadruped control\\nconvex model-predictive control\\ncmpc\\noffline nonlinear optimization\\nsize 0.3 m\\nmass 9.0 kg\",\"648\":\"legged locomotion\\nstability criteria\\nrobot sensing systems\\nfoot\\nforce control\\ngait analysis\\nstability\\nhexapod subject\\noptimal leg sequence selection method\\nhexapod robot stability\\nterrain slope\\nstable leg sequence\\nsearch method\\nforce-angle stability margin criterion\",\"649\":\"legged locomotion\\ntorque\\nforce\\nmeasurement\\nrobot sensing systems\\nfoot\\nrobot dynamics\\nrobot kinematics\\nlegged robots\\nquasidirect-drive quadruped\\nopen-source\\nquasidirect-drive design methodology\\nperforming animal\\naverage vertical speed\\nvertical jumping agility\\ncommon performance metrics\\ndynamic locomotion\\nstanford doggo\",\"650\":\"foot\\nmachine vision\\nlegged locomotion\\ntrajectory\\ntransforms\\nphase change materials\\nmotion control\\npath planning\\npose estimation\\nrobot vision\\nvision system\\ndof\\ncentral pattern generator\\ndegree-of-freedom\\ngaze orientation\\nbody velocity\\nlegged robot\\nomnidirectional locomotion\\nhexapod robot\\ncpg framework\\nbody pose control\\nrobot body\",\"651\":\"cameras\\nrobot vision systems\\nvisualization\\ngravity\\nmobile robots\\nposition control\\nrobot vision\\nservomechanisms\\nslam (robots)\\nvisual servoing\\nrobotic forceps\\nposition sensors\\nvisual slam\\nwrist joint\\njoint angle sensing\\nrear end\\nrear joint\\nparallel linkage\\nmonocular camera\\nposition sensing\\njoint angles\\nvisual servo system\\nstatic experiments\\ndynamic positioning experiments\\nvisual servoing system\\nforceps tip\",\"652\":\"three-dimensional displays\\ndetectors\\nsimultaneous localization and mapping\\ncameras\\nlaser radar\\nfeature extraction\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nslam (robots)\\npoint cloud registration\\nloop closure\\nheterogenous multirobot slam applications\\nnarf detector\\n3d keypoint repeatability\\nheterogeneous multirobot slam\\nmultirobot slam scenario\\nsensor measurement point clouds\\npoint density\\n3d keypoint detectors\\nmultirobot slam system\\nkpq-si\\nrelative repeatability\",\"653\":\"cameras\\nrobot vision systems\\nvisual odometry\\ncalibration\\npose estimation\\ndistance measurement\\nfeature extraction\\nimage matching\\nimage sequences\\nlenses\\nmotion estimation\\noptimisation\\nstereo image processing\\nhybrid projection model\\nmultiview p3p ransac algorithm\\nmultiview images\\nwide-baseline wide-fov camera systems\\nrobust visual odometry system\\nwide-baseline camera rig\\nfield-of-view fisheye lenses\\nomnidirectional stereo observations\\nego-motion estimation\\nvo pipeline\\nfeature matching\\nomnidirectional visual odometry\",\"654\":\"simultaneous localization and mapping\\nsemantics\\nthree-dimensional displays\\nmeasurement\\nbenchmark testing\\nheuristic algorithms\\nc++ languages\\ncontrol engineering computing\\nconvolutional neural nets\\nimage reconstruction\\nnatural scenes\\nrobot vision\\nslam (robots)\\nscene understanding\\nnonrigid environments\\ndynamic slam\\nslambench 3\\nevaluation infrastructure\\nsystematic automated reproducible evaluation\\nvisual slam\\nslam research area\\nvisulation aids\\nvisulation metrics\\nconvolutional neural networks\\ndynamicfusion\",\"655\":\"simultaneous localization and mapping\\nestimation\\ngeometry\\ncameras\\nmotion estimation\\nvisualization\\nvisual odometry\\npose estimation\\nslam (robots)\\nphotometric loss\\nself-supervised ego-motion estimation\\naccurate relative pose\\nslam\\nself-supervised learning framework\\nimage depth\\nphotometric error\\nsystematic error\\nrealistic scenes\\ngeometric loss\\nmatching loss\\nself-supervised framework\\nunsupervised egomotion estimation methods\",\"656\":\"sensors\\nmonitoring\\ncurrent measurement\\nmathematical model\\nfault detection\\nsafety\\ntime measurement\\nkalman filters\\nlocation based services\\nmobile radio\\nmobile robots\\nkf-based localization\\nlocalization safety\\naviation performance metric\\naviation integrity monitoring solutions\\nrobot navigation\\nintegrity monitoring\\nglobal navigation satellite system\\npositioning sensors\\nkalman filter-based localization\\nautonomous navigation\",\"657\":\"cameras\\nglobal positioning system\\ndatabases\\nrobots\\nfeature extraction\\nthree-dimensional displays\\nasphalt\\nimage capture\\nimage texture\\nlocation based services\\nlocation-aware applications\\nsatellite-based localization\\nimage-based global localization system\\nindex distinctive local keypoints\\nfine texture\\nimage processing pipeline\\ncaptured texture patch\\ntest images\\noutdoor ground surfaces\\nindoor ground surfaces\\nground textures\",\"658\":\"laser radar\\ndistortion measurement\\nthree-dimensional displays\\ndistortion\\noptimization\\ntrajectory\\ngyroscopes\\nmobile robots\\nmotion estimation\\noptical radar\\noptimisation\\nprobability\\nrobot vision\\nin2lama\\nspinning mechanisms\\nresulting point clouds\\nlidar mapping literature\\nconstant velocity motion model\\nupsampled inertial data\\nmotion distortion\\nexplicit motion-model\\ntemporally precise upsampled preintegrated measurement\\nframe-to-frame planar\\nedge features association\\nprobabilistic framework\\ninertial lidar localisation and mapping\\nbatch on-manifold optimisation formulation\\nstate change estimation\\nfront-end interaction\\nback-end interaction\",\"659\":\"three-dimensional displays\\nstandards\\nstochastic processes\\neuclidean distance\\ncost function\\nsensors\\ngradient methods\\nimage colour analysis\\nimage registration\\niterative methods\\noptimisation\\npose estimation\\nslam (robots)\\nstochastic gradient descent\\n3d laser scanners\\nrgb-d cameras\\nmodel registration\\niterative closest point\\nsgd\\nconvergence speed\\n3d point clouds\\nslam\\noptimisation problem\",\"660\":\"unmanned aerial vehicles\\nimpedance\\nrobots\\nsprings\\npropellers\\nmathematical model\\nobservers\\nautonomous aerial vehicles\\ncontrol system synthesis\\ncontrollability\\nend effectors\\nfeedback\\nforce control\\nhelicopters\\nmobile robots\\nnonlinear control systems\\nstability\\ntrajectory control\\nfully-actuated hexarotor\\ngeometrically consistent manner\\nwrench observer\\ngeometric port-hamiltonian approach\\naerial robot\\nport-hamiltonian framework\\nspecial euclidean group\\nuav nonlinear geometric structure\\nenergy tanks concept\\ncontact stability\",\"661\":\"backstepping\\naerodynamics\\nkinematics\\nkalman filters\\nnavigation\\nrail transportation\\nsensors\\naerospace robotics\\nhelicopters\\nmechanical stability\\nmobile robots\\npose estimation\\nposition control\\nrobot dynamics\\nrobot vision\\nslam (robots)\\ntunnels\\nvision-based localisation\\ncross-sectional localisation system\\nintegral backstepping controller\\nquadrotors\\ntunnel-like confined environments\\nintegral backstepping position control\\nkinematic kalman filter\\nsemiautonomous system\\nflying robots\\naerodynamic disturbances\",\"662\":\"payloads\\nkinematics\\nvehicle dynamics\\ndynamics\\nmathematical model\\ntrajectory\\npropulsion\\nautonomous aerial vehicles\\ncables (mechanical)\\nfeedback\\nhelicopters\\nlinearisation techniques\\nmanipulator dynamics\\nmanipulator kinematics\\nmanipulators\\nmobile robots\\npath planning\\nrobust control\\ntrajectory control\\naerial cable towed system\\nrobot configuration\\nkinematic models\\ncentralized feedback linearization controller\\noptimal configurations\\ndynamic trajectories\\nacts\\nquadrotors manipulating\\nrobustness\\naerial systems\\nmulti-robot systems\\nquadrotors\\ncontrol\\nreconfiguration\\ncable-driven parallel robots\",\"663\":\"rotors\\naerodynamics\\nvehicle dynamics\\ntorque\\npropellers\\nanalytical models\\nrobots\\nadaptive control\\naerospace propulsion\\naircraft control\\nautonomous aerial vehicles\\ncontrol nonlinearities\\ncontrol system synthesis\\nhelicopters\\nleast squares approximations\\nlinear systems\\nmomentum\\ntime-varying systems\\ntorque control\\naerobatic flight\\ntime-varying torque generation\\npropeller-aerodynamic-coefficient\\ntorque-latency time-variations\\nmomentum-theory-based analysis\\ndynamic linear time-varying description\\nflyer\\nbackstepping controller\\ntime-varying dynamics\\naerobatic quadrotor\\nrobot\\nltv\\nrecursive least-squares estimation\\npugachev's cobras\\ntriple flips\\naerial vehicle\",\"664\":\"convergence\\nmanifolds\\nattitude control\\nstability analysis\\nsliding mode control\\ntrajectory\\nbackstepping\\nautonomous aerial vehicles\\nclosed loop systems\\ncontrol system synthesis\\nhelicopters\\nlyapunov methods\\nnonlinear control systems\\nposition control\\nstability\\nvariable structure systems\\nfast terminal sliding mode super twisting controller\\naltitude tracking\\nnonlinear fast terminal sliding manifold\\nsuper twisting reaching law\\nquadrotor position\\nftsmstc design\\nchattering phenomena\\nlyapunov stability theory\\nmatlab simulation\\ndji matrice m100\\ncomplete closed loop system stability\",\"665\":\"cameras\\nobservability\\ndrag\\nestimation\\nforce\\nmathematical model\\nvehicle dynamics\\nautonomous aerial vehicles\\nhelicopters\\nimage sensors\\nkalman filters\\nmobile robots\\nnonlinear filters\\nrobot vision\\nslam (robots)\\nonline scale estimation\\nextended kalman filter framework\\nmultirotor dynamics model\\nmonocular camera\\nmetric sensor\\nconventional scale estimation methods\\nmonocular slam\\nmonocular vision\",\"666\":\"robot sensing systems\\nestimation\\nlaser radar\\ncameras\\npipelines\\nthree-dimensional displays\\ngray-scale\\nimage fusion\\nimage matching\\nstereo image processing\\nsparse depth measurements\\nstereo pair\\nsparse range measurements\\nlidar sensor\\nrange camera\\nsensor modalities\\nrandomly sampled ground truth range measurements\\nsparse depth input\\npmdtec monstar sensor\\nstereo information\\nstereo images\\nreal time dense depth estimation\\nanisotropic diffusion\\nsemi-global matching\\nfrequency 5.0 hz\",\"667\":\"task analysis\\ndrones\\nrobot sensing systems\\ntraining\\ncameras\\ncomputer architecture\\nautonomous aerial vehicles\\nhelicopters\\nlearning (artificial intelligence)\\nrobot vision\\nstate estimation\\nvision-based control\\nquadrotor\\nonboard camera\\ncontrol signals\\nhigh-level state estimation\\nlearning approaches\",\"668\":\"mathematical model\\nrobustness\\ngenerators\\nmeasurement\\nnumerical models\\natmospheric modeling\\nunmanned aerial vehicles\\nautonomous aerial vehicles\\nfault diagnosis\\nparticle swarm optimisation\\nrobust control\\nreal flight data\\nparity-based methodologies\\nparity-based diagnosis\\nparticle swarm optimization\\nstatic residuals\\nrobustness metrics\\nrobustness analyses\\ndetectability\\nnonlinear residual generators\\nuav model\\nfault detection system\\ndynamic residuals\",\"669\":\"task analysis\\nresource management\\ncameras\\nrobot kinematics\\nrobot sensing systems\\ndynamic scheduling\\nhuman-robot interaction\\nmobile robots\\nmotion control\\nmulti-robot systems\\nrobot vision\\nhuman participation\\nhuman models\\nhuman-robot teaming framework\\nhuman-aware world model\\ndynamic task allocation\\nhuman-robot teams\\nhuman-robot cooperation\\neye-in-hand camera images\\ntask operations\\ntrust measure\\naction selection algorithm\",\"670\":\"navigation\\ncollision avoidance\\nrobot kinematics\\nroads\\nlegged locomotion\\nedge detection\\nhuman-robot interaction\\nmobile robots\\npedestrians\\ntraffic engineering computing\\npedestrian-based approach\\nsidewalk robot navigation\\npedestrian-rich sidewalk environments\\npedestrian-shared space\\nindoor spaces\\npedestrian movement\\nlinear flows\\nopposing directions\\nrandom movements\\nsafe navigation\\nsidewalk space\\nnatural human motion\\nsocially-compliant manner\\ngroup surfing method\\noptimal pedestrian group\\npedestrian-sparse environments\\nsidewalk edge detection\\nfollowing method\\nintegrated navigation stack\",\"671\":\"dentistry\\nrobots\\ntask analysis\\nreliability\\nprototypes\\nstandards\\ncollision avoidance\\nhuman-robot interaction\\nmedical robotics\\ndentronics\\nnew application domain\\ncollaborative robots\\ndental assistance\\nmultimodal interaction framework\",\"672\":\"robot sensing systems\\ntask analysis\\nwearable sensors\\ngrasping\\nautomotive engineering\\ntracking\\nassembling\\nfeature extraction\\nhuman-robot interaction\\nimage classification\\nimage motion analysis\\nindustrial robots\\nmanufacturing systems\\nproduction engineering computing\\nsensor fusion\\nwearable computers\\nunimodal sensor data\\nucsd-mit human motion dataset\\nvicon motion capture system\\ngross motion recognition\\nsensor modalities\\nsemg\\nhuman activity recognition\\ninertial wearables\\nfine motion detection\\nmanufacturing\\nsafety-critical environments\\nassembly tasks\\nhar algorithms\",\"673\":\"trajectory\\nservice robots\\nprobabilistic logic\\ncomputational modeling\\ncollaboration\\ntask analysis\\ncollision avoidance\\nindustrial manipulators\\nmobile robots\\nmotion control\\npath planning\\nmotion controllers\\nproduction plants\\noptimal proactive path planning\\nindustry 4.0\\nabb yumi\\nindustrial contexts\\ncollaborative robots\\n7 degrees robotic arm\\nhuman operator\\nrobotic paths\\nproactive approach\\nlocal corrective actions\\nsafe motion planning strategies\",\"674\":\"cameras\\nlayout\\ntechnological innovation\\ntuners\\noptical sensors\\nautonomous vehicles\\nimage colour analysis\\nimage motion analysis\\nimage registration\\nimage resolution\\nimage sensors\\nimage sequences\\nobject detection\\nremotely operated vehicles\\nsingle hybrid camera\\nhybrid camera array\\ndisparity images\\nspatial-alignment\\nautonomous vehicle\\nthermal rgb frames\\nthermal eo cameras\\npixel-wise spatial registration\\nvisible-light camera\\nhybrid thermal\\/eo camera\\nrgb frames\\nconstant homography warping\\nimage modalities\",\"675\":\"robot sensing systems\\ncameras\\nthree-dimensional displays\\nautomobiles\\nimage color analysis\\nlaser radar\\nreliability\\ndriver information systems\\nimage colour analysis\\nlearning (artificial intelligence)\\nparticle filtering (numerical methods)\\npose estimation\\nslam (robots)\\nstate estimation\\nvehicle dynamics\\nreliable autonomous racing\\nsensor failure\\ncritical consequences\\nstate estimation approaches\\nautonomous race car\\ntrack delimiting objects\\nsensor modalities\\nlearning-based approaches\\ncamera data\\nredundant perception inputs\\nprobabilistic failure detection algorithm\\nreal-world racing conditions\\nslip dynamics\\nparticle filter based slam algorithm\\npose estimates\\nvelocity 90.0 km\\/h\",\"676\":\"laser radar\\npeer-to-peer computing\\ndistance measurement\\nsimultaneous localization and mapping\\ntwo dimensional displays\\nlaser ranging\\nmobile robots\\noptical radar\\nslam (robots)\\nultra wideband radar\\nwireless sensor networks\\ncooperative sensor network\\n2d lidar sensor\\nuwb-lidar fusion\\nuwb beacon nodes\\npeer-to-peer ranges\\nnearby objects-obstacles\\nsurrounding environment\\ndrift-free slam\\nmobile robot\\n2d laser rangefinder\\nultra-wideband node\\ncooperative range-only slam\\nlidar-based slam algorithm\\nuwb ranging measurements\\nuwb-only localization accuracy\\nlidar mapping\\nlaser scanning information\",\"677\":\"cameras\\ntwo dimensional displays\\ntracking\\nwearable sensors\\nthree-dimensional displays\\nrobot sensing systems\\nparticle filters\\nfuzzy set theory\\nmarine control\\nmulti-agent systems\\nobject tracking\\nparticle filtering (numerical methods)\\nnonlinear agent dynamics\\nsparse camera observations\\nrobust agent localization\\nuncontrollable underwater agents\\nstationary cameras\\nmultiagent control\\nmultiagent tracking problem\\nuncontrollable biological agents\\ncamera detections\\non-body imus\\nfuzzy observation concept\\nnongaussian noise\\nparticle filter based fusion\",\"678\":\"laser beams\\nultrasonic imaging\\nmirrors\\nacoustic beams\\noptical beams\\noptical sensors\\nbeam steering\\nintegrated optics\\nlaser ranging\\nmicromechanical devices\\nmicromirrors\\nmicrosensors\\noptical communication equipment\\nphotodetectors\\nremotely operated vehicles\\nunderwater optics\\nunderwater vehicles\\nhall effect\\nreception modes\\ntransmission modes\\nunderwater ranging and communication\\nsteering co-directional optical beams\\nsteering co-directional acoustic beams\\nsteering co-centered laser beams\\nsteering co-centered ultrasonic beams\\nsteering co-directional ultrasonic beams\\nsteering co-directional laser beams\\nsteering co-centered acoustic beams\\nsteering co-centered optical beams\\nbi-modal communication\\nscan position sensors\\nultrasound beams\\nwater-immersible mems scanning mirror\",\"679\":\"correlation\\nvisualization\\nobject tracking\\nhistory\\nstandards\\nconvolution\\ntraining\\nconvex programming\\ngradient methods\\nlearning (artificial intelligence)\\nobject detection\\nvot benchmark dataset\\notb benchmark dataset\\ngradient-based convex optimization\\nmtcf\\nappearance changes\\nadaptive tracker\\ntemporal windows\\nbase trackers\\nensemble method\\nvisual object tracking\\ncorrelation filters\\nreminiscences\\nvisual appearance\\nvideo history\",\"680\":\"visualization\\noptimization\\nmanipulators\\nimage edge detection\\ntwo dimensional displays\\nrobot sensing systems\\ncomputer vision\\nedge detection\\nimage colour analysis\\nimage sequences\\nlearning (artificial intelligence)\\nobject tracking\\ncoarse-to-fine articulated robot tracking\\narticulated tracking approach\\nrobotic manipulators\\nvisual cues\\nsubpixel-level accurate correspondences\\ndiscriminative depth information\\ncoarse-to-fine articulated state estimator\\nrobot state distribution\\ndepth image\\nschunk sdh2 hand interacting\\nedge tracking\\ndepth keypoints\\narticulated model fitting\\ncolour edge correspondences\\nrgb-d sequences\\nkuica lwr arm\\npalm position estimation\",\"681\":\"visual servoing\\ncameras\\nconvergence\\nobservers\\ntransmission line matrix methods\\nvoltage control\\nrobot vision\\ndiagonally-decoupled direct visual servoing\\nvision-based robot control\\nreference image\\nintensity-based nonmetric solutions\\nfully coupled control error dynamics\\ntranslational part\\nlower triangular system\\nsystem dynamics\\nanalysis complexity\\nsystem performance\\nnonlinear observer\\nrotational part\\ndecoupling properties\\nrobotic arm\\ncontrol error dynamics\",\"682\":\"laser radar\\ntwo dimensional displays\\ndynamics\\nlogic gates\\nrecurrent neural networks\\ntraining\\nrobot sensing systems\\nfeature extraction\\nimage sequences\\nlearning (artificial intelligence)\\nmotion estimation\\noptical radar\\nradar imaging\\nrecurrent neural nets\\nlidar-flownet model\\nmotion flow based method\\n2d lidar map prediction\\noptical flow\\nrecurrent neural network\\nmotion flow estimation\\ngated recurrent unit\\nrobotics navigation\\npath planning\",\"683\":\"task analysis\\nrobot kinematics\\nvideos\\ntraining\\nvisualization\\nvisual servoing\\nfeedback\\nfunction approximation\\nlearning (artificial intelligence)\\nmanipulators\\nrobot vision\\nhuman demonstrations\\ntask function approximation approach\\nrobot eye-hand coordination learning method\\nvisual task specification\\ninverse reinforcement learning\\nlearned reward model\\nuncalibrated visual servoing\\nhand-engineered task specification\\ntraditional uvs controller\\nlearned policy\\nrobot platforms\",\"684\":\"aerodynamics\\nmobile robots\\nvelocity measurement\\nestimation\\nheuristic algorithms\\nautomobiles\\nlyapunov methods\\nrobot vision\\nstability\\nsteering systems\\nvelocity control\\ncar-like mobile robots\\nclmr\\nsteering control system\\nslipping effects\\nvelocity estimation error\\nspeed tracking error\\nvision-based dynamic control\\nvisual algorithm\\nskidding effects\\nspeed control system\\nlyapunov method\\nelectric autonomous tractor\",\"685\":\"laser radar\\ncameras\\ncalibration\\nuncertainty\\nsensors\\ndistortion\\nthree-dimensional displays\\ndistance measurement\\noptical radar\\nradar imaging\\nheterogeneous sensors\\nlidar sensors\\nprecise range information\\nvisual image data\\ncontext based algorithms\\nintrinsic calibration\\nextrinsic calibration\\nlidar measurements\\nconsistent odometry frame\\nimage frame\\nmoving platforms\\nprojection error\\nmotion correction algorithm\\nextended uncertainty model\\nreal-world data\\nwide-angle cameras\\n16-beam scanning lidar\\nuncertainty estimation\\nlidar points\\ncamera images\\nadvanced perception\\ncrucial requirement\\nautonomous vehicle navigation\\nsensor frames\",\"686\":\"sea state\\nmarine vehicles\\ntime series analysis\\nestimation\\nsensors\\nfeature extraction\\ndata models\\nconvolutional neural nets\\ndata analysis\\nfast fourier transforms\\nlearning (artificial intelligence)\\nmarine engineering\\nposition control\\nrecurrent neural nets\\nsensitivity analysis\\nsensor fusion\\nships\\nstate estimation\\ntime series\\nlong dependency\\nship motion data\\nconvolutional neural network\\nfrequency features\\nfeature fusion layer\\nraw time series data\\nhand-engineered features\\nsensitivity analysis method\\ndata preprocessing\\nship motion dataset\\nseastatenet\\nsea state estimation\\ndynamically positioned vessels\\nautonomous ship\\ndeep neural network model\\ntime-invariant feature extraction\\nlong-short-term memory recurrent neural network\\nfast fourier transform block\",\"687\":\"roads\\nthree-dimensional displays\\nsemantics\\nimage segmentation\\npipelines\\ngeometry\\ntask analysis\\ncomputer vision\\nfeature extraction\\nimage reconstruction\\nneural nets\\nobject detection\\npose estimation\\nroad vehicles\\nstereo image processing\\ntraffic engineering computing\\nego-vehicle localization\\nautonomous road driving\\nonline vision-based method\\ndigital map service\\npixel-level semantic segmentation\\nintersection approaches\\nvisual localization\\ndeep neural networks\\ncoarse street-level pose estimation\",\"688\":\"generators\\ngallium nitride\\npredictive models\\ntraining\\ngenerative adversarial networks\\nstate estimation\\noptimization\\ndecision making\\ninteractive systems\\nlearning (artificial intelligence)\\nmulti-agent systems\\nneural nets\\nprobability\\nhyperparameter values\\nadversarial learning\\ninteraction-aware multiagent tracking\\nprobabilistic behavior prediction\\nmotion planning\\nintelligent systems\\nmultiple interactive agents\\ndistribution learning\\ngenerative adversarial network\",\"689\":\"roads\\nautomobiles\\nprediction algorithms\\nartificial neural networks\\nanalytical models\\noptimization\\npredictive models\\npredictive control\\nroad traffic control\\nmodel predictive control approach\\nself-driving vehicles\\non-demand mobility\\ntime-expanded network flow model\\nreal-time mpc algorithm\\ncustomer-carrying vehicles\\nsocial welfare\\nramod system\\nride-sharing autonomous mobility-on-demand systems\\nempty vehicle\\ncustomer-carrying vehicle\\nsan francisco\\nca\",\"690\":\"robot kinematics\\npath planning\\ntask analysis\\ncollision avoidance\\nplanning\\ntopology\\nmobile robots\\nmotion control\\nmulti-robot systems\\nroad traffic control\\nhierarchical framework\\nlarge-scale robot networks\\nmotion coordination problems\\nmultirobot system\\nrobotic warehouses\\nautomated transportation systems\\nlife-long planning problem\\ncoordination performance\\nrobot motion uncertainties\\nhierarchical path planning\\nmotion coordination structure\\ntraffic heat-map\\npath planning level\\nsector-level path\\npath distance\\nmotion coordination level\\ncollision-free local path\\nrolling planning manner\\ntraffic condition\\nrobot uncertainty\",\"691\":\"robots\\nthree-dimensional displays\\ntools\\nobject segmentation\\nclutter\\nimage segmentation\\ntask analysis\\nfeature extraction\\nimage colour analysis\\nmobile robots\\nobject detection\\nobject recognition\\nrobot vision\\nvideo signal processing\\nobject segmentation methods\\nobject-wise annotation\\nocid\\nrobots face\\nrobot perception systems\\ncomputer vision algorithms\\nexpected operating domain\\nground truth data\\neasylabel tool\\nhigh-quality ground truth annotation\\npixel-level\\ndensely cluttered scenes\\nsemiautomatic process\\ncomplex scenes\\nsensor\\nscene distance\\nrobotic rgb-d datasets\\nobject masks\\nobject cluttered indoor dataset\",\"692\":\"three-dimensional displays\\nbenchmark testing\\ntrajectory\\nroads\\ntask analysis\\nsemantics\\nlegged locomotion\\nimage segmentation\\nobject detection\\nrobot vision\\nstereo image processing\\nautonomous driving community\\nlarge-scale dataset platform\\nlarge-scale 5d semantics benchmark\\n3d+temporal\\n4d+interactive\\n5d interactive event recognition\\n5d intention prediction\\ndynamic 4d tracking\",\"693\":\"benchmark testing\\ntask analysis\\nservice robots\\ngrasping\\nprotocols\\nsystem performance\\nend effectors\\nindustrial manipulators\\nmaterials handling equipment\\npath planning\\nrobot vision\\nbenchmarking framework\\nindustrial grocery setting\\nrobotic manipulation\\nindustrial robotic applications\\nrobotic solution\\nindustrial setting\\nmotion planning\\npick-and-place operations\\npick-and-place task\\nobject placement\\nend-effectors\\nrobotic pick-and-place systems\",\"694\":\"simultaneous localization and mapping\\ntrajectory\\ntime measurement\\nvisualization\\nbenchmark testing\\nmotion estimation\\nrendering (computer graphics)\\nslam (robots)\\nwasserstein distance\\nmotion estimation algorithm\\nrobotics slam benchmarking\\nvisual localization\\nmapping algorithms\\nreal-world trajectories\\nhigh-quality scenes\\nsynthetic datasets\\ndense map\\nkey slam applications\\nground robotics\\nmapping datasets\\nmotion estimation algorithms\\ncomputer vision\",\"695\":\"physics\\nengines\\ntask analysis\\nhardware\\nmanipulators\\ncomputational modeling\\nimage motion analysis\\nmobile robots\\nrobot vision\\nrobotic manipulation tasks\\nkinova robotic manipulator\\nmotion capture system\\nmanipulation-oriented robotic tasks\\nrobotic reaching task\\nrobotic interaction task\\nquantitative data\",\"696\":\"drones\\noptical imaging\\ncameras\\ntrajectory\\noptical sensors\\nmeasurement\\nhigh-speed optical techniques\\nhelicopters\\nimage capture\\nimage sensors\\nimage sequences\\nmotion estimation\\nremotely operated vehicles\\nvideo cameras\\nuzh-fpv drone racing dataset\\nfirst-person-view racing quadrotor\\nstate estimation algorithms\\nautonomous drone racing\\nvisual-inertial state estimation\",\"697\":\"splines (mathematics)\\nrobot motion\\noptimal control\\nmathematical model\\njacobian matrices\\ngeometry\\nlegged locomotion\\nlie algebras\\nlie groups\\nmotion control\\ndiscrete representation\\ndirect collocation method\\nb-splines\\nbiped robot\\nrobot motions\\narticulated robots\\njacobian function\\nlie algebra\\ntranscription methods\\nnumerical optimal control\\ngeometric algorithms\\nminimum-effort problem\",\"698\":\"dynamics\\nlegged locomotion\\ntrajectory\\noptimization\\nconvex functions\\noptimal control\\nhumanoid robots\\nmanipulator dynamics\\nmotion control\\noptimisation\\npendulums\\nwhole-body level\\nwhole-body optimal control problem\\ncentroidal dynamics\\neffective locomotion\\nhrp-2 robot\\ndynamics consensus\\nlegged robots\\nlarge control problem\\ncomplex optimal control problem\\nnumerical solver\\ninverted pendulum\\ncapture points\\nwhole-body constraints\\nreduced level\\nreduced solution\\ncentroidal state dynamics\\nmathematical framework\",\"699\":\"measurement uncertainty\\npose estimation\\nuncertainty\\nkalman filters\\ngaussian distribution\\nadaptation models\\nadaptive filters\\nso(3) groups\\nincremental properties\\npose parameters\\nadaptive filtering ability\\nadaptive bingham distribution\\nfilter-based methods\\nautonomous tuning\\nse (3) estimation\\n3d pose estimation problems\\nso(3) group\",\"700\":\"trajectory optimization\\nconvergence\\noptimal control\\nrobots\\nheuristic algorithms\\nplanning\\nconvex programming\\ngusto\\ncontrol-affine systems\\nindirect optimal control\\nguaranteed sequential trajectory optimization\\noptimal control setups\\nscp-based methods\\ntheoretical convergence\\nsequential convex programming\\nalgorithmic framework\",\"701\":\"robots\\nfeedback control\\noptimal control\\ntrajectory optimization\\ncomputational complexity\\ndynamic programming\\ndiscrete time systems\\nfeedback\\nlinear quadratic control\\noptimisation\\nriccati equations\\nequality-constrained lqr\\ndiscrete-time finite-horizon linear quadratic regulator problem\\nauxiliary linear equality constraints\\nfixed end-point constraints\\nstandard riccati recursion\\nlinearly-constrained lqr problem\\nrobotic trajectory optimization\",\"702\":\"legged locomotion\\nsprings\\ndamping\\nmedia\\nfoot\\nforce\\nenergy conservation\\nmotion control\\nposition control\\nsand\\nmitigating energy loss\\nphysically emulated dissipative substrate\\nerosion\\ndesertification\\nspatial resolution\\ntemporal resolution\\ndata collection\\nattractive scout robot candidate\\nheavily geared sensor-laden rhex\\nlong-distance locomotion\\nvirtual damping force\\nminitaur foot\\nsimulated granular media\\nbulk-behavior force law\\nground emulator\\nsingle-legged hopper\\nphysical hopping experiments\\nsubstrate emulator\\nlinear stiffness\\nquadratic damping\\nminitaur robot\\nground properties\\nbulk-behavior model\\nenergy savings\\nrobot hopping\\nphysical single-legged hopper jumping\",\"703\":\"legged locomotion\\ntrajectory\\ncomputational modeling\\npredictive models\\nplanning\\nheuristic algorithms\\nenergy conservation\\nenergy consumption\\nmotion control\\noptimisation\\npath planning\\nrobot dynamics\\nsampling methods\\nsearch problems\\ntrajectory control\\nmobile robots\\nenergy savings\\nenergy efficient navigation\\nrunning legged robots\\nsampling-based model predictive optimization\\nllama quadrupedal platform\\nheuristic-based search\\nrobot motion plan\\ntrajectory planning\",\"704\":\"robot sensing systems\\ntorque\\nactuators\\nlegged locomotion\\ntorque measurement\\nbiomechanics\\nforce control\\ngait analysis\\nmotion control\\nrobot dynamics\\ntorque control\\nforce-controllable quadruped robot system\\ncjts\\nzero-force control\\nwalking\\/trot gait\\ncapacitive-type joint torque sensor\\njoint torque controllability\\nactuator module\\nsize 0.05 nm\\nsize 70.0 nm\\ntime 0.04 s\",\"705\":\"feature extraction\\nrobots\\nimage color analysis\\nimage edge detection\\nvisualization\\ntarget tracking\\nautonomous underwater vehicles\\nconvolutional neural nets\\nhuman-robot interaction\\nmobile robots\\nmulti-robot systems\\nobject detection\\npattern clustering\\nvisual diver recognition\\nunderwater human-robot collaboration\\nautonomous underwater robot\\nvisual scene\\nhuman leader\\ndetected bounding boxes\\nmobile robot\\nk-means clustering algorithm\\nfeature vector\\nfrequency domain descriptors\\nspatial domain descriptors\\nregion proposal network\\nfaster r-cnn algorithm\\ndiver identification\\nmultihuman-robot teams\\nmultiple divers detection\",\"706\":\"robots\\nattenuation\\nnavigation\\nacoustics\\ncomputer architecture\\nnoise measurement\\nmeasurement uncertainty\\nautonomous underwater vehicles\\nmarine navigation\\nmicrorobots\\nmobile robots\\nposition control\\nradio-frequency localization\\nmicroautonomous underwater vehicles\\n\\u03bcauvs\\nintegrated navigation\\ncontrol architecture\\nlow-cost embedded localization module\\nintegrated approach\\nunderwater localization systems\\nmicrounderwater robotics\\nunderwater way-point tracking controller\",\"707\":\"trajectory\\noceans\\nplanning\\nconvex functions\\ndelays\\nenergy consumption\\noptimization\\nconvex programming\\ngradient methods\\nmarine engineering\\nmobile robots\\ntime-varying systems\\ntrajectory control\\nonline utility-optimal trajectory design\\ntime-varying ocean environments\\ntime-varying environments\\nenergy-efficient trajectories\\nstrong disturbances\\nuncertain disturbances\\ntime-varying goal location\\nconstrained online convex optimization formalism\\ngradient descent algorithm\\nvehicle locations\\nregional ocean modelling system\\nocean velocity measurements\",\"708\":\"surface treatment\\nplanning\\nnoise measurement\\ntraining\\nrobot kinematics\\nrobot sensing systems\\napproximation theory\\ngaussian processes\\nmobile robots\\npath planning\\nregression analysis\\nimplicit surface\\nsdf\\nregressor\\ngaussian process implicit surface\\nrobotic tasks\\nsigned-distance function\\nsparse measurements\\ngrid-based methods\\nonline continuous mapping\",\"709\":\"three-dimensional displays\\nsemantics\\nsolid modeling\\nimage reconstruction\\nimage segmentation\\nstructure from motion\\nsurface reconstruction\\ndata visualisation\\nmesh generation\\nrobot vision\\nsolid modelling\\nsemantic image segmentation\\nperceived point cloud\\nglobal statistics\\nclass boundaries\\ninfra-class edges\\n3d dense model\\n3d delaunay triangulation\\nvariable point cloud density\\nsemantic simplification\\ndense 3d visual mapping estimates\\ndense point clouds\\npixel depths\\npoint cloud simplification methods\\nroughly planar surface\",\"710\":\"layout\\nmeasurement\\nrobots\\nindoor environment\\ntwo dimensional displays\\nthree-dimensional displays\\nfeature extraction\\ncontrol engineering computing\\nmobile robots\\npath planning\\nrobot vision\\nslam (robots)\\nslam algorithms\\n2d metric grid map\\nglobal structure\\npartially observed rooms\\nautonomous mobile robots\\nindoor environments\\nrobot sensors\\ngeometrical primitives\",\"711\":\"laser radar\\ncameras\\nthree-dimensional displays\\nimage reconstruction\\nsurface reconstruction\\npipelines\\nrobot sensing systems\\ngraph theory\\nmobile robots\\noptical radar\\nrobot vision\\nstereo image processing\\nlidar measurements\\nmultiview stereo pipeline\\nwatertight surface mesh\\nstate-of-the-art camera-only\\nlidar-only reconstruction methods\\nmonocular vision\\nsurface reconstruction pipeline\\nmonocular camera images\\nmoving sensor rig\\nindoor scenes\\nindoor environments\\n3d mesh models\\nstate-of-the-art multiview stereo\\ngraph cut algorithm\",\"712\":\"mutual information\\napproximation algorithms\\nrobot sensing systems\\nmeasurement\\nstandards\\nrandom variables\\napproximation theory\\ngradient methods\\ninformation theory\\nmobile robots\\nmulti-robot systems\\nfsmi algorithm\\nfast shannon mutual information\\ncauchy-schwarz quadratic mutual information\\ninformation-based mapping algorithms\\ninformation-theoretic mapping\\ncsqmi\\nfsmi\",\"713\":\"cameras\\nimage reconstruction\\nstrain\\nfuses\\nrobot vision systems\\nsimultaneous localization and mapping\\nthree-dimensional displays\\nimage fusion\\npose estimation\\nslam (robots)\\nintensity images\\ndepth images\\nglobally consistent model\\nroom-scale environments\\nurban-scale environments\\nrgb-d cameras\\nstereo cameras\\nmonocular camera\\nsuperpixel-based surfels\\nreconstructed models\\nfast map deformation\\nglobal consistency\\nroom-scale reconstruction\\ntime scalable dense surfel\\ncpu computation\\nsparse slam system\\ncamera poses\\ndense surfel mapping system\",\"714\":\"grounding\\nnatural languages\\ncomputational modeling\\nadaptation models\\nrobots\\nsemantics\\ntask analysis\\ncontrol engineering computing\\ngraph theory\\nhuman-robot interaction\\ninference mechanisms\\nlearning (artificial intelligence)\\nmobile robots\\nnatural language processing\\nprobability\\nrobot programming\\ncompact environment model\\nperceptual classifiers\\nsuccinct instruction-specific environment representation\\nprobabilistic graphical models\\nnatural language symbol grounding\\nrobot instructions\\napproximate inference algorithms\\nnatural language understanding\\nenvironment-related information\\ncompact representations\",\"715\":\"semantics\\ntask analysis\\ngrounding\\nnatural languages\\nrobot sensing systems\\npipelines\\ngrammars\\nhuman-robot interaction\\ninteractive systems\\nlearning (artificial intelligence)\\nnatural language processing\\nnatural language understanding\\nhuman-robot dialog\\nvirtual setting\\namazon mechanical turk\\nphysical robot platform\\nconcept grounding\\nlanguage parsing\\nrobot actions\\nnatural language commands\\nend-to-end pipeline\\nperceptual concepts\\nlanguage constructions\\nhuman environments\\nhuman commands\",\"716\":\"task analysis\\nrobots\\ntraining\\nnatural languages\\nvisualization\\nplanning\\npredictive models\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nnatural language processing\\nrobot programming\\nhigh-level human instructions\\nnatural-language command\\ncrowd-sourcing\\nplan fidelity\\nrepresentations learning\\nrobot agent\",\"717\":\"drones\\nnatural languages\\nvirtual reality\\nrobots\\ntask analysis\\nplanning\\ngrounding\\naerospace computing\\nautonomous aerial vehicles\\ncontrol engineering computing\\nhuman-robot interaction\\nmarkov processes\\nmobile robots\\nnatural language processing\\nuser interfaces\\nnatural language commands\\nmarkov decision process framework\\nweb interface\\nmr interface\\nexploratory user study\\nfully autonomous language grounding\\nautonomous drone\\nnatural language grounding\\ngoal-oriented setting\\nmixed reality\\nradio-controlled controller\\nusers control\",\"718\":\"layout\\ndogs\\nrobots\\ngenerators\\nnatural language processing\\nsemantics\\ntraining\\ndexterous manipulators\\ndiscrete event systems\\nlearning (artificial intelligence)\\nnatural scenes\\ntext analysis\\ninteractive scene generation\\nrobotic drawing\\ndiscrete event system\\nmscoco evaluation dataset\\ncider metric\\nrough-l metric\\nmeteor metric\\namazon mechanical turk\\nnatural language descriptions\",\"719\":\"robots\\ngrounding\\nnatural languages\\ncost function\\nplanning\\ndynamics\\nheuristic algorithms\\ngraph theory\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nnatural language processing\\noptimisation\\npath planning\\nrobot programming\\ndynamic constraint mapping\\nrobot motion planning\\ndynamic grounding graph\\n7-dof fetch robot\\nfactor graph\\noptimization-based motion planning\\nparametric constraints\\nattribute-based natural language instructions\\nmotion plans\",\"720\":\"path planning\\nplanning\\nellipsoids\\nrobots\\ndata structures\\niris\\nconvex programming\\ngraph theory\\nmobile robots\\nrandom processes\\ncluttered environment\\ncontiguous free-space partitioning\\nconvex optimization\\nconvex navigable free-spaces\\ncontiguous convex free-spaces\\nrandom-walk based seed generation method\\ncontiguous navigable geometry\\ngraph search problem\\nmultiple query planning algorithm\\nfast path planning\\nundirected graph\\nsafe path planning\",\"721\":\"trajectory\\ncost function\\nrobot kinematics\\nfunction approximation\\nkernel\\nestimation\\ncontrol engineering computing\\nextrapolation\\nlearning (artificial intelligence)\\nrobot programming\\nextrapolated corrections\\ncost functions\\nuser guidance\\nextrapolation problem\\nonline function approximation\\nfunction space\\nnoneuclidean norms\\nrobot learning\",\"722\":\"quaternions\\nrobots\\nstability analysis\\ntrajectory\\ntask analysis\\nacceleration\\nclocks\\nimage motion analysis\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\npath planning\\nposition control\\nrobot programming\\nstability\\norientation motion primitives\\ncomplex robotic trajectories\\ntime series\\ndynamical systems\\nmerging position\\nmerging sequential motion primitives\\npose trajectories\",\"723\":\"grippers\\nrobot sensing systems\\ntask analysis\\nforce\\nmotion segmentation\\nend effectors\\nhaptic interfaces\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\nrobot vision\\nteaching\\nhaptic exploration schemes\\nadaptive task execution\\ncompliant robots\\nkinesthetic teaching\\nprogramming physical interactions\\nforce data\\nforce sensing\\nautonomous exploration strategies\\nobject targeted manner\\nlearning framework\\nadaptive robot\\nsystematically changing environment\\ngenerated behavior\\nhaptic exploration skills\\nrelative manipulation skills\\nmanipulation task\\nadaptive task structure\\nunseen object locations\\nteaching strategy\",\"724\":\"trajectory\\nmathematical model\\ndynamics\\ntask analysis\\nmotion segmentation\\nadaptation models\\nspace exploration\\napproximation theory\\ndexterous manipulators\\nlearning (artificial intelligence)\\nlinear systems\\nmotion control\\npath planning\\nregression analysis\\ntrajectory control\\nclosed form solutions\\ntask generalization\\nphase space analysis\\nmotion trajectories\\nkinematic based task\\ndynamic motion\\nphase space model\\nsequential trajectory task\\nenergy-based analysis\\nlinear time invariant equations\\ntrajectory segments\\nlinear piece-wise regression method\",\"725\":\"rain\\nimage segmentation\\nlenses\\ntask analysis\\ncomputational modeling\\nroads\\ngenerators\\ndrops\\nimage denoising\\nimage restoration\\nlearning (artificial intelligence)\\nstereo image processing\\ntraffic engineering computing\\nrealrain dataset\\nimage reconstruction\\ncomputer-generated adherent water droplets\\nstreaks\\ncamvid road marking segmentation dataset\\ncityscapes semantic segmentation datasets\\nstereo dataset\\ndenoising generator training\\nde-raining\\nlens\",\"726\":\"semantics\\ntraining\\nc++ languages\\ntools\\nrobot sensing systems\\nhardware\\ncomputer vision\\ncontrol engineering computing\\nconvolutional neural nets\\nimage segmentation\\nlearning (artificial intelligence)\\npublic domain software\\nrobot vision\\ndeployment interface\\nexisting robotics codebase\\nlabel prediction\\nopen-source training\\ncnns\\nsemantic segmentation labels each pixel\\nclass label\\nconvolutional neural networks\\nhigh-quality open-source frameworks\\nneural network implementation\\nsemantic segmentation task\\nmodular approach\\nrobotic platform\\ncnn approach\\nrobotics research\\nopen-source codebase\\nsemantic segmentation cnn\\nsemantic annotation\\nbonnet framework\\npython\\ntensorflow\\nc++ library\",\"727\":\"task analysis\\nsemantics\\nestimation\\nreal-time systems\\nadaptation models\\nimage segmentation\\nrobots\\ncomputer vision\\nimage annotation\\nimage reconstruction\\nlearning (artificial intelligence)\\ndepth estimation\\nasymmetric annotations\\ndeep learning models\\nsensory information extractors\\ngeneric gpu cards\\nasymmetric datasets\\nreal-time semantic segmentation network\\nfloating point operations\\nhard knowledge distillation\\ndense 3d semantic reconstruction\",\"728\":\"three-dimensional displays\\ncameras\\nsimultaneous localization and mapping\\nsemantics\\ntwo dimensional displays\\nvisualization\\ntask analysis\\nfeature extraction\\nimage representation\\nobject detection\\npose estimation\\nrobot vision\\nslam (robots)\\nsemantic mapping\\nview-invariant relocalization\\naccurate local tracking\\nview-invariant object-driven relocalization\\nsampling-based approach\\n2d bounding box object detections\\nview-invariant representation\\ncamera relocalization\\nview-invariance\\nrelocalization rate\\nvisual simultaneous localization and mapping\\nobject landmarks\\nlocal appearance-based features\\nslam\\n3d pose\\nsift\\nmean rotational error\",\"729\":\"visualization\\ntarget tracking\\nimage segmentation\\nmotion segmentation\\ntrajectory\\nuncertainty\\nbiology computing\\nbotany\\ncellular biophysics\\nmicromanipulators\\nneedles\\nrobot vision\\nplant cell manipulation\\nplant cell centroids\\nautomatic targeting\\nrobust scene-adaptive tracking\\ncell segmentation method\\nplant cell detection\\nplant cell localization\\ntemplate tracking\\nmanipulator trajectory\\nscore-based normalized weighted averaging\\nmicroneedle tracking\\nvisual tracking\\nplant biology\",\"730\":\"simultaneous localization and mapping\\nsemantics\\nimage reconstruction\\nreal-time systems\\ncameras\\nthree-dimensional displays\\nconvolutional neural nets\\nfeature extraction\\nlearning (artificial intelligence)\\nmobile robots\\nobject detection\\nrobot vision\\nslam (robots)\\nmobile robotics\\nsparse point-based slam methods\\ncnn-based plane detector\\nsemantic slam\\ncamera localization\\ndeep-learned object detector\\ncnn network\\nsemantic objects representation\\nmonocular object-model aware sparse slam framework\",\"731\":\"semantics\\nprobabilistic logic\\nlabeling\\ntwo dimensional displays\\ncameras\\ntracking loops\\ntrajectory\\nconvolutional neural nets\\nimage coding\\nimage recognition\\nimage reconstruction\\nimage representation\\nimage retrieval\\nimage segmentation\\nobject detection\\nprobability\\nrobot vision\\nslam (robots)\\nconvolutional neural networks\\ngeometric quality\\nprobabilistic projective association\\nloop frames\\n2d labeling\\ncnn\\nsemantic prediction\\nsimultaneous localization and mapping system\\nslam system\\n3d scenes\\nrandomized ferns\\nsemantic recognition\\ngeometric reconstruction\\nloop detection\\nreconstruction pipeline\\nsemantic information\\ncamera trajectory estimation\\nsemantic labels\\nreal-time dense mapping system\\ndense reconstruction\\nsemantic guided relocalization\",\"732\":\"testing\\nfeature extraction\\nindexes\\ntask analysis\\ntrajectory\\nvisualization\\nrobots\\nimage filtering\\nimage matching\\nimage recognition\\nimage resolution\\nimage sampling\\nimage sequences\\nmobile robots\\nnavigation\\nobject recognition\\nparticle filtering (numerical methods)\\npath planning\\nrobot vision\\nslam (robots)\\nglobal visual place recognition method\\nmultiresolution sampling\\nparticle filter-based global sampling scheme\\nmatching efficiency\\nbrute-force sequential matching method\\nlong-term localization\\nlong-term visual navigation tasks\\nloop closure detection\\nmrs-vpr\\nseqslam\",\"733\":\"hidden markov models\\nthree-dimensional displays\\ncloud computing\\nprobabilistic logic\\nrobot sensing systems\\nsolid modeling\\nmeasurement\\nfeature extraction\\nimage registration\\niterative methods\\nmarkov processes\\nstereo image processing\\nhidden markov random field model\\niterative closest point algorithm\\noutlier rejection\\n3d point cloud registration\",\"734\":\"power capacitors\\nmathematical model\\nrobots\\napproximation algorithms\\nmarkov processes\\nprocess control\\nnavigation\\ndecision making\\nnonlinear programming\\ncontroller family policy\\nfinite state controller\\ngeneralized controller policies\\npomdp model\\ncustomized pomdp policy form\\nbelief-integrated fsc\\nvanilla fsc-based policy form\\npomdp robotic solutions\\ngeneralized controllers\\npomdp decision-making\\ngeneral policy formulation\\npartially observable markov decision processes\",\"735\":\"aerospace electronics\\nrobot kinematics\\ntask analysis\\ntrajectory\\nmathematical model\\nvoltage control\\niterative methods\\nlearning (artificial intelligence)\\noptimal control\\nstate-space methods\\ncontinuous action\\nstate space controllers\\ngoal space\\noptimal value functions\\nnonparametric estimators\\nmultiple arbitrary goals\\nreal-world voltage controlled robot\\nnonobservable cartesian task space\\nmultigoal\\nmodel-free reinforcement learning algorithm\",\"736\":\"planning\\ncurrent measurement\\nrobots\\nuncertainty\\nhistory\\ntime measurement\\nlinear programming\\ncomputational complexity\\nmobile robots\\npath planning\\nstatistical analysis\\nrobotics replanning\\nstatistical simulation\\nincremental expectation calculations\\nplanning session\\nbelief space planning\\nix-bsp\",\"737\":\"legged locomotion\\nrobot sensing systems\\ncomputer architecture\\nconvolution\\nforce\\nfoot\\ncontrol engineering computing\\nhaptic interfaces\\nimage classification\\nlearning (artificial intelligence)\\npattern clustering\\nrobot vision\\nterrain mapping\\nhaptic sensing\\nmobile robots\\nreal-world outdoors applications\\nrobot control\\noptimal terrain negotiation\\nterrain classification\\nterrain identification\\nlegged robot foot\\nfixed-length step\\ncontrolled environment\\nclustering method\\nrobot perception\\nmachine learning\",\"738\":\"task analysis\\nuncertainty\\nsearch problems\\nrobot sensing systems\\nplanning\\nobject oriented modeling\\nmarkov processes\\nmobile robots\\nmonte carlo methods\\npath planning\\nobject-oriented pomdps\\noo-pomdp\\nobservable markov decision process\\nobject-oriented partially observable monte-carlo planning\\nmultiobject search task\\nsequential decision making\\nmobile robot\",\"739\":\"training\\nestimation\\nthree-dimensional displays\\ndata models\\nfats\\ncameras\\nrobots\\nimage colour analysis\\nlearning (artificial intelligence)\\nstereo image processing\\ndepth generation network\\nreal world depth\\ndense depth estimation\\ndeep-learning technique\\nstereo rgb images\\nstereo pairs\\ndepth ground-truth\\nstereo setting parameters\\nimage pairs\\nsupervision learning\\nsynthetic depth maps\\nrelative dense depth\\nstereo geometric settings\\noptic settings\\nepipolar geometric cues\\ndgn\\nfalling things dataset\\nvariational method\",\"740\":\"training\\npose estimation\\nfeature extraction\\ntask analysis\\nimage segmentation\\nrobots\\nsolid modeling\\nimage colour analysis\\nimage matching\\nimage sampling\\nobject detection\\nmultitask template matching\\ncolor images\\ntarget object\\nsegmentation masks\\nobject region\\ntexture-less objects\",\"741\":\"task analysis\\nmotion segmentation\\nprotocols\\nelbow\\nwrist\\nprosthetics\\ntrajectory\\nimage motion analysis\\nmotion average\\nrecorded motions\\non-table motion\\nclustering methodology\\nk-medoids clustering\\nclustering approach\\nnaturalistic human arm motions\\nclustering techniques\\nheuristic interpretation\\nunsupervised approach\\nhierarchical description\\nnatural human motion\\ntask achievement\\nsemiautonomous prosthetic device applications\\nmotion segments\\ndtw barycenter averaging\",\"742\":\"robot sensing systems\\nbelief propagation\\npose estimation\\nthree-dimensional displays\\nquaternions\\nkinematics\\nbelief networks\\nindustrial manipulators\\nmarkov processes\\nmessage passing\\nmobile robots\\nrandom processes\\nrobot vision\\nuncertainty handling\\narticulated objects\\narticulation constraint\\ncontinuous pose variable\\nrobot perception\\npmpnbp\\npull message passing algorithm for nonparametric belief propagation\\nhidden node model\\npairwise markov random field\\npairwise mrf\\nobject-part pose beliefs\\n3d sensor data\\ngeometrical models\\nnonparametric belief propagation algorithm\\nmultimodal uncertainty\\nperception problem\\nhigh-dimensional continuous space\\nhuman environments\\nfactored pose estimation\",\"743\":\"pose estimation\\ncameras\\npredictive models\\nthree-dimensional displays\\nrobot vision systems\\ntask analysis\\nimage sequences\\nimage texture\\nneural nets\\ndomain randomization\\nactive pose estimation\\nrobotic control\\nrobotic manipulation tasks\\nrobot trains\\ndomain-randomized simulation\",\"744\":\"image segmentation\\ngrippers\\ngrasping\\nmotion segmentation\\nimage color analysis\\ntask analysis\\nimage fusion\\nimage matching\\nlearning (artificial intelligence)\\nobject detection\\ndeep learning\\nmultimodal grippers\\nsimultaneous pinch\\nmultimodal grasp fusion\\nobject-class-agnostic grasp\\nmodality detection\\nobject-class-agnostic instance segmentation\\ngrasp template matching\\nobject manipulation\\nobject geometry\\ngrasp modalities\\ninstance segmentation\\nintegrated system\",\"745\":\"task analysis\\ntrajectory\\noptimization\\nbayes methods\\nrobot kinematics\\nentropy\\nlearning (artificial intelligence)\\nrobots\\nsearch problems\\ntruly complex tasks\\nlocally learned policies\\ndata-efficient learning\\nparametric context space\\ncontextual policy representation\\ntarget contexts\\ntask objectives\\ntarget position\\nenvironment contexts\\ncontextual policy search algorithms\\nbayesian optimization approach\\nactive learning settings\\nfaster learning\\nfactored contextual policy search\\nscarce data\\ntask contexts\",\"746\":\"splines (mathematics)\\nroads\\nobject oriented modeling\\nautomobiles\\nobject detection\\nneural networks\\ntwo dimensional displays\\ncomputer vision\\nneural nets\\nprobability\\nsynthetic sdr data\\nstructured domain randomization\\ncontext-aware synthetic data\\nuniform probability distribution\\nsdr places objects\\nprobability distributions\\nsdr-generated imagery\\n2d bounding box car detection\",\"747\":\"search problems\\ntraining\\nrobots\\ntask analysis\\nprobabilistic logic\\nclutter\\nuncertainty\\ngaussian processes\\ngraph theory\\ngrippers\\nimage filtering\\nlearning (artificial intelligence)\\nprobability\\nrobot vision\\ncomplex state-action space\\ngaussian process active filtering strategy\\nobject search\\nstate dynamics\\nobject search problem\\nlarge-scale model\\nheavy occlusions\\nprobabilistic active filtering\",\"748\":\"three-dimensional displays\\ndeep learning\\nfeature extraction\\nsolid modeling\\nsensors\\nrobots\\nconvolution\\nconvolutional neural nets\\ngraph theory\\nimage classification\\nimage matching\\nimage reconstruction\\nlearning (artificial intelligence)\\nobject classification\\nvital semantic information\\nhigh-level tasks\\npoint pair features\\nmodern deep learning methods\\ndiscriminative features\\ngraph convolutional networks\\nstanford 3d indoor dataset\\nrobust 3d object classification\",\"749\":\"three-dimensional displays\\ntwo dimensional displays\\ntask analysis\\nfeature extraction\\nrobots\\ndeep learning\\ncomputer architecture\\nimage recognition\\nlearning (artificial intelligence)\\ndiscrete rotation equivariance\\npoint cloud recognition\\npoint clouds\\ndeep networks\\ndeep learning architecture\\nrotation group\\nrotated inputs\\npoint cloud based networks\",\"750\":\"three-dimensional displays\\ntwo dimensional displays\\nfeature extraction\\nlaser radar\\nobject detection\\nproposals\\nfuses\\ncameras\\nimage colour analysis\\nimage fusion\\nlearning (artificial intelligence)\\nneural nets\\nstereo image processing\\nmvx-net\\nmultimodal voxelnet\\n3d object detection\\nneural network architectures\\npoint cloud data\\npoint cloud modalities\\nstate-of-the-art multimodal algorithms\\n3d detection categories\\nsimple single stage network\\nearly-fusion approach\\nvoxelnet architecture\\npointfusion\\nvoxelfusion\\nrgb modalities\\nkitti dataset\\nbirds eye view\",\"751\":\"image segmentation\\ntraining\\nsolid modeling\\nthree-dimensional displays\\nrobots\\ncameras\\ngrasping\\ncad\\nconvolutional neural nets\\nimage coding\\nimage colour analysis\\nimage enhancement\\nimage resolution\\nlearning (artificial intelligence)\\nmasks\\nobject tracking\\ncategory-agnostic instance segmentation\\nhand-labeled data\\nautomated dataset generation\\nnetwork training\\ncomputer vision research\\nrgb imaging\\nsynthetic depth data sensors\\nunknown object segmentation\\nsd mask r-cnn\\nhigh-resolution synthetic depth imaging\\nsynthetic depth mask r-cnn\\nunknown 3d object segmentation\\n3d cad models\\ndomain randomization\\npoint cloud clustering baselines\\ncoco benchmarks\\nhand-labeled rgb datasets\\ninstance-specific grasping pipeline\\nsynthetic training dataset\",\"752\":\"geometry\\nrobot sensing systems\\nthree-dimensional displays\\nshape\\ntraining\\ngrasping\\ncomputational geometry\\ncontrol engineering computing\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nmanipulators\\nsolid modelling\\nmultimodal geometric learning\\nrobotic manipulation tasks\\n3d convolutional neural network\\ncaptured depth information\\nobject geometry\\nvisual-tactile approaches\\n3d models\\ntactile information\\ngeometric prediction\\ngeometric reasoning\",\"753\":\"robots\\nfasteners\\nwheels\\nlead\\ncleaning\\ncouplings\\nmeters\\ndesign engineering\\nhygiene\\nmobile robots\\nmotion control\\npedestrians\\nroads\\nrobot kinematics\\nservice robots\\nshafts\\nsteering systems\\npanthera\\nurban hygiene\\nsingle lead screw shaft\\nlinkages mechanism\\nreconfigurable pavement sweeping robot design\\npavement cleaning robot\\npedestrian\\nin-wheels motors\\nsteering kinematics\\nonboard batteries\",\"754\":\"legged locomotion\\nrobot sensing systems\\nmaintenance engineering\\nfabrication\\npermanent magnet motors\\nthree-dimensional displays\\nfault tolerance\\nmicroprocessor chips\\nautomatic leg regeneration\\nrobot mobility recovery\\nautomatic repair\\nrobotic system\\nmodular approach\\nrobot functionality\\nrobot structure regeneration\\nleg fabrication\\nmechanical structures\\nregeneration-based approach\\nlegged robot disengagement\",\"755\":\"robots\\nmathematical model\\nkinematics\\ntools\\nanalytical models\\ncalibration\\nthree-dimensional displays\\nindustrial robots\\nrobot kinematics\\nserial-link robot\\nhelical joints\\nrevolute joints\\ngeneral joints\\nlow pair joints\\ndegree of freedom\\ndenavit-hartenberg model\\nprismatic joints\\ngeometric interpretation\\nproduct of exponentials formula\\nkinematic parameters\",\"756\":\"friction\\nload modeling\\ntorque\\nmathematical model\\ntemperature dependence\\nestimation\\nrobots\\ndrives\\ngears\\nsliding friction\\nslip\\ndynamic friction model\\nexternal force estimation\\nstatic friction model\\ngross sliding regime\\nlund grenoble\\ndynamic simulation\\nexternal torque estimation\\ngeneralized-maxwell-slip\\nharmonic drive csd 25 gear\\ntest-bed\\nfriction compensation\\nthermal-load dependency\\nnonlinear temperature dependency\\nnonlinear velocity dependency\",\"757\":\"shape\\nactuators\\njamming\\nforce\\nmuscles\\nsoft robotics\\nbiomechanics\\ndesign engineering\\ndexterous manipulators\\nelastic constants\\nelastomers\\ngrippers\\nmobile robots\\nmuscle\\nrobot dynamics\\nrobot kinematics\\nstructural engineering\\nconnective tissue\\ninterossicular muscle\\nsoft material robots\\nstiffness modulation method\\nstructural stiffness\\ncalcite ossicles\\nechinoderm inspired soft actuator\\nossicle structure\\nload-bearing capability\\nelastomer\\nfinger-shaped stiffening structure\\nvacuum level\\nrobotic gripper\",\"758\":\"soft robotics\\nfinite element analysis\\nactuators\\nnumerical models\\nanalytical models\\ncontrollability\\ngalerkin method\\nreduced order systems\\nrobots\\nfinite-element method\\nsilicone soft robots\\ndifferential geometric method\\ncontrollable parallel soft robot\\ncontrollability pre-verification\\nemergent research field\\nvariant promising applications\\ndesign soft robots\\npre-checking controllability\\nnumerical design phase\\ntrial-and-error process\",\"759\":\"grippers\\nskin\\nskeleton\\nrubber\\nconnectors\\ngrasping\\nshape\\ncontrol system synthesis\\ndesign engineering\\nmechanical testing\\nmobile robots\\npneumatic actuators\\nrobust control\\nvacuum-driven origami magic-ball\\ndesigning soft grippers\\nsubstantial grasping strength\\nvacuum-driven soft robotic gripper\\nrobustness\\nflexible thin membrane\\nmechanical load tests\\npneumatic pressure\\nfabrication method\",\"760\":\"actuators\\nstrain\\nwindings\\nprototypes\\nlimiting\\nfabrication\\nshafts\\nbending\\ndisplacement measurement\\nelasticity\\nelectroactive polymer actuators\\nfinite element analysis\\nforce measurement\\npneumatic actuators\\nshear deformation\\ntorsion\\nfiber pattern lead\\nazimuthal deformation\\nanisotropically distributed fiber element\\nhyper elastic material\\nazimuthal shear deformation\\nsoft materials\\nelastic inflatable actuators\\nsoft fiber-reinforced rotary pneumatic actuator\\nstructure design\\nfabrication process\\nfem simulation\\nrotation angles\",\"761\":\"actuators\\nfabrication\\ngrippers\\ntask analysis\\nmathematical model\\ngrasping\\nlaser beam cutting\\ncontrol system synthesis\\npneumatic actuators\\ninfora\\nsoft robotics\\ninflatable thin membranes\\nrigid foldable structure\\nhigh stiffness\\ninflatable origami-based actuator\\ntendril-like structure\\ngrasping tasks\",\"762\":\"legged locomotion\\ntrajectory\\nbifurcation\\nnumerical models\\ndynamics\\nrobot kinematics\\ngait analysis\\npendulums\\nreduced order systems\\nrobot dynamics\\nsprings (mechanical)\\ndynamic period-two gait generation\\nhexapod robot\\nfixed-point motion\\nreduced-order model\\nperiod-two dynamic running motion\\nspring-loaded inverted pendulum model\\nstance phases\\nflight phases\\nperiod-two fixed points\\nmotion cycle\\nperiod-two motion trajectories\\nlanding angles\\nr-slip model\",\"763\":\"legged locomotion\\ntrajectory\\ncomputational modeling\\nkinematics\\noptimization\\ntraining\\ncontrol engineering computing\\ngradient methods\\nlearning (artificial intelligence)\\nmotion control\\nprincipal component analysis\\nrobot kinematics\\nrobot programming\\nquadruped locomotion behavior learning\\nsingle gait learning\\nstoch\\npolicy gradient\\npca\\nquadrupedal walking\\nwalking gaits\\nrobust locomotion behaviors\\nd-rl\\ndeep reinforcement learning\\nkmps\\nkinematic motion primitives\",\"764\":\"legged locomotion\\nfoot\\nkinematics\\nrobot kinematics\\ncollision avoidance\\nneural networks\\ncontrol engineering computing\\nconvolutional neural nets\\ngeometry\\nmotion control\\noptimal control\\nrobot dynamics\\nsingle-shot foothold selection\\nconstraint evaluation\\nquadruped locomotion\\noptimal footholds\\nlegged systems\\nswing leg\\nlocal elevation map\\nkinematic constraints\\nconvolutional neural network\\ngeometrical characteristics\",\"765\":\"legged locomotion\\noptimization\\ntorque\\nrobot kinematics\\nhardware\\nactuators\\noptimisation\\nposition control\\nrobot dynamics\\nmit cheetah 3 robot\\noptimized jumping behavior\\nquadruped robots\\nprecise high-frequency tracking controller\\nrobust landing controller\\nrobot body position\\nexperimental validation\\nrobot hardware\\ntrajectory optimization\\nrobot body orientation\",\"766\":\"legged locomotion\\nhip\\nfoot\\ndrag\\nmathematical model\\nactuators\\nenergy consumption\\ngait analysis\\nrobot dynamics\\nfluid interaction mechanics\\ncenter of mass\\ngait stability\\nmud\\nslip runner\\nviscous medium\\nsnow\\nstream banks\\nbeach-head\\ndense fluids\\nshallow fluids\\noutdoor environments\\nunstructured environments\\nlegged robotic platforms\",\"767\":\"planning\\ntrajectory\\nsafety\\ncomputational modeling\\nreal-time systems\\nvehicle dynamics\\nprobabilistic logic\\napproximation theory\\ncollision avoidance\\nmobile robots\\npredictive control\\nprobability\\nreachability analysis\\nrobot dynamics\\nrobust control\\nsafe backward reachable set\\nreal-time simulation\\nsafely probabilistically complete real-time planning\\nmotion planning\\nkinodynamic planners\\na priori unknown\\nstatic environments\\nrobust controller\\nmotion plans\\nrobot operating system software environment\",\"768\":\"task analysis\\noptimization\\nsafety\\nkinematics\\nrobots\\njacobian matrices\\nredundancy\\nmanipulator kinematics\\noptimisation\\nredundant manipulators\\nsafety related tasks\\nset-based task\\nequality tasks\\nset-bases tasks\\noptimization tasks\\nset-based multitask priority framework\\nset-based multitask priority inverse kinematics framework\\nrobot constraint handling\\nredundant structures\\n7dof jaco2 arm\",\"769\":\"robot sensing systems\\ncollision avoidance\\nimpedance\\nsafety\\nforce\\nanalytical models\\nforce control\\nfriction\\nhuman-robot interaction\\nrobot dynamics\\nrobot kinematics\\nstability\\ntransient response\\ncontroller parameters\\nmaximum safe operating velocity\\nlinear model\\n1 dof robotic joint\\ntraditional admittance control law\\nsimple control structure\\ncompliant limb sensing\\nsafe human-robot interactions\\ncontrol methodology\\ncompliant sensor\\nrobot links\\nexisting robots\\nmechanical redesign\\nlinear robot model\\nstability analysis\\nadmittance control law\\ncomparable transient response\\ncontrol structure\",\"770\":\"wheels\\nlegged locomotion\\nhip\\nrobot kinematics\\nbatteries\\ninspection\\nrobot dynamics\\nascento\\njumping robot\\nmobile ground robots\\ncomplex indoor environments\\nmobile robotics\\nindoor inspection tasks\\ncompact wheeled bipedal robot\\nflat terrain\\nmechanical design\",\"771\":\"acceleration\\nmobile robots\\nforce\\nwheels\\ntorque\\nrobot kinematics\\nkalman filters\\nnonlinear filters\\nrobot dynamics\\ntorque control\\nwheel torque commands\\nmotor torque limits\\ninternal control elements\\ndifferentially driven planar robots\\nasymmetrical planar robots\\nlimited motor torques\\nenvironmental forces\\nunscented kalman filter\\nrobot inertia\",\"772\":\"observers\\ntires\\nmobile robots\\nvehicle dynamics\\nwheels\\nnonlinear optics\\nlinear quadratic control\\nnonlinear control systems\\noff-road vehicles\\npath planning\\npredictive control\\nsteering systems\\ntyres\\nopen environments\\nrear contact cornering stiffnesses\\nsoil proprieties\\nsteering angles\\nlqr controller\\nnonlinear tire cornering stiffness observer\\ndouble steering off-road mobile robot\\npath tracking controllers\\nautonomous vehicle\\ndynamic model\\nwheel-ground contact\\nkalman-bucy observer\\nground parameter estimation\",\"773\":\"wheels\\nmanipulator dynamics\\nhumanoid robots\\ntask analysis\\nmobile robots\\nrobot kinematics\\ncontrol system synthesis\\nend effectors\\nmanipulator kinematics\\nmotion control\\nnonlinear control systems\\noptimisation\\npendulums\\nredundant manipulators\\nhigh-level controller plans\\nhierarchical optimization\\nwhole-body control framework\\noptimal participation\\nlow level controller\\ncontrol zero dynamics\\nlow-level controller plans\\nbody joint manipulation\\nwip humanoids\\nwheeled inverted pendulum humanoids\",\"774\":\"pneumatic systems\\nactuators\\njamming\\nshape\\nservice robots\\nmuscles\\nfinite element analysis\\nfriction\\nhuman-robot interaction\\nmicroactuators\\npneumatic actuators\\nactively controlled variable stiffness structure\\ncollaborative robots\\nrobotic structures\\nactuation system\\nactively controlled stiffness structure\\nstructure shape\\nshape morphing\\nmorphed curvature\\ninput actuator pressure\\nstiffness variation range\\nrobotics field\\nlightweight morphing structures\\npneumatic artificial muscles\\nbidirectional morphing\",\"775\":\"actuators\\npistons\\nforce\\nmanipulators\\nhydraulic systems\\nseals\\nbrushless dc motors\\ndesign engineering\\ndexterous manipulators\\ndiaphragms\\nelastomers\\ngears\\ngrippers\\nhydraulic actuators\\nhydrostatics\\nmedical robotics\\nmotion control\\nprosthetics\\nseals (stoppers)\\nstiction\\nremote-direct-drive 2-dof gripper\\nserial-chain motor-driven robotic arms\\npassive compliance\\nremote direct-drive manipulator\\nlow-friction hydrostatic transmission\\nsoft fiber-elastomer rolling-diaphragm seals\\nstatic friction\\nseal rubbing\\ngear ratios\\ndexterous robotic arm\\nfloating-piston hydrostatic linear actuator design\\nbackdrivable brushless electric motors\\nsystem hysteresis\\npowered prosthetic hand design\\nsize 20.0 mm\",\"776\":\"electron tubes\\nmagnetic moments\\nferrofluid\\nactuators\\nmagnetic separation\\nmagnetic resonance imaging\\nthree-dimensional displays\\nmagnetic actuators\\nmagnetic fluids\\nmagnetic particles\\nthree-dimensional printing\\npolymeric matrix\\nactuator response\\n3d printed material\\n3d printed tubes\\n3d printed more complex actuators\\ncomplex motion\\nferrofluid based soft actuators\\n3d printed soft actuators\\ncomplex shapes\\nremote actuation\\nexternal magnetic field\\nferrofluid-based actuator\",\"777\":\"robot kinematics\\nsports\\ntask analysis\\ntraining\\nlegged locomotion\\nintelligent robots\\nlearning (artificial intelligence)\\nmulti-robot systems\\nrobot vision\\nrobot soccer small-size domain\\ntactical level team strategies\\nhigh-level team strategies\\nindividual robot ball-based skills\\nrobot primitive skills\\ncontinuous action space\\nhardware fidelity\\nlearned skills\\nmobile robots\\nhand-coding algorithms\\ntraining parameters\\nmobile robot system\\ndeep reinforcement learning algorithm\\nprimitive skills\\ntask performance\\nlearning algorithms\",\"778\":\"robot sensing systems\\nplanning\\npath planning\\nuncertainty\\ncollision avoidance\\nc++ language\\ncontrol engineering computing\\nlawnmowers\\nmobile robots\\nnavigation\\noperating systems (computers)\\nrobot programming\\ncoverage path planning\\nbelief space\\nrobotic lawn mowers\\nsafety-critical tasks\\nrobot safety\\ncheap range sensors\\nlow range sensors\\nuncertainty-aware coverage path\\nlawn mower\\nsafe navigation\\nros\",\"779\":\"training\\nrobots\\nshape\\ntask analysis\\nadaptation models\\ndecoding\\nmachine learning\\ninteractive systems\\nlearning (artificial intelligence)\\ninteractive learning approach\\ndeep reinforcement learning\\ncorrective advice communicated by humans\\ndrl agent\\nhuman training effort\\nd-coach framework\\nhuman corrective feedback\\nhuman knowledge\\nmachine learning methods\\nreward function\\nsimulated environments\\nrobotics applications\\ncomplex decision-making problems\\nhigh-dimensional state spaces\\ncontinuous control\",\"780\":\"autonomous vehicles\\nroads\\npredictive models\\nreinforcement learning\\ndecision making\\nobject detection\\nimage processing\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nroad traffic\\nroad vehicles\\nrobot vision\\ntraffic engineering computing\\nautonomous driving vehicles\\ndeep predictive network\\npredictive reward function\\nhuman-like driving\\nvehicle control\\ntraffic flow\\noccupancy grid image\\nprediction network training\\nreal driving data\\nreinforcement learning agent training\\ndeep neural networks\\nautonomous driving\\nprediction\\nreward\\ndeep learning\\nnaturalistic driving data\",\"781\":\"task analysis\\ntraining\\naccidents\\nautonomous vehicles\\ntraining data\\ntrajectory\\nlearning systems\\nhierarchical systems\\nremotely operated vehicles\\nroad traffic control\\nrobust control\\nadaps\\nautonomous driving\\nrobust control policy\\nsimulation platforms\\nlearning mechanism\\nhierarchical control policy\\ndagger method\",\"782\":\"videos\\nrobot kinematics\\ncameras\\nrobot vision systems\\nmobile robots\\nobservers\\nfinite automata\\nmulti-robot systems\\nsport\\nstructured narratives\\nautonomous robots\\nrobot teams\\nlarge-scale road race\\nmarathon\\nlegible form\\nweighted finite automaton\\nsimulated race scenario\\ncoordinated event observation planning\",\"783\":\"switches\\nmathematical model\\nmechanical systems\\nrobots\\ndynamics\\nheuristic algorithms\\nplastics\\ncomplementarity\\ndifferential algebraic equations\\nimpact (mechanical)\\niterative methods\\nlegged locomotion\\nplasticity\\nrobot dynamics\\nalgorithmic resolution\\nmultiple impacts\\nnonsmooth mechanical systems\\nswitching constraints\\ndifferential-algebraic formulation\\nnonsmooth dynamics\\nrobotic systems\\nchanging constraints\\nkinematic constraints\\nalgorithmic impact resolution method\\nclassical plastic impact law\\nmultiple simultaneous impacts\\nprior linear-complementarity-based formulations\\nimplicit impact resolution\",\"784\":\"mathematical model\\ndynamics\\nnumerical models\\ntransmission line matrix methods\\nrobots\\nbars\\nsymmetric matrices\\ncomputational geometry\\nmanipulator dynamics\\nmechanical contact\\nshear modulus\\nplanar nonconvex contact patch\\nintermittent contact\\nrigid body dynamic simulation\\nconvex contact patches\\ncontact detection\\ncontacting rigid bodies\\nmultiple point contact\\nsingle point contact\\nrigid body motion prediction\\nconvex hull\",\"785\":\"substrates\\ncomputational modeling\\nforce\\nmedia\\nlegged locomotion\\nfoot\\ncontrol engineering computing\\ndata analysis\\ngranular materials\\nmechanical contact\\noptimisation\\nshear modulus\\nstick-slip\\ndata-driven approach\\nrobot locomotion\\ngranular media\\nsemiempirical approach\\ncontact model\\nstick-slip behavior\\nrigid objects\\ngranular grains\\ngranular substrate\\noptimization-based contact force\\ncontact solver\\ncontact wrenches\\nfast simulation\\nconvex volume\\nfrictional dissipation\\nplausible interaction response\",\"786\":\"aerospace electronics\\nswitches\\nmathematical model\\noptimization\\nlegged locomotion\\ncomputational complexity\\ncontrol system synthesis\\ndiscrete time systems\\nfeedback\\nlinear programming\\nlinear systems\\nmanipulator dynamics\\noptimisation\\npolynomials\\nstability\\ndiscrete-time hybrid polynomial system\\noccupation measures\\ncontroller synthesis\\npolynomial dynamics equation\\nfeedback design\\nrigid body system stabilization\\nstate-input space\\nfinite-dimensional semidefinite programs\\nrobot locomotion\\nrobot manipulation\",\"787\":\"games\\nautomata\\nsurveillance\\ncost function\\npath planning\\npartitioning algorithms\\nautomata theory\\ncomputational complexity\\ncontrol system synthesis\\ndiscrete time systems\\ngame theory\\noptimal control\\noptimisation\\ntrajectory control\\noptimal path planning\\n\\u03c9-regular objective\\nabstraction-refinement based framework\\noptimal controller synthesis\\ndiscrete-time concrete system\\nfinite weighted transition system\\noptimal abstract controller\\nformal controller synthesis algorithms\\nrobot surveillance scenario\\nb\\u00fcchi automaton\",\"788\":\"trajectory\\nrobots\\nprogramming\\nintegrated circuit modeling\\noptimization\\noptimal control\\ncomputational modeling\\napproximation theory\\nclosed loop systems\\ncontrol system synthesis\\nconvex programming\\ndiscrete time systems\\nfeedback\\ninteger programming\\nlinear quadratic control\\nlyapunov methods\\nnonlinear control systems\\npiecewise linear techniques\\npredictive control\\nstability\\npiecewise affine systems\\ncontact dynamics\\nrobot locomotion\\ncontrol techniques\\nfeedback control policies\\ndiscrete-time pwa systems\\nclosed-loop trajectories\\ncost function\\nlqr-trees\\nopen-loop trajectory optimization\\npwa dynamics\\ncontact-based dynamics\\nsampling-based polytopic trees\\napproximate optimal control\",\"789\":\"optimal control\\nreachability analysis\\ntools\\nneural networks\\nsafety\\nsystem dynamics\\napproximation theory\\ncomputational complexity\\ncontrollability\\nnonlinear control systems\\npattern classification\\ngoal satisfaction\\nsafety verification\\nnonlinear systems\\nrestrictive problem classes\\noptimal controller\\nhj reachability problem\\ncontrol-affine systems\\ndynamical systems\\nreachability value function\\nclassification-based approach\\napproximate reachability\\nhamilton-jacobi reachability analysis\\nsimple binary classifiers\\ngrid-based methodologies\\nphysical quadrotor navigation task\",\"790\":\"wind turbines\\nblades\\ndrones\\ninspection\\npoles and towers\\ncameras\\nthree-dimensional displays\\nconvolutional neural nets\\nglobal positioning system\\ngraph theory\\nimage matching\\nimage representation\\ninertial navigation\\nmobile robots\\nobject tracking\\npose estimation\\nrobot vision\\nstereo image processing\\nimage-based measurements\\ndrone navigation system\\nautomated inspection\\n3d skeleton representation\\nimage data\\nconvolutional neural network\\ngeneric turbine model\\nturbine shapes\\nimage measurements\\ndrone localisation\\nmonocular model-based tracking\\npose graph optimiser\",\"791\":\"robot sensing systems\\ninterpolation\\ngaussian processes\\ndispersion\\nnoise measurement\\nneural networks\\nair pollution\\nair quality\\nautonomous aerial vehicles\\nenvironmental monitoring (geophysics)\\nmobile robots\\nmonte carlo methods\\nregression analysis\\npoint measurements\\nautonomous robots\\nmapping algorithms\\npiecewise linear interpolation\\nsteady state ground truth\\nunmanned aerial vehicle\\ngaussian process regression\\npolynomial interpolation\\nplume mapping\",\"792\":\"training\\nfuzzy logic\\nunmanned aerial vehicles\\ntrajectory\\nreal-time systems\\ntrajectory tracking\\nautonomous aerial vehicles\\nlearning (artificial intelligence)\\nneurocontrollers\\ntrajectory control\\ninput-output dataset\\ndeep neural network-based controller\\ntrained dnn\\nexpert knowledge\\nlearning-based approach\\ntrajectory tracking performance\\nonline deep learning\\nonline learning-based control method\",\"793\":\"robots\\npayloads\\nsprings\\nforce\\ntask analysis\\nshock absorbers\\npython\\nautonomous aerial vehicles\\ndecentralised control\\nmicrorobots\\nmobile robots\\nremotely operated vehicles\\nmicrouav\\nsmall unmanned aerial vehicles\\nbuzz swarm-specific scripting language\\nfully decentralized control infrastructure\\ntask demands\\nunstructured environments\\nmaximum flexibility\\njoint payload capacity\\ndecentralized collaborative transport\",\"794\":\"bars\\nrobots\\nacceleration\\naerodynamics\\nharmonic analysis\\ncouplings\\nkinematics\\naerospace components\\naircraft control\\nautonomous aerial vehicles\\ncascade control\\ncompensation\\nmobile robots\\nrobot dynamics\\nrobot kinematics\\ntorque control\\ntrajectory control\\nrobotic hummingbird project\\nflapping mechanism\\nwing trajectory\\ncascade control strategy\\nprecision stationary flight\\nresidual parasitic torques compensation\\nlift vector\\nautopilot\",\"795\":\"quaternions\\nestimation\\ncovariance matrices\\nmagnetometers\\nkalman filters\\naccelerometers\\nmathematical model\\nattitude control\\nattitude measurement\\nmanipulators\\nnonlinear filters\\nrobust control\\nukf innovation\\nadaptive strategy\\nmeasurement covariance matrix online\\noutlier detection\\nrobust attitude estimation\\nstandard ukf\\nunit quaternion algebra\\noutlier detector algorithm\\nrobust adaptive unscented kalman filter\",\"796\":\"task analysis\\nreinforcement learning\\nneural networks\\ntraining\\nrobot sensing systems\\ntraining data\\ninference mechanisms\\nlearning (artificial intelligence)\\nvideo signal processing\\none-shot learning\\nmultistep tasks\\nactivity localization\\naction policies learning\\nreward functions inference\\nuser-segmented demonstration\\nauxiliary video data\",\"797\":\"humanoid robots\\ntrajectory optimization\\nneural networks\\nforce\\ntraining\\nconcave programming\\nfeedback\\ninteger programming\\nlearning (artificial intelligence)\\nmechanical contact\\nmobile robots\\nneurocontrollers\\noptimal control\\npredictive control\\nrobot dynamics\\ntree searching\\nlvis\\ncontact-aware robot controllers\\nguided policy search\\nhigh-dimensional systems\\nnonconvex trajectory optimization\\nlocal minima\\noptimal policy\\nindependently-optimized samples\\noptimal value function\\nmixed-integer programs\\nglobal optimality\\ninterval samples\\nterminal cost\\nfeedback control\\nlearning from value function intervals\\ncontroller training\\nglobal mixed-integer optimization\\nnonuniqueness issue\\nbranch-and-bound algorithm\\nneural net training\\nlearned cost-to-go\\none-step model-predictive controller\\npiecewise affine models\\ncart-pole system\\nplanar humanoid robot\",\"798\":\"robots\\ntrajectory\\ntask analysis\\nliquids\\nforce measurement\\ngrippers\\ndynamics\\ngaussian processes\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmanipulators\\nmixture models\\nmobile robots\\nteaching\\naction model learning\\nnongeometric features\\nmanipulation actions\\naction-induced reactions\\nmeasured liquid levels\\nexplicit case dependent programming\\nexternal features\\ndynamic system\\naction imitation\\ngeometric trajectory\\nreal-world robot experiments\\ngaussian mixture model representation\",\"799\":\"trajectory\\nlaplace equations\\ncost function\\ntask analysis\\nencoding\\nrobots\\nconvex programming\\nintelligent robots\\nlearning (artificial intelligence)\\nlearning framework\\nmccb\\npoint-to-point movement skills\\nmultiple differential coordinates\\nlocal geometric properties\\nconvex optimization problem\\nmulticoordinate cost function\\ncomplex skill datasets\\nskill acquisition\\nautomated multicoordinate cost balancing\",\"800\":\"robot sensing systems\\nhidden markov models\\ntrajectory\\nadaptation models\\nreal-time systems\\ndata models\\nhaptic interfaces\\nlearning (artificial intelligence)\\nmanipulators\\nrobot vision\\nadaptive object manipulation\\nrmac\\nmultisensory inputs\\nreal-time multisensory affordance-based control\\naffordance models\",\"801\":\"task analysis\\ndecision trees\\nservice robots\\nhidden markov models\\nreal-time systems\\ngames\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nrobot programming\\nlfd methods\\nhousehold cleaning task\\nrobotic learning from demonstration\\nprimitive actions\\nfetch robot\\nhuman teaching\\nbehavior trees\",\"802\":\"task analysis\\nhidden markov models\\ncognition\\ncontext modeling\\nrobots\\nalgebra\\nprobabilistic logic\\ngraph theory\\nlearning by example\\nprobability\\ntemporal reasoning\\npolicy selection\\ntemporal reasoning model\\nallen's interval algebra\\nsequential relations\\nparallel temporal relations\\nprobabilistic inference\\ntemporal context graph\\nlearning from demonstration\\nperceptual aliasing\",\"803\":\"trajectory\\ntask analysis\\nrobot sensing systems\\nsearch problems\\nsockets\\nimpedance\\ncollision avoidance\\nend effectors\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmobile robots\\nmotion control\\nposition control\\nrobot vision\\nalignment tasks\\nhuman demonstrations\\nstate invariant dynamics model\\nexploration distribution\\nsearch trajectory\\ndeterministic ergodic control\\nposition domains\\nsuperposed forces\\nlearnt strategy\\n3d electricity socket task\\nsearch task\\nhuman search strategies\",\"804\":\"end effectors\\nsurface treatment\\nrobot sensing systems\\nshape\\ngaussian processes\\nentropy\\nmobile robots\\npath planning\\ntactile sensors\\ntouch (physiological)\\nactive multicontact continuous tactile exploration\\ngaussian process differential entropy\\nactive tactile exploration framework\\nexploration strategy\\ninformation theoretic context\\nnonmyopic multistep planning\\nend-effectors\\ntactile stimuli\\ncompliant controller framework\\ntactile exploration approach\\nnonconvex objects\\ngaussian process implicit surface model\\nsliding based tactile exploration\",\"805\":\"neural networks\\ntrajectory\\nrobots\\nrobustness\\nreinforcement learning\\nspace exploration\\ntraining\\nlearning (artificial intelligence)\\nmanipulators\\nneural nets\\nsearch problems\\ntrajectory control\\nrobust manipulation skills\\ncontrol policies\\ncomplex manipulation tasks\\nhigh-dimensional neural networks\\nrobot actions\\nreal-world trajectory samples\\nresulting neural networks\\npolicy representation\\nrobust actions\\nbroader state space\\nstate-dependent motor reflex\\nsimilar motor reflexes\\nreal-world manipulation tasks\\nguided policy search\\ngenerative motor reflexes map states\\nstate-action policies\",\"806\":\"task analysis\\nhidden markov models\\nmixture models\\npredictive models\\ndata models\\ntrajectory\\nadaptation models\\nassembling\\nindustrial robots\\nlearning (artificial intelligence)\\nmobile robots\\nmotion control\\nproduct quality\\nproductivity\\nspatial-temporal features\\nhuman motion patterns\\ncollaborative robot\\nassembly lines\\nincremental learning system\\nplanning motion\\nproducts quality\\nworkers behavior\",\"807\":\"task analysis\\nplanning\\nrobots\\nskeleton\\nsearch problems\\ncompanies\\nneural networks\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\npath planning\\nmultiobject manipulation problems\\ncontinuous state\\ncontinuous operator parameters\\nstate description\\ndiscrete parameters\\nsingle specializer\\nmodular meta-learning approach\\naction spaces\\n3d pick-and-place tasks\",\"808\":\"haptic interfaces\\nrobot sensing systems\\nvisualization\\nconvolution\\nnetwork architecture\\nneural networks\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nobject recognition\\nrecurrent neural nets\\nrobot vision\\nmultisensory object category recognition\\ninteractive behavioral exploration\\ndeep learning methodology\\nvisual data\\nhaptic sensory data\\nhaptic data\\nauditory data\\nsensory modality\\nvisual information\\ndominant modality\\nauditory networks\\nconvolutional neural networks\\nhaptic networks\\ntensor-train gated recurrent unit network\\nrobot category recognition\",\"809\":\"trajectory\\ntraining\\noptimal control\\nneural networks\\noptimization\\nstandards\\npredictive models\\napproximation theory\\nfunction approximation\\niterative methods\\nlearning (artificial intelligence)\\nneural nets\\nnonlinear control systems\\noptimisation\\ndiscontinuity-sensitive optimal control learning\\nmachine learning method\\nparametric input\\nproblem parameters\\noptimal solutions\\nproblem-optimum map\\ndiscrete homotopy classes\\ncontrol switching\\nmoe\\nstandard neural networks\\ndynamic vehicle control problems\\nnonlinear optimal control problems\\nfunction approximators\\nmixture of experts model\\ntrajectory prediction\",\"810\":\"detectors\\ntask analysis\\nrobot sensing systems\\ncameras\\nlighting\\ntraining\\nmutual information\\nimage colour analysis\\nimage sensors\\ninfrared detectors\\nlearning (artificial intelligence)\\nobject detection\\nobject detector\\ninvariance properties\\nvisible-light rgb camera\\ninfrared sensor\\ntemporary sensor\\ninfrared detector\\nrgb-inferred labels\\ninfrared-inferred labels\\ntransfer learning\\nwormhole learning\\npretrained rgb detector\",\"811\":\"muscles\\ntask analysis\\nelectromyography\\nrobot kinematics\\nsignal processing algorithms\\npipelines\\ncontrol engineering computing\\ngroupware\\nhuman computer interaction\\nhuman-robot interaction\\nlifting\\nmuscle\\nneural nets\\nrobotic assembly\\nsignal detection\\nhuman-robot team lifting\\nmuscle activity\\nsurface electromyography\\nmuscle signals\\ncontinuous setpoint algorithm\\nbiceps activity\\ntriceps activity\\nassembly tasks\\nphysical human-robot collaboration\\nsurface emg\\nneural network\",\"812\":\"instruments\\nhysteresis motors\\nhysteresis\\ntraining\\nkinematics\\nrobot sensing systems\\ncables (mechanical)\\ncontrol engineering computing\\nendoscopes\\nflexible manipulators\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nmedical computing\\nmedical robotics\\nposition control\\nrobot kinematics\\nsurgery\\nmedical cable-driven flexible instruments\\nmachine learning\\nkinematic analysis\\ncable transmissions\\nmedical endoscopic systems\\nflexible medical robotic systems\\nopen-loop accuracy\\nposition inverse kinematic model\\noff-line learning\\nlearning process\\nstras medical robot\\nkinematic models\\nhysteresis effects\",\"813\":\"collision avoidance\\ntrajectory\\nknee\\nprosthetics\\nlegged locomotion\\nsensitivity\\nfeature extraction\\nadaptive control\\nkinematics\\nmedical control systems\\nregression analysis\\nregression model\\nkinematic data\\nobstacle avoidance system\\nobstacle avoidance success rates\\nprosthetic limb\\nadaptive system\\npowered prosthetic limbs\\nstumble recovery systems\\npowered prostheses\\ndirect knee control\\nmechanically-passive transfemoral prosthetic limbs\\npowered transfemoral prostheses\\nproactive obstacle avoidance\\nonline learning\\namputee subject\\nobstacle negotiation success rate\\ntrip avoidance system\\nnonamputee subject\",\"814\":\"dynamics\\nlegged locomotion\\ngravity\\nmanipulator dynamics\\nkinematics\\nrocks\\nend effectors\\ngait analysis\\nmanipulator kinematics\\nmotion control\\npath planning\\npassive dynamic object locomotion\\nwalking manipulation\\nzigzag path\\nrocking motion\\nmanipulator arm\\nrobotic manipulation technique\\nrock-and-walk object transportation technique\\nsimple end-effector\\nstable gait\\nrock-walk object locomotion\\ngravity force\",\"815\":\"robots\\nboats\\nsockets\\nlatches\\ncomputer interfaces\\nthree-dimensional displays\\nlaser beams\\nmobile robots\\nmotion control\\nmulti-robot systems\\nrobot vision\\nrobotic assembly\\nautonomous latching system\\nautonomous robotic boats\\nmultiple boats\\ndynamic united floating infrastructure\\nlatching mechanism\\nvision-based robot controller\\ndocking\\nrotation movement\\nself-driving robotic boats\",\"816\":\"robot sensing systems\\nbandwidth\\noceans\\ndata models\\nvisualization\\nbayes methods\\nautonomous underwater vehicles\\ngeophysical image processing\\nimage representation\\nobject detection\\noceanographic techniques\\nprobability\\nrobot vision\\nslam (robots)\\nunsupervised learning\\nco-robotic exploration\\nbandwidth tunable technique\\nreal-time probabilistic scene modeling\\ncommunication constrained environments\\ndeep sea\\nscene complexity\\nbandwidth requirements\\nunderwater robot\\nhigh-level semantic scene constructs\\nartificially constructed tank environment\\nscience interests\\nunsupervised scene model impact\\nresulting scene model\\ncoral reef\\nbandwidth constraints\\nscene maps streaming\",\"817\":\"image color analysis\\nestimation\\nimage restoration\\ncameras\\nattenuation\\nstereo vision\\ndeep learning\\ncomputerised instrumentation\\nfeature extraction\\ngeophysical image processing\\nimage colour analysis\\nimage matching\\nimage resolution\\nlight propagation\\nneural nets\\noceanographic techniques\\nspatial variables measurement\\nstereo image processing\\nunsupervised learning\\nvisual perception\\nstereo cameras\\nnavigation\\nunderwater robotic systems\\nconstrained camera geometry\\nfeature detection\\nunderwater light propagation lead\\nunderwater image restoration\\nunsupervised deep neural network\\ninput raw color underwater stereo imagery\\ncolor corrected imagery\\nunderwater image formation\\nimage processing techniques\\ndepth estimation\\nstereo vision algorithms\\ndisparity estimation\\ndnn\",\"818\":\"boats\\nfasteners\\ntesting\\nrails\\nsea state\\nkinematics\\nunmanned aerial vehicles\\nautonomous aerial vehicles\\nmarine safety\\nmotion control\\noptimisation\\npitch control (position)\\nball joint mounting angle\\nexperimental testing\\nboat motion replication\\nparameter optimization\\n3-psr parallel mechanism\\nthree-degree-of-freedom\\nprismatic-spherical-revolute parallel mechanism\\nunmanned aerial vehicle\\nunmanned surface vehicle\\nlookup table\\ngeometric constraints\\nwave replication\\nthree actuated linear rails\\nuav winch payload\\ninertial measurement unit\",\"819\":\"robots\\nvehicle dynamics\\nadaptation models\\ncomputational modeling\\nheuristic algorithms\\nsupport vector machines\\ndata models\\nmarine navigation\\nmobile robots\\nregression analysis\\nrobot dynamics\\ntracking\\nunderwater vehicles\\non-line learning\\nunderwater vehicles dynamic models\\naccurate tracking controllers\\nnavigation algorithms\\nhigh fidelity performance\\nincremental support vector regression method\",\"820\":\"acoustics\\nfeature extraction\\nspeech recognition\\ntraining\\ncomputational modeling\\ncomputer architecture\\nemotion recognition\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nnatural language processing\\npattern classification\\nrecurrent neural nets\\ntext analysis\\nlinguistic modality\\nexpressed emotion\\nspoken text\\nground-truth transcriptions\\nspeech recognition mistakes\\nautomatic speech recognition output\\ncharacter-level recurrent neural network\\nsentiment recognition\\nacoustic modality\\nbinary sentiment classification task\\nsynergistic effect\\nauditory transcribed text\\ntranscribed text\\nsentiment intensity\\nend-to-end speech recognition models\\nsentiment analysis\\nasr systems\\nmultimodal corpus of sentiment intensity\\nmosi\",\"821\":\"estimation\\noptical imaging\\nfeature extraction\\ngesture recognition\\nrobots\\noptical fiber networks\\nhuman-robot interaction\\nneural nets\\nservice robots\\ngesture interaction\\nhuman motion\\ngesture-based human-robot interaction\\nhouse service robot\\npractical robot applications\\ngesture recognition methods\\noptical flow estimation method\\nspeed-accuracy trade-off\\ndeep learning-based methods\\nfeature extractors\\nmidway features\",\"822\":\"neural networks\\npins\\ntask analysis\\ntraining\\ngames\\ncomputer architecture\\ndecoding\\ndecentralised control\\nmulti-agent systems\\nmulti-robot systems\\nneurocontrollers\\noptimisation\\nmultiagent policies\\nagent architecture\\ntraining methodology\\ntask-oriented communication semantics\\ncommunication-unaware expert policy\\nperimeter defense game\\ncommunication constraints\\ncollaborative tasks\\noptimization\\ndecentralization\",\"823\":\"robots\\nsemantics\\nsyntactics\\nlearning systems\\nlinguistics\\neducation\\ncomputer architecture\\nhuman-robot interaction\\nknowledge acquisition\\nlearning (artificial intelligence)\\nrobot programming\\nhuman-robot dialogues\\nword-object associations\\nhuman-human dialogues\\nrobotic system\\nai agents\\ncross-situational learning\\ninstruction-based word learning\",\"824\":\"task analysis\\ncollaboration\\nlegged locomotion\\ncommunication channels\\nvisualization\\ncomputer science\\nhelmet mounted displays\\nhuman-robot interaction\\nmobile robots\\nobject detection\\ntarget object\\npr2 robot\\nrobot object\\nlegible situated projections\\nreference objects\\ncomplex task-oriented human-robot collaborations\\nrobot-to-human information transfer\\nvisual referencing\\narrow-object match functions\\nhead-mounted projector\",\"825\":\"games\\ntask analysis\\nprotocols\\ngeology\\nstochastic processes\\nsecurity\\nglobal positioning system\\nautonomous aerial vehicles\\ncommand and control systems\\ncontrol engineering computing\\nformal verification\\nlearning (artificial intelligence)\\nmilitary aircraft\\nstochastic games\\nsecurity-aware synthesis\\nhuman-uav protocols\\ncollaboration protocols\\nhuman-unmanned aerial vehicle\\ngeolocation task\\nstochastic game-based model\\nstealthy false-data injection attacks\\ncollected experimental data\\nhuman-uav coalition\\nh-uav protocol synthesis\\nhuman operators\\nuav hidden-information constraint\\nreschu-sa testbed\\ngeolocation strategies\\nmodel checkers\",\"826\":\"robots\\nencoding\\ndecoding\\nthree-dimensional displays\\nvisualization\\nheuristic algorithms\\ntarget tracking\\nconvolutional neural nets\\ngesture recognition\\nlearning (artificial intelligence)\\nmobile robots\\nmulti-robot systems\\nprotocols\\nvariable length codes\\noptimal variable-length prefix codes\\ninterrobot communication\\naction sequences\\nswimming robot\\ncommunication protocol\\nwhole-body gestures\\nradio-denied environments\\npassive communication\\nfull-body gestures\\nunderwater communication\\nrobot gesture execution\\nclassical decoding methods\\nobserver robot\\nconvolutional network\\nnatural activity\",\"827\":\"three-dimensional displays\\nsimultaneous localization and mapping\\nrobot kinematics\\nvisualization\\nmerging\\nleast mean squares methods\\nmobile robots\\npath planning\\npose estimation\\nrobot vision\\nslam (robots)\\nwisdom\\nspatial sensing\\nrobotics\\naugmented reality\\nurban spaces\\nwireless access points\\ncoarse orientation\\naverage root mean square mapping error\\nwireless sensing-assisted distributed online mapping\\nrobot swarm\\ncustom icp algorithm\\nabsolute trajectory error\\nsize 0.2 m\\nsize 1.3 m\",\"828\":\"sensor fusion\\ncomputational modeling\\ngaussian processes\\ntime measurement\\nkinematics\\nvelocity measurement\\nbayes methods\\nlearning (artificial intelligence)\\nrecursive estimation\\nmobile decentralized sensors\\nmultisensor applications\\ngp recursive fusion law\\nrecursive dpgp fusion approach\\ndata fusion\\nrecursive bayesian nonparametric modeling\\ndirichlet process\",\"829\":\"robot sensing systems\\nnavigation\\nthree-dimensional displays\\nunmanned aerial vehicles\\ntrajectory\\nattitude control\\nautonomous aerial vehicles\\ncollision avoidance\\ninertial navigation\\nmobile robots\\noff-road vehicles\\nrobot vision\\nslam (robots)\\nunmanned aerial vehicle\\nunmanned ground vehicle\\ntether attachment device\\nsteep terrain\\ntether anchoring\\nugv autonomous cooperation\\nuav autonomous cooperation\\nvisual inertial navigation\\ncollaborative navigation\\n3d voxel mapping\\nobstacle avoidance planning\\ntraversability analysis\",\"830\":\"trajectory\\nmathematical model\\nrobot sensing systems\\noptimization\\ntomography\\nestimation\\ninverse problems\\nlinear systems\\nmulti-agent systems\\nnonlinear equations\\nnonlinear systems\\noptimisation\\ndistributed nonlinear kaczmarz method\\nconstrained consensus problem\\ngyre flow field\\ndistributed motion tomography\\nmobile sensing agents\\ninverse problem\\ndistributed multiagent systems\\nflow field reconstruction\\nnonlinear system of equations\\noptimization approach\\nlinear system of equations\",\"831\":\"cameras\\nskeleton\\nfeature extraction\\nmonitoring\\nreliability\\ntrajectory\\nrobot sensing systems\\nhuman computer interaction\\nimage colour analysis\\nimage matching\\nimage sensors\\ninternet of things\\nrobot vision\\nvideo signal processing\\nwtw\\nimu data\\ninertial sensor\\nrgb-d camera\\nunmanned monitor\\niot\\nhuman-environment interaction\\nvision-based approaches\\nuser-object matching\\nvideo recorded\",\"832\":\"visualization\\nobject detection\\nrobots\\nfeature extraction\\ntask analysis\\nsemantics\\ndeep learning\\nacoustic signal detection\\naudio signal processing\\nmicrophones\\nmobile robots\\nneural net architecture\\nsource separation\\nsupervised learning\\nrobotic exploration\\ncameras\\nphysical world\\nvisual modalities\\naudio modalities\\nrobotic platforms\\nrobotic sound-indicated visual object detection framework\\ntwo-stream weakly-supervised deep learning architecture\\nsounding object localization\\naudioset\",\"833\":\"safety\\ncloning\\ntraining\\nmeasurement\\ntrajectory\\ntask analysis\\nlearning (artificial intelligence)\\nhg-dagger\\ninteractive imitation learning\\nbehavioral cloning\\ndata mismatch\\ndagger algorithm\\nsampling schemes\\naction labels\\nautonomous driving task\\ncorrective actions\",\"834\":\"robot kinematics\\nrobot sensing systems\\nreal-time systems\\ndrones\\ntrajectory\\nmobile robots\\ngesture recognition\\nhuman-robot interaction\\npointed locations\\nslow ground robots\\nproximity human-robot interaction\\npointing gestures\\nwrist-mounted imu\\nco-located humans\\nmoving robot\",\"835\":\"laser radar\\nlaser beams\\ntwo dimensional displays\\nsensors\\nlaser modes\\nrobots\\ncomputational modeling\\ngaussian distribution\\noptical radar\\nsensor fusion\\nmultiple sensors\\nrobosense rs-lidar-16\\naccurate maps\\nlidar measurement bias estimation\\nreturn waveform modelling\\nzero-mean gaussian distribution\\nlocalisation drifts\\nbias estimation\\nsensor error modelling\\nwaveform modelling\\nlidar\\n3d mapping\",\"836\":\"calibration\\ncameras\\nlaser radar\\ntools\\nrobot vision systems\\ngaussian noise\\noptical radar\\npose estimation\\nprobability\\nradar imaging\\nlidar\\njoint extrinsic calibration\\nsensing modalities\\ncalibration target design\\ncalibration procedure\\nmultimodal measurements\\noptimization criterion\\nerror terms\\nsensor pairs\\ncalibration boards\\ncalibration performance\\ncamera errors\\nradar errors\\nextrinsic calibration tool\\nnovel open-source tool\\nloop closure constraints\\nzero mean gaussian noise\\nprobabilistic model\",\"837\":\"angular velocity\\nvelocity measurement\\nnoise measurement\\nmeasurement uncertainty\\nq measurement\\nestimation\\nquaternions\\napproximation theory\\ngeometry\\nkalman filters\\nnonlinear filters\\nmeasurement noise\\ngeometric attitude estimation\\ngeometry-based analytic attitude estimation\\nsingle reference vector\\nrigid body attitude estimation\\nresidual error\\ngeometric solution\\nrate measurements\\nmethodical perturbation analysis\\nbias estimator\\nnonlinear problem\\noptimal kalman gain\\nvector measurement\\nlinearization approximations\\nsxtended kalman filter\",\"838\":\"three-dimensional displays\\nconvolution\\nfeature extraction\\nsemantics\\nmemory management\\nshape\\nconvolutional neural networks\\nconvolutional neural nets\\ngraph theory\\nimage segmentation\\nlearning (artificial intelligence)\\nstereo image processing\\nhierarchical depthwise graph convolutional neural network\\n3d semantic segmentation\\npoint clouds\\npoint cloud semantic segmentation\\ndepthwise convolution\\npointwise convolution\\nlocal feature extraction\\nlocal features\\nglobal features\\ngraph convolution\\ndepthwise graph convolution\",\"839\":\"three-dimensional displays\\nestimation\\nlinear programming\\nuncertainty\\nprediction algorithms\\nmeasurement\\ncomputational modeling\\ncovariance analysis\\ncovariance matrices\\ndata analysis\\nimage registration\\niterative methods\\nstate estimation\\ncello-3d\\nstate estimation frameworks\\nclosed-form covariance estimation algorithms\\ndata-driven approach\\nuncertainty estimation\\nclosed-form solutions\\ncovariance estimation and learning through likelihood optimization framework\\niterative closest point registrations\\n3d datasets\\nicp registrations\\nreal 3d point clouds\",\"840\":\"three-dimensional displays\\nbuildings\\noptimization\\nfeature extraction\\nindexing\\nsimultaneous localization and mapping\\nfile organisation\\nimage enhancement\\nimage fusion\\nmobile robots\\npose estimation\\nrobot vision\\nslam (robots)\\nappearance-enhanced local map building\\nlocal map module\\nlocal map contents\\nco-visibility local map building\\ncompact local map\\ndownstream data association\\nmapped features\\nlocal map size\\nappearance-based local map building method\\nlow-latency visual slam\\nmulti-index hashing\\nonline hash table selection algorithm\\nmih\\nvo-vslam mean performance\",\"841\":\"three-dimensional displays\\noptimization\\ntwo dimensional displays\\nstate estimation\\ncameras\\nhistograms\\nmesh generation\\ncomputational complexity\\ncomputational geometry\\ngraph theory\\nmobile robots\\noptimisation\\nrobot vision\\nslam (robots)\\nstructural regularities\\npoint cloud representation\\ntightly couple mesh regularization\\nvio optimization\\nper-frame approach\\nvisual-inertial odometry algorithms\\nvisual-inertial 3d mesh generation\\ndecouple state estimation\\nfactor-graph formulation\\nmemory usage\\nlocalization accuracy\\nslam\\nvision-based navigation\\nsensor fusion\",\"842\":\"databases\\nunsupervised learning\\nsports\\ncontext modeling\\nlegged locomotion\\nvideo sequences\\nconvolutional neural nets\\nimage sequences\\nmotion estimation\\nhuman action\\nvideo sequence\\nunsupervised label\\nsynthetic databases\\nunsupervised learning method\\no2ca ground truth\\nsurreal-o2ca\\nunsupervised out-of-context action understanding\",\"843\":\"planning\\nsurveillance\\nmarkov processes\\nprediction algorithms\\ntrajectory\\nmeasurement\\ndrones\\nautonomous aerial vehicles\\nmobile robots\\nobject tracking\\npath planning\\ntarget tracking\\nair-to-ground surveillance\\npredictive pursuit\\nmarkov decision process\\ntracking time\\nlocation detection accuracy\\nair-to-ground robot surveillance scenario\\nsurveillance algorithms\\ncamera\\nunmanned ground vehicle\\nugv\\nobserved path\\npursuit algorithm\\ntarget localization\\nhigh predictive accuracy\",\"844\":\"search problems\\ntask analysis\\nrobot sensing systems\\nrobot kinematics\\nuncertainty\\nthree-dimensional displays\\nmanipulators\\nmarkov processes\\nmonte carlo methods\\npath planning\\npa-pomcp algorithm\\naction partially observable monte-carlo planning\\nobject movements\\nmanipulation actions\\npartially observable markov decision process\\ntarget object search task\\npartial observability\\nonline planning\",\"845\":\"reinforcement learning\\nautonomous vehicles\\ntask analysis\\nmarkov processes\\nglobal positioning system\\nsensors\\ntraining\\nlearning (artificial intelligence)\\nmobile robots\\nroad safety\\nroad traffic control\\nroad vehicles\\nrobot vision\\nautonomous driving tasks\\nsingle monocular image\\nsafety driver\\nmodel-free deep reinforcement learning algorithm\\nlane following\\non-vehicle\",\"846\":\"optimization\\naccidents\\nrendering (computer graphics)\\nbayes methods\\nreinforcement learning\\nroads\\ntrajectory\\nautomobile industry\\ncomputer vision\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmobile robots\\noptimisation\\nprogram testing\\nroad traffic\\nroad vehicles\\ntraffic engineering computing\\ntransportation\\nself-driving policy\\nsimulated pedestrians\\nself-driving behavior\\nhigh-fidelity simulators\\npublic roads\\nsoftware tests\\nself-driving software\\nadversarial self-driving scenarios\\nself-driving vehicles\\ntransportation systems\\nsimulated driving scenarios\\ndriving scenario generation\\nself-driving car industry\\nbayesian optimization\\nvision-based imitation learning\",\"847\":\"force\\nnumerical models\\nrobot sensing systems\\ndata models\\nnumerical stability\\noptimization\\ncontrol engineering computing\\nforce sensors\\nlearning (artificial intelligence)\\nmultilayer perceptrons\\noptimisation\\npattern clustering\\nrobots\\ndata-driven contact clustering\\nrigid-body robot simulation\\nmultilayer perceptron network\\nconstraint-based optimization contact solver\\ncontact simulation\\ndata-driven learning-based contact clustering\\ntorque sensors\\nmlp network\",\"848\":\"engines\\nrobot sensing systems\\ndata models\\nreal-time systems\\npipelines\\noptical sensors\\ncontrol engineering computing\\nimage sequences\\nmobile robots\\nsensor fusion\\nvirtual reality\\npavilion\\nbridging photo-realism\\nrobotics\\nrobot control\\nnovel open-source simulation system\\nrobot perception\\nkinematic control\\nros\\nshader-based method\\noptical flow ground-truth data\\ngazebo-compatible real-time simulation system\\ncontrol algorithms\\nsimulation environment\\nstate-of-the-art simulators\\nsimulation accuracy\\nsimulation environments\\nunreal engine\\nsimulation description format robot models\\nrobot operating system\",\"849\":\"tools\\nrobots\\ntumors\\nimage color analysis\\ncameras\\ninstruments\\nbiomedical imaging\\naugmented reality\\nkinematics\\nmedical computing\\nmedical robotics\\nsurgery\\nstereo-no cde\\ncde technique\\nforward kinematics joint encoder data\\nsurgical field\\nvirtual surgical instrument method\\nar technique\\nblue-red color spectrum\\ntissue surface\\ntumor\\nmedical abnormality\\nsurgical robotics\\ncolor depth encoding\\nreal-time interactive augmented reality depth estimation\",\"850\":\"force\\nautonomous vehicles\\nroads\\nacceleration\\nbicycles\\ntesting\\nurban areas\\ncomputer simulation\\ncontrol engineering computing\\ndriver information systems\\nmobile robots\\nroad safety\\nroad traffic control\\nroad vehicles\\ntraffic engineering computing\\nself-driving tests\\nforce-based concept\\nheterogenous traffic simulation\\nrealistic urban environment\\npersonal mobility devices\\npedestrians\\ntraffic control\\nhigh-fidelity driving simulator\\nautonomous vehicle testing\\nforce-based heterogeneous traffic simulation\",\"851\":\"feature extraction\\ndetectors\\nhead\\nproposals\\nobject detection\\nfuses\\npipelines\\nconvolutional neural nets\\nsingle-stage detector\\ndual refinement network\\nanchor refinement\\nsingle-shot object detection\\nobject detection methods\\ntwo-stage detectors\\nanchor-offset detection\\npascal voc\\nimagenet vid datasets\",\"852\":\"cameras\\nradar imaging\\ndetectors\\nobject detection\\ndoppler radar\\ntraining\\ncomputer vision\\nconvolutional neural nets\\nimage capture\\nroad vehicle radar\\ndistant vehicle detection\\nautonomous vehicles\\nconvolutional neural networks\\nimage-based object detectors\\nradar data\\nvision\\nkitti\\nfocal lengths\",\"853\":\"detectors\\nobject detection\\nrobots\\nfeature extraction\\nlabeling\\ncomputer architecture\\ntask analysis\\ncontrol engineering computing\\nconvolutional neural nets\\nhelicopters\\nneurocontrollers\\nrobot vision\\nindoor robots\\nobject detection models\\nconvolutional neural networks\\nlarge-scale labeled datasets\\ntraining data\\ndunet\\ndense upscaled network\",\"854\":\"image segmentation\\ntask analysis\\nobject segmentation\\nclutter\\nsemantics\\nlabeling\\nobject detection\\nconvolutional neural nets\\ndata acquisition\\nsupervised learning\\nclass-agnostic object detection\\nsemisupervised labeling\\nocclusion aware clutter synthesis\\nonline learning\\nsemisupervised deep quick instance detection\\nsemisupervised deep quick learning framework\\nconvolutional neural network\\npixelwise semantic segmentation\",\"855\":\"voltage control\\nvision sensors\\nfeature extraction\\nneuromorphics\\ncameras\\nobject detection\\ncomputer vision\\nconvolutional neural nets\\nimage fusion\\nimage sensors\\npedestrians\\ntraffic engineering computing\\nconventional frame-based camera\\nbad light condition\\nhigh-speed motion\\ngray-scale frames\\ntraffic monitoring scenario\\nyolov3 models\\nyolo-tiny models\\nconfidence map fusion method\\ncnn-based detection results\\ndavis channels\\nintelligent transportation system\\nmixed frame-event-driven fast pedestrian detection\\nits\\nframe-based camera\\ndynamic and active pixel sensor\\nasynchronous low-latency temporal contrast events\\nconvolutional neural networks\\ntum campus\",\"856\":\"convolution\\nneural networks\\nproposals\\nvehicle detection\\nobject detection\\ncomputational modeling\\nfeature extraction\\nmobile computing\\nmotorcycles\\nneural nets\\nroad vehicles\\ntraffic engineering computing\\nmobilenet family network engineering\\ncompressed mobilenet\\nfeature map downsampling stage\\nfeature map plateau stage\\nreduced inference time\\nvehicle categories\\ncrowded bicycles\\nhigh detection accuracy\\nreal-time detection speed\\nreal time vehicle detection\\nobject interference\\nshort-range aerial image\\ncrowded motorcycles\\ntruck\\ncar\\nbus\",\"857\":\"tools\\nforce\\nthree-dimensional displays\\nsurgery\\nend effectors\\nstability analysis\\nforce control\\nhaptic interfaces\\nhuman-robot interaction\\nmanipulator dynamics\\nmedical robotics\\nmobile robots\\npath planning\\npoint cloud-approximated regions\\nsurgical applications\\nhuman-robot interaction controller\\nphri\\nsensitive tissues\\nrestricted region\\nconstraint enforcement\\ninteraction force\\nconstraint satisfaction\\nkuka lwr4+ robot\\n3d point-cloud\\nartificial potential fields\\nactive constraint enforcement\\nkinesthetic guidance\\nkuka lwr4+ robot end-effector\\nkuka virtual slave\",\"858\":\"needles\\nhaptic interfaces\\nforce\\nanesthesia\\nprototypes\\nbones\\ntraining\\ncomputer simulation\\nmedical computing\\npneumatic cylinder\\nelectrical haptic interface\\nepidural anesthesia haptic simulator\\nmedical procedure\",\"859\":\"muscles\\nforce\\ntorque\\nactuators\\nfitting\\npulleys\\nbladder\\nbiomechanics\\nmedical robotics\\nmuscle\\northotics\\npneumatic actuators\\npam types\\nsleeve pneumatic artificial muscles\\npaper sleeve pams\\npopular muscle configuration\\njoint rotation\\ntraditional pams\\njoint configuration\\nantagonistically actuated joints\",\"860\":\"tactile sensors\\nsensitivity\\nforce\\ncalibration\\nforce measurement\\ndexterous manipulators\\nforce feedback\\nforce sensors\\ngrippers\\nhaptic interfaces\\nsensors\\nshear strength\\nbite acquisition success\\nshear forces\\nfood manipulation\\nautonomous assistive feeding systems\\ndeformable food items\\nshear sensing fingertip tactile sensors\\nsensing range\\nbite acquisition successes\\nrobotic gripper\\nvarying weights\\ncompliance\",\"861\":\"resilience\\nsoft robotics\\nrobustness\\nstrain\\nstandards\\nprosthetics\\nactuators\\ndexterous manipulators\\nend effectors\\nhuman-robot interaction\\nimpact testing\\nartificial hands\\nharsh interactions\\nirregular physical interactions\\ndisaster scenario\\nstandardized test\\nhand resilience\\nimpact tests\\nstandard test\\nrobot hands\\nresilience evaluation framework\",\"862\":\"trajectory\\nskin\\nplanning\\nkernel\\nrobot sensing systems\\ncost function\\ngradient methods\\nmanipulators\\nmobile robots\\npath planning\\nrobot vision\\nstochastic processes\\ntactile sensors\\nmultimodal robot skin\\ncontact-based robot system\\nskin compliant control\\ntactile-based explorative behavior\\nchimp\\nskin-based sparse contact data\\ncontact-based 3d path planning approach\\n6 dof robot arm\\nstochastic functional gradient path planner\\ncontact based hilbert map planner\",\"863\":\"springs\\nrubber\\npins\\nshafts\\ncouplings\\nrobots\\nsolid modeling\\nelasticity\\nmechanical engineering computing\\nsprings (mechanical)\\nadjustable stiffness\\njasr\\nzero-length base link four-bar linkage\\nhard-spring behaviour\\nlight-weight structure\\ndesign parameters\\nreconfigurable revolute joint\",\"864\":\"springs\\noptical variables measurement\\noptical sensors\\nlaser beams\\nrobot sensing systems\\nforce\\nforce sensors\\nfibre optic sensors\\nmedical robotics\\nzero stiffness\\ncontact transition\\noptical line generation\\nrobotic system\\nstiff environment\\npassive compliance\\nrobot control\\noptical measurement process\\ncompliant sensor body\\nforce sensor\\nmedical robotization\\nlow off-axis sensitivity\\nadditive manufacturing\",\"865\":\"gears\\nhydraulic systems\\nrobots\\nstandards\\nshape\\nforce\\nthree-dimensional printing\\nbiomedical mri\\ncompliance control\\ndesign engineering\\nhydraulic actuators\\nmanipulator kinematics\\nmedical image processing\\nmedical robotics\\npolymers\\nrapid prototyping (industrial)\\ntelerobotics\\nhydraulic cylinder\\nhydraulically-actuated compliant revolute joint\\nmedical robotic systems\\nmultimaterial additive manufacturing\\nhydraulic energy\\nseal design\\nminiature hydraulic cylinders\\nrack-and-pinion mechanism\\nmagnetic resonance imaging\",\"866\":\"torque\\nrobot sensing systems\\nfriction\\nmathematical model\\nmanipulator dynamics\\nactuators\\ncontrol system synthesis\\nelasticity\\nfeedback\\nfeedforward\\nflexible manipulators\\ngears\\nleast squares approximations\\nposition control\\ntime-varying systems\\nmodel-based feedforward controllers\\ne-series manipulators\\nur5e manipulator\\ntransmission deformation\\nelastic torques\\nparametric model\\nrecursive least squares strategy\\nrobot joint stiffness\\nfeedback controllers\\nrobot joints\\ngear meshing\\ndriven link\\ndrive actuator\\ndynamic time-varying displacement\\nlightweight strain-wave type transmissions\\ne-series universal robots manipulator\\ntime-varying nonlinear joint stiffness\\nmodel-based on-line estimation\",\"867\":\"force\\nactuators\\nsprings\\ntorque\\nstress\\nrobots\\ntorque control\\nelasticity\\nfinite element analysis\\nmobile robots\\nprototypes\\nsprings (mechanical)\\nprogressive stiffness actuators\\nrobot performance\\n2d components\\nphysical interaction performance\\npassive rolling flexure design principle\\nlinear series elastic actuators\\ntorque-deflection characteristics\\nlaboratory prototypes\",\"868\":\"conferences\\nautomation\\nactuators\\nforce control\\nfriction\\nlegged locomotion\\nmotion control\\npipes\\nrobot dynamics\\nlocomotion dynamics\\ndynamic locomotion analysis\\nwave robot\\npropulsion force\\nlocomotion models\\nadvance time ratio\\nfriction forces\\nminiature model\\nminiature wave-like robot\\nminimally actuated wave-like robot kinematics\\ncrawling environments\\nflexible tube-like shapes\",\"869\":\"actuators\\nfabrics\\npayloads\\nmanipulators\\nheating systems\\nforce\\ncomputational modeling\\nartificial limbs\\nfinite element analysis\\nsoft-waist belt\\nphysical assistance\\ndaily living tasks\\nadditional limb\\nmobile manipulation assistance\\nsoft actuators\\nhigh-strength inflatable fabrics\\nsystematic design rules\\nhighly compliant soft robotic limbs\\nfabric based components behavior\\nfinite-element method models\\nfem models\\nfspl articulation capabilities\\nfabric soft arm\\nfabric soft poly-limbs\",\"870\":\"actuators\\nfoot\\nfabrics\\nforce\\nfabrication\\nmuscles\\nelastic constants\\nelectromyography\\nfinite element analysis\\ngait analysis\\nkinematics\\nmedical control systems\\northotics\\nsoft ankle-foot orthosis exosuit\\nfoot drop assistance\\nnatural gait restoration\\nsoft actuators\\nthermally-bonded nylon\\nswing phase\\ngait cycle\\nankle joint proprioception\\nvariable stiffness soft actuator\\ncomputational model\\nfabric actuators\\ndorsiflexion actuator\\nsoft afo\\nankle dorsiflexion\\nelectromyography studies\",\"871\":\"robot sensing systems\\ncameras\\nthree-dimensional displays\\nforce\\nestimation\\nimage reconstruction\\ndeformation\\nestimation theory\\nrobots\\ntactile sensors\\ndepth camera-based soft fingertip device\\ncontact region estimation\\nperception-action coupling\\nrobotic applications\\nunconstrained environments\\ndynamic environments\\nsoft robotic technologies\\nsoft tactile sensor design\\nhuman fingertip\\nperception mechanism\\ncompliance-modulating capabilities\\nestimation sensitivity\\ninternal fluid states\\nforce-deformation characteristics\",\"872\":\"actuators\\npneumatic systems\\nprototypes\\nsoft robotics\\nstrain\\ntorso\\nbending\\ndesign engineering\\nelastomers\\nmobile robots\\nmotion control\\npipes\\npneumatic actuators\\nmodular design\\nflexible actuators\\npipe-climbing soft robot\\nbio-inspired soft pneumatic robot\\ncylinder\\nsoft pneumatic actuators\\nfrees\\ndeformation behavior\\nbending actuators\\nfiber reinforced elastomeric enclosure\\nforward motion\\ncrawling robot\\nartificial muscles\\nmckibben muscles\",\"873\":\"legged locomotion\\nfoot\\nmathematical model\\nwheels\\nroads\\ntrajectory\\nforce\\nfriction\\ngait analysis\\nmotion control\\nrobot dynamics\\namcc\\nhorizontal ground reaction force\\nlow-friction road surface\\nplanar underactuated rimless wheel\\nstealth walking gait\\nadaptive walking gaits\\nunderactuated walkers\\ncontrol torques\\nfrictionless road surface\\nangular momentum constraint control\\nstance-leg motion\\nlinearized motion equation\",\"874\":\"vegetation mapping\\nsurface topography\\nkernel\\nestimation\\nrobot sensing systems\\ngaussian processes\\nlegged locomotion\\npath planning\\nregression analysis\\nrobot dynamics\\nterrain mapping\\nlegged robots\\nlegged systems\\nrugged outdoor environments\\nterrain geometry\\nfoothold planning\\nsafe locomotion\\npenetrable terrain\\ndepth sensors\\nhaptic information\\nfoot contact closure locations\\nexteroceptive sensing\\ndense support surface estimate\\ngaussian process regression\\nsupport surface estimation procedure\\npenetrable surface layer\\ndiscrete penetration depth measurements\\ncontinuous support surface estimate\\npartial exteroceptive information\\nterrain topography\\nquadrupedal robot anymal\",\"875\":\"legged locomotion\\ntask analysis\\ngrippers\\nrobot kinematics\\ntracking\\ndynamics\\nend effectors\\nhuman-robot interaction\\nmanipulator dynamics\\nmotion control\\noptimisation\\npath planning\\ntorque control\\nunstructured environments\\nalma\\ntorque-controlled quadrupedal robot\\ndynamic locomotion\\nonline motion planning framework\\nwhole-body controller\\nhierarchical optimization algorithm\\noperational space end-effector control\\nhuman-robot collaboration\\ntorso posture optimization\\ntorque-controllable robot\\nrobotic mobile manipulation\\nsix degrees of freedom robotic arm\\narticulated locomotion and manipulation\\nreactive human-robot collaboration\",\"876\":\"legged locomotion\\ndynamics\\nrobot kinematics\\nthree-dimensional displays\\ntrajectory\\nreal-time systems\\nmotion control\\npredictive control\\nquadratic programming\\nrobot dynamics\\nversatile dynamic motions\\nquadrupedal robot\\nsingle rigid body dynamics\\nrotation matrices\\nquaternions\\nunwinding phenomenon\\nmpc control law\\nperiodic quadrupedal gaits\\nmodel predictive control framework\\nquadratic program\\nqp\\neuler angles\\nacrobatic maneuvers\",\"877\":\"robot sensing systems\\nsecurity\\nactuators\\ninternet\\nservice robots\\ncontrol engineering computing\\nimage sensors\\nip networks\\noperating systems (computers)\\nrobot programming\\nrobot vision\\nros\\npublic internet\\nimage sensor information\\npublicly-accessible platforms\\nrobotics research\\nrobot operating system\\nrobotic sensors\\nrobotic actuators\\nipv4 address space\\namerican university\",\"878\":\"reinforcement learning\\ntraining\\nmathematical model\\nrobustness\\nautonomous vehicles\\ntask analysis\\naccidents\\nlearning (artificial intelligence)\\noptimisation\\nprobability\\nrisk averse robust adversarial reinforcement learning\\ndeep reinforcement learning\\ncomputer games\\nrobotic control\\nautomotive accidents\\noptimization\\nrararl\\nself-driving vehicle controller\",\"879\":\"force\\ncollision avoidance\\nrobot sensing systems\\nmeasurement\\nend effectors\\nadmittance\\ncompliance control\\ncontrol system synthesis\\nfeedback\\nforce control\\nh2 control\\nmanipulator dynamics\\nmechanical contact\\nsprings (mechanical)\\nsobolev norm\\nrobot inertia\\nanalytical models\\nmaximum collision force\\nsimplified mass-spring robot model\\nend-effector compliance\\nsystem norm\\nmaximum force\\ngeneral dynamic system\\nfeedback control\\ncontrol theory\\ncontroller synthesis\\nadmittance-controlled robot\\nlinear flexible-joint robot\\nbounded collision force\\nrobot contact\\nsafety risks\\ncollision force minimisation\",\"880\":\"automobiles\\nplanning\\nautonomous automobiles\\nethics\\nmeasurement\\ntrajectory\\nsemantics\\ncultural aspects\\nethical aspects\\nformal specification\\nintelligent transportation systems\\nmobile robots\\nmulti-agent systems\\nculture-aware behavior specification\\nautonomous agents\\nself-driving domain\\nliability-aware behavior specification\\nethics-aware behavior specification\\nself-driving car behavior\\naptiv company\\nnutonomy\\nviolation metric\\nrulebook semantics\",\"881\":\"uncertainty\\nbayes methods\\ntask analysis\\nneural networks\\nsafety\\ntraining\\nautonomous vehicles\\nbelief networks\\ncontrol engineering computing\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nlearning systems\\nobservability\\npredictive control\\nlearned control policies\\nreinforcement learning\\nend-to-end imitation\\npredictive uncertainty\\nmodel predictive controller\\nfully-observable vision-based partially-observable systems\\ndeep convolutional bayesian neural networks\\ndeep end-to-end control policy\\nbayesian networks\\nmean value\\ncorrective action\\npartial state observability\",\"882\":\"safety\\nautomation\\nreinforcement learning\\nrobots\\noptimal control\\njacobian matrices\\nreachability analysis\\napproximation theory\\ncontrol engineering computing\\ndynamic programming\\ngradient methods\\nlearning (artificial intelligence)\\nmobile robots\\npartial differential equations\\ndynamic programming equation\\ncontraction mapping\\nhamilton-jacobi safety analysis\\ncontrol-theoretic safety analysis\\noptimal safety policy\\nquantitative safety analysis\\nreinforcement learning techniques\\ntime-discounted modification\\noptimal control problems\\nrobust optimal control theory\\nautonomous robotic systems\\npolicy gradient techniques\\nvalue learning\",\"883\":\"trajectory\\nplanning\\nagricultural machinery\\nkinematics\\noptimal control\\nwheels\\ndispersion\\npath planning\\nsampling methods\\nsearch problems\\nvehicle dynamics\\ntrajectory planning\\nmultiple trailers\\nextremely narrow environments\\nunified approach\\nvehicle kinematics\\nunderactuated constraints\\nnonholonomic constraints\\nprevalent sampling-based\\nrigid-body vehicles\\ntractor-trailer vehicle cases\\ngeneric n-trailer cases\\ntiny environments\\nadaptively homotopic warm-starting approach\\nnumerical solution process\\nextremely tiny scenarios\\nonline planning opportunities\",\"884\":\"mobile robots\\nkinematics\\nwheels\\nforce\\npredictive models\\nacceleration\\nfriction\\nrobot dynamics\\nrobot kinematics\\nfriction-based kinematic model\\nskid-steer drive systems\\nmobile robot platforms\\nnormal operation\\nslippages\\nforward kinematics\\nslip prediction\\nskid-steer wheeled mobile robots\\ntranslational prediction error\\nrotational prediction error\\nskid-steer robot\\nsystem identification\\nmodel learning research\",\"885\":\"turning\\nautomobiles\\nheuristic algorithms\\nplanning\\npath planning\\napproximation algorithms\\nterminology\\ncollision avoidance\\nmobile robots\\noptimisation\\nclosed-form optimal path\\nefficient path planner\\nshortest collision-free dubins paths\\nsufficient condition\\nclosed-form solution\\ninterior corner\\nelementary dubins paths\\nrsrsr\\nrsrsl\\nlsrsr\\nlsrsl\",\"886\":\"robot kinematics\\nwheels\\nmathematical model\\nrobot sensing systems\\ntorque\\nkalman filters\\nmobile robots\\nnonlinear control systems\\nnonlinear filters\\nrobot dynamics\\nstate estimation\\nmicroball-balancing robot\\nextended kalman filter\\nlinearized dynamic model\\nstate estimator\\nyaw-rate ball-balancing robot dynamic model\\nraw on-board sensor measurements\\nekf\",\"887\":\"aerospace electronics\\nrobot kinematics\\nharmonic analysis\\nnavigation\\nshape\\napproximation algorithms\\ncollision avoidance\\nmobile robots\\nmotion control\\norientation-aware motion planning\\ncomplex workspaces\\nadaptive harmonic potential fields\\nhybrid control scheme\\nnavigation problem\\nplanar robotic platform\\napproximate configuration space decomposition techniques\\nappropriate workspace transformations\\nadaptive potential field based control laws\\nconfiguration space representation\\nobstacle cluttered workspace\",\"888\":\"trajectory\\ncharging stations\\nbatteries\\nplanning\\ntask analysis\\nmobile robots\\ncomputability\\npath planning\\npower aware computing\\ntemporal logic\\ntrajectory control\\nmotion planning problem\\nsmt solving problem\\noptimal trajectory\\nenergy-aware trajectories\\nltl specification\\nmobile robot\\nmotion plan\\nbattery charge\\nltl formula\\nlinear temporal logic\\ncharging station locations\\nenergy-aware temporal logic motion planning\\nsatisfiability modulo theory\",\"889\":\"databases\\nplanning\\ntask analysis\\nkinematics\\ntrajectory\\nmanipulators\\ncollision avoidance\\nmobile robots\\nsampling methods\\nglobal motion planning\\nsampling-based planners\\ncollision-free path\\nsampling strategy\",\"890\":\"mobile robots\\ntrajectory\\nmathematical model\\nrobot kinematics\\ncollision avoidance\\ndynamics\\nfuzzy logic\\ngradient methods\\nlearning (artificial intelligence)\\nlyapunov methods\\nmanipulator dynamics\\nmotion control\\nposition control\\nradial basis function networks\\nstability\\nsteering systems\\ntrajectory control\\ndynamic movement primitive\\nmotion planning\\nrobot manipulator\\nnonholonomic mobile robot\\nrobot goal position\\nlyapunov stability theory-based analysis\\ndynamic obstacles\\nautomatic goal adaptation\\nrbfn\\ndmp\\ngradient descent\\nstatic obstacles\\ntrajectory tracking\\ndamped spring model\\nsteering angle dynamics\",\"891\":\"trajectory\\nvehicle dynamics\\ndynamics\\nacceleration\\ncollision avoidance\\nrobot sensing systems\\nmobile robots\\nmotion control\\npredictive collision avoidance\\ndynamic window approach\\nforesighted navigation\\nfactory floor installations\\ndynamic collision model\\nnonholonomic vehicles\",\"892\":\"kinematics\\ntrajectory\\nmobile robots\\nnavigation\\nplanning\\ncollision avoidance\\npath planning\\nrobot dynamics\\nrobot kinematics\\nbi-rrt algorithm\\nparameterized trajectories\\nrobot path planning\\ncomplex missions\\nnavigation capability\\nautonomous mobile robots\\nrobust path planning algorithms\\nbidirectional-rrt\\nkinodynamic constraints\\nbidirectional rrt\\ntrajectory planning\\nmemory utilization\\nkinematic constraints\\nkb-rrt algorithm\",\"893\":\"planning\\ntrajectory\\nvehicle dynamics\\nencoding\\nautonomous vehicles\\nrecurrent neural networks\\nprediction methods\\nrecurrent neural nets\\nroad vehicles\\ntraffic engineering computing\\nrecurrent neural network\\ninteraction modeling\\nvehicle behaviors\\nbehavior detection\\nlong-term future rewards\\nvehicle behavior interaction network\\nobservation encoding\",\"894\":\"optical imaging\\nadaptive optics\\ncomputational modeling\\nkernel\\ntraining\\nroads\\ncameras\\ndriver information systems\\nimage sequences\\nroad safety\\nsteering systems\\nvisual input data\\nonboard vehicle camera\\nempirical comparison\\nspatial spatio-temporal\\nreal-life driver\\npredicted steering command\\nrecurrent multimodal model\\nsteering correction concept\\nmultimodal spatio-temporal information\\nend-to-end networks\\nautomotive steering prediction\\nend-to-end steering problem\\nautonomous steering\\ndeep learning\\nspatio-temporal model\\nmultimodal input\\noptical flow\\nrnn-lstm\",\"895\":\"three-dimensional displays\\nnavigation\\nrobot sensing systems\\nplanning\\nlaser radar\\nreal-time systems\\ncomputational geometry\\nmesh generation\\nmobile robots\\npath planning\\nremotely operated vehicles\\nrobot vision\\nstereo image processing\\novpc mesh\\n3d free-space representation\\nlocal ground vehicle navigation\\nautonomous unmanned ground vehicle\\nvisible point clouds mesh\\nlocal point cloud data\\nugv navigation\\non visible point clouds mesh\\nwatertight 3d mesh generation\\ntrajectory planning\\nrobot\",\"896\":\"automobiles\\npredictive models\\nhidden markov models\\ntask analysis\\nvehicle dynamics\\nbayes methods\\npath planning\\nrecurrent neural nets\\nroad vehicles\\ntraffic engineering computing\\nfunction estimation problem\\nmodel understandability\\nlane change prediction model\\nattention-based recurrent model\\nprediction quality\\nattention-based lane change prediction\\ndriver discomfort\",\"897\":\"uncertainty\\ncollision avoidance\\nneural networks\\ncomputational modeling\\ntraining\\ndata models\\nreinforcement learning\\nbayes methods\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nneural nets\\nsafety\\nmodel uncertainty estimates\\ncurrent autonomous systems\\nstrong reliance\\nblack box predictions\\ndeep neural networks\\ndnns\\nunpredictable results\\nfar-from-distribution test data\\ndistributional shift\\nsafety-critical applications\\npedestrians\\nstate-of-the-art extraction methods\\nbayesian neural networks\\nmc-dropout\\ncomputationally tractable uncertainty estimates\\nparallelizable uncertainty estimates\\nuncertainty-aware navigation\\ncollision avoidance policy\\nunseen behavior\\nuncertainty-unaware baseline\\nsafe reinforcement learning framework\",\"898\":\"search problems\\nroads\\nvisualization\\nshortest path problem\\nmathematical model\\ntask analysis\\ncomputational modeling\\ndirected graphs\\ndynamic programming\\ngraph theory\\nimage segmentation\\nobject detection\\ntraffic engineering computing\\nline segments\\ndirected graph model\\nshortest path problem-related application\\ncurved lanes\\nautonomous driving systems\\nvisual recognition tasks\\nlane detection\\ntwo-dimensional graph searching problem\\noptimal path finding\",\"899\":\"task analysis\\nmanipulators\\ntools\\njamming\\nwrist\\ntrajectory\\nforce control\\nhuman-robot interaction\\nindustrial manipulators\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmotion control\\nmulti-robot systems\\nrobotic assembly\\ntelerobotics\\nlearning method\\nmaster-slave compliance\\ndual-arm assembly task\\ncompliance parameters\\nhuman demonstration\\ncompliant motions\\nassembly tasks\\nconvergence region\\nalignment task\\ncompliant axes\\norientation error\\nsingle teleoperated manipulator\\nmanipulators compliant\\ntotal joint motions\",\"900\":\"trajectory\\nmanipulators\\nsplines (mathematics)\\noptimization\\nrobot kinematics\\ntools\\nmulti-robot systems\\noptimisation\\nsynchronized configuration space trajectories\\nmultirobot systems\\npath-constrained trajectory generation\\nsynchronous motion\\nnonlinear optimization problem\\nconfiguration variables\\nsuccessive refinement techniques\\nparametric representation\",\"901\":\"robots\\nbenchmark testing\\ntask analysis\\ngrasping\\nhardware\\ncameras\\ncalibration\\ncontrol engineering computing\\nlearning (artificial intelligence)\\nmanipulators\\nrobot programming\\nrobot vision\\nreproducible low-cost arm benchmark\\nrobotic learning\\nvision-based manipulation benchmark\\nrobot arm\\nrobotics\\ngrasping benchmark\\nevaluation protocol\\nstandardized evaluation\\nmachine learning\\nreplab\",\"902\":\"robots\\nthree-dimensional displays\\ncontainers\\nstability analysis\\ncollision avoidance\\npipelines\\ngeometry\\ncomputational geometry\\ncontrol engineering computing\\nindustrial manipulators\\nproduction engineering computing\\nwarehouse automation\\nnonconvex objects\\nbin packing\\nheightmap-minimization heuristic\\nconstructive packing pipeline\\nplacement plans\\nrobot motion\\nautomated warehousing domain\\npacking problem\\nfully automatic object packing\\nrobot manipulator\\nnonconvex 3d objects\\nhigh-quality packing\\nrobot packability constraints\",\"903\":\"task analysis\\nplanning\\nmanipulators\\nrobot kinematics\\njob shop scheduling\\nservice robots\\nconstraint handling\\nindustrial manipulators\\nmotion control\\noptimisation\\nrobot programming\\nrobotic assembly\\nscheduling\\nrobotic platforms\\nconstraint programming approach\\nsimultaneous task allocation\\nmotion scheduling\\nindustrial dual-arm manipulation tasks\\ndual-arm robots\\nindustrial manipulation\\nassembly tasks\\nrobot motion models\\nconstraint optimization problems\\nmakespan-optimized robot programs\\nindustrial workplaces\\nrobot-independent task model\\nlightweight dual-arm robots\\nordered visiting constraint\\nordering constraints\",\"904\":\"tools\\nimage segmentation\\nkinematics\\nshape\\nrobot kinematics\\ncost function\\ncalibration\\nconvolutional neural nets\\nendoscopes\\nimage classification\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\noptimisation\\npose estimation\\nrobot vision\\nsurgery\\ntraining labels\\noptimization method\\nunknown hand-eye calibration\\nimprecise kinematic model\\nfully-convolutional neural network\\nendoscopic images\\nflexible robotized endoscopy system\\nself-supervised surgical tool segmentation\\nkinematic information\\ntask automation\\nminimally invasive surgical operations\\nmodern machine learning methods\\nmanually-annotated images\\nsurgical context\\npatient-to-patient differences\\nannotated data\\nself-supervised approach\\nrobot-assisted context\\nsubtask automation\\nhand-eye calibration\\npixel-wise classification\",\"905\":\"needles\\nretina\\nsurgery\\nimage segmentation\\nrobots\\nmicroscopy\\nagriculture\\nbiomedical optical imaging\\neye\\nlearning (artificial intelligence)\\nmedical image processing\\nmedical robotics\\noptical tomography\\nvisual feedback\\nmicroscope-integrated optical coherence tomography\\nrobotic subretinal injection\\nneedle segment\\nretinal surface\\noct volumetric images\\nmi-oct\\nneedle detection\\nhuman surgeons\\nrobot-assisted surgery\\nhigh surgical precision\\ndeep learning\\nrobot-assisted subretinal injection\\nneedle localization\",\"906\":\"hidden markov models\\nmixture models\\nprobabilistic logic\\nmaximum likelihood estimation\\ndata models\\nprincipal component analysis\\nnonhomogeneous media\\ncomputer vision\\nexpectation-maximisation algorithm\\ngaussian distribution\\ngaussian processes\\nimage registration\\niterative methods\\noptimisation\\ngeneralized point set registration\\ntranslation vector\\nsurface points\\ninhomogeneous hybrid mixture models\\nexpectation maximization\\nbiomedical engineering communities\\norientational vector\\nexpectation-maximization framework\\nregistration methods\\nfisher distribution mixture models\\ngmm\\nfmm\\npsr\",\"907\":\"microscopy\\nrobot kinematics\\nneedles\\nwebcams\\nmanipulators\\naneurysm\\nblood vessels\\ncameras\\ndiseases\\nimage matching\\nintelligent robots\\nlearning (artificial intelligence)\\nmanipulator dynamics\\nmedical image processing\\nmedical robotics\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nstents\\nstereo image processing\\nrobotic platforms\\nautonomous personalized stent graft manufacturing\\nstereo vision systems\\npersonalized stent-graft manufacturing\\nrobotic arm\\ndynamic stereo microscope\\nstatic wide angle view stereo webcam\\nmultiple stereo camera configuration\\nsewing process\\nstereo matching\\nfeature identifications\\nvisual-servoing system\\nreal-time intelligent robotic control\\nvisual guidance\\nautomatic control\\nrobotic personalized stent graft manufacturing\\naaa patient\\nhybrid vision system\\nabdominal aortic aneurysms patient\\nddpg\\nreinforcement learning\\nobject localization\",\"908\":\"three-dimensional displays\\ntwo dimensional displays\\nskeleton\\nindexes\\nrobots\\narteries\\npath planning\\nblood vessels\\ncomputerised tomography\\ndiagnostic radiography\\nimage registration\\nimage segmentation\\nmedical image processing\\nmedical robotics\\nphantoms\\nct scans\\ncomputed tomography\\n2d intra-operative aaa skeletons\\ngraph matching method\\n3d preoperative aaa\\n3d distance error\\nskeleton length\\nskeleton deformation\\nreal-time 3d robotic path planning\\nabdominal aortic aneurysm\\nskeleton instantiation framework\\n2d fluoroscopic images\\nfenestrated endovascular aortic repair\\nsingle 2d fluoroscopic image\",\"909\":\"visualization\\nentropy\\ncameras\\ngrasping\\nclutter\\nrobot vision systems\\ngrippers\\nobject detection\\npose estimation\\nrobot vision\\nuncertainty handling\\nocclusions\\nactive perception approach\\ninformative viewpoints\\nmvp controller\\nmultiple fixed viewpoints\\nnext-best-view reaching\\nimproved grasping\\ncamera viewpoint selection\\nvisual grasp detection\\nmultiview picking controller\\nreal-time grasp pose estimates\\nuncertainty reduction\",\"910\":\"modeling\\nrobot sensing systems\\nthree-dimensional displays\\ncameras\\ntask analysis\\ncomputational geometry\\nimage reconstruction\\nimage sensors\\ninspection\\nmobile robots\\nrobot vision\\nsolid modelling\\nmodel building process\\nweld seam inspection\\nmultisensor next-best-view framework\\ngeometric model-based robotics applications\\nconsecutive sensing actions\\nrobotic 3d reconstruction systems\\nreconstruction goals\",\"911\":\"estimation\\nrobot sensing systems\\nvibrations\\nkinematics\\nmanipulators\\ntelerobotics\\ncovariance matrices\\nelasticity\\nmanipulator kinematics\\nmaximum likelihood estimation\\nmotion control\\noptimal control\\nprobability\\nsensor placement\\nsensors\\ninertial measurement unit sensors\\nhigh-dof ekc\\nhigh-degree-of-freedom ekc\\nmaximum a posteriori estimation\\nposterior probability\\nreal-time output estimation\\noptimal placement\\noptimal imu placement\\nmap estimation\\npod mode\\nnondominant modes\\nproper orthogonal decomposition\\nimu sensors\\nelastic kinematic chain\\nsensor placement framework\\nmodel-free optimal estimation\",\"912\":\"games\\nsearch problems\\nplanning\\nmonte carlo methods\\ngame theory\\ntask analysis\\nreconnaissance\\nminimax techniques\\ntree searching\\ntrees (mathematics)\\ntree search techniques\\nreconnaissance mission\\npursuit-evasion problem\\nfinite-horizon path\\nzero-sum game\\ngame tree search algorithms\\nminimax search tree\\nmonte-carlo search tree\\ndetectability minimization\\nvisibility maximization\\nvisibility-based target search\\npruning techniques\",\"913\":\"planning\\nrobots\\ntrajectory\\ncollision avoidance\\ntask analysis\\nestimation\\nuncertainty\\nmobile robots\\noptimal control\\nprobability\\nchance constrained motion planning\\nhigh-dimensional robots\\nprobabilistic chekov\\nchance-constrained motion planning system\\ndegree-of-freedom robots\\nmotion uncertainty\\nstate information\\nobservation noise models\\ndeterministic motion planning\\nintegrated trajectory optimization\\nsparse roadmap framework\\nplanning speed\\nhigh-dimensional tasks\\nlinear-quadratic gaussian motion planning approach\\nrobot state probability distribution\\ncollision risk estimation\\nrobotic planning tasks\\np-chekov system\\nuser-specified chance constraints\\nreal-world planning scenarios\",\"914\":\"robot sensing systems\\nplanning\\ninspection\\nspace exploration\\nsurface cracks\\nfilling\\nmobile robots\\noptimal control\\npath planning\\nsensors\\nnear-optimal path planning\\nrobotic crack\\nsimultaneous robotic footprint\\nrange sensors\\ncomplete sensor coverage\\nplanning strategy\\ncrack-filling robotic prototype\\nonline planning algorithm\\nsensor-based inspection\\nnear-optimal footprint coverage\\nonline sensor-based complete coverage planning\\nonline scc planning\",\"915\":\"stability analysis\\nforce\\nfriction\\nmathematical model\\nstacking\\nnumerical stability\\nbuildings\\nbuilding materials\\ngeometry\\nlinear programming\\nmechanical stability\\nplanning\\nroad building\\nshrinkage\\nconstruction materials\\ncontact geometry\\ngeometric safety factor\\nautomated dry stacking procedure\\nbuilding elements\\nstructural stability analysis\\nkern\\nfully simulated shaking test\\nheuristics-based planning\\nassembly process\",\"916\":\"trajectory\\nend effectors\\ninterpolation\\nkinematics\\noptimization\\naerospace electronics\\ncontrol system synthesis\\nmanipulator kinematics\\nmotion control\\ntrajectory control\\norientation goals\\njoint-space discontinuities\\nuser specifications\\nuser-guided offline synthesis\\nrobot arm motion\\n6-dof path\\nrobot arms\\nend-effector\\ntrajectories\\npose goals\\nself-collisions\\nkinematic singularities\",\"917\":\"task analysis\\nplanning\\nvisualization\\npredictive models\\nrobots\\ntransforms\\ncomputer architecture\\nlearning (artificial intelligence)\\nmobile robots\\nmonte carlo methods\\nneural net architecture\\nplanning (artificial intelligence)\\nrobot vision\\ntree searching\\nvisual robot task planning\\nvisual information\\nplanning algorithm\\nneural network architecture\\nmonte carlo tree search\\nblock-stacking simulation\",\"918\":\"planning\\nheuristic algorithms\\ntask analysis\\nservice robots\\nindustries\\nrobot sensing systems\\nmobile robots\\nmulti-agent systems\\npath planning\\ntrees (mathematics)\\nplanning algorithm\\ndynamic environment\\nsolution blend acting\\nexternal disturbances\\nexternal agent\\nbehavior trees\\nrobotics scenarios\\nblended reactive planning\\nback chaining\",\"919\":\"navigation\\nvisualization\\nsemantics\\ntraining\\nadaptation models\\nrobots\\ntask analysis\\nimage representation\\nimage segmentation\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\nvisual representations\\nsemantic target driven navigation\\nsemantic visual navigation\\nsemantic segmentation\\ndomain adaptation\\ncomputer vision algorithms\\nrobot\\ndeep network\\nnavigation policy learning\",\"920\":\"task analysis\\ntraining\\ntaxonomy\\nautomobiles\\nautonomous vehicles\\nfeature extraction\\nrobots\\ncomputer games\\nconvolutional neural nets\\ndata visualisation\\nlearning (artificial intelligence)\\ntraffic engineering computing\\nobject-centric models\\nobject instances\\nend-to-end learning\\ngrand theft auto v simulator\\nobject-agnostic methods\\nobject-centric policies\\nautonomous driving\\nvisuomotor skills\\ndeep neural networks\\nrobotics tasks\\nintuitive visualization\\nberkeley deepdrive video dataset\",\"921\":\"acceleration\\ngeometry\\nautonomous robots\\nmeasurement\\nneural networks\\nkinematics\\ncollision avoidance\\nlearning (artificial intelligence)\\nmobile robots\\nneurocontrollers\\noptimal control\\npredictive control\\nrobot vision\\nimage-based autonomous navigation technique\\nriemannian motion policy framework\\nvehicular control\\ndeep learning\\npolicy structure\\ndata complexity\\nmodeling error\\nend-to-end learning\\nneural autonomous navigation\\nlocal geometry\\nrmp representation\\nindoor obstacle avoidance\\ngibson environment\\noptimal control commands\\nvisual images\\ncontrol point rmps\\ndeep neural network\",\"922\":\"robot kinematics\\ncollision avoidance\\ncameras\\nrobot vision systems\\ncalibration\\nconvolutional neural nets\\ndexterous manipulators\\nfeature extraction\\nimage classification\\nimage colour analysis\\nimage segmentation\\nlearning (artificial intelligence)\\nmulti-robot systems\\nrobot vision\\ntwo-stage transfer learning approach\\nmultiobjective convolutional neural network\\nheterogeneous robot arms\\neye-to-hand calibration\\nuniversal robots\\ntwo-stage transfer learning\\n2d colour image\\ncollision avoidance algorithms\\nfixed robot-camera setups\\ncollision detection\\nfactory floors\\ncollaborative robots\\n2d camera image\\n3d joint position estimation\\nheterogeneous robot detection\\nmultiobjective cnn\\ndata collection approach\",\"923\":\"arteries\\nelectromagnets\\nforce\\ntrajectory\\nmanipulators\\nvelocity control\\nnavigation\\nbiomechanics\\nbiomedical equipment\\nblood\\nblood vessels\\nmedical control systems\\npropulsion\\nswimmer designs\\nrotating millimeter-scale swimmers\\naorta\\nblood clot\\nrotational movement\\nhigh speed 3d navigation\",\"924\":\"cameras\\ntracking\\nrobot vision systems\\nrobot kinematics\\nface\\nbiomedical optical imaging\\neye\\nmedical image processing\\nmedical robotics\\noptical tomography\\npupil tracking accuracy\\ntracking bandwidth\\noptical coherence tomography imaging\\nsub-millimeter eye tracking accuracy\\ntracking eyes\\nstationary eyes\\ncommercial robot arm\\nfast axial tracking\\nreference arm adjustment\\nfine alignment\\nstereo pupil cameras\\ncoarse pupil cameras\\nfixed-base rgb-d cameras\\nautomatic eye imaging\\noct scanner capable\\nophthalmology offices\\noct screening\\nunconscious patients\\noct diagnostics\\nophthalmic photographers\\nchinrest stabilization\\ntabletop instruments\\nclinical ophthalmic oct systems\\nrobotically-aligned scanner\\nsize 12.0 mum\\ntime 83.2 ms\\nfrequency 9.7 hz\\noptical coherence tomography\\nimage stabilization\",\"925\":\"planning\\ntrajectory\\nlead\\nvehicle dynamics\\nrobots\\ndynamics\\nunmanned underwater vehicles\\nautonomous underwater vehicles\\ncollision avoidance\\nmobile robots\\ntrajectory control\\nunderwater robot\\nonline multilayered motion planning\\nloosely coupled multilayered planning design\\nmotion planner\\nhydro-dynamic forces\\ntrajectory planning\\nrobots onboard computer\\nauvs\\ninevitable collision states\",\"926\":\"task analysis\\nrobot sensing systems\\nhaptic interfaces\\nvisualization\\ngeometry\\nreinforcement learning\\nlearning (artificial intelligence)\\nmanipulators\\nhaptic feedback\\nvisual feedback\\nrobot controller\\ndeep reinforcement learning\\nhigh-dimensional inputs\\nsample complexity\\nmultimodal representation\\nsensory inputs\\npolicy learning\\npeg insertion task\\nself-supervised learning\\nmultimodal representations\\ncontact-rich manipulation tasks\",\"927\":\"tactile sensors\\ntraining\\ntime series analysis\\ndecoding\\ncontrol engineering computing\\nend effectors\\nfeature extraction\\nimage colour analysis\\nlearning (artificial intelligence)\\nmobile robots\\nneural nets\\nrobot vision\\nvisual perception\\ndeep visuo-tactile learning\\ntactile sensor data\\nwebcam\\ntactile properties\\nencoder-decoder network\\nlatent variables\\ntactile features\\nvisual features\\nrgb images\\nuskin tactile sensor\\nend-effector\\nfeature space\\nsawyer robot\",\"928\":\"navigation\\nroads\\ncameras\\nrobot sensing systems\\nvisualization\\npartitioning algorithms\\nglobal positioning system\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nprobability\\nvariational techniques\\npoint-topoint navigation algorithms\\nfull-scale autonomous vehicle\\nlocalization algorithm\\nvariational end-to-end navigation\\ndeep learning\\nautonomous vehicle control\\nraw sensory data\\nnavigation instruction\\nend-to-end driving networks\\npoint-to-point navigation\\nprobabilistic localization\\nnoisy gps data\\nraw camera data\\nhigher level roadmaps\\nprobability distribution\\ndeterministic control command\\nrough localization\\nreal-world driving data\\nvariational network\",\"929\":\"adaptation models\\ntraining\\ndata models\\nrobots\\ncomputational modeling\\ntrajectory\\ntask analysis\\nlearning (artificial intelligence)\\nmobile robots\\nreal world experience\\nsimulation parameter distribution\\npolicy training\\npolicy transfer\\npolicy behavior\\nsim-to-real loop\\nsimulation randomization\\nswing-peg-in-hole\\ncabinet drawer opening\",\"930\":\"position control\\nstrain\\nforce\\nrobots\\nshape\\npath planning\\nstandards\\nbiomechanics\\ncellular biophysics\\ndeformation\\nmanipulators\\nmedical robotics\\nmobile robots\\nneurocontrollers\\nrobotic orientation control\\ndeformable cells\\nrobotic manipulation\\ndeformable objects\\nrigid objects\\nrobotics\\ndeformable synthetic objects\\nrubber balls\\nclothes\\nbiological cells\\nmanual cell rotation control\\nrobotic approach\\nmathematical modeling\\nminimal cell deformation\\ncell damage\\nforce model\\nminimal force\\ncontact mechanics model\\nmanipulation path\\ncompensation controller\\noocyte orientation control\\nmaximum oocyte deformation\",\"931\":\"legged locomotion\\nrobot sensing systems\\nattitude control\\nestimation\\ngyroscopes\\nacceleration control\\nmotion control\\npath planning\\nstability\\nvelocity control\\nmonopedal jumping robots\\nonboard rate gyroscopes\\nencoders\\nattitude estimate disturbances\\nonboard velocity estimation\\nextreme stance accelerations\\nhigh-acceleration hopping\\nuntethered robot\\ndrift-free roll and pitch estimation\\ndrift-free roll and pitch attitude estimation scheme\\nfully autonomous stable hopping control\\nrectangular path\\nonboard dead-reckoning\\nhuman wireless joystick direction\",\"932\":\"task analysis\\nrobots\\nplanning\\ngames\\nrobotic assembly\\nsemantics\\nbinary decision diagrams\\nmobile robots\\npick-and-place tasks\\nbinary decision diagram\\nur5 robot\\nefficient symbolic reactive synthesis\\ncompositional approach\\nexplicit state approach\\nfinite-horizon tasks\",\"933\":\"trajectory\\nplanning\\noptimization\\nobservability\\ntask analysis\\nrobots\\ndecision trees\\ncomputational complexity\\nmobile robots\\noptimisation\\npath planning\\npartial observability\\noptimization-based approach\\ncompute optimal plans\\nsymbolic decision tree\\npath tree\\noptimal motion\\nindependent optimizations\\ncombined task and motion planning\\noptimization-based tamp methods\",\"934\":\"end effectors\\ntask analysis\\nrobot sensing systems\\npipelines\\nthree-dimensional displays\\nrobot kinematics\\ndesign engineering\\nmobile robots\\nobject detection\\nrobot vision\\nwarehouse automation\\nobject detection algorithms\\nmanipulation primitives\\ncubic objects\\nvacuum-based end-effector\\nsingle robot arm\\nrgb-d data\\nfailure conditions\\nrobust pipeline\\nunstructured piles\\npacking tasks\\norder fulfillment\\nhardware designs\\nsensor technologies\\nminimalistic end-effector\\ntowards robust product packing\",\"935\":\"electrodes\\nmuscles\\ncapacitance\\ngesture recognition\\nsensors\\nwrist\\nskin\\nbiomechanics\\nbiomedical electrodes\\nbiomedical measurement\\nbluetooth\\ncapacitive sensors\\nmedical computing\\nmicrocontrollers\\nrandom forests\\ntactile sensors\\ngesture recognition accuracy\\nbasic finger\\nflexible capacitive touch electrodes\\nclassification algorithms\\nrandom forest algorithm\\nwrist motions\\ncutkosky grasp taxonomy\\nbluetooth transceiver\\nmicrocontroller\\nelbow\\nwireless wearable device\",\"936\":\"force\\nrobot sensing systems\\nbiological system modeling\\ntask analysis\\nbiosensors\\nneural networks\\ncontrol engineering computing\\nforce feedback\\nlearning (artificial intelligence)\\nneural nets\\nrobots\\ntactile sensors\\nlearned force model\\nrobust learning\\ntactile force estimation\\nrobot interaction\\nanalytic models\\nrobust model\\nsyntouch biotac sensor\\nvoxelized input feature layer\\nspatial signals\\nsensor surface\\nrobust tactile force model\\nforce torque sensor\\nft sensor\\nforce inference\\nplanar pushing task\\nforce direction\\nforce estimation\\ntactile sensor signals\\nforce feedback grasp controller\",\"937\":\"actuators\\nforce\\nsensor phenomena and characterization\\ntextiles\\ncapacitive sensors\\nstrain\\nbiomechanics\\ndata gloves\\nelastomers\\nhandicapped aids\\ninjuries\\nneurophysiology\\npatient rehabilitation\\nsoft robotic glove\\nintegrated sensing\\nintuitive grasping assistance post spinal cord injury\\nmultiarticular textile actuators\\ncustom soft sensors\\nintuitive state machine intent detection controller\\npressurized actuators\\nnatural human fingers\\ntextile-elastomer capacitive sensors\\nfinger flexion\\nintuitive user control\\nstate machine controller\\nintegrated sensors\\nhand-object interactions\\ninjury levels\\ninadvertent grasp triggers\",\"938\":\"electrodes\\ntomography\\nactuators\\nwelding\\nshape\\nrobot sensing systems\\nbiological tissues\\ncomputerised tomography\\nelectric impedance imaging\\nelectric impedance measurement\\nimage reconstruction\\nmedical robotics\\npneumatic actuators\\nsurgery\\nreduced contact trauma\\nsoft tissues\\ntortuous paths\\nminimally invasive surgery\\nintraoperative shape sensing\\nproprioceptive soft actuator\\nself-sensing\\nelectrically conductive working fluid\\ntomographic reconstructions\\ntwo-degree-of-freedom designs\\nhydraulic hinged actuator\\npneumatic finger actuator\\neit images\\nfdm-eit\\nshape sensor\\nvariable stiffness soft robots\\nelectrical impedance tomography\\ndeformability\\nelectrical impedance measurements\\nfrequency division multiplexed eit system\\ntemporal resolution\",\"939\":\"robot kinematics\\nsurgery\\nforce\\ntools\\nadaptive control\\nretina\\nbiological tissues\\neye\\nforce feedback\\nforce sensors\\nmedical robotics\\nrobot vision\\n1-dimensional adaptive control method\\n3-dimensional control\\nsclera force components\\ntool insertion depth\\nvelocity-controlled johns hopkins steady-hand eye robot\\nsafe robot-assisted retinal surgery\\nsurgical tools\\ntool-to-eye interactions\\nrobotic light pipe holding\",\"940\":\"robot sensing systems\\ncollision avoidance\\nmerging\\nnavigation\\npartitioning algorithms\\nheuristic algorithms\\ndistributed control\\ngraph theory\\nhelicopters\\nmobile robots\\nmulti-robot systems\\ndistributed consensus\\nobstacle-free convex regions\\ndistributed multirobot formation splitting\\ndistributed multirobot formation merging\\nmoving obstacles\\nstatic obstacles\\nintersection graph\\nquadrotors\",\"941\":\"force\\ntactile sensors\\npiezoelectric transducers\\nsensor arrays\\ncontrol engineering computing\\ndexterous manipulators\\nfeature extraction\\nobject detection\\nservice robots\\ndomestic service robots\\ncontrol boards\\ntactile sensor unit\\nhand features\\nvisual data\\ntactile data\\neagle shoal\\nmodular tactile sensing dexterous hand\\nfully-actuated hand\\nembedded tactile sensors\\n2 degrees of freedom\\ndofs\\nperceive continuous vibration data\\ngrasp ability\\nconsumer market\\nrobotic manipulation research\",\"942\":\"training\\ndecoding\\nfeature extraction\\nrobots\\nimage reconstruction\\ngeometry\\nvisualization\\nimage colour analysis\\nimage retrieval\\nlearning (artificial intelligence)\\nobject detection\\nrobot vision\\ndaytime images\\nlearning scene geometry\\nvisual localization\\noutdoor large scale image\\ncross-season\\nlearned global image descriptor\\nscene geometry information\\ndepth map\\nquery image\\nlocalization accuracy\\ncross-weather\\nlong-term localization scenario\\nnight images\\nwinter localization sequence\\nsummer localization sequence\",\"943\":\"robot kinematics\\nrobot sensing systems\\nmanipulators\\nplanning\\nthree-dimensional displays\\nmonte carlo methods\\nmulti-robot systems\\nnavigation\\npath planning\\ntree searching\\nmotion planning\\nhigh-dimensional configuration space\\nmultirobot region-of-interest reconstruction\\ndec-mcts\\nmultiple robot arms\\nrgb-d sensors\\nprecision agriculture\\ninfrastructure inspection\\nviewpoint evaluation function\\nnonmyopic planning algorithm\\ndecentralised monte carlo tree search\\nnavigation graph\\nfruit detection\",\"944\":\"propellers\\nvehicle dynamics\\nsprings\\nforce\\nfasteners\\ndynamics\\nactuators\\naerospace components\\ndesign engineering\\nhelicopters\\nhinges\\nnonlinear dynamical systems\\nposition control\\npassively morphing quadcopter\\npassive rotary joints\\nrapid aerial morphing\\nsprung hinges\\nnonmorphing quadcopter\\nquadcopter controllers\\ntrajectory generation algorithms\\ncontrol inputs\\ngap traversal maneuvers\\nrigid connections\\nquadcopter design\\nnonlinear dynamics\",\"945\":\"planning\\nrobot sensing systems\\nthree-dimensional displays\\ntrajectory optimization\\nvehicle dynamics\\naerospace safety\\ncollision avoidance\\ngraph theory\\nnavigation\\nsearch problems\\ntrajectory optimisation (aerospace)\\nsensor visibility constraints\\nobstacle-free flight paths\\nvelodyne puck lite 3d laser scanner\\nflight dynamics\\nnavigation safety\\nallocentric complete planning\\nmicroaerial vehicle flight safety\\nsearch-based 3d planning\\ngraph search\",\"946\":\"conductors\\nwheels\\nsprings\\nmobile robots\\nblades\\nrotors\\ninspection\\nmaintenance engineering\\nmathematical analysis\\npower grids\\npower overhead lines\\npower transmission control\\nrobots\\nfield testing\\ninnovative robot\\nbundled high-voltage powerlines\\nrobotic platforms\\npower grid\\nhydro-qu\\u00e9bec\\nline maintenance technicians\\npassive obstacle-crossing system\\nlarge-scale inspection\\nbundled-type powerlines\\nlineranger prototype\\npowerline inspection efficiency\\nlinescout prototype\",\"947\":\"legged locomotion\\ndc motors\\nsprings\\ntorque\\ncouplings\\nfoot\\nactuators\\nkinetic power output\\nhigh power mode\\nlow-power modes\\nadjustable power modulation\\nmechanical systems\\nrobotic locomotor\\nactuator system\\nleg mechanism\\nterrestrial locomotion\\nenergetic performance\\nforce-torque ratio\\nflat terrain\\nfinite root generation method\",\"948\":\"drones\\ntrajectory\\nvehicle dynamics\\nsurface waves\\nplanning\\naerodynamics\\nautonomous aerial vehicles\\nhelicopters\\nmobile robots\\nmotion control\\nmulti-robot systems\\npath planning\\nmotion delays\\nperiodic swarm patterns\\nquadrotor swarm performances\\nintegrated unit\\ncoordinated unit\\nswarm motion primitives\\nflexible framework\\nchoreography design\\ntrajectory generation algorithms\\nperiodic motion pattern\",\"949\":\"task analysis\\nmotion segmentation\\ntrajectory\\nfeature extraction\\nneedles\\nsurgery\\nlearning (artificial intelligence)\\nmedical robotics\\ntransfer learning\\nsurgical task segmentation\\nsegmentation points\\nmanually labeled data\\ncorrelated features\\nsegmentation rule\\nhigh segmentation rates\",\"950\":\"optimization\\nbayes methods\\ntime measurement\\nnoise measurement\\nestimation\\nstandards\\nphase measurement\\ndata acquisition\\ngradient methods\\nkalman filters\\nlegged locomotion\\noptimal control\\nparameter estimation\\nkalman filter-based metabolic estimator\\nsoft exosuits\\nmetabolic estimator stopping process\\nhuman-in-the-loop optimization studies\\nwearable devices\\nimproved average metabolic reduction\\nslow metabolic dynamics\\nbayesian optimization\\ngradient descent optimization\",\"951\":\"prosthetics\\nexoskeletons\\nrobot kinematics\\nprototypes\\ntask analysis\\nend effectors\\nartificial limbs\\nbiomechanics\\nmedical robotics\\nmobile robots\\nmotion control\\northopaedics\\npatient rehabilitation\\n3d visual perception\\nsemiautonomous coordinated motion strategies\\napp-based programming framework\\nestablished standard sequential strategies all joints\\nhuman embodied dynamics model\\nrobot-based exoskeleton substitute\\nsoft-robotics design\\nintelligent coordinated control concepts\\nupper body\\ngravity effects\\nresidual limb\\nunnecessary interaction forces\\ncentral goal\\nprostheses\\nrobot-based emulation prototype\\nsoft-robotics enabled upper-limb exoprosthetics\\nstrategy goals\",\"952\":\"force\\ngrippers\\ninstruments\\nwires\\nmanipulators\\nhoses\\nbiological tissues\\ninjuries\\nmedical robotics\\nneedles\\nneurophysiology\\nsurgery\\nneurosurgical robot\\nnervous tissues\\nrobotic surgical instrument\\nnerve damages\\nnerve bundles\\nflexible peripheral nerves\\nactive microneedle arrays\\npassive microneedle arrays\\nminiature suction-gripper\\nperipheral nerve\\nsuction mechanism\",\"953\":\"three-dimensional displays\\ncollision avoidance\\nshape\\nvehicle dynamics\\nposition measurement\\nsensors\\ntopology\\naircraft control\\nautonomous aerial vehicles\\ndistributed control\\nhelicopters\\nmobile robots\\nposition control\\nrobust control\\ndistributed control strategy\\nlocal relative position measurements\\ncollision avoidance strategy\\nrobust 3d distributed formation control\\n3d formation\\nmultirotor aerial vehicles\\nquadrotors\\nmulti-robot systems\\ndistributed robotic systems\\n3d formation control\\ndistributed collision avoidance\",\"954\":\"delays\\nservers\\nuplink\\ndownlink\\npredictive models\\nunmanned aerial vehicles\\nmathematical model\\nautonomous aerial vehicles\\ncompensation\\ncontrol nonlinearities\\ngaussian processes\\nmobile robots\\nnetworked control systems\\nnonlinear control systems\\npath planning\\npredictive control\\nstability\\nstate estimation\\nstate feedback\\ntime-varying systems\\ngaussian process-based delay compensation\\nmodel predictive control\\ntime-varying network delay\\nuav control system\\ndelayed state feedback\\nmultirotor-type uavs\\nnetworked control system\\nuav networked operation\",\"955\":\"aerodynamics\\nrobots\\nvehicle dynamics\\nforce\\ntorque\\nanimals\\naerospace components\\naerospace robotics\\naircraft control\\nautonomous aerial vehicles\\nclosed loop systems\\ncontrol system synthesis\\nlearning (artificial intelligence)\\nmicrorobots\\nmobile robots\\nmotion control\\nnonlinear control systems\\nrobot dynamics\\nrobot kinematics\\nstability\\nflappy hummingbird\\nopen source dynamic simulation\\nflapping wing robots\\nhummingbirds\\nextraordinary flight performance\\nstable hovering maneuvering\\naggressive maneuvering\\nconventional small scale man-made vehicles\\nfwmavs\\nperformance gap\\nopen source high fidelity dynamic simulation\\noptimization\\nflight control\\nat-scale hummingbird robot\\nsystem identification\\ndynamic response\\nopen-loop\\nloop systems\\nsimulated flights\\nexperimental flights\\nhighly nonlinear flight dynamics\\ncontrol problems\\ncontrol algorithms\\nlinear controller\\ncontrol policy\\nsimulation-to-real transfer\\nphysical robot\\nflapping wing microair vehicles\",\"956\":\"planning\\ndatabases\\ntask analysis\\nvisualization\\nrobots\\nprobability distribution\\nfeature extraction\\ncollision avoidance\\ngaussian processes\\nimage sampling\\nlearning (artificial intelligence)\\nmanipulators\\nmobile robots\\nrobot vision\\ntrees (mathematics)\\ngaussian mixture models\\nrapidly-exploring random tree\\nbiased sampling methods\\nreal-time applications\\nlonger planning times\\noptimization-based methods\\ncomplex environments\\nsampling-based motion planners\\nrobot manipulation\\nvisual repetition sampling\\nrrt motion planner\\nsampling efficiency\\nvisual input\\ngmm\",\"957\":\"task analysis\\ncollision avoidance\\nrobot sensing systems\\naerospace electronics\\nrobot kinematics\\njacobian matrices\\nmobile robots\\nuncertain environments\\nunexpected contact\\nsafe robot behavior\\ncontact-driven approach\\nsafe robot operation\\ninteractive robot operation\\nunforeseen contact events\\nrobot tasks\\nrobot model\\nfreedom robot arm\\nsafe contact behavior\\nrobot posture requirements\\ncontact-driven posture behavior\",\"958\":\"estimation\\nconvolutional codes\\ncameras\\nspatial resolution\\nthree-dimensional displays\\ntraining\\nconvolutional neural nets\\nimage classification\\nimage motion analysis\\nimage resolution\\nimage sampling\\nlearning (artificial intelligence)\\npose estimation\\nself-supervised monocular depth estimation\\nself-supervised monocular depth prediction\\nsubpixel convolutional layer extension\\ndepth super-resolution\\nhigh-resolution disparities\\nlow-resolution convolutional features\\nflip-augmentation layer\\nsingle-image super-resolution\\ndeep learning methods\\nsuper-resolved monocular depth estimation\\npublic kitti benchmark\",\"959\":\"mobile robots\\ntask analysis\\npayloads\\nhumanoid robots\\nmanipulators\\nnavigation\\nfeedback\\nmotion control\\nrobot dynamics\\nwheels\\ntwo-wheeled dynamically stable mobile manipulator robots\\nheavy object\\nballbot\\nfeedback control laws\\ndynamically stable mobile robot\\nspherical-wheel robots\\nunknown mass\\nquasistatic center of mass computation\\nfeedforward control laws\\nmass 15.0 kg\\nmass 10.0 kg\",\"960\":\"image segmentation\\nsemantics\\ncameras\\nthree-dimensional displays\\nreal-time systems\\ntraining\\nrobots\\nfeature extraction\\nimage classification\\nimage enhancement\\nimage matching\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobot vision\\nstereo image processing\\nimage-based underwater obstacle detection\\nsparse stereo point clouds\\nmonocular semantic image segmentation\\ncluttered underwater environments\\nrobust robotic path planning\\nfeature-based stereo matching\\nlearning-based segmentation\\nrobust obstacle map\\ndirect binary learning\\nunderwater obstacles\\nmulticlass learning approach\\nbinary map\\nsparse stereo matching\\n3d obstacle maps\\ncoral reef environments\\nimage-wide obstacle detection\\ndynamic objects\\nimage-based obstacle maps\",\"961\":\"cameras\\nrobot vision systems\\nsimultaneous localization and mapping\\nlaparoscopes\\nthree-dimensional displays\\nrobot kinematics\\nimage motion analysis\\nimage reconstruction\\nmedical robotics\\nmobile robots\\npose estimation\\nrobot vision\\nslam (robots)\\nsurgery\\nlaparoscopic camera motion\\nrcm constraints\\nminimal solver\\nabsolute camera\\n2d-3d point correspondences\\nbundle adjustment optimiser\\nrcm-constrained parameterisation\\nrelative pose estimation\\nslam pipeline suitable\\nrobotic surgery\\nrcm position\\nrobotic prostatectomy show\\nrcm-slam\\nvisual localisation\\nremote centre\\nmotion constraints\\ninsertion ports\\nsimultaneous localisation and mapping\\nmapping approach\\nrcm-pnp\",\"962\":\"robot sensing systems\\ncollision avoidance\\nspirals\\nswarm robotics\\ntask analysis\\ncameras\\ndeterministic algorithms\\nmulti-robot systems\\nstochastic processes\\nrobot swarms\\ncollective robot foraging\\ncentral-place foraging algorithm\\ncpfa\\ndistributed deterministic spiral algorithm\\nddsa\\nswarm robotic algorithms\\nbioinspired stochastic foraging strategy\\nresource-collection algorithms\",\"963\":\"legged locomotion\\nwheels\\ngears\\ntorque\\nforce\\nrobot kinematics\\nmobile robotic platform\\ngear ratio\\nwheel-leg reconfigurable mechanism\\npassive gears\\nmobile robot applications\\npassive wheel-leg transformation mechanism\\ncentral gear\\nseamless circular wheel\\nleg mode\\ngeared structure\\nobstacle climbing\\nlocomotion capabilities\",\"964\":\"vehicle dynamics\\ntraining\\npredictive models\\ntask analysis\\ncomputer architecture\\nrecurrent neural networks\\ncorrelation\\nconvolutional neural nets\\nlearning (artificial intelligence)\\nmonte carlo methods\\nneural net architecture\\noptical radar\\nrecurrent neural nets\\nlong-term occupancy grid prediction\\nscene evolution\\nautomated driving\\nlidar grid fusion\\nbirds eye view\\nrnns\\ncnn architecture\\nconvolutional long short-term memories\\nconvlstms\\nmonte carlo approach\",\"965\":\"simultaneous localization and mapping\\noptimization\\nmaximum likelihood estimation\\nposition measurement\\nnoise measurement\\ntransmission line matrix methods\\ngraph theory\\nmobile robots\\nnavigation\\nnonlinear programming\\npath planning\\npolynomials\\npose estimation\\nrobot vision\\nslam (robots)\\nautonomous navigation\\nnonlinear optimization techniques\\nmaximum likelihood estimate\\nrobot trajectory\\npolynomial optimization programs\\nplanar pose graph\\nlandmark slam\\nsparse-bounded sums-of-squares programming\\npose-graph slam problem\\nsum-of-squares convex\\nsos convex\\nsparse bounded degree sum-of-squares optimization method\\nsparse-bsos optimization method\",\"966\":\"monte carlo methods\\ntrajectory\\npredictive control\\noptimization\\nsafety\\noptimal control\\nnonlinear dynamical systems\\nrobust control\\nsampling methods\\nstochastic systems\\ntrust regions\\nsafe sampling-based model predictive control\\nnonlinear control systems\\ncomplex dynamics\\nnonlinear dynamics\\nsampling-based mpc scheme\\nsampling based estimation\\nsafe constraint satisfaction\\nprobabilistic information\",\"967\":\"safety\\ndimensionality reduction\\noptimal control\\nlevel set\\ncollision avoidance\\ntrajectory\\ngame theory\\nmobile robots\\nnonlinear dynamical systems\\nreachability analysis\\nhj computation\\nhamilton-jacobi reachability\\nrobotic systems\\nnonlinear system dynamics\\nsafety-preserving controllers\\ncomputational scalability\\ncontinuous state dimensions\\ncomputational burden\\nsystem decomposition methods\\ncoupled hj formulation\\nleaking corners removal\\nsafety verification\\ncomputational scalability limits\\nvehicle obstacle avoidance problem\\n5d car model\",\"968\":\"cloud computing\\nautomation\\ncomputer architecture\\nedge computing\\nproduction\\nprocess control\\nmanufacturing\\nagile manufacturing\\nembedded systems\\nproduct development\\nproduction engineering computing\\ndigital shadow\\nagile product development\\nproduction systems\\nautomation pyramid\\ninterconnected cyber physical systems\\nadaptive process control\\nlife cycle data management\",\"969\":\"task analysis\\nnull space\\nrobot kinematics\\ntrajectory\\njacobian matrices\\noptimization\\npath planning\\nposition control\\nredundant manipulators\\nauxiliary null space tasks\\ntask achievement\\nnull space task\\nmultiple prioritized tasks\\ntime scaling-based relaxation\\nprimary task\\nkinematic redundancy\\nrobot manipulators\\nsafety criterion\\noptimization criterion\\nconstraint relaxation\\ntime scaling schemes\\ndlr lightweight robot\\nkuka lightweight robot\",\"970\":\"fasteners\\nsoft robotics\\nthree-dimensional printing\\ntask analysis\\nshape\\nbuildings\\nactuators\\nintelligent robots\\nmanipulators\\nrapid prototyping (industrial)\\nshape memory effects\\nsoft robotic systems\\nautomated design process\\nmonolithic soft robotic structures\\npre-defined end poses\\nshape memory structures-automated design\\nmonolithic soft robot structures\\nadditive manufacturing methods\\n3d-printable\",\"971\":\"collision avoidance\\nstructural beams\\nprototypes\\nservomotors\\nrobot sensing systems\\nmathematical model\\nactuators\\nbeams (structures)\\nbuckling\\ndesign engineering\\nelastic constants\\nindustrial robots\\nsafety benefits\\ncompact design\\nrotating beams\\nlateral stiffness ratio\\nparallel guided beams\\nbeam roots\\nmechanics model\\ndesign concept\\nrotating beam link\\nvariable stiffness robotic arms\\ncolumn buckling\",\"972\":\"sensor arrays\\nresonant frequency\\nfabrication\\nrobot sensing systems\\nshafts\\npower capacitors\\ndamping\\nfourier transforms\\noscillators\\npolymers\\nrapid prototyping (industrial)\\nthree-dimensional printing\\nmixed-frequency signals\\nmechanical fourier transform\\ndamped oscillator system\\nadditively manufactured soft whisker-like sensor array\\nsingle-step additive manufacturing approach\\nbioinspired soft whisker-like sensors\\npolymer based whisker-array\",\"973\":\"task analysis\\nactuators\\nstrain\\nrobot sensing systems\\nlayout\\ncapacitive sensors\\ndeformation\\ndexterous manipulators\\nelastic constants\\npneumatic actuators\\nsensors\\nsensorized actuators\\nsoft actuators\\nmultitask sensorization\\nsensor hardware\\nmultitask method\\nrbo hand 2\\npneuflex actuator\\nsensor placement\\nsoft actuator\\ntask-relevant deformations\\nsoft robotic actuators\",\"974\":\"legged locomotion\\nmorphology\\nservomotors\\ncomputer architecture\\nrobot sensing systems\\nswitches\\nadaptive systems\\nrobot dynamics\\nself-reconfiguration\\nquadruped robots\\ndyret\\nembodied testing\\ndynamic robot morphology\\nlocomotion modes\\nself-reconfigurable morphology\\nservo supply voltage\\nfour-legged robot\\nself-modifying morphology experiments\\nuncontrolled outdoor environments\",\"975\":\"valves\\nlegged locomotion\\nhumanoid robots\\nactuators\\nvelocity control\\nhydraulic systems\\nposition control\\nrobot dynamics\\nhigh-efficiency hydraulic direct-drive system\\nbiped humanoid robot-comparison\\nhigh-power large electrical motors\\nmechanical transmission systems\\nvalve-based control system\\nposition-following capability\\nenergy consumption\",\"976\":\"mobile robots\\nrobot kinematics\\nwheels\\nrobot sensing systems\\npoles and zeros\\nacceleration\\nmotion control\\nnonlinear control systems\\npendulums\\nbalance control\\nfast movements\\nnarrow support\\nreaction wheel pendulum\\nmotion commands\\nhigh-performance robotic balancing\",\"977\":\"legged locomotion\\ndc motors\\nsensors\\nclamps\\nlaser beam cutting\\nshafts\\naccelerometers\\ncameras\\ncontrol engineering computing\\ngyroscopes\\noperating systems (computers)\\ndurable open-source hexapedal platform\\nhexapedal robot\\nonboard single-board computer\\nlaser cutter\\nbeacon sensors\\ncolor vision sensors\\nlinescan sensors\\nlegged robot\\ncontinuous walking burn-ins\\nstatic payload\\ndynamic payload\\nrobot operating system\",\"978\":\"frequency modulation\\nforce\\nconferences\\nautomation\\nindexes\\ndeburring\\nforce control\\nend effectors\\nfeedback\\nforce sensors\\ngeometry\\ngrinding\\ngrinding machines\\nindustrial robots\\nmachine tool spindles\\npolishing\\nposition control\\nprototypes\\nquality control\\nsensorless force control\\ncontroller feedback\\nconstant contact force\\nforce regulation mechanism\\ngrinding spindle tool\\nindustrial grinding-deburring operations\\nmultiaxis force sensor\\npolishing quality\\ngrinder prototype\\ncompliant mechanism\\nend-effector\\nautomated deburring and polishing\\nconstant force\\nzero stiffness\",\"979\":\"task analysis\\ndynamics\\nimpedance\\noptimization\\nfeedback control\\nrobot kinematics\\nclosed loop systems\\nfeedback\\nlegged locomotion\\nmotion control\\noptimisation\\nposition control\\ncontrol inputs\\noperational space inverse dynamics control\\nconstrained prioritized multiobjective optimization-base control formulation\\ndynamic-model-free prioritized feedback control formulation\\ncombined inverse dynamics impedance controller\\nplanar anthropometric biped robot\\nstable locomotion\",\"980\":\"task analysis\\nrobot kinematics\\nmanipulators\\noptimization\\nplanning\\njacobian matrices\\ncompliance control\\ncontrol system synthesis\\nend effectors\\nforce control\\nhumanoid robots\\nmanipulator dynamics\\nmanipulator kinematics\\nmobile robots\\nmotion control\\noptimal contact force control\\nsupport plane\\ncontact control point\\nthree-level hierarchical compliance controller\\nnonend-effector support contact\\nwrist level manipulation\\nupper arm\\nelbow joint\\narm joints\\nloco-manipulation tasks\\nenvironment constraints\\nrobot arm\\nmanipulation effort reduction\\nenvironment support contacts\\ncontrol scheme\\ninteraction forces\\nimpedance control\",\"981\":\"legged locomotion\\ntask analysis\\nrobot kinematics\\nactuators\\nelasticity\\nforce\\ngait analysis\\nmotion control\\nposition control\\ncoordinate-based approach\\nstatic balancing\\nelastically actuated legged robots\\nmotor positions\\nbijective relation\\nlink positions\\nstatic external forces\\nfully determined system\\nvertical foot positions\\nwalking task\\nimposed constraints\\ncom\\ncompliantly actuated legged robots\",\"982\":\"legged locomotion\\nimpedance\\njacobian matrices\\nstability analysis\\ndamping\\ntorque\\nactuators\\nforce control\\nposition control\\nrobot kinematics\\nstability\\njoint kinematic configuration influence\\nimpedance-controlled robotic leg\\nlegged robots\\nimpedance controller\\ninner force loop gains\\npassivity conditions\\njoint configurations\\nleg workspace\\nactuation bandwidth\\npassive impedance\\nz-width diagram\\nnyquist plot\",\"983\":\"impedance\\nrobot sensing systems\\nrobot kinematics\\ntask analysis\\ndamping\\nservice robots\\nadaptive control\\nagricultural robots\\nhuman-robot interaction\\nmobile robots\\nrobot dynamics\\nself-adjusting systems\\ncontext-aware\\nadaptive interaction\\nself-tuning impedance controller\\nrobot quasistatic parameters\\nrobot sensory data\\nvision module\\nrobot interaction autonomy\\nrobot sensory vision\\nautonomous robot behaviours\\nstiffness\\nagricultural task\",\"984\":\"adaptation models\\ntask analysis\\nsensors\\ncomputational modeling\\ndata models\\nhardware\\ntraining\\nbody sensor networks\\nfeature extraction\\nimage classification\\nimage motion analysis\\nlearning (artificial intelligence)\\npose estimation\\npersonalized online learning\\nwhole-body motion classes\\nonline action classification\\nmachine learning applications\\npersonal behavior patterns\\noffline average user models\\npersonalized models\\nmotion sequences\\ninertial measuring units\",\"985\":\"robots\\nvisualization\\ntraining\\nfeature extraction\\ntask analysis\\nartificial neural networks\\nsemantics\\ndata mining\\nlearning (artificial intelligence)\\nmobile robots\\nobject recognition\\nrobot vision\\ndeep learning architecture\\ndeep network\\ndeep extension\\nnonparametric model\\nautonomous mining\\nrobot platform\\ndeep open world recognition\\nvisual knowledge gaps\\nopen set recognition\\nvisual modules\",\"986\":\"robot sensing systems\\nrobot kinematics\\nthree-dimensional displays\\nmorphology\\ntask analysis\\nmanipulators\\nimage colour analysis\\nlearning (artificial intelligence)\\nmanipulator kinematics\\nmobile robots\\nmotion control\\nmulti-robot systems\\nrecurrent neural nets\\nrobot vision\\nlow-cost rgb-d camera output\\ntask sharing\\nshared communication protocol\\ncentralized planner\\nshared action\\nkinematic model\\nlarge-scale data\\nrnn-based methods\\nunscripted movement observation\\nrobots morphological structure\",\"987\":\"three-dimensional displays\\nautomobiles\\nlaser radar\\nlabeling\\nrobot sensing systems\\nglobal positioning system\\nobject detection\\nobject tracking\\noptical radar\\noptical scanners\\nroad traffic\\nstereo image processing\\nlarge-scale 3d point cloud dataset\\ncrowded urban scenes\\nhonda research institute 3d dataset\\ntracking dataset\\n3d lidar scanner\\nhighly interactive traffic scenes\\nh3d dataset\",\"988\":\"feature extraction\\nimage segmentation\\nsemantics\\ntask analysis\\nrobots\\ncognition\\npredictive models\\nlearning (artificial intelligence)\\nmanipulators\\nrobot vision\\nimage-level reasoning\\njoint learning model\\nvisible region masks\\noccluded region masks\\ninstance occlusion segmentation\\nsemantic occlusion segmentation\\ninstance segmentation model\\nfeature extractor\\nrobotic pick-and-place tasks\",\"989\":\"needles\\ntrajectory\\nkinematics\\nmeasurement\\ntools\\ntask analysis\\nrobots\\nfeature extraction\\ngaussian processes\\ngesture recognition\\nimage classification\\nimage motion analysis\\nimage representation\\nimage segmentation\\nmedical image processing\\nmedical robotics\\nrobot kinematics\\nsurgery\\ntrajectory control\\nunsupervised learning\\naction recognition\\nsurgical trajectories\\nground truth annotations\\nsurgical demonstrations\\nkinematic trajectories\\nsurgical robots\\nsurgical gestures\\nautomatic segmentation\\nsurgical skill assessment\\nsurgical automation\\nunsupervised learning methods\\naction units\\nsupervised recognition approaches\\ngmm-based algorithm\\ntask-agnostic initialization methods\\nclassification\\ngaussian mixture models\\nrobotic surgery\\nsurgical gesture recognition\",\"990\":\"cameras\\nvisualization\\nnavigation\\nsensors\\nacceleration\\nnoise measurement\\nfuses\\naugmented reality\\nimage sensors\\ninertial navigation\\nmobile computing\\nmobile robots\\nrobot vision\\nslam (robots)\\ninertial sensors\\nvisual sensors\\nvisual-inertial navigation systems\\nmobile augmented reality\\naerial navigation\\nautonomous driving\\nvins\\nslam\",\"991\":\"cameras\\nreal-time systems\\nnavigation\\ncomputer architecture\\nlaser radar\\nvisualization\\nsystems architecture\\nautomobiles\\nclosed loop systems\\nmobile robots\\nmulti-robot systems\\nrobot vision\\nbasic autonomy features\\nrobust algorithms\\nmultisensor visual localization solution\\nwinning self-driving car\\nsae autodrive challenge\\nlevel 4 autonomous vehicle\\nyuma\\narizona\\nzeus' complete system architecture\\ncpu\\nclosed-loop performance\",\"992\":\"vehicle dynamics\\nautonomous vehicles\\nplanning\\ngames\\ntrajectory\\ncomputational modeling\\ndecision making\\ngame theory\\npath planning\\nremotely operated vehicles\\nroad traffic control\\nroad vehicles\\ntrajectory control\\ngame-theoretic trajectory planning algorithm\\ntrajectory optimization\\ndynamic games\\nautonomous driving technology\\ndrivers\\ndynamic game theory\\nhierarchical game-theoretic planning\\nhuman driver\\nautonomous vehicle\\nplanning horizon\\nsimplified information structure\\nshort-horizon tactical game\\nlong-horizon strategic game\",\"993\":\"cameras\\nimage coding\\ntools\\nroads\\nautomobiles\\nautonomous vehicles\\ncomputer vision\\ndriver information systems\\nimage recognition\\nroad traffic\\nvisual perception\\ncamera settings\\nweather conditions\\ntraffic sign images\\nrussian winter roads\\nrussian traffic code\\nicevisionset\\nlossless video dataset\\ntraffic sign annotations\\ntraffic signs\\nimage data\\ncomputer vision systems\",\"994\":\"displacement measurement\\nunmanned aerial vehicles\\ndistance measurement\\nvisualization\\noptical variables measurement\\noptical feedback\\noptical saturation\\nautonomous aerial vehicles\\ncomputer vision\\nglobal positioning system\\nmobile robots\\nrobot vision\\nultrawideband ranging sensor\\napproaching phase\\nautonomous approaching landing capabilities\\nvision-based techniques\\ngps-denied environments\\nautonomous docking\\nintegrated uwb-vision approach\\nvision-derived poses\\nuwb measurements\\nonboard vision system\\nrelative displacement measurements\\nuav relative\",\"995\":\"trajectory\\ncognition\\noptimization\\nhidden markov models\\npredictive models\\ngeometry\\nadaptation models\\ninference mechanisms\\noptimisation\\nregression analysis\\nroad traffic control\\ntraffic engineering computing\\nubiquitous computing\\nonline vehicle trajectory prediction\\ntwo-level vehicle trajectory prediction framework\\nurban autonomous driving\\ncomplex contextual factors\\ntraffic regulations\\nmoving agents\\nhigh-level policy anticipation\\nlow-level context reasoning\\nshort-term memory network\\nsequential history observations\\nlow-level optimization-based context reasoning process\\noptimization-based reasoning process\\ntwo-level reasoning process\\ncontinuous trajectory\\nregression-based trajectory prediction methods\\nvehicle motions\",\"996\":\"robot sensing systems\\nsonar\\nmicrophones\\nsonar navigation\\nuniversal serial bus\\nultrasonic imaging\\nbioacoustics\\nbiomimetics\\nmobile robots\\nnavigation\\npath planning\\nflexible low-cost biologically inspired sonar sensor\\nrobotic applications\\nbiomimetic sonar experiments\\nautonomous sonar navigation\\nultrasound\\nsubsumption architecture\\nautonomous navigation control system\\np3dx robotics platform\\nbig-eared bat\\nsonar sensor platform\\nbiomimetic control mechanisms\\nmicronycteris microtis\\necholocating animals\",\"997\":\"trajectory\\nnavigation\\nrobot kinematics\\nclustering algorithms\\nreinforcement learning\\nassisted living\\ngeriatrics\\nhuman-robot interaction\\nlearning (artificial intelligence)\\nmobile robots\\npath planning\\nrobust control\\nservice robots\\ntelerobotics\\nnonexpert users\\nclusternav\\ncluttered environments\\nrobust autonomous navigation\\nsocial robots\\nelderly users\\ntraditional model-based navigation techniques\\nstable theoretical foundation\\npractical foundation\\nautonomous operation\\ndomestic environments\\nacceptable behaviour\\nnovel learning-based technique\\ngeometric representation\\nacceptable manner\\nelderly care facility\\ntraditional model-based approach\\nlearning-based robust navigation\",\"998\":\"neuromorphics\\nneurons\\nbiological neural networks\\ncomputer architecture\\nrobot sensing systems\\nsociology\\ncontrol engineering computing\\nfeedback\\nlearning (artificial intelligence)\\nmobile robots\\nneural chips\\nneurocontrollers\\nadaptive motor control\\nmixed-signal neuromorphic processor\\nneuromorphic computing\\nspiking neural network architecture\\nsensory feedback\\ncontrol rotational velocity\\nrobotic vehicle\\ncorrect motor command\\nminiature mobile vehicle\\ntwo-layer spiking neural network\\nneuromorphic chip\\npurely neuromorphic motor control\\nspiking neurons\\nneuromorphic device\\non-chip plastic synaptic weights\",\"999\":\"genomics\\ntopology\\nsociology\\nstatistics\\nartificial neural networks\\nnetwork topology\\nautonomous aerial vehicles\\ncollision avoidance\\ngenetic algorithms\\nlearning (artificial intelligence)\\nneurocontrollers\\nautonomous agents\\nneural networks\\nnn\\nevolutionary algorithm\\nstate-to-action mapping model\\nneuroevolution process\\npopulation diversity\\nunmanned aerial vehicle collision avoidance problem\\nadaptive genomic evolution\\nneural network topologies\\naugmented topologies formalism\\nopen ai platform\\nuav collision avoidance problem\",\"1000\":\"neurons\\ntarget tracking\\ntask analysis\\nsynapses\\nrobot sensing systems\\ntraining\\nlearning (artificial intelligence)\\nmobile robots\\nneurocontrollers\\npath planning\\nmultilayered snn\\ntarget tracking snake-like robot\\nend-to-end learning approach\\nr-stdp\\nsnn controller\\ntarget tracking tasks\\nreward-modulated spike-timing-dependent plasticity\\nmultilayered spiking neural network\\nlearning algorithms\\nlateral tracking\",\"1001\":\"robot sensing systems\\nrobot kinematics\\nnoise measurement\\ntask analysis\\nswarm robotics\\nanalytical models\\ndecision making\\nmulti-robot systems\\nstochastic processes\\ntime-varying cross-inhibition\\ndecentralised decision-making\\nrobot swarm\\ndiffusive search\\ndecentralised algorithm\\nhouse-hunting honeybees\\nsingle decentralised parameter\\nbalance exploration\\nswarm robotics simulations\\ncollective decision accuracy\",\"1002\":\"manipulators\\nloading\\ntask analysis\\nrobot kinematics\\nkinematics\\nmobile robots\\ncollision avoidance\\nmotion control\\nprobability\\nobstacle avoidance\\nrobotic agents\\nprobabilistic road maps technique\\ncooperative loading task\\nredundant static manipulator\\nmobile platform\\nstatic obstacles\\ncluttered workspace\\ncontrol architecture\\ndecentralized motion planning\\nmotion planning scheme\\nconvergence properties\\nmotion control scheme\\noptimal loading configuration\",\"1003\":\"robot kinematics\\nrobot sensing systems\\ncollision avoidance\\ntask analysis\\ninterference\\nanalytical models\\ndecentralised control\\nmobile robots\\nmulti-robot systems\\ndecentralized interference reduction\\ndensely-packed robot swarms\\nconfined regions\\nphysical space-forces robots\\ndecentralized algorithm\\ndistributed collection task\\nspatial interference\",\"1004\":\"robot kinematics\\nrobot sensing systems\\nmobile robots\\nwheels\\narea measurement\\nmulti-robot systems\\nnavigation\\npath planning\\nrandom processes\\nrobot redundancy\\noptimized random walk\\nperformance improvements\\nsub-millimeter-sized robots\\nspatial coverage\\nanonymous robots\\ntwo-dimensional space\\nextremely simple robots\\nrun-time computation\\ncomputer simulations\\ndeterministic controller\\noff-line optimization\\nphysical e-puck robots\",\"1005\":\"feature extraction\\nconvolution\\nvehicles\\nvisualization\\nstreaming media\\ncomputer architecture\\nlogic gates\\nalarm systems\\nbelief networks\\nimage sequences\\nlearning (artificial intelligence)\\nneural nets\\npsychology\\ntraffic engineering computing\\nvideo signal processing\\ndriver intention detection\\nemergency flashers\\nturn signals\\nstops\\nlane changes\\nsudden events\\nvisual signals\\ndeepsignals\\ntemporal information\\nspatial information\\ndeep neural network\\nvideo sequences\\npotentially critical reaction time\",\"1006\":\"real-time systems\\ntask analysis\\npredictive models\\ntwo dimensional displays\\nimage sequences\\ncameras\\nactivity recognition\\ncontrol engineering computing\\nimage colour analysis\\nmobile robots\\nobject detection\\nobject tracking\\npedestrians\\nroad traffic\\ntraffic engineering computing\\ncomplex environments\\nvulnerable road users\\nintent action prediction\\nurban traffic environments\\nmonocular rgb camera\\ntracking-by-detection technique\\nspatio-temporal densenet model\\nautonomous ground vehicles\\nreal-time intent prediction\",\"1007\":\"trajectory\\ncameras\\noptical imaging\\ndecoding\\nvideos\\npredictive models\\nadvanced driver assistance systems\\ndriver information systems\\nimage coding\\nimage sequences\\nrecurrent neural nets\\nroad vehicles\\negocentric vision-based future vehicle localization\\nintelligent driving assistance systems\\nsafety-critical applications\\nautonomous driving\\ntarget vehicles\\nfirst-person view\\nego-vehicle\\nmultistream recurrent neural network encoder-decoder model\\nobject location\\npixel-level observations\\nfuture motion\\nprediction accuracy\\nintelligent vehicles\\nautomated vehicles\\nmotion planning capability\\nvehicle trajectories\\ndense optical flow\",\"1008\":\"trajectory\\nvehicles\\npredictive models\\nsensors\\nuncertainty\\nautonomous systems\\ntask analysis\\ndriver information systems\\nneural nets\\nroad safety\\nroad traffic\\nroad vehicles\\nvariational neural network approach\\nmultiple sensors\\nconditional variational distribution\\nconfidence estimate\\ndifferent time horizons\\nadditional predictors\\nvariational predictor\\nphysics-based predictor\\nconfidence estimations\\nsystem performance\\nvehicle autonomy\\nreal-world urban driving data\\nprediction error\\nphysics-based model\\nuncertainty-aware driver trajectory prediction\\nurban intersections\\nadvanced driving systems\\nshared control\\nautomation systems\\nuncertain situations\\ndriver trajectory distributions\",\"1009\":\"robots\\nuncertainty\\npredictive models\\nbuildings\\nprobabilistic logic\\ndata models\\ncollision avoidance\\nmobile robots\\nnavigation\\npath planning\\npedestrians\\nstochastic processes\\npedestrian flow patterns\\npartial observations\\nsafe robot navigation\\nspatial constraints\\ntemporal constraints\\nmultiple poisson processes\\nlong-term pedestrian datasets\\nuninformed exploration strategies\\nhuman motion patterns\\nrobot navigation\",\"1010\":\"climbing robots\\nforce\\nrobot kinematics\\ntask analysis\\nadhesives\\ntorque\\nactuators\\ndesign engineering\\nlegged locomotion\\nclimbing robot\\nrobust external transitioning capabilities\\nrobust internal transitioning capabilities\\n4-way external transitions\\n4-way internal transitions\\nindepth force analysis\\nrobust transitioning capabilities\\ntwo-wheg miniature\\nplane-to-plane transitioning\",\"1011\":\"bridges\\nclimbing robots\\nforce\\nwheels\\ninspection\\nbridges (structures)\\ncables (mechanical)\\ndesign engineering\\nlegged locomotion\\ncable-stayed bridge inspection\\ndesign method\\nbio-inspired climbing robotic technology\\npalm-based gripping module\\nclimbing gait\\ntrajectory algorithm\\nmass 25.0 kg\\nmass 30.0 kg\\nsize 1.1 m\",\"1012\":\"legged locomotion\\nfoot\\nkinematics\\nmathematical model\\nrobot kinematics\\nrobot sensing systems\\ngait analysis\\nhumanoid robots\\nmicrorobots\\nrobot dynamics\\ndynamic modeling\\nfoot placement control\\ncomprehensive dynamic model\\nminiature foldable robot\\ngait planning\\nfoot placement\\nminiature robots\\nminiaq-ii robot\\norigami-inspired robots\\nfoldable robots\\nlegged robots\\ndynamics\",\"1013\":\"clustering algorithms\\ntraining\\nprototypes\\nrobots\\nbuffer storage\\npartitioning algorithms\\nneural networks\\ncognition\\nlearning (artificial intelligence)\\nneural nets\\nneurophysiology\\nlearning settings\\nmemory efficient experience replay\\nsupervised machine learning\\nstatic settings\\ndata stream\\nstreaming learning\\nconventional deep neural networks\\nfull rehearsal\\nmemory efficient rehearsal\\nexstream algorithm\",\"1014\":\"semantics\\ntraining\\nrobot sensing systems\\ncognition\\ncomputational modeling\\ntraining data\\nbelief networks\\nembedded systems\\nhome automation\\nmobile robots\\nservice robots\\nstatistical analysis\\nrobocse\\nrobot common sense embedding\\nautonomous service robots\\nsemantic knowledge\\nai2thor\\nstatistical significant\\nhome environment simulator\\nword2vec\\nbayesian logic network\\nmatterport3d\",\"1015\":\"aerodynamics\\nstability analysis\\nrotors\\nneural networks\\ntrajectory tracking\\ntraining\\nvehicle dynamics\\naircraft control\\nautonomous aerial vehicles\\ncontrol system synthesis\\nfeedback\\nhelicopters\\nlearning (artificial intelligence)\\nlinearisation techniques\\nneurocontrollers\\nnonlinear control systems\\nrobust control\\ntrajectory control\\ncross-table trajectory tracking cases\\nneural lander\\nstable drone landing control\\nlearned dynamics\\nprecise near-ground trajectory control\\nmultirotor drones\\ncomplex aerodynamic effects\\nmultirotor airflow\\ncomplex effects\\nsmooth landing\\nrobust nonlinear controller\\ncontrol performance\\nnominal dynamics model\\ndeep neural network\\nhigh-order interactions\\nlipschitz constant\\nlipschitz property\\nnonlinear feedback linearization controller\\ndnn-based nonlinear feedback controller\\narbitrarily large neural nets\\nbaseline nonlinear tracking controller\",\"1016\":\"measurement\\nreinforcement learning\\ngames\\napproximation algorithms\\nneural networks\\nautonomous vehicles\\nstochastic processes\\ngaussian processes\\nlearning (artificial intelligence)\\nstatistical distributions\\ndiscrete distribution\\nsoftmax parametrization\\nkl divergence loss\\ndiscretization hyperparameters\\natari games\\ndistributional deep reinforcement learning\\nmixture density network\\nreturn distribution\\nmixtures of gaussians\\njensen-tsallis distance\\nautonomous vehicle driving\",\"1017\":\"physical design\\noptimization\\nreinforcement learning\\ntraining\\nlegged locomotion\\ntask analysis\\nlearning (artificial intelligence)\\noptimisation\\ncontrol policy\\nwalking gaits\\ndeep reinforcement learning\\nlearning-based approaches\\ncontrol network\\ndesign distribution\\ncontroller access\\ndesign parameters\",\"1018\":\"coils\\nmagnetic devices\\nsynchronous motors\\nthree-dimensional displays\\nprototypes\\ncatheters\\ncomputational modeling\\nbiomagnetism\\ncalibration\\nclosed loop systems\\nelectromagnetic actuators\\nmagnetic fields\\nmanipulators\\nmedical robotics\\nmicrorobots\\nposition control\\nmagnetic capsule mock-up\\nmagnetic catheter mock-up\\nmagnetic field computation\\nembedded system\\nmultiple parallel mobile coils\\n3d magnetic field\\nsingle coil\\nfield distribution\\ncalibrated mathematical model\\ngood space utilization\\nelectromagnetic coils\\nproof-of-concept prototype\\nparallel mechanism\\nenlarged workspace\\nmagnetic untethered devices\\nremote actuation\\nnovel magnetic manipulation system\\nelectromagnetic manipulation system\\ndeltamag\",\"1019\":\"instruments\\nimage segmentation\\nfeature extraction\\nkinematics\\ntraining\\nshape\\nreal-time systems\\nconvolutional neural nets\\nendoscopes\\nlearning (artificial intelligence)\\nmedical image processing\\nobject tracking\\nparticle filtering (numerical methods)\\npose estimation\\nsensor fusion\\nsurgery\\nendoscopic vision\\ndata fusion\\nrobust surgical instrument segmentation\\ninstrument segmentation method\\nconvolutional neural networks prediction\\nkinematic pose information\\ncnn model toolnet-c\\nconvolutional feature extractor\\npixel-wise segmentor\\nlabeled images\\nsilhouette projection\\ninstrument body\\nendoscopic image\\nshape matching likelihood\\naccurate silhouette mask\\nfinal segmentation output\\nsurgical navigation system\\ndebrider instrument\\nunlabeled images\",\"1020\":\"colon\\nelectron tubes\\nsurgery\\ntask analysis\\nbars\\nshape\\nvalves\\nbiological tissues\\nendoscopes\\nmedical control systems\\npneumatic actuators\\npneumatic systems\\npneumatically actuated deployable tissue distension device\\ncolon tortuous pathway\\npig colon\\nnatural orifice transluminal endoscopic surgery\\nendoscopic channel\\npneumatically driven deployable structure\\ncolon tissue\\nnotes technology\\nsurgical tasks\\nactuated deployable tissue distension device\\nsurgical operations\\nsurgical instruments\\nair pressure\\nsize 4.5 mm\\nsize 60.0 mm\\npressure 3.5 bar\",\"1021\":\"electron tubes\\nforce\\nkinematics\\nshape\\nendoscopes\\nmanipulators\\ndexterous manipulators\\nelasticity\\nmedical robotics\\nneurophysiology\\npipes\\nrobot kinematics\\nsurgery\\nkinematic model\\nmultiarmed robotic sheath\\neccentric precurved tubes\\nsingle-port minimally invasive procedures\\nmultiple robotic arms\\nprecurved superelastic tubes\\ncosserat rod theory\\ntwo-arm sheath\\nconcentric tube balanced pair\\nneuroendoscopy\\nelastic backbone\\npush-pull tendons\\ncontinuum robot sheath\\nsteerable sheath\\nmultiple arms\\nconcentric tube robots\"},\"Benchmark Setup\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":-1,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1},\"Experimental Results\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":-1,\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":-1,\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":-1,\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":-1,\"65\":-1,\"66\":-1,\"67\":-1,\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":-1,\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":-1,\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":-1,\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":-1,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":-1,\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":-1,\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":-1,\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":-1,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":-1,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":-1,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":-1,\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":-1,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":-1,\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":-1,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":-1,\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":-1,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":-1,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":-1,\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":-1,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":-1,\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":-1,\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":-1,\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":-1,\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":-1,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":-1,\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":-1,\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":-1,\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":-1,\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":-1,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":-1,\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":-1,\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1},\"Code Link\":{\"0\":\"'Both encoder and decoder networks of DLPG compose of two hidden layers with 128 units with a softplus activation function and a 32-dimensional latent space. We use 20 anchor points per each joint trajectory whose periods, T, are 1\\/3 second and 1\\/2 second for Half cheetah and Ant-v2, respectively. The length parameter of a kernel function which governs the smoothness is set to\\\\n1\\\\n4T\\\\nfor both tasks. The actor and critic networks of PPO have two hidden layers where the number of the first layer units equals to the dimension of observation multiplied by five and the number of units in the second layer equals to the geometric mean of the sizes of the first layer and the output layer. Both actor and critic networks of DDPG have two hidden layers with 64 units with ReLU activations. We also implement REINFORCE [14] with a GRP to model a trajectory distribution. All configurations including a PID controller and a GRP are identical to DLPG except the policy function is updated with (5).'\\n\\n'In all scenarios, the encoder network\\\\nq\\\\n\\u03d5\\\\nand decoder network (policy function)\\\\np\\\\n\\u03b8\\\\nboth consist of a fully-connect layer with two hidden layers where each hidden layer has 256 units and tanh activations.'\",\"1\":\"'Traveling between positions or configurations is a fundamental problem in mobile robotics. The typical approach is to encode the environment into a map before running ab planning algorithm to generate trajectories. To this end, there is ab plethora of mapping algorithms for different applications, ranging from complex 3D surroundings [4] to highly dynamic environments [5].b Given a map along with robot dynamics, the task of selecting desirable robot actions prior to task execution can be a computationally complex task [6]. To address this, [7] considers obstacle correlations only between neighboring regions dependent on the direction from which the robot enters. The computational burden is further reduced by allowing the robot to re-plan during execution as its map changes. Algorithms like\\\\nD\\\\n\\u2217\\\\nLite [8] and lifelong planning\\\\nA\\\\n\\u2217\\\\n[9] provide fast re-planning in order to approach realtime reaction to obstacles. In this work, the mapping objective is to capture only regions of the environment critical to task completion. We discuss conditions to encourage the robot to map only regions that may benefit future tasks.'\",\"2\":null,\"3\":\"'The experiments are conducted using MacBook Pro running Intel i7 processor and 16 Gigabytes of memory. The method is coded using MATLAB 2017a, and it is tested on the \\u201cLine\\u201d shape from the LASA human handwriting dataset [15]. The chosen dataset consists of 7 demonstrations where the handwriting motions were collected from a pen input using a Tablet PC. The estimates of the slopes and biases are obtained using the BIP algorithm presented in [20]. The objective is to design a barrier constraint to be enforced while learning the parameters, i.e., the output weights of the ELM, in order to guarantee the solutions of the underlying dynamical system stay within the selected barrier certificate. The barrier certificate is selected such that it encloses all the demonstrations. Two shapes for the barrier certificate are chosen, an ellipse and a circle. For each case the reciprocal BFB(x)=\\\\n1\\\\nh(x)\\\\nis used. To solve the constrained optimization problem, lsqlin function in MATLAB is used. Once the model is learned, the parameters are used to forward propagate the dynamics from various initial conditions inside and outside the invariant set. Figs. 3(a) and 3(b) illustrate reproduction of trajectories using model learned by enforcing barrier certificates. As it can be seen from Figs. 3(a) and 3(b), trajectories that are initialized outside the safe region do not re-enter the barrier region and those trajectories that are initialized inside the barrier region remain inside the invariant set. From Figs. 3(a) and 3(b), it is seen that the trajectories initialized just outside the barrier region and neighboring initial points just inside the barrier region have completely different behavior which is to be expected from enforcing the Barrier certificates in the learning process.'\\n\\n'Driving-Style-Based Codesign Optimization of an Automated Electric Vehicle: A Cyber-Physical System Approach'\",\"4\":null,\"5\":\"'https:\\/\\/youtu.be\\/OMR7hHNSEKM'\\n\\n'Mean SNR of found latent space parameters. In both environments, we used the two dimensions with highest SNR for policy adaptation. For the pendulum, dimensions 0 and 3 encode mass and torque cost\\\\n\\u03ba\\\\n. For the pushing task, dimension 7 corresponds to the rotational offset\\\\nx\\\\nrot\\\\n.'\\n\\n'Our results show clear reconstructions of environment parameters in the latent space, encoded in the dimensions with the highest SNR, giving us a method selecting a low dimensional space for policy optimization.'\\n\\n'We note that the latent space encodes only environment differences when policies are optimal. When policies are suboptimal, the Q-function is no longer unique, and the latent space possibly also encodes policy differences.'\",\"6\":\"'http:\\/\\/www.ok.sc.e.titech.ac.jp\\/res\\/LHD\\/'\\n\\n'All generated data and labeled real data are presented in following url. Trained network weight and test sample code also included.'\",\"7\":\"'Our proposed method outperforms the state-of-the-art on the popular DAVIS and FBMS benchmarks with 6.8% and 1.2% in F-measure respectively. On our new IVOS dataset results show the motion adapted network outperforms the baseline with 46.1% and 25.9% in mIoU on Scale\\/Rotation and Manipulation Tasks respectively. Our code 1 and IVOS dataset 2 are publicly available. A video description and demonstration is available at 3. Our main contributions are:'\",\"8\":\"'As one of the methods used to process abundant tactile information, deep learning is used for tactile sensors. Using stacked autoencoders, in-hand manipulation with differently sized and shaped objects can be achieved [7]. Deep reinforcement learning is also used and has accomplished the changing of orientation of a cylindrical object; however, the hand is multi-fingered, which could make situations difficult because of the many degrees of freedom of the fingers\\u2019 joints [19]. Furthermore, CNNs have been widely utilized for distributed tactile sensors on robotic hands [20] [21] [22] [23] [24]. Texture recognition and slip detection have also been achieved and CNNs get better recognition rates in comparison with other machine learning methods like, for example, SVM [25]. In these cases, the CNNs are trained from the viewpoint for positions of tactile sensors like image processing. The state of the art is focused on CNNs and distributed tactile sensors [26] [27]. When it comes to multi-fingered hands, they have differently sized and shaped distributed sensors [28] [29], which means that the question of how to input tactile information from such sensors to CNNs needs to be considered; this question has not yet been investigated.'\",\"9\":\"'In the following, we first introduce the probabilistic sensor model, on the basis of which we then formulate plane extraction as a maximum likelihood estimation problem. We describe in detail how our agglomerative hierarchical clustering algorithm solves this optimization problem, and finally explain the pseudocode.'\\n\\n'Algorithm 1 provides the PPE pseudocode. Line 1 initializes the set of atomic planes. The function crt\\\\n(Z, L)\\\\nin line 2 loops over all valid tetrominoes of atomic planes and returns the minimum error\\\\ne\\\\ncrt\\\\nalong with the associated indices\\\\nQ\\\\ncrt\\\\n. As there are no regular planes which could be extended or merged at this point, line 3 sets the corresponding error increments\\\\ne\\\\next\\\\nand\\\\ne\\\\nmfg\\\\nto infinity. After these initializations, the algorithm starts iteratively reducing the number of planes. In the first iteration, it always creates a regular plane out of four atomic ones. This means it first adds the new plane to the map (line 6) and then removes the merged atomic planes (line 7). Here, the function fit\\\\n(Z, Q)\\\\nfits a plane\\\\nl\\\\n\\u2217\\\\nto the rays indexed by Q:'\",\"10\":\"'https:\\/\\/youtu.be\\/zOnqVaSl9nM'\\n\\n'https:\\/\\/youtu.be\\/p8D3JTb8qLM'\\n\\n'https:\\/\\/youtu.be\\/tM9xBQFzBks'\",\"11\":\"'http:\\/\\/www.vicon.com\\/'\",\"12\":null,\"13\":null,\"14\":null,\"15\":null,\"16\":\"'https:\\/\\/youtu.be\\/WGuB1cO0mCY'\\n\\n'The GEN-SLAM architecture, a conditional VAE with shared latent space in test configuration. The paired RGB and Depth VAEs are indicated in blue and orange respectively. The RGB image is passed through the Topo-CNN to get topological pose, and simultaneously through the RGB encoder to get the latent vector for this image. This shared latent vector is concatenated with the topological pose, and decoded through the Depth decoder, to get the depth map for the RGB image.'\\n\\n'Most of these modern methods use an encoder-decoder architecture for single image depth, and each branch of our twin CVAE uses the same architecture.'\\n\\n'Two commonly used classes of Deep Generative models are Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) [1], [33], which are both latent variable models that can construct the data distribution\\\\np(x)\\\\nfrom a much lower dimensional latent variable distribution\\\\np(z)\\\\n. VAEs maximize a variational lower bound for the data distribution, resulting in an encoder-decoder formulation wherein a recognition model (the encoder) maps the data into a lower dimensional latent space, and a reconstruction model or decoder (sometimes called a generator) reconstructs the input, thereby training it as an autoencoder. It differs from an autoencoder in that it one can sample from the latent gaussian prior\\\\nz\\u223cN(0,I)\\\\n, to produce new samples from the data. At test time, the setup becomes a generative model in which data can be produced by sampling from the latent variable distribution\\\\nz\\u223cN(0,I)\\\\n.'\\n\\n'A. Paired Variational Auto Encoders (VAEs) for RGB and Depth'\\n\\n'Our model has an encoder-decoder architecture with shared latent space for the VAE, as shown in Figure 4. This paired VAE has 2 encoders and 2 decoders, one each for the RGB and depth images respectively. The encoders encode input RGB images and depth maps into a shared latent space representation, which forces a shared reduced representation of both domains. This latent representation is then decoded through the RGB and depth decoders to give the RGB and depth reconstructions respectively.'\\n\\n'Just using the above losses, we would get a standard AutoEncoder (AE). The VAE enforces the latent distribution\\\\nq(z)\\\\nto be Gaussian, so that we can sample from it and generate new RGB and depth maps from a trained network. It uses a Kullback-Leibler (KL) distance (and loss) to make the input-conditioned latent distribution\\\\nq(z|x)\\\\nto be as close as possible to a 0 mean, unit variance Normal distribution pn.'\\n\\n'We use the PyTorch framework for coding our CVAE, which is made location-aware by concatenating the one-hot encoded topological node associated with each image to the latent vector for that image. We use an additional CNN that operates on the input RGB image, and outputs the topological node as a classification output. We use Alex-net, pre-trained on ImageNet, downloaded from the pytorch model zoo. We remove the last classifier layer, and substitute it with our own, with the number of outputs equal to the number of topological nodes. This pre-trained network is then fine-tuned with (RGB image, topological node) pairs. Our CVAE uses convolutional and de-convolutional layers (Figure 2) with instance norm (INS), leaky RELU and residual (Res) layer modifications. Training of the CVAE is done with (RGB image, depth image, topological node) tuples.'\\n\\n'Output tensor sizes for each layer of the RGB VAE. Layers are coloured according to type, with encoder layers (first column) being a darker shade compared to the corresponding decoder layers (second column). The Depth VAE (not shown here) is identical and shares the yellow convolutional layers with the RGB VAE. The topological node in the map the image belongs to is concatenated as a one-hot vector with the sample from the encoder.'\\n\\n'The GEN-SLAM model, with RGB and Depth encoders and decoders, and a shared latent space. It is trained with tuples of rgb, depth and topological pose.'\\n\\n'One might ask why we use a VAE and not a regular encoder-decoder architecture that has so far been used for single image depth. The VAE forces the latent vector to conform to a distribution, a Gaussian. In our case, both the 3D geometry of the scene (depth map) and its 2D projection (RGB image) conditioned on topological pose, are forced to belong to the same distribution, and the network internalizes the connection between location in an environment, its geometry and appearance, which is embedded in the latent space manifold that the network learns. Figure 7 shows the results of sampling RGB images and depth maps from the CVAE for the Living Room dataset. These sampled images are created by sampling z from random noise, concatenating with the topological label, and then passing\\\\n[z|label]\\\\nto the RGB and depth decoders. This visualization of sampling from the latent space manifold shows that the network has learnt the appearance and underlying geometry of each topological node.'\\n\\n'The twin VAE architecture allows us to use weak supervision to train our networks. Here, we have used noisy datasets depth maps obtained from the stereoZed camera to train our networks. using both within domain and cross domain losses, as opposed to the single cross domain loss that would be used in a regular encoder-decoder architecture that goes from RGB\\\\n\\u2192\\u2212\\\\ndepth domains.'\",\"17\":\"'Simultaneous Localization and Mapping is a key requirement for many practical applications in robotics. In this work, we present RESLAM, a novel edge-based SLAM system for RGBD sensors. Due to their sparse representation, larger convergence basin and stability under illumination changes, edges are a promising alternative to feature-based or other direct approaches. We build a complete SLAM pipeline with camera pose estimation, sliding window optimization, loop closure and relocalisation capabilities that utilizes edges throughout all steps. In our system, we additionally refine the initial depth from the sensor, the camera poses and the camera intrinsics in a sliding window to increase accuracy. Further, we introduce an edge-based verification for loop closures that can also be applied for relocalisation. We evaluate RESLAM on wide variety of benchmark datasets that include difficult scenes and camera motions and also present qualitative results. We show that this novel edge-based SLAM system performs comparable to state-of-the-art methods, while running in real-time on a CPU. RESLAM is available as open-source software 1.'\\n\\n'RESLAM will be released as open-source1'\\n\\n'We presented RESLAM, a novel edge-based SLAM system that utilizes edges for VO, local mapping and loop closure\\/relocalisation verification. Edge-based algorithms are very interesting due to their favorable properties such as larger convergence basin and fast optimization speed. We also demonstrate that we can compete with many state-of-the-art methods that require a strong GPU. In contrast, our method runs in real-time on a CPU, which is essential for mobile robotics applications and navigation tasks. RESLAM is available as open-source1 to encourage further research in the area of edge-based methods.'\",\"18\":null,\"19\":\"'In the real-world experiments the execution of plans reacts to measurements of object poses as well as obstacles that are detected online as shown in the accompanying video. The plans consisting of constraint-controllers encode not only the current target state but also the constraints of the planning domain such as axis-limits and self-collision avoidance. Therefore, the robot does not only avoid the detected obstacle during execution, but does so in a way that no self-collisions occur. If redundant degrees of freedom remain, they are used to achieve the current target that is encoded in the plan.'\\n\\n'A Fast Pose Estimation Method Based on New QR Code for Location of Indoor Mobile Robot'\",\"20\":null,\"21\":\"'The Task Constructor library provides a connecting stage and two basic propagating stages, which all are driven by individual planner instances. We decided to decouple the planning from the stage implementation to increase modularity and facilitate code reuse. While stages specify a subtask, i.e., which robot states to connect, planners perform the actual work to find a feasible trajectory between these two states. Hence, planners can be reused in different stages. Two basic planning instances are provided: (i) MoveIt\\u2019s planning pipeline offering wrappers for OMPL [14], CHOMP [15], and STOMP [16]; and (ii) a Cartesian path generator based on straight-line Cartesian interpolation and validation.'\\n\\n'We presented a modular and flexible planning system to fill the gap between high-level, symbolic task planning and low-level motion planning for robotic manipulation. Given a concrete task plan composed of individually characterized sub-stages, our system can yield combined trajectories that achieve the whole task. Failures can be readily analyzed by visualization and isolation of problematic stages. A number of generic planning stages are already in place and were employed to demonstrate the potential of the framework for use on multiple robotic platforms. The Task Constructor is meant to enhance the functionality of the MoveIt! framework and replace its previous, severely limited pick-and-place pipeline. The open-source software library is under continuous development and various extensions were outlined directly within the corresponding sections.'\",\"22\":null,\"23\":null,\"24\":null,\"25\":null,\"26\":null,\"27\":null,\"28\":null,\"29\":null,\"30\":null,\"31\":null,\"32\":\"'Contact detection is an important topic in contemporary humanoid robotic research. Up to date control and state estimation schemes readily assume that feet contact status is known in advance. In this work, we elaborate on a broader question: in which gait phase is the robot currently in? We introduce an unsupervised learning framework for gait phase estimation based solely on proprioceptive sensing, namely joint encoder, inertial measurement unit and force\\/torque data. Initially, a meaningful physical explanation on data acquisition is presented. Subsequently, dimensionality reduction is performed to obtain a compact low-dimensional feature representation followed by clustering into three groups, one for each gait phase. The proposed framework is qualitatively and quantitatively assessed in simulation with ground-truth data of uneven\\/rough terrain walking gaits and insights about the latent gait phase dynamics are drawn. Additionally, its efficacy and robustness is demonstrated when incorporated in leg odometry computation. Since our implementation is based on sensing that is commonly available on humanoids today, we release an open-source ROS\\/Python package to reinforce further research endeavors.'\\n\\n'Fusion of joint encoder, IMU, and F\\/T measurements in a solely unsupervised learning framework.'\\n\\n'In this paper we employ two dimensionality reduction approaches, namely the Principal Component Analysis (PCA) and the autoencoders [27]. The former is a linear projection method while the latter employs Neural Networks (NN) that can discover nonlinear relationships between the high dimensional data and the latent space, yielding potentially a more accurate low-dimensional representation. Both approaches minimize the reprojection error:'\\n\\n'Initially, to obtain quantitative assessment results, we performed omni-directional gaits on uneven \\/rough terrain with the NASA\\u2019s Valkyrie robot in Gazebo for approximately 15 minutes to record the training dataset. The IMU, joint encoder, and F\\/T measurements are available at a 500Hz rate, thereby resulting in a dataset of approximately 450000 entries. In addition, i.i. d. Gaussian noise is added to the measurements to provide an accurate assessment with realistic noise levels.'\\n\\n'Subsequently, autoencoders were employed to investigate whether a lower reprojection error could be achieved when reducing to 2D. While experimenting with the NN structure and activation functions, we observed that good results were obtained when the encoder was formulated with two hidden layers and linear activation functions which facilitated reduction to five and two dimensions sequentially. For the decoder part the reverse structure was used. The obtained reprojection error was 8.83e-04, which is almost identical to the one obtained with PCA. Additionally, the derived latent-space has the same shape as the one computed with the PCA transformation but different orientation and scale (Figure 2 right). Accordignly, it is deduced that linear projection is sufficient for this particular data selection and, therefore, is adopted for simplicity in the proposed framework. The extracted latent-space with the ground-truth labels is illustrated in Figure 3.'\\n\\n'Given that the proposed framework utilizes measurements from sensors that are commonly available on humanoids nowadays, we offer the Gait-phase Estimation Module (GEM), an open-source implementation to reinforce further research endeavors [24].'\",\"33\":\"'We consider dynamic stair climbing with the HRP-4 humanoid robot as part of an Airbus manufacturing use-case demonstrator. We share experimental knowledge gathered so as to achieve this task, which HRP-4 had never been challenged to before. In particular, we extend walking stabilization based on linear inverted pendulum tracking [1] by quadratic programming-based wrench distribution and a whole-body admittance controller that applies both end-effector and CoM strategies. While existing stabilizers tend to use either one or the other, our experience suggests that the combination of these two approaches improves tracking performance. We demonstrate this solution in an on-site experiment where HRP4 climbs an industrial staircase with 18.5 cm high steps, and release our walking controller as open source software.'\\n\\n'We illustrate the performance of this controller in an onsite experiment in the Airbus Saint-Nazaire site where the HRP-4 humanoid climbs a staircase with 18.5 cm steps. To the best of our knowledge, this is the first time that dynamic stair climbing is demonstrated with HRP-4. The controller used in this experiment is also open source and open to comments.1'\",\"34\":null,\"35\":\"'https:\\/\\/github.com\\/CAOR-MINES-ParisTech\\/lwoi'\\n\\n'Robot Dead Reckoning with Corrected Wheel Encoders and IMU Outputs'\\n\\n'Odometry techniques are key to autonomous robot navigation, since they enable self-localization in the environment. However, designing a robust odometry system is particularly challenging when camera and LiDAR are uninformative or unavailable. In this paper, we leverage recent advances in deep learning and variational inference to correct dynamical and observation models for state-space systems. The methodology trains Gaussian processes on the residual between the original model and the ground truth, and is applied on publicly available datasets for robot navigation based on two wheel encoders, a fiber optic gyro, and an Inertial Measurement Unit (IMU). We also propose to build an Extended Kalman Filter (EKF) on the learned model using wheel speed sensors and the fiber optic gyro for state propagation, and the IMU to update the estimated state. Experimental results clearly demonstrate that the (learned) corrected models and EKF are more accurate than their original counterparts.'\\n\\n'This paper introduces a methodology for Gaussian processes combined with neural networks and stochastic variational inference to improve both the propagation and measurement functions of a state-space dynamical model by learning error residuals between physical prediction and ground truth data. These corrections are also shown to be usable for EKF design. The applications on publicly available datasets for consumer car and Segway navigation systems based on wheel encoders, gyro and IMU clearly reveals the gains of performances of the proposed approach.'\\n\\n'https:\\/\\/github.com\\/CAOR-MINES-ParisTech\\/lwoi'\",\"36\":\"'For each azimuth, radar outputs a one-dimensional signal, termed the power-range spectrum, which encodes the power reflected by the scatterers within the beam at each range. After a full rotation, radar returns a two-dimensional scan, as shown in Fig. 2. Radar scans are both information-rich and data-efficient, but they contain several unwanted artifacts visible in Fig. 2, including noise and false detections [3]. Another consideration of radar is its lower resolution and slower measurement update speeds compared to lidar.'\",\"37\":null,\"38\":\"'Localization is one of the main challenges to be addressed to develop autonomous vehicles able to perform complex maneuvers on roads opened to public traffic. Having an accurate dead-reckoning system is an essential step to reach this objective. This paper presents a dead-reckoning model for car-like vehicles that performs the data fusion of complementary and redundant sensors: wheel encoders, yaw rate gyro and steering wheel measurements. In order to get an accurate dead-reckoning system with a drift reduced to the minimum, the parameters have to be well calibrated and the procedure has to be simple and efficient. We present a method able to accurately calibrate the parameters without knowing the ground truth by using a Rauch-Tung-Striebel smoothing scheme which enables to obtain state estimates as close to the ground truth as possible. The smoothed estimates are then used within a optimization process to calibrate the model parameters. The method has been tested using data recorded from an experimental vehicle on public roads. The results show a significant diminution of the dead-reckoning drift compared to a commonly used calibration method. We evaluate finally the average distance a vehicle can navigate without exteroceptive sensors by using the proposed four-wheeled dead reckoning system.'\\n\\n'In order to reduce the natural drift that dead-reckoning processes are known for, the fusion of multiple sensors is performed. The measurements from a gyro, a speed sensor, four-wheel encoders as well as the steering wheel angle are used in the estimation process. These sensors operate at different frequencies (50 Hz for the wheel encoders, 100 Hz for the rest) and are not synchronized. When these sensors are used in the fusion scheme, the state estimate is updated when the wheel encoder measurements are received. The most recent measurements of the other sensors are used in the fusion (half of the measurements from the other sensors are therefore discarded). We have observed experimentally that given the high frequency of the sensors and the relatively slow dynamic of the vehicle, interpolating the measurements does not lead to a significant accuracy improvement and is therefore not performed.'\\n\\n'3) Wheel encoders:'\\n\\n'The wheel encoders provide the number of ticks\\\\n\\u25b3\\\\nXX\\\\n1 that occurred during a time\\\\nT(20\\\\nms in our case). These measurements can be linked to the state variables using the Ackermann steering geometry and parameters such as the number of ticks per turn Nwheel , the wheel radii\\\\n\\u03c1\\\\nXX\\\\nand the front\\\\n(\\\\nl\\\\nF\\\\n)\\\\nand rear\\\\n(\\\\nl\\\\nR\\\\n)\\\\ntrack widths. Under the non-slippage assumption, the speed measured on each wheel of the vehicle can be linked to the vehicle longitudinal speed vk and its yaw rate\\\\n\\u03c8\\\\n\\u02d9\\\\nk\\\\nusing geometrical considerations. Therefore, the number of ticks elapsed within a time interval T can be linked to the vehicle state using the tick resolution Nwheel and the wheel circumference as follows:'\",\"39\":\"'VLAD is a feature encoding and pooling method, which encodes a set of local feature descriptors extracted from an image by using a clustering method such as K-means clustering. For the feature extraction module, we extract multi-domain place features from the raw image, by utilizing a CapsuleNet module. Let qik be the strength of the association of data vector xi to the cluster\\\\n\\u03bc\\\\nk\\\\n, such that\\\\nq\\\\nik\\\\n\\u22650\\\\nand\\\\n\\u2211\\\\nk=1\\\\nK\\\\nq\\\\nik\\\\n=1\\\\n, where K is the clusters number. VLAD encodes feature x by considering the residuals'\\n\\n'The framework of feature separation. The networks are combined with four modules: the feature extraction module as given in the previous section; a classification module estimating the environmental conditions; a decoder module mapping the extracted feature back to the data domain; a discriminator module distinguishing the generated data and raw data; and two reconstruction loss modules on data and feature domain respectively.'\\n\\n'Condition entropy reduction sub-module can restrict the mapping uncertainty from data domain to the feature domain, this restriction is highly related to the generalization ability of the encoder module. For the place recognition task, there will be highly diverse scenes in practice, however, we can only generate limited samples for network training. In theory, the GAN uses a decoder and discriminator module to learn the potential feature-to-data transformation with limited samples. Thus, we improve the data generalization ability by applying GANs.'\",\"40\":\"'https:\\/\\/youtu.be\\/ISgXVgCR-lE'\\n\\n'http:\\/\\/rpg.ifi.uzh.ch\\/direct'\",\"41\":null,\"42\":null,\"43\":\"'The platform uses a custom quadrotor. Each rotor can generate a maximum force of 1024 g, while the platform weighs 971 g. The mounted flight controller is the PIXFAL-CON autopilot with the open source PX4 autopilot firmware. The on-board computer communicates with a ground control station using a Wi-Fi module. The ROS platform [25] runs on the Odroid for controls and image processing. A Matrix Vision mvBlueFOX-MLC200wG camera with a wide angle lens is mounted on-board to transmit the captured images through USB 2.0 at 30 - 40Hz. The camera provides monocular images with 752\\\\n\\u00d7\\\\n480 resolution.'\",\"44\":null,\"45\":null,\"46\":null,\"47\":null,\"48\":null,\"49\":\"'https:\\/\\/github.com\\/TAMS-Group\\/TeachNet_Teleoperation'\\n\\n'To better process the geometric information in the input depth image and the complex constraints on joint regression, we adopt an encode-decode style deep neural network. The upper branch in Fig. 2 illustrates the network architecture we used. However, the human hand and shadow hand basically come from different domains, thus it could be difficult for ffeat to learn an appropriate latent feature zpose in pose space. In contrast, the mapping from IR to joint target\\\\n\\u0398\\\\nwill be more natural as it is exactly a well-defined hand pose estimation problem. Intuitively, we believe that for a paired human and robotic image, their latent pose features zpose should be encouraged to be consistent as they represent the same hand pose and will be finally mapped to the same joint target. Also, based on the observation that the mapping from IR to\\\\n\\u0398\\\\nperforms better than IH (these preliminary results can be found in Fig. 5), the encoder ffeat of IR could extract better pose features, which could significantly improve the regression results of the decoder.'\\n\\n'Besides the encoder-decoder structure that maps the input depth image to joint prediction, we define a consistency loss\\\\nL\\\\ncons\\\\nbetween two latent features zH and zR to exploit the geometrical resemblance between human hands and the robotic hand. Therefore,\\\\nL\\\\ncons\\\\nforces the human branch to be supervised by a pose space shared with the robot branch. To explore the most effective aligning mechanism, we design two kinds of consistency losses and two different aligning positions:'\\n\\n'https:\\/\\/github.com\\/TAMS-Group\\/TeachNet_Teleoperation'\",\"50\":null,\"51\":\"'Any constraint of the system can be modeled through the function H. Without loss of generality, here we consider joint limits and singularities at the slave side and provide the operator with haptic guidance to effectively avoid them. Constraints are encoded through the following functions (see [9], [34] for more details)'\",\"52\":\"'A test cell was constructed to secure one 2-DOF modular link and measure output position in real time while incorporating stiffness of the belt transmission (1.3 kNm\\/rad), and lumping the stiffness of the differential, 3D-printed plastic shells, and 3D-printed joint coupling (lumped at 1.2 kNm\\/radian). Masses were held vertically to avoid directional transmission pre-load from gravity and a rotary encoder was used to record arm translation via cable capstan transmission.'\\n\\n'Robots must cost less and be force-controlled to enable widespread, safe deployment in unconstrained human environments. We propose Quasi-Direct Drive actuation as a capable paradigm for robotic force-controlled manipulation in human environments at low-cost. Our prototype - Blue - is a human scale 7 Degree of Freedom arm with 2kg payload. Blue can cost less than $5000. We show that Blue has dynamic properties that meet or exceed the needs of human operators: the robot has a nominal position-control bandwidth of 7.5Hz and repeatability within 4mm. We demonstrate a Virtual Reality based interface that can be used as a method for telepresence and collecting robot training demonstrations. Manufacturability, scaling, and potential use-cases for the Blue system are also addressed. Videos and additional information can be found online at berkeleyopenarms.github.io.'\",\"53\":null,\"54\":null,\"55\":null,\"56\":null,\"57\":null,\"58\":\"'https:\\/\\/github.com\\/jacqu\\/rpit'\\n\\n'https:\\/\\/github.com\\/jacqu\\/rpit'\",\"59\":null,\"60\":\"'https:\\/\\/youtu.be\\/ZsSo4RxwrwU'\\n\\n'https:\\/\\/youtu.be\\/KtbHZuGzHuU'\\n\\n'Two preliminary experiments were performed in this work. The first was to evaluate the device\\u2019s performance by tracking its shoulder plate positional accuracy across a given workspace. This test was performed for two different shoulder plates to demonstrate that the workspace is dependent on the shoulder plate design. The results showed that the overall kinematic model used for the 4B-SPM exoskeleton is correct, though some minor errors due to part tolerances issues and substructure misalignment were identified. This means for upper limb studies requiring less than 1\\u00b0 Euler angle accuracy of the shoulder, the 4B-SPM shoulder exoskeleton\\u2019s onboard encoders should not be solely relied on for position feedback. Instead, an off-board 3D position tracking system should be used in conjunction with on-board feedback. Precise machining and assembly of the 4B-SPM exoskeleton would eliminate the need for a secondary feedback mechanism. Ultimately though, for the purposes of characterizing the neuromuscular properties of the shoulder, using a secondary source of feedback is not an issue.'\",\"61\":null,\"62\":null,\"63\":\"'(a) The implemented device with the footplate attached on top, and (b) Custom made omni wheels, gearbox and MAXON EC45 50W motor equipped with 1024CPT MILE encoders'\\n\\n'Orientation and angular velocity measurement: To measure the position and the angular velocity of the dome, two alternative methods are possible, namely (i) indirect method using the position of the actuators and kinematics calculations, and (ii) direct measurement of the dome\\u2019s position. The indirect method cannot be used as feedback for the position\\/velocity control loop of the robot, since the proposed mechanical safety feature allows the wheels to slip when the applied force is higher than the adjusted threshold. As a result, it is not possible to calculate the Cartesian domain motion of the end-effector (which interacts with the patient\\u2019s limb), based on the joint space measurements. However, the estimated position calculated by the indirect method can be used to measure the amount of slippage occurred. This can be done by comparing the position measured with direct method and the position obtained from indirect measurement. The slippage calculation is beneficial in practice to (a) minimize the power consumed by the actuators during slippage episodes, and (b) prevent excessive erosion caused by slippage between the inner surface of the dome and the wheels. The alternative solution is the direct measurement of the orientation and the angular velocity of the dome. For this, one approach is to attach incremental or absolute encoders to the corresponding axes of the universal joint at the base. However due to the size limitation of the spherical joint and the cost of the small-size encoders, this method may not be the best option. Another alternative, is to attach a gyro sensor to the dome (or the connection rod). By calculating the integral of the output of the gyro, which is the angular velocity, the relative amount of rotation with respect to the initial position can be measured. Using this sensor an accuracy of\\\\n4\\u00d7\\\\n10\\\\n\\u22123\\\\ndeg\\/sec was achieved.'\",\"64\":\"'https:\\/\\/github.com\\/CogRob\\/Rorg'\\n\\n'The benefits of Rorg are not free \\u2014 the developer needs to write extra code to configure Rorg. Since Rorg runs all programs inside Docker containers, deploying a robotic program with Rorg requires creating a Docker image and defining runtime parameters. Seemingly an extra effort, the image blueprint (Dockerfile) and the runtime configuration actually serve as a document to reproduce the execution environment, which helps organizing robotic software from another perspective; many readily available ROS images and examples further make this process easier [19].'\\n\\n'https:\\/\\/github.com\\/CogRob\\/Rorg'\",\"65\":\"'https:\\/\\/youtu.be\\/eVmDBVL83WY'\\n\\n\\\"A modeling language eases the job of formulating and solving a complex optimization problem; yet, the need to write C++ code that is customized for the specific robotic platform and task to be solved remains. This should be avoided, both with a view to promote code reuse, and also considering that, for complex platforms and according to the authors' experience, writing a hierarchy of tasks\\/constraints that makes the robot show the desired behavior can be \\u201can art\\u201d. Therefore, it should be left to \\u201cexperts in the field\\u201d, while users should simply customize the problem to better fit their needs. Furthermore, this code would be of little use without an I\\/O infrastructure that allows a control module to communicate with the external world. Again, the developer should be relieved from writing its own from scratch. With this motivation, we started developing an interface layer that:\\\"\\n\\n\\\"By careful code development and profiling, we ensure satisfaction of these constraints both by the OpenSoT library (including its solver back-ends), and by the CartesianInterface layer. However, it is not possible to run our ROS API Generator on the RT layer directly, mainly because of ROS's usage of networking primitives2. To address this issue, a dual-thread architecture is needed, where a nonRT thread runs our ROS API component, while a RT thread runs the CartesianInterface implementation. We put this idea into practice by leveraging the XBotCore framework, that provides us both a RT \\u201ccontrol thread\\u201d and a non-RT \\u201ccommunication thread\\u201d. We design the synchronization between the two to be non-blocking for the RT thread, by employing a lock-free callback queue mechanism. This avoids priority inversion problems, and results in a deterministic execution time for our controller, as we experimentally demonstrate in Section IV-A.\\\"\",\"66\":\"'The DeRoS observer has been implemented in a single component that reads data in the laser. The evaluation of the formula has been implemented following the pseudo-code given in [9, Fig. 6]. The period of the DeRoS component is one of the benchmark parameters, and is denoted TD.'\",\"67\":\"'The two-language approach has some drawbacks, however. First, users who start out prototyping their robotics application in the \\u2018high-level\\u2019 language often need to duplicate their efforts by porting code to the lower-level language once they are satisfied with their prototype, a tedious and error-prone process. Second, in our experience, maintaining a software package written in multiple programming languages tends to cause a large amount of developer overhead: the code bases for each of the two languages have to be kept in sync, and a build system and documentation have to be maintained for each language.'\\n\\n'Julia provides convenient features common to high-level languages, such as built-in support for list comprehensions, generators, and parallelism. Julia also features excellent support for functional programming, with functions represented as first-class objects and efficient higher-order functions that are used extensively throughout the standard library. Julia borrows an important feature from LISP: Julia code is represented as a data structure in the language itself. This enables powerful metaprogramming features, as it is straightforward to generate Julia code in Julia without requiring a preprocessor or a separate template system.'\\n\\n'This implies that, currently, Julia\\u2019s garbage collector should be disabled for online control purposes, which in turn implies that dynamic memory allocation should be avoided so as to not run out of memory. Hence, our current approach is to completely avoid dynamic allocation in code that is meant to be run in low-level control loops (after an initial preallocation phase). This includes evaluation of the dynamics, setting up and solving quadratic programs, and network communication. This is a serious constraint, but it should be noted that it is nontrivial to provide hard realtime constraints in the presence of dynamic allocation in any language, requiring e.g., a specialized memory allocator in C++ [21]. Avoiding dynamic allocation also has the added benefit that it tends to improve performance.'\\n\\n'Given the constraint that dynamic allocation should be avoided, Julia has clear benefits over JVM-based languages like Java. The standard JVM implementation currently only provides the guarantee that instances of a predefined set of primitive types are stack-allocated. Julia additionally guarantees that immutable data structures (recursively) composed of such types are stack-allocated. This feature is used extensively throughout the Julia robotics code, allowing points, transforms, twists, and other fixed-size quantities to be freely constructed and used without requiring pre-allocation.'\\n\\n'Julia\\u2019s JIT compilation model is also more suitable to realtime control than Java\\u2019s. Whereas Java\\u2019s JIT compiler optimizes hot code at runtime [22], a potential source of jitter, Julia by default guarantees that functions are compiled to native code the first time they are called with a given set of argument types, providing more predictable behavior. While specialized JVM implementations with real-time JIT compilation and garbage collection capability are available [20], these solutions tend to have much lower throughput and\\/or a prohibitively high price tag for research robotics [4].'\\n\\n'github.com\\/tkoolen\\/julia-robotics-paper-code'\\n\\n'RigidBodyDynamics.jl also exploits Julia\\u2019s metaprogramming and the ability to switch back and forth between running and compiling code, by generating specialized code for the specific joint types present in a given mechanism, an approach reminiscent of RobCoGen [27], but all in the same language.'\\n\\n'RigidBodySim.jl inherits most of its features from its dependencies and composition with third-party packages such as ForwardDiff.jl and Measurements.jl that provide non-standard input types. Version 1.0.0 of RigidBodySim.jl consists of only 760 lines of Julia code, making it a good example of the ease with which Julia packages can be combined to create useful applications.'\\n\\n'This section provides benchmark results for the dynamics algorithms in RigidBodyDynamics.jl (IV-A) and the balancing controller from QPControl.jl (IV-B). All results were obtained on a desktop machine with an Intel Core i7-6950X CPU @ 3.00GHz. The code used to generate these results can be found at github.com\\/tkoolen\\/julia-robotics-paper-code.'\\n\\n'github.com\\/rdeits\\/StrandbeestRobot.jl'\\n\\n'This section presents a set of robotics-related Julia packages developed by the authors, which together form a foundation for simulation and control (III-A\\u2013III-E). All of the presented packages are MIT-licensed. Each package provides Jupyter notebooks containing usage examples. Some of the packages are part of the JuliaRobotics GitHub organization [24], which also incorporates packages for state estimation, SLAM, and parameter estimation (not discussed in this paper). Section III-F lists some other relevant packages and Section III-G discusses the current state of dissemination. Links to the packages can be found at github.com\\/tkoolen\\/julia-robotics-paper-code.'\",\"68\":\"'Motion Planning Templates (MPT) is a C++ template-based library that uses compile-time polymorphism to generate robot-specific motion planning code and is geared towards ...'\\n\\n\\\"Motion Planning Templates (MPT) is a C++ template-based library that uses compile-time polymorphism to generate robot-specific motion planning code and is geared towards eking out as much performance as possible when running on the low-power CPU of a battery-powered small robot. To use MPT, developers of robot software write or leverage code specific to their robot platform and motion planning problem, and then have MPT generate a robot-specific motion planner and its associated data-structures. The resulting motion planner implementation is faster and uses less memory than general motion planning implementations based upon runtime polymorphism. While MPT loses runtime flexibility, it gains advantages associated with compile-time polymorphism- including the ability to change scalar precision, generate tightly-packed data structures, and store robot-specific data in the motion planning graph. MPT also uses compile-time algorithms to resolve the algorithm implementation, and select the best nearest neighbor algorithm to integrate into it. We demonstrate MPT's performance, lower memory footprint, and ability to adapt to varying robots in motion planning scenarios on a small humanoid robot and on 3D rigid-body motions.\\\"\\n\\n'The process flow of Motion Planning Templates (MPT) starts with a developer supplying a robot\\u2019s motion planning problem scenario and selecting an algorithm setup. At compile time, the template system of MPT generates code for a robot-specific implementation of a motion planning algorithm. This system trades off runtime flexibility (algorithms and their data structures cannot be changed without recompiling) in favor of improved performance and reduced memory utilization, both of which are critical to battery-powered small robots that use their on-board low-power CPU to perform motion planning.'\\n\\n'The system behind MPT\\u2019s code generation is C++ templates which is a Turing-complete [3] compile-time polymorphic system\\u2014which is a fancy way of saying that C++ templates are programs that write code.'\\n\\n'Polymorphism, from the Greek meaning \\u201cmany forms\\u201d, refers to the ability of a single code interface to provide many different implementations [20]. In practice this means that the data and code behind a name can be changed without changing the code that refers to that name. When the executed code can be changed while the program is running, it uses runtime polymorphism, a concept that is likely familiar to people with experience with class-based object oriented programming in languages such as Java, Python, and C++. In runtime polymorphism, when code invokes a virtual method, it finds the the concrete implementation through a virtual table (vtable) lookup. Fig. 2 shows an example of a sampling-based motion planner\\u2019s outer loop using runtime polymorphism to change its behavior. The loop continues until the done() method returns true\\u2014the exact meaning of done() is dependent on the cond object\\u2019s concrete type. Similarly, the loop can work in any state space using sampler object of the appropriate concrete type.'\\n\\n'With template-based compile-time polymorphism, the compiler substitutes placeholders with direct function calls. In contrast to runtime polymorphism, flexibility to change the termination condition at runtime is lost, but execution time is sped up. The speedup comes from saving a level of indirection, and giving the compiler the ability to perform additional optimizations since it knows which code will be called\\u2014e.g., it can move simple code inline and remove the call altogether.'\\n\\n'Compile-time polymorphism, also called static polymorphism, operates on a similar principle, but instead resolves implementations when the code is compiled, so it does not need a virtual table. Fig. 3 shows a compile-type polymorphic equivalent of Fig. 2. In this case, the behavior cannot be changed at runtime, and as a result, can run faster than the vtable-based approach. Virtual calls are an important enough performance consideration that researchers have put effort into devirtualizing calls at runtime [21]. The loss of runtime flexibility in this example is likely to be acceptable for the performance gained by the robot-specific motion planner.'\\n\\n'MPT uses compile-time polymorphism based on C++ templates. Templates are like functions that run in the compiler that take data types and constants as parameters and generate code that will be executed. Template data type parameters can be arbitrarily complex structures, which allows seemingly simple template substitutions to transitively lead to complex results\\u2014e.g. robot-specific motion planners.'\\n\\n'In this section we demonstrate MPT\\u2019s performance on an articulated robot and in OMPL\\u2019s SE (3) rigid-body planning benchmarks. We compare to OMPL as it is an example of a well-designed flexible motion planning library that uses runtime polymorphism. To the extent possible, we set up corresponding motion planners from MPT and OMPL to run identical algorithms. The performance benefit of MPT over OMPL thus comes from the MPT\\u2019s compile-time data-structure and algorithm selections, compact state representation, non-virtual methods, and affordances that allow the compiler to inline and vectorize code. This does however come at the cost of losing runtime flexibility and a potentially steeper learning curve. We run MPT with both single (\\u201cfloat\\u201d) and double precision arithmetic. OMPL only supports double precision arithmetic. OMPL uses GNAT for nearest neighbor searching so we compare to MPT using GNAT. We also compare against MPT\\u2019s automatic selection of kd-trees for nearest neighbor searching.'\\n\\n'We presented Motion Planning Templates, a framework based upon the compile-time polymorphic system of C++ templates for building motion planners for robots with low-power CPUs. MPT\\u2019s template system generates custom planning code specific to the robot and a set of tasks encompassed by a concept of a scenario.'\",\"69\":null,\"70\":null,\"71\":\"'This paper is concerned with the algorithmic approach, and the approach is based on deep learning. The need for a large corpus of training data of human painter activity is circumvented by basing the approach on an autoencoder, requiring only a training set of images of the desired texture. The autoencoder\\u2019s decoder learns to reconstruct a desired texture while being constrained by the painting capabilities of the robotic system. The painting commands can then be inferred for an input texture ink map by a pass through the encoder. Desired properties of the robot\\u2019s traversal path are also favored during the training. In this paper, we assume that the robot will utilise a grid-like trajectory, and we determine painting commands that both reproduce the input ink map and are efficient to execute. Illustrative results are shown for rock-type texture in Fig. 1.'\\n\\n'The fact that the simulator assumes the form of convolutions with pre-specified kernels allows us to propagate gradients through the autoencoder and train it end to end. We use a fully convolutional neural network (CNN) architecture [33]. The implementation details are all grouped in Section IV. The CNN acts as an autoencoder, mapping the input image through a bottleneck layer to an output image. The decoder of the CNN emulates the spray paint simulator (Fig. 2) as explained above; its parameters are fixed and not learnable (the N spray patterns). The last convolution tensor is thus made up of N matrices, each holding a discretized two-variate Gaussian. The previous layer\\u2019s outputs are the impulses or commands that drive the spray painting.'\\n\\n'Architecture of the constrained autoencoder convolutional neural network. The encoder is made up of a wide ResNet architecture containing a sequence of res-blocks [34]. The blocks are constituted of 2D convolutions and ReLU [35] activation layers with skip connections. The output is then passed through an affine mapping to shift the distribution of activation values similar to a batch normalization [36]. This affine mapping allows for a varied output in the range [0,1] after passing through the sigmoid activation [37]. The average pooling followed by upsampling with zero filling is only used when sparsity is to be imposed. The decoder scales the spray pattern magnitudes and applies our version of a spray simulator. The largest magnitude can be mapped in the physical world to the longest spray duration before droplets are formed.'\\n\\n'The CNN architecture we implement is fully convolutional. It is an autoencoder network with a constrained decoder. The constraint is that the decoder emulates a spray paint simulator that is reproducible by the given mobile robotic system. The autoencoder CNN follows the residual networks approach [41] but with a shallow and wide architecture [42] portrayed in Fig 2. In total, 51k parameters are trainable in the network. The encoder outputs are the spraying commands since the decoder is forced to be a non-trainable layer that emulates spray painting.'\\n\\n'Code'\",\"72\":null,\"73\":null,\"74\":\"'http:\\/\\/georacer.git;hub.io\\/fault-diagnosis\\/demos\\/mavlink\\/mavlink.html'\",\"75\":null,\"76\":\"'https:\\/\\/youtu.be\\/UuQvijZcUSc'\\n\\n'https:\\/\\/youtu.be\\/UuQvijZcUSc'\",\"77\":\"'Holistic Attention on Pooling Based Cascaded Partial Decoder for Real- Time Salient Object Detection'\",\"78\":\"'In this step, the estimate result of EKF and the constraint information is used to obtain the estimation result coherent with the kinematic constraint given by the system configuration. The constraint application process can be described with pseudo-code as following where'\\n\\n'We can now describe the function \\u201cConstraintApply () \\u201d in the pseudo-code as following equations.'\",\"79\":null,\"80\":null,\"81\":\"'We evaluate the performance of the proposed algorithm in different simulated scenarios. The simulator uses C++ custom code for the dynamics engine, and Gazebo [28] to simulate perception data (in the form of a depth map and a point cloud). In all these simulations, the depth camera has a horizontal FOV of 90\\u00b0, and a sensing range of 10 m.'\\n\\n'The Gazebo worlds and the code for the optimizer used in this paper are available here.'\",\"82\":\"'Flight tests at Guardian Centers. Left: Exemplary flight trajectory. The \\u2018S\\u2019 marks the starting point and numbers present the given waypoints. The trajectory color encodes the speed. As desired, the velocity increases in wide, open spaces, while decreases in narrow passages and turns. Right top: Visualization of the Occupancy Grid at entrance of narrow passage, next to waypoint 4. Right bottom: Camera view of the narrow passage.'\",\"83\":null,\"84\":\"'https:\\/\\/sites.google.com\\/view\\/dpgfd-insertion\\/home'\\n\\n'Rewards\\/Goal: Because we do not assume access to any object tracker, the task goal must be defined solely using robot sensors. Notably, for deformable plugs this eliminates the standard method of determining success via the robot\\u2019s kinematics, since flexion is not observed by joint encoders. Gripper Pose: Episodes begin with the plug roughly 5 cm from the socket opening.'\",\"85\":\"'Our proposed uncertainty estimation technique was evaluated on an existing autonomous driving model provided by Codevilla et al. [6]. The model was trained on two hours of human-driving data collected in simulation in Town 1 in the CARLA environment. The imitation network takes image of the front camera and the speed of the vehicle as input and outputs steering angle and throttle value. The imitation agent and our uncertainty estimation system are tested in a novel environment (Town 2) with both seen and unseen weather conditions using a subset5 of test cases provided in the CARLA benchmark [8]. Collisions, intersections with the opposite lane, and driving onto the curb are recorded as infractions. The network outputs two control signals: steering angle and throttle value. Uncertainties for the two control signals were computed independently and summed with weights in the total uncertainty estimation function. The tested uncertainty estimation signals are:'\\n\\n'To test these hypotheses, we obtained the set of demonstration data provided by Codevilla et al. [6] and cleaned it up by removing data files that contained infractions. We refer to this data set as the passive dataset. We selected a subset of data files from the clean data set and used it as the starter set from which we will improve the trained model\\u2019s performance using different data selection methods.'\",\"86\":null,\"87\":\"'For example, consider the problem of training autonomous vehicles to navigate in the presence of human road users. Since physical road tests are expensive and dangerous, simulation is an essential part of the training process. However, such training requires a realistic simulator which, in turn, requires realistic models of other agents, e.g., vehicles, cyclists, and pedestrians, that the autonomous vehicle interacts with. Hand-coded models of road users are labour intensive to create, do not generalise to new settings, and do not capture the diversity of behaviours produced by humans.'\",\"88\":\"'Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage simulation and off-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is first trained through a meta-learning strategy in simulation. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with only a few real world expert demonstrations, we show successful planning performances in different navigation tasks.'\\n\\n'We also generalize the adversarial domain transfer method for sim-to-real transfer of an end-to-end gradient-descent based planner, where separate supervisory signals are not available for the perception and control modules separately. We first train using expert trajectories in simulation and then perform adversarial transfer on the encoder\\u2019s output space to learn mappings from the real environment that are similar to the mappings from the simulation environment. In particular, we claim the following contributions:'\\n\\n'We demonstrate on a real robot that the developed policy (encoder + planner) trained in simulation can transfer to a real environment (by using very few real expert demonstrations for fine-tuning) through an adversarial transfer approach.'\\n\\n'This setup is elegant since it is able to learn a latent encoding without wasting additional optimization effort on reconstruction as is the case in a variational autoencoder setup such as DARLA [24]. However, in our experience it suffers from the following shortcomings:'\\n\\n'The source encoder is first pre-trained using labelled simulated data of images and corresponding target positions. Then, the source encoder\\\\n(\\\\nE\\\\ns\\\\n)\\\\nis locked and a reference target encoder\\\\n(\\\\nE\\\\nr\\\\n)\\\\nis trained through images sampled from both the simulation\\\\n(\\\\nI\\\\ns\\\\n)\\\\nand the real\\\\n(\\\\nI\\\\nr\\\\n)\\\\nsetup. They use an adversarial loss\\\\nL\\\\nAd\\\\n=\\\\nL\\\\nD\\\\n+\\u03b3\\\\nL\\\\nE\\\\nwhere'\\n\\n'We build our planner, which consists of the encoder\\\\nf\\\\n\\u03c6\\\\n, the forward dynamics model\\\\ng\\\\n\\u03b8\\\\nand the planning loss Lplan in a UPN-style framework.'\\n\\n'Update Generator (Target Encoder)\\\\nf\\\\nt\\\\n\\u03c8\\\\nby ascending its stochastic gradient:\\\\n\\u2207\\\\n\\u03b8\\\\nd\\\\nL\\\\nG'\\n\\n'2) Adversarial transfer of encoder from sim-to-real:'\\n\\n'Once we have a policy that is performing well in the simulator, we aim to learn an encoder that generates the same distribution of latent states over real images as the pre-trained encoder. To achieve this we begin by freezing the source encoder\\u2019s learned weights. We feed in images sampled randomly from the simulation environment and execute one forward pass through the source encoder to yield a latent embedding\\\\nx\\\\nsim\\\\n=\\\\nf\\\\ns\\\\n\\u03c6\\\\n(\\\\no\\\\nsim\\\\n)\\\\nwhere\\\\nf\\\\ns\\\\n\\u03c6\\\\n(\\u22c5)\\\\nis the simulator encoder. We initialize the target encoder with the same weights as the source encoder but do not freeze them (i.e. the weights of the target encoder are trainable). The target encoder is fed images randomly sampled from the real environment and we execute one forward pass to yield a latent embedding\\\\nx\\\\nreal\\\\n=\\\\nf\\\\nt\\\\n\\u03c6\\\\n(\\\\no\\\\nreal\\\\n)\\\\nwhere\\\\nf\\\\nt\\\\n\\u03c6\\\\n(\\u22c5)\\\\nis the real robot encoder.'\",\"89\":\"'https:\\/\\/github.com\\/sisl\\/ngsim_env'\\n\\n'The link to the github repository containing the source code for the experiments can be found at https:\\/\\/github.com\\/sisl\\/ngsim_env.'\\n\\n'The constrained minimax is solved by transforming the problem to an unconstrained form. The constraint for parameter sharing is naturally encoded by sharing the same policy for all agents. The constraint for reward augmentation is enforced by adding a reward augmentation regularizer in the function. Thus, the unconstrained problem becomes:'\\n\\n'Algorithm 1 provides the pseudo code that enacts the above two step procedure and incorporates the reward augmentation by providing penalties.'\\n\\n'Normally, the constraint from reward augmentation is satisfied in the demonstration data. For example, the vehicles do not drive off the road. However, the constraint from parameter sharing may not be satisfied by the demonstrated data, i.e., the assumption that all vehicles are homogeneous may not hold. Then, as discussed above, the learned policy takes an average over different policies. Therefore, reward augmentation improves the learning performance in practice, since it encodes prior knowledge.'\\n\\n'https:\\/\\/github.com\\/sisl\\/ngsim_env'\",\"90\":\"'Given the input rendered depth images, the network encodes the information in these images into a high level representation for predicting NDP. It then decodes this representation through up-sampling these high level representations and merging with higher resolution features of the images using skip layer connections. Figure 2b shows a visual depiction of our architecture. Our model outputs a probability map where each pixel in the image describes the probability of a NDP. We use this probability map to instantiate a mask to apply to a simulated depth image.'\",\"91\":\"'Modern autonomous robots require fast vision capabilities in order to perceive and assess their environment. Computer vision and image processing algorithms are computationally expensive as they have to compute results on millions of pixels. Naturally, the hopes to better understand the nature of visual information, and to secure, store and process them efficiently with the help of quantum properties, like entanglement and parallelism, and above mentioned quantum computing algorithms are very high. Related endeavours resulted in a sub-discipline called quantum image processing (QIMP). The basic idea is that properties of an image, like the colors at certain positions, can be encoded as qubit-lattices [36] , which was widely accepted and formally extended by many representations and possible applications, including videos [37] .'\",\"92\":null,\"93\":\"'Touch sensing is widely acknowledged to be important for dexterous robotic manipulation, but exploiting tactile sensing for continuous, non-prehensile manipulation is challenging. General purpose control techniques that are able to effectively leverage tactile sensing as well as accurate physics models of contacts and forces remain largely elusive, and it is unclear how to even specify a desired behavior in terms of tactile percepts. In this paper, we take a step towards addressing these issues by combining high-resolution tactile sensing with data-driven modeling using deep neural network dynamics models. We propose deep tactile MPC, a framework for learning to perform tactile servoing from raw tactile sensor inputs, without manual supervision. We show that this method enables a robot equipped with a GelSight-style tactile sensor to manipulate a ball, analog stick, and 20-sided die, learning from unsupervised autonomous interaction and then using the learned tactile predictive model to reposition each object to user-specified configurations, indicated by a goal tactile reading. Videos, visualizations and the code are available here: https:\\/\\/sites.google.com\\/view\\/deeptactilempc.'\",\"94\":\"'To avoid any zero error that may be present inherently in the differential pressure sensor, readings are zeroed at the start of every flexion-extension cycle. A 3 second delay is also hard coded into each transition between finger flexion-extension and vice versa to ensure that the ISPAs are fully pressurized and depressurized. We also used a red LED as a visual cue to indicate that intent detection is primed.'\",\"95\":\"\\\"Assistive robotic systems endeavour to support those with movement disabilities, enabling them to move again and regain functionality. Main issue with these systems is the complexity of their low-level control, and how to translate this to simpler, higher level commands that are easy and intuitive for a human user to interact with. We have created a multi-modal system, consisting of different sensing, decision making and actuating modalities, leading to intuitive, human-in-the-loop assistive robotics. The system takes its cue from the user's gaze, to decode their intentions and implement low-level motion actions to achieve high-level tasks. This results in the user simply having to look at the objects of interest, for the robotic system to assist them in reaching for those objects, grasping them, and using them to interact with other objects. We present our method for 3D gaze estimation, and grammars-based implementation of sequences of action with the robotic system. The 3D gaze estimation is evaluated with 8 subjects, showing an overall accuracy of 4.68\\\\\\\\pm 0.14cm. The full system is tested with 5 subjects, showing successful implementation of 100% of reach to gaze point actions and full implementation of pick and place tasks in 96%, and pick and pour tasks in 76% of cases. Finally we present a discussion on our results and what future work is needed to improve the system.\\\"\",\"96\":null,\"97\":\"'This system is designed as a standalone system. Arduino Uno is our selected controller due to its low cost, ease of controlling and programming. The open-source software that is used is Arduino 1.8.5 for motor controlling and CoolTerm software for recording the Flexiforce sensor reading. A baud rate is set at 9600. This system utilizes 2 Arduino Uno. First Arduino Uno is connected to a push button, a distance sensor, and a stepper motor represented in Fig. 4 and Fig. 5 (green text box). Second Arduino Uno is related to a Flexiforce force sensor represented in Fig. 4 and Fig. 5 (yellow text box). The process flow of the resistive force system is illustrated in Fig. 4 and the circuit layout is shown in Fig. 5 . To enter a home position of the puck, the motor slightly rotates upward, followed by a step by step downward rotation. When the distance sensor is triggered, the motor stops downward rotation. And then, the system proceeds to obtain data required to zeroize the system which will be explained in section III . After the data is collected, the motor rotates in an upward direction to flex the rat\\u2019s paw. The motor holds it in position for 10 seconds before rotating downwards. The vertical ground reaction force is recorded by Flexiforce.'\",\"98\":null,\"99\":null,\"100\":null,\"101\":null,\"102\":\"'The CNN is implemented using Python 2.7 and PyTorch 0.4.1. The planner is implemented in C++. Communication is realized via ROS. Code for the CNN, the training data generator and the framework to use the CNN as a heuristic is available online1.'\",\"103\":\"'The pseudo-code of the stochastic approximate kernel path planner is shown in Algorithm 1. The output of this algorithm is an optimised path\\\\n\\u03be\\\\noptimal\\\\n(\\u22c5)\\\\n, parametrised by the weight vector woptimal.'\",\"104\":null,\"105\":null,\"106\":\"'Most of the related works on temporal logic-based motion planning and control resort to full workspace partitioning in order to encode safety specifications (e.g., obstacle avoidance) and to facilitate the synthesis of controllers that implement the transitions of the abstracted discrete system. Regarding timed temporal tasks, many related works (e.g., [20], [24]) use optimization techniques that yield maximum velocity controllers, to obtain upper bounds on the transition times between the states of the abstracted system. This approach is, in general, conservative and might lead to unnecessarily high control effort. Works that focus solely on task assignment neglect the continuous dynamics entirely [15], [17], [21].'\\n\\n'that encode the physical transition from\\\\n\\u03c0\\\\np\\\\nj\\\\nto\\\\n\\u03c0\\\\np\\\\nj+1\\\\nin\\\\nA\\\\n7\\\\n\\u2283\\\\n. The intersection of the respective guards\\\\ng\\\\nj,j+1\\\\n,\\\\ng\\\\nj\\\\n\\u03b9\\u03b9+1\\\\n,\\u03b9\\u2208{0, . . . , \\\\n\\u2113\\\\nj\\u22121\\\\n}\\\\n, provides a time interval of the form\\\\nI\\\\nj,j+1\\\\n\\u2208{[a, b],[a, b)\\\\n,\\\\n(a, b], (a, b)\\\\n,\\\\n[a, \\u221e)\\\\n,\\\\n(a,\\u221e\\\\nwith\\\\na,b\\u2208\\\\nQ\\\\n>0\\\\n,b>a\\\\n, such that,\\\\nt\\\\nj,j+1\\\\n\\u2208\\\\nI\\\\nj,j+1\\\\n\\u21d2\\\\nt\\\\nj,j+1\\\\n\\u22a8\\\\ng\\\\nj,j+1\\\\n,\\\\nt\\\\nj,j+1\\\\n\\u22a8\\\\ng\\\\nj\\\\n\\u03b9\\u03b9+1\\\\n, for\\\\n\\u03b9\\u2208{0, . . . , \\\\n\\u2113\\\\nj\\u22121\\\\n}\\\\n, where\\\\nt\\\\nj,j+1\\\\nis the time duration of the navigation\\\\n\\u03c0\\\\nj\\\\n\\u2192\\\\n\\u03c0\\\\nj+1\\\\nt\\\\njj+1\\\\n. Note that\\\\nI\\\\ni\\\\nj,j+1\\\\nmight be a function of the previous transition duration\\\\nt\\\\nj\\u22121,j\\\\n.'\",\"107\":null,\"108\":null,\"109\":\"'One challenge in developing automated controllers using heuristics is that human experience and common sense needs to be coded into the form of an algorithm. This may not be trivial given the intricate coordination of the gas command and bucket actuation required to successfully load a bucket, which can change with different material properties. Another challenge is that humans and computers may find different types of sensing information to be useful. A human operating an excavation machine may use the sense of hearing, for example, to estimate the current load on the engine, while a computer could directly sense engine parameters.'\\n\\n'When analyzing recorded signals from the initial testing, it was found that spikes in the gas command occasionally occurred near the end of the scooping action. This caused the problem of driving up the slope after extraction, and tended to happen only with LM NNs. The reason for this, however, is not well understood, and highlights a challenge in using NNs as controllers: unpredictable behaviours can arise, and some hard-coded safety limits may be needed to prevent accidents. This particular problem could again be avoided by disengaging the NN after the bucket has been lifted a certain amount.'\",\"110\":null,\"111\":null,\"112\":null,\"113\":\"'https:\\/\\/bic.1y\\/2IzeDde'\",\"114\":null,\"115\":null,\"116\":\"'We propose a combined method for the collaborative transportation of a suspended payload by a team of rotorcraft. A recent distance-based formation-motion control algorithm based on assigning distance disagreements among robots generates the acceleration signals to be tracked by the vehicles. In particular, the proposed method does not need global positions nor tracking prescribed trajectories for the motion of the members of the team. The acceleration signals are followed accurately by an Incremental Nonlinear Dynamic Inversion controller designed for rotorcraft that measures and resists the tensions from the payload. Our approach allows us to analyze the involved accelerations and forces in the system so that we can calculate the worst case conditions explicitly to guarantee a nominal performance, provided that the payload starts at rest in the 2D centroid of the formation, and it is not under significant disturbances. For example, we can calculate the maximum safe deformation of the team with respect to its desired shape. We demonstrate our method with a team of four rotorcraft carrying a suspended object two times heavier than the maximum payload for an individual. Last but not least, our proposed algorithm is available for the community in the open-source autopilot Paparazzi.'\",\"117\":\"'https:\\/\\/github.com\\/ndehio\\/2019_ICRA'\\n\\n'Tracking multiple prioritized tasks simultaneously with redundant robots have been investigated extensively over the last decades. Recent research focuses on combining advantages from both classical soft and strict prioritization schemes which is non-trivial. Among the proposed methods to tackle this issue, Generalized Hierarchical Control (GHC) seems to have a reasonable performance, however, it does not include a weighting matrix in the computation of the nullspace projection operator and hence cannot construct dynamically-consistent stack-of-tasks hierarchies as a special case. We extend GHC by adding dynamic-consistency to the control scheme and refer to it as DynGHC. The extension is also advantageous when choosing non-strict priorities because inertia coupling between tasks is reduced. DynGHC allows to smoothly rearrange priorities which is important for robots acting in dynamically changing contexts. Comparative simulations with a 4 DOF planar manipulator and a KUKA LWR validate our approach. Matlab and C++ source code is made available.'\\n\\n'This section proposes a novel approach referred to as DynGHC which adopts and extends GHC such that dynamic consistency can be achieved and therefore combines advantages from both strict and soft prioritization schemes. The approach is published as Matlab and C++ source code on https:\\/\\/github.com\\/ndehio\\/2019_ICRA.'\\n\\n'https:\\/\\/github.com\\/ndehio\\/2019_ICRA'\",\"118\":null,\"119\":null,\"120\":null,\"121\":\"'The goal of the simulations is to investigate how SB impacts the estimation and control performance of a robot tracking a pre-defined trajectory through varying slip conditions. The choice to use a space-based\\u2013rather than time-based\\u2013trajectory was made because the unknown slip conditions can significantly impact the robot speed, and the time deviations arising from these uncertainties can challenge the tracking of a time-based reference trajectory. With a space-based trajectory, the robot is not forced to be at any given point on the trajectory at a particular time-step, but may still be provided with a speed reference as part of the trajectory to encode the desired velocity at any given point on the path.'\\n\\n'Since the ACADO Code Generation tool does not yet support parameter estimation functionality, we instead augment the state and system model (1) with the slip parameter terms and two corresponding virtual controls,\\\\nu\\\\n\\u03ba\\\\nand\\\\nu\\\\n\\u03bc\\\\n.'\",\"122\":null,\"123\":null,\"124\":null,\"125\":\"'https:\\/\\/git.io\\/fAX7k'\\n\\n'We propose a data-driven approach that does not rely on explicit assumptions about the environment, but instead learns regularities from examples. Specifically, we employ Variational Autoencoders (VAE) [16] to predict unknown map regions beyond frontiers. In our chosen indoor setting this allows us to learn generalizations over many building floor plans to make informed decisions for faster exploration of novel floor plans. As illustrated in Figure 1, using our VAE based map prediction, we estimate the potential areas that can be mapped from each frontier which we demonstrate can be more accurate and efficient than a single or multi-step lookahead in the sensor measurements.'\\n\\n'Figure 5 shows the architecture of our map prediction network. The encoder part of the network learns to output a latent representation z in a lower dimensional space and the decoder reconstructs the missing parts of the map using the compact latent representation.'\\n\\n'Encoder: Our encoder is based on the ResNet architecture [41]. We use the ResNet architecture on account of its demonstrated performance in the task of image classification on benchmark datasets and short training time.'\\n\\n'Encoder:'\\n\\n'The encoder outputs a mean (\\\\n\\u03bc\\\\n) and a\\\\nlog\\\\nvariance\\\\nlog(\\u03c3)\\\\nof the encoding of size\\\\n8\\u00d78\\u00d7512\\\\n. The final encoding fed to decoder network is sampled from the Gaussian distribution\\\\nN(\\u03bc, \\u03c3)\\\\n. We do not use fully connected layers as they explode the number of parameters in the network and did not yield a significant improvement in our results.'\\n\\n'Decoder: The decoder network is essentially a mirror image of the encoder. While the encoder reduces the size of the feature plane while increasing their number, the decoder does the opposite. This is achieved by transposed convolution layers (also known as deconvolution layers) [42]. The output from the decoder network is a probability map of obstacles, with pixel values in the range 0 to 1. We then apply a threshold of 0.5 to determine obstacles and free spaces.'\\n\\n'Decoder:'\\n\\n'Reproduction, code and data products'\\n\\n'All code, data and models used in the experiments are available at https:\\/\\/git.io\\/fAX7k.'\",\"126\":null,\"127\":null,\"128\":\"'End loop actuator (Fusion360 perspective drawing): (1) Upper layer (2) Bottom layer (3) Inner channel (4) Screw hole (5, 6) Channel ends (7, 8) Tubing fitting and adapter (9) Carriage (10) Slot (11) Piston (12) Optical encoder strip (13, 14) Stop switch.'\\n\\n'RTS Fusion 360 3D schematics: (1, 2) SMT tubing connector (3) Screw opening (4) Optical encoder and strip for translation (5) Optical encoder and strip for rotation (6) Tool carriage (7) Inner channel of DoF-1 (rotation) (8) Inner block (9) Extended piston (10) Mount slot (11) Inner channel of DoF-2 (translation) (12) Ball-and-socket A (13) Sprocket (14) Bridge connection (15) Ball bearing. See video for an exploded drawing and animation.'\\n\\n'We are at an early stage of understanding the SMT mechanism and assessing its potential and means for implementing it. The data presented herein illustrated that closed-loop SMT can quickly and accurately respond to a reference signal. Additionally, the error for lengths from 1 to 4 m can be regulated within the resolution of a 0.05 mm optical encoder. From the system studies we concluded that the experiment results on our one-DoF manipulator verified the hypothesis of four primary factors (friction, length, expansion, and mechanical design) proposed in our previous work [12]. We also presented and analyzed an SMT-actuated two-DoF manipulator design and its closed-loop positioning control performance. Because of the high nonlinear characteristics presented in the experiment results, in the future we will focus on system identification to achieve better control results, and will optimize the mechanical design of SMT-based manipulators.'\",\"129\":null,\"130\":null,\"131\":null,\"132\":null,\"133\":null,\"134\":null,\"135\":null,\"136\":null,\"137\":null,\"138\":null,\"139\":\"'https:\\/\\/www.youtube.com\\/watch?v=8fWLFA97TCo'\",\"140\":\"'We address the problem of executing tool-using manipulation skills in scenarios where the objects to be used may vary. We assume that point clouds of the tool and target object can be obtained, but no interpretation or further knowledge about these objects is provided. The system must interpret the point clouds and decide how to use the tool to complete a manipulation task with a target object; this means it must adjust motion trajectories appropriately to complete the task. We tackle three everyday manipulations: scraping material from a tool into a container, cutting, and scooping from a container. Our solution encodes these manipulation skills in a generic way, with parameters that can be filled in at run-time via queries to a robot perception module; the perception module abstracts the functional parts of the tool and extracts key parameters that are needed for the task. The approach is evaluated in simulation and with selected examples on a PR2 robot.'\\n\\n'For example consider the manipulation of scraping a sticky substance from a hand-held domestic tool into some container. We would like to endow the robot with this skill, coded in a sufficiently generic way that it could automatically adapt to scenarios such as depicted in Fig. 1: 1) peanut butter on a knife can be put back in a jar of peanut butter by scraping the knife on the inner edge of the jar opening; 2) butter stuck to a spatula can be put in a frying pan by scraping on the inner edge of the pan.'\\n\\n'Their work had a hard-coded interpretation of objects for pouring; we learn affordances through simulated trials of tools [3], thereby acquiring a deeper semantic grounding for our task skills. This is a significant step towards more cognitive robots that form their own interpretation of the world rather than relying on the pre-formed interpretation of a human designer.'\\n\\n'The system relies on task descriptions that are hand-engineered and coded in a generic way, referring to key features of tools and target objects, such as \\u2018edge point\\u2019 or \\u2018tool tip\\u2019 or \\u2018centre of top\\u2019. These features can be grounded, by vision, in many different tools and target objects, thereby permitting the manipulation to be adapted to these objects. An example of a partial task description appears at the bottom of Fig. 2.'\\n\\n'The entire process is not real-time, but done offline and later exported to the robot. Some components are rather slow, e.g. the vision module, because it is coded in Matlab, incurs the overhead of loading Matlab Runtime. However the superquadric fitting algorithms used are fast enough to be implemented in a real-time perception system if required.'\\n\\n'Currently our approach relies on the human designer identifying the key features that are needed from vision to perform tasks robustly. The human then hand codes a generic motion script for the skill, where these features are used to parameterise the motions. In future it would be better if the robot learns its own specific skill from demonstration in one situation, and then generalises. In fact the robot motion component that this paper is based on has already been extended in this direction [5]. The next logical step would then be to tackle the full transfer problem as described by Fitzgerald et al. [2], including, e.g. when new planning steps might be introduced to deal with situations such as a pot having a lid which needs to be removed.'\",\"141\":null,\"142\":null,\"143\":null,\"144\":null,\"145\":null,\"146\":null,\"147\":null,\"148\":null,\"149\":\"'In this paper, we designed and characterised a refinement unit \\u201cLookUP\\u201d to our localisation system for vehicles in underground mine tunnel environments. It works by finding homographies based on matched pixels between query and reference images of the mine ceiling. The accuracy of LookUP is enhanced by generating pixel correspondences only on high-quality sample points proposed by an FCN. Selectively processing high-quality sample points also significantly increased the frame rate to \\u223c5 fps. This result was obtained using code that is yet to be optimised and could potentially be even faster if a GPU is available in the system. The proposed system provides a viable framework for industrial applications in underground mines.'\",\"150\":null,\"151\":null,\"152\":\"'https:\\/\\/github.com\\/joaquinballesteros\\/Smart-Cane'\\n\\n'https:\\/\\/github.com\\/joaquinballesteros\\/Smart-Cane'\\n\\n'https:\\/\\/github.com\\/joaquinballesteros\\/Smart-Cane'\",\"153\":null,\"154\":null,\"155\":\"'(a) Drawing of the differential gear system. (b) CAD model cross section of the DCSEA. The asterisk indicates an encoder-measured body.'\\n\\n'The motor and encoder pair used in this actuator is a Cytron IG42E-24K with a 24:1 reduction gearbox with a nominal torque of 980 mNm with an encoder resolution of 480 pulses per rotation. The brake is a magnetic particle brake made by Placid Industries (B15-12-1) with a torque range of 34 mNm to 1,700 mNm.'\",\"156\":null,\"157\":null,\"158\":null,\"159\":null,\"160\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/33628614'\",\"161\":null,\"162\":null,\"163\":null,\"164\":null,\"165\":null,\"166\":null,\"167\":null,\"168\":null,\"169\":null,\"170\":\"'This work is partially supported by a Google Focused Research Award and was performed jointly at the AUTOLAB at UC Berkeley and at the Stanford Vision & Learning Lab, in affiliation with the Berkeley AI Research (BAIR) Lab, Berkeley Deep Drive (BDD), the Real-Time Intelligent Secure Execution (RISE) Lab, and the CITRIS \\u201dPeople and Robots\\u201d (CPAR) Initiative. Authors were also supported by the SAIL-Toyota Research initiative, the Scalable Collaborative Human-Robot Learning (SCHooL) Project, the NSF National Robotics Initiative Award 1734633, and in part by donations from Siemens, Google, Amazon Robotics, Toyota Research Institute, Autodesk, ABB, Knapp, Loccioni, Honda, Intel, Comcast, Cisco, Hewlett-Packard and by equipment grants from PhotoNeo, and NVidia. This article solely reflects the opinions and conclusions of its authors and do not reflect the views of the Sponsors or their associated entities. We thank our colleagues who provided helpful feedback, code, and suggestions, in particular Jeff Mahler.'\\n\\n'When operating in unstructured environments such as warehouses, homes, and retail centers, robots are frequently required to interactively search for and retrieve specific objects from cluttered bins, shelves, or tables. Mechanical Search describes the class of tasks where the goal is to locate and extract a known target object. In this paper, we formalize Mechanical Search and study a version where distractor objects are heaped over the target object in a bin. The robot uses an RGBD perception system and control policies to iteratively select, parameterize, and perform one of 3 actions - push, suction, grasp - until the target object is extracted, or either a time limit is exceeded, or no high confidence push or grasp is available. We present a study of 5 algorithmic policies for mechanical search, with 15,000 simulated trials and 300 physical trials for heaps ranging from 10 to 20 objects. Results suggest that success can be achieved in this long-horizon task with algorithmic policies in over 95% of instances and that the number of actions required scales approximately linearly with the size of the heap. Code and supplementary material can be found at http:\\/\\/ai.stanford.edu\\/mech-search.'\",\"171\":null,\"172\":null,\"173\":null,\"174\":null,\"175\":null,\"176\":\"'A motor test bench that couples two motors with a non-contact torque meter (MCRT 48200V, S. Himmelstein and Company, Hoffman Estates IL) was constructed. An analog signal adapter board was built in house to shift and scale the torque meter output signals for reading with FlexSEA. Two 14-bit magnetic on-axis relative encoders (AS5047P and AS5047D, AMS AG, Premstaetten, AT) and two FlexSEA systems drive the motors. Of the two motors, one is being tested and will be referred to as the test motor; the other acts as a variable load and will be referred to as the load motor. The two motors can be used interchangeably on the test bench.'\",\"177\":null,\"178\":null,\"179\":null,\"180\":null,\"181\":null,\"182\":\"'MCL estimates a belief over the robot pose using a set of weighted particles where each particle represents a possible pose of the robot. For our implementation, we consider pose as the position and its orientation of the robot on the field surface. The MCL filter performs two main steps to maintain the belief over the pose. It first propagates the particles based on the odometry estimated by the wheel encoder measurements from the robot. We use the odometry motion model based on the wheel encoder readings as described in [18] to implement this step. Whenever a new measurement\\\\nZ\\\\nis available, it updates the weight of each particle based on an observation model. This model provides a measure of how well the observation agrees with the map given the current pose. Finally a new set of particles is re-sampled from the old ones, where the chance of survival for each particle is proportional to its weight in the old particle set.'\\n\\n'The experiments were performed on a real sugarbeet field, where we recorded data over several weeks. The images for generating the aerial map were taken at the beginning of the season using a DJI Phantom 4 UAV. These images were captured from a height of 10 m covering the whole field. The orthomosaic map generated from the images has a ground resolution of 5 mm per pixel. For recording the ground robot data, we use a Clearpath HuskyA200 equipped with wheel encoders, Ublox EVK-7 GPS and a ZED stereo camera. We only use the RGB images from the left camera for our experiments. The camera was mounted at a height of 1.2 m from the base, tilted at an angle of 45o towards the ground, see Fig. 3. We operated the ground robot by manually joysticking it with an average speed of 0.6 m\\/s. We collected the data over five different sessions, each roughly separated by a week. During this period, the crop size ranged between 5 cm to 20 cm in diameter. Additionally, a weeding treatment was performed by the farmers just before the third session where most of the weeds in the field were removed.'\",\"183\":null,\"184\":\"\\\"This paper presents a pipeline for semantic segmentation of trees into their components. Given a single RGB-D image of a tree, we employ a deep network to predict labels to classify each pixel of the tree into trunk, branches, twigs and leaves. Multiple convolutional neural network architectures to combine the complementary modalities of depth and colour data are investigated. An asynchronous training approach where two networks trained separately on RGB and depth encoded as a 3-channel HHA image are combined using a late fusion architecture with different learning rates performs the best. Training and evaluation are performed on a synthetic dataset of 6 species of broadleaf trees. We further demonstrate the network's generalization capabilities, across various tree species on the synthetic dataset, achieving an accuracy of upto 92.5%. Furthermore, we present a qualitative evaluation of our approach on real-world data.\\\"\\n\\n'Deep learning based approaches have surpassed traditional geometry based approaches for semantic segmentation. These methods, mostly based on CNNs, typically consist of a pre-trained encoder as in [16] and a decoder. The Fully Convolutional Network (FCN) [17], extends CNNs by replacing the fully connected layers with convolutional ones thus allowing arbitrary input sizes. FCNs form the basis of most state-of-the-art segmentation networks. Another highly successful network, the SegNet [18] follows a similar architecture, with the novelty being the use of pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear up-sampling. An alternate solution to upsampling is presented in DeepLab [19]. Multimodal learning introduced in [20] leverages complementary benefits if the different modalities to improve performance. For example Kwang et al. [21] use multi-spectral images combining thermal and colour images to detect pedestrians in a scene. With the availability of commercial RGB-D sensors, approaches combining depth and colour information have been developed. Gupta et al. [22] propose the HHA encoding for depth images and combine features extracted on this image with colours images using an SVM classifier. Eitel et al. [23] instead apply a colour-map to normalized depth images and use that as another 3-channel image stream similar to colour images. More recently, Valada et al. [13] use depth information directly but replace the convolutional layers with residual [24] layers to increase the depth of the networks. The method proposed in this paper is similar to the method of [22] but instead we replace the classifier with a set of trainable convolutional layers. We also use an asynchronous training approach with different learning rates for different parts of the network. For a comprehensive review of deep semantic segmentation, we direct the reader to [25].'\\n\\n'Figure shows the 3 channels of an HHA encoded depth image. Blue represents small values while red represents large values. The depth image has no noise added for the sake of illustration.'\\n\\n'The second approach to incorporate depth information is to perform late fusion, i.e. in the feature space. The depth image is encoded as a 3-channel HHA [22] image comprising of depth, height from the ground and angle of the surface normal with gravity as the three channels.'\\n\\n'In a general robot scanning scenario, the height above ground may not always be available, since only a relative pose with respect to an arbitrary initial reference frame is typically estimated. In order to account for this lack of height information, we also train a variation of the RGBHHA network where the depth image is encoded as a two channel image with just depth and angle with the vertical direction as the channels. We refer to this architecture as the RGB-HA network.'\",\"185\":null,\"186\":null,\"187\":\"'We propose a palpation procedure to assess the ripeness of mango, and devise a simplified system to model the fruit\\u2019s pulp and skin behavior throughout palpation. In the model we exemplify the scenario where a finger, equipped with a force sensor, is in contact with the surface of a mango. The finger is actuated by a motor, and its displacement is known by means of a motor encoder. Fig. 2 shows the modeled elastic response of the probing finger, and the object\\u2019s surface, as system of springs. We choose a linear model as the simplest mechanical model of the fruit, and make the simplifying assumption that each spring is constant. The probing finger has a spring stiffness constant of Kr, while the mango can be seen as a two layered structure, the first layer of which consists of the spring response of the skin, with a spring constant of Ks, and the second the spring response of the pulp, with a spring constant of Kp. The lengths of each are also respectively\\\\nX\\\\nr\\\\n,\\\\nX\\\\ns\\\\nand Xp. The estimation of the produce\\u2019s stiffness is equivalent to retrieving the elastic constant Ke. The motor generates a torque capable of directly influencing the distance between the finger and the produce. At equilibrium, the forces generated by the probing fingers Fr equate the reacting forces from the produce\\u2019s surface Fe, thus\\\\nF\\\\nr\\\\n=\\\\nF\\\\ne\\\\n, i.e.:'\\n\\n'Finally, the motor displacement as computed by the encoder corresponds to\\\\n\\u25b3\\\\nX\\\\nm\\\\n=\\u25b3\\\\nX\\\\nr\\\\n+\\u25b3\\\\nX\\\\np\\\\n. So from equation 2, and the simplifying assumption we have:'\\n\\n'Fig. 4 shows the gripper used for the experiments. The gripper is composed by a main rectangular case containing a lead screw and two metallic rods. The chamber contains two opposite fingers, which remain parallel to each other throughout the gripper\\u2019s actuation. We designed two fingers with flat surfaces at the extremities, capable of holding the referenced tactile sensor. A central actuation unit reduces the distance between the fingers by actuating one finger thorough a Micro Metal Gearmotor motor, with a 6:1 gear ratio and equipped with a rotary encoder. The rotational actuation movement is then transfered into a linear displacement by the lead screw and metallic rod. We control the motor via a TB6612FNG Dual Motor Driver Carrier controller. Each gripper component was 3D-printed, for fast prototyping.'\",\"188\":\"'For visual odometry we implement the open-source ORB-SLAM2 [1] package on our image sequence. To obtain the calibration file required for ORB-SLAM2, we overlay the standard checkerboard [25] on Google Earth and capture screenshots from different viewpoints. Additionally, we scale the ORB-SLAM2 output using the true position information for the first 100m of the trajectory. We do this to simulate the presence of GPS in the beginning of the flight. After scaling the ORB-SLAM2 trajectory output, we extract incremental position estimates\\\\n\\u25b3\\\\np\\\\nVO\\\\nand incremental rotation matrix\\\\n\\u25b3\\\\nR\\\\nVO\\\\nbetween consecutive images.'\",\"189\":\"'http:\\/\\/open.vision.computer\\/'\\n\\n'http:\\/\\/open.vision.computer\\/'\\n\\n'As much as possible, the design is intended to be open. Specifically, the schematics, PCB layouts, bill of materials, FPGA firmware, Linux kernel and userland driver source code are available online and released under permissive licenses: http:\\/\\/open.vision.computer'\\n\\n'Source code of the higher level software, including the Visual Inertial Odometry system, is also released under an open source license, so that others may develop on or learn from our efforts.'\\n\\n'The electrical design and circuit board layout were accomplished using KiCAD, a cross-platform open source design automation suite 1. As such, the design can be viewed and modified without requiring proprietary tool licenses. 1'\\n\\n'We have made the design of the Open Vision Computer and the associated software available as an open source project to allow others to build on or learn from our work.'\",\"190\":null,\"191\":\"'In our application, the encoder and decoder architecture are based on two long short-term memory (LSTM) networks [31] with 512 hidden units. This allows the network to learn when to forget previous hidden states and when to update hidden states given new information. In addition, we wrap the LSTM with an attention layer [29, 30] to handle possible long-length sequences.'\",\"192\":null,\"193\":\"'Our work presents multiple contributions. First, we show the ability to have a real-time, on-board framework that is able to estimate the dynamic properties of an aerial platform, without relying on external infrastructure. Second, we show that if vehicle parameters are subject to changes during the operation, the platform is able to quickly re-estimate the parameters of the new configuration. Third, we provide an observability analysis, which shows that the presented system is observable and goes into more detail about which parameters can be estimated with the IMU or pose sensor respectively. Finally, we release our current framework as an open source package to the community1. This will allow other researchers to compare their developments to our baseline as our approach is applicable to multirotors and with small modifications to a wide class of other dynamic systems.'\",\"194\":null,\"195\":null,\"196\":null,\"197\":null,\"198\":null,\"199\":\"'As with torque, many solutions exist for measuring shaft speed. Magnets can be mounted on the shaft and hall effect sensors can be used to determine when a point on the shaft passes by. Differentiating this signal produces the angular speed. Similarly, encoders and photo tachometers track when parts of the shaft pass and differentiate for speed. In addition, motors can be driven by the shaft and the output voltage is approximately proportional to the speed. However, all these require part of the device to be stationary in the non-rotating reference frame. In some cases, it is advantageous to have no parts fixed to the stationary reference frame.'\",\"200\":\"'it encodes the directions of force control. Then we have\\\\nT=\\\\ndiag\\\\n(\\\\nI\\\\nu\\\\n,\\\\nR\\\\na\\\\n)\\\\n. The procedures are summarized below:'\\n\\n'It encodes the least-square solution for the equality constraints. Finally we solve (22) together with inequalities (17) for all forces. The procedure is summarized below:'\\n\\n'The Matlab implementation of the two algorithms along with several examples can be obtained from https: \\/\\/github.com\\/yifan-hou\\/pub-icra19-hybrid-control.'\",\"201\":\"'The front segment\\u2019s shape during bending can be estimated from the lengths of the actuating tendons, available from onboard encoders. In this case, the segment was assumed to be a long hollow tube of constant diameter and the three tendons equally spaced 120\\u00b0 apart. When bending, it was assumed the segment formed an arc of constant curvature. The tendon arrangement is shown in Fig. 3 (a) and the kinematic variables,\\\\n(\\u03b8,\\u03d5 and s)\\\\ndescribing the shape of the segment are shown in Fig. 3 (b). Here,\\\\n\\u03b8\\\\nis the bending angle,\\\\n\\u03d5\\\\nis the angle which the bending plane makes about the z-axis and s is the length of the segment\\u2019s center line.'\\n\\n'Thus, with feedback control of the tendon lengths from on-board encoders, the shape of a segment can be estimated. As each segment is light weight, gravity is not considered. In this model, only the hollow, flexible section is considered to be bending (see Fig. 2). The camera housing, tension sensor assembly and DC motor actuation assembly are considered rigid.'\",\"202\":\"'Autonomous intelligent systems outperform human workers in an expanding range of domains, typically those in which success is a function of speed, precision and repeatability, but many tasks remain beyond their reach. As humans, we excel at solving problems requiring planning, spatial reasoning, semantic interpretation, social collaboration and creativity. Each of these skills is in high demand in the robotics industry, and each is tested rigorously during video gameplay. By way of example: in the Shakespeare Anthology puzzle of the 2003 survival game Silent Hill 3, the goal is to enter a four-digit code into a keypad. To obtain the numbers, the player must discover a poem scrawled on a nearby scrap of paper, peruse the stanzas to identify cryptic allusions to a variety of Shakespearean plays, and follow a convoluted series of deductions requiring an intimate knowledge of the bards literary output. Such a feat is far beyond the capacities of today\\u2019s artificial intelligent systems.'\\n\\n'The formulation is also summarised as pseudo-code in Algorithm 1 that is presented below. Operations that must be completed by bespoke crowd computation software are represented by black imperative statements. Operations completed by human gamers or within the robot\\u2019s context are represented by grey declarative statements. The designer of a crowd computer would have no direct control over the latter kind of operation.'\",\"203\":null,\"204\":null,\"205\":null,\"206\":\"'https:\\/\\/youcu.be\\/b0mr5GHHjBg'\\n\\n'https:\\/\\/youCu. be\\/K9R4y2wluPw'\\n\\n'https: \\/\\/youtu.be\\/b0mr5GHHjBg'\\n\\n'Cloud Robotics refer to any robot or automation system that relies on either data or code from a network to support its operation [13]. The term was introduced by James Kuffner in 2010. It was evolved from Networked Robotics [14]. Well-known Cloud Robotic Systems includes: RoboEarth\\u2019s Rapyuta [15], motion planning for services at both cloud [16] and edge [17], Berkeley robotics and automation as a service (Brass) [18], and Dex-Net as a Service (DNaaS) [19] [20] [7], just to name a few. However, network costs in the form of privacy, security latency, bandwidth, and reliability present a challenge in Cloud Robotics [21].'\\n\\n'With the \\u201cheartbeat\\u201d design, we can teleoperate the self-balancing robot reliably through the Cloud. To teleoperate the robot for box pickups, we hardcode a box pickup motion with the two robot arms. We first attempt to pick up a box with a joystick via direct cloud teleoperation. However, even with a reliable teleoperation module and pre-programmed pickup motion, we find it extremely difficult to finish the task using cloud teleoperation. We suspect that natural, immersive 3D visual perception is critical for a human to pickup objects efficiently, but a human teleoperator loses these perceptions using our current cloud teleoperation interface. In another word, our cloud teleoperation interface is not intuitive for efficient object pickups.'\\n\\n'Like other service robots, Igor needs to interact and cooperate with human beings. We demonstrate the advantages of visual servoing: (1) it requires no calibrations before each robotic task; (2) it can handle dynamic human robot interaction, such as following a human to pick up a box from that person. One failure case is when the human carrier tricks the robot. It happens if the human carrier move the box after the robot commits to the final phase of box picking, which is hard-coded. Instead, we can automate the pickup motion based on visual servoing as well, and we will leave it as future work.'\",\"207\":null,\"208\":null,\"209\":null,\"210\":null,\"211\":null,\"212\":\"'https:\\/\\/drive.google.com\\/drive\\/folders\\/1rzP5nhhFL9jfXn_ipbE29lJ8-QfrcaPW?usp=sharing'\",\"213\":\"'The problem of minimizing cost in nonlinear control systems with uncertainties or disturbances remains a major challenge. Model predictive control (MPC), and in particular sampling-based MPC has recently shown great success in complex domains such as aggressive driving with highly nonlinear dynamics. Sampling-based methods rely on a prior distribution to generate samples in the first place. Obviously, the choice of this distribution highly influences efficiency of the controller. Existing approaches such as sampling around the control trajectory of the previous time step perform suboptimally, especially in multi-modal or highly dynamic settings. In this work, we therefore propose to learn models that generate samples in low-cost areas of the state-space, conditioned on the environment and on contextual information of the task to solve. By using generative models as an informed sampling distribution, our approach exploits guidance from the learned models and at the same time maintains robustness properties of the MPC methods. We use Conditional Variational Autoencoders (CVAE) to learn distributions that imitate samples from a training dataset containing optimized controls. An extensive evaluation in the autonomous navigation domain suggests that replacing previous sampling schemes with our learned models considerably improves performance in terms of path quality and planning efficiency.'\\n\\n'To overcome these limitations, in this work we propose to extend IT-MPC by adding an informed sampling process, i.e., a learned distribution, that is aware of the robot dynamics, of the environment and of the global task to accomplish. We use Conditional Variational Autoencoders (CVAE) [21] to learn distributions that imitate samples from a training dataset containing optimized controls. Sampling from this distribution leads to improved performance since less samples fall in costly parts of the state space. Furthermore, to learn task-driven robot behaviors, differently from its original formulation, we propose a novel loss function for the CVAE. The latter considers also the environmental conditions in its reconstruction error. In a series of experiments, we show that our approach outperforms a set of baselines in different environments in terms of final trajectory cost, traveled distance and task success.'\\n\\n'In this work we will use Conditional Variational Autoencoders [21] to learn a model that can be used to generate informed sampling distributions for IT-MPC-based controllers, i.e., to provide samples in low-cost areas of the state space to improve efficiency of the controller in the desired task. A Conditional Variational Autoencoder (CVAE) is trained to imitate a distribution of observed data\\\\nX\\\\n(i)\\\\n\\u2208\\\\nR\\\\nN\\\\nx\\\\nconditioned on\\\\nC\\u2208\\\\nR\\\\nN\\\\nb\\\\nusing an unobserved, latent representation\\\\nZ\\u2208\\\\nR\\\\nN\\\\nz\\\\n, i.e.,\\\\np(X|C)=\\\\n\\u222b\\\\nz\\\\np(X|z,C)p(z|C)dz\\\\nof the states. Fig. 2 illustrates the two components of a CVAE, an encoder and a decoder. The encoding process finds a parametric function (e.g., a neural network) that maps the input X and conditions C to a normal distribution\\\\nq(z|X,C)\\\\nwith a mean\\\\n\\u03bc\\\\nand variance\\\\n\\u03a3\\\\nin the latent space z. The decoder can be intuitively interpreted as the reverse process of encoding. It finds a parametric function that given a latent variable and conditions computes samples from the input distribution, that is\\\\np(X|z,C)\\\\n. The main idea of CVAE is not to directly maximize the marginal likelihood\\\\nlogp(\\\\nX\\\\n(i)\\\\n|C)\\\\n, but its Variational Lower Bound of the form'\\n\\n'Algorithm 1 Informed IT-MPC algorithm. Blue represents the pseudocode to remove to obtain the original IT-MPC.'\\n\\n'U\\\\n^\\\\n\\u2190\\\\nCVAEDecoder(C)'\\n\\n'Scheme of the CVAE, in which the decoder generates an approximation\\\\nX\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\nof the given inputs X. The network receives two concatenated vectors with the input values and the conditions C. The encoder and decoder are both composed of four fully connected layers with a ReLU as the activation function, each circle in the Figure represents 100 nodes. Texts in the dotted boxes describe components of the loss function.'\\n\\n'In this work we present an Informed approach to Information Theoretical Model Predictive Control (IIT-MPC). By using Conditional Variational Autoencoders (CVAE) to learn distributions that imitate samples from a training dataset containing optimized controls, we guide the sampling distribution of IIT-MPC towards less costly area of the state space. This allows our approach to achieve better minimization of the designed cost function. An extensive evaluation in the autonomous navigation domain suggests that replacing the previous IT-MPC sampling scheme with our learned models considerably improves performance in terms of path quality and also planning efficiency (i.e., completions of path tracking and avoiding obstacles tasks, reducing the number of necessary samples) compared to a set of baselines. Moreover, we show in the evaluation that learning the CVAE parameters by using task-dependent loss functions (i.e., reconstruction term) results in better planning performance compared to a standard Euclidean distance loss. We plan to extend the approach to dynamic environments by including into the CVAE conditions also human attributes, positions and velocities.'\",\"214\":\"'Real robot manipulator joints suffer from backlash, and noisy encoder angle feedback which translates into end effector frame uncertainty once forward kinematics is computed. For that reason end effector frame was also presenting an unsteady behavior.'\\n\\n'We observe a decline in performance when testing in the real robot, due to three issues: i) incorrect arm calibration, ii) unreliable marker detection, and iii) unreachable goal pose. Failures caused by inaccurate calibration were revealed by running Rviz real-world representation, along with the real arm execution. In the simulation software, the robot reached the goal pose, while that in the real world the end effector was slightly shifted with respect to the marker, due to arm calibration bias. Another issue was occlusion of the marker by the robot body. By observing the Rviz real-world representation we could easily observe that the goal pose some times disappears from the vision, which results in absense of a goal pose. One possible solution for this problem is to include an object tracking algorithm that is robust to occasional misdetections of the target. This problem of losing the marker detection affected negatively our experiments, since it increased the failure rate in the side grasping real robot experiment. Because of the starting arm configuration, the probability of covering the marker with the manipulator physical part was higher than in the front grasp task. Another issue is limited camera vision. In most of the cases, we were able to turn the robotic head to keep the marker in the vision. The issue encountered during side grasp tests when the starting head position is turned into the direction of the goal pose, while it is already close to its limits on the left side. A last observed issue, also contributing to a higher failure rate on the real robot testing, was missing a pregrasp planning module. In order to bypass this problem, we hardcoded reasonable realistic orientation for the end effector.'\\n\\n'The solution was evaluated quantitatively both in simulation and on a real robot. Moreover, our solution is fully integrated into ROS framework, and the source code was made openly available.'\\n\\n'The implementation is based on the well known middle-ware ROS (Robot Operating System). The Jacobian matrix was obtained using the open-source package PyKDL3, which loadw A kinematic chain from an URDF4 file, to then compute the Jacobian matrix based on the current joint state.'\\n\\n'For numerical optimization we used the open-source package SciPy5. SciPy is an open source software package providing various numerical algorithms, namely optimization solvers. In this work, we use the Sequential Least SQuares Programming (SLSQP) constrained optimization solver.'\\n\\n'a ROS based open source implementation available under1.'\",\"215\":null,\"216\":null,\"217\":\"'We can implement our approach in practice by using a deep neural network generative model. The particular model we use in our implementation is based on the variational autoencoder (VAE) [12], [13], which provides a scalable and efficient way to model high-dimensional observations such as images. A VAE models observations as being generated from a low-dimensional latent variable z (top-left of Figure 1), trained using approximate Bayesian variational inference that maximizes the following lower bound on the marginal loglikelihood:'\\n\\n'We introduced a probabilistic framework that aims to cope with uncertainty stemming from out-of-distribution states, undersampling, and noisy data. We combined recent advances in generative models with model-uncertainty estimation methods to improve the tradeoff between avoiding catastrophic collisions and maintaining a high degree of autonomy on simulated and real-world robot car navigation datasets. Our approach to uncertainty estimation via generative modeling can be combined with any existing Bayesian neural network approach, and we found experimentally that it provides an improvement with respect to this tradeoff metrics for all Bayesian neural network methods that we evaluated. While our method empirically outperforms prior techniques on a range of comparisons, our approach also has a number of limitations. The inference procedure at test-time still uses the VAE encoder network, which itself may not be robust to out-of-distribution inputs. This issue could in principle be mitigated by using other approximate inference methods, such as Markov chain Monte Carlo, and practically we observed that our method still produces actionable uncertainty estimates in spite of this limitation. Our method also does not provide any theoreticalguarantee that the model will respond correctly in practice. Indeed, to our knowledge, neither does any other prior method, in the case of neural network models and image observations. Theoretical analysis of uncertainty estimation for such settings is an important direction for future work.'\",\"218\":\"\\\"Autonomous driving presents one of the largest problems that the robotics and artificial intelligence communities are facing at the moment, both in terms of difficulty and potential societal impact. Self-driving vehicles (SDVs) are expected to prevent road accidents and save millions of lives while improving the livelihood and life quality of many more. However, despite large interest and a number of industry players working in the autonomous domain, there still remains more to be done in order to develop a system capable of operating at a level comparable to best human drivers. One reason for this is high uncertainty of traffic behavior and large number of situations that an SDV may encounter on the roads, making it very difficult to create a fully generalizable system. To ensure safe and efficient operations, an autonomous vehicle is required to account for this uncertainty and to anticipate a multitude of possible behaviors of traffic actors in its surrounding. We address this critical problem and present a method to predict multiple possible trajectories of actors while also estimating their probabilities. The method encodes each actor's surrounding context into a raster image, used as input by deep convolutional networks to automatically derive relevant features for the task. Following extensive offline evaluation and comparison to state-of-the-art baselines, the method was successfully tested on SDVs in closed-course tests.\\\"\",\"219\":\"\\\"Fig. 1.Generating predictions of a future for a pedestrian attempting to cross the street. We pick out two key frames from the (a) input sequence and the (b) ground truth sequence, 16 frames apart. Image (c) shows our prediction at the same time instant as the ground truth.We explore prediction of urban pedestrian actions by generating a video future of the traffic scene, and show promising results in classifying pedestrian behaviour before it is observed. We compare several encoder-decoder network models that predict 16 frames (400-600 milliseconds of video) from the preceding 16 frames. Our main contribution is a method for learning a sequence of representations to iteratively transform features learnt from the input to the future. Then we use a binary action classifier network for determining a pedestrian's crossing intent from predicted video. Our results show an average precision of 81%, significantly higher than previous methods. The model with the best classification performance runs for 117 ms on commodity GPU, giving an effective look-ahead of 416 ms.\\\"\\n\\n'decode'\\n\\n'Our objective is to predict the future positions of salient objects like vehicles and pedestrians by learning their motion. Functionally, an encoder reads a sequence of frames\\\\nx={\\\\nx\\\\nT\\\\n,\\u2026,\\\\nx\\\\n1\\\\n}\\\\nto yield dense representations\\\\nz={\\\\nz\\\\n1\\\\n,\\u2026,\\\\nz\\\\nT\\\\n}.\\\\nConditioned on z, a decoder will then auto-regressively predict an image sequence\\\\ny\\\\n\\u2032\\\\n={\\\\ny\\\\n\\u2032\\\\nT+1\\\\n,\\u2026,\\\\ny\\\\n\\u2032\\\\n2T\\\\n}\\\\nby minimizing a pixel-wise loss between\\\\ny\\\\n\\u2032\\\\nand ground truth frames\\\\ny={\\\\ny\\\\nT+1\\\\n,\\u2026,\\\\ny\\\\n2T\\\\n}\\\\n. Each generated frame is of the same resolution as the input. We reverse the temporal ordering of input data to condition the latent space with spatial information from the latest frame. The most recent frame carries forward the closest contextual resemblance. Recursively learning representations from each input frame, we expect to first learn a temporal regularity in the early representations and parametrize a temporal variance in the later representations. We later visualize the learned representations to validate our intuition.'\\n\\n'A. Encoder'\\n\\n'The encoder is a spatio-temporal neural network composed of three-dimensional convolutional layers. In contrast to the results of Tran et al. [22] we found that in our case, kernels with decreasing sizes in the spatial dimension (11x11 \\u2192 5x5 \\u2192 3x3) and constant size in the time dimension capture the input scene and temporal variations in greater detail. A large spatial kernel (11x11) along with a stride with much overlap between them (4) in the first layer was observed to produce sharper images. We believe that this allows the network to account for more spatial features per time frame. Residual connections are introduced at two image resolutions in our downsampling pipeline: 32 \\u00d7 54 and 16 \\u00d7 26. Two 3D convolutional layers feed into these residual blocks, where the filters are time dilated for larger temporal reception. We forward the features learnt at the first residual block to the decoder, building another residual connection. Each hidden representation\\\\nz\\\\ni\\\\n,i\\u2208{1,\\u2026,T}\\\\nis a function of all input frames, with the learnt weights determining the contribution of each frame towards each zj. The learnt z is 16x26 dimensional. We abstract the mathematical formulation for 3D convolutions in Equation (1) to show the temporal order of processing. The equations presented are for an l-th residual block in the encoder with two 3D convolutional layers, a and b in each block. k is the kernel size in the time dimension and the equations do not show dilations in the kernel. We use time distributed 1 \\u00d7 1, 2D convolution operations for dimensionality matching in addition operations for residual connections.'\\n\\n'B. Decoder'\\n\\n'The decoder is recurrent, containing convolutional LSTM layers [23]. ConvLSTM layers interspersed with up-sampling layers go from the low-dimensional representation space of z to the image space of\\\\ny\\\\n\\u2032\\\\n. Unlike the encoder, the decoder layers up-sample steadily to facilitate fluid transforms. We found that a fixed kernel size of 3 \\u00d7 3 provided an appropriate balance between training time and quality of generation.'\\n\\n'We introduce residual connections at three image scales of the upsampling pipeline: (16 \\u00d7 26; 32 \\u00d7 52 and 64 \\u00d7 104), following the intuition that each block would optimize for mutually different visual features. We introduce another residual connection to factor in the first convolutional level image features forwarded from the encoder. This is added at the 32 \\u00d7 52 image resolution, or the second level of decoding. We only add the first feature vector corresponding to the last few input frames for a balance between keyframe retention and over-conditioning of the decoder. We choose a greater number of filters in the earlier stages of the decoder, reducing them rapidly towards the end (128 \\u2192 64 \\u2192 16 \\u2192 3) to generate a 3-channel colour image. Our interpretation is that the greater number of filters early on offers more opportunities for a structural transformation due to smaller image resolutions. The reduced number of filters in the later stages eases the network\\u2019s optimizing efforts. The final transformations are encouraged to be more refining than compositional because of iterative refinement in residual networks [21]. Each decoding layer\\u2019s function can be elementarily defined as in Equation (2). We abstract the hidden state dependences from less relevant convolution operations in the recurrence formulation of the ConvLSTM.'\\n\\n'The sequential nature of the video frames can be benefit from recurrent layers over 3D-convolutions. We change the decoder from the Conv3D model to adopt recurrent convolutional layers to build our Segment model. The convolutional layers help preserve the spatiality of the data relative to standard vector LSTM layers. The Segment model is designed with kernel sizes and number of filters drawn from the common image segmentation models [28]. The mean error for this model is\\\\n(1.43\\u00b10.36)\\u00d7\\\\n10\\\\n\\u22121\\\\n,\\u223c54%\\\\nbetter than the Conv3D model and\\\\n\\u223c4%\\\\nworse than the Res-EnDec model.'\\n\\n'Scatter plot of t-sne minimized representations learnt by the encoder. A gradual trend from deep blue to red suggests that the representations are different from each other. Autoregressively processing them encourages iterative transformation.'\\n\\n'Scatter plot of t-sne minimized tensors obtained from the first residual connection in the decoder. Close association of samples in the form of blue-red streaks suggests that the recurrent convolutions assist in reforming the representation space into a hypersphere with motion projecting the transformations outwards.'\\n\\n'In this paper, we proposed and demonstrated three broad categories of neural network algorithms tasked with generating video predictions of the future. We then introduced a Temporal Variation Graph for all models, to measure their contributions in per-frame visual reproducibility and a temporal coherence. Our results suggest that the residual connections encourage learnt intermediate representations to be mutually different. Along with multi-stage recurrent decoding, iterative refinement can be seen. The novelty of our approach is that we learn a sequence of representations from an encoder rather than a comprehensive vector as done in many sequence generation approaches.'\",\"220\":\"'The pyramid average pooling is built up of three different pyramid sizes, i.e. small, middle and large size. The large-size pooling highlighted in yellow is the coarsest global pooling to generate a single bin output. The following other size pooling partitions the feature map into different sub-regions and forms pooled representation for each corresponding sub-region with bin sizes of 4 \\u00d7 4, 16 \\u00d7 16 respectively. Furthermore, to lighten the model, we reduce the dimension of different-size output features to 1\\/3 of the original input one after through one depth-wise separable convolution layer. Compared with that only using one 1 \\u00d7 1 convolution to reduce the feature dimension, our strategy using one depth-wise separable convolution can extract the more expressive features and further improve the accuracy performance by more than 0.5% with the negligible computation cost. Consequently, the outputs are of disparate sizes. Then we directly upsample the shrunken outputs via bilinear interpolation to achieve the final pyramid contrast features with the same resolutions as the one of original feature map. Finally, the original features and the final pyramid contrast features are concatenated to generate the final encoder feature maps.'\\n\\n'We first bilinearly upsample the encoder features by a factor of 4 and then concatenate them with the corresponding low-level features from the feature representation that have the same spatial resolution. The channel number of the encoder features is much larger than that of the low-level features, which overshadows the importance of the low-level features and make the network harder to train. Therefore, we apply a few 3\\\\n\\u00d7\\\\n3 depth-wise convolutions and one 1\\\\n\\u00d7\\\\n1 convolution on the encoder features to reduce their channels while retaining crucial contrast features. After the concatenation, we apply another 3\\\\n\\u00d7\\\\n3 depth-wise convolutions to refine the features and obtain sharper contrast results, and one 1\\\\n\\u00d7\\\\n1 convolution to further lighten the module. In what follows, another simple bilinear upsampling by a factor of 4 is used.'\\n\\n'Effectiveness of Lightweight Refinement Module. The encoder features from our feature representation are computed with output stride =16, thus the BS and MSC both perform upsampling using in-network bilinear interpolation by a factor of 16, whereas our final model with lightweight refinement module adapts two-step upsampling: first bilinearly upsampling the encoder features by a factor of 4 and then concatenating them with the corresponding low-level features from the network backbone that have the same spatial resolution; and finally, upsampling the concatenated features by a factor of 4 again. To better show the strength of our proposed lightweight refinement module, we compare the aforementioned saliency map M2 directly upsampling with output stride\\\\n=16\\\\nand the saliency map M3 generated from our final model integrated with the lightweight refinement module using the testing images in the MSRAB dataset. The results are also shown in Fig. 5 and Table III. They are evident that the lightweight refinement module improves the accuracy of our model by successfully restoring object saliency details at the fairly low additional overhead. Moreover, we adopt a few depth-wise separable convolutions, rather than 1\\\\n\\u00d7\\\\n1 convolutions to reduce channels of feature maps after concatenations and boost F-measure by about 1.3% in comparison with the latter.'\",\"221\":\"'Learning to Write Anywhere with Spatial Transformer Image-to-Motion Encoder-Decoder Networks'\\n\\n'Spatial Transformer Image-To-Motion Encoder-Decoder Networks'\\n\\n'Learning to recognize and reproduce handwritten characters is already a challenging task both for humans and robots alike, but learning to do the same thing for characters that can be transformed arbitrarily in space, as humans do when writing on a blackboard for instance, significantly ups the ante from a robot vision and control perspective. In previous work we proposed various different forms of encoder-decoder networks that were capable of mapping raw images of digits to dynamic movement primitives (DMPs) such that a robot could learn to translate the digit images into motion trajectories in order to reproduce them in written form. However, even with the addition of convolutional layers in the image encoder, the extent to which these networks are spatially invariant or equivariant is rather limited. In this paper, we propose a new architecture that incorporates both an image-to-motion encoder-decoder and a spatial transformer in a fully differentiable overall network that learns to rectify affine transformed digits in input images into canonical forms, before converting them into DMPs with accompanying motion trajectories that are finally transformed back to match up with the original digit drawings such that a robot can write them in their original forms. We present experiments with various challenging datasets that demonstrate the superiority of the new architecture compared to our previous work and demonstrate its use with a humanoid robot in a real writing task.'\\n\\n'Writing digits with the Talos humanoid robot using a spatial transformer image-to-motion encoder-decoder network (STIMEDNet) prediction. The movements are generated using DMP trajectories predicted by the network from the images shown to the robot and the robot can write the digits in matching poses.'\\n\\n'The proposed STIMEDNet architecture. Input images are fed to a spatial transformer network (STN) which rectifies attended objects into canonical form before being passed to a pre-trained convolutional image-to-motion encoder-decoder network (CIMEDNet) producing DMP parameters k that are integrated via\\\\nD\\\\n(Euler\\u2019s method) into motion trajectories\\\\ny\\\\nDMP\\\\nand transformed via\\\\nT\\\\n\\u03b8\\\\nto final output form\\\\ny\\\\nDMP\\\\n\\u03b8\\\\nby a motion transformer (MTN). The same\\\\n\\u03b8\\\\ntransform parameters learned by the STN to transform the images at the bottom of the network are passed to the MTN via skip connection where they are reused to transform the trajectories at the top of the network. The solid arrows mark differentiable paths through the network, the dashed arrow marks auxiliary output.'\\n\\n'The newly proposed spatial transformer image-to-motion encoder-decoder network (STIMEDNet) architecture is illustrated in Fig. 2. In order to describe the proposed architecture in more detail, we begin by discussing the nature of the data that it must process, the structure of which is the same as in [1]. The input and output data pairs take the following form:'\\n\\n'The image-to-motion encoder-decoder network (CIMEDNet) at the core of the overall architecture shown in Fig. 2 with an encoder consisting of two convolutional layers followed by two fully-connected layers, and a decoder consisting of three fully-connected layers.'\\n\\n'2) Image-to-Motion Encoder-Decoder:'\\n\\n'The image-to-motion encoder-decoder network that lies at the heart of the architecture presented in Fig. 2 is a convolutional version of the fully-connected IMEDNet from [1]. It uses both convolutional and fully-connected layers in the image encoder and fully-connected layers in the DMP decoder. It is illustrated in more detail in Fig. 3. The convolutional layers are pretrained as part of a basic CNN classifier that was trained on the original MNIST dataset [15] The input is a\\\\n40\\u00d740\\u00d71\\\\ngrayscale pixel image, followed the encoder consisting of a convolutional layer with 5\\\\n\\u00d7\\\\n5 kernel size and 10 feature maps, a convolutional layer with 5\\\\n\\u00d7\\\\n5 kernel size and 20 feature maps, and two fully-connected layers with sizes of 600 neurons and 200 neurons respectively. Following these fully-connected layers, at the bottleneck of the network that forms the latent space representation, a decoder is formed with more fully-connected layers that gradually expand the number of units in each layer until the final output layer, which has a size set to 54 units in order to match the DMP parameters specified in Equation (4). The layers of the decoder are illustrated on the right side of 3 starting with the bottleneck of size 20, followed by a layer of size 35 and finishing with the output layer. This is the same decoder structure as used [1] and we retain it here as-is, having found it to be effective throughout our experiments for this particular use case. It should be noted, however, that there are no particular restrictions on the nature of the network that is used here, and it is easy to imagine a more advanced pretrained CNN being used for the image encoder, for example, if the application demanded it.'\\n\\n'We have presented a novel neural network architecture for image-to-motion prediction that employs a spatial transformer module, a convolutional image-to-motion encoder-decoder module and a motion transformer module in a fully-differentiable overall model. We have demonstrated that this architecture outperforms its predecessor on a variety of different datasets and we have demonstrated its use with a full-scale humanoid robot that is able to use the network to learn to read and write digits in arbitrary poses as presented to it. Regarding future work, we intend to expand the capabilities of these models still further by incorporating layers from more powerful pre-trained CNN models into the image encoder of the CIMEDNet part of the network and training the network on more challenging image sets. One challenge here lies in either finding suitable image datasets that include trajectory information in their target outputs or in finding ways to convert the data in existing datasets into trajectories. A possible approach would be to convert the borders of object labels in existing semantic segmentation datasets into draw trajectories, however, this would present the additional difficulty of having to account for DMP representations for trajectories that vary significantly in length given the large variation in object size in such datasets. This might be resolved by employing a recurrent neural network (RNN) architecture and using fixed-sized DMPs in a temporal piecewise construction scheme. An interesting extension to the original spatial transformer network that includes an RNN capability was proposed in [20] and we envisage using a similar technique within our architecture.'\",\"222\":\"'Fast and efficient motion planning algorithms are crucial for many state-of-the-art robotics applications such as self-driving cars. Existing motion planning methods become ineffective as their computational complexity increases exponentially with the dimensionality of the motion planning problem. To address this issue, we present Motion Planning Networks (MPNet), a neural network-based novel planning algorithm. The proposed method encodes the given workspaces directly from a point cloud measurement and generates the end-to-end collision-free paths for the given start and goal configurations. We evaluate MPNet on various 2D and 3D environments including the planning of a 7 DOF Baxter robot manipulator. The results show that MPNet is not only consistently computationally efficient in all environments but also generalizes to completely unseen environments. The results also show that the computation time of MPNet consistently remains less than 1 second in all presented experiments, which is significantly lower than existing state-of-the-art motion planning algorithms.'\\n\\n'To address the above-mentioned challenges, we propose a Deep Neural Network (DNN) based iterative motion planning algorithm, called MPNet (Motion Planning Networks) that efficiently scales to high-dimensional problems. MPNet consists of two components: an encoder network and a planning network. The encoder network learns to encode a point cloud of the obstacles into a latent space. The planning network learns to predict the robot configuration at time step\\\\nt+1\\\\ngiven the robot configuration at time t, goal configuration, and the latent-space encoding of the obstacle space. Once trained, MPNet can be used in conjunction with our novel bi-directional iterative algorithm to generate feasible trajectories. We evaluate MPNet on a large test dataset including multiple planning problems such as the planning of a point-mass robot, rigid-body, and 7 DOF Baxter robot manipulator in various 2D and 3D environments. As neural networks do not provide theoretical guarantees on their performance, we also propose a hybrid algorithm which combines MPNet with any existing classical planning algorithm, in our case RRT*. The hybrid planning technique demonstrates a 100% success rate consistently over all tested environments while retaining the computational gains. Our results indicate that MPNet generalizes very well, not only to unseen start and goal configurations within workspaces which were used in training, but also to new workspaces which the algorithm has never seen.'\\n\\n'Our proposed method uses two neural models to solve the motion planning problem. The first model is an encoder network which embeds the obstacles point cloud, corresponding to a point cloud representing Xobs, into a latent space (see Fig. 2(a)). The second model is a planning network (Pnet) which learns to do motion planning for the given obstacle embedding, and start and goal configurations of the robot (see Fig. 2(b)).'\\n\\n'1) Encoder Network:'\\n\\n'The encoder network (Enet) embeds the obstacles point cloud into a feature space\\\\nZ\\u2208\\\\nR\\\\nm\\\\nwith dimensionality\\\\nm\\u2208N\\\\n. Enet can be trained either using encoder-decoder architecture with a reconstruction loss or in an end-to-end fashion with the Pnet (described below). For encoder-decoder training, we found that the contrative autoencoders (CAE) [18] learns robust and invariant feature space required for planning and genalization to unseen workspaces. The reconstruction loss of CAE is defined as:'\\n\\n'The encoder network Enet\\\\n(\\\\nx\\\\nobs\\\\n)\\\\n, trained during the offline phase, is used to encode the obstacles point cloud\\\\nx\\\\nobs\\\\n\\u2208\\\\nX\\\\nobs\\\\ninto a latent space\\\\nZ\\\\n\\u2208\\\\nR\\\\nm\\\\n.'\\n\\n'For all environments except Baxter, we use encoder-decoder training, whereas for Baxter, we train the encoder and planning network end-to-end. Since the decoder is usually the inverse of the encoder, we only describe the encoder\\u2019s structure. The encoding function Enet\\\\n(\\\\nx\\\\nobs\\\\n)\\\\ncomprised of three linear layers and an output layer, where each linear layer is followed by the Parametric Rectified Linear Unit (PReLU) [23]. The input to the encoder is a vector of point clouds of size\\\\nN\\\\npc\\\\n\\u00d7\\\\nd\\\\nw\\\\nwhere Npc is the number of data points, and\\\\nd\\\\nw\\\\n\\u2208N\\\\nis the dimension of a workspace.'\\n\\n'In this paper, we present a fast and efficient Neural Motion Planner called MPNet. MPNet consists of an encoder network that encodes the point cloud of a robot\\u2019s surroundings into a latent space,, and a planning network that takes the environment encoding, and start and goal robotic configurations to output a collision-free feasible path connecting the given configurations. The proposed method (1) plans motions irrespective of the obstacles geometry, (1) demonstrates mean execution time of about 1 second in all presented experiments, (3) generalizes to new unseen obstacle locations, and (4) has completeness guarantees.'\",\"223\":\"'https:\\/\\/youtu.be\\/0P3FJAo4ZOA'\",\"224\":null,\"225\":null,\"226\":\"'The sense of touch is essential for reliable mapping between the environment and a robot which interacts physically with objects. Presumably, an artificial tactile skin would facilitate safe interaction of the robots with the environment. In this work, we present our color-coded tactile sensor, incorporating plastic optical fibers (POF), transparent silicone rubber and an off-the-shelf color camera. Processing electronics are placed away from the sensing surface to make the sensor robust to harsh environments. Contact localization is possible thanks to the lower number of light sources compared to the number of camera POFs. Classical machine learning techniques and a hierarchical classification scheme were used for contact localization. Specifically, we generated the mapping from stimulation to sensation of a robotic perception system using our sensor. We achieved a force sensing range up to 18 N with the force resolution of around 3.6 N and the spatial resolution of 8 mm. The color-coded tactile sensor is suitable for tactile exploration and might enable further innovations in robust tactile sensing.'\\n\\n'We address this problem by developing an optical tactile sensing array, which uses physical interaction with an object to provide force sensing and contact localization. We deliver the light from three sources with different colors to a commodity vision camera via plastic optical fibers (POFs) embedded inside transparent silicone. We use this compressible silicone as a color-coded tactile sensor (see Fig. 1). Our tactile sensor is energy efficient thanks to the use of less number of light sources than the camera POFs. Moreover, working with light beams rather than with a flow of electrons can be advantageous for some applications. For instance, data flow through electrical cables can be distorted by a magnetic field but magnetic fields do not interfere with optical signals [10].'\\n\\n'Color-coded tactile sensor. (a) Assembly with LEDs, camera and plastic optical fibers. (b) Camera snapshot overlaid onto the sensor. The camera image is processed to infer the sensor deformation.'\\n\\n'The block diagram illustrating the sensing principle of the color-coded optical tactile sensor.'\\n\\n'As soon as the silicone gets compressed after contact with an object, the light scattering pattern changes. If its color changes, one can use the color chart to determine whether and to what depth the silicone is deformed. Thus, the color-coded silicone substrate acts as a pressure-sensing media and changes color to signal pressure level. A geometrical explanation of this principle is shown in Fig. 3. The Figure exemplifies a one-dimensional case with one light source, e.g. red, POF and one camera POF. A given POF can get more light than others and vice versa as the directions of the reflected light beams change during the deformation of the sensing surface under an external force. Since the silicone substrate can be approximated by a spring with a constant Youngs modulus E, then this external force as function of deformation d is given by\\\\nF\\\\n\\u221d\\\\nEA\\\\nD\\\\nd\\\\n, where A is the contact area over the sensor and D is the thickness of the silicone rubber [23]. Depending on the location of the deformation and its depth, the emitted light intensity, I0, decreases with the beam path\\\\nI(r)\\u221d\\\\nI\\\\n0\\\\ne\\\\n\\u22122r22\\\\n\\u03c9\\\\n[10], where\\\\nI(r)\\\\nis the measured reflected intensity, r is the beam path to a camera POF, and\\\\n\\u03c9\\\\n0\\\\nand\\\\n\\u03c9\\\\nare the spot size of the Gaussian beam at the source\\\\nr=0\\\\nand at the camera POF, respectively. In some cases, e.g. Fig. 3(a), r is shorter than when the sensor is not in contact, e.g. Fig. 3(b). The light is trapped inside the silicone thanks to the reflection that is realized using a thin film. The angle of the reflection,\\\\n\\u03b8\\\\nmodifies this path too. Moreover, the compression of the silicone eventually alters the scattering and absorption properties, which dramatically increase for large deformations. For example, the LED light intensity decreases due to the increased density of irregularities. This effect depends on many properties such as the chemical composition and impurities, the details of which are beyond the scope of this work. However, we should note that I0 is heavily affected by absorption when the silicone is compressed.'\\n\\n'Change of color in our color-coded tactile sensor. Depending on an external force, the silicone deforms and the colors acquired by the camera via POFs changes. (a) External force is zero. (b) Light intensities change as the sensing surface deforms under applied external force.'\\n\\n'Eventually, we aim to attach the color-coded tactile sensor to robot grippers for object manipulation. Therefore, the size of our sensor is\\\\n40\\u00d740\\u00d75mm\\\\n. There are three fiber connections from the sides and nine fiber connections from the bottom. Figure 5(a,b) illustrate the fabricated sensor with the attached POFs. The distance between two neighboring POFs at the bottom side is 5 mm in both directions. The fibers that are on the sides are placed at the center of the corresponding edges. The depth of the inserted fibers is 5 mm. There are also extruded adapter sockets for the POFs. These sockets with the height of 8 mm increase the robustness of the connection between the POFs and silicone. The shape of the sensor is obtained by molding silicone into a plastic mold. The mold is 3D-printed using PLA filament. The structure of the sensor includes three layers: translucent silicone, reflective thin film, and silicone with injected reflecting particles for durability and better light reflection.'\\n\\n'Color-coded tactile sensor: (a) Front view of the sensor with silver thin film. (b) Entrance of the optofibers and their sockets into the sensor. (c) Silicone without the silver thin film. (d) Silicone with the silver thin film.'\\n\\n'The robot end-effector moved vertically to press the color-coded tactile sensor, which was placed inside a 3D-printed plastic box and fixed on an optical table. First, we found the end-effector pose when the indentor was at the closest proximity to the sensor without registering any force. Then, the end-effector progressively squeezed the tactile sensor up to 3 mm with 0.6 mm steps (i.e. five depth levels). Afterward, the robot end-effector moved back until to the pre-touch position, positioned itself to a new location to squeeze the sensor again. There were 25 total locations, and therefore,\\\\n25\\u00d75\\\\ncontact states. We recorded ten images at each contact state.'\\n\\n'In this work, we presented the design and implementation of our POF-based tactile sensor. Our approach combines known optical design concepts with soft materials and utilizes three different colors to increase the spatial resolution. The sensor acts as a force-sensing media \\u2013 as its sensing surface deforms, the color acquired by the optical fibers changes. Such sensor provides several benefits that normally involve more complex designs (e.g. robustness and durability). We evaluate the design idea experimentally and benchmark the efficacy of our sensor using a machine learning-based approach. Experimental results confirm that the color-coded tactile sensor is able to infer contact forces and their locations. Thus, our sensor has the potential to improve the dexterity of various robots in physical interaction tasks. In the future, we will improve our sensor design (by optimizing the design parameters) and also utilize it for tactile motion control (e.g. squeezing an object for determining its deformability).'\\n\\n'Color-Coded Fiber-Optic Tactile Sensor for an Elastomeric Robot Skin'\\n\\n'Color-Coded Sensor Design'\",\"227\":\"'In comparison to case i), using only the Writhe matrix performed worse after the training converged at 1500 episodes. This has two reasons: firstly, the Writhe matrix by itself does not encode enough relative spatial information between the robot and the humanoid, it is not able to describe geometric interactions. More importantly, by definition, different robot states can potentially result in the same Writhe matrix. Lastly, we can see that using only position information of landmark points performed the worst. In our evaluation, it was not able to execute the task even after convergence. This emphasizes the importance of using the topological representation.'\",\"228\":\"'https:\\/\\/ait.ethz.ch\\/projects\\/2019\\/DRL-handshake\\/)'\\n\\n'https:\\/\\/youtu.be\\/ZSgEqyltaN4)'\",\"229\":null,\"230\":null,\"231\":\"'In this section, we present an algorithm for forming a set of coalitions by a group of robots to be allocated to a set of tasks. We use the one-to-many bipartite matching idea described in the last section to derive our algorithm, the pseudo-code for which is shown as Algorithm 1. Note that this algorithm is an extension of the classical bipartite matching method presented in [10].'\",\"232\":\"'http:\\/\\/users.cis.fiu.edu\\/%7Ejabobadi\\/securemp\\/'\",\"233\":null,\"234\":null,\"235\":null,\"236\":null,\"237\":null,\"238\":null,\"239\":null,\"240\":null,\"241\":\"'We use an encoder-decoder CNN based on U-net [37]. Our final network architecture is illustrated in Fig. 2, and contains\\\\n5.14\\u00d7\\\\n10\\\\n6\\\\ntrainable parameters. We store all floating points numbers in single precision (32-bit), and a single forward-pass inference requires\\\\n1.07\\u00d7\\\\n10\\\\n10\\\\nfloating point operations. The encoder consists of a sequence of 3D convolutional and max-pooling layers, followed by a fully connected layer. The decoder applies upsampling and 3D convolutional layers to generate the three output channels at the original resolution. We found that upsampling with nearest-neighbor interpolation combined with stride-1 convolution resulted in smoother outputs with lower error than standard transpose convolution layers, which tended to introduce artifacts in the output channels [38]. The network utilizes skip connections to preserve high-resolution feature information from the encoder to aid in the decoding to higher resolution. On a single CPU core one forward-pass inference requires 2.5 GB RAM and is finished on average in 1.6 s. We experimented with training the network both with\\\\nL\\\\n1\\\\nand mean squared error (MSE) loss functions, and found that extrema values were better predicted when training with MSE loss than with\\\\nL\\\\n1\\\\nloss. Such extrema often occur close to ridges or steep slopes and can be critical for safe navigation.'\\n\\n'CNN architecture. We use a 3D encoder-decoder with skip-connections.'\\n\\n'Removing the skip connections or the fully connected layers resulted in increases in MSE loss on the test set of 37% and 25% respectively. We also explored alternative minimum code sizes in the fully connected layer, and found that 512 resulted in the best performance compared to\\\\n256(+4.6\\\\n% MSE test loss),\\\\n1024(+2.4%),2048(+10.0\\\\n%) or\\\\n4096(+12.6\\\\n%). Using trilinear interpolation instead of nearest neighbor reduced visual artifacts in the output, but increased error slightly\\\\n(+6.9\\\\n%).'\\n\\n'Drone Path Planning and Object Detection via QR Codes; A Surrogate Case Study for Wind Turbine Inspection'\",\"242\":null,\"243\":\"'Both our baseline and our Siamese similarity network are based on the VGG16 architecture. The Siamese variant has two tied VGG16 branches. Their outputs are concatenated (subtracting them works equally well) and fed through a multi-layer perceptron with three fully connected layers to obtain the final scores. Training is done with the ADAM variant of stochastic gradient descent, with mini-batches of size 16 for the Siamese network, respectively 32 for the single-branch baseline. The smaller batch size is meant to ensure a fair comparison in terms of ressources, since image pairs need twice as much memory. GPU memory is the bottleneck for CNN training when working with large images (like our \\u201cdoors\\u201d dataset). For the \\u201cLearning to Compare\\u201d baseline we use the architecture and hyper-parameters of the original, publicly available code.'\",\"244\":null,\"245\":null,\"246\":\"'rotation_averaging: code provided in [33].'\\n\\n'known_rotation_prob: code provided in [39].'\",\"247\":null,\"248\":null,\"249\":null,\"250\":\"'https:\\/\\/sites.google.com\\/view\\/haoangli\\/'\",\"251\":null,\"252\":null,\"253\":null,\"254\":\"\\\"One of the key challenges in realizing a robot that is capable of completing a variety of manipulation tasks in the real world is the need to utilize sufficiently compact and rich world models. If the assumed prediction model does not match real observations, planning systems are unable to perform properly. We propose a system that corrects the models based on information collected from the robot's sensors. We encode prior experiences in a neural network to generate possible parameters of the models for a physics engine from real observations. An online POMDP solver is used to plan actions to complete the task while progressively validating and improving the models. We perform experiments in simulations and on a real robot. The results show that this approach appropriately clarifies observed environments, can handle dynamics with discontinuities, and with increasing domain complexity achieves a better success rate than baseline methods.\\\"\",\"255\":null,\"256\":null,\"257\":null,\"258\":null,\"259\":null,\"260\":\"'https:\\/\\/sites.google.com\\/view\\/learning%E2%80%93robot%E2%80%93morphology'\\n\\n'For the controller, we optimize six CPG parameters, which correspond to the frequency, amplitude, and offsets of the vertical and horizontal motors. In addition, we separately consider parameters that control the amplitude of leg swings on the left and right sets of legs respectively. Although most of the parameters are related to our controller of choice, the CPG, our method is agnostic to the type of controller. The three morphology parameters control the lengths of pairs of legs (front, middle, and rear) and are encoded as ratios relative to a normalized leg length. We use a batch size\\\\nK=5\\\\nand run the controller optimizer for 50 iterations for each morphology evaluation. Videos and code for reproducing the experiments are available at https:\\/\\/sites.google.com\\/view\\/learning\\u2013robot\\u2013morphology'\",\"261\":\"'http:\\/\\/www.robo-implant.com\\/'\",\"262\":null,\"263\":null,\"264\":\"'Our main control loop relies on a high resolution relative position encoder, therefore an initializing homing motion has to be executed on startup. For the eyeball this homing is based on the absolute reference position that is acquired by using a 14-Bit magnetic position encoder [33] that is calibrated once during assembly. After initialization the main position and velocity control loop for the eyeball uses positional feedback from the relative position encoder with 512 ticks per motor shaft revolution which corresponds to 0.0023 \\u00b0 and 0.0016 \\u00b0 per encoder tick for the eyeball panand tilt-axis, respectively. This is one order of magnitude better than the absolute encoder (0.021 \\u00b0 per encoder tick) and allows very accurate, smooth, and silent eye movements that are mandatory for this application. The eyelid position is initialized by a similar homing motion based on a calibrated end switch. Again, the position and velocity control loop uses a relative position encoder that is mounted to the motor axis.'\\n\\n'By using state-of-the-art off-the-shelf components such as motors, encoders, and the camera subsystem we want to foster the further evolution of robot heads. The CAD model of our prototype is available free of charge on request for non-commercial applications.'\",\"265\":null,\"266\":\"'This paper proposes a novel approach to achieve highly scalable and energy efficient wireless localization while providing guaranteed real-time capabilities. Next to the approach, we provide a ROS-based open-source implementation of the scheduling and the second release of the underlying ATLAS localization system. We were able to analytically illustrate the systems potential and experimentally show the system performance. Furthermore, we provide the raw experimental data, a video and an interactive demonstration (by re-playing the raw data with the provided software) of multi-asset tracking in an industrial scenario showing the system\\u2019s capabilities.'\\n\\n'The ever increasing need for precise location estimation in robotics is challenging a significant amount of research. Hence, new applications such as wireless localization based aerial robot control or high precision personal safety tracking are developed. However, most of the current developments and research solely focus on the accuracy of the required localization systems. Multi-user scalability, energy efficiency and real-time capabilities are often neglected. This work aims to overcome the technology barrier by providing scalable, high accuracy, real-time localization through energy-efficient, scheduled time-difference of arrival channel access. We could show that simultaneous processing and provisioning of more than a thousand localization results per second with high reliability is possible using the proposed approach. To enable wide-spread adoption, we provide an open source implementation of our system for the robot operating system (ROS). Furthermore, we provide open source access to the raw data created during our evaluation.'\",\"267\":\"\\\"In this paper, we present a change detection algorithm that can run in real time as part of a backend-based stream processing pipeline. It can process the floating car data collected by series-production vehicles to detect changes in an automotive high definition digital (HD) map used for automated driving. The algorithm uses a particle filter approach with odometry, GNSS and landmark readings to localize the vehicle within the digital map. While all particles together represent the probability distribution for the vehicle's position at a given time, each individual particle also serves as a hypothesis about the vehicle's position. This is used to compute various metrics for how well the current sensor readings match the world model encoded in the HD map. The different metrics are evaluated by a number of weak classifiers that are used as input for a trained Adaboost classifier. The achievable detection rate of a single vehicle is then compared to that of a simple crowd-based approach, where each vehicle votes on whether or not the current section of the road has changed.\\\"\\n\\n'In this paper, we present a change detection algorithm that can run in real time as part of a backend-based stream-processing pipeline that processes the FCD. It uses a particle filter based on odometry, GNSS and landmark detections to localize the vehicle within the digital map. While all particles together represent the probability distribution for the vehicle\\u2019s position at a given time, each individual particle also serves as a hypothesis about the vehicle\\u2019s position that is used to compute various metrics for how well the current sensor readings match the world model that is encoded in the HD map. The different metrics are evaluated by a number of weak classifiers that are used as input for a trained Adaboost classifier [6]. The achievable detection rate of a single vehicle is then compared to that of a simple crowd-based approach, where each vehicle has a vote of whether or not the current section of the road has changed.'\",\"268\":\"'https:\\/\\/osf.io\\/pkbq4\\/'\\n\\n'In summary, we have found that our non-parametric error modeling makes our method robust to NLOS conditions, and our modeling and simultaneous calibration of antenna delays leads to very high accuracy is LOS conditions. We found that only 16 to 128 measurements are necessary per link and that a network of 8 UWB nodes requires less than a minute for both the collection of measurements and execution of the algorithm. On this network we achieved accuracies of 3cm RMSE in the LOS dataset 1, and 30cm in the NLOS dataset 4. Our datasets and source code files can be found at https:\\/\\/osf.io\\/pkbq4\\/.'\",\"269\":\"'This paper presents an incremental learning framework for mobile robots localizing the human sound source using a microphone array in a complex indoor environment consisting of multiple rooms. In contrast to conventional approaches that leverage direction-of-arrival (DOA) estimation, the framework allows a robot to accumulate training data and improve the performance of the prediction model over time using an incremental learning scheme. Specifically, we use implicit acoustic features obtained from an auto-encoder together with the geometry features from the map for training. A self-supervision process is developed such that the model ranks the priority of rooms to explore and assigns the ground truth label to the collected data, updating the learned model on-the-fly. The framework does not require pre-collected data and can be directly applied to real-world scenarios without any human supervisions or interventions. In experiments, we demonstrate that the prediction accuracy reaches 67% using about 20 training samples and eventually achieves 90% accuracy within 120 samples, surpassing prior classification-based methods with explicit GCC-PHAT features.'\\n\\n'The proposed approach using a self-supervised incremental learning scheme. (a) The multi-channel signals from the user\\u2019s wake-up word are picked up by VAD. Each signal is transferred to the amplitude spectrum and normalized to [0, 1], from which (b) an auto-encoder is trained to extract implicit features. Each block represents a 2D convolution with stride\\\\ns[\\u22c5,\\u22c5]\\\\n, kernel size\\\\nk[\\u22c5,\\u22c5]\\\\nand the number of channels. In addition, (c) an occupancy map obtained from the reconstructed point cloud is down-sampled by pooling (d). (b)(d) together form the feature for learning. (e) Individual rooms are segmented from the point cloud. (f) The HARAM model is adopted to predict the priority rank of rooms the robot should visit. (g) The robot self-supervises the learning by exploring the rooms. (h) The exploration will be labeled as the positive sample if the robot detects the user, which will update the HARAM model incrementally.'\\n\\n'We provide a Robot Operating System (ROS) package that integrates all modules of the proposed framework, including the acoustic signal processing, room segmentation, and the learning and inference, which allows a robot to perform SSL task without any human supervisions or interventions. The code will be made publicly available.'\\n\\n'This section introduces the feature extraction process. The features consist of both the acoustic features based on the collected signals from the microphone array and the geometry feature extracted from the SLAM results, which encode both the geometry structure of the environment and the robot\\u2019s current position.'\\n\\n'Signal Low-dimensional Embedding: The dimensions of the normalized spectrum are still large, and the data contains certain levels of noises. To address these issues, we use an auto-encoder to extract a low-dimensional embedding from the spectrum per channel. Figure 2b depicts the encoder structure that contains multiple convolutional layers; each layer is followed by a Leaky-ReLU activation layer and the batch normalization. The decoder is symmetrical to the encoder. Such structure results in a 256-dimensional embedding by minimizing the weighted Mean Square Error (MSE) between the original spectrum and the reconstructed spectrum by the decoder:'\\n\\n'There are three advantages using such an auto-encoder method to encode the acoustic signals: (i) The dimension reduction process reduces the noise contained in the raw signal, such as the background noise and reverberation. (ii) Reducing the dimension shrinks the memory required in the proposed incremental learning framework. (iii) Since the auto-encoder is designed to minimize reconstruction loss, the encoding process still preserves meaningful information in the signal as implicit features.'\\n\\n'(a) The original spectrum normalized to [0, 1]. (b) The reconstructed spectrum using an auto-encoder. (c) The reconstruction error as a binary image, in which the black pixel indicates the relative error larger than 5%.'\\n\\n'MLP + AE. We choose an incremental learning version of the classic multi-layer perceptron (MLP) classification method instead of HARAM and learn from the encoded implicit acoustic feature.'\\n\\n'This paper has proposed a self-supervised incremental learning method for SSL in a complex indoor environment consisting of multiple rooms. Specifically, the method localizes the human sound source to one of the rooms. We designed an auto-encoder to extracted implicit acoustic features from the signals collected from a uniform circular microphone array with 16 microphones. These features are concatenated with the environment geometry features obtained from pooling the occupancy map of the 3D environment. A HARAM model is adopted to learn the rank of rooms to explore with a probability from high to low. The self-supervision is achieved through robot actively exploring the rooms according to the predicted rank and detecting sound sources by human poses, which improves model performance incrementally. In the experiment, we demonstrate that the proposed method has first and second hit rates of 67% and 84% after 20 samples, and of 90% and 96% after 120 samples, which significantly outperform three baselines.'\",\"270\":null,\"271\":null,\"272\":null,\"273\":null,\"274\":null,\"275\":\"'A navigation system may accommodate incidental patient motion by registering and tracking the pertinent anatomy. An example is in a cardiac catheterization lab, where the patient table is encoded with respect to the C-arm. Fluoroscopy, rather than fiducials, can then be used to track the patient [30], [31] so that the transform\\\\np\\\\nT\\\\nw\\\\nfrom the workspace (table) to patient frame p is known intraoperatively. Attaching the fixture of Fig. 1 to the table then permits the device to be registered to the patient as\\\\np\\\\nT\\\\nd\\\\n=\\\\np\\\\nT\\\\nw\\\\n\\u00d7\\\\nw\\\\nT\\\\nd\\\\n, where\\\\nw\\\\nT\\\\nd\\\\nis computed as above.'\",\"276\":null,\"277\":\"'Despite the manifold design solutions, teleoperation commands are standardized across the different models: surgeons instruct the system with a direction of displacement, univoquely coded as up\\/down, left\\/right and in\\/out, generally referred to the image frame. The input motion is then mapped into the slave joint space as a desired position and a PID control tracks this desired position.'\",\"278\":null,\"279\":\"'For robots to become widely used, humans must be able to program their actions. For example, consider the task of binning items. A roboticist might accomplish this task by specifying a series of waypoints in computer code for the robot to visit one by one. If the action must be modified, the roboticist would explicitly modify the waypoints specified in the code. This method is widely popular, but will not work for end-users. The abstraction of breaking down actions into a series of waypoints could be communicated, but requiring the use of programming languages to specify those waypoints is beyond their scope. Therefore, we need an alternate method of interfacing with the waypoint action system.'\\n\\n'The traditional way to program a robot is to write code. ROS [8] is an extremely powerful middleware environment for roboticists. ROS includes packages to allow programmers to use languages like\\\\nC++\\\\nand Python to interface with robot hardware. However, leveraging the expertise of end-users that lack software engineering skills would help make robots more widely accessible.'\\n\\n'Implementation was split into front-end Unity code for the HoloLens MR-HMD and back-end ROS code for MoveIt! and the Baxter. The 2D monitor baseline used the same backend as the 3D interface. The front-ends were identical, except that the 2D monitor interface displayed graphics on a screen instead of holograms. Additionally, the 2D monitor interface had a rendering of a point cloud of the workspace ( Fig. 3a).'\\n\\n'Unity code supported both GUIs. This code made requests to the back-end as waypoints were altered, updated the robot arm movement trails upon receiving motion plans from the back-end, and sent execution requests to the back-end. We used ROS Reality [19] to visualize the robot model and 3D sensor data for the monitor interface.'\",\"280\":\"'Each of the manipulator joints consists of a prismatic linear motor (LM1247; Faulhaber) without gearboxes. The maximum force of the chosen motor is 3.6 [N] which results the maximum force of the device through the parallel mechanism is over 10 [N]. Therefore, we do not deploy any transmission mechanisms on the device. This may be acceptable as a haptic master device compare to other haptic devices such as 12 [N] of Omega.3 (Force Dimension) and 8.5 [N] of PHANToM Premium 1.5 (SensAble). The motor also contains analog Hall sensors without the need of additional encoders. An associated motor control hardware were specifically developed. Six microcontrollers (STM32F722; STM) are for low-level current control of each linear motor and generate PWM signals at 10 [kHz]. A intermediate-level control is implemented in a different microcontroller (F28M36; Texas Instruments) and communicates with low-level controllers via SPI protocol to transmit target torques and position values computed by Hall sensors. At the same time, the position values transmitted to the high-level control platform and received target torques for each motor via Ethernet communications (UDP). The high-level control is for complex numerical computations such as kinematics, Jacobian, and controllers. It is implemented in real-time control platform (Speedgoat) with MATLAB Simulink. The intermediate-level and high-level controls and communications between them run at 4 [kHz] to minimize control bandwidth problems of the device. Fig. 2 shows schematic diagram of the device and the integrated master and slave devices.'\\n\\n'The communication platform includes a tele-echography hardware codec for encoding and decoding data streams of ultrasound images, surrounding videos\\/audios, and robot data. By using the codec, data streams are transferred between local and remote sites bilaterally via real-time transport protocol (RTP). The resolution of ultrasound image is adjusted to 1024 \\u00d7 768 pixels and transferred to the master site in conjunction with audio and HD video stream. Robot data are structured as an array of single-precision value and transferred to the other site through the codec. More details on the commnunication platform for tele-echography system can be found in [23].'\",\"281\":null,\"282\":null,\"283\":null,\"284\":null,\"285\":null,\"286\":null,\"287\":\"'The Pseudo-Code of Ga'\",\"288\":null,\"289\":null,\"290\":\"'Algorithm 1: Pseudocode for pose estimation of washers grasped on a radially arranged sensor array'\",\"291\":null,\"292\":\"\\\"The comparison between the human data (a-d) and the prosthetic data (e-h) from the encoder on both joints for a single gait cycle. All the bold lines in the prosthetic results indicate are the average of five steps performed by the subject while the shades indicate \\u00b1 1 standard deviation. Note that the results on \\u00b17\\u00b0 slope are shown only in the prosthesis' result since the corresponding human data were not collected.\\\"\\n\\n'The encoder data from the experiment for a single gait cycle is indicated in Fig. 7. Fig. 7e-h show that both ankle and knee joint trajectories have qualitatively similar walking compared to human slope walking trajectories (Fig. 7a-d). Specifically, at the knee joint, compliant walking during the stance phase and the enlarged flexion depending on the downslope during the swing phase are clearly shown. However, it is shown that the kneejoint angle has a relatively large difference compared to the human results. This is because the aforementioned hardware limitation to the knee joint mainly causes the joint angle difference from to the human data. From the supplemental video result [31], we would check the enlarged knee flexion by tracking the desired Bezier trajectory can avoid the collision with the slope even though it has less flexion compared to the human data. At the ankle joint, as the slope increases the initial joint angle increases. This is also seen in the human data although, in the prosthesis, these differences are not as great. Also, for both upslope and downslope, PO can be observed in the prosthetic walking even though this is not as great as human walking. Differences between human walking and prosthesis walking can be due to a variety of reasons. Since the experiment was conducted with the able-bodied subject, using a simulator could affect the subject\\u2019s gait itself which is related to the difference from human data. The joint angle differences during the stance phase can be improved by a tuning process to provide better impedance parameters. This also can improve the result at swing phase because the proposed method can be varying depending on the initial conditions at PO.'\",\"293\":null,\"294\":null,\"295\":null,\"296\":\"\\\"We address the problem of learning to recognize new objects on-the-fly efficiently. When using CNNs, a typical approach for learning new objects is by fine-tuning the model. However, this approach relies on the assumption that the original training set is available and requires high-end computational resources for training the ever-growing dataset efficiently, which can be unfeasible for robots with limited hardware. To overcome these limitations, we propose a new architecture that: 1) Instead of predicting labels, it learns to generate discriminative and separable embeddings of an object's viewpoints by using a Supervised Triplet Loss, which is easier to implement than current smart mining techniques and the trained model can be applied to unseen objects. 2) Infers an object's identity efficiently by utilizing a lightweight classifier in the features embedding space, this keeps the inference time in the order of milliseconds and can be retrained efficiently when new objects are learned. We evaluate our approach on four real-world images datasets used for Robotics and Computer Vision applications: Amazon Robotics Challenge 2017 by MIT-Princeton, T-LESS, ToyBoX, and CORe50 datasets. Code available at [1].\\\"\",\"297\":null,\"298\":\"'The main contributions of this paper are summarized as follows. First, we propose a lightweight network called DSNet for joint disparity estimation and scene parsing. Benefited from a series of shared convolutional encoder modules, the DSNet is efficient at generating semantic and disparity information together, surpassing previous models [18] [10] that extract convolutional features for semantic segmentation and disparity estimation separately. Second, through extensive experiments, we designed an efficient matching module for the learning of semantic and disparity information, building a bridge between the two tasks. At last, we put forward a training method to effectively leverage the annotated labels of semantic and disparity information in a single network.'\\n\\n'The training process can be divided into three steps: (1) Firstly, we train the semantic network, which extracts auxiliary features for subsequent disparity estimation. (2) Then we fix the weights of the semantic network and train the disparity decoder independently. (3) Finally, we conduct joint learning of two tasks by employing a match module and warping operation. As demonstrated by the experimental results, such a training method achieves the best performance.'\",\"299\":null,\"300\":null,\"301\":\"'Top down views of two mapped sequences. Color encodes height (yellow low, red high). Left: Raw measurements mapped into one coordinate frame. Measurements on dynamic objects result in artifacts visible as orange lines on the yellow ground. Right: Same scans with dynamic objects filtered out. Top: Pilot only. Bottom: Four persons.'\",\"302\":\"'We present a method for localization of Unmanned Aerial Vehicles (UAVs) which is meant to replace an onboard GPS system in the event of a noisy or unreliable GPS signal. Our method requires only a downward-facing monocular RGB camera on the UAV, and pre-existing satellite imagery of the flight location to which the UAV imagery is compared and aligned. To overcome differences in the image capturing conditions between the satellite and UAV, such as seasonal and perspective changes, we propose the use of Convolutional Neural Network (CNN) representations trained on readily available satellite data. To increase localization accuracy, we also develop an optimization which jointly minimizes the error between adjacent UAV frames as well as the satellite map. We demonstrate how our method improves on recent systems from literature by achieving greater performance in flight environments with very few landmarks. For a GPS-denied flight at 0.2km altitude, over a flight distance of 0.85km, we achieve average localization error of less than 8 meters. We make our source code and datasets available to encourage further work on this emerging topic.'\",\"303\":null,\"304\":null,\"305\":null,\"306\":null,\"307\":\"'https:\\/\\/bitbucket.org\\/lucacarlone\\/codesigncode\\/'\\n\\n'https:\\/\\/bitbucket.org\\/lucacarlone\\/codesigncode\\/'\\n\\n'https:\\/\\/bitbucket.org\\/lucacarlone\\/codesigncode\\/'\\n\\n'computational robot codesign'\\n\\n'Codesign Experiments'\\n\\n'B. Codesign of Heterogeneous Multi-Robot Teams'\",\"308\":\"'A trajectory of each vehicle is encoded by the array\\\\nN\\\\nv\\\\nwhere each node\\\\nlJ\\u2208\\\\nN\\\\nv\\\\nis associated to (i) a position in the input space denoted\\\\n\\u03bd\\u2208\\\\nR\\\\n3\\\\n; (ii) the particular object\\\\n\\u03bd.0\\u2208O\\\\nto be visited; and (iii) the waypoint\\\\n\\u03bd.p\\u2208\\\\nR\\\\n3\\\\ninside the\\\\n\\u03b4\\\\n-neighborhood of 0, i.e.,\\\\n\\u2225(\\u03bd.0, \\u03bd.p)\\u2225\\u2264\\u03b4(\\u03bd.0)\\\\n, which allows collecting the reward of the object within\\\\n\\u03b4\\u2212\\\\nradius from 0. Besides, the trajectory is a sequence of B\\u00e9zier curves, and thus two consecutive nodes\\\\n\\u03bdi\\\\nand\\\\n\\u03bdi+1\\\\nof\\\\nN\\\\nv\\\\ndefine each particular B\\u00e9zier curve where the waypoints\\\\n\\u03bd\\\\ni\\\\n.p\\\\nand\\\\n\\u03bd\\\\ni+1\\\\n.p\\\\nare the curve endpoints. Hence, two additional control points defining the departure and terminal tangents are associated to\\\\n\\u03bdi\\\\nand\\\\n\\u03bdi+1\\\\nthat are used for determining the TTE similarly to the solution of the TSP with B\\u00e9zier curves presented in [5]. On the other hand, in the solution of the OP, each trajectory has to satisfy Tmax, and therefore, the adaptation of\\\\nN\\\\nv\\\\nto 0 is conditioned to increase the sum of the collected rewards R while satisfying Tmax.'\",\"309\":\"'https:\\/\\/youcu.be\\/b9H-zOYWLbY'\\n\\n'https:\\/\\/github.com\\/hungpham2511\\/toppra'\\n\\n'https:\\/\\/youtu.be\\/b9H-zOYNLbY'\\n\\n'The full pipeline is available as open-source1 Experimental results are reported and discussed in Section IV. A discussion of related works is postponed to Section V.'\\n\\n'https:\\/\\/github.com\\/hungpham2511\\/toppra'\",\"310\":null,\"311\":\"'Fig. 5 displays the target SEA system for extending and flexing the metacarpophalangeal joints (MCP) of the hand that consists of a hand holder, a torsional spring and a motor. The hand holder is mounted to support and fixate the hand on it. The torsional spring (Lee Spring, LTL092G10M) of stiffness 0.8 Nm\\/rad is included between the motor and the hand holder. The motor (LS Mecapion, APM-SA01ACN8) embeds an encoder of a resolution of 2048 pulses per revolution and is connected with a harmonic drive of a gear ratio of 50:1. The hand holder weighs 273 g in total. A custom ACM motor drive (ADVANCED Motion Controls) is used to convert the motor command. A potentiometer (P3 America, STC12 R10K) is located inside the spring to measure its deformation. The absolute position of the hand holder is obtained using both the encoder and the potentiometer. The external torque that is fed back to the control input is estimated with the deformation of the spring multiplied by its spring, assumed that the spring behaves in a linear manner. Data processing is conducted in a dedicated LabVIEW Real-Time PC. The control signal is limited to \\\\\\\\pm 10 V. The sampling time is set as 1000 Hz.'\",\"312\":null,\"313\":null,\"314\":\"'Precise robotic manipulation skills are desirable in many industrial settings, reinforcement learning (RL) methods hold the promise of acquiring these skills autonomously. In this paper, we explicitly consider incorporating operational space force\\/torque information into reinforcement learning; this is motivated by humans heuristically mapping perceived forces to control actions, which results in completing high-precision tasks in a fairly easy manner. Our approach combines RL with force\\/torque information by incorporating a proper operational space force controller; where we also exploit different ablations on processing this information. Moreover, we propose a neural network architecture that generalizes to reasonable variations of the environment. We evaluate our method on the open-source Siemens Robot Learning Challenge, which requires precise and delicate force-controlled behavior to assemble a tight-fit gear wheel set.'\",\"315\":\"'Compliant and precise rotary drive units are essential for the design of articulated robots that are capable of safe human-robot collaboration. In this paper, we present a new pneumatic rotary drive unit that combines the compliance of pneumatic systems with the ability to perform high precision positioning. We use pneumatic artificial muscles (PAMs) pulling on a swash plate to avoid the stick-slip phenomenon and to realize adjustable stiffness. Furthermore, the presented drive unit can operate in 360\\u00b0 continuous rotation. These properties make the drive particularly suitable for the later use in human-robot collaboration. We explain the mechanic design as well as the pneumatic and electric control system that we use to operate the drive unit. We derive the equations to calculate the static torque distribution and compare the theoretical results to the data measured on the realized laboratory test stand, depicted in figure 1. The accuracy of the used 16-bit encoder is achieved and adjustable stiffness is realized and measured on the laboratory test stand. The measurements of the reaction to a step response are discussed based on a first and basic control strategy.'\\n\\n'Sectional view of the 3D-CAD model with three PAMs, encoder a, coupling b, cable drum c, structure d, drive shaft e, inner swash plate f, additional fixture g, leveling foot h, PAMs i, connecting elements j, four point bearing k, outer swash plate 1, shaft bearings m, centerline of the rotary axis n, red arrow indicating the positive direction of rotation and\\\\n\\u03c6'\\n\\n'The Sick 16-bit encoder detects the angular position with an accuracy of 0.0055\\u25cb The proportional valves can be operated between 6 and 600 kPa and receive input signals from 0\\u201310 V. In figure 9 , the angular position is depicted in relation to the smallest possible incremental steps for pressure values detected on the input of the control. To evaluate the possibility of reaching every incremental step of the angular position encoder, PAM one is inflated to 400 kPa, PAM two to 300 kPa and PAM three to 200 kPa. Then the pressure value in PAM three is increased by 0.000183 kPa every 10 ms. This is the smallest amount of pressure that can be added within the resolution of the PLC. Following these steps, it is shown that in the depicted section the pressure has to increase by one or more increments before the angular position increases one increment. Hence, we do not encounter any issues during our investigations that are caused by the occurrence of stick-slip phenomena. As a result, it is possible to position the drive at any desired set point position within the 360\\u25cb detectable with the encoder.'\",\"316\":\"'The experiments were conducted with a single link antagonistic tendon-driven mechanism. This mechanism was composed of two motors and one encoder as shown in Fig. 3 . In the figure, both motorl and motor2 provide the tensile forces to actuate the link, and the encoder was installed to confirm the actual position and torque of the link but it was not used for the controller. The steel wires were installed for applying the tensile forces to the link and they were connected independently to the drums on the link to prevent them from being slipped on the drum surfaces. The tendon map of the experimental setup shown in Fig. 1 and 3 can be obtained as follow:'\\n\\n'Antagonistic tendon-driven mechanism for a single joint, where the motorl performs a negative directional movement of thejoint and the motor2 conducts a positive directional movement of the joint, and the encoder is installed to calculate the exactness of the link movement but it is not used for the control.'\",\"317\":null,\"318\":null,\"319\":\"'The source code is available online 1 . This is the first open-source implementation for tightly coupled lidar and IMU fusion available to the community.'\",\"320\":\"'Non-Gaussian and multimodal distributions are an important part of many recent robust sensor fusion algorithms. In difference to robust cost functions, they are probabilistically founded and have good convergence properties. Since their robustness depends on a close approximation of the real error distribution, their parametrization is crucial. We propose a novel approach that allows to adapt a multi-modal Gaussian mixture model to the error distribution of a sensor fusion problem. By combining expectation-maximization and non-linear least squares optimization, we are able to provide a computationally efficient solution with well-behaved convergence properties. We demonstrate the performance of these algorithms on several real-world GNSS and indoor localization datasets. The proposed adaptive mixture algorithm outperforms state-of-the-art approaches with static parametrization. Source code and datasets are available under https:\\/\\/mytuc.org\\/libRSF.'\",\"321\":null,\"322\":null,\"323\":null,\"324\":null,\"325\":null,\"326\":null,\"327\":null,\"328\":\"'Our UGV is a heavily modified 1\\/5 scale Redcat Racing Rampage XB-E equipped with an onboard Gigabyte BRIX computer with an i7 processor. A LORD Microstrain 3DM-GX4-25 IMU measures inertial data while a u-blox C94-M8P RTK GPS computes global position. A hall effect sensor encoder measures the rotation rate of the drive shaft, which is converted to the body velocity of the vehicle after calibration.'\",\"329\":null,\"330\":null,\"331\":\"'https:\\/\\/youtu.be\\/hKDuGV-wIqo'\",\"332\":null,\"333\":null,\"334\":\"'https: \\/\\/rosbag.tier4.jp\\/'\\n\\n'A new decoder with a residual block structure, which improves point cloud decompression accuracy.'\\n\\n'Deep learning has already achieved state-of-the-art results in many fields, and data compression may be a task which deep leaning is good at. Many researchers have investigated image compression using deep learning, which can be thought of as the compression of a 2D matrix. Auto-encoders have also been used to reduce the dimensionality of images [23] or to convert images to a smaller volume format so they can be used in other applications [24], [25]. In a subsequent study, auto-encoders were used directly in a compression task [26]. Recently, Toderici [10] proposed a recurrent neural network based approach which can compress arbitrary resolution images, slightly outperforming conventional image compression methods.'\\n\\n'During compression, data passes through the encoder, binarizer and decoder, but during decompression only the decoder is needed.'\\n\\n'2) Encoder:'\\n\\n'Fig. 4(a) shows the detail of encoder network which consists of one convolutional layer and three convolutional LSTM layers. Different with normal LSTM, here we use use convolution operator in place of matrix multiplication.'\\n\\n'To output binary compressed data, a binarizer follows encoder. Binarizer consists of one convolution layer, a tanh function and a sign function. Convolution layer fix output size into a\\\\n2\\u00d72\\u00d732\\\\narray, then a tanh function with sign function binarize the value into bits. After it, each\\\\n32\\u00d732\\u00d71\\\\ninput array is reduced to a\\\\n2\\u00d72\\u00d732\\\\nbinarized representation per iteration, which results in each iteration representing\\\\n1\\/s\\\\nbit per point (bpp).'\\n\\n'4) Basic decoder:'\\n\\n'In decoder, we need to reconstruct original\\\\n32\\u00d732\\u00d71\\\\ninput from\\\\n2\\u00d72\\u00d732\\\\nbinary compressed data with previous state. Basic decoder\\u2019s structure follows Toderici et al. [10], like Fig. 4(b) shows. At the beginning of decoder, a convolution layer expands the input channel. Next, a convolutional LSTM layer and a pixel shuffle layer [27] are used to double the size of the input sensor, which is similar to a super resolution task. After four pixel shuffle layers, we can reconstruct the same size output as original input.'\\n\\n'5) Decoder with residual block:'\\n\\n'When using the basic decoder network, it is difficult to achieve highly accurate decompression, even if we increase the number of iterations. Many issues may cause this, but we think one of the most important is that the decoder is not \\\"flexible\\\" enough to handle different textures or scenarios. If this is the case, adding more iterations will not really improve the quality of the decompression results.'\\n\\n'To address this problem, we propose utilizing a residual block structure within the decoder, which we think will allow processing of a wider range of scenarios, leading to more accurate decompression.'\\n\\n'In this paper we proposed the use of a recurrent neural network to compress point cloud data from 3D LiDAR. By taking advantage of the feature extraction and context analysis capability of RNN with convolutional layers, the proposed method can tune the compression rate with decompression error and outperform previous image compression based approach and octree compression approach. By adding a residual block structure to the decoder, we were able to further improve decompression performance at almost the same calculation cost, without adding volume. Considering application scenarios such as SLAM and localization using a provided map, this paper shows the potential uses of the proposed method in real robotics applications.'\",\"335\":\"'In this paper, we present an end-to-end convolutional neural network (CNN) for depth completion. Our network consists of a geometry network and a context network. The geometry network, a single encoder-decoder network, learns to optimize a multi-task loss to generate an initial propagated depth map and a surface normal. The complementary outputs allow it to correctly propagate initial sparse depth points in slanted surfaces. The context network extracts a local and a global feature of an image to compute a bilateral weight, which enables it to preserve edges and fine details in the depth maps. At the end, a final output is produced by multiplying the initially propagated depth map with the bilateral weight. In order to validate the effectiveness and the robustness of our network, we performed extensive ablation studies and compared the results against state-of-the-art CNN-based depth completions, where we showed promising results on various scenes.'\\n\\n'The geometry network takes four-channel input, where the first three channels represent an RGB image and the last channel is composed of a prior sparse depth. We pass the four-channel input through an encoder-decoder network as shown in the red box of Fig. 2. The encoding part is based on ResNet -50 pre-trained on the ImageNet [19] (which neglects the last average pooling layer and linear transformation layer) with an additional convolution layer. The decoding part contains four up-projection layers as proposed in [12], followed by an upsampling layer. This preserves both the high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps.'\",\"336\":\"'The proposed network follows an encoder-decoder paradigm [32] with early fusion of RGB and d, as displayed in Figure 2. The encoder consists of a sequence of convolutions with increasing filter banks to downsample the feature spatial resolutions. The decoder, on the other hand, has a reversed structure with transposed convolutions to upsample the spatial resolutions. Long skip connections between the encoder and decoder empirically improves the sharpness of depth prediction.'\",\"337\":\"'https:\\/\\/www.rgbdinhandmanipulation.com\\/'\",\"338\":\"'We present a novel approach of multi-modal deep generative models and apply this to coordinated heterogeneous multi-agent active sensing. A major approach to achieve this objective is to train a multi-modal variational Auto Encoder (M 2 VAE) that integrates the information of different sensor modalities into a joint latent representation. Furthermore, we derive an objective from the M 2 VAE that enables the maximization of the evidence lower bound via selection of sensor modalities. Using this approach as a direct reward signal to a multi-modal and multi-agent deep reinforcement learning setup leads intuitively to an epistemic active sensing behavior that coordinately resolves the ambiguity of observations.'\\n\\n'variational Auto Encoder'\\n\\n'A. Variational Auto Encoder'\\n\\n'B. Multi-Modal Auto Encoder'\\n\\n'1) Joint Multi-Modal Variational Auto Encoder:'\\n\\n'Joint Multi-Modal Variational Auto Encoder from Variation of Information'\\n\\n' Unimodal PDF fitting of encoder b '\\n\\n' Unimodal PDF fitting of encoder a '\\n\\n'Therefore, uni-modal encoders are trained, so that their distributions\\\\nq\\\\n\\u03d5\\\\na\\\\nand\\\\nq\\\\n\\u03d5\\\\nb\\\\nare close to a multi-modal encoder\\\\nq\\\\n\\u03d5\\\\nab\\\\nin order to build a coherent posterior distribution. The introduced regularization by Suzuki et al. [12] puts learning pressure on the uni-modal encoders just by the distributions\\u2019 shape, disregarding reconstruction capabilities and the prior\\\\np(z)\\\\n. Furthermore, one can show that deriving the ELBO from the VI for a set of\\\\nM\\\\nobservable modalities, always leads to an expression of the ELBO that allows only training of\\\\nM={m|m\\u2208P(M), |m| = |M|\\u22121}\\\\nmodality combinations. This leads to the fact that for instance in a tri-modal setup, as shown in Fig. 2, one can derive three bi-modal encoders from the VI, but no uni-modal ones.'\\n\\n'A. Multi-Modal Variational Auto Encoder'\\n\\n'all encoder\\/decoder networks can jointly be trained using SGVB'\\n\\n'The tVAE founds a much more coherent embedding between the encoders. This was achieved by the fact, that first the full multi-modal VAE, consisting out of the encoder\\\\nq\\\\n\\u03d5\\\\nab\\\\nand two decoder\\\\np\\\\n\\u03b8\\\\na\\\\nand\\\\np\\\\n\\u03b8\\\\nb\\\\n, was trained. Second, the decoder weights are pinned to train the remaining uni-modal networks which enforces coherence. However, the ELBO per embedding also does not allow any direct conclusion between the embeddings of the various encoders. This is depicted by\\\\n(\\u223c)\\\\n, where the multi-modal encoder\\\\nq\\\\n\\u03d5\\\\nab\\\\nproduces embeddings of higher energy than these of the unimodal ones. This can happen as there is no regularizer which enforces the variational distribution of the encoders to match each other and thus, the KL-divergence may differ between the models for similar encodings.'\",\"339\":null,\"340\":null,\"341\":\"'This research was funded by the Office of Naval Research, Code 321.'\",\"342\":\"'A robot system is designed as a set of embodied agents. An embodied agent is decomposed into cooperating subsystems. In our previous work activities of subsystems were defined by hierarchical finite state machines. With their states activities were associated. In that approach communication between subsystems was treated as an implementation issue. This paper represents activities of a robot system using hierarchical Petri nets with conditions. Such net is created by specifying consecutive layers: multi-agent robot system layer, agent layer, subsystem layer, behaviour layer and communication layer. This decomposition not only organizes in a systematic manner the development of a robot system, but also introduces a comprehensive description of concurrently acting subsystems. Based on those theoretical considerations, a tool was created for producing hierarchical Petri nets defining the model of a robotic system and enabling automatic generation of the robot controller code, resulting in a significant acceleration of the implementation phase. The capabilities of the tool are presented by the development of a robot controller performing a rudimentary task.'\\n\\n'The presented design methodology is currently tested by specifying robot systems and automatically generating their control code. The example presented here is purposefully kept simple to explicate the methodology and not to obscure it by overly complex robot and its task. Thus the control system of a rudimentary simulated robot executing the follow the line task is presented.'\\n\\n'The generation of a robot controller code requires the creation of the\\\\nH\\\\nHPN and its subnets. For that purpose a tool has been developed by the authors. The designed Petri net is automatically transformed into C++ code, forming the robot controller. The generated controller representing the HPN with an initial marking is then merged with the library executing the HPN.'\\n\\n'(a) Generated code simulation, (b) Agent layer net\\\\nH\\\\n1\\\\ndesigned using the tool developed by the authors'\\n\\n'The resulting C++ code is compiled and the outcome is loaded into the control computer. The code invokes the scheduler, which searches for active transitions (enabled transition with fulfilled condition). One of such transitions is fired, i.e. a token is removed from each input place (place directly connected to the fired transition by a directed arc pointing at the transition) and inserts tokens into each output place (place directly connected to the transition by a directed arc pointing at the place). The operations associated with the output places are executed in separate threads. When the directed arc connects the firing transition with an output page, the input place of that page receives a new token and the associated operation of that input place starts its execution in a new thread. The scheduler repeats the above-mentioned steps either endlessly or until a behaviour commands it to terminate its activities.'\\n\\n'A natural inclusion of the communication model into the system specification is possible by using HPNs with conditions. The proposed approach, follows the primary principle of structured programming, stating that the programmer must keep at all times the produced code within his or her intellectual grasp [4]. It structures a robotic system into layers of PNs describing the activities of ever smaller modules. The presented approach enables automatic code generation of the robotic controller. The resulting specification is a single HPN describing the activities of the concurrently executed activities of subsystems, also describing their interactions. The proposed specification method can be used to describe the activities of any robotic system. For a multi-robot system the developer defines separate HPNs for each robot (being itself a multi-agent system). Those HPNs communicate with each other using communication models implemented as interprocess communication.'\\n\\n'The purpose of the example presented in this paper has been the exemplification of the proposed specification and implementation method by focusing the reader\\u2019s attention only on the methodology, treating the specified robotic system as of secondary importance, thus its rudimentary character. More complicated systems require the development of more complex HPNs. Any addition of a place to a net requires 6 extra lines of code, a new transition generates 6 additional lines, an extra directed arc only a single line, while a page 48 lines. Thus it can be shown that the size of the generated code grows linearly with the size of the designed Petri net, hence the presented approach based on HPNs is scalable.'\",\"343\":null,\"344\":null,\"345\":null,\"346\":null,\"347\":null,\"348\":null,\"349\":null,\"350\":null,\"351\":null,\"352\":null,\"353\":null,\"354\":null,\"355\":\"'https:\\/\\/github.com\\/uwgraphics\\/stampede'\\n\\n'We provide open-source code that implements the methods discussed in this work at https:\\/\\/github.com\\/uwgraphics\\/stampede.'\\n\\n'https:\\/\\/github.com\\/uwgraphics\\/stampede'\",\"356\":\"'Hand exoskeletons allow us to interact with virtual or remote environments intuitively. It is necessary to acquire the position and orientation of the human hand and the joint angles of the fingers, for determining contact or penetration of remote or virtual environments, positioning the slave manipulator or virtual human hand, and calculating force feedback. Several dexterous haptic interfaces use encoders on mechanical joints fixed to joints of the fingers [1], or sensors in a data glove [2], [3], which often results in bulky interface designs when force-feedback is also required. We present an approach to accurately estimate pose and configuration of the human hand from only the positions of well-chosen attachment points to the exoskeleton.'\",\"357\":\"'https:\\/\\/www.siemens.com\\/us\\/en\\/home\\/company\\/fairs-events\\/robot-learning.html'\",\"358\":null,\"359\":null,\"360\":\"'Monte-Carlo simulation results: (top-left) 250 meter long 3D trajectory with the start and end locations denoted as green square and red diamond, respectively. (bottom-left) RMSE of pose estimates, color-coded segments based on which base IMU was active. (center) IMU-IMU calibration RMSE of the first 10 seconds of the dataset (as it converges after that). (right) IMU-CAM calibration RMSE for the first 10 seconds for the stereo pair transformation and time offset.'\",\"361\":\"'We implemented CL-VO using Tensorflow and Keras, and ran the training code on a NVIDIA TITAN V GPU. Before training, we computed the dataset mean and used it to normalize the image intensity. In order to provide more trajectory variations, we generated sequences with random start and end points, and random lengths. In every epoch, we constructed 10 random trajectories for each training sequence. The training can extend to 200 epochs for each training stage which takes around 10 hours, or can be stopped earlier if the validation loss shows no improvement. We used the Adam optimizer with\\\\n1e\\u22123\\\\nas the initial learning rate. We also applied Dropout [29] with 0.2 dropout rate for regularizing the network. For parameter in (3)\\u2013(6), we set\\\\n[\\u03b4;\\u03b6]=[1;100]\\\\nfor the KITTI dataset, and\\\\n[\\u03b4;\\u03b6]=[1;0.001]\\\\nfor the human motion dataset. For GA-CL setting, we mostly set the window\\\\nw=2\\\\nor 3 and\\\\n\\u03b1=1\\\\nfor the 1st stage,\\\\n\\u03b1=0.5\\\\nfor the 2nd stage, and\\\\n\\u03b1=0.1\\\\nfor the 3rd stage as it get the best performance in KITTI dataset.'\",\"362\":\"'https:\\/\\/github.com\\/izhengfan\\/se2lam'\\n\\n'https:\\/\\/github.com\\/izhengfan\\/se2lam'\",\"363\":\"'https:\\/\\/youtu.be\\/-hnL5kLqT4Q'\\n\\n'https:\\/\\/youtu.be\\/-hnL5kLqT4Q'\",\"364\":\"'Considering that encoders are available on the links and the brushless DC motors, the output of the encoders includes noise'\",\"365\":\"'We model our problem using the open source physics simulation environment Project Chrono [18]. This environment provides the flexibility to mix finite elements and rigid bodies, with a fine grained control of physical parameters. For this work, we consider flat terrains without any obstacles, with physical properties close to that of hard rubber. We model each MDU as a perfect 10cm diameter sphere made of rubber with a weight of 110g. The surface properties of the ground and the MDU are the same (summarized in Table I). We used a collision margin of 0.1mm for every simulated object.'\",\"366\":\"'Fig. 9 shows the transition of contact mode by using recoded data from the case in (A) of Fig. 8, but with the mentioned previous method. The case in (A) of Fig. 9 successfully estimates the timing for the ready-to-close signal as intended, using the trigger information of two collision signals that are only collisions of the trial. On the other hand, ((B) of Fig. 9) fails due to the remaining rate of\\\\nw\\\\nd10\\\\n=p(\\\\nd\\\\n(10)\\\\n)=0.0163\\\\nso that\\\\np(\\\\nd\\\\nrtc\\\\n)\\\\ncannot go to 1.00. It occurs because impedance control induces a longer collision time, and during that period, some particles can reach Vertex 10 in certain cases. However, the filter triggered by sliding can save such a situation by adding another instance for filtering. The success rate is 100% with the contact-event-triggered filter, and 87.9% without it as summarized in Table III. It shows the advantage of contact mode estimation using the contact-event-triggered filter.'\",\"367\":null,\"368\":\"'https:\\/\\/youtu.be\\/0QK0-Vx7WJkI'\\n\\n'In this paper, we present our work on a learning-based tactile servoing algorithm that does not assume a specific sensor geometry. Our method comprises three steps. At the core of our approach, we treat the tactile skin as a manifold, hence first we perform an offline neural-network based manifold learning, to learn a latent space representation which encodes the essence of the tactile sensing information. Second, we learn a latent space dynamics model from human demonstrations. Finally, we deploy our model to perform an online control \\u2014 based on both the current and target tactile signals \\u2014 for tactile servoing on a robot.'\\n\\n'Our auto-encoder takes in 19 dimensional input vector s, and compresses it down to a 3 dimensional latent state embedding, z. The intermediate hidden layers are fully connected layers of size 19, 12, 6, all with tanh activation function, forming the encoder function fenc. The decoder part is a mirrored structure of the encoder function, forming\\\\nf\\\\ndec\\\\n.\\\\nh\\\\nfcnnNL\\\\nis a feedforward neural network with 9 dimensional input (3 dimensional latent state z and 6 dimensional action policy a), 1 hidden layer of size 15 with\\\\ntanh\\\\nactivation functions, and 3 dimensional output. hfcnnLL is a feedforward neural network with 3 dimensional latent state z as input, 3 hidden layers of size 8, 15, 23, all with\\\\ntanh\\\\nactivation function, and 30 dimensional output which corresponds to the parameters of\\\\nA\\\\nt\\\\n,\\\\nB\\\\nt\\\\n, and ct in Eq. 9.'\\n\\n'B. Auto-Encoder Reconstruction Performance'\\n\\n'Our first evaluation is on the reconstruction performance of the auto-encoder in terms of normalized mean squared error (NMSE). NMSE is the mean squared prediction error divided by the variance of the ground truth. We obtain all NMSE values are below 0.25 for the training (85% split), validation (7.5% split), and test (7.5% split) sets.'\",\"369\":\"https:\\/\\/github.com\\/lianghongzhuo\\/PointNetGPD.git\",\"370\":\"'https:\\/\\/sites.google.com\\/view\\/hand-vil\\/'\",\"371\":\"'High-level diagram of our cross-modality instance recognition model. ResNet-50 CNN blocks are used to encode both of the tactile readings and the visual observation. Note that the weights of the ResNet-50s for the two tactile readings are tied together. The features from all modalities are fused via concatenation and passed through 2 fully connected layers before outputting the probability that the readings match.'\",\"372\":\"'https:\\/\\/sites.google.com\\/view\\/deeprl-handmanipulation'\\n\\n'A. State Estimation with Encoder Sensors'\",\"373\":null,\"374\":\"'In this experiment the main spring characteristics are measured to evaluate the maximum output torque and to allow accurate output torque estimation based on encoder readings. The magnetic encoder is used to measure the deflection of the main spring. To be able to predict the output torque with the measured encoder output, a linear model is fitted on the measured output torque.'\",\"375\":\"'http:\\/\\/graphics.ucdenver.edu\\/generativedeformation.html'\\n\\n'Procedural generation of elastic structures provides the fundamental basis for controlling and designing 3D printed deformable object behaviors. The automation through generative algorithms provides flexibility in how design and functionality can be seamlessly integrated into a cohesive process that generates 3D prints with variable elasticity. Generative deformation introduces an automated method for perforating existing volumetric structures, promoting simulated deformations, and integrating stress analysis into a cohesive pipeline model that can be used with existing consumer-level 3D printers with elastic material capabilities. In this work, we present a consolidated implementation of the design, simulate, refine, and 3D print procedure based on the automated generation of heterogeneous lattice structures. We utilize Finite Element Analysis (FEA) metrics to generate perforated deformation models that adhere to deformation behaviors created within our design environment. We present the core algorithms, automated pipeline, and 3D print deformations of various objects. Quantitative results illustrate how the heterogeneous geometric structure can influence elastic material behaviors towards design objectives. Our method provides an automated open-source tool for quickly prototyping elastic 3D prints.'\\n\\n'In this work, we introduced an automated method for generating heterogeneous material structures through the use of volumetric perforation. The presented method provides a generative deformation pipeline that enables user constraints and stress analysis using recorded FEA simulations to control the elastic behaviors of elastic 3D printed objects. The contributions of this work are consolidated within an open-source tool for use with various types of consumer-level 3D printers with limited selections of flexible materials.'\",\"376\":\"'As a remotely accessible robotics development facility, the Robot Learning Lab\\u2019s main purpose is to increase the availability of state-of-the-art industrial robots for education and research. Thus it has to implement a number of high-level design requirements aimed at accessibility, scalability, automation and safe and secure code execution.'\\n\\n'Enable fully automated execution of code submissions with an efficient job processing pipeline that can handle a large amount of users and allows for parallel execution of jobs in multiple robot work cells, and that requires minimal maintenance and monitoring by a human operator.'\\n\\n'Integrate safety and security measures to protect the Robot Learning Lab from damage and misuse through safe operation of the robots with collision checks and user code isolation.'\\n\\n'Overview of the job processing pipeline and the user code execution. The left figure shows the job processing pipeline that includes the web API and the workers for simulation and the real robots. The right figure illustrates how user code is executed by the worker in an isolated environment with on-line checks for collision-free execution on the robot.'\\n\\n'The Robot Learning Lab is able to fully automatically process a submission. By making a new submission through the web interface, a user triggers the scheduling of a single execution of his submitted code. In the following, such a submission is called a \\u201cjob\\u201d. Each job is tracked in the system by a unique job ID. Jobs need to be submitted for a specific \\u201cproject\\u201d and the submitted code, that corresponds to a job, should provide solutions to the tasks that are defined within a project. A task could be to implement an inverse kinematics solution for the robot or to detect the poses of objects that should be grasped by the robot. A project is defined by a certain experimental setup in a robot work cell and by its own software user interfaces that are supplied by a project software package. The system is aware for which projects the robot work cells are currently configured and processes the jobs accordingly. The jobs are processed by \\u201cworkers\\u201d which will be further detailed in the following (see Figure 2 for an overview).'\\n\\n'The status of the job is reflected by several status codes like \\u201cdownloading code\\u201d, \\u201cbuilding\\u201d, \\u201cfinished\\u201d. When a job is submitted and waiting for execution, the position in the queue is returned. If the job is currently running in the lab, a link to the live video feed of the network camera in the respective robot work cell, where the job is executed, is returned. A new submission can be made by providing an archive of the code1.'\\n\\n'Simulation runs are controlled by sim workers and code executions for the robot work cells in the lab are handled by real workers (see Figure 2). The expressions \\u201csim\\u201d and \\u201creal\\u201d are generally used to differentiate between simulation and the real robot work cells in the lab. The workers are responsible for code download, building the user code and code execution. They also retrieve and save the job data, including job metadata like job result codes (e.g. \\u201cbuild failed\\u201d, \\u201csim success\\u201d, \\u201claunching project failed\\u201d), and trigger a reset of the robot work cell back to the start state after a job run. The server and workers communicate through a database that stores the job metadata. The workers are configured to process jobs for a specific project. They search the database for jobs, that were submitted for this specific project, and select the oldest submission as the next job to be executed.'\\n\\n'The Robot Learning Lab leverages the ROS robotics middleware. Every worker instance is isolated in its own ROS namespace, together with the user code execution environment. This way, several sim workers for parallel simulation checks and multiple real workers for an arbitrary number of robot work cells can run at the same time and process jobs in parallel.'\\n\\n'The worker and the server as part of the job processing pipeline are hardware-agnostic and can be used with any robotic hardware. Alongside this paper, we are publishing their source code as free and open-source software in the hope, that it will allow others to build their own remotely accessible robotics testbed.2'\\n\\n'The system is able to detect failures that cannot automatically be corrected by itself. Such \\u201cinternal errors\\u201d are registered during job execution when methods fail that should always succeed or when the system detects an unexpected or unknown state of the environment (simulation or real) from which it cannot get back to a desired state. Internal errors are also reported if the environment reset failed after job execution. Although the system and project design should minimize the likelihood of internal errors, users might still be able to trigger them. If a worker detects an internal error, it immediately stops its job execution environment including the user code, notifies the operator and terminates itself so that no further jobs are executed in the environment. This allows an operator to inspect the error and ensures that the system does not operate in an undesired state which can lead to hardware damage.'\\n\\n'For security reasons, the user code is compiled and run inside a Docker container without root privileges. Docker containers are more lightweight than virtual machines because they directly run on the host system\\u2019s kernel. It is possible to restrict system resources for containers, like the number of CPU cores, RAM and disk space. For every project, an image with all the needed software packages and configurations can be created and the worker sets up a proxy and client container based on the images and additional configuration options. The client container runs the downloaded user code and the proxy container provides the project-specific interface for the user code and runs the separate ROS master for the client. The worker places the downloaded user code in the client container and extracts log files from both the proxy and client container.'\\n\\n'The main purpose of the proxy container is to prevent the user code from accessing the host network and the lab network. If users were able to call arbitrary ROS network resources or even send commands directly to the robot, they could bypass all safety measures and cause serious damage to the robot work cells. For every job, the worker starts a new client container and attaches it to the same Docker network as the one the proxy container uses. The client container only has access to the proxy container, while the proxy container is also able to connect to the lab network. The project-specific interface inside the proxy container offers all the ROS network resources the user needs for the project (see Section III-C.3).'\\n\\n'The separate ROS master inside the proxy container allows the user code to look up the IP address and ports of interface nodes and to register its own services, topics, actions and parameters. The interface in the proxy container uses the ROS master from the host, so that ROS nodes running on the host are able to connect to it. The worker needs to synchronize the registrations that the interface makes at the host master to the client master so that the user code is able to see them. The registrations at the client master by the client code are synchronized selectively by the worker to the host master, so that the interface can communicate with client implemented code (see Figure 2). The worker reads the registrations that need to be synchronized from project-specific configuration files.'\\n\\n'Besides a video, users can retrieve the build log, the log file that their code generated during execution and the part of the log file from the interface that corresponds to their job run, both for the simulation and the real run. These files usually suffice for debugging and evaluation. Furthermore, data can be recorded during job execution and provided as ROS bag files6 to users.'\\n\\n'The software structure of every project needs to consist of two parts: user-facing interface nodes and the user code. While users can run both on the same PC they use for development, the interface nodes are meant to be running in the proxy container and the user code should run in the client container in the Robot Learning Lab (see Section III-C.2). The user code is developed by the users of the Robot Learning Lab and it accesses ROS resources provided by the interface nodes. The interface nodes can implement new functionality or they directly relay other resources like sensor data.'\",\"377\":null,\"378\":null,\"379\":null,\"380\":null,\"381\":\"'The simulations were performed using the open source simulator Gazebo with a maximum step size of\\\\n0.0001[s]\\\\n. The results of the simulation with the velocity approach are depicted in Fig.2. The CoM height increases of 3.5 [cm] during the flight phase, while the maximum height reached by the feet soles is 3.7 [cm].'\",\"382\":null,\"383\":\"'https:\\/\\/youtu.be\\/MrqmnBbXc70'\\n\\n'https:\\/\\/youtu.be\\/yM6nGk_qGHk'\\n\\n'https:\\/\\/youtu.be\\/jYbjGKG4c-U'\",\"384\":null,\"385\":null,\"386\":\"'The custom Cartesian robot was comprised from ballscrew-driven Parker\\u2122 404XR series linear stages with 200 mm stroke. Each stage was actuated using a 90 Watt brushed DC motor (Maxon\\u2122 RE35 #273754) equipped with a 500 counts per revolution encoder (Maxon\\u2122 HEDL #110512). A computed torque controller was tuned and verified to provide motion accuracy of better than\\\\n30\\u03bcm\\\\nin each direction.'\",\"387\":null,\"388\":null,\"389\":\"'https:\\/\\/bitbucket.org\\/opticalpcf\\/'\",\"390\":\"'Autoencoders are a type of artificial neural network that learns representations, or codings, of data by compressing data into a lower-dimensional space and then uncompressing the data to match the original input as closely as possible [16]. Because they try to reduce the error between the input and the reconstructed output, they learn the coding in an unsupervised manner and are task independent. Autoencoders have been shown to generate robust features [17].'\\n\\n'The minimization can be performed for a predefined number of iterations or until the reconstruction error reaches a predefined threshold. Once a dictionary is learned, it can be used to compute sparse code representations of new observations. These codes can be used as is or pooled to create more abstract features.'\\n\\n'To extract features from the sequences of tactile images, the sparse codes are spatially and then temporally max pooled. To perform spatial pooling, the tactile image is split into spatial cells\\\\nC\\\\ns\\\\n, each containing a number of patches. To form the feature vector for a cell, the sparse codes representing each patch in a single cell,\\\\n{\\\\nx\\\\ni\\\\n|i\\u2208\\\\nC\\\\ns\\\\n}\\\\n, are max pooled over each component\\\\nx\\\\nm\\\\ni\\\\n, where\\\\nx\\\\nm\\\\ni\\\\nis the\\\\nm\\u2212\\\\nth component of\\\\nx\\\\ni\\\\n. This pooling is done for all cells at varying scales, and the feature vectors for each cell are concatenated. A similar process is performed for temporal max pooling, where the feature vectors from tactile images within a temporal cell\\\\nC\\\\nt\\\\nare max pooled over each component. This pooling is also done at varying scales, resulting in a single feature vector for each tactile sequence.'\\n\\n'To extract features from a tactile sequence, we temporally max pool the sparse codes. A sequence is first split into temporal cells of multiple sizes. The sparse codes that represent the observations contained in or overlapping each cell are max pooled. Finally, the aggregated sparse codes from each cell are concatenated to form the feature vector for a single sequence. In our case, almost all of the sequences were partitioned into 16, 8, 4, 2, and 1 cells for a total of 31 temporal cells. Fast Slidesequences from\\\\nP\\\\nDC\\\\n,\\\\nT\\\\nAC\\\\n, and\\\\nT\\\\nDC\\\\nwere only long enough to split into 8, 4, 2, and 1 cells.'\",\"391\":\"'Contact force distribution. The deformation of the sensor skin under contact encodes the spatial distribution of internal contact forces. These are key for precise object manipulation [5], [6].'\",\"392\":null,\"393\":null,\"394\":\"'Graphs fulfill both requirements. Domain-specific knowledge can be encoded in separate graphs and then be composed to an application by adding the application specific relations between the different graphs.'\",\"395\":null,\"396\":null,\"397\":null,\"398\":null,\"399\":null,\"400\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/32368361'\",\"401\":null,\"402\":null,\"403\":null,\"404\":null,\"405\":null,\"406\":null,\"407\":\"'A workflow of the control algorithm encoded in the HIL is depicted in Figure 5. Signals from the RVDTs and the load cell are first pre-processed in order to obtain the sidestick pitch and roll angles'\",\"408\":null,\"409\":null,\"410\":null,\"411\":null,\"412\":null,\"413\":\"'As there was no open-source version of CNN-SLAM available at the time of writing, we evaluate DeepFusion on the same sequences used in [27] and compare with their reported results. The sequences used for the comparison come from two different datasets: the synthetic ICL-NUIM RGB-D dataset [35] and the real world TUM RGB-D SLAM dataset [36]. The ICL-NUIM dataset provides rendered depth maps as a ground truth comparison and the TUM RGB-D dataset approximates this with Kinect depth camera images. As proposed by [27], we measure the percentage of estimated depth values that are within 10% of the corresponding ground truth depth values in order to evaluate both the reconstruction accuracy and density.'\",\"414\":\"'https:\\/\\/bitbucket.org\\/vguizilini\\/cvpp'\",\"415\":null,\"416\":null,\"417\":null,\"418\":\"'In order to estimate the pose and velocity of the vehicle and enable its automatic control, the following sensors were used: An Inertial Measurement Unit (IMU) with integrated 3 axes accelerometer and gyroscope to measure the accelerations and angular velocities that are acting on the system, a magnetometer to measure the magnetic flux, a GNSS module for position updates, a barometer to assess the atmospheric pressure and a differential pressure sensor to determine the airspeed. All the information gathered from the sensors is then fused to estimate the pose and velocity using the Extended Kalman Filter (EKF) of the PX4 open source software stack [11].'\",\"419\":null,\"420\":null,\"421\":null,\"422\":null,\"423\":null,\"424\":null,\"425\":null,\"426\":null,\"427\":\"\\\"In this work, we present a novel framework for on-line human gait stability prediction of the elderly users of an intelligent robotic rollator using Long Short Term Memory (LSTM) networks, fusing multimodal RGB-D and Laser Range Finder (LRF) data from non-wearable sensors. A Deep Learning (DL) based approach is used for the upper body pose estimation. The detected pose is used for estimating the body Center of Mass (CoM) using Unscented Kalman Filter (UKF). An Augmented Gait State Estimation framework exploits the LRF data to estimate the legs' positions and the respective gait phase. These estimates are the inputs of an encoder-decoder sequence to sequence model which predicts the gait stability state as Safe or Fall Risk walking. It is validated with data from real patients, by exploring different network architectures, hyperparameter settings and by comparing the proposed method with other baselines. The presented LSTM-based human gait stability predictor is shown to provide robust predictions of the human stability state, and thus has the potential to be integrated into a general user-adaptive control architecture as a fall-risk alarm.\\\"\\n\\n'Our contributions presented in this work is the design of a novel deep-based framework for on-line human gait stability state prediction using the detection of the upper body 3D pose and the respective CoM estimation from an RGB-D camera and the human gait states estimated from LRF data. Differing from the common gait stability analysis methods in literature, we propose an LSTM-based network for fusing the multi-modal information, in order to decode the hidden interaction of the body\\u2019s CoM with the legs motion and the gait phases, in order to predict the gait stability of the elderly considering two possible classes: safe and fall-risk walking. The proposed on-line LSTM-based human gait stability predictor is evaluated using multi-modal data from real patients. To justify the model selection, we present an exploratory study regarding the network architecture, the selected hyperparameters and compare the performance of our framework with baseline methods. The results demonstrate the great efficiency of the LSTM-based approach to provide robust predictions of the human stability state, and show its potential to be integrated into a general user-adaptive control architecture as a fall-risk alarm.'\",\"428\":null,\"429\":null,\"430\":\"'https:\\/\\/wenhao.pub\\/publication\\/trajectory-supplement;.pdf'\\n\\n'Generating multi-vehicle trajectories from existing limited data can provide rich resources for autonomous vehicle development and testing. This paper introduces a multi-vehicle trajectory generator (MTG) that can encode multi-vehicle interaction scenarios (called driving encounters) into an interpretable representation from which new driving encounter scenarios are generated by sampling. The MTG consists of a bi-directional encoder and a multi-branch decoder. A new disentanglement metric is then developed for model analyses and comparisons in terms of model robustness and the independence of the latent codes. Comparison of our proposed MTG with \\u03b2-VAE and InfoGAN demonstrates that the MTG has stronger capability to purposely generate rational vehicle-to-vehicle encounters through operating the disentangled latent codes. Thus the MTG could provide more data for engineers and researchers to develop testing and evaluation scenarios for autonomous vehicles.'\\n\\n'Introducing a deep generative model that uses latent codes to characterize the dynamic interaction of multi-vehicle trajectories.'\\n\\n'Scheme of multi-vehicle trajectory generator, consisting of three parts: encoder (green), sampling process (purple) and decoder (red).'\\n\\n'In order to to explore the benefits of the modified structure, Baseline 1 is developed using a single-directional GRU encoder of\\\\n\\u03b2\\\\n-VAE. The encoder processes multiple sequences simultaneously with one GRU module, and the outputs (\\\\n\\u03bc\\\\nand\\\\n\\u03c3\\\\n) of the encoder are re-sampled through the reparameterization process [8]. The process is formulated as:'\\n\\n'Compared to the Baseline 1, the MTG has two improvements. First, the bi-directional counterparts replace the single-directional GRU module, which enables the encoder to extract deeper representations from the trajectories, because the traffic trajectories are still practically reasonable after being reversed in the time domain. The pipeline of the encoder of MTG is formulated as:'\\n\\n'Second, we separate the decoder into multiple branches and share the hidden states among them. In this way, the hidden state retains all the information over past positions and provides guidance to generate the other sequence. We note that generating two sequences independently avoids mutual influence. The pipeline of the decoder of MTG is formulated as:'\\n\\n'Encoder'\\n\\n' Decoder '\\n\\n'Comparison of two evaluation metrics on Autoencoder and VAE.'\\n\\n'Fig. 3 shows the generated trajectories from the two baselines and our MTG. Each row shows the variation of one latent code with all others fixed. For the InfoGAN baseline, the last three rows are almost the same, i.e., the codes do not affect any features. This can be explained by the unstable training of InfoGAN. Generating trajectories capable of deceiving the discriminator makes the generator difficult to obtain diversity, since the generator tends to generate similar trajectories that are more likely to mislead the discriminator. As a contrast, the VAE baseline and MTG obtain more diverse trajectories.'\\n\\n'Table I lists some \\u2018zoom-in\\u2019 figures for more detailed analysis of the generated trajectories of MTG. We connect the associated points in the two sequences, from the starting point (green) to the end point (red), with lines. In each row, the four figures derive from four different values of one latent code with a continuous change from left to right. These trajectories indicate that MTG is able to control some properties of generated trajectories (e.g., the location where two vehicles meet and their directions) through the latent codes.'\\n\\n'Developing safety policies and best practices for autonomous vehicle deployment and low-cost self-driving applications will always depend on the data provided by the high-quality generation of multi-vehicle and pedestrian encounters. This paper proposed a novel method to generate the multi-vehicles trajectories by using publicly available data. Toward the end of extracting the features of spatiotemporal sequences, a separate generator architecture with shared information was implemented. A new disentanglement metric capable of comprehensively analyzing the generated trajectories and model robustness was also proposed. An evaluation of traffic rationality using the proposed disentanglement metric found that the MTG obtained more stable latent codes and generated high-quality trajectories.'\",\"431\":null,\"432\":\"'https:\\/\\/github.com\\/crslab\\/%20TactileLearning'\\n\\n'Within our comparison framework, three models were developed and compared from a performance standpoint; the connectionist model that uses both the CNN and LSTM achieved a high accuracy of 98% on our dataset of 23 textures. Further analysis revealed that only a short sliding motion was required to isolate the correct texture class. We have made our tactile dataset (comprising tactile sensor readings, encoder data, and force estimates) publicly available online at https:\\/\\/github.com\\/crslab\\/ TactileLearning .'\\n\\n'The iCub is an open source humanoid robot with tendon based actuation and tactile skin on its parts. It contains 18 patches of tactile sensors on its hand, forearm, upper arm and torso. A patch is made of triangular modules, and each module consists of 10 taxels. Each taxel is a capacitive sensor; the dielectric deforms when pressure is applied [32] . Note that the distances between taxels are unequal.'\\n\\n'https:\\/\\/github.com\\/crslab\\/ TactileLearning'\",\"433\":\"'We conduct several experiments to measure the properties of the cross-modal data generation models for generating the desired output. We trained the visual-to-tactile and tactile-to-visual networks to generate tactile and visual images against a number of materials with different fabric properties, and altering the networks internal parameters and input. Outputs are evaluated against a Colour-SSIM metric comparing the generated artificial image against the real dataset. Our code is implemented in Python using the Tensorflow framework 1 and all experiments are computed using the University of Liverpool GPU server running on 3\\u00d7Nvidia GeForce GTX 1080 Titan GPUs.'\",\"434\":null,\"435\":\"'https:\\/\\/git;hub.com\\/Kevin315\\/Pisa-Kings_Tactile_Sensing.git'\",\"436\":null,\"437\":\"'Co-speech gestures enhance interaction experiences between humans as well as between humans and robots. Most existing robots use rule-based speech-gesture association, but this requires human labor and prior knowledge of experts to be implemented. We present a learning-based co-speech gesture generation that is learned from 52 h of TED talks. The proposed end-to-end neural network model consists of an encoder for speech text understanding and a decoder to generate a sequence of gestures. The model successfully produces various gestures including iconic, metaphoric, deictic, and beat gestures. In a subjective evaluation, participants reported that the gestures were human-like and matched the speech content. We also demonstrate a co-speech gesture with a NAO robot working in real time.'\\n\\n'In the present study, a speech text is represented as a sequence of words, and each word is encoded as a one-hot vector that indicates the word index in a dictionary. One-hot vectors are high-dimensional and sparse, so it is typical to convert them to compact representations, known as word embedding. In the space of word embedding, words of similar meaning have similar representations, so understanding natural language is easier. We used the pretrained word embedding model GloVe, trained on the Common Crawl corpus [21]. The dimension of word embedding is 300, and a zero vector is used for unknown words.'\\n\\n'Proposed network architecture. The encoder GRU interprets s speech words, and the decoder GRU generates m human poses of gestures. The decoder GRU inputs n previous poses to make the series of poses continuous. The soft attention mechanism is used but not depicted here.'\\n\\n'Training a recurrent neural network on long sequences of more than hundreds of steps is usually not feasible owing to gradient vanishing or exploding. Therefore, we designed the network to generate a limited number of motion steps. For a long speech text, the network was inferred multiple times and the resulting sequences were concatenated. The decoder inputs n previous poses and generates m successive poses; this configuration makes the output poses of multiple inferences smooth. The parameters n and m were fixed to 10 and 20, respectively, in all experiments.'\",\"438\":\"'The questionnaire data was coded using the following details. The friendliness questions were coded from 1 to 5, with 5 being strongly agree and 1 being strongly disagree. In addition, for the freeform questions four and six, we counted the number of participants who agreed, disagreed or partially agreed with each statement.'\",\"439\":null,\"440\":null,\"441\":null,\"442\":null,\"443\":\"'The SEG-VoxelNet contains three components, the SEG-Net, alignment module, and the improved-VoxelNet. The SEG-Net is a FCN that outputs semantic segmentation map, whose channel represents the probability of a pixel belonging to the category. Alignment module projects each 3D point into the probability map, and the improved-VoxelNet encodes the semantic probability of 3D point with its geometrical information together, and learns to generate accurate 3D bounding boxes. Fig. 2 is an overview of the SEG-VoxelNet architecture, we will introduce each module in the following in details.'\\n\\n'The SCSE module recalibrates the feature maps adaptively to enhance informative features while reducing the non-informative ones when embedding the inherent multi-scale context or encodes features of different adjacent stages. Within the SCSE, the channel squeeze block tunes to emphasize the important information which is effective for classification when squeezing the feature map along channels. The Spatial Excitation Block ignores irrelevant spatial location and recognizes the relevant as the important ones.'\\n\\n'The Receptive Field Block (RFB) explicitly adjusts filter\\u2019s field-of-view and encodes affluent spatial information by atrous or dilated convolution. RFB is constructed by combining multiple branches with different kernel sizes and different rates, as well as diverse dilated convolution layers, followed by a concatenating\\\\n1\\u00d71\\\\nconvolution. This module preserves multi-scale spatial information with strongest consistency.'\",\"444\":\"'Multi-modal features can be learned unsupervised using Autoencoder (AE) architectures as proposed in [17] and [5]. During training the AE is exposed to all possible input combinations including modality losses. Thus, it is able to overcome modality losses. Compared to ours, these works are trained on data containing modality losses which our classifier is explicitly not. AE architectures without specialized feature extractions for each modality use fully connected neural networks for processing each input separately [18], [5]. Early fusion, late fusion and temporal fusion strategies for multi-modal input are evaluated in means of accuracy for a gesture detection application in [18], which leads to similar accuracies for all fusion approaches. This finding is in line with suggestions in [23] and [1] that there is no clear preferable strategy yet when to fuse modalities. A late fusion scheme with fusing modality specific features for video and audio input is presented [26] to compute interestingness for video streams. Similar modality specific features are also applied for semantic segmentation tasks as in [28]. Here, late fusion weights are computed for each class based on early feature layers and so they are able to adapt to different challenging conditions. Whereas the class weights are learned in a data driven approach in [28], authors in [4] propose a late fusion based on a statistical fusion approach taking the performance of each single modality classifier into account. However, compared to our method, these approaches trained their classifiers with containing known challenging conditions already in their training set. Except [17] and [5] whose architectures are trained on modality losses none of the other architectures reports overcoming total sensor failures.'\\n\\n'A. Image Encoder \\/ Decoder'\\n\\n'The architecture for unsupervised feature extraction is visualized. Processing blocks for image encoding and decoding are shown in green. Within the Image Encoder image features are extracted following [12]. The processing blocks for point cloud encoding and decoding are visualized in blue. For extracting point cloud features we utilized the PointNet [21] architecture within the Point Cloud Encoder. The Sensor Drop Network following [16] supports the network in becoming invariant to sensor losses and is shown in red.'\\n\\n'For decoding the stacked feature code is splitted in point cloud features and image features again. The code representing image features is processed through a CNN using convolutional filters and transposed convolutions. The code representing point cloud features is first reshaped to two dimensional structure of shape 48x32x1. Afterwards convolutional layers and transposed convolutions are applied to the point cloud features. Once both features are processed by individual CNNs the output is concatenated and a convolution is applied on the fused output. The mean squared error loss on pixel values is used for comparing the generated output image to the gray scale version of the input image.'\\n\\n'B. Point Cloud Encoder \\/ Decoder'\\n\\n'Similar to the image decoder the code gets first splitted into point cloud features and image features within the decoder. Both features are separately processed by a fully connected network inspired by the decoder presented in [12]. These networks consist of parallel applied fully connected layers which are then summed up. Finally the output of the image feature processing and the point cloud feature processing is concatenated and further processed by a multi layer perceptron (MLP) to fuse both processing streams. In comparison to [12] no short cuts between encoder and decoder are used because the main objective is learning descriptive features. The loss for comparing the generated output point cloud to the input point-cloud is based on the Chamfer distance as suggested in [12] and [30].'\\n\\n'Weights within the encoders get fixed after the unsupervised training step. The now extracted features are forwarded to classification module highlighted with a dashed orange rectangle in Fig. 2. The classifier follows the idea of splitting the code into image and point cloud features like both decoders do. Both features are processed by a fully connected architecture separately and single classifiers are trained for each modality in a pretraining step. Furthermore, a mean feature representation is stored and updated for each single class per modality during the classifier training. Afterwards, a late fusion module scales and adds up outputs from the single modality classifier. The late fusion classifier is trained following the sensor drop strategy with the intention to overcome modelity losses. However, empirical results (see Tab. II and Fig. 11) show that the late fusion classifier works best if both modalities produce good results or both modalities have weaknesses. Compared to uni-modal classifiers the accuracy of the late fusion classifier also drops significantly if one of the modalities is noised.'\\n\\n'We also show the ability of the decoders to reconstruct full object point clouds computed from images and depth sensor input. For this evaluation the same training and validation data set as in [12] is used. Within Fig. 4 the reconstruction of a car is visualized using both modalities as input as well as only image and only point cloud features when enabling the sensor drop switch.'\",\"445\":\"'Earlier work demonstrates the promise of deep-learning-based approaches for point cloud segmentation; however, these approaches need to be improved to be practically useful. To this end, we introduce a new model SqueezeSegV2. With an improved model structure, SqueezeSetV2 is more robust against dropout noises in LiDAR point cloud and therefore achieves significant accuracy improvement. Training models for point cloud segmentation requires large amounts of labeled data, which is expensive to obtain. To sidestep the cost of data collection and annotation, simulators such as GTA-V can be used to create unlimited amounts of labeled, synthetic data. However, due to domain shift, models trained on synthetic data often do not generalize well to the real world. Existing domain-adaptation methods mainly focus on images and most of them cannot be directly applied to point clouds. We address this problem with a domain-adaptation training pipeline consisting of three major components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model exhibits segmentation accuracy improvements of 6.0-8.6% over the original SqueezeSeg. When training our new model on synthetic data using the proposed domain adaptation pipeline, we nearly double test accuracy on real-world data, from 29.0% to 57.4%. Our source code and synthetic dataset are open sourced. https:\\/\\/github.com\\/xuanyuzhou98\\/SqueezeSegV2.'\\n\\n'The contributions of this paper are threefold: 1) We improve the model structure of SqueezeSeg with CAM to increase its robustness to dropout noise, which leads to significant accuracy improvements of 6.0% to 8.6% for different categories. We name the new model SqueezeSegV2. 2) While most of the domain adaptation method focuses on images, we propose a domain-adaptation training pipeline for LiDAR point cloud that significantly reduces the distribution gap between synthetic data and real data. Model trained on synthetic data achieves 28.4% accuracy improvement on the real test data. 3) We create a large-scale 3D LiDAR point cloud dataset, GTA-LiDAR, which consists of 100,000 samples of synthetic point cloud augmented with rendered intensity. The source code and dataset are open-sourced.'\",\"446\":null,\"447\":\"'The generation of RGBD data is realized by a specialized virtual camera in the simulated environment that generates RGB, object mask and depth data. The rendering code of this camera is based on [6] . It has been extended to support the BSON encoding of the sensor data, the inclusion of sensor noise models and the necessary integration to use it within ROS including the underlying coordinate frames and camera parameters.'\",\"448\":null,\"449\":null,\"450\":\"'Three servomotors (DCX22L, Maxon Inc.) with the GPX-22 gearheads (gear ratio of 21: 1) and the ENX16 encoders (512 Counts per Turn), are used to drive the leadscrews and the linear module. The servomotors are controlled by three Maxon EPOS224\\/2 digital controllers. An industrial microcomputer with a Celeron J19004-core CPU serves as the central controller of the system. It communicates with the EPOS2 by CAN buses through the USB-CAN converter (CANalyst-II, Chuangxin Tech. Inc., Chin a).'\",\"451\":\"'Our next step is to develop another iteration of the segment that addresses the flaws identified above, and also incorporates a universal interface for connecting segments to each other or to end effectors, providing an air tight seal for the interface of the central bore and connections for power supply lines to proximal sections. To further enhance the modularity, the embedding of a micro-controller and battery will also be investigated. The next iteration will use a revised gearbox system, eliminating the bending load on the motor shaft causing the gearbox to strip, also equipped with encoders to enable closed-loop feedback control. After finalising the segment design, we will assemble a full manipulator system using several such segments and evaluate its performance and their dynamic characteristics.'\",\"452\":null,\"453\":null,\"454\":null,\"455\":null,\"456\":\"'https:\\/\\/youtu.be\\/sSSQgnmjmJw'\",\"457\":null,\"458\":null,\"459\":\"'The following provides an overview of Salty, including language features; sanity checking, debugging, and optimization capabilities; and code generation. The syntax used for logic operators is fairly standard, including a\\u2019 to denote \\u25efa, a ! to denote\\\\n\\u00aca\\\\n, a -> b to denote\\\\na\\u2192b\\\\n, and so on.'\\n\\n'Salty can produce controller implementations for realizable specifications in three different languages: Python, Java, and C++. Each code generator produces a class that, when instantiated, provides a move method that transitions the controller given a list of input values from the environment. Additionally, all of the user-defined types used in input and output variables are translated into enumerations in the host language, preserving user intent from the Salty specification.'\\n\\n'This workflow relies on a naming convention for inputs and outputs. The naming convention is varname ==([a-zA-z0-9]+) ([0-9]+)*, where the first group [a-zA-z0-9]+ becomes the name of a function, and subsequent groups _[0-9] are the IDs of the UAVs or UGVs whose low-level, continuous states will be passed into the function. The semi-automated workflow includes a Python script that generates a Python controller interface. At every simulation time step, this interface gets UAV state data from AMASE, evaluates the input functions, calls the Salty-synthesized controller with a list of current inputs, and evaluates the resulting output functions, which make calls to UxAS. The Python script also generates Python function declarations for inputs and outputs, and the user fills in the logic with the help of provided auxiliary classes for capturing UAV state data. For instance, this script generates a file inRange. py corresponding to input inRange that includes a function definition def inRange (vehicle_State1, vehicle -State2), where vehicle state data is passed in by the controller interface. The user codes the logic for the function in a vehicle agnostic way, in this case checking whether two vehicles are within some distance threshold. The controller interface calls this function twice each simulation step, once with state data for vehicle\\/UAV 1 and vehicle\\/UGV 3 and once with vehicle\\/UAV 2 and vehicle\\/UGV 3, since the names of the corresponding inputs in the specification are inRange -and 1nRange_2_3. As another example, the script generates a file Track. py for output Crack that includes a function definition def track (vehicle_State1). In this case, the user provides logic to set up an \\u201cautomation request\\u201d to UxAS to have the vehicle track a target. The controller interface calls this function with the vehicle state data for UAV 1 whenever track-l is true or with the vehicle state data for UAV 2 whenever Crack_2 evaluates to true. Note that due to implementation issues with UxAS, this particular function is hard coded to track UGV 3, and it does not require state data for UGV 3 to set up the call to UxAS.'\\n\\n'C. Code Generation'\",\"460\":\"'Applying this algorithm to each waypoint, we derive the shortest distance trajectories between waypoints. Given that we have encoded the informative value of waypoints in W, we optimize utility by maximizing the number of waypoints addressed per route. This is preferable to heavily investing in computing informative paths at this point, as many paths between waypoints will not contribute to the final solution.'\",\"461\":\"'https: \\/\\/sites.google.com\\/view\\/fogrobotics'\\n\\n'The term \\u2018Cloud Robotics\\u2019 describes robots or automation systems that rely on either data or code from the Cloud, i.e. where not all sensing, computation, and memory is integrated into a single standalone system [1], [2]. By moving the computational and storage resources to the remote datacenters, Cloud Robotics facilitates sharing of data across applications and users, while reducing the size and the cost of the onboard hardware. Examples of Cloud Robotics platforms include RoboEarth [3], KnowRob [4], RoboBrain [5], Dex-Net as a Service [6], [7]. Recently, Amazon RoboMaker [8] and Google Cloud Robotics [9] released platforms to develop robotic applications in simulation with their Cloud services.'\",\"462\":null,\"463\":null,\"464\":null,\"465\":\"'http:\\/\\/www.subcultron.eu\\/'\\n\\n'The procedure for starting both types of depth controllers is the same. The aMussel begins with maximal volume (piston is completely outside the aMussel), and the aMussel floats at the surface. Motors start with maximal speed until the piston reaches a predefined value that should correspond to zero buoyancy determined by the encoder count. In this moment the controller is activated.'\",\"466\":\"'https:\\/\\/youtu.be\\/InHvNlPHUaA'\",\"467\":null,\"468\":\"'To validate the use of motion as a communication technique, we conducted a study using simulated videos of the Aqua AUV to test the accuracy, efficiency, and ease of learning provided by such a system. Twenty-four participants tested the system against a baseline system comprised of colored lights flashing in codes, and the resultant data was analyzed to determine whether motion-based communication could be adequately accurate, efficient, and easy to learn for use in underwater robot-to-human communication.'\\n\\n'We show that there is a statistically significant improvement in the accuracy of communication when using kinemes compared to light codes.'\\n\\n'We show that kinemes outperform light codes in ease of learning and that given enough education, they can be nearly as quick to understand as light codes.'\\n\\n'Emblems in nonverbal, non-facial HRI on non-humanoid platforms should be body movements which code directly to some linguistic meaning. This is the area to which our kineme communication system belongs, though it has little company here. The previously mentioned case study by DeMarco et al. [16] is one of the few attempts to communicate information rather than emotion using nonverbal methods, in this case via changes in the illumination of a light. Another example of emblems in this type of communication is the up-and-down tilt of a pan-tilt camera being used as a nod in [24], in which the robot simulates head gestures by controlling its camera\\u2019s pan and tilt.'\\n\\n'In this section, we introduce the design and implementation of our kineme communication system using robot motion and the light codes system to which it is compared.'\\n\\n'While the light system was designed as a baseline to compare to the kineme system, care was taken to make the light system as robust as possible. To be used as a baseline system, the same meanings for the kineme system were selected. To guide the development of the light codes, a number of principles were used, based on human perception of color [35] and blink frequency [36].'\\n\\n'For related information, share a portion of the light code (i.e., both battery info light codes have a single solid yellow light).'\\n\\n'The list of light codes can also be found in Table I, as well as in the video submitted with this work.'\\n\\n'The light codes were chosen after the development of the kinemes and were implemented using an Arduino controlling 3 LEDs of each color. Blink frequencies were selected subjectively, with a duration of five seconds on for solid lights, 1 Hz blinking for five seconds for slow blink rates, and 5 Hz blinking for four seconds for fast blink rates.'\\n\\n'The hypotheses we wish to test are simple: the accuracy of kinemes will be higher than that of light codes at all education levels (see Section IV-C) and the operational accuracy (accuracy of answers with confidence \\u22653 on a scale of 1 to 5) of kinemes will be higher than light codes at all education levels. We also hypothesize that the confidence of participants will be higher with kinemes than with light codes and that the time-to-answer for kinemes will be significantly longer than light codes at low education, but eventually fall to approximately the same time as the education of participants increases.'\\n\\n'Participants were split into three groups, based on the amount of preparation for the communication task they would receive. We designate these groups EDU0, EDU1, and EDU2, each with 8 participants. Within each group, the order in which the systems were displayed was alternated, so that half would see the kinemes first and half would see the light codes first.'\\n\\n'EDU2: Participants were told the communication vector and shown videos of each kineme and light code while being told the meaning.'\\n\\n'Education was offered directly before testing each system. Once a participant was oriented and educated to a system, they were shown videos of the kinemes or light codes in a random order. The random order of the videos limits the order dependencies of the kinemes and light codes and produces independent measurements of each kineme.'\\n\\n'Accuracy \\u2013 The accuracy of a participant\\u2019s understanding of a kineme or light code, rated from 0 to 10 in order of increasing accuracy.'\\n\\n'Confidence \\u2013 The confidence a participant has in their understanding of a kineme or light code, rated from 1 to 5, in order of increasing confidence.'\\n\\n'Time To Answer \\u2013 The time it takes a participant to give the meaning of a kineme or light code, measured in seconds from the beginning of the signal to the beginning of their answer.'\\n\\n'We compare kineme and light code accuracies at each education level, using a right-tailed Mann-Whitney test with this hypothesis:'\\n\\n'When testing accuracy, we find statistically significant increases in accuracy when comparing kinemes to light codes for EDU0 (p = 0. 0113, z = 2.282), EDU1 (p = 0. 0009, z = 3.114), and EDU2 (p = 0. 000009, 4.27). For operational accuracy, we again find statistically significant increases for EDU0 (p = 0. 029, z = 1.890), EDU1 (p = 0. 007, z = 2.418), and EDU2 (p = 0. 0006, z = 3.221). In all of these cases, we can reject the null hypothesis in favor of the alternative. We can also see this visually in the plots of these statistics in Figures 3 and 4.'\\n\\n'We also test the median of confidence participants reported in their answers, and here we find that while there is no statistically significant increase in confidence in kinemes vs light codes at EDU0 (p = 0. 156, z = 1.01), there is a statistically significant increase present at EDU1 (p = 0. 006, z = 2.496) and EDU2 (p = 0. 0004, z = 3.297). Finally, when considering the time to answer, we must slightly reformulate our test to be a left tailed test, testing the null hypothesis that the median time to answer for light codes is not lower than for kinemes, with the alternative being that the median for lights is lower. Here, we show that there is a statistically significant reduction in time when comparing lights to kinemes for EDU0 (p = 0. 0000002, z = \\u20135.031) and EDU1 (p = 0. 0003, z = \\u2013 3.39), but at EDU2 (p = 0. 4074, z = \\u20130.828) there is no significant reduction. This indicates that time to answer for kinemes drops with higher education, while time to answer for light codes remains approximately the entire length of the light code at all levels. These trends can be seen in Figures 5 and 6.'\\n\\n'TABLE I Kinemes and Light Codes with Their Associated Meanings'\\n\\n'Experimental platforms for Kineme and Light Codes.'\\n\\n'3) Comparison Between Specific Kinemes and Codes'\",\"469\":null,\"470\":\"'https:\\/\\/youtu.be\\/QwJZdiZlE8g'\",\"471\":null,\"472\":null,\"473\":null,\"474\":null,\"475\":\"'Results presented in this section have been generated on a 2.2 GHz Intel i5-5200U. With non-optimized code and a downscaled image resolution of 94 \\u00d7 60 running in a single thread, calculations currently take 125ms per frame. This calculation time scales linearly with the image resolution.'\\n\\n'Motion estimation over non-flat scene: The 3D object (box) is correctly reconstructed within our combined approach to simultaneously estimate the system motion and scene geometry in the same state vector. Left: camera image, middle left: estimated depth color coded, middle right: depth uncertainty, right: top view of reconstructed point cloud. (real-world data)'\\n\\n'Vision based distance measurement system using two-dimensional barcode for mobile robot'\",\"476\":\"'Our work is focused on running efficiently semantic segmentation tasks. CNNs for semantic segmentation typically follow an encoder-decoder structure: an encoder which learns features while reducing the resolution and a decoder which upsamples the learned features and maps them into the segmentation result. Recent works towards efficient segmentation architectures inlcude Deeplab-v3 [2], [3] and ERFNet [27], that use atrous convolutions [33] to avoid the need to reduce much the input resolution. Many architectures targeting efficiency, e.g., ERFNet [27] and ENet [23], perform several consecutive early downsampling operations for a quick reduction of the input resolution and they have light decoders with very few parameters and layers.'\\n\\n'c) Decoder Block:'\",\"477\":null,\"478\":\"'One of the cross-validations used during the analysis is comparing the estimated camera motion with that measured by the horizontal axis encoder, to ensure agreement. At a specific slip, the actual horizontal motion of the camera is calculated to compare with the estimation of the output flow field on each gravity condition (see Table II ). The estimated error is usually under 5% and never over 11.4% ( Table III ).'\",\"479\":null,\"480\":null,\"481\":null,\"482\":null,\"483\":null,\"484\":null,\"485\":\"'The unique characteristic of our local model design is that we apply an extraction procedure to the encoded tensor of shape (100, 100, 64) to extract the (5, 5, 64) tensor centered at the current location of the agent. The reasons why we extract local information are:'\\n\\n'The global network also has the residual network to encode state images, and it shares all weights with the one in our local model, except for the very first initialization layer. To adjust the channel size of input tensors, the initialization layer in our residual network maps a tensor of shape\\\\n(N, N, M)\\\\nto\\\\n(N, N, 16)\\\\n. Due to the discrepancy in shapes between local and global states, the size of the layer is different. We obtain the global action\\\\na\\\\nG\\\\nt\\\\nas:'\\n\\n'Second, we considered when to apply our extraction procedure. Instead of waiting to extract a (5,5,64) tensor from the (100, 100, 64) encoded tensor, we extract a (5,5) patch from each image in our local state to obtain a(5,5,3) tensor, feed it into the residual network and receive another (5, 5, 64). We name this procedure as Initial Extraction to differentiate it from our original extraction procedure.'\",\"486\":\"\\\"The neural network for the actor model consists of 7 convolution layers to process the image, followed by a spatial softmax layer to extract 128 feature points. The coordinates of the feature points are then processed with 2 fully connected layers to produce the final representation of the input images, which is used to predict the parameters of the Gaussian mixture, and\\/or concatenated with the latent code to predict the log scale and translation for Real NVP's affine coupling layers.\\\"\",\"487\":null,\"488\":\"'https:\\/\\/wayve.ai\\/sim2real'\\n\\n'Model architecture for domain-transfer from a simulated domain to real-world imagery, jointly learning control and domain translation. The encoders\\\\nE\\\\nsim,real\\\\nmap input images from their respective domains to a latent space Z which is used for predicting vehicle controls\\\\nc\\\\n^\\\\n. This common latent space is learned through direct and cyclic losses as part of learning image-to-image translation, indicated conceptually in Figure 3 and in Section III-B.'\\n\\n'Ideally we wanted to encode the semantic content of the images within the latent space such that Z is independent of the domain from which an image came. We therefore applied a latent reconstruction loss\\\\nL\\\\nZrecon\\\\n, an L1 loss between the latent representation of an image Zd and the reconstruction of the latent representation after it was decoded to the other domain and then encoded once more,\\\\nZ\\\\nrecon\\\\nd\\\\n=\\\\nE\\\\nd\\u2032\\\\n(\\\\nG\\\\nd\\u2032\\\\n(\\\\nZ\\\\nd\\\\n))\\\\n.'\",\"489\":null,\"490\":null,\"491\":null,\"492\":null,\"493\":null,\"494\":null,\"495\":null,\"496\":null,\"497\":\"'To decode the dry-EEG signals efficiently in order to ensure effective teleoperation of the robot, we use our deep CNN architecture of [17]) (see reference for more details) for signal to object\\/motion label classification.'\\n\\n'In this work, we present a number of novel contributions spanning the use of variable SSVEP stimuli (pattern, size, shape) as an enabler to future telepresence BCI applications in a real-world natural environment. We integrate recent advances in the use of deep CNN architectures for both scene object detection and dry-EEG bio-signal decoding. Within this context, we develop a novel SSVEP interface to flicker the on-screen frequency of naturally occurring objects detected within the scene, as seen from the on-board camera of a teleoperated robot, and decode these dry-EEG brain-based bio-signals based on the frequency of the visual fixation detected to navigate the robot within the scene. Uniquely, we train and utilize a common CNN model (SCU, Figure 4) for use with SSVEP stimuli that vary in size, on-screen position and internal (pixel pattern) throughout the duration of the experiment, significantly advancing such decoding generality against prior work in the field [3], [20]. Our evaluation is presented in terms of accuracy and ITR, both on the a priori experimental training set used for the off-line training phase (via cross validation) and the online real-time teleoperated navigation of a humanoid robot through a natural indoor environment. The introduction of these highly novel and variable BCI SSVEP stimuli, based on scene object occurrence, demonstrates adaptable BCI-driven robot teleoperation within a natural environment (without scene markers and alike). Strong statistical classification performance is observed, comparable to and often exceeding those reported in the general BCI literature [10], despite the introduction of the serious challenges associated with variable SSVEP stimuli.'\",\"498\":\"'An illustration of the GPF in 2D. Grey ellipse: uncertainty of prior belief. Dark red ellipse: the uncertainty of posterior. Light red ellipse: uncertainty of the recovered position measurement. The color of a particle encodes its weight with darker color corresponds to a higher weight.'\\n\\n'This paper presents a novel geometric degeneration modeling method that encodes the sensitivity of measurements w.r.t. robot poses. We find an analogy between the force-closure characterization and our method, which helps to explain the physical meaning of the localizability. Additionally, it is shown that the LiDAR and the UWB ranging sensor are complementary in terms of localizability and the presented fusion method is demonstrated to allow for robust localization inside real geometrically degenerated tunnels.'\",\"499\":null,\"500\":null,\"501\":\"'Pseudocode 1 Synthetic data generation'\\n\\n'A summary of the steps followed to generate the synthetic images is given in Pseudocode 1. RDSL can use any available 3D modelling tools, such as Autodesk 3ds Max, Unreal Engine and Blender. It can also use any renderer.'\\n\\n'In this test, we used RDSL to generate 100 synthetic images for each logo in the FlickrLogos-32 data set. The logos were downloaded from TrademarkVision\\u2019s database through their API. The randomization procedure of RDSL is implemented as a script inside Autodesk 3ds Max 2018. We limit the type of objects (both objects of interest and distractor objects in Pseudocode 1) to simple geometry, such as cylinders, boxes, etc. and only use textures from the Autodesk libraries for generating the synthetic data. The scanline renderer was the primary rendering engine used. While the Autodesk Raytracer (ART) renderer is able to produce more realistic images and is compatible with a wider range of 3ds Max materials, it is significantly slower to render and did not produce a noticeable effect on the classifier performance.'\",\"502\":\"'In our method, we have adapted RRPN to our grasping rectangle detection task by means of transfer learning. We call this adapted network the Grasping Rectangle Proposal Network (GRPN). Although textboxes and grasping rectangles are two very different domains, our experiments show that the previously encoded knowledge in the network is very useful when adapting it to the grasping rectangle proposal domain.'\",\"503\":null,\"504\":\"'To show the difference between optoacoustic signals of these three materials, a classifying experiment is conducted using the open source BOSS classifier implemented by A. Bagnall et al. [25]. The classifier is trained to identify steel, rubber and acrylic, the ratio of testing data to training data is 1\\/3. The experimental data are transformed into BOSS histogram, serving as feature set for classification ( Fig. 15). Because the BOSS histogram serves as a feature set in the classifying process, the unique symbols that only appear in one specific material will be informative for classifier. After 50 random trials, BOSS classifier gives an average accuracy of 92.8% as shown in the confusion matrix in Fig. 16. Signals from acrylic and steel can be identified with accuracy > 95%, while the possibility to misrecognize rubber as acrylic is 12%. This preliminary result demonstrates the feasibility of differentiating dense solid materials by optoacoustics.'\",\"505\":\"'http:\\/\\/ait.ethz.ch\\/projects\\/2019\\/handcam'\",\"506\":null,\"507\":null,\"508\":\"'To generate the control for the robot, we encode an SMT formula over the following continuous variables and constants:'\\n\\n'Robot dynamics: We model the dynamics of the robot as a differential drive robot and encode it as:'\\n\\n'Human motion prediction: The generated control for the robot depends on the predicted pose of the human, which is encoded as part of the SMT formula:'\\n\\n'Desired behavior: We encode the desired robot behavior in additional predicates which include integer variables\\\\nb\\\\ni\\\\n\\u2208{0,1}\\\\n, emulating Boolean variables indicating whether a constraint is satisfied or not. If the value of bi is 1, the constraint is satisfied, while if it is 0, the constraint may or may not be satisfied for a valid solution of the SMT formula. If the SMT solver finds a solution where all bi are 1, then all the constraints are satisfied. If not, the bi are used to generate feedback, as described in Section II-C.'\\n\\n'We encode the distance constraints in Equation 4 with the integer variables\\\\nb\\\\nmin\\\\nand\\\\nb\\\\nmax\\\\n.'\",\"509\":null,\"510\":\"'http:\\/\\/tiny.cc\\/FOSAPT'\",\"511\":null,\"512\":null,\"513\":null,\"514\":null,\"515\":\"'The AIDER exoskeleton system: 1. The subject; 2. The crutches; 3. The embedded computer and IMU; 4. DC servo motors; 5.Angle encoders; 6. Smart shoes with plantar pressure sensors inside.'\",\"516\":\"\\\"Modeling individual-specific gait dynamics based on kinematic data could aid development of gait rehabilitation robotics by enabling robots to predict the user's gait kinematics with and without external inputs, such as mechanical or electrical perturbations. Here we address a current limitation of data-driven gait models, which do not yet predict human gait dynamics nor responses to perturbations. We used Switched Linear Dynamical Systems (SLDS) to model joint angle kinematic data from healthy individuals walking on a treadmill during normal gait and during gait perturbed by functional electrical stimulation (FES) to the ankle muscles. Our SLDS models were able to generate joint angle trajectories in each of four gait phases, as well as across an entire gait cycle, given initial conditions and gait phase information. Because the SLDS dynamics matrices encoded significant coupling across joints that differed across indivdiuals, we compared the SLDS predictions to that of a kinematic model, where the joint angles were independent. Joint angle trajectories generated by SLDS and kinematic models were similar over time horizons of a few milliseconds, but SLDS models provided better predictions of gait kinematics over time horizons of up to a second. We also demonstrated that SLDS models can infer and predict individual-specific responses to FES during swing phase. As such, SLDS models may be a promising approach for online estimation and control of and human gait dynamics, allowing robotic control strategies to be tailored to an individual's specific gait coordination patterns.\\\"\\n\\n'B. SLDS dynamics matrices encode joint coupling'\",\"517\":null,\"518\":null,\"519\":null,\"520\":null,\"521\":null,\"522\":\"'We encode region transition and gait selection behavior by appending the following LTL formulas to\\\\n\\u03c6\\\\ns\\\\nt\\\\n. Formulas encoding proposition mutual exclusivity are omitted for brevity.'\\n\\n'Region Transitions: Each gait propels the robot in a certain direction and allows the robot to make certain region transitions. We encode the region transition behavior in the following way:'\\n\\n'Gait Restrictions: For any inoperable actuator, the robot is restricted from selecting any gait that utilizes that actuator. These restrictions are encoded in the following way:'\\n\\n'We encode actuator degradation and replacement by appending the following LTL formulas to\\\\n\\u03c6\\\\ne\\\\nt\\\\n.'\\n\\n'Fig. 5 (left) depicts the example scenario we address in this work. The robot operates in a grid and is able to move \\u201cnorth,\\u201d \\u201csouth,\\u201d \\u201ceast,\\u201d or \\u201cwest\\u201d at each time step. The mission specification is a patrolling task - the robot must repeatedly visit two of the regions in the map, regions 11 and 12, depicted with yellow diamonds. This is encoded by appending the following to\\\\n\\u03c6\\\\ns\\\\ng\\\\n:'\\n\\n'In this example we show how one can encode more complex environment behaviors and tasks. We specify sets of regions of the workspace that contain hazards that are detecTable BY The robot and may be \\u201cactivated\\u201d or \\u201cdeactivated\\u201d by the environment. if the robot moves into an active hazard region, all of the actuators associated with the currently selected gait are immediately rendered inoperable regardless of health state.'\",\"523\":null,\"524\":null,\"525\":null,\"526\":null,\"527\":\"'CNN-SVO: Camera motion estimation in the high dynamic range (HDR) environment. (Left) The single-image depth prediction model from a CNN demonstrates the illumination invariance property in estimating depth maps, and the colour-coded reprojected map points on a sample sequence of five consecutive frames show the reprojected map points to those frames for camera motion estimation (best viewed in colour). Note that CNN-SVO only predicts depth maps for the keyframes. (Right) Camera trajectory (depicted with line) and map points in magenta generated by CNN-SVO.'\\n\\n'To provide depth prediction in the initialization of map points in CNN-SVO, we adopt the Resnet50 variant of the encoder-decoder architecture from [10] that has already been trained on Cityscape dataset. Next, we fine-tune the network on stereo images in KITTI raw data excluding KITTI Odometry Sequence 00-10 using original settings in [10] for 50 epochs. To produce consistent structural information, even on overexposed or underexposed images, the brightness of the images has been randomly adjusted throughout the training, creating the effect of illumination variation. This consideration is useful for a neural network to handle high dynamic range (HDR) environments (see Fig. 2).'\\n\\n'Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that improved SVO mapping results in increased robustness and camera tracking accuracy. The implementation of this work is available at https: \\/\\/github.com\\/yan99033\\/CNN-SVO'\",\"528\":\"'After the segmentation, we identified the moveable objects from all the instances in the result, according to a predefined shortlist in which only objects that are likely to move or be moved (such as person, cars, cup, chair, etc.) among all the 80 classes are selected. The result is in the form of a mask image with the region and instance ID of each segmented instance encoded, and will be sent to the vSLAM module to proceed the tracking and mapping computation.'\",\"529\":null,\"530\":null,\"531\":null,\"532\":null,\"533\":null,\"534\":null,\"535\":\"'The classifier has 1D binary output, which represents the feasibility of the transition, and the regressor has 7D continuous value outputs, which includes 1D objective, 3D CoM position, and 3D CoM velocity. The inputs of the neural networks are all the contact poses in the contact transition, and the initial CoM position and velocity, as same as the dynamics optimizer. To simplify the problem, we ignore CoM angular velocities in the input\\/output vectors, and encode the angular momentum in the objective function. We train separate neural network for each kind of contact transition using different end-effectors. Since most of the humanoid robots have symmetric kinematic structure, we further exploit this symmetry to define 10 categories of contact transition, and show its corresponding input dimensions in Figure 3.'\",\"536\":\"'We present a new framework to generate human-like lower-limb trajectories in periodic and non-periodic walking. In our method, walking dynamics is encoded in 3LP, a linea...'\\n\\n'We present a new framework to generate human-like lower-limb trajectories in periodic and non-periodic walking. In our method, walking dynamics is encoded in 3LP, a linear simplified model composed of three pendulums to simulate falling, swing, and torso balancing dynamics. To stabilize the motion, we use an optimal time-projecting controller which suggests new footstep locations. On top of gait generation and stabilization in the simplified space, we introduce a kinematic conversion that synthesizes more humanlike trajectories by combining geometric variables of the 3LP model adaptively. Without any tuning, numerical optimization or off-line data, our walking gaits are scalable with respect to body properties and gait parameters. We can change body mass and height, walking direction, speed, frequency, double support time, torso style, ground clearance, and terrain inclinations. We can also simulate constant external dragging forces or momentary perturbations. The proposed framework offers closed-form solutions with simulation speeds orders of magnitude faster than real time. This can be used for video games and animations on portable electronic devices with limited power. It also gives insights for generation of more human-like walking gaits on humanoid robots.'\\n\\n'In this work, we use a combination of physics-based and interpolation methods. The physics of walking in our method is encoded in the 3LP model which is composed of three linear pendulums to model falling, swing and torso balancing dynamics [11]. 3LP can produce walking at different speeds, frequencies, double-support times, torso bending styles, terrain inclinations and subject heights and weights. On top of this model, we use our previously developed controller called time-projection [12] to stabilize the gait and perform transitions. 3LP\\u2019s equations support external forces and torques applied to the torso while the time-projection control automatically captures these perturbations by adjusting footstep locations. Since the masses in 3LP are constrained in constant-height planes, we introduce a kinematic conversion to produce height variations. This part of our method involves adaptive trajectory synthesis without any tuning of parameters unlike the literature [13], [14]. Given a 3LP state (pelvis, torso and toe positions), our conversion adaptively varies the pelvis height to produce human-like excursions [15], lifts the swing toe to provide ground clearance [16] and resolves a single Degree of Freedom (DoF) in each leg to produce thigh-shank-foot kinematics.'\\n\\n'Using an intuitive mixture of geometric variables, we can produce smooth vertical excursions and human-like thigh-shank-foot coordinations. Although we do not simulate dynamics of these leg segments explicitly, each leg follows the approximate dynamics encoded by the 3LP model.'\",\"537\":null,\"538\":null,\"539\":null,\"540\":\"'https:\\/\\/www.youtube.com\\/watch?v=ZoNNjQfUdJw'\",\"541\":\"'https:\\/\\/github.com\\/utiasSTARS\\/PhoenixDrone'\\n\\n'Our goal is to enable researchers and educators to build high-performance tail-sitter vehicles easily, by leveraging online resources and taking advantage of modern digital manufacturing techniques. Towards this end, the Phoenix drone utilizes off-the-shelf actuators and computing hardware. The vehicle frame design incorporates a cast polyurethane foam core and 3D-printed plastic parts. We adopt the widely-used PX4 middleware to support our custom flight control software. We also include a MATLAB Simulink model and software-in-the-loop (SITL) Gazebo plugins to seamlessly compile and test flight code on the desktop.'\\n\\n'In this section, we briefly summarize the resources available in the open-source Phoenix drone repository. All of the code and the design documents can be found in our public GitHub repository located at https:\\/\\/github.com\\/utiasSTARS\\/PhoenixDrone. The overview below is not intended to be a comprehensive listing\\u2014more information is provided in the various repository README.md files.'\\n\\n'During development of the Phoenix, we found that STIL simulation was a crucial debugging tool. We have included the ROS Gazebo plug-ins required for SITL (software-in-the-loop) simulation using the PX4 firmware as part of our open-source package. SITL simulation allows end-users to verify their code behaviour in a virtual environment with realistic dynamics. Our customized Gazebo plug-ins simulate the dynamics of the vehicle based on a physical model extracted from real experimental data. An example view of the Gazebo GUI with the drone flying in simulation is shown in Figure 11.'\\n\\n'All of the associated design documents, schematics, and code have been released on GitHub under the permissive MIT licence. Our hope is that this release will encourage the aerial robotics community (researchers, educators, and hobbyists) to experiment, and to create innovative new modifications and derivatives. There are a number of possible use cases, ranging from preliminary research studies (in academia or industry) to teaching in a classroom environment (\\u201cBuild Your Own Drone!\\u201d). To the best of our knowledge, there is no existing package that provides such a comprehensive and complete set of materials for tail-sitter development (royalty-free and completely open for use and modification).'\\n\\n'In this paper, we introduce the Phoenix drone: the first completely open-source tail-sitter micro aerial vehicle (MAV) platform. The vehicle has a highly versatile, dual-...'\\n\\n'In this paper, we introduce the Phoenix drone: the first completely open-source tail-sitter micro aerial vehicle (MAV) platform. The vehicle has a highly versatile, dual-rotor design and is engineered to be low-cost and easily extensible\\/modifiable. Our open-source release includes all of the design documents, software resources, and simulation tools needed to build and fly a high-performance tail-sitter for research and educational purposes.The drone has been developed for precision flight with a high degree of control authority. Our design methodology included extensive testing and characterization of the aerodynamic properties of the vehicle. The platform incorporates many off-the-shelf components and 3D-printed parts, in order to keep the cost down. Nonetheless, the paper includes results from flight trials which demonstrate that the vehicle is capable of very stable hovering and accurate trajectory tracking.Our hope is that the open-source Phoenix reference design will be useful to both researchers and educators. In particular, the details in this paper and the available open-source materials should enable learners to gain an understanding of aerodynamics, flight control, state estimation, software design, and simulation, while experimenting with a unique aerial robot.'\\n\\n'The availability of open-source MAV software libraries and hardware designs (e.g., the PX4 autopilot [4]) have enabled researchers, educators, and hobbyists to quickly prototype and test aerial robots without the burden of starting from scratch. This has led to a thriving MAV development community that continues to grow. However, to date, the vast majority of the accessible reference designs and associated tools have been for quadrotor vehicles, which are relatively easy to build and fly. In contrast, there are very limited resources available for individuals who wish to assemble and test VTOL platforms.'\\n\\n'The Phoenix drone, our open-source dual-rotor tail-sitter vehicle, in hovering flight in our laboratory.'\\n\\n'The open-source flight stack and middleware which comes with PX4 has proven to be a very popular tool for enabling MAV research and for educational purposes. We note that other open-source robotics packages, such as the Duckietown [14] autonomous vehicle testbed, have been very successful in the education space. Resources to build the Crazyflie [15] nano-quadcopter are available in open-source form, and the recently-released PiDrone package [16] also provides an open-source implementation of an easy-to-build quadrotor capable of indoor flight. The success of these examples clearly shows that open-source reference designs, available for free, are highly valuable to the robotics community (and thus that further releases should be encouraged).'\\n\\n'Although the literature contains multiple examples describing the design and control of dual-rotor tail-sitter vehicles, to date, none of the designs have been released in open-source form. We reiterate that our goal is to provide a description of the design and development of a versatile tail-sitter and also to give researchers and educators a complete set of resources to simulate, build, and fly such a platform.'\\n\\n'The Phoenix drone is based upon the PX4 autopilot and uses the open-source PixRacer flight computer and PX4 middleware to support both flight control and SITL simulation. The PixRacer flight computer incorporates a 168 MHz ARM\\u00ae Cortex M4F microprocessor, which executes all control loops in real time. We use MAVROS, an open-source Robot Operating System (ROS) package, to communicate with the vehicle from our ground station (a laptop or desktop) over the MAVLink communication protocol.'\\n\\n'https:\\/\\/github.com\\/utiasSTARS\\/PhoenixDrone'\",\"542\":null,\"543\":null,\"544\":\"'Photo of experimental setup. 1) DC motor with angular incremental encoder, 2) Control unit, 3) Linear potentiometer 4) TSA, 5) Static load'\",\"545\":\"'The robot is controlled by an Arduino Mega microcontroller. The tendons are accurately actuated by directing the motors to specified encoder counts, and the linear actuators can be instructed to move specific distances due to the placement of ultrasonic distance sensors. The configuration of the entire robot, including the position of the tip, is modeled using a modified application of the constant curvature kinematics described in [21]. The main modification we made was ensuring that the arc length (section length) of the distal section is adjusted as the middle section is extended\\/retracted.'\",\"546\":null,\"547\":null,\"548\":null,\"549\":\"'GelSlim\\u2019s signal strength depends on the visual contrast between contact and non-contact scenarios. For example, when the gel is illuminated with grazing light, a contact region changes the surface normal to reflect more or less light back to the camera. Because of this, the illuminated side appears extra bright and the opposite side appears extra dark. In GelSlim 2.0, we increase signal contrast with dualcolor illumination relative to the white illumination used in the previous version. In this case, the gel pad is illuminated with red grazing light from the left and green grazing light from the right. These colors are easy to separate by isolating two of the camera\\u2019s three (RGB) channels. Therefore, contact information is encoded by differences between channels and the different shadows they cast on the contact surface. These particular LEDs (LUXEON CZ) are chosen because their spectral emissions were well-matched with the detection bands of the Raspberry Pi Spy Camera\\u2019s CCD while also being far apart enough spectrally to be non-overlapping.'\",\"550\":null,\"551\":\"'In order to encode an assumption that in the absence of training data we expect our model to be explicitly uncertain we introduce a prior\\\\np(z)=N(z|\\u03bc, \\u03b3I)\\\\non the uncertainty associated with the occluded scene which our network reverts back to in the absence of a supervised training signal. To do this, we begin by treating\\\\np\\\\n\\u03d5\\\\n(z|x)\\\\nas an approximate posterior to\\\\np(z|y)\\\\ninduced by the joint\\\\np(z, y)=p(y|z)p(z)\\\\nwhere,'\",\"552\":\"'In this paper, we demonstrate the ability to generate future predictions of occupancy maps without an explicit model using a U-Net style autoencoder as shown in Fig. 2. This is a symmetric style of autoencoder that uses skip connections between the downsampling and corresponding upsampling layers. The goal of the skip connections is to allow information to bypass the bottleneck layer of a traditional autoencoder network. We show how the U-Net architecture can be used to predict future occupancy maps and how the architecture can be modified to provide a heuristic for map exploration.'\\n\\n'Because the generated, predicted occupancy map contains highly correlated data from the input image, we selected an autoencoder architecture based on U-Net [21], [17]. The UNet architecture consists of skip connections which allows a direct connection between layers i and\\\\nn\\u2212i\\\\nenabling the option to bypass the bottleneck associated with the downsampling layers in order to perform an identity operation. The encoder network consists of 7 convolution, batch normalization and ReLU layers where each convolution consists of a\\\\n4\\u00d74\\\\nfilter and stride length of 2. The number of filters for the 7 layers in the encoder network are: (64, 128, 256, 512, 512, 512, 512). The decoder network consists of 7 upsampling layers with the following number of filters: (512, 1024, 1024, 1024, 512, 256, 128).'\\n\\n'Autoencoder Network:'\\n\\n'(a) Input occupancy map based on lidar\\u2019s FOV, (b) Prediction using our method, (c) Prediction using U-Net with L1 loss, (d) Prediction using border padding algorithm, (e) Autoencoder network, and (f) Ground truth image'\\n\\n'To answer the first question, we evaluate different neural network architectures and loss functions and find that a U-Net autoencoder architecture with skip connections combined with a multi-term loss function that encourages reconstruction while preserving structure and edges resulted in the best prediction.'\",\"553\":\"https:\\/\\/github.com\\/BertaBescos\\/EmptyCities\",\"554\":\"'We propose to encode these priors formally into the parameters,\\\\n\\u03b8\\\\n, of a learned function:'\",\"555\":\"'We would like to thank the authors referred in this study for sharing their code. This work is funded by the NIST grant 70NANB17H185. YA would like to thank the Ministry of National Education in Turkey for their funding and support.'\",\"556\":\"'Several methods for instance segmentation have been proposed over the last years, most of them based on some CNN backbone for feature extraction but using different types of decoders. Such approaches can be grouped according to how they exploit different decoder architectures.'\\n\\n'Our architecture. The different resolution features are skipped to all decoders in their respective output stride (OS). Each decoder predicts a volume of lower resolution than the input, in this case of OS8, reducing the number of pixels to cluster by a factor of 64. After the decoders predict the semantic mask, and the centers and embeddings are joined into individual instances, the superpixels are used to upsample the results. The encoder shown is Darknet53 [25].'\\n\\n'Our method uses a common CNN encoder extracting features from RGB images at different resolutions, and has three separate decoder heads, see Fig. 2. Each decoder head combines and upsamples the multi-resolution features into a low-dimensional grid, which makes the processing fast. We explain these three output grids extensively, and we call the value at a certain\\\\n(x,y)\\\\nposition of this grid a \\u201cgrid element\\u201d. Such an element addresses all corresponding feature values at that spatial position. The three heads predict: (i) a semantic segmentation mask which maps each grid element to a softmax pseudo-probability distribution over the desired semantic classes; (ii) a high-dimensional embedding for each grid element, which is to be close in Euclidean similarity for elements belonging to the same instance, and distant otherwise; and (iii) the confidence of each grid element being an object center. Parallelly, the input is processed by a fast GPU-based superpixel algorithm [26] based on SLIC [1] which is able to upsample each \\u201cgrid element\\u201d into the locally connected pixels in the original resolution output.'\\n\\n'Our approach can be summarized in three main steps. First, a CNN backbone summarizes the image as a set of features of different resolution, and three task decoders upsample these features into task grids of lower resolution than the input. These decoders contain the semantic segmentation, center confidences, and embeddings respectively for each of the groups of input pixels that are mapped to it. Second, and in parallel with the CNN, a fast superpixel extraction [26] of the original image is performed resulting in a mapping used to upsample the decoder grids. Finally, a post processing is performed to map each embedding to an individual object center and extract the instances, previous to upsampling using the mappings from step two.'\\n\\n'Our CNN structure is composed of four main components: (i) a fully convolutional encoder which extracts features at different resolutions for the decoders, (ii) a decoder which infers a downsampled semantic segmentation softmax distribution over the semantic classes, (iii) a decoder which infers a confidence of each superpixel being an object center, and (iv) a decoder which infers a 32-dimensional embedding for each superpixel using a discriminative metric loss. All four components are trained jointly using a weighted sum of the three task losses. In the following subsections, we introduce each of these modules in detail:'\\n\\n'Encoders:'\\n\\n'Semantic Segmentation Decoder: all decoders have the same architecture, but their last layer is passed through a different activation function. Furthermore, during training they are optimized with different losses. On top of the encoder we attach a module that upsamples the last layer\\u2019s features and concatenates them to their matching skip resolution in the encoder (see Fig. 2). After the concatenation we use a[1 \\u00d7 1] convolution that squashes the upsampled features with the skipped ones, and 2 layers of [3 \\u00d7 3] convolutions to combine them and learn task-specific parameters. This is done iteratively until the output volume matches the size of the grid needed to perform the superpixel upsampling. For the semantic segmentation head, the output depth is the number of classes, and the activation function is a softmax\\\\ny\\\\n^\\\\nc\\\\n=\\\\ne\\\\nlogit\\\\nc\\\\n\\u2211\\\\nc\\\\ne\\\\nlogit\\\\nc\\\\nwhere logit c is the unbounded output in the slice corresponding to class c. This gives a pseudoprobability distribution per grid-element, which is optimized using a weighted cross-entropy loss:'\\n\\n'Semantic Segmentation Decoder:'\\n\\n'Embedding Decoder:'\\n\\n'Instance Center Decoder: The last decoder head is the object center confidence head, which is analogous to the other two in terms of architecture design. This branch predicts, for each grid element, the confidence of it being the center of an object. In this decoder, the output volume is of depth 1, followed by a sigmoid activation of shape\\\\ny\\\\n^\\\\n=\\u03c3(logit)=1\\/(1+\\\\ne\\\\n\\u2212 -logit \\\\n)\\\\n. This branch is optimized by a weighted cross-entropy loss between the output and the object center targets. However, because the number of grid elements is orders of magnitude larger than the average amount of objects in each image, the easy background elements overwhelm the loss. Therefore, we follow [16] and add an extra focal loss term modulated by\\\\n\\u03b3\\\\n. This makes the loss of easy background examples lower to prevent this overwhelming:'\\n\\n'Instance Center Decoder:'\\n\\n'Tab. I shows the results of training in the 2,975 training set images with fine, pixel-wise annotations, and validating the results in 500 held out images from different cities. The first row shows the results of training an architecture with a strong, state of the art backbone (DN53) and inferring only the semantic and embedding branch, without the embedding centers or superpixel upsampling, and applying an all-toall clustering of the instance pixels afterwards. The subsequent rows show our approach inferring the cluster centers with different backbones, and decoder output strides. The results show two interesting effects. First, even though the baseline with no superpixel upsampling or center inferrence performs slightly better than our best performing architecture (row 0 vs. 1), this costs almost 3 times more to run, due to the expensive post-processing in the absence of the inferred cluster centers. Second, when using decoder OS 4, which means upsampling with superpixels of size k=4, the models perform similarly to their non-upsampled counterparts, but run 2 times faster in the case of the Darknet based model, and almost 3 times faster for the Mobilenets based one.'\\n\\n'TABLE I Performance On Validation Set By Encoder, Input Size, and Output Strides'\\n\\n'Examples of results on the validation set for our MobilenetsV2 backend using a decoder output grid downsampled 16 times (decoder OS 4).'\",\"557\":\"'In this paper we propose an approach to embed multi-dimensional continuous cues in binary feature descriptors used for visual place recognition. The embedding is achieved by extending each feature descriptor with a binary string that encodes a cue and supports the Hamming distance metric. Augmenting the descriptors in such a way has the advantage of being transparent to the procedure used to compare them. We present a concrete application of our methodology, demonstrating the considered type of continuous cue. Additionally, we conducted a broad quantitative and comparative evaluation on that application, covering five benchmark datasets and several state-of-the-art image retrieval approaches in combination with various binary descriptor types.'\\n\\n'Feature descriptors are vectors that encode the local appearance of an image around a point of interest (keypoint). Floating-point descriptors are vectors of continuous numbers and are usually compared with the\\\\nL\\\\n2\\\\n\\u2212\\\\nnorm. Binary descriptors are stored in binary vectors and are compared using the Hamming distance\\\\nL\\\\nH\\\\n[24]. Descriptors are computed so that the distance between them grows with the dissimilarity between the corresponding, described image regions.'\\n\\n'In their Bags of Binary Words (DBoW) open-source library, G\\u00e1lvez-L\\u00f3pez and Tard\\u00f3s [2] implemented the BOF approach and added further improvements while making it accessible for the SLAM research community.'\",\"558\":null,\"559\":null,\"560\":null,\"561\":null,\"562\":null,\"563\":\"'With node selection being directed by a heuristic cost [1]-[3], A-search guided tree (AGT) is constructed on-the-fly and enables fast kinodynamic planning. This work presents two variants of AGT to improve computation efficiency. An improved AGT (i-AGT) biases node expansion through prioritizing control actions, an analogy of prioritizing nodes. Focusing on node selection, a bi-directional AGT (BAGT) introduces a second tree originated from the goal in order to offer a better heuristic cost of the first tree. Effectiveness of BAGT pivots on the fact that the second tree encodes obstacles information near the goal. Case study demonstrates that i-AGT consistently reduces the complexity of the tree and improves computation efficiency; and BAGT works largely but not always, particularly with no benefit observed for simple cases.'\\n\\n'Three algorithms, AGT, i-AGT and BAGT, are coded to solve Problem 2.2 with system kinematics (2). Simulation is conducted in MadabO 2016b.'\",\"564\":null,\"565\":\"'The concepts introduced in the previous section are combined into a single BO algorithm. In the following, we introduce the GP regression formulation with the associated termination criterion and further specify both the kernel function and the merit function employed. The corresponding pseudo-code is provided in Algorithm 1.'\\n\\n'The environment is encoded via a heightmap representation. Terrain height h is sampled along the horizontal with discretization\\\\n\\u0394x\\\\non the interval\\\\nx\\u2208[\\\\nx\\\\nmin\\\\n,\\\\nx\\\\nmax\\\\n]\\\\n. For a total of nS samples, the resulting heightmap is'\\n\\n'The UL optimization determines the desired gait by setting certain phases to zero. For example, a single stance phase is encoded by action\\\\ns\\\\nA\\\\n=[\\\\nn\\\\n1\\\\n, 0, 0, 0, 0]\\\\n, a single jump by\\\\ns\\\\nB\\\\n=[\\\\nn\\\\n1\\\\n, \\\\nn\\\\n2\\\\n, \\\\nn\\\\n3\\\\n, 0, 0]\\\\n, and a double jump by\\\\ns\\\\nC\\\\n=[\\\\nn\\\\n1\\\\n, \\\\nn\\\\n2\\\\n, \\\\nn\\\\n3\\\\n, \\\\nn\\\\n4\\\\n, \\\\nn\\\\n5\\\\n]\\\\n. Our nomenclature requires\\\\ns\\\\nD\\\\n=[\\\\nn\\\\n1\\\\n, 0, 0, \\\\nn\\\\n4\\\\n, \\\\nn\\\\n5\\\\n]\\\\nto be encoded by\\\\ns\\\\nB\\\\n.'\",\"566\":\"'In this work, we do not explicitly address socially acceptable navigation. However, our framework can be extended to incorporate social norms. For example, in the graph construction step (described below), the interaction between people can be encoded in the graph is obtainable (one person taking pictures for the other, a group of friends walking together) so that edges between pedestrians can be denoted as non-crossable for the robot. Moreover, the path generated by our planner naturally captures some social norms. For example, when passing a pedestrian, our planner will favor passing from behind rather than in front of the pedestrian, because the former leads to a shorter traveling distance.'\",\"567\":null,\"568\":null,\"569\":null,\"570\":\"'C. Experiments Using Encoder Readings'\\n\\n'The encoder readings are read from the robot controller using the the ABB RobotStudio software. The encoder readings may not exactly capture the the kinematic errors in the trajectory and introduce its own errors (e.g.: sensor errors). However, they do capture most of the non-kinematic errors (eg: gearbox backlash and stiffness, etc). Hence, the polled values obtained from the encoder readings are a good way to collect the end-effector position data. Moreover, the polled values are reliable and simple to obtain to test the devised C-DC scheme. The All-Net is trained and data analysis is performed using the encoder polled readings. The encoder readings are polled at equal time interval checkpoints on the trajectory.'\\n\\n'The C-DC scheme is tested using three different types of trajectories at a zero end-effector load using the encoder polled measurements.'\",\"571\":\"'https:\\/\\/www.youtube.com\\/watch?v=aKGL47-xb6s'\\n\\n'We denote the orientation of a frame by X, Y, Z unit vectors and use color code red, green, and blue respectively to represent them. We consider Z axis of any pi to be normal to the workpiece\\u2019s surface, and X, Y axes are assigned appropriately depending on the task. For some tasks, they can be arbitrarily assigned, whereas some tasks specify their direction. We consider a frame attached to the end-effector or flange of the manipulator, represented with respect to robot base, by a homogeneous transformation matrix\\\\nR\\\\nT\\\\nE\\\\n[33]. A tool center point (TCP) frame is attached to the tooltip which defines the transformation of TCP with respect to end-effector\\\\nE\\\\nT\\\\nT\\\\n. We consider each of the waypoints on the path to be reached by the TCP satisfying the constraints. We consider the description of a robot is constituting its kinematics and dynamics model. Configuration space of the redundant manipulator is its joint-space. We denote the joint vector of variables as\\\\n\\u0398=\\\\n\\u03b8\\\\n1\\\\n,\\\\n\\u03b8\\\\n2\\\\n,\\u2026,\\\\n\\u03b8\\\\nn\\\\nfor a nDOF manipulator. For KUKA iiwa 7 manipulator used in our work,\\\\nn=7.A\\\\nmanufacturing process specifies the minimum process velocity and force to be exerted during contact of the TCP with the workpiece at a given waypoint. These process requirements are set in the form of constraint violation functions (see Section-IV-B).'\\n\\n'A capability map is a precomputed discretized representation of the workspace that encodes the position and orientation reachability of the robot. However, computing a high fidelity capability map for redundant manipulators by solving IK for each discrete position and orientation in the workspace is computationally intractable. We have developed a sampling-based approach to compute the capability map C that is computationally efficient.'\",\"572\":\"'We propose a geometric method to solve inverse kinematics (IK) problems of 7-DoF manipulators with joint offsets at shoulder, elbow, and wrist. Traditionally, inverse position kinematics for redundant manipulators are solved by using an iterative method based on the pseudo-inverse of the manipulator Jacobian. This provides a single solution among the infinitely many possible solutions for the IK problem of redundant manipulators. There are no closed-form IK solutions for redundant manipulators with multiple joint offsets. Using our method we can compute multiple IK solutions using two-parameter search by exploiting geometry of the structure of a redundant manipulator. Our proposed IK algorithm can handle multiple joint offsets and is mathematically simple to implement in a few lines of code. We apply our algorithm to compute IK solutions for 7-DoF redundant Baxter robot (that has joint offsets at shoulder, wrist, and elbow joints) for end-effector configurations where existing geometry-based IK solvers fail to find solutions. We also demonstrate the use of our algorithm in an application where we want to compute an IK solution (among the infinitely many possible solutions) that has minimum error bound in end-effector position, in the presence of random joint actuation and sensing uncertainties.'\",\"573\":\"'A dynamical model of the RCV and its components is created in Simulink, and connected to the Stateflow model of the Supervisor. This setup allows for testing the Stateflow model, subject to the requirements. However, testing can only show presence of errors, not abscence, so regardless of how exhaustively the Stateflow model is tested, only a statistical argument can be made for the correctness of the design. To prove absence of errors, the Stateflow model is translated into Promela code and formally verified with the model checker SPIN [8] . Prior research has described methods for translating Stateflow models into Promela code [27] , [28] , but to our knowledge there is no freely available tool for automatic translation. Since the Stateflow model uses a small and simple subset of the semantics, it is relatively straightforward to do the translation manually.'\\n\\n'This paper studies the problem of selecting an appropriate action in case of hazardous situations during level 4 automated driving. We propose a method based on model-based design, formal methods, and code generation to design a supervisor that arbitrates between planner functions such that a minimal risk condition may always be reached. The method is validated through simulations and experiments. Detection of hazardous internal or external states that should cause safe stop is a non-trivial problem left for future research. Also, the safe stop capability depends on the correctness of the functionality of all other components in Figure 1 , which are typically less suitable for verification by formal methods.'\",\"574\":null,\"575\":null,\"576\":null,\"577\":null,\"578\":null,\"579\":null,\"580\":null,\"581\":\"\\\"In this work we present a bioinspired visual system sensor to estimate angular rates in unmanned aerial vehicles (UAV) using Neural Networks. We have conceived a hardware setup to emulate Drosophila's ocellar system, three simple eyes related to stabilization. This device is composed of three low resolution cameras with a similar spatial configuration as the ocelli. There have been previous approaches based on this ocellar system, most of them considering assumptions such as known light source direction or a punctual light source. In contrast, here we present a learning approach using Artificial Neural Networks in order to recover the system's angular rates indoors and outdoors without previous knowledge. A classical computer vision based method is also derived to be used as a benchmark for the learning approach. The method is validated with a large dataset of images (more than half a million samples) including synthetic and real data. The source code of the algorithms and the datasets used in this paper have been released in an open repository.\\\"\",\"582\":\"'To address safety concerns, the Federal Aviation Administration (FAA) has suggested a series of regulations (e.g., Title 14 Code of Federal Regulations) about AAV safety requirements [1]. A standout amongst the most vital concerns for reliability is the behavior of the system during a breakdown, which raises the need for AAVs to have the capacity to detect faults in the system and react accordingly. Larger aircraft usually devise redundant hardware to address the safety and reliability concern, which is a reliable approach, but is more expensive, adds weight and occupies more space. However, for smaller aircraft, including smaller AAVs, the hardware redundancy is generally not possible due to space and load constraints. To provide the necessary reliability to these aircraft, a Fault Detection, Isolation and Recovery (FDIR) is required. Fault Diagnosis is a fundamental piece of FDIR techniques which can be divided into three sections: Fault Detection, Fault Isolation and Fault Estimation. Fault Detection is to recognize if a problem has happened; Fault Isolation is to decide the area in which the fault has occurred; Fault Estimation is to find the type of fault and its impacts.'\",\"583\":\"'In this paper, we propose an approach to learn compact representations from salient landmarks detected by a visual attention algorithm to recognize previously visited places in underwater environments. Instead of using hand-crafted local descriptors as it has been typically done in visual place recognition, we use a convolutional autoencoder to obtain an ad hoc descriptor generator from salient landmarks. The main advantage of using an autoencoder is that it can learn in an unsupervised manner directly from the salient landmarks. In addition, we show that it is possible to do the training with less than 100,000 examples instead of several hundreds of thousands or even millions of labeled examples as in other convolutional architectures. The trained convolutional autoencoder is used to obtain descriptors for salient landmarks that are later utilized in a voting scheme to calculate similarity between images with the objective of finding if a place has already been visited. The proposed method has obtained good results compared to SeqSLAM and FAB-MAP in different datasets obtained from robotic explorations of coral reefs in real life conditions. Moreover, when the visual attention algorithm is used, fewer features are required to get a good performance in terms of precision and recall compared when using the SURF method to extract visual features.'\\n\\n'After detecting salient patches, it is necessary to describe them. A convolutional autoencoder (CAE) [6] is utilized to generate a compact representation of the salient landmarks that can be used as descriptor. A CAE is a neural network that can learn in an unsupervised manner unlike other architectures that require a set of labeled training images. Because of that, it is possible to train the network with a set of patches extracted from few videos from the environment where the proposed system is going to be used. The main contribution of this paper is centered on the combination of a human-inspired algorithm to extract visual features with an unsupervised neural network to learn how to describe them, applied to the task of recognizing previously seen places from a stream of images.'\\n\\n'The objective of our approach is to recognize previously seen places within a sequence of images obtained from the exploration of an underwater environment. To find matching images, we describe them in terms of the prominent features that appears in them. These features are obtained by using a computational visual attention algorithm that detects regions of the image that are more likely to attract the attention of a human. In addition, the salient features are described by a code obtained from a convolutional autoencoder trained with patches of underwater images. A general overview of the system can be seen in Fig. 1. Each of the parts in the proposed system is explained in the following sections.'\\n\\n'B. Convolutional Autoencoder to describe the salient landmarks'\\n\\n'The complete descriptor for a given patch in CIELab is built by concatenating the coding of each channel obtained from the encoding stage of the autoencoder. As it can be seen in the architecture of Fig. 1, the form of the code generated after the three layers of max pooling is\\\\n5\\u00d75\\u00d74\\\\n. This code is flattened into a vector of size 100. The final descriptor composed of the three channels is of size 300.'\\n\\n'This work has demonstrated the usage of salient landmarks detected by a visual attention algorithm described by a compact representation learned with a convolutional autoencoder (CAE) in tasks of recognizing previously seen places. It is important to mention that, unlike other approaches that require labeled datasets or pre-trained networks [38], [24], when using an autoencoder we do not need labels for each training patch. In addition, CAE training patches can be extracted from few videos from the environment to be explored, which can be easily obtained by a robotic agent or even a diver. For example, in this work the CAE was trained using 81,084 patches extracted from 5 videos of no more than 5 minutes.'\\n\\n'We have presented a method to visually recognize previously visited places with salient landmarks detected by a visual attention algorithm. These features are described by a compact representation (descriptor) learned by a convolutional autoencoder trained in patches extracted by the same visual attention algorithm. The proposed method is designed to handle streams of underwater images, therefore frames are processed as they are received. To identify previously viewed places with the most recent received image, a voting scheme is utilized to calculate the similarity between the current frame and all the previous ones. The voting scheme is based on increasing a similarity score of the previous images where similar salient landmarks to the ones in the current image are detected. All the salient landmarks are organized in an inverted index,\\\\ni.e\\\\n., a record of the image where each of them has appeared. Moreover, the Fast Library for Approximate Nearest Neighbors is used to efficiently search for the most similar landmarks pairs. We have obtained good results in terms of precision and recall compared to other approaches, such as SeqSLAM [39] or FAB-MAP [35]. However, we have observed that the proposed approach does not perform very well when dealing with images where few salient landmarks appear or when the landmarks are not sufficiently distinctive. This issue causes that images from the same place are not detected as such.'\\n\\n'Future work will focus on increasing the number of recognized places (improve the recall). For that, we can modify the convolutional autoencoder, for example, adding more layers or more filters to each layer. Secondly, the recall can be improved with a better scheme for the calculation of similarity between images, for example, by using a Bayesian Framework as in [40].'\",\"584\":null,\"585\":null,\"586\":\"'Designing this buoyancy engine with a gear pump instead of a piston pump allows us to take advantage of higher flow rates in and out of the bladder, but it also removes the reliable flow rate feedback that an encoder can provide on a piston pump. We have observed an issue with measuring low flow rates due to hysteresis in the differential pressure sensor used. This issue could explain some of the noisy response past 30 seconds in the trial shown in Fig. 7. It could also explain the over-estimation of the bladder mass in the experiment.'\",\"587\":null,\"588\":\"'https:\\/\\/github.com\\/cogsys-tuebingen\\/gerona'\\n\\n'This work presented an efficient traversability analysis method designed for real-time operation and short range sensors. By using a detailed vehicle model, highly accurate pose estimation and collision detection can be performed. This allows a traversability analysis using vehicle specific safety criteria and is used to perform local path planning for reactive obstacle avoidance. The performance of the proposed method was demonstrated on two mobile platforms in different simulated and real world environment setups. The safety relevant angles never exceeded the thresholds by more than 1.02%, providing a level of safety sufficient for many robotics applications. The proposed method is part of the open-source GeRoNa-framework, which is based on ROS and available under: https:\\/\\/github.com\\/cogsys-tuebingen\\/gerona.'\\n\\n'https:\\/\\/github.com\\/cogsys-tuebingen\\/gerona'\",\"589\":null,\"590\":\"'We express our gratitude to Jan Hakenberg for providing many valuable inputs, code contributions and reviews.'\",\"591\":\"'In this paper, we examine the problem of navigating cluttered environments without explicit object detection and tracking. We introduce the dynamic risk density to map the congestion density and spatial flow of the environment to a cost function for the agent to determine risk when navigating that environment. We build upon our prior work, wherein the agent maps the density and motion of objects to an occupancy risk, then navigate the environment over a specified risk level set. Here, the agent does not need to identify objects to compute the occupancy risk, and instead computes this cost function using the occupancy density and velocity fields around them. Simulations show how this dynamic risk density encodes movement information for the ego agent and closely models the object-based congestion cost. We implement our dynamic risk density on an autonomous wheelchair and show how it can be used for navigating unstructured, crowded and cluttered environments.'\",\"592\":null,\"593\":null,\"594\":null,\"595\":\"'In this work, we have contributed a novel computational framework for tool construction based on geometric reasoning, which we have demonstrated on the construction of three tools, both when part attachment information is known and unknown. In all cases, the robot efficiently constructed the tool, exploring only a small percentage of all possible part combinations. Also, we have introduced a formalization of tool Macgyvering, to guide future research in this area by defining varying levels of difficulty for this problem. Future work should address the following limitations of the current approach. Two key assumptions, the current limitation that the number of candidate parts must equal the number of tool components, and that \\u039b parameters are hand-coded, should be relaxed. Further, as tool complexity is increased, metrics and heuristics should be explored that maintain the computational tractability of the search problem. Finally, the tool representation should be extended to include a broader range of properties, including material and density.'\",\"596\":null,\"597\":\"'To facilitate reproduction and extension, we have made our code public, and include the git hash reflecting the state of the code at the time of this paper\\u2019s writing 3.'\",\"598\":\"'In the proposed method, an observed view data is assumed to be an RGB image. In order to approximate the object observation model, we train the variational auto-encoder to achieve the object generative model. The encoded features of the 3D object is estimated from the RGB single view. Using the approximated observation model, complete SLAM formulation and optimization becomes possible.'\\n\\n'Therefore, we propose a method to approximate an object observation model to a tractable distribution, considering the 3D shape and the viewpoint orientation. Using the proposed observation model, not only the approximated solutions for the coordinates of the object landmarks but feature values and the orientations can be estimated under the consideration of data association. We use the Variational auto-encoder (VAE) [14] to infer a variational likelihood of 3D object observation model from RGB single view so that the robot can exploit the observation with RGB mono camera.'\\n\\n'Proposed network architecture. The encoder part for the variational likelihood estimation is constructed using darknet -19 which is used in YOLOv2. We add an additional 2D convolution layer at the end of the darknet -19 so that the encoder becomes a fully convolution network. The decoder is composed of a dense layer and several 3D convolution-transpose layers. The prior networks which is trained with VAE simultaneously consist of fully connected layers. The end-to-end training of the proposed networks is achievable.'\\n\\n'Overview of the proposed system. Basically we estimate the visual odometry for every sequences. For each of the keyframes, object detection and feature encoding are performed simultaneously. Using the encoded features and the approximated observation model, SLAM optimization considering the object shape and the orientation is conducted. To solve the complete SLAM formulation, only the encoding process using the trained encoder is necessary. However, the full shape reconstruction of each observed object can be obtained at anytime.'\\n\\n'Recent researches of the monocular SLAM typically aim the real-time performance. To meet this trend, we adopt darknet -19 for our encoder, which is capable of real-time operation [7]. The encoder is then constructed by adding a convolution layer followed by a global max-pooling layer on top of the darknet -19 network. When the encoder is solely used for feature encoding except the decoder for 3D shape reconstruction, it can operate at about 30 fps. In practice, basically 10 to 15 objects are observed in a single scene; when the images of objects are cropped from a scene go parallel to the encoder, the encoding process still can operate at about 30 fps. Since our network learns about the objects and their pose from the 2D RGB images, we pre-train the encoder for two tasks in order; object category classification of the Imagenet dataset [29], and orientation classification of the Render for CNN dataset [30].'\\n\\n'The decoder of our network follows the structure introduced in [17]. To make the decoder familiar with the 3D object shape, we construct the VAE using an additional 3D encoder together, and pre-train the network using Model-Net40 dataset [18]. In this pre-training of the decoder, a prior network consists of 3 dense layers are pre-trained together. After pre-trainings, we combine these networks and train the proposed observation model. The proposed network structure is displayed in Fig. 3.'\\n\\n'We have shown that a variational object observation model enables the complete probabilistic semantic SLAM. The high-dimensional features such as the 3D shape of objects follow an intractable distribution; therefore in the existing SLAM formulation it is scarcely considered to the optimize the feature as well as the poses of the object. To overcome this problem, we approximate the observation model of the 3D object to a tractable distribution using a generative model. Since the robot with a mono camera basically observes an RGB single view of an object, the proposed algorithm infers a variational likelihood of the full shape from the single view. Pose and feature optimization can be simultaneously performed using the encoded features, and the 3D full shape can also be obtained through the decoding process. Our experiments suggest that the graph optimization and automatic loop closing is performed smoothly. To evaluate the approximated observation model and the encoded features, we conduct SLAM and shape reconstruction for the environments with various objects.'\",\"599\":\"'Learning to efficiently navigate an environment using only an on-board camera is a difficult task for an agent when the final goal is far from the initial state and extrinsic rewards are sparse. To address this problem, we present a self-supervised prediction network to train the agent with intrinsic rewards that relate to achieving the desired final goal. The network learns to predict its future camera view (the future state) from a current state-action pair through an Action Representation Module that decodes input actions as higher dimensional representations. To increase the representational power of the network during exploration we fuse the responses from the Action Representation Module in the transition network, which predicts the future state. Moreover, to enhance the discrimination capability between predictions from different input actions we introduce joint regression and triplet ranking loss functions. We show that, despite the sparse extrinsic rewards, by learning action representations we achieve a faster training convergence than state-of-the-art methods with only a small increase in the number of the model parameters.'\\n\\n'The main contributions of this paper are as follows. We propose an Action Representation Module for efficient RL that can be easily integrated with existing convolutional neural network (CNN) architectures. This module expands the dimensions of an input action to improve the representational power of the network during training. We usejoint regression and triplet ranking loss functions to predict the future state while discriminating predictions from different actions to encode meaningful features effectively during training. The proposed module equipped with these losses has a faster training convergence than state-of-the-art methods with only a 0.5% increase in the total number of parameters. Our approach is generic and, in this work, we evaluate it using as state a first-person view (camera view).'\\n\\n'In training, the parameters from the Action Representation Module are implicitly learned from the input actions without an explicit loss function related to this module, which derives higher dimensional features from one-hot action codes. Fig. 3 shows a sample response generated by each action code.'\\n\\n'TABLE I Details of the configuration of the networks. The Action Representation Module decodes the responses by setting stride to 2 [41]. \\u2018Concat\\u2019 denotes a concatenation that fuses two responses.'\\n\\n'To quantify the benefits of using intrinsic rewards, we perform transfer learning to evaluate if the model learned by self-supervised training encodes meaningful information for navigation. First, we train a model learning features from intrinsic rewards only (i.e. in a self-supervised manner). Then we fine-tune this pre-trained model in VzDoom with sparse extrinsic rewards. In the fine-tuning stage, we set\\\\n\\u03b7\\\\n= 1 to decrease the influence of the intrinsic rewards and to force the network to focus on the extrinsic rewards.'\\n\\n'We proposed an Action Representation Module that expands the dimensions of one-hot codes of input actions, which are then fused with the state-transition network. The network can predict the future state efficiently in an exploration task. Moreover we train the Forward Model with joint regression and triplet ranking loss functions, enabling the network to discriminate predictions from different actions.'\",\"600\":\"https:\\/\\/github.com\\/zswang666\",\"601\":null,\"602\":\"'Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process 1242\\u00d7375 resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error - using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https:\\/\\/github.com\\/mileyan\\/AnyNet.'\",\"603\":null,\"604\":null,\"605\":\"'https:\\/\\/github.gatech.edu\\/DCSL\\/NP_Informed_Sampling'\\n\\n'The performance of the NP-Informed sampling was benchmarked against conventional Informed Sampling and Uniform rejection sampling. The above exploration strategies were paired with local rewiring for exploitation. A C++ implementation of the above samplers was used (Informed sampling implementation based on the open-source OMPL version [24]). Please see: https:\\/\\/github.gatech.edu\\/DCSL\\/NP_Informed_Sampling. All experiments were run on a 64-bit PC with 64 GB RAM and Intel Xeon(R) Processor. The operating system used was Ubuntu 16.04. Data was recorded over 100 trials for all the cases. The algorithms were tested in a single obstacle and a multiple obstacle setting in\\\\nR\\\\n2\\\\n,\\\\nR\\\\n3\\\\n,\\\\nR\\\\n4\\\\nand\\\\nR\\\\n6\\\\n(\\\\nFig. 2). A (hyper)cube problem environment X of width 10 units was considered in both cases. In the first case, a single (hyper)cube obstacle with width of 2 units was placed with its center at the origin. The obstacle lies between the start and the goal point,\\\\nx\\\\ninit\\\\n=[1.5,0,\\u2026,0\\\\n]\\\\nT\\\\n,\\\\nx\\\\ngoal\\\\n=[\\u22121.5,0,\\u2026,0\\\\n]\\\\nT\\\\n. The second case consists of multiple obstacles with\\\\nx\\\\ninit\\\\n=[0,4,\\u2026,0\\\\n]\\\\nT\\\\n, TABLE I: Case 1:Average time\\/length of first solution\\\\nx\\\\ngoal\\\\n=[0,0,\\u2026,0\\\\n]\\\\nT\\\\n. The 2D environment in case 2 was extended to higher dimensions by imparting a length of 2 units (symmetrically) to each obstacle in dimensions\\\\nd\\u22653.A\\\\ngoal bias of 10% was used in Informed and Uniform rejection sampling. Experiments were performed with following the step-sizes (maximum edge length\\\\n)\\u03b7:0.3,0.6,1.,2.\\\\nfor\\\\nR\\\\n2\\\\n,\\\\nR\\\\n3\\\\n,\\\\nR\\\\n4\\\\n,\\\\nR\\\\n6\\\\nrespectively. Simulations were run for\\\\n2,3,4,8\\\\nseconds in\\\\nR\\\\n2\\\\n,\\\\nR\\\\n3\\\\n,\\\\nR\\\\n4\\\\n,\\\\nR\\\\n6\\\\nrespectively. The parameters of the proposed algorithm were set as follows. All simulations were performed with\\\\n\\u03f5\\\\nk\\\\n=0.5\\\\n, i.e., 50 % split between conventional Informed sampling and NP-informed sampling. Weighting constants\\\\n(\\\\n\\u03bb\\\\n1\\\\n,\\\\n\\u03bb\\\\n2\\\\n,\\\\n\\u03bb\\\\n3\\\\n)\\\\nin (6) were set to\\\\n(3\\/9,2\\/9,4\\/9)\\\\nbefore an initial solution is found and then changed to\\\\n(0,1\\/2,1\\/2)\\\\n. This causes the sampler to first focus on finding a good initial solution and then prioritize exploring the informed set. The probability of generating a random direction\\\\ns\\\\n(\\\\nAlg. 4, line 4) was initialized to 0 and changed to 0.5 after an initial solution was found. The maximum magnitude of travel for generating a random sample\\\\n\\u03b3\\\\nk\\\\nwas initialized to\\\\n1.5\\u03b7\\\\nfor every kernel and then decreased by a small quantity\\\\n(\\u0394\\u03b3=0.01)\\\\nevery time a sampling attempt was made from that kernel. The threshold distance for initializing a new kernel vertex:\\\\n(\\\\n\\u03b7\\\\n0\\\\n)\\\\nwas set to\\\\n\\u03b7\\\\n. The number of candidate directions generated\\\\nT\\\\n, were 5 in\\\\nR\\\\n2\\\\n,\\\\nR\\\\n3\\\\nand 10 in\\\\nR\\\\n4\\\\n,\\\\nR\\\\n6\\\\n.'\\n\\n'https:\\/\\/github.gatech.edu\\/DCSL\\/NP_Informed_Sampling'\",\"606\":\"'To validate our approach in real-time, 28 computations of the described scenario are performed on an embedded computer (2.10 GHz Intel Core i7-3612QM CPU, 8 GB RAM) running a Visual C\\/C++ Solution File for Simulink Coder. The mean\\u00b1 standard deviation\\/minimum\\/maximum values for the calculation time are\\\\n72\\u00b110\\/61\\/99\\\\nms, which is satisfyingly fast for a real-time predictive motion planner.'\",\"607\":null,\"608\":null,\"609\":\"'https:\\/\\/github.com\\/AAnoosheh\\/ToDayGAN'\\n\\n'https:\\/\\/github.com\\/AAnoosheh\\/ToDayGAN'\\n\\n'Code for ToDayGAN is publicly available at https:\\/\\/github.com\\/AAnoosheh\\/ToDayGAN.'\",\"610\":\"'A drawback of our geometric primitive features is that they don\\u2019t have a strong signature such as e.g. visual point features. This makes association of a detection to a map element highly ambiguous. To solve the association problem, we consider multiple detections. The relative positions of these detections serve as a pattern. If the pattern is unique in the map the association can be solved easily. Ambiguity can occur because of too few detections and periodic patterns. Practical reasons for that are missed features because of limited sensor range and imperfection of the detectors. Further, periodicity is common in road scenes, especially for road markings. The problem of ambiguities can be solved by accumulating detections over time. The higher the number of different detections the less ambiguous the pattern gets. To generate a single pattern from detections of different points in time, the detections are transformed into a static world frame. Therefore the movement of the car has to be known. We use odometry based on wheel encoders as an estimate for the movement of the car. The local map consists of detections in the time span\\\\n8=[\\\\nt\\\\nb\\\\n, \\\\nt\\\\ne\\\\n]\\\\n. The time te is always defined by the most recent detection. The traveled distance d.) between two time points is used to set tb:'\\n\\n'Results of one evaluation drive with all feature types. Color code: low dynamic(green), normal dynamic(blue), high dynamic(red)'\",\"611\":null,\"612\":\"'We address the problem of visual place recognition with perceptual changes. The fundamental problem of visual place recognition is generating robust image representations which are not only insensitive to environmental changes but also distinguishable to different places. Taking advantage of the feature extraction ability of Convolutional Neural Networks (CNNs), we further investigate how to localize discriminative visual landmarks that positively contribute to the similarity measurement, such as buildings and vegetations. In particular, a Landmark Localization Network (LLN) is designed to indicate which regions of an image are used for discrimination. Detailed experiments are conducted on open source datasets with varied appearance and viewpoint changes. The proposed approach achieves superior performance against state-of-the-art methods.'\",\"613\":\"'https:\\/\\/youtu.be\\/q3YqIyaFUVE'\\n\\n'https:\\/\\/github.com\\/uzh-rpg\\/rpg-information-field'\\n\\n'We make our implementation of the Fisher information field open source to benefit the research community. The rest of this paper is structured as follows. In Section II, we briefly introduce the Fisher information matrix and its application in active localization, from which we identify the computational bottleneck of using a point cloud. In Section III, we show the separation of the rotation-independent kernels from the approximated Fisher information matrix, which enables a volumetric representation. In Section IV, we describe the method to build and update the proposed information field and introduce an alternative formulation to reduce the memory usage. Finally, we show experimental results from simulated and real-world data in Section V.'\\n\\n'https:\\/\\/github.com\\/uzh-rpg\\/rpg-information-field'\\n\\n'Code: https:\\/\\/github.com\\/uzh-rpg\\/rpg-information-field'\",\"614\":null,\"615\":\"'This sparse reward function is general and does not encode any information about the actual geometry of the game. The action space is discretized into five actions. The first four actions constitute 1\\u00b0 rotation increments, clockwise and counterclockwise around the x, and y axes up to a fixed maximum angle. Figure 1\\u2013Left shows the orientation of the x, and y axes with respect to the maze. The 1\\u00b0 increment is sufficient to overcome the static friction, while simultaneously avoiding accelerations that are too large. We define a fifth action as no-op, i.e., maintain the current orientation of the maze. We empirically determined the fixed maximum angle to be 5\\u00b0 in either direction.'\",\"616\":\"'Unsupervised (VAE) transfer: A variational autoencoder [9] generative model is trained on the simulated image data. The encoder, which maps input images to a concise latent state, is then used as the perception module for the action-conditioned reward predictor, which is trained on the real-world data.'\\n\\n'Deep reinforcement learning provides a promising approach for vision-based control of real-world robots. However, the generalization of such models depends critically on the quantity and variety of data available for training. This data can be difficult to obtain for some types of robotic systems, such as fragile, small-scale quadrotors. Simulated rendering and physics can provide for much larger datasets, but such data is inherently of lower quality: many of the phenomena that make the real-world autonomous flight problem challenging, such as complex physics and air currents, are modeled poorly or not at all, and the systematic differences between simulation and the real world are typically impossible to eliminate. In this work, we investigate how data from both simulation and the real world can be combined in a hybrid deep reinforcement learning algorithm. Our method uses real-world data to learn about the dynamics of the system, and simulated data to learn a generalizable perception system that can enable the robot to avoid collisions using only a monocular camera. We demonstrate our approach on a real-world nano aerial vehicle collision avoidance task, showing that with only an hour of real-world data, the quadrotor can avoid collisions in new environments with various lighting conditions and geometry. Code, instructions for building the aerial vehicles, and videos of the experiments can be found at github.com\\/gkahn13\\/GtS.'\",\"617\":\"'https:\\/\\/gichub.com\\/viCa-epfl\\/CrowdNav'\\n\\n'https:\\/\\/youcu.be\\/0sNVCQ9eqA'\\n\\n'As an alternative, reinforcement learning frameworks have been used to train computationally efficient policies that implicitly encode the interactions and cooperation among agents. Although significant progress has been made in recent works [19]\\u2013[22], existing models are still limited in two aspects: i) the collective impact of the crowd is usually modeled by a simplified aggregation of the pairwise interactions, such as a maximin operator [19] or LSTM [22], which may fail to fully represent all the interactions; ii) most methods focus on one-way interactions from humans to the robot, but ignore the interactions within the crowd which could indirectly affect the robot. These limitations degrade the performance of cooperative planning in complex and crowded scenes.'\\n\\n'When humans walk in a densely populated scene, they cooperate with others by anticipating the behaviors of their neighbors in the vicinity, particularly those who are likely to be involved in some future interactions. This motivates us to design a model that can calculate the relative importance and encode the collective impact of neighboring agents for socially compliant navigation. Inspired by the social pooling [13], [15] and attention models [14], [44]\\u2013[48], we introduce a socially attentive network that consists of three modules:'\\n\\n'Interaction module: models the Human-Robot interactions explicitly and encodes the Human-Human interactions through coarse-grained local maps.'\",\"618\":\"'We train an agent directly in the real world to solve a model assembly task involving contacts and unstable objects. An outline of our method, which consists of combining hand-engineered controllers with a residual RL controller, is shown on the left. Rollouts of residual RL solving the block insertion task are shown on the right. Residual RL is capable of learning a feedback controller that adapts to variations in the orientations of the standing blocks and successfully completes the task of inserting a block between them. Videos are available at. residualrl.github.io'\",\"619\":\"'The learning algorithm is natively implemented in Ray RLLib, an open-source distributed reinforcement learning library [18].'\",\"620\":null,\"621\":null,\"622\":\"'We use these notions of adjacency to encode the mission and DoS-resilience constraints as follows.'\\n\\n'We encode single-hop communication adjacency as the conjunction of the following constraints:'\\n\\n'To encode multi-hop communication, we generate the following constraints:'\\n\\n'Table I reports the execution time of the three steps in Alg. 1 for a basic reach-avoid specification as the number of robots and the number of integrators (per robot) in the chain, hence the number of state variables, increase in the presence of one and two base stations. The table also reports the number of Boolean variables needed to encode the DoS-resilience constraints.'\",\"623\":\"'We propose an alternate approach that encodes the constraints directly in terms of the state of the manipulator. In this encoding, the synthesis problem is to find the parameters of the different modules\\\\nr\\\\n1\\\\n,\\u22ef\\\\n,\\\\nr\\\\nn\\\\nmod\\\\n,\\\\nc\\\\n\\u2212\\\\n\\u03b1\\\\n1\\\\n,\\u22ef\\\\n,\\\\nc\\\\n\\u2212\\\\n\\u03b1\\\\nn\\\\nmod\\\\nalong with the states of the manipulator\\\\nP\\\\n1\\\\n,\\u22ef\\\\n,\\\\nP\\\\nn\\\\ntRk\\\\nto reach each of the\\\\nn\\\\ntask\\\\npoints in the task where\\\\nc\\\\n\\u2212\\\\n\\u03b1\\\\nj\\\\nrepresents\\\\ncos(\\\\n\\u03b1\\\\nj\\\\n)\\\\n. We use the subscript i to index a point in the task and j to index a module of the manipulator.'\\n\\n'The synthesis problem is encoded as:'\\n\\n'In the future, we will explore techniques to automatically infer the new points required for trajectory generation. We will also encode more complex tasks such as following trajectories, as well as additional physical constraints such as torque limits. Moreover, we will extend our approach to handle more complex modular robots that combine locomotion and manipulation.'\",\"624\":null,\"625\":\"'Three-dimensional vision plays an important role in robotics. In this paper, we present a 3D surface reconstruction scheme based on combination of stereo matching and pattern projection. A two-step matching scheme is proposed to establish reliable correspondence between stereo images with high computation efficiency and accuracy. The first step (coarse matching) can quickly find the correlation candidates, and the second step (precise matching) is responsible for determining the most precise correspondence within the candidates. Two phase maps serve as codewords and are utilized in the two-step stereo matching, respectively. The phase maps are derived from phase-shifting patterns to provide robustness to the background noises. Only five patterns are required, which reduces the image acquisition time. Moreover, the precision is further enhanced by applying a correspondence refinement algorithm. The precision and accuracy are validated by experiments on standard objects. Furthermore, various experiments are conducted to verify the capability of the proposed method, which includes the complex object reconstruction, the high-resolution reconstruction, and the occlusion avoidance. The real-time experimental results are also provided.'\\n\\n'To solve the aforementioned problems in 3D surface reconstruction, in this paper, we develop a novel scheme based on active stereo matching. In the scheme, a two-step matching method is proposed to enhance the algorithm efficiency and achieve accurate reconstruction. Dual-frequency sinusoidal phase-shifting fringes are utilized to encode each pixel. The codification method has high encoding accuracy and robustness to noises. Totally five patterns are used to generate the codewords, which is much less than the number of patterns used in the common codification methods [25] [13] . Another advantage of the proposed scheme is that phase unwrapping and projector calibration are not required. Fig. 1 demonstrates the high-precision 3D surface reconstruction of a liver model by using the proposed method.'\\n\\n'coarse codeword'\\n\\n'precise codeword'\\n\\n'The two established codeword maps are then utilized in a two-step matching, i.e. coarse matching and precise matching. The process is as follows.'\\n\\n'First, correspondence is built between left coarse codeword map\\\\n\\u03d5\\\\nc\\\\n|l\\\\nand right coarse codeword map\\\\n\\u03d5\\\\nc\\\\n|r\\\\n. For each pixel\\\\n(\\\\nu\\\\n|l\\\\n, \\\\nv\\\\n|l\\\\n)\\\\non the left codeword map\\\\n\\u03d5\\\\nc\\\\n|l\\\\n, a pixel\\\\n(\\\\nu\\\\n|r\\\\n, \\\\nv\\\\n|r\\\\n)\\\\nfrom the right camera is regarded as one of the corresponding candidates of the pixel\\\\n(\\\\nu\\\\n|l\\\\n, \\\\nv\\\\n|l\\\\n)\\\\nif it satisfies\\\\n|\\\\n\\u03d5\\\\nc\\\\n|l\\\\n(\\\\nu\\\\n|l\\\\n, \\\\nv\\\\n|l\\\\n)\\u2212\\\\n\\u03d5\\\\nc\\\\n|r\\\\n(\\\\nu\\\\n|r\\\\n, \\\\nv\\\\n|r\\\\n)|<\\u03be\\\\n, where\\\\n\\u03be\\\\nis a user defined small constant. Here we denote the set of all the corresponding candidates as\\\\n(\\\\nU\\\\n|r\\\\n, \\\\nv\\\\n|r\\\\n)\\\\n, as shown in Fig. 2(c). Since the coarse codeword encodes the whole image uniquely, at most one group of connected pixels can be found as the corresponding candidates for each pixel from left camera. This step is called coarse matching, as shown Fig. 2(b) and (c).'\\n\\n'The captured images and generation of codeword maps. (a)-(e) The acquired images. (f) Coarse codeword map generated from images (a)-(b). (g) Precise codeword map obtained from images (c)-(e). (h) Phases of a row of the codeword map (f). (i) Phases of a row of the codeword map (g). An arbitrary point (marked with red star) has two codewords: the coarse codeword can be obtained from (h), the precise codeword can be obtained from (i).'\\n\\n'Unique codeword should be assigned to each pixel.'\\n\\n'Phase-shifting fringes are widely used in pattern codification strategies because merely three patterns can uniquely encode the image. In addition, the phase-shifting fringes have high encoding accuracy and robustness to the background noises [13]. In this work, we utilize phase-shifting patterns of two frequencies wc and\\\\nw\\\\np\\\\n(\\\\nw\\\\np\\\\n\\u226b\\\\nw\\\\nc\\\\n)\\\\nto generate the coarse codeword and precise codeword, respectively. The projected patterns are described by'\\n\\n'The coarse codeword\\\\n\\u03d5\\\\nc\\\\n(u, v)\\\\nis designed to encode the whole image. However, since an arctangent function is used in the Eq. (6), the obtained phases have the problem of\\\\n2\\u03c0\\\\ndiscontinuity, as shown in Fig.3 (h). The\\\\n2\\u03c0\\\\ndiscontinuity will lead to ambiguity in the correspondence establishment. To prevent the ambiguity, we introduce the geometry constraint, as illustrated in the Fig. 4. Suppose the measured range in the depth direction\\\\n[\\\\nH\\\\nmin\\\\n, \\\\nH\\\\nmax\\\\n]\\\\nis known, then for any pixel on the left image whose u coordinate is\\\\nu\\\\n|l\\\\n, the u coordinate of its corresponding pixel on the right image will always locate in a closed interval\\\\n[\\\\nu\\\\nmin|r\\\\n, \\\\nu\\\\nmax|r\\\\n]\\\\n. The relationship is given by'\\n\\n'The precise codeword is designed to encode the corresponding candidates decided by the coarse codeword. To assign each candidate with unique phase value, the period of the precise phase Tp should be larger than the maximum range of the u coordinates of the candidates, which is\\\\n\\u03be\\\\nT\\\\nc\\\\n\\/\\u03c0\\\\n. Therefore, it is reasonable to choose the period of precise codeword as ceil\\\\n(\\u03be\\\\nT\\\\nc\\\\n\\/\\u03c0)\\\\n.'\\n\\n'Results of experiments conducted on a plate. (a)(b)(c) The reconstructed surface using gray code method, the proposed method without correspondence refinement, and the proposed method with correspondence refinement, respectively. (d)(e)(f) Cross-sections of the three surface and the fitted lines, respectively.'\\n\\n'In the precision test, comparative experiments are conducted using different methods: the gray code [25] which is one of the most accurate 3D surface reconstruction methods [13], the proposed two-step matching scheme without correspondence refinement, and the proposed scheme with correspondence refinement. The reconstructed surfaces are illustrated in Fig.\\\\n7(a)\\u2212(c)\\\\nand the supporting video. The numbers of reconstructed points are 159870, 163743, and 163730, respectively. For the method without refinement, the reconstructed result presents step-like surface. Because the correlation points are considered to locate exactly on the pixels of the image, the disparities are discrete integers, and the resultant depth are discrete values. Cross-sections of the three surface are shown in Fig.\\\\n7(d)\\u2212(D\\\\n, whose rootmean-square errors (RMSEs) are 0.0128 mm, 0.0229 mm and 0.0078 mm, respectively. The results show that through the correspondence refinement, the reconstruction precision has been largely improved. Besides, the proposed scheme can achieve better performance in precision compared with the gray code method.'\\n\\n'1) Coarse Codeword.'\\n\\n'2) Precise Codeword:'\",\"626\":null,\"627\":null,\"628\":\"\\\"Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors' knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.\\\"\\n\\n'Current state-of-the-art depth estimation algorithms rely on deep learning based methods, and while these achieve significant improvement in accuracy, they do so at the cost of increased computational complexity. Prior research on designing fast and efficient networks has primarily focused on encoder networks for tasks such as image classification and object detection [1]. In these applications, the input is an image (pixel-based), and the output is reduced to a label (an object class and position). To the best of our knowledge, little effort has been put into the efficient design of both encoder and decoder networks (i.e., auto-encoder networks) for tasks such as depth estimation, where the output is a dense image. In particular, reducing decoder complexity poses an additional challenge since there is less information reduction at each of the decoding layers and the decoder\\u2019s output is high dimensional.'\\n\\n'To address these challenges, this paper presents a low latency, high-throughput, high-accuracy depth estimation algorithm running on embedded systems. We propose an efficient encoder-decoder network architecture with a focus on low latency design. Our approach employs MobileNet [2] as an encoder and nearest neighbor interpolation with depthwise separable convolution in the decoder. We apply state-of-the-art network pruning, NetAdapt [3], and use the TVM compiler stack [4] to further reduce inference runtime on a target embedded platform. We show that our low latency network design, FastDepth, can perform real-time depth estimation on the NVIDIA Jetson TX2 [5], operating at over 120 frames per second (fps) on the TX2 GPU (see Figure 1) and at over 25 fps on the TX2 CPU only1, with active power consumption under 10 W. The attained throughput is an order of magnitude higher than prior work on depth estimation, with only a slight loss of accuracy; FastDepth achieves\\\\na\\\\n\\u03b4\\\\n1\\\\naccuracy 2 of 77.1% on NYU Depth v2.'\\n\\n'However, most previous work in this space has focused on encoder networks that reduce an input image into a label. Designing efficient neural networks for applications requiring pixel-based results, where an encoder network is followed by a decoder network, has been less explored. As will be shown in Figure 3, in existing designs that achieve close to state-of-the-art accuracy on the depth estimation task, the decoder largely dominates inference runtime. In our work, we emphasize efficient encoder-decoder network design. In particular, our usage of depthwise separable convolution in the decoder differentiates us from existing approaches and enables us to develop an architecture in which the decoder no longer dominates inference runtime.'\\n\\n'Our proposed fully convolutional encoder-decoder architecture is shown in Figure 2. The encoder extracts high-level low-resolution features from the input image. These features are then fed into the decoder, where they are gradually upsampled, refined, and merged to form the final high-resolution output depth map. In developing a depth estimation network that can run in real-time, we seek low-latency designs for both the encoder and the decoder.'\\n\\n'1) Encoder Network:'\\n\\n'The encoder used in depth estimation networks is commonly a network designed for image classification. Popular choices include VGG-16 [27] and ResNet-50 [17] because of their strong expressive power and high accuracy. However, such networks also suffer from high complexity and latency, making them unsuitable for applications running in real-time on embedded systems.'\\n\\n'2) Decoder Network:'\\n\\n'The objective of the decoder is to merge and upsample the output of the encoder to form a dense prediction. A key design aspect of the decoder is the upsample operation used (e.g., unpooling, transpose convolution, interpolation combined with convolution).'\\n\\n'Our decoder network (termed NNConv5) consists of five cascading upsample layers and a single pointwise layer at the end. Each upsample layer performs\\\\n5\\u00d75\\\\nconvolution and reduces the number of output channels by 1 \\/2 relative to the number of input channels. Convolution is followed by nearest-neighbor interpolation that doubles the spatial resolution of intermediate feature maps. Interpolating afterconvolution instead of before lowers the resolution of feature maps processed by the convolutional layers. We use depthwise decomposition to further lower the complexity of all convolutional layers, resulting in a slim and fast decoder.'\\n\\n'Encoder networks typically contain many layers to gradually reduce the spatial resolution and extract higher-level features from the input. The output of the encoder into the decoder becomes a set of low resolution features in which many image details can be lost, making it more difficult for the decoder to recover pixel-wise (dense) data. Skip connections allow image details from high resolution feature maps in the encoder to be merged into features within the decoder; this helps the decoding layers reconstruct a more detailed dense output. Skip connections have been previously been used in networks for image segmentation such as U-Net [33] and DeeperLab [34], showing that they can be beneficial in networks producing dense outputs.'\\n\\n'We incorporate skip connections from the MobileNet encoder to the outputs of the middle three layers in the decoder. Feature maps are combined with via addition rather than concatenation, to avoid increasing the number of feature map channels processed by the decoding layers.'\\n\\n'Our proposed network architecture is fully convolutional and makes use of depthwise decomposition in both the encoder and the decoder. Depthwise separable convolutional layers are currently not yet fully optimized for fast runtime in commonly-used deep learning frameworks. This motivates the need for hardware-specific compilation to translate the complexity reduction achievable with depthwise layers into runtime reduction on hardware. We use the TVM compiler stack [4] to compile our proposed network design for deployment on embedded platforms such as the Jetson TX2.'\\n\\n'In this section, we present experiment results to demonstrate our approach. We first present an evaluation against existing work and then provide an ablation study of our design. We offer comparisons of various encoder and decoder options, analysing them based on accuracy and latency metrics. We also show that hardware-specific compilation reduces the runtime cost of the depthwise separable layers within our network and that network pruning helps improve the efficiency of both of the encoder and the decoder.'\\n\\n'Reduction in inference runtime achieved with our approach. Stacked bars represent encoder-decoder breakdown; total runtimes are listed above the bars. The row of\\\\n\\u03b4\\\\n1\\\\naccuracies listed at the bottom shows the impact of individual steps in our approach on accuracy. Relative to ResNet-50 with UpProj, our final model achieves 65 times speedup while maintaining accuracy.'\\n\\n'C. Ablation Study: Encoder Design Space'\\n\\n'A common encoder used in existing high-accuracy approaches [12, 37] is ResNet-50 [17]. Targeting lower encoder latency, we consider the smaller ResNet-18 [17] and MobileNet [2] as alternatives to ResNet-50. The last average pooling layer and fully connected layers are removed from the MobileNet and ResNet architectures. To make the encoders compatible with a fixed decoder structure, we append\\\\na1\\u00d71\\\\nconvolutional layer to the end of ResNet encoders, such that the output from all encoder variants has a consistent shape of\\\\n7\\u00d77\\\\nwith 1024 channels.'\\n\\n'We compare all three encoder options against each other in Table III. The reported runtimes are obtained by compiling and running the encoder networks in PyTorch. Runtimes for ResNet-50 and ResNet-18 are too high, even on the TX2 GPU, to achieve real-time speeds (i.e., above 30 fps) if these encoders are paired with decoders of similar latency. In comparison, MobileNet efficiently trades off between accuracy and latency, and has a noticeably lower GPU runtime. We therefore select MobileNet as our encoder.'\\n\\n'TABLE III Comparison of encoders. RMSE and\\\\n\\u03b4\\\\n1\\\\nare for encoder-decoder networks with the decoder fixed As NNConv5. All other metrics are for the encoder in isolation. Runtimes are measured on a TX2. Mobilenet is selected as best encoder option.'\\n\\n'D. Ablation Study: Decoder Design Space'\\n\\n'While encoders have been well characterized in deep learning applications, decoders have been less extensively explored, especially in the context of efficient network design. We consider several decoder design aspects: upsample operation, depthwise decomposition, and skip connections.'\\n\\n'We survey four ways of upsampling in the decoder. Their characteristics are listed below, and visual representations are shown in Figure 5:'\\n\\n'We implement four decoder variants using these upsample operations, keeping the structure fixed at 5 decoding layers with\\\\n1\\u00d71\\\\nconvolution at the end. Table IV compares the four decoders. UpProj is most complex, due to its larger number of convolutions per upsample layer. It achieves the highest\\\\n\\u03b4\\\\n1\\\\naccuracy but is the slowest. UpConv is less complex and faster than UpProj, but its CPU and GPU runtimes are still too slow for real-time processing. DeConv5 has an identical number of weights and MACs as UpConv and is noticeably faster on both the CPU and GPU. However, it can be prone to introducing checkerboard artifacts in its outputs [38], which helps explain its lower accuracy. NNConv5 achieves higher\\\\n\\u03b4\\\\n1\\\\naccuracy and lower RMSE than both UpConv and DeConv5, with a slightly lower GPU runtime. We therefore select NNConv5 as our decoder.'\\n\\n'TABLE IV Comparison of decoders. RMSE and\\\\n\\u03b4\\\\n1\\\\nare for encoder-decoder networks with a Mobilenet encoder. All other metrics are for the decoder in isolation. Runtimes are measured on a TX2. Nnconv5 is selected as best decoder option.'\\n\\n'After selecting MobileNet as our encoder and NNConv5 as our decoder, we observe that the runtime of our network is dominated by the decoder (see Figure 3 for the encoder-decoder breakdown). This motivates us to simplify our decoder even further. Similar to how depthwise decomposition lowers the complexity and latency in MobileNet, we now replace all convolutions within the decoder with depthwise separable convolutions.'\\n\\n'Table V shows that depthwise decomposition in the decoder lowers inference runtime on the GPU by almost half.9 However, as was the case with MobileNet, depthwise layers in the decoder result in a slight accuracy loss, due to the reduction in trainable parameters and computation. In order to restore some of the lost accuracy, we incorporate skip connections between the encoding and decoding layers.'\\n\\n'We consider both additive and concatenative skip connections. Concatenative skip connections increase the computational complexity of the decoder since decoding layers need to process feature maps with more channels. Table V shows that this improves the\\\\n\\u03b4\\\\n1\\\\naccuracy but also noticeably increases CPU and GPU runtimes. In contrast, using additive skip connections leaves the number of channels in the decoder unchanged and has a negligible impact on inference runtime while achieving almost the same accuracy boost. We therefore use additive skip connections in our final network design. As shown in Figure 4(d), skip connections noticeably improve the sharpness and visual clarity of the depth maps output by our network design.'\\n\\n'Current deep learning frameworks rely on framework-specific operator libraries, where the level of hardware-specific optimization of operator implementations may vary. Our proposed network architecture incorporates depthwise layers throughout the encoder and decoder. These layers are currently not yet fully optimized in commonly-used deep learning frameworks. As a result, although depthwise decomposition significantly reduces the number of MACs in a network, a similar reduction is not reflected in latency. The left portion of Table VI highlights exactly this: the TX2 CPU runtime of MobileNet-NNConv5 is high to begin with, due to the prevalence of depthwise layers in MobileNet, and it increases even more when we use depthwise layers in the decoder. To address the observed runtime inefficiencies of depthwise layers, we use the TVM compiler stack [4]. TVM performs hardware-specific scheduling and operator tuning that allows the impact of reduced operations to be translated into reduced processing time. The right portion of Table VI reports TX2 runtimes for networks compiled with TVM. Depthwise decomposition in the decoder now reduces CPU runtime by 3.5 times and GPU runtime by 2.5 times.'\\n\\n'Prior to network pruning, our architecute (MobileNet-NNConv5 with depthwise decomposition in the decoder and additive skip connections) already surpasses real-time throughput on the TX2 GPU but does not yet achieve real-time speeds on the TX2 CPU. Network pruning lowers the model\\u2019s runtime so that it can achieve a CPU framerate above 25 fps that is more suitable for real-time inference. As shown in Table VII, pruning achieves a 2 times reduction in MACs, a 1.5 times reduction in GPU runtime, and a 1.8 times reduction in GPU runtime with almost the same accuracy. Figure 4(e) shows that pruning process preserves the sharpness and visual clarity of output depth maps.'\\n\\n'Fig. 6 shows the pruned architecture. We can see that there are two bottlenecks: one in the encoder (the layer mobilenet.9) and one in the decoder (the layer decoder.2). This is consistent with the observations in [3, 39].'\\n\\n'TABLE VII Impact of Pruning on Our Encoder-Decoder Network. Runtimes are Measured Post-Compilation for the TX2.'\\n\\n'In this work, we enable high-speed depth estimation on embedded systems. We achieve high frame rates by developing an efficient network architecture, with a low-complexity and low-latency decoder design that does not dominate inference runtime even when combined with a small MobileNet encoder. The size of our compact model is further reduced by applying a state-of-the-art pruning algorithm. Hardware-specific compilation is used to translate complexity reduction into lower runtime on a target platform. On the Jetson TX2, our final model achieves runtimes that are an order of magnitude faster than prior work, while maintaining comparable accuracy.'\",\"629\":null,\"630\":null,\"631\":\"'We present an approach to safe physical Human-Robot Interaction (pHRI) for industrial robots, including collision detection, distinguishing accidental from intentional contacts, and achieving collaborative tasks. Typical industrial robots have a closed control architecture that accepts only velocity\\/position reference inputs, there are no joint torque sensors, and little or no information is available to the user on robot dynamics and on low-level joint controllers. Nonetheless, taking also advantage of the presence of a Force\\/Torque (F\\/T) sensor at the end-effector, a safe pHRI strategy based on kinematic information, on measurements from joint encoders and motor currents, and on end-effector forces\\/torques can be realized. An admittance control law has been implemented for collaboration in manual guidance mode, with whole-body collision detection in place both when the robot is in autonomous operation and when is simultaneously collaborating with a human. Several pHRI experiments validate the approach on a KUKA KR5 Sixx R650 robot equipped with an ATI F\\/T sensor.'\",\"632\":null,\"633\":null,\"634\":null,\"635\":\"'The procedure of handling TPC was implemented with the open source point cloud library that provides visualization and filtering of point cloud data. In this system, the 5mm voxel grid filtering was applied to the TPC in order to cancel the density bias [8], [9].'\",\"636\":null,\"637\":null,\"638\":null,\"639\":\"'The robotic inflation device is entirely managed by a program developed in Processing, an open-source Java-based programming language and integrated development environment. The program relies on the Firmata protocol, which is implemented in the firmware uploaded on the Arduino, to communicate with the microcontroller via USB cable.'\",\"640\":null,\"641\":null,\"642\":\"'An experimental platform, the Learning Cube, that is easy accessible and inexpensive, supported by open source software and open access guides.'\\n\\n'Our software is provided as open source [18].'\",\"643\":null,\"644\":\"'In addition to benchmarking ChainQueen\\u2019s performance and demonstrating its capabilities on a diverse set of inverse problems, we have interfaced our simulator with high-level python scripts to make ChainQueen user-friendly. Users at all levels will be able to develop their own soft robotics systems using our simulator, without the need to understand its low-level details. We will open-source our code and data and we hope they can benefit the robotics community.'\\n\\n'We have presented ChainQueen, a differentiable simulator for soft robotics, and demonstrated how it can be deployed for inference, control, and co-design. ChainQueen has the potential to accelerate the development of soft robots. We have also developed a high-performance GPU implementation for ChainQueen, which we plan to open source.'\",\"645\":null,\"646\":\"\\\"Overview of SpaceBok's control framework. The exact same control and state estimation code can be used for hardware and simulation tests.\\\"\",\"647\":\"'A custom motor controller with integrated magnetic encoder IC is located behind the rotor. The motor controller is designed for 24V nominal operation, 30A continuous and 40A peak current. The controller handles field oriented control of motor currents at a loop rate of 40 kHz and closed-loop bandwidth of up to 4.5 kHz, as well as position and velocity control if desired. The controller receives torque, position, velocity and gain commands, and returns position, velocity, and estimated torque over CAN bus at a rate of up to\\\\n4kHz\\\\n\\u2013\\\\n\\u2013\\\\n\\u2013\\\\n\\u2013\\\\n\\u2013\\\\n. For robots with many degrees of freedom, like the Mini Cheetah, multiple CAN networks can be used to keep communication rates high. In the case of the Mini Cheetah, one network is used per-leg, allowing communication between all joints and the control computer to happen at 1 kHz.'\\n\\n'To generate a back-flip trajectory, we used the open-source CasADi library [18] to set up a trajectory optimization problem to be solved by IPOPT [19]. The optimization was done on a 5-link sagittal plane model of the robot, which included five rigid links, as well as the effects of the rotors of the actuators. The 2D model was significantly faster than our fu113-D optimization, and captured the relevant dynamics for a backflip. We found it was not necessary to specify a nonzero cost function; simply allowing the optimization to find a feasible backflip given our constraints was sufficient. Some of the constraints in addition to the dynamics used in the optimization were:'\",\"648\":null,\"649\":\"'This paper presents Stanford Doggo, a quasi-direct-drive quadruped capable of dynamic locomotion. This robot matches or exceeds common performance metrics of state-of-the-art legged robots. In terms of vertical jumping agility, a measure of average vertical speed, Stanford Doggo matches the best performing animal and surpasses the previous best robot by 22%. An overall design architecture is presented with focus on our quasi-direct-drive design methodology. The hardware and software to replicate this robot is open-source, requires only hand tools for manufacturing and assembly, and costs less than $3000.'\\n\\n'Stanford Doggo: an open-source, quasi-direct-drive quadruped robot.'\\n\\n'In order to allow others to replicate Stanford Doggo we made the design open-source [20]. The project repository includes all CAD files, a detailed bill of materials, general assembly instructions, wiring schematics, and all necessary software. The total cost estimated to build Stanford Doggo is $3000. This includes estimated costs to have outsourced machining services perform any manufacturing beyond those possible with hand tools, such as a hand drill or soldering iron. In order to increase accessibility, all components and quoted machining services are available online. The overall dimensions of the Stanford Doggo chassis are 42cm in length, 20cm in width, and 14cm in height. The minimum and maximum leg extensions, measured from the foot to the center of rotation are 8cm and 25cm, respectively.'\\n\\n'In this work we introduce Stanford Doggo, a robot that merges the dexterity and inherent stability of quadruped robots with a vertical jumping agility greater than specialized monopods and matching that of the highest performing animal, the galago. Stanford Doggo also meets or exceeds state-of-the-art legged robotic systems in a number of common performance metrics. The complete design is open-source with a focus on low manufacturing cost, less than $3000, and requires only hand tools for assembly [20]. In making an accessible, state-of-the-art legged robot platform, we hope to improve research and education in legged robotics by lowering the barriers to entry. We plan to continue the open-source development to improve the design and closed-loop gait controllers. In the future, we intend to explore additional control schemes to fully utilize Stanford Doggo\\u2019s extreme mobility.'\",\"650\":\"'https:\\/\\/goo.gl\\/jP5TnL'\\n\\n'https:\\/\\/goo.gl\\/jP5TnL'\\n\\n'To validate our approach, we performed hardware experiments using an 18-degree-of-freedom hexapod robot. The robot features a versatile mounting point on the front of the body, which allows for the attachment of a camera. The joint-modules themselves fully provide the robot\\u2019s on-board sensing capabilities; each contains an inertial measurement unit (IMU) and encoders [20].'\",\"651\":null,\"652\":null,\"653\":null,\"654\":\"'The lack of publicly-available code and datasets makes the evaluation of non-rigid reconstruction algorithms challenging. Creating a reliable ground-truth reconstruction is inherently difficult as it requires specialised, costly devices such as motion capture systems. In contrast, synthetic datasets contain reliable ground-truth, but often have unrealistic inputs that do not simulate real-world conditions accurately.'\\n\\n'To the best of our knowledge, no implementation of any real-time non-rigid reconstruction system is publicly available. We contribute an open-source implementation of DynamicFusion [45], the first real-time non-rigid reconstruction algorithm, to serve as a baseline for future evaluation and benchmarking.'\\n\\n'We have presented a benchmarking suite that goes beyond traditional SLAM algorithms and integrates relevant algorithms, datasets and metrics for evaluating related problems. SLAMBench 3.0 provides means for evaluating semantic segmentation, depth estimation and non-rigid reconstruction in the context of SLAM. We hope that providing open-source implementations of some of the most recent methods in an unified framework, as well as the first public implementation of a non-rigid real-time reconstruction system will help researchers in performing comparative studies and evaluating their work. In total, SLAMBench 3.0 contains 6 new metrics, 4 new datasets and 5 new algorithms. In future, we plan to include trajectory difficulty metrics such as the metrics described in [54] and [55], and also to extend this paper to include systems that use multiple cameras [56].'\",\"655\":\"'Accurate relative pose is one of the key components in visual odometry (VO) and simultaneous localization and mapping (SLAM). Recently, the self-supervised learning framework that jointly optimizes the relative pose and target image depth has attracted the attention of the community. Previous works rely on the photometric error generated from depths and poses between adjacent frames, which contains large systematic error under realistic scenes due to reflective surfaces and occlusions. In this paper, we bridge the gap between geometric loss and photometric loss by introducing the matching loss constrained by epipolar geometry in a self-supervised framework. Evaluated on the KITTI dataset, our method outperforms the state-of-the-art unsupervised egomotion estimation methods by a large margin. The code and data are available at https:\\/\\/github.com\\/hlzz\\/DeepMatchVO.'\",\"656\":null,\"657\":\"'Location-aware applications play an increasingly critical role in everyday life. However, satellite-based localization (e.g., GPS) has limited accuracy and can be unusable in dense urban areas and indoors. We introduce an image-based global localization system that is accurate to a few millimeters and performs reliable localization both indoors and outside. The key idea is to capture and index distinctive local keypoints in ground textures. This is based on the observation that ground textures including wood, carpet, tile, concrete, and asphalt may look random and homogeneous, but all contain cracks, scratches, or unique arrangements of fibers. These imperfections are persistent, and can serve as local features. Our system incorporates a downward-facing camera to capture the fine texture of the ground, together with an image processing pipeline that locates the captured texture patch in a compact database constructed offline. We demonstrate the capability of our system to robustly, accurately, and quickly locate test images on various types of outdoor and indoor ground surfaces. This paper contains a supplementary video. All datasets and code are available online at microgps.cs.princeton.edu.'\\n\\n'Describing a low-cost global localization system based on ground textures and making relevant code and instructions available for reproduction.'\",\"658\":null,\"659\":null,\"660\":null,\"661\":\"'The controller presented in this paper has been implemented using the firmware from the open source project, ArduCopter3 on a PixHawk 2 flight controller4. A custom flight mode was derived to execute our control algorithm and the position estimation algorithm described in real-time. The attitude of the quadrotor is controlled by a cascaded PID controller5, which tracks the control inputs in (23) and (24) that are produced by the IBS controller. Also, the heading-lock controller of the ArduCopter firmware is used as is.'\",\"662\":null,\"663\":\"'http:\\/\\/www.uscamsl.com\\/resources\\/ICRAl9sl.mp4'\",\"664\":\"'The DJI On-board SDK (OSDK) is an open source software library which enables the DJI Manifold to handle Input-Output data coming from the on-board control unit as well as the sensor units. The on-board SDK includes:'\",\"665\":null,\"666\":null,\"667\":\"'https:\\/\\/github.com\\/idsia-robotics\\/proximity-quadrotor-learning'\\n\\n'Videos, data, and code to reproduce our results are available at: https:\\/\\/github.com\\/idsia-robotics\\/proximity-quadrotor-learning.'\\n\\n'https:\\/\\/github.com\\/idsia-robotics\\/proximity-quadrotor-learning'\\n\\n'Videos, Datasets, and Code'\",\"668\":null,\"669\":null,\"670\":\"'We use the PowerBot from Omron Adept Mobile Robots as our differential drive mobile base. The robot is equipped with multiple sensors: a Velodyne VLP-16 3D LiDAR sensor; a SICK LMS-200 2D laser sensor; a RealSense RGBD sensor, and GPS and IMU sensors. These sensors are both simulated in Gazebo and equipped on the real robot. In simulation we use the Velodyne to track both the curb and pedestrians. However, in physical testing the Velodyne is only used for curb detection and the RGB-D sensor is used for pedestrian tracking. To track people with the Velodyne in simulation, we compress its pointcloud into a 2D scan. This scan is input to an open source leg tracker [20] whose output is then fed through the people tracking pipeline from the SPENCER project [13], [21]. In our simulated testing we have found that occasionally round street poles are also detected as legs through this method. To reduce these false positives, we switched to an RGB-D-based upper body detector provided in the SPENCER project for testing within the real robot. The output of the RGB-D sensor is fed through the same people tracking pipeline from the SPENCER project. Group tracking results from SPENCER are used for our group surfing algorithm. The SPENCER group tracker models the evolution of groups through time such as forming, splitting, and merging by considering social relations among people [22].'\",\"671\":null,\"672\":null,\"673\":\"'https:\\/\\/www.intel.com\\/content\\/www\\/us\\/en\\/products\\/boards-kits%20\\/nuc.html'\",\"674\":\"'https:\\/\\/www.youtube.com\\/playlist?list=PLoxxUaKK06F3BVvei-o5plxilxWXQ6Lvi'\\n\\n'Note that our code is almost based on OpenCV 2.4.13 and LIBELAS [10] with little modifications in the multi-thread and pipeline techniques without using GPU. If the size of stitched images is resized to 30% of the original size, the same code can run in about 12fps on a server with an Intel Xeon E5 CPU (i.e. the server used for acquiring data).'\",\"675\":null,\"676\":\"'Development of the Method Local Navigation of Mobile Robot a Based on the Tags with QR Code and Wireless Sensor Network'\",\"677\":\"'(A) Visualization of the tracking system with 30-second track plotted, including annotations of track components, where \\u201cNC\\u201d stands for Non-Causal, Pf\\u201d for Particle Filter. (B) Non-causally estimated 3D track of Agent 2 in second deployment, color-coded by depth.'\",\"678\":null,\"679\":\"'https:\\/\\/github.com\\/chrisdxie\\/reminiscent-tracker'\\n\\n'https:\\/\\/github.com\\/chrisdxie\\/reminiscent-tracker'\",\"680\":\"\\\"In this work we present an articulated tracking approach for robotic manipulators, which relies only on visual cues from colour and depth images to estimate the robot's state when interacting with or being occluded by its environment. We hypothesise that articulated model fitting approaches can only achieve accurate tracking if subpixel-level accurate correspondences between observed and estimated state can be established. Previous work in this area has exclusively relied on either discriminative depth information or colour edge correspondences as tracking objective and required initialisation from joint encoders. In this paper we propose a coarse-to-fine articulated state estimator, which relies only on visual cues from colour edges and learned depth keypoints, and which is initialised from a robot state distribution predicted from a depth image. We evaluate our approach on four RGB-D sequences showing a KUICA LWR arm with a Schunk SDH2 hand interacting with its environment and demonstrate that this combined keypoint and edge tracking objective can estimate the palm position with an average error of 2. 5cm without using any joint encoder sensing.\\\"\\n\\n'The combination of these stages makes our proposed tracking approach independent from joint encoder sensing and consecutively refines the state from the initially sampled configuration via keypoint tracking until the basin of convergence for pixel-accurate edge correspondences is reached.'\\n\\n'In summary, our proposed tracking approach is able to reliably track an occluded manipulator in grasping scenes, without making any assumptions on the presence of objects or the availability of joint encoder readings. This solves a common problem of articulated tracking approaches, which often need to be initialised from a known robot state. Our approach is therefore more generally applicable to scenarios where direct access to the robot is not available.'\",\"681\":null,\"682\":\"'It is a significant problem to predict the 2D LiDAR map at next moment for robotics navigation and path-planning. To tackle this problem, we resort to the motion flow between adjacent maps, as motion flow is a powerful tool to process and analyze the dynamic data, which is named optical flow in video processing. However, unlike video, which contains abundant visual features in each frame, a 2D LiDAR map lacks distinctive local features. To alleviate this challenge, we propose to estimate the motion flow based on deep neural networks inspired by its powerful representation learning ability in estimating the optical flow of the video. To this end, we design a recurrent neural network based on gated recurrent unit, which is named LiDAR-FlowNet. As a recurrent neural network can encode the temporal dynamic information, our LiDAR-FlowNet can estimate motion flow between the current map and the unknown next map only from the current frame and previous frames. A self-supervised strategy is further designed to train the LiDAR-FlowNet model effectively, while no training data need to be manually annotated. With the estimated motion flow, it is straightforward to predict the 2D LiDAR map at the next moment. Experimental results verify the effectiveness of our LiDAR-FlowNet as well as the proposed training strategy. The results of the predicted LiDAR map also show the advantages of our motion flow based method.'\",\"683\":null,\"684\":\"'Most existing controllers for Car-Like Mobile Robots (CLMR) are designed to handle dynamic effects by decoupling speed and steering controls, also assume that full states are accessible, which are unrealistic for real-world applications. This paper presents a combined speed and steering control system for CLMR. To provide the essential state for the controller, a newly developed visual algorithm is adopted for estimating the high-update rate longitudinal and lateral velocities of the robot which cannot be accurately measured by wheel encoders due to the skidding and slipping effects. The stability of the proposed system can be guaranteed by Lyapunov method since the velocity estimation error, the speed tracking error and the lateral deviation converging to zero simultaneously. Real-world experiments are conducted on an electric autonomous tractor with online estimation to demonstrate the feasibility of the approach.'\",\"685\":\"'Our electrical vehicles are equipped with a 16-beam lidar (Velodyne VLP-16) and six fixed lens gigabit multimedia serial link (GMSL) cameras to provide a 360-degree view. Each camera has a 100-degree of field of view. The camera images have a resolution of 1920 X 1208 and a frame rate of 30 Hz. The extrinsic camera calibration is calculated relative to the lidar sensor frame, and both are registered to the local frame of reference of the vehicle. Further, the platform also contains wheel encoders and an IMU containing gyroscopes, accelerometers and magnetometers.'\",\"686\":\"'TABLE I Code of Sea State [14]'\",\"687\":null,\"688\":null,\"689\":null,\"690\":null,\"691\":null,\"692\":\"'In autonomous driving community, numerous benchmarks have been established to assist the tasks of 3D\\/2D object detection, stereo vision, semantic\\/instance segmentation. However, the more meaningful dynamic evolution of the surrounding objects of ego-vehicle is rarely exploited, and lacks a large-scale dataset platform. To address this, we introduce BLVD, a large-scale 5D semantics benchmark which does not concentrate on the static detection or semantic\\/instance segmentation tasks tackled adequately before. Instead, BLVD aims to provide a platform for the tasks of dynamic 4D (3D+temporal) tracking, 5D (4D+interactive) interactive event recognition and intention prediction. This benchmark will boost the deeper understanding of traffic scenes than ever before. We totally yield 249, 129 3D annotations, 4, 902 independent individuals for tracking with the length of overall 214, 922 points, 6, 004 valid fragments for 5D interactive event recognition, and 4, 900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime). The benchmark can be downloaded from our project site https:\\/\\/github.com\\/VCCIV\\/BLVD\\/.'\",\"693\":null,\"694\":\"'We now focus on discussing the relevance of the Wasserstein distance for characterizing motion in visual SLAM. The Wasserstein distance takes into account both geometrical properties, as encoded in d(x, y), and also the probabilistic structure by integration over\\\\n\\u0398(p,q)\\\\n. Taking the infimum yields a single non-negative number which quantifies the optimal transport plan between two distributions p and q. In the context of visual SLAM, one could think of p and q as two probability distributions over desired quantities, such as poses, measurements and\\/or landmarks, which are consecutive in time. Hence the argument is that, for two such consecutive distributions p and q, higher value of Wasserstein distance would be associated with a higher discrepancy between p and q, and hence harder to characterize motion for such scenario. There are plenty of options when it comes to choosing divergences and metrics between two probability measures, whose appropriateness is often dictated by the application context, computational tractability and other mathematical properties. One such example is the Kullback-Leibler (KL) divergence, which is popular in the fields of Statistics, Machine Learning and Information Theory, and has previously been considered in [13] in the context of motion characterization. Even though both Wasserstein metric and KL divergence have convenient closed-form solutions under Gaussian distributions, we note that the KL divergence is not symmetric unlike the Wasserstein distance, and hence does not translate to plausible physical meaning given the symmetric nature of motion in time. Our initial experiments have also considered the Jensen-Shannon (JS) divergence, due to being symmetric, but since this quantity is bounded above by a constant value, it was not appropriate for comparing motion change across different trajectories and environments. The reader is referred to [14], Example 1 from Section 2, for a non-Gaussian example where the Wasserstein distance provides a reasonable answer, in contrast with the KL divergence and JS divergence.'\",\"695\":null,\"696\":\"'http:\\/\\/rpg.ifi.uzh.ch\\/uzh-fpv'\\n\\n'http:\\/\\/rpg.ifi.uzh.ch\\/uzh-fpv'\",\"697\":\"'Sketching the inner computations of the pseudo-code for\\\\nm=3\\\\nand\\\\nj=3\\\\nleads to'\\n\\n'This paper has provided a practical implementation of the minimum effort problem to compute dynamically feasible robot motions. We described in detail the computation of B-Splines to discretize the continuous optimal control problem. Also, we explain the structure and consistent dimensions of some involved geometric operators that appear when the inverse dynamics is derived with respect to the control points. The geometric formulation of robot dynamics combined with B-Splines are powerful tools to compute optimized robot motions without making use of finite differences or automatic differentiation. We provided some comparisons with available open-source trajectory optimization methods in Matlab with the same nonlinear programming method to evaluate the numerical sensitivity and convergence time. Geometric algorithms can also be extended to compute the forward dynamics as it has been suggested recently in [13] with Featherstone\\u2019s spatial operators.'\",\"698\":null,\"699\":null,\"700\":null,\"701\":null,\"702\":null,\"703\":null,\"704\":\"'The Actuator Module is designed to provide strong power and to be able robust torque control. A specially designed torque sensor, a high specific torque density BLDC motor, a zero-backlash reducer, and a high resolution absolute encoder are all embedded for modular design and easy maintenance.'\\n\\n'Exploded view of Actuator Module consists of CJTS, gear transmission, BLDC motor, high-resolution absolute encoder (18-bits), incremental encoder and signal processing PCB module.'\",\"705\":null,\"706\":null,\"707\":null,\"708\":null,\"709\":null,\"710\":null,\"711\":null,\"712\":null,\"713\":\"'In this paper, we propose a novel dense surfel mapping system that scales well in different environments with only CPU computation. Using a sparse SLAM system to estimate camera poses, the proposed mapping system can fuse intensity images and depth images into a globally consistent model. The system is carefully designed so that it can build from room-scale environments to urban-scale environments using depth images from RGB-D cameras, stereo cameras or even a monocular camera. First, superpixels extracted from both intensity and depth images are used to model surfels in the system. superpixel-based surfels make our method both runtime efficient and memory efficient. Second, surfels are further organized according to the pose graph of the SLAM system to achieve O(1) fusion time regardless of the scale of reconstructed models. Third, a fast map deformation using the optimized pose graph enables the map to achieve global consistency in real-time. The proposed surfel mapping system is compared with other state-of-the-art methods on synthetic datasets. The performances of urban-scale and room-scale reconstruction are demonstrated using the KITTI dataset [1] and autonomous aggressive flights, respectively. The code is available for the benefit of the community.'\",\"714\":\"'The speed and accuracy with which robots are able to interpret natural language is fundamental to realizing effective human-robot interaction. A great deal of attention has been paid to developing models and approximate inference algorithms that improve the efficiency of language understanding. However, existing methods still attempt to reason over a representation of the environment that is flat and unnecessarily detailed, which limits scalability. An open problem is then to develop methods capable of producing the most compact environment model sufficient for accurate and efficient natural language understanding. We propose a model that leverages environment-related information encoded within instructions to identify the subset of observations and perceptual classifiers necessary to perceive a succinct, instruction-specific environment representation. The framework uses three probabilistic graphical models trained from a corpus of annotated instructions to infer salient scene semantics, perceptual classifiers, and grounded symbols. Experimental results on two robots operating in different environments demonstrate that by exploiting the content and the structure of the instructions, our method learns compact environment representations that significantly improve the efficiency of natural language symbol grounding.'\\n\\n'Commanding mobile robot movement based on natural language processing with RNN encoder\\\\xaddecoder'\",\"715\":null,\"716\":\"'At each timestep, the model receives an LSTM encoded language vector\\\\nL\\\\n\\u20d7 \\\\n, the initial world state W0 , and the current world state Wt . Using these, it predicts the next world state\\\\nW\\\\n^\\\\nt+1\\\\nand sub-goal Gt .'\\n\\n'The prediction cell is a simple autoencoder mapping inputs W to and from a learned latent space, as show in Fig. 2. World observations Wt and W0 are combined into a single estimated latent state zt. The vector containing the predicted subgoal Gt is tiled onto this state. We use a bottleneck within each prediction cell to force information to propagate across the entire predicted image, and then estimate a change in latent state\\\\n\\u25b3z\\\\nsuch that\\\\nz\\\\n^\\\\nt+1\\\\n=\\\\nz\\\\nt+\\u25b3z\\\\n.'\\n\\n'We train the encoder and decoder jointly when training the Prediction and Actor modules and optimize with Adam [30], using an initial learning rate of 1 e-3. We fix the latent state encoder and decoder functions after this step, then use the learned hidden space to train the Subgoal module.'\",\"717\":\"'Commanding mobile robot movement based on natural language processing with RNN encoder\\\\xaddecoder'\\n\\n'We required an autonomous drone that can localize itself in an environment. We chose the PiDrone as our robotic platform because it is a open-source system that is fully customizable [6]. The drone is equipped with one downward-facing camera. We also implemented localization with a particle filter (Monte Carlo localization) [39]. The drone flies over a highly-textured planar surface, and we use the OpenCV library [5] to extract and detect Oriented FAST and Rotated BRIEF (ORB) features. Each frame from the camera, along with the altitude value from the infrared sensor, are used to compute the bearing and the distance from the last frame and to update the weights of particles [39]. The general goal is to keep track of the drone\\u2019s position constantly, and to match features from the current frame with features from the current estimated position from the map to update the location of the drone based on the matched features. The drone will keep sending the current position to the Mixed Reality system and the base station via ROS. Although feature-based localization might not be as precise as the OptiTrack motion tracking system, it simulates the uncertainty of the real environment, and can be changed to ORB-SLAM [29] with a high-performance drone in a complex environment. For faster planning and language grounding to specific discrete areas of the environment, we chose to discretize our environment by creating a grid of cells where each cell is 50 \\u00d7 50 \\u00d7 30 centimeters (width, length, height).'\",\"718\":null,\"719\":\"'Commanding mobile robot movement based on natural language processing with RNN encoder\\\\xaddecoder'\",\"720\":null,\"721\":\"'This research was supported by funding from Open Philanthropy, AFOSR, and an NSF Career Award. We thank Andrea Bajcsy for insightful discussion and sharing code. We would also like to thank all members of the members of the InterACT lab for helpful feedback.'\",\"722\":null,\"723\":null,\"724\":\"'The dynamic motion primitive (DMP) exploits the stable point-attractor dynamics of a spring-mass system to encode motion data [8]. The stable dynamics naturally enable generalization in new contexts, such as on-line trajectory adaptation and obstacle avoidance [7], [8], [18]. DMPs have been used to teach various skills to LfD-powered robots, e.g. locomotion [13], T-ball batting [16], playing tic-tac-toe [14], and drumming [6]. DMPs can encode joint trajectory motion from a single demonstration. The phase variable, however, in the DMP is a function of time, which implicitly makes DMP a time dependent approach. Accordingly, the issue of temporal re-indexing, as discussed earlier in this section, occurs when reproducing trajectories learned with DMPs. Another critical limitation of DMP in learning trajectories is that DMP only considers the temporal constraint of the task (i.e., it tries to reach the goal within a specified time). There are many tasks, however, that require other kinematic constraints to be satisfied. For example, learning to throw a ball a specific distance requires a specific velocity and position combination. Modifications of the original DMP formulations have been proposed to model such behavior [12], [11]. These modifications, however, will fail if a trajectory needs to execute multiple instances of such behavior.'\",\"725\":\"'https:\\/\\/ciumonk.github.io\\/RobotCar-rainy\\/'\\n\\n'https:\\/\\/ciumonk.github.io\\/RobotCar-rainy\\/video.html'\\n\\n'A proto-raindrop is created using a simple refractive model that assumes a pinhole camera. The refraction angle is encoded following a scheme similar to normal mapping [29] by using a 2D look-up table represented by the RED and GREEN channels of a texture T, with the thickness of the drop encoded in the BLUE channel of the same texture. This texture T is then masked using an alpha layer that allows blending of the water drops with the background image and other drops, as shown in Figure 3a. With the drop acting as a simple lens, the coordinate\\\\n(\\\\nx\\\\nr\\\\n,\\\\ny\\\\nr\\\\n)\\\\nof the world point that is rendered at the location\\\\n(u,v)\\\\non the surface of a drop is given by the following simplified distortion model:'\\n\\n'https:\\/\\/ciumonk.github.io\\/RobotCar-rainy\\/'\\n\\n'https:\\/\\/ciumonk.github.io\\/RobotCar-rainy\\/video.html'\",\"726\":\"'https:\\/\\/github.com\\/PRBonn\\/bonnet'\\n\\n'The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.'\\n\\n'Therefore, we see the need for a tool that allows a developer to easily train and deploy semantic segmentation networks for robotics. Such a tool should allow developers to easily add new research approaches into the robotic system while avoiding the effort of re-implementing them from scratch or modifying the available code until it becomes at least marginally usable for the research purpose. This is something that we experienced ourselves and observed in the community too often.'\\n\\n'The contribution of this paper is a stable, easy to use, software tool with a modular codebase which implements semantic segmentation using CNNs. It solves training and deployment on a robot. Thus, we do not propose a new CNN approach here. Instead, we provide a clean and extensible implementation to make this technology easily usable in robotics and to enable a larger number of people to use CNNs for semantic segmentation on their robots. We strongly believe that our tool allows the scientific robotics community to save time on the CNN implementations, enabling researchers to spend more time to focus on how such information can aid robot perception, localization, mapping, path planning, obstacle avoidance, manipulation, safe navigation, etc. We show this with different example use cases from the community, where robotics researchers with no expertise in deep learning were able to, using Bonnet, train and deploy semantics in their systems with minimal effort. Bonnet relies on TensorFlow for our graph definition and training, but provides the possibility of using different backends with a clean and stable C++ API for deployment. It allows for the possibility to transparently exploit custom hardware accelerators that become commercially available, without modifying the robotics codebase.'\\n\\n'One of the pioneers in efficient feed-forward encoder-decoder approaches to semantic segmentation is Segnet [4]. It uses an encoder based on VGG16 [31], and a symmetric decoder outputting a semantic label for each pixel of the input image. The decoder uses the encoder pooling indexes to perform the unpooling to recover some of the lost spatial resolution during pooling. Segnet is available as a Caffe implementation and has pre-trained weights for several datasets. U-Net [28], which was released contemporaneously, exploits the same encoder-decoder architecture but uses a decoder concatenation of the whole encoder feature map instead of sharing pooling indexes. This allows for more accurate decision boundaries, which comes at a higher computational and memory cost. U-Net is available as an implementation in a modified Caffe version and provides pre-trained weights for a medical dataset. PSP-Net [36] uses ResNet [12] as the encoder, and exploits global information through a pyramid of average-pooling layers after the latter, to provide more accurate semantics based on the environment of the image objects. PSP-Net is also available as a modified Caffe implementation and comes with pre-trained weights from different scene parsing datasets. All of these architectures are based on encoders such as VGG and ResNet, which focus on accuracy of the predictions rather than the execution speed for a near real-time application in robotics.'\\n\\n'Example of an encoder-decoder semantic segmentation CNN implemented in Bonnet. It is based on the non-bottleneck idea behind ERFNet [27]. Best viewed in color.'\\n\\n'Abstraction of the codebase. Python interface is used for training and graph definition, and C++ library can use a trained graph and infer semantic segmentation in any running application, either linking it or by using the ROS node. Both interfaces communicate through the four configuration files in yaml format and the trained model weights.'\\n\\n'Once the dataset is properly parsed into the standard format, the CNN architecture has to be defined. We provide three sample architectures and provide pre-trained weights for different datasets, and different network sizes, depending on the complexity of the problem. Other network architectures can be easily added, given the modular structure of our codebase, and it is the main purpose of the tool to allow the implementation of new architectures as they become available. For this, the user can simply create a new architecture file, which inherits the abstract Network class, and define the graph using our library of layers. If a novel layer needs to be added, it can be implemented using TensorFlow operations. The abstract class Network, see Fig. 3, contains the definition of the training method that handles the optimization through stochastic gradient descent, inference methods to test the results, metrics for performance assessment, and the graph definition method, which each architecture overloads in order to define different models. If a new architecture requires a new metric or a different optimizer, these can be modified simply by overloading the corresponding method of the abstract class. The interface with the model architecture is done through the net.yaml configuration file, which includes the selection of the architecture, the number of layers, number of kernels per layer, and some other architecture dependent hyperparameters such as the amount of dropout [13], and the batch normalization [14] decay.'\\n\\n'C++ code showing simplicity of semantic segmentation CNN inference in C++ application, using Bonnet tool as a library.'\\n\\n'Since Bonnet is meant to serve as a general starting point to implement different architectures, we advise referring to the code in order to have an up-to-date measure of the latest architecture design performances.'\\n\\n'Most methods, which represent the current state of the art in semantic segmentation, use fully convolutional neural networks. The success of neural networks for many tasks from machine vision to natural language processing has triggered the availability of many high-quality open-source development and training frameworks such as TensorFlow [1], Caffe [15], or Pytorch [24]. Even though these frameworks have simplified the development of new networks and the exploitation of GPUs dramatically, it is still non-trivial for a novice to build a usable pipeline from training to deployment in a robotic platform. Companies such as NVIDIA and Intel have furthermore developed custom accelerators such as TensorRT or the Neural Compute SDK. Both use graphs created with TensorFlow or Caffe as inputs and transform them into a format in which inference can be accelerated by custom inference hardware. As with the other frameworks, their learning curve can be steep for a developer that actually aims at solving a robotics problem but which relies on the semantic understanding of the environment. Last but not least, source code from computer vision research related to semantic segmentation is often made available, which is a great achievement. Each research group, however, uses a different framework and adapting the trained networks to an own robotics codebase can sometimes take a considerable amount of development time.'\\n\\n'Although we do not propose a new scientific method, we believe that this work has a strong positive impact on the robotics community. Six months after becoming publicly available, Bonnet already has a considerable user base and won \\u201cBest Demo Award\\u201d at the Workshop on Multimodal Robot Perception at ICRA 2018. Our open-source software is available at https:\\/\\/github.com\\/PRBonn\\/bonnet.'\\n\\n'In this paper, we presented Bonnet, an open-source semantic segmentation training and deployment tool for robotics research. Bonnet eases the integration of semantic segmentation methods for robotics. It provides a stable interface allowing the community to better collaborate, add different datasets and network architectures, and share implementation efforts as well as pre-trained models. We believe that this tool speeds up the deployment of semantic segmentation CNNs on research robotics platforms. We provide three sample architectures that operate at framerate, and include pre-trained weights for diverse and challenging datasets with the goal that the robotics community will exploit them and contribute to the tool.'\\n\\n'https:\\/\\/github.com\\/PRBonn\\/bonnet'\",\"727\":null,\"728\":null,\"729\":null,\"730\":null,\"731\":\"'Reliable and compact codes for keyframes are essential for efficiently retrieving similar poses. As used in various reconstruction systems, the original encoding strategy based on the Randomized Ferns [32] defines 4-channel binary tests at randomized but fixed image locations (\\\\nN\\\\nin total) to generate a compact code for each keyframe\\\\nX\\\\n. Intuitively by integrating higher-level labeling results with low-level color and depth tests, a richer and more effective code can be acquired for better measuring similarities between keyframes. We expand the original code\\\\nb\\\\nX\\\\n=[\\\\nb\\\\nX\\\\n1\\\\n\\u2026\\\\nb\\\\nX\\\\nN\\\\n]\\u2208\\\\nB\\\\n4N\\\\nby adding the maximum likelihood of labeling denoted as\\\\nq\\\\nX\\\\n\\u2217\\\\nof each location, and thus form a revised code\\\\nb\\\\n\\u2032\\\\nX\\\\n=[(\\\\nb\\\\nX\\\\n1\\\\n, \\\\nq\\\\nX\\\\n1\\\\n)\\u2026(\\\\nb\\\\nX\\\\nN\\\\n, \\\\nq\\\\nX\\\\nN\\\\n)]\\u2208\\\\nB\\\\n(4+\\u2308\\\\nlog\\\\n2\\\\nM\\u2309)N\\\\n. Thus, the dissimilarity measured by both pixel and semantic differences of two encodings\\\\nb\\\\n\\u2032\\\\nI\\\\nand\\\\nb\\\\n\\u2032\\\\nJ\\\\ncan be calculated as:'\",\"732\":null,\"733\":\"'https:\\/\\/github.com\\/JStech\\/ICP'\\n\\n'https:\\/\\/youtu.be\\/w4eVOgd7Zes'\\n\\n'https:\\/\\/github.com\\/JStech\\/ICP'\",\"734\":\"'In conclusion, is there an underlying principled framework to design POMDP policies? This paper defines the controller family as an answer to this question. We show they generalize the policy and value representations used by state-of the-art solutions. To validate its effectiveness, we construct a novel policy formulation that infuses beliefs into an FSC. We demonstrate this improved policy form\\u2019s execution on a real robot acting in the world, and show it overcomes some well-known issues with a vanilla FSC. Finally, we will provide our source code with the goal of building new controller family policies to improve POMDP solutions under this unified formal language with the greater research community.'\",\"735\":null,\"736\":null,\"737\":\"'Ground truth one-hot encoded labels are denoted as\\\\ny\\\\ntrue\\\\nk\\\\n, whereas\\\\ny\\\\npredict\\\\nk\\\\nis a predicted probability distribution. Balancing weight of the k-th class is identified as wk. To overcome a problem with unbalanced data we computed weights for each class as proposed in [33]. Finally, we also added an L2 regularisation term to the model objective.'\\n\\n'The proposed clustering model comprises an autoencoder based on CNN (similar to the one used by us for classification), and an additional decoder part built with the same 1D residual layers, as in the classification CNN model, and a bilinear upsampling. In contrast to the encoder, residual layers in the decoder do not use dropout. The architecture is presented in Figure 6. The main task performed by the model is an auto-association. The first step of processing involves encoding the input data into a denser, hidden representation which we call latent representation. Then, the decoder reconstructs the input data from the latent representation.'\\n\\n'Encoder-Decoder architecture employed in the clustering task. Representation obtained in latent vector used for clustering. The number of filters used by convolutions, size of the fully-connected layer and output sizes of the bilinear upsamplings are described in the below blocks.'\",\"738\":null,\"739\":\"'https:\\/\\/github.com\\/DrawZeroPoint\\/hope'\\n\\n'https:\\/\\/github.com\\/DrawZeroPoint\\/hope'\",\"740\":null,\"741\":\"'Table 1 Each tasks is segmented according to the description below. The task code is used in the results section. Unless otherwise specified, standing tasks started and ended with the subjects\\u2019 hands by their side while for sitting tasks the hands were to start and end on the table palm side down. The height of the table is 74 cm, and is elevated to 92 cm to simulate a counter top for the standing cup and mug tasks. the mug (9.5 cm height, 8 cm diameter), can (7.5 cm height, cm diameter), box (21x37x19 cm), and suitcase (43x9x30 cm) weigh 0.36, 0.09, 0.23, and 1.36 kg respectively. The shelves are 80, 140, and 180 cm above the floor. door knob and handle are 90 cm above the floor, and the simulated door swivels with an 84 cm radius.'\\n\\n'Table 2 The nearest two motions belonging to different clusters and the farthest two motions within the same cluster are summarized. \\u201cCluster #\\u201d indicates which cluster the motions belong to. The first column of motions belong to the cluster under examination, while the second column is the motion from the nearest cluster or the same cluster for the farthest case. The motion codes follow the format [subject ID: motion type].'\",\"742\":\"'https:\\/\\/youcu.be\\/eKdoC8Mq46U'\\n\\n'A Fast Pose Estimation Method Based on New QR Code for Location of Indoor Mobile Robot'\",\"743\":null,\"744\":null,\"745\":null,\"746\":null,\"747\":null,\"748\":null,\"749\":null,\"750\":\"'VFE is a feature learning network that aims to encode raw point clouds at the individual voxel level. Given a point cloud, the 3D space is divided into equally spaced voxels, followed by grouping the points to voxels. Then each voxel is encoded using a hierarchy of voxel feature encoding layers. First, every point\\\\np\\\\ni\\\\n=[\\\\nx\\\\ni\\\\n, \\\\ny\\\\ni\\\\n, \\\\nz\\\\ni\\\\n, \\\\nr\\\\ni\\\\n]\\\\nT\\\\n(containing the XYZ coordinates and the reflectance value) in a voxel is represented by its co-ordinates and its relative offset with respect to the centroid of the points in the voxel. That is each point is now represented as:\\\\np\\\\n^\\\\ni\\\\n=[\\\\nx\\\\ni\\\\n,\\\\ny\\\\ni\\\\n,\\\\nz\\\\ni\\\\n,\\\\nr\\\\ni\\\\n,\\\\nx\\\\ni\\\\n\\u2212\\\\nv\\\\nx\\\\n,\\\\ny\\\\ni\\\\n\\u2212\\\\nv\\\\ny\\\\n,\\\\nz\\\\ni\\\\n\\u2212\\\\nv\\\\nz\\\\n]\\\\nT\\\\n\\u2208\\\\nR\\\\n7\\\\n, where\\\\nx\\\\ni\\\\n,\\\\ny\\\\ni\\\\n,\\\\nz\\\\ni\\\\n,\\\\nr\\\\ni\\\\nare the XYZ coordinates and the reflectance value of the point pi, and\\\\nv\\\\nx\\\\n,\\\\nv\\\\ny\\\\n,\\\\nv\\\\nz\\\\nare the XYZ coordinates of the centroid of the points in the voxel which pi belongs to. Next, each\\\\np\\\\n^\\\\ni\\\\nis transformed through the VFE layer which consists of a fully connected network (FCN) into a feature space, where information from the point features can be aggregated to encode the shape of the surface contained within the voxel. The FCN is composed of a linear layer, a batch normalization (BN) layer, and a rectified linear unit (ReLU) layer. The transformed features belonging to a particular voxel are aggregated using element-wise max-pooling. The max-pooled feature vector is then concatenated with point features to form the final feature embedding. All non-empty voxels are encoded in the same way and they share the same set of parameters in FCN. Stacks of such VFE layers are used to transform the input point cloud data into high-dimensional features.'\",\"751\":\"'We presented WISDOM, a dataset of images and object segmentation masks for the warehouse object manipulation environment, images that are currently unavailable in other major segmentation datasets. Training SD Mask R-CNN, an adaptation of Mask R-CNN, on synthetic depth images from WISDOM-Sim enables transfer to real images without expensive hand-labeling, suggesting that depth alone can encode segmentation cues. SD Mask R-CNN outperforms PCL segmentation methods and Mask R-CNN fine-tuned on real color and depth images for the object instance segmentation task, and can be used as part of a successful instance-specific grasping pipeline.'\\n\\n'This work was supported in part by a Google Cloud Focused Research Award for the Mechanical Search Project jointly to UC Berkeley\\u2019s AUTO-LAB and the Stanford Vision Learning Lab, in affiliation with the Berkeley AI Research (BAIR) Lab, Berkeley Deep Drive (BDD), the Real-Time Intelligent Secure Execution (RISE) Lab, and the CITRIS \\u201cPeople and Robots\\u201d (CPAR) Initiative. The Authors were also supported by the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program, the SAIL-Toyota Research initiative, the U.S. National Science Foundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot Collaborative Systems, Scalable Collaborative Human-Robot Learning (SCHooL) Project, the NSF National Robotics Initiative Award 1734633, and in part by donations from Siemens, Google, Amazon Robotics, Toyota Research Institute, Autodesk, ABB, Knapp, Loccioni, Honda, Intel, Comcast, Cisco, Hewlett-Packard and by equipment grants from PhotoNeo, and NVidia. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Sponsors. We thank our colleagues who provided helpful feedback, code, and suggestions, in particular Michael Laskey, Vishal Satish, Daniel Seita, and Ajay Tanwani.'\\n\\n'Training was based on Matterport\\u2019s open-source Keras and TensorFlow implementation of Mask R-CNN from GitHub, which uses a ResNet 101 and FPN backbone [50]. This implementation closely follows the original Mask R-CNN paper in [6]. We made the modifications listed above and trained the network on WISDOM-Sim with an 80-20 train-val split for 60 epochs with a learning rate of 0.01, momentum of 0.9, and weight decay of 0.0001 on a Titan X GPU. On our setup, training took approximately 24 hours and a single forward pass took 105 ms (average of 600 trials). We call the final trained network a Synthetic Depth Mask R-Cnn (SD Mask R-CNN).'\\n\\n'The Point Cloud Library, an open-source library for processing 3D data, provides several methods for segmenting point clouds [28]. We used two of these methods: Euclidean clustering and region-growing segmentation. Euclidean clustering adds points to clusters based on the Euclidean distance between neighboring points. If a point is within a sphere of a set radius from its neighbor, then it is added to the cluster [27]. Region-growing segmentation operates in a similar way to Euclidean clustering, but instead of considering Euclidean distance between neighboring points, it discriminates clusters based on the difference of angle between normal vectors and curvature [29, 30]. We tuned the parameters of each method on the first ten images of the high-res and low-res WISDOM-Real training sets.'\\n\\n'The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https:\\/\\/bit.ly\\/2letCuE.'\",\"752\":\"'http:\\/\\/crlab.cs.columbia.edu\\/visualtactilegrasping\\/'\\n\\n'The contributions of this work include: 1) a framework for integrating multi-modal sensory data to holistically reason about object geometry and enable robotic grasping, 2) an open source dataset for training a shape completion system using both tactile and depth sensory information, 3) open source code for alternative visual-tactile general completion methods, 4) experimental results comparing the completed object models using depth only, the combined depth-tactile information, and various other visual-tactile completion methods, and 5) real and simulated grasping experiments using the completed models. This dataset, code, and extended video are freely available at http:\\/\\/crlab.cs.columbia.edu\\/visualtactilegrasping\\/.'\\n\\n'Our method provides an open source novel visual-tactile completion method which outperforms other general visualt-actile completion methods in completion accuracy, time of execution, and grasp posture utilizing a dataset which is representative of household and tabletop objects. We demonstrated that even small amounts of additional tactile information can be incredibly helpful in reasoning about object geometry. Experimental results verified that utilizing both vision and tactile was superior to using depth alone. In the future we hope to relax the fixed object assumption by using novel tactile sensors [33] we are developing with fast dynamic response that allow object contact without motion. We are also interested in using Next-Best-View planning [34], [35] for refining and optimizing the tactile exploration strategy to recover unseen geometry.'\",\"753\":null,\"754\":\"'The hexapod is based on the open-source robot Open-RoACH. The untethered legged robot measures 15 cm long, 11 cm wide, and 250 grams including a 7.4V Li-Po battery. It has two DC motors (Micro Metal Gearmotors, Pololu, USA) driving six legs. It has an onboard microcontroller (mbed LXP LPC1768, ARM, UK), a linescan sensor (TSL1401, Parallax, USA), and an inertial measurement unit (IMU) (GY-521 MPU-6050, Phantom YoYo, China). All the original six legs are made from the same sandwich structure as the ribbon, manually folded into a square shape. Each of the legs is attached to the main body of the robot with a pair of permanent magnets. There are two more magnets mounted on a supporting structure over the robot, serving as the engagement mechanism with the regenerating robot.'\",\"755\":null,\"756\":null,\"757\":null,\"758\":null,\"759\":null,\"760\":null,\"761\":null,\"762\":null,\"763\":\"'https:\\/\\/youtu.be\\/kiLKSqI4KhE'\\n\\n'We used the open source implementation of PPO by Tensorflow Agents [27] that creates the symbolic representation of the computation graph. The implementation is highly parallelized and performs full-batch gradient ascent updates, using Adam [28] optimizer, on the batch of data collected from multiple environment instances.'\",\"764\":null,\"765\":null,\"766\":null,\"767\":null,\"768\":null,\"769\":null,\"770\":\"'The control-related software must be computationally efficient to enable high bandwidth controllers, hence all software is written using C++. In addition, the robot operating system (ROS) framework is used for high level communication. A Kalman filter is implemented using sensor data obtained by the IMU and motor encoder measurements. In combination with the model knowledge from III-C, the Kalman filter provides an estimate of the system\\u2019s state dependent on the hip motor position as described in IV-A. The estimated state information is fed to the controller together with the desired pose from the user as shown in Figure 3. The user input originates either from a 3D mouse or a gesture control device [18] offering easy and intuitive steering. The controller block includes the stabilizing, jump and fall recovery controllers as well as a high level position controller. Jumping and driving maneuvers are considered decoupled due to the optimized leg geometry and are therefore controlled independently. The stabilizing controller computes and sends torque commands to actuate the wheel motors. Similarly, the jump and fall recovery controllers take full control of the hip and wheel motors.'\",\"771\":null,\"772\":null,\"773\":null,\"774\":null,\"775\":null,\"776\":null,\"777\":\"'It has been a great challenge to achieve effective task performance on real mobile robots due to a great amount of effort involved. When a new skill is designed, the code needs to be rewritten and new parameters need to be tuned. Using learning algorithms shows the potential of lightening up the challenge above by using a single set of training parameters, but the feasibility of learning algorithms using real robots still remains a research pursuit. In this work, we focus on one of the mobile robot systems - robot soccer \\u201csmall-size\\u201d domain. We use Deep Reinforcement Learning (DRL) algorithm to learn primitive skills instead of end-to-end skills for complex tasks so that primitive skills can be learned easily, and they can be easier to reuse, compose to complex skills.'\\n\\n'In this section, we show the experimental results of skill learning. We first present the result of training curves, and then evaluate policies by comparing with hand-coded skills from perspectives of success rate and time performance. We also evaluate skills on real robots, showing that the policies achieve good performance by directly transferring the policy from simulation.'\\n\\n'In this part, we evaluate the policies trained for three skills. We empirically demonstrate that our trained policy has reasonable performance by comparing the learned skills with existing hand-coded skills. We support our argument by showing the statistics collected for trained skills and hand-coded skills.'\\n\\n'We further compare the time duration it takes between hand-coded skills and learned skills. In table III, we show the mean and standard deviation of time taken by skills to complete tasks. For this statistics, we evaluate final policies over 500 runs. This table shows that learned skills take slightly slower time (0.29 seconds on average of all skills) when they have the overall better success rate than other approaches.'\\n\\n'Table II Comparison of Hand-Coded Policy and Trained Policy'\\n\\n'Table III Comparison of Time Taken Between Hand-Coded Policy and Trained Policy (Unit: S)'\",\"778\":\"'Compute the set of vertices that encode possible solutions for all decomposition cells. There are 4 different starting points for every cell, each inducing an exit point, see Eq. (1).'\",\"779\":\"'To tackle this shortcoming, state representation learning strategies are employed. The state representation of the policy is trained with additional criteria, adding an autoencoder to help with the training of the convolutional layers (or encoding layers) of the network.'\\n\\n'In order to eliminate the requirement of recording demonstrations and pre-training an autoencoder, an enhanced version of Deep COACH is proposed, which learns everything in a single interaction step, as the original COACH does. Hence, it allows to train all the parameters of the network interactively from scratch. State representation strategies have been included in order to make the networks converge faster. The basic idea is to train the state representation of the policy with additional auxiliary criteria (autoencoding). So, in addition to the loss function for predicting the policy based on the data generated by the human corrections, it also includes the loss function of the reconstruction at the output of the autoencoder, based on the same data stored during the human corrections.'\\n\\n'Enhanced D-COACH. The state representation is shared between the autoencoder and the policy training.'\\n\\n'Both networks, the policy and the autoencoder, share the convolutional layers of the encoder as shown in Fig. 2. The policy network has the convolutional layers at the input, followed by a second part that is a fully-connected layer, then this network maps from STATE to ACTION, while the autoencoder network involves the computation from STATE to STATE\\\\n\\u2217\\\\n, wherein STATE\\\\n\\u2217\\\\nis the reconstructed image at the output of the decoder, according to Fig. 2.'\\n\\n'Two different updates are computed, one for the layers involved in the policy computation, and another for the layers of the autoencoder. When an advice of correction is given, the update policy instruction updates the policy network for the current state and the batch update subroutine is called. This subroutine updates the policy and the autoencoder using a mini-batch sampled from the replay buffer. If the reconstruction error of the autoencoder (difference between STATE and STATE\\\\n\\u2217\\\\n)\\\\nis greater than a threshold (line 6), the autoencoder is updated with the same mini-batch using the instruction update AE (autoencoder). Otherwise, the convolutional layers are frozen, so that the instruction update policy in lines 5 and 19, only modifies the non-convolutional layers with the Stochastic Gradient Descent (SGD) operation. The batch update subroutine is also called every b time steps (line 24).'\\n\\n'Finally, it is possible to see the contribution of the condition stated for freezing the convolutional layers, when the error of the decoder is small. This rule provides more stability to the learning process. In the Car Racing experiments, the variant that always updates the AE undergoes an \\u201cunlearning\\u201d stage after 10 minutes of training, whereas in the Duckie Racing experiments is not possible to notice any considerable difference between both approaches. When the error of the decoder is small it means that the latent vector is a good representation of the state, but still the gradient and the error of the policy can be large; therefore, in some cases there may be conflicts that harm the AE performance and consequently the performance of the policy. Freezing these layers is a detail that solves this conflict.'\",\"780\":null,\"781\":\"'http:\\/\\/gamma.cs.unc.edu\\/ADAPS\\/'\",\"782\":null,\"783\":null,\"784\":null,\"785\":null,\"786\":null,\"787\":null,\"788\":\"'We encode (7) using big-M method, which is a standard procedure for translating PWA systems into mixed-integer constraints. We introduce binary variables\\\\n\\u03b4\\\\ni\\\\nt\\\\n\\u2208{0,1}\\\\n, which are used in a way that\\\\n\\u03b4\\\\ni\\\\nt\\\\ntakes 1 if mode at time t is i, and zero otherwise. Thus, we have the constraint:'\\n\\n'Eq. (7) is encoded as follows. For all\\\\ni\\u2208M\\\\n, we have:'\",\"789\":\"'https:\\/\\/youtu.be\\/_thXAaEJYGM'\\n\\n'https:\\/\\/github.com\\/HJReachability\\/Classification_Based_Reachability'\\n\\n'https:\\/\\/github.com\\/HJReachability\\/Classification_Based_Reachability'\",\"790\":null,\"791\":null,\"792\":null,\"793\":\"'Finally, any Buzz script is compiled into an optimized, memory-efficient, and platform-agnostic bytecode to be executed on the Buzz Virtual Machine (BVM). To interface the BVM with the robots\\u2019 actuators and sensors, the integrator needs to write C hooks that are callable from a Buzz script.'\\n\\n'Rather than embedding the BVM into the firmware of each robot, we decided to accelerate the development of the control algorithms by creating a centralized emulator for our decentralized configuration. Emulating a distributed software architecture requires a wrapper that can be instantiated to connect to each hardware node. To solve this problem, we created a Python module for wrapping the BVM called PyBuzz, such that in Python, one can construct a BVM as a Python object, and link Python functions as callable functions in Buzz. When interpreting the Buzz object code, the BVM performs calls to these Python functions.'\",\"794\":null,\"795\":\"'https:\\/\\/bicbuckec.org\\/coroufmg\\/raukf_cm'\\n\\n'https:\\/\\/goo.gl\\/mCFSqG'\\n\\n'The main contribution of this paper is the proposition of the robust adaptive unscented Kalman filter (RAUKF) algorithm for attitude estimation. This algorithm is robust to fast and slow perturbations on both accelerometers and magnetometers and, to the best of authors\\u2019 knowledge, is the first one with such characteristics that precisely and consistently represent the attitude using quaternions. The proposed algorithm is tested with real experimental data collected from a MARG sensor. The performance of the proposed algorithm is confronted against the non-adaptive UKF, the open source algorithm based on complementary filter proposed in [9] and the commercial algorithm embedded in the MARG device used in our experiments, which was executed using a manipulator robot for validation purposes.'\",\"796\":\"'Network architectures and baselines For the reacher environment, a three-layer 3D CNN was used across different configurations as a base architecture, and only the final classification layer was modified, depending on the number of steps in a target task. For example, for the base reacher environment with two steps, a fully connected layer outputting two logits was added on top of the base architecture. We used our MAML-based framework for training. Detailed hyperparameters used in the experiment can be found in the publicly available code 1.'\",\"797\":null,\"798\":\"'Learning from demonstration is a powerful tool for teaching manipulation actions to a robot. It is, however, an unsolved problem how to consider knowledge about the world and action-induced reactions such as forces imposed onto the gripper or measured liquid levels during pouring without explicit and case dependent programming. In this paper, we present a novel approach to include such knowledge directly in form of measured features. To this end, we use action demonstrations together with external features to learn a motion encoded by a dynamic system in a Gaussian Mixture Model (GMM) representation. Accordingly, during action imitation, the system is able to couple the geometric trajectory of the motion to measured features in the scene. We demonstrate the feasibility of our approach with a broad range of external features in real-world robot experiments including a drinking, a handover and a pouring task.'\\n\\n'In this paper, we presented an approach to integrate non-geometric features into action model learning. We encode the motion in a dynamical system parametrized by a Gaussian Mixture Model. The correlation between the geometric course of the imitated motions and the additional features is captured in the covariances of the model. Our experiments show that we achieve a significant improvement over purely geometric approaches in the motion imitation. In all evaluated scenarios our approach is able to establish the desired geometric reaction to the perceived non-geometric features, thus yielding a convenient and safe way for robots to learn and execute new skills. In this fashion arbitrary reproducible signals could be integrated to influence the geometric course of a motion. A potential extension could build on trying to detect promising influential features automatically based on the available sensors.'\",\"799\":\"'We propose a learning framework, named Multi-Coordinate Cost Balancing (MCCB), to address the problem of acquiring point-to-point movement skills from demonstrations. MCCB encodes demonstrations simultaneously in multiple differential coordinates that specify local geometric properties. MCCB generates reproductions by solving a convex optimization problem with a multi-coordinate cost function and linear constraints on the reproductions, such as initial, target, and via points. Further, since the relative importance of each coordinate system in the cost function might be unknown for a given skill, MCCB learns optimal weighting factors that balance the cost function. We demonstrate the effectiveness of MCCB via detailed experiments conducted on one handwriting dataset and three complex skill datasets.'\\n\\n'In this work, we contribute a learning framework that encodes demonstrations simultaneously in multiple coordinates, and balances the relative influences of the learned models in generating reproductions. The proposed framework, named Multi-Coordinate Cost Balancing (MCCB), encodes demonstrations in three differential coordinates: Cartesian, tangent, and Laplacian (Section III-A). Simultaneously learning in these three coordinates allows our method to capture all of the underlying geometric properties that are central to a given skill. MCCB encodes the joint density of the time index and the demonstrations in each differential coordinate frame using a separate statistical model. Thus, given any time instant, we are able to readily obtain the conditional mean and covariance in each coordinate system (Section III-B). MCCB generates reproductions by solving an optimization problem with a blended cost function that consists of one term per coordinate. Each term penalizes deviations from the norm, weighted by the inverse of the expected variance in the corresponding coordinate system (Section III-C). Further, we subject the optimization problem to linear constraints on the reproductions, such as initial, target, and via point constraints. Our constrained optimization problem is convex with respect to the reproduction and hence can be solved efficiently.'\",\"800\":null,\"801\":\"'A common policy representation in LfD is the finite state machine. Finite state machine nodes can be thought as, the arguably defunct, goto statement. Control is transfered from node to node; resulting in a large web of transitions akin to spaghetti code. Behavior trees, on the other hand, can be thought of as functions. A node is called by its parent, executes its program, and returns a status from Success, Failure, Running. The parent\\u2019s execution is altered by its children\\u2019s return statuses. Control flows back and forth between parent and children; resulting in concise, transparent, and modular behavior trees.'\",\"802\":\"'https:\\/\\/goo.gl\\/TeibPC'\\n\\n\\\"High-level human activities often have rich temporal structures that determine the order in which atomic actions are executed. We propose the Temporal Context Graph (TCG), a temporal reasoning model that integrates probabilistic inference with Allen's interval algebra, to capture these temporal structures. TCGs are capable of modeling tasks with cyclical atomic actions and consisting of sequential and parallel temporal relations. We present Learning from Demonstration as the application domain where the use of TCGs can improve policy selection and address the problem of perceptual aliasing. Experiments validating the model are presented for learning two tasks from demonstration that involve structured human-robot interactions. The source code for this implementation is available at https:\\/\\/github.com\\/AssistiveRoboticsUNH\\/TCG.\\\"\\n\\n'The Temporal Context Graph proposed in this paper is a novel way of learning complex temporal structures present in high-level tasks. TCGs employ Allen\\u2019s interval algebra to encode the interval temporal relations (ITR) among the atomic actions of the task. These ITRs are then used to train an n-gram model that learns the dependencies between the state transitions and the temporal context of the task. This probabilistic approach is capable of handling cyclical atomic actions and can be leveraged to address instances of perceptual aliasing. To the best of our knowledge, this approach is the first to propose an interval-based temporal reasoning model capable of learning tasks with repetitive atomic actions.'\\n\\n'The first stage of the TCG learning process consists of learning the graphical structure for the given task. The first step in this stage consists of sorting the atomic actions available in the demonstration set, according to their start times (line 3). The sorted action sequences of each demonstration are then processed individually to learn the graphical structure that represents the task. Nodes are created for each distinct non-transition action (line 16), while transition actions are used to create the edges connecting those nodes (line 15). Additionally, timeout transition edges are created when two consecutive non-transition actions exist in a sequence (line 13). During this process the mean duration and waiting periods for each atomic action are learned and stored in the edges and nodes of the TCG (lines 17,19, 21). The output of this stage can be considered a finite state machine that encodes the valid transitions between the atomic actions of the given task.'\\n\\n'The second step consists of learning a probabilistic n-gram model using the factored ITR sequences as input (line 28). During the training of the n-gram model, the previous n-1 ITRs of the sequence are used as the evidence to encode the temporal context that generates the action executed in the n-th ITR. The resulting n-gram model encodes all the observed temporal contexts; therefore, it can be leveraged during policy selection to address the issue of perceptual aliasing. This is possible because the actions selected by the n-gram model will be dependent on the current temporal context of the task.'\\n\\n'This paper introduces the Temporal Context Graph, the first interval-based temporal reasoning model capable of learning structures with cyclical atomic actions. The model relies on three principal components to perform temporal reasoning. The first is a graphical structure that captures the set of valid state transitions of the task and is used to filter incorrect action observations. The second is Allen\\u2019s interval algebra, which is used to create ITR sequences that encode the temporal context of the task. The third, and last, component is a probabilistic n-gram model. This model leverages the rich temporal context created with the ITR sequences to perform policy selection during the execution of the task. The model was evaluated using two use cases consisting of structured human-robot interactions. The results demonstrate that TCGs can be used to learn the underlying temporal structure of a task and perform policy selection, exploiting this structure to address the issue of perceptual aliasing. Additionally, the validation use cases demonstrate that using an interval-based approach allows TCGs to learn non-sequential temporal relationships. As a result, TCGs can effectively learn tasks that cannot be modeled using pointbased temporal reasoning models.'\\n\\n'Future work could include expanding the model to encode the uncertainty of incoming action observations and learning relevant non-sequential ITR sequences during the learning phase. These enhancements would increase the performance of the model during policy execution, reducing the dependency on accurate perception modules and increasing its robustness when faced with unseen ITR sequences during the execution of the task.'\",\"803\":null,\"804\":null,\"805\":\"'https:\\/\\/philippente.github.io\\/pub\\/GMR.html'\\n\\n'Generative Motor Reflex (GMR) policy and training: The state encoding uses a variational autoencoder to encode the state x to a latent representation z using\\\\nh\\\\nenc\\\\nand reconstruct it using\\\\nh\\\\ndec\\\\n. The translation model\\\\nh\\\\ntrans\\\\ntransforms z to motor reflex parameters\\\\n\\u03a8\\\\nK\\\\n,\\\\n\\u03a8\\\\nk\\\\nand\\\\n\\u03a8\\\\n\\u03a3\\\\nwhich form a stochastic motor reflex\\\\nu\\u223c\\\\n\\u03c0\\\\n\\u03a8\\\\n(u|x)\\\\n. The GMR weights\\\\n\\u0398\\\\nz\\\\n,\\\\n\\u0398\\\\nx\\\\nand\\\\n\\u0398\\\\n\\u03a8\\\\nare adapted to imitate the local policies\\\\np\\\\ni\\\\n(u|x)\\\\nby minimizing the loss term\\\\nL\\\\nGMR\\\\n. For this, the local policies are derived by trajectory optimization using the dynamics\\\\np(\\\\nx\\\\nt+1\\\\n|\\\\nx\\\\nt\\\\n, \\\\nu\\\\nt\\\\n)\\\\nand loss term\\\\nJ(\\u03c4)\\\\n. For the GMR model we set the hyperparameters to the stated number of neurons. Note: RL stands for leaky rectified linear units, fc for fully connected linear layers.'\\n\\n'In GMR, the VAE enables to encode the robotic state x to a latent representation z while z is sufficiently diverse to (ii) reconstruct the robotic state x with the decoder network. By introducing noise in z during the S-step, the latent representation does not only represent the exact state x but also similar states. This increases the robustness of the reconstruction of the original state and as a consequence the robustness of the generated motor reflex parameters.'\\n\\n'The loss term has to enforce a compressed and meaningful latent state representation while the motor reflex parameters are generated with high precision. Therefore, we propose a loss term which consists of four parts: (i) minimizing the mean-squared state reconstruction loss, (ii) the KL divergence from a unit distribution to the latent state distribution, (iii) the KL divergence prediction error of the motor reflex parameters and (iv) an L2 regularization term. Therein, the sum of (i) and (ii) is the standard loss function of a variational autoencoder, where (ii) forces the encoding of the state space to a latent state space which is in VAE a unit Gaussian distribution. The prediction error (iii) trains the translation model for generating motor reflex parameters and (iv) is a regularization term that improves overall stability and is already used by previous GPS policies.'\\n\\n'We could improve the robustness of neural network policies by using motor reflexes as an intermediate step and evaluated our approach for manipulation tasks. Robustness is reached, if input states can be reliable encoded into the same latent state distribution which was explored during the training procedure. Then, for this latent state, a reliable translation to valid motor reflex parameters is expected.'\\n\\n'https:\\/\\/philippente.github.io\\/pub\\/GMR.html'\",\"806\":\"'One effective approach of stochastic modeling is to utilize graphical models such as Hidden Markov Model (HMM). HMM can stochastically encode spatial and temporal features simultaneously [7]. Moreover, it can update model parameters incrementally by combining with instantaneous topological mapping algorithm [8] and Topological Gaussian Adaptive Resonance Theory (TGART) [9]. However, it is difficult to decode the temporal feature in detail since trajectories are discretized and abstracted. To explicitly incorporate temporal features into the model, modeling methods using explicit-duration HMM [10] and autoregressive HMM [11] are proposed. However, it is difficult to extend them to an incremental algorithm because the model learning does not converge well unless a proper structure of the graphical model is set in advance.'\",\"807\":\"'Concretely, in this work we focus on improving the interface between symbolic aspects of task planning and continuous aspects of motion planning. At this interface, given a symbolic plan structure, it is necessary to select values for continuous parameters that will make lower-level motion planning queries feasible, or to determine that the symbolic structure itself is infeasible. Typical strategies are to search over randomly sampled values for these parameters, or to use hand-coded \\u201cgenerators\\u201d to produce them [1], [2].'\",\"808\":null,\"809\":null,\"810\":null,\"811\":null,\"812\":null,\"813\":null,\"814\":null,\"815\":null,\"816\":null,\"817\":\"'https:\\/\\/github.com\\/kskin\\/UWStereo'\\n\\n'https:\\/\\/github.com\\/kskin\\/UWStereo'\",\"818\":\"'An open-source numerical ocean simulator based on the Phillips spectrum was used for generating wave data [31]\\u2013[35]. Fig. 8 shows the numerical simulator along with the specified heave profile. Simulating boat motion in rough waves, or converting known ocean motion to boat motion is a challenging problem [36], [37]. A simplification, used here, is to assume the orientation of the boat is fixed to three separate points on the wave surface, making a plane. From this plane, the specified roll, pitch, and heave can be backed out using a similar calculation to Eq. 7, and Eq. 8.'\",\"819\":null,\"820\":\"'https:\\/\\/github.com\\/EgorLakomkin\\/icra\\/_2019\\/_speech'\\n\\n'Our sentiment recognition model based on the ASR character output. The multiplicative LSTM model is used to encode spoken text to a fixed-length vector with a logistic regression on top modeling the sentiment. The mLSTM model is pretrained in an unsupervised way on the Amazon reviews with a language-modelling objective.'\\n\\n'https:\\/\\/github.com\\/EgorLakomkin\\/icra\\\\\\\\_2019\\\\\\\\_speech'\",\"821\":null,\"822\":\"'communication encoder module'\\n\\n'communication decoder module'\\n\\n'The policy with a full 360\\u00b0 field of view of intruders attains near expert-level performance at a message width of 1 (all agents broadcast a single 8-bit value), and increasing the message width does not significantly raise performance. Each defender need only hear all the other defenders\\u2019 locations to infer the full joint state, and agents have learned to encode this information into a single scalar value.'\\n\\n'Fig. 6 provides qualitative insight into how these communication channels are being used in a 7 vs. 3 game with a message width of one. The left plot shows how one defender\\u2019s broadcast value changes as one intruder is moved around the playing field. The right plot shows how that defender\\u2019s broadcast value changes as its own position is varied. As expected, the 360\\u00b0 field of view policy generates messages that are a function only of the defender\\u2019s own position, and not attacker locations. The agent has learned to omit redundant information (attackers seen by everyone else) and summarize critical information (the self x,y coordinates encoded as an angular value). Agents with only a 180\\u00b0 field of view must learn a more sophisticated policy. Their message values are a function of the defender position, but observed attackers are also taken into account.'\",\"823\":\"'https:\\/\\/bit.ly\\/2NtR1pf'\\n\\n'https:\\/\\/bit.ly\\/2IEWASV'\",\"824\":null,\"825\":null,\"826\":\"'In this paper we consider inter-robot communication in the context of joint activities. In particular, we focus on convoying and passive communication for radio-denied environments by using whole-body gestures to provide cues regarding future actions. We develop a communication protocol whereby information described by codewords is transmitted by a series of actions executed by a swimming robot. These action sequences are chosen to optimize robustness and transmission duration given the observability, natural activity of the robot and the frequency of different messages. Our approach uses a convolutional network to make core observations of the pose of the robot being tracked, which is sending messages. The observer robot then uses an adaptation of classical decoding methods to infer a message that is being transmitted. The system is trained and validated using simulated data, tested in the pool and is targeted for deployment in the open ocean. Our decoder achieves.94 precision and.66 recall on real footage of robot gesture execution recorded in a swimming pool.'\\n\\n'We present a vision-based robot-to-robot communication system that leverages the rich geometric information recovered from 3D target tracking to unambiguously transmit messages through gesturing. The communication protocol relies on a robot executing a set of gestures that are in turn decoded by another robot from their visual appearance.'\\n\\n'codewords'\\n\\n'An Aqua robot performing codeword\\\\n\\u03b2\\\\n4\\\\n\\u03b2\\\\n2\\\\nin the pool with neutral codebits\\\\n\\u03b2\\\\nz\\\\nin between.'\\n\\n'Finding a minimal cost prefix-free code in which the encoding alphabet features r symbols of unequal letter costs is a well-studied problem [42] \\u2013[44]. Such an encoding represents a generalization of the classical Huffman coding problem [1] of constructing a binary (r=2) prefix code which minimizes the expected transmission cost. The generalization relaxes the binary requirement for the encoding alphabet and introduces variable costs for each encoding character (codebit). This is desirable when it is preferable to minimize the average number of codebits and when codebits of the encoding alphabet have a varied transmission cost such as in the Morse-code alphabet {.,-} In our setting, the variable cost is also an excellent way of penalizing pose configurations that are energy intensive, harder to reliably detect and more ambiguous during day-to-day operation of the robot.'\\n\\n'Our method consists of two main components: 1) an optimal prefix-free encoding of poses where each codebit corresponds to an orientation bin with a defined transmission cost and 2) a visual decoder which relies on a CNN-based orientation regressor to detect the 3D orientation of the robot to in turn decode the codeword.'\\n\\n'r codebits'\\n\\n'Each codebit\\\\n\\u03b2\\\\ni\\\\nis associated with a transmission cost ci = T(\\\\n\\u03b2\\\\ni\\\\n) and a codeword cw=\\\\n{\\\\n\\u03b2\\\\ni\\\\n1\\\\n\\u03b2\\\\ni\\\\n2\\\\n\\u2026\\\\n\\u03b2\\\\ni\\\\nk\\\\n}\\\\n- a list of codebits from\\\\n\\u03a3\\\\n. A codeword has a transmission cost equivalent to the sum of the costs of its individual codebits:'\\n\\n'codeword cw'\\n\\n'code'\\n\\n'In order to choose codebits of the encoding alphabet\\\\n\\u03a3\\\\n, the orientation space of the robot is binned. The roll, pitch and yaw axes are each discretized into bins of size\\\\n\\u03b8\\\\n\\u2218\\\\nr\\\\n,\\\\n\\u03b8\\\\n\\u2218\\\\np\\\\n,\\\\n\\u03b8\\\\n\\u2218\\\\ny\\\\nrespectively. We then take the combinations of the bins from each axis to represent the codebits. Individual axes can be ignored as needed depending on the robot capabilities. In this paper, we choose to forego the roll axis to maintain a smaller number of codebits which is sufficient for our needs. For an example list of codebits, please see Tab. I.'\\n\\n'To assign codewords to messages we sort the messages by their probability, and assign higher probability messages to codewords with lower transmission cost.'\\n\\n'p(\\\\n\\u03b2\\\\ni\\\\n)\\\\n: the probability of a codebit in regular operation. This value allows us to ensure high probability codebits are penalized and not used in our code so that gestures are not confused with regular operation. In order to obtain this probability distribution, we run our pose estimator on footage of the robot in operation and extract the histogram of orientation bins.'\\n\\n'e\\\\n\\u00af\\\\n\\u00af\\\\n\\u00af\\\\n(\\\\n\\u03b2\\\\ni\\\\n)\\\\n: the normalized mean error of the orientation regressor when executed on the corresponding bin of the codebit. This helps avoid using difficult to detect codebits in our encoding.'\\n\\n'd(\\\\n\\u03b2\\\\ni\\\\n)\\\\n: an application-specific value which can represent the time it takes to execute a codebit, or other engineering restrictions in maintaining a certain codebit, also normalized to [0, 1]. This penalizes gestures that are difficult to execute.'\\n\\n'A histogram showing the codebits probabilities captured from 5 minutes of footage of regular operation of the robot. We discourage the use of high probability codebits in message encoding to prevent the false detection of gestures dunng regular operation.'\\n\\n'Given our list of codebits and their associated costs, we implement the optimal prefix-free dynamic programming algorithm presented by [43] to obtain the code-tree that minimizes the total cost of the prefix-free code. An example code tree is presented in Fig. 3.'\\n\\n'B. Visual Decoder'\\n\\n'Using the codewords from the tree generated in Sec. III-A, we can now execute each encoded message on the robot. In order to simplify the decoding algorithm, we insert a neutral codebit\\\\n\\u03b2\\\\nz\\\\nbetween every codebit in a codeword. This serves as a marker to register when every codebit is executed. For example, codeword\\\\n{\\\\n\\u03b2\\\\n1\\\\n\\u03b2\\\\n2\\\\n}\\\\nbecomes\\\\n{\\\\n\\u03b2\\\\n1\\\\n\\u03b2\\\\nz\\\\n\\u03b2\\\\n2\\\\n\\u03b2\\\\nz\\\\n}\\\\n. Fig. 1 shows Aqua executing the codeword\\\\n{\\\\n\\u03b2\\\\n4\\\\n\\u03b2\\\\nz\\\\n\\u03b2\\\\n2\\\\n\\u03b2\\\\nz\\\\n}'\\n\\n'To account for pose estimation errors and viewpoint variations, the codebits are detected if they are within a bin of the target codebit angles with the bin limits offset from the center by [\\u201320, 20].'\\n\\n'In order to decode an executed codeword, we obtain the filtered pose estimate on every frame and bin the orientation estimate. We then check if the bin corresponds to any codebits of the encoding alphabet. If we have detected a valid codebit that is not identical to the previous detected ones, we check if this codebit\\\\n\\u03b2\\\\nt\\\\nis a prefix of any of our codewords. If we are already tracking a candidate sub-codeword candt -1, we instead check if candt\\\\n=can\\\\nd\\\\nt\\u22121\\\\nU\\\\n\\u03b2\\\\nj\\\\nis a prefix of a codeword in code W. If it is a codeword in W, we have detected a message.'\\n\\n'The prefix-free nature of the code means that codewords are non-ambiguous and the transmission costs used to generate the code help to ensure that codewords are not confused with regular pose configurations that occur on a normal execution of the robot.'\\n\\n'Note that this algorithm assumes the observer is mostly following the target and looking at it from a limited view-point window. The observer can have translation offsets but generally assumes the target is executing messages with a local frame of reference that is relative to its camera. To better handle smaller viewpoint variations expected from any moving observer, we update the codebit bin centers according to the latest neutral codebit detected and its offset from its original neutral codebit center up to [-10\\u00b0, 10\\u00b0].'\\n\\n'To evaluate our visual encoder, we prepare a dataset containing the Aqua executing codewords in both real pool trials and in the Aqua simulator [9]. To create this testing dataset, we record both generated synthetic videos of a simulated robot and real videos of the physical robot executing the motions that correspond to a subset of the codewords generated in Fig. 3.'\\n\\n'The synthetic testing data is comprised of recordings of the simulated robot executing motions for each of the codewords listed in Tab. III within the realistic simulated underwater environment, totalling 50 recordings per codeword.'\\n\\n'Once the target is reached, there is a pause for approximately 1.5 seconds before returning to the neutral orientation and continuing onto the next codebit or codeword. This simple, idealized dynamics model gives a solid baseline for comparing real world examples.'\\n\\n'Variations to each execution of a codeword include a) a random starting orientation within the simulated underwater environment with bounds of [-10\\u00b0, 10\\u00b0] for roll, [-20\\u00b0, 20\\u00b0] for pitch, and [-180\\u00b0, 180\\u00b0] for yaw, b) random additions to the target orientation for each axis with bounds of [-\\\\n5\\\\n\\u2218\\\\n, \\\\n5\\\\n\\u2218\\\\n], and c) changes in speed of the robot through random scaling of the amount of time allotted for each motion, normally 2 seconds, with bounds of [.8, 1.2]. Each random variable is chosen with uniform distribution.'\\n\\n'Images of the synthetic Aqua performing a particular codebit in Unreal are presented in Fig. 5.'\\n\\n'Synthetic Aqua robot positioned according to codebit\\\\n\\u03b2\\\\n1\\\\n(left) and\\\\n\\u03b2\\\\n4\\\\n(right).'\\n\\n'The testing dataset used to evaluate visual encoding in a real world setting consists of, on average, 10 recorded examples of the Aqua executing gestures in the McGill University pool for each of the codewords listed in Table IV.'\\n\\n'In order to execute the gestures corresponding to codebits on the physical Aqua, a custom PID autopilot controller [6] is utilized. Given a target orientation offset for a chosen codebit, the autopilot controller causes the Aqua to rotate, stopping when the IMU reading indicates the Aqua is within 5 degrees of the target angles. To prevent the Aqua from drifting and accidentally appearing to execute an undesired motion, the controller maintains the neutral orientation for 3 seconds before a new motion is attempted, where the neutral orientation is considered to be the orientation at which the Aqua starts executing a gesture.'\\n\\n'An important evaluation of our method involves decoding messages from a robot as it moves around its environment in regular operation. This evaluation ensures that messages aren\\u2019t missed while performing basic navigation and ensures the decoder\\u2019s ability to discern regular operation from messaging. In order to test our visual decoder\\u2019s performance in simulation, we generate a dataset of 10 synthetic videos showcasing the Aqua executing gestures intermittently as it explores the custom-designed underwater world described in Sec. IV-A.I. Each recording features 1 codeword repeated 5 times at random over the course of approximately 2 minutes of navigation.'\\n\\n'In order to simplify our deployment and encoding alphabet, we forego the roll axis and generate codebits by using 3 bins with\\\\n\\u03b8\\\\ny\\\\n=\\\\n60\\\\n\\u2218\\\\nand\\\\n\\u03b8\\\\np\\\\n=\\\\n60\\\\n\\u2218\\\\n, restricting the orientation space to\\\\n{\\u2212\\\\n90\\\\n\\u2218\\\\n, \\\\n90\\\\n\\u2218\\\\n}\\\\n. The corresponding codebit list is shown in Tab. I. We assign the neutral codebit to be\\\\n\\u03b2\\\\nz\\\\n=\\\\n\\u03b2\\\\n5\\\\n.'\\n\\n'To derive the transmission cost of each codebit, we plot the probability of codebits in Fig. 2 and a randomly sampled subset of angle errors relative to their respective angle value in Fig. 7.'\\n\\n'To measure our decoding performance, we evaluate our decoder on the Unreal and real underwater gestures datasets described in Sec. IV. We generate confusion matrices for each dataset and summarize precision and recall values in the matrices in Tab. III and Tab. IV.'\\n\\n'On synthetic data, the visual decoder achieves a mean precision of 0.96 and mean recall of 0.91. Note that the requirement for the robot to return to a neutral codebit\\\\n\\u03b2\\\\nz\\\\nresults in some missed detections of certain messages, as can be seen in the False negatives (FNs) column.'\\n\\n'On real data, our visual decoder achieves a mean precision of 0.94 and mean recall of 0.66. An explanation for the particularly worse performance of the system in the real pool on codebit\\\\n\\u03b2\\\\n1\\\\nis the imperfect execution of it by the physical robot. Codebit\\\\n\\u03b2\\\\n1\\\\nfeaturing both yaw and pitch variations were found more likely to overshoot and undershoot on pitch. Fine-tuning of the autopilot controller for such tasks can help mitigate these errors. Most notably, codebit\\\\n\\u03b2\\\\n1\\\\nexecutions tend to not pitch enough and were at times more closely executed as codebit\\\\n\\u03b2\\\\n2\\\\n. Slight overshoot in the yaw axis also lead to more false negatives than expected as the robot skipped the neutral codebit at times which is supposed to signal the end or transition of a codeword. These errors in executions are typical of real systems deployed in the field. A way to tackle these limitations is to use codebits that are more spread out in the orientation space of the robot to allow some room for error.'\\n\\n'We evaluate our system on synthetic swimming trajectories of the Aqua in order to better understand the performance of the visual decoder in a deployment setting. The dataset, described in Sec. IV-B.3, consists of typical swimming trajectories with messages communicated at random times. The goal of this evaluation is to ensure the reliability of the code even when the robot performs a variety of swimming poses. We summarize the precision\\/recall values on these trajectories in Tab. V. Common false negatives are codebits\\\\n\\u03b2\\\\n2\\\\nand\\\\n\\u03b2\\\\n8\\\\nwhich represent basic left and right yaw configurations. As shown in Fig. 2, these codebits have a high probability of occurrence in regular deployment and our simplified cost structure did not fully capture this cost. Using the more refined transmission cost defined in Sec. III-A would help mitigate this issue and ensure these codebits are used less often individually. One can also introduce a cut-off on the codebit probability\\\\np(\\\\n\\u03b2\\\\ni\\\\n)\\\\nterm of the transmission cost and not rely on codebits with high probability.'\\n\\n'We have presented a method for vision-based communication between robots in radio-denied environments. The method allows a robot to encode sequences of pose configurations (codebits) to convey a message. Our method uses optimal variable-length prefix codes to encode these codebits while minimizing the likelihood of false positive detection. The following robot decodes the transmitted message by using a pose estimation CNN. We demonstrate our technique on synthetically generated tracking sequences with a mean precision and recall of 0.96 and 0.91 respectively, and on real data with 0.94 and 0.66. The system runs in real-time on the Aqua robot underwater. We expect to use this technique in our own work of underwater multi-robot convoying using the Aqua robots to signal important messages and achieve more robust tracking.'\\n\\n'A Generic Design for Encoding and Decoding Variable Length Codes in Multi-codec Video Processing Engines'\\n\\n'Iterative symbol decoding of convolutionally-encoded variable-length codes'\\n\\n'Underwater Communication Using Full-Body Gestures and Optimal Variable-Length Prefix Codes'\\n\\n'C. Optimal Prefix-free Codes'\\n\\n'Codebit\\\\n\\u03b2\\\\ni\\\\n: a particular pose configuration.'\\n\\n'Codeword cwi: an ordered list of codebits.'\\n\\n'Code W: a set of codewords.'\\n\\n'Table 1 Example List of 9 Codebits Generated from Binning of the YAW and Pitch Axes With\\\\n\\u03b8\\\\np\\\\n=\\\\n\\u03b8\\\\ny\\\\n=\\\\n60\\\\n\\u2218\\\\nand Associated Angles'\\n\\n'Table III Confusion Matrix for Codewords Executed in Unreal. Class-Specific Recall Values are Highlighted in Grey'\\n\\n'Table IV Confusion Matrix for Codewords Executed in the Pool. Class-Specific Recall Values are Highlighted in Grey'\",\"827\":null,\"828\":null,\"829\":\"'In this paper, we introduced a novel UAV\\/ UGV cooperation system, which uses the UAV not only as a flying sensor but also as a tool to attach a tether to an unreachable area for the UGV to assist the UGV. We designed the autonomous system architecture based on ROS and open-source frameworks. Furthermore, we compared several tether attaching methods and chose a simple hybrid method that uses a grappling hook and winding technique to increase the probability of successful anchoring. We conducted several experiments that include the testing of the autonomous navigation, cliff climbing and tether attachment evaluation. Lastly, we executed the whole mission autonomously. The robots successfully finished the whole mission and both robots arrived on top of the cliff thus proving the feasibility of our proposed system.'\",\"830\":null,\"831\":null,\"832\":\"'https:\\/\\/wangfengl8.github.io\\/SoundIndicatedDetection\\/'\\n\\n'https:\\/\\/wangfengl8.github.io\\/SoundIndicatedDetection\\/'\\n\\n'https:\\/\\/wangfeng18.github.io\\/SoundIndicatedDetection\\/'\\n\\n'https:\\/\\/wangfeng18.github.io\\/SoundIndicatedDetection\\/'\\n\\n'https:\\/\\/wangfengl8.github.io\\/SoundIndicatedDetection\\/'\\n\\n'https:\\/\\/wangfengl8.github.io\\/SoundIndicatedDetection\\/'\\n\\n'https:\\/\\/wangfeng18.github.io\\/SoundIndicatedDetection\\/'\",\"833\":null,\"834\":\"'http:\\/\\/people.idsia.ch\\/~gromov\\/proximity-hri-pipeline'\\n\\n'Video, datasets and code to reproduce our results are available at: http:\\/\\/people.idsia.ch\\/~gromov\\/proximity-hri-pipeline'\\n\\n'Videos, Datasets and Code'\",\"835\":null,\"836\":\"'We present a novel open-source tool for extrinsic calibration of radar, camera and lidar. Unlike currently available offerings, our tool facilitates joint extrinsic calib...'\\n\\n'We present a novel open-source tool for extrinsic calibration of radar, camera and lidar. Unlike currently available offerings, our tool facilitates joint extrinsic calibration of all three sensing modalities on multiple measurements. Furthermore, our calibration target design extends existing work to obtain simultaneous measurements for all these modalities. We study how various factors of the calibration procedure affect the outcome on real multi-modal measurements of the target. Three different configurations of the optimization criterion are considered, namely using error terms for a minimal amount of sensor pairs, or using terms for all sensor pairs with additional loop closure constraints, or by adding terms for structure estimation in a probabilistic model. The experiments further evaluate how the number of calibration boards affect calibration performance, and robustness against different levels of zero mean Gaussian noise. Our results show that all configurations achieve good results for lidar to camera errors and that fully connected pose estimation shows the best performance for lidar to radar errors when more than five board locations are used.'\\n\\n'In contrast to the discussed related work, our work provides the following contributions. First, three extrinsic calibration configurations to jointly calibrate lidar, camera and radar are investigated. We study the three configurations, required number of calibration board locations, and choice for the reference sensor using a real multi-modal sensor setup. Second, we propose a calibration target design that is detectable by lidar, camera and radar. Third, we provide an open-source extrinsic calibration tool for these sensors, with bindings to Robot Operating System (ROS)1'\\n\\n'We presented an open-source extrinsic calibration tool for lidar, camera and radar, and proposed three configurations to estimate the sensor poses from simultaneous detections of multiple calibration board locations. Experiments on a setup with all sensing modalities show that all configurations can provide good calibration results. Furthermore, the results with five calibration board locations show that the expected RMSE is approximately 2 cm for lidar to camera and lidar to radar, and approximately 2.5 cm for camera to radar. When using more than five board locations, fully connected pose estimation shows the best performance. Future work involves investigating the effect of more than three sensors on the calibration performance.'\",\"837\":\"'Attitude estimation for a pure sinusoidal roll manoeuvre on a real system. The solid red line is the true roll angle returned by the encoder, and the dashed blue curve is the estimated roll angle. The small residual estimation errors can be mostly traced to alignment inaccuracies of the testbed. The vector measurements, gy and gz, are also included to show the instantaneous response from the measurement to the estimation.'\\n\\n'The roll and yaw motions are generated using stepper motors which can be programmed to rotate the model according to a prescribed trajectory. As the motors rotate, real-time measurement is provided using 517 counts-per-revolution differential encoders which provide feedback to the motor controller and also a record of the actual position of the tested model. Pitch and plunge motion are generated by actuating the pitch plunge rods using linear accelerators. The output of the rods is again measured using encoders to provide real-time feedback and a record of the actual position of the model. During all the tests, the IMU is positioned on the rotation axis so that there is no acceleration, and the accelerometer is sensing only the gravitational field.'\\n\\n'Attitude estimation for a pure sinusoidal pitch manoeuvre on a real system. The solid red line is the true pitch angle returned by the encoder, and the dashed blue curve is the estimated pitch angle. The small residual estimation errors can be mostly traced to alignment inaccuracies of the testbed. The vector measurement, gx, is also included to show the instantaneous response from the measurement to the estimation.'\",\"838\":\"'Illustration of the change of the point set during the whole network. The input is simliar to a chair. Farthest point sampling (FPS) is used to downsample the point set from the last layer. When upsampling, it gets the point set from the corresponding layer in encoder by skip connection.'\\n\\n'For point cloud semantic segmentation, we choose the encode-decode structure for hierarchical feature extraction. Given n0 points, farthest point sampling (FPS) can be used to downsample the input point set to n1 points\\\\n(\\\\nn\\\\n1\\\\n<\\\\nn\\\\n0\\\\n)\\\\nwhich is a subset of the input. Meanwhile, features from n0 points should be aggregated to n1 points using some algorithm. In this paper, we use the depthwise graph convolution which will be described later.'\\n\\n'The whole architecture of the encode-decode structure is shown in Fig. 2. FPS is used to iteratively downsample points which generated by the last layer. The encoder in Fig. 2 consists of three times of downsampling. After downsampling, the remaining points extract more general shape information and have a larger receptive field which is beneficial for semantic segmentation. After encoding, the point number needs to be upsampled to the same as input for pointwise labeling. When decoding, skip connection is used to obtain point coordinates and feature information from the corresponding layer in the encoder.'\\n\\n'The whole network structure includes the encoder part and the decoder part shown in Fig. 4. When encoding, three DGConv blocks are used to iteratively downsample points and extract global and high-level semantic information. When decoding, skip connection is used to upsample points and concatenate features from the encoder. The DGConv block in the decoder is the same as that in the encoder except for the replacement of FPS. Finally, the network outputs the pointwise label.'\\n\\n'use four downsampling DGConv blocks in the encoder and correspondingly four upsampling DGConv blocks in the decoder. The architecture is DGConv (2048,256)- DGConv (768,512)-DGConv(384,768)-DGConv(128,1024)- DGConv (128, 1024)-DGConv(384,768)-DGConv(768,512)- DGConv(2048,256). Fully connected layers are used to get the final prediction. We choose the Adam optimizer with base learning rate 0.001 and batch size 16 (each GPU with 8). The learning rate declines by 20% every 5000 training steps.'\",\"839\":null,\"840\":\"'A Fast Pose Estimation Method Based on New QR Code for Location of Indoor Mobile Robot'\",\"841\":\"'We will only encode in the optimization the 3D positions\\\\n\\u03c1\\\\nl\\\\nfor a subset\\\\n\\u039b\\\\nt\\\\nof all landmarks\\\\nL\\\\nt\\\\nvisible up to time t:\\\\n{\\\\n\\u03c1\\\\nl\\\\n}\\\\nl\\u2208\\\\n\\u039b\\\\nt\\\\n, where\\\\n\\u039b\\\\nt\\\\n\\u2286\\\\nL\\\\nt\\\\n. We will avoid encoding the rest of the landmarks\\\\nS\\\\nt\\\\n=\\\\nL\\\\nt\\\\n\\u2216\\\\n\\u039b\\\\nt\\\\nby using a structureless approach, as defined in [8], [Sec. VII], which circumvents the need to add the landmarks\\u2019 positions as variables in the optimization. This allows trading-off accuracy for speed, since the optimization\\u2019s complexity increases with the number of variables to be estimated.'\\n\\n'Point cloud sampled from the estimated 3D mesh color-encoded with the distance to the ground truth point cloud (V1_01), for SP approach (top) and SPR (bottom).'\",\"842\":null,\"843\":null,\"844\":null,\"845\":\"'https:\\/\\/wayve.ai\\/blog\\/12diad'\\n\\n'We found that we could reliably learn to lane follow in simulation from raw images within 10 training episodes. In simulation, we observed a slight advantage when using a compressed state representation (provided by a Variational Autoencoder). We found the following hyperparameters to be most effective, which we use for our real world experiments: future discount factor of 0.9, noise half-life of 250 episodes, noise parameters of\\\\n\\u03b8\\\\nof 0.6 and\\\\n\\u03c3\\\\nof 0.4, 250 optimisation steps between episodes with batch size 64 and gradient clipping of 0.005.'\\n\\n'Table I shows the results of these experiments. Here, the major finding is that reinforcement learning can solve this problem in a handful of trials. Using 250 optimisation steps with batch size 64 took approximately 25 seconds, which made the experiment extremely manageable, considering manoeuvring the car to the centre of the lane to commence the next episode takes approximately 10 seconds anyway. We also observe in the real world, where the visual complexity is much more difficult than simulation, a compressed state representation provided by a Variational Autoencoder trained online together with the policy greatly improved reliability of the algorithm. We compare our method to a zero policy (driving straight with constant speed) and random exploration noise, in order to confirm that the trial indeed required a non-trivial policy.1'\\n\\n'The second area for development suggested by the results here is a better state representation. Our experiments have shown that a simple Variational Autoencoder greatly improves the performance of DDPG in the context of driving a real vehicle. Beyond pixel-space autoencoders is a wealth of computer vision research addressing effective compression of images: here existing work in areas such as semantic segmentation, depth, egomotion and pixel-flow provide an excellent prior for what is important in driving scenes [35], [1], [36]. This research needs to be integrated with reinforcement learning approaches for real tasks, both model-free and model-based.'\",\"846\":\"'http:\\/\\/www.cim.mcgill.ca\\/~mrl\\/adversarial_driving_scenarios'\",\"847\":\"'We also compare the performance with the existing open-source robot simulator V-REP [36] while changing its physics engine: Bullet, ODE, and Vortex. Each physics engine is widely used for robot simulation (e.g., bullet for Roboschool with OpenAI Gym, ODE for CHAI3D, and Vortex for many industrial training simulators). To compare them in identical environment, we use the remote API of the V-REP to control the virtual robot with C++ controller which is exactly same for the one used for the real KUKA robot. The configuration of V-REP simulator with the remote API is illustrated in Fig. 8.'\",\"848\":\"'https:\\/\\/github.com\\/profFan\\/UnrealOpticalFlowDemo'\\n\\n'https:\\/\\/github.com\\/ProfFan\\/assimp'\\n\\n'https:\\/\\/github.com\\/ProfFan\\/sdformat'\\n\\n'Numerous approaches have been proposed by researchers in the literature before. Some tried to create a simulator from the ground up, with entirely open source libraries and code. The best effort among all the robot simulators is Gazebo [1], which currently has a large number of scenes, models and plugins, in addition to a built-in scene and model editor. However, Gazebo is in short of high graphics quality to generate data for algorithm fine training. It also requires an intermediate message bridge to communicate with ROS, increasing the code complexity of the system. Meanwhile, many efforts have also been made in using powerful game engines to simulate robots and generate training data with high resolutions for algorithm development, such as URoboSim [2], AirSim [3], Sim4CV [4] and CARLA [5], just to name a few. Those simulation systems can generate very realistic 3D scenes and 2D images. However, they are not able to easily import external models, and usually a lot of manual labors are involved to create new scenes and models. The process also requires the use of proprietory vendor tools, such as the Unreal Editor, the wide applications of which usually are limited.'\\n\\n'Very little documentation has been released for the UE4 rendering pipeline. The pipeline is defined by four things: the C++ renderer code, the HLSL shader code, the Unreal Material Blueprints, and the engine settings. The engine settings actually affect the shader compile process directly by controlling a set of HLSL macros enabling and disabling certain parts of the HLSL code. To add more complexity, the Material Blueprints are first converted into HLSL code with a template file and then compiled into a GPU shader. These four parts are inter-dependent and hidden from the user, making it hard to use the Unreal rendering pipeline for non-standard algorithms.'\\n\\n'As a modern, high-performance game engine, Unreal has its own very strict C++ coding standards. As such, features like Run-Time Type Information (RTTI) and exceptions are not available. However, these features, though considered unfavorable by a large portion of the C++ community, are widely used by the entire ROS ecosystem, especially the core library roscpp. This creates a huge barrier for direct integration of ROS into UE4, as the C++ Application Binary Interface (ABI) is different for RTTI-enabled and RTTI-disabled code, preventing any form of direct binary interoperation.'\\n\\n'In the case of URoboSim, the authors tried to tackle this problem by converting the model files off-line. Off-line conversion methods have the advantage in the easeness of integration, as no code need to be written for the task. However, one significant limit of this method is that the engine itself does not directly read FBX files. FBX files are first loaded by the Unreal Editor, undergo many postprocessing steps, and finally imported as a.uasset file. This process is inside the Unreal Editor, which is not redistributable by license terms. Such a simulator will then become non-distributable in binary formats.'\\n\\n'To tackle this problem, we added a patch to the Unreal Engine postprocessing code, which essentially copies the contents of the velocity buffer to an unused custom buffer slot (Post Processing Input2 in Fig. 3), so it will be passed to the compiled shader code when our postprocessing shader executes. This effectively solved the problem and the performance impact is also minimal.'\\n\\n'If you want to integrate this pipeline into your work, the standalone optical flow generation code and a bash script for patching Unreal Engine can be retrieved at https:\\/\\/github.com\\/profFan\\/UnrealOpticalFlowDemo.'\\n\\n'In our design process, we tested each of the available approaches, and finally chose to use cROS, the C-based ROS client, as our main ROS interface, due to its small code footprint and versatility. cROS is the only non-dynamic ROS library that is able to read in ROS message definition files at runtime, allowing the user to specify new types of sensors and message types easily. As it is written in C, it does not have ABI compatibility issues and does not require the complex dependencies required by roscpp.'\\n\\n'Similar challenges in the ABI is also observed in the integration of assimp and sdformat, however, in this case, we solved the problem by rewriting the code of the two libraries to use no RTTI features. This is because compared to roscpp, these libraries are in general have less code complexity and have less dependencies, making rewriting easier.'\\n\\n'The source code of these libraries patched to work without RTTI can be retrieved on GitHub at https:\\/\\/github.com\\/ProfFan\\/assimp for assimp and https:\\/\\/github.com\\/ProfFan\\/sdformat for sdformat. An overview of this design is shown in Fig. 4.'\\n\\n'To test our SDF importing functionality, we tested the import function code with 5 models gathered from the internet. The results are shown in Table II. \\u201cGazebo\\u201d in the Source column means the model is collected from models.gazebosim.org, the official gazebo model collection, and \\u201cOfficial\\u201d means that the model is gathered from the robot\\u2019s official ROS package. An example of a successful import is shown in Fig 6.'\\n\\n'Refactor the code to increase the stability of our simulator against malformed Collada and STL data.'\\n\\n'Simulation environments play a centric role in the research of sensor fusion and robot control. This paper presents Pavilion, a novel open-source simulation system, for r...'\\n\\n'Simulation environments play a centric role in the research of sensor fusion and robot control. This paper presents Pavilion, a novel open-source simulation system, for robot perception and kinematic control based on the Unreal Engine and the Robot Operating System (ROS). The novelty of this work includes threefold: (1) developing a shader-based method to generate optical flow ground-truth data with the Unreal Engine, (2) developing a toolset to remove binary incompatibility between ROS and the Unreal Engine to enable real-time interaction, and (3) developing a method to directly import Simulation Description Format (SDF) robot models into the Unreal Engine at runtime. Finally, a Gazebo-compatible real-time simulation system is developed to enable training and evaluation of a large number of sensor fusion, planning, decision and control algorithms. The system can be implemented on both Linux and macOS, with the latest version of ROS. Various experiments have been performed to validate the superior performance of the proposed simulation environment over other state-of-the-art simulators in terms of number of modalities, simulation accuracy, latency and degree of integration difficulty.'\\n\\n'How to integrate open-source robot models for fast system development and validation.'\\n\\n'In this paper, we present Pavilion as a modular, systematic approach to integrate the open-source robot operating system and a powerful game engine, Unreal Engine 4 (UE4), as shown in Fig. 1. We develop a set of methods to patch UE4 for outputing real-time optical flow data, in addition to regular images, depth data and segmentation information [2], [3], [4], [5]. Besides, we develop a set of tools to enable Pavilion to directly import industry-standard Simulation Description Format (SDF) files at runtime without using UE4 vendor tools. Pavilion can interface with ROS using standard ROS messages, without using a separate bridge application. With the help of above features, the real-time sensorimotor control of imported robot models can be achieved in large-scale simulated environments using high-resolution cameras, LIDARs and RGBD sensors. Pavilion is also the first UE4based simulator that is able to generate ground-truth optical flow information in real time.'\\n\\n'In our design, we used the open-source library Open Asset Import Library (assimp) to directly read in the model information (vertices, edges, texture coordinates and textures) at runtime, and render them using Unreal\\u2019s procedural mesh component. This approach does not require Unreal\\u2019s asset cooking process, thus is much more versatile than the Editor approach.'\\n\\n'https:\\/\\/github.com\\/profFan\\/UnrealOpticalFlowDemo'\\n\\n'https:\\/\\/github.com\\/ProfFan\\/assimp'\\n\\n'https:\\/\\/github.com\\/ProfFan\\/sdformat'\",\"849\":\"'Augmented reality (AR) is a promising technology where the surgeon can see the medical abnormality in the context of the patient. It makes the anatomy of interest visible to the surgeon which otherwise is not visible. It can result in better surgical precision and therefore, potentially better surgical outcomes and faster recovery times. Despite these benefits, the current AR systems suffer from two major challenges; first, incorrect depth perception and, second, the lack of suitable evaluation systems. Therefore, in the current paper we addressed both of these problems. We proposed a color depth encoding (CDE) technique to estimate the distance between the tumor and the tissue surface using a surgical instrument. We mapped the distance between the tumor and the tissue surface to the blue-red color spectrum. For evaluation and interaction with our AR technique, we propose to use a virtual surgical instrument method using the CAD model of the instrument. The users were asked to reach the judged distance in the surgical field using the virtual tool. Realistic tool movement was simulated by collecting the forward kinematics joint encoder data. The results showed significant improvement in depth estimation, time for task completion and confidence, using our CDE technique with and without stereo versus other two cases, that are, Stereo-No CDE and No Stereo-No CDE.'\\n\\n\\\"Therefore in the current paper we tried to address these two major problems in medical AR research, i.e. the lack of correct depth perception and the lack of robust evaluation methods for surgical AR. We present a depth estimation technique where we mapped the distance between the tumor surface and the surgical instrument tip to the red-blue color spectrum. We called our technique Color Depth Encoding (CDE). Second, to bypass the very many calibrations steps to evaluate interactive AR techniques, we propose to use a virtually rendered surgical instrument using surgical robot's forward kinematic joint encoder data for a depth reaching task. The users were asked to take the virtual tool to the position of the judged distance.\\\"\\n\\n'In the following paragraphs to render the virtual tool at the right position various transformation matrices are calculated to make the virtual setup similar to the endoscope setup. Virtual Tool was rendered using CAD model of Intuitive Surgical Da Vinci (DV) Prograsp instrument. To simulate the realistic tool movement, the joint encoder data was obtained from Da Vinci clinical API. We collected the position and orientation information of the various joints of the surgical tool by reading Denavit-Hartenberg (DH) tables provided by the surgical API. The forward kinematics data thus obtained was used to estimate the model-view matrices\\\\n(\\\\nE\\\\nT\\\\nR\\\\n)\\\\nin OpenGL to model a virtual camera. This transformation brought the points from the robot base coordinate frame to robot endoscope coordinate system. Furthermore, to bring the points from the robot endoscope camera frame to the actual right and left camera frames, the hand-eye transformation matrix,\\\\nC\\\\nL\\\\nT\\\\nE\\\\n, was estimated.'\\n\\n'Windows 7 platform was used for the visualizations with 3.60-GHz Intel(R) Core i7. OpenGL version 4.5 and C++ programming language was used for the renderings. Intuitive Surgical Da Vinci Clinical API was used to collect the forward kinematics joint encoder data from the surgical robot. The stereo video stream with graphics was sent in through Intuitive surgical Da Vinci surgical console for the evaluation.'\\n\\n'For future work, it will be interesting to design experiments to study the affect of combination of CDE with other depth cues like motion parallax or inter-sensory cue like auditory information. Additionally, for evaluation, in the current work we modelled the patient side manipulator (PSM) information to render the virtual tool. It can be of interest to use the MTM joint encoder data to render the virtual tool and see its effects on the interaction. Using MTMs can open gates for more innovative evaluation tasks using a live video feed and thus replacing the real surgical tool from the camera endoscopic view.'\",\"850\":\"'In this paper, we devised a virtual mixed-traffic environment that can approximately emulate the intricate urban traffic for autonomous vehicle testing. Pedestrians, bicycles, and vehicles are considered as the main road users. Their behaviors are encoded in a general, unified force-based framework, whose forces can be classified as the desire force to a target, the repulsive forces with neighbors and the built environment, and interaction forces between different kinds of road-users. Our approach offers a simple, efficient, and extensible method to simulate different behavioral characteristics of different road users and the realistic interaction effects in complex urban traffic environments. Experimental results have been conducted to validate the performance of our approach by comparing with two popular simulation platforms specifically for autonomous vehicle testing.'\\n\\n'Interaction with Road-crossing Pedestrians: We compare our force-based approach for pedestrian crossing to two popular open-source simulation platforms: Apollo [12] and Carla [14]. Fig. 5 shows the comparison results between Apollo simulation and our method. It is observed that in Apollo simulation (Fig. 5(a)), the autonomous vehicle (in blue) reacted to a pedestrian (represented as yellow rectangle) crossing the road with an initially defined constant velocity of 2.24 km\\/h. Since the pedestrian\\u2019s reaction to the real-time behavior of the vehicle was not considered in Apollo simulation platform, the pedestrian continued to follow the predefined path after the vehicle had completely stopped, thereby colliding with the vehicle (at the 880th frame in Fig. 5(a)). By contrast, our force-based approach attempted to mimic the real pedestrian-vehicle interaction in which both the pedestrian and vehicle made decisions based on the other one\\u2019s instantaneous status (Fig. 5(b)).'\",\"851\":null,\"852\":null,\"853\":\"'Indoor mobile robots operate in spaces that are not as rigorously controlled as manufacturing areas, nor as rich with diversity as outdoor scenes. The open source release of pre-trained object detection models such as the TensorFlow Object Detection API [1] has been a boon to robotics, but in indoor spaces, many objects, particularly small ones, are omitted from the common object datasets. This is a hindrance for creating indoor robots that can be tasked to find or manipulate objects on tables, walls, and desks. Our aim is to develop a system that can be used to rapidly create customized detectors for vision-based robots that require real-time object detection. This is related to the challenge of using deep learning to perform visual SLAM [2] but with the objective of tasking the robot to use the objects rather than learning landmarks for visual navigation. This paper addresses the following challenges:'\",\"854\":\"'Generated synthetic clutter and color coded labels'\",\"855\":null,\"856\":\"'As a consequence, we propose a compressed MobileNet based on the aforementioned five rules. First, we achieve the feature map spatial \\u00d7 32 downsampling (224 \\u00d7 224 to 7 \\u00d7 7) via 8 bottlenecks, unlike the 16 bottlenecks used by MobileNe V2. This is achieved by reducing the bottlenecks for each spatial resolution. The fast downsampling strategy reinforces the neural network to encode feature expressiveness into feature map channels, relatively ignoring the spatial resolution. Second, the saved memory can be further used to further enhance the model expressiveness on 7 \\u00d7 7 feature map (plateau stage) by stacking multiple identity blocks (echoes rule 2) and 3)). Third, unlike MobileNet v2 that exploits feature map at different spatial resolution scales (14 \\u00d7 14, 7 \\u00d7 7, 4 \\u00d7 4, 2 \\u00d7 2 and 1 \\u00d7 1), compressed MobileNet merely extracts features from 14 \\u00d7 14 and 7 \\u00d7 7 scales. The reason is twofold: since vehicles in short-range aerial images are homogeneous and small in size, feature map with smaller spatial resolution is not that needed because smaller spatial resolution often corresponds to larger object detection in SSD detection framework due to the anchor point generation (echoes rule 4 multiple feature extraction from 7 spatial resolution fully leverages the powerful feature expressiveness we have constructed with multiple identity blocks (responsible for challenges in 5)).'\",\"857\":null,\"858\":null,\"859\":null,\"860\":null,\"861\":\"'To easy the accessibility and diffusion of the proposed testing method, we realized a custom designed system, developed with standard easy to reach components and the design will be open sourced through the Natural Machine Motion Initiative [21]. The system consist of a mechanical pendulum with a fixed length (Fig. 6). A mass is attached on the free end of the pendulum. Its oscillations are measured by a magnetic encoder (Austrian Microsystem AS5045). The hand is fixed to the base of the mechanical frame and it is connected to a Force\\/Torque Sensor (ATI OMEGA 160). Therefore, it is possible to evaluate the energy absorbed during the impact through the measure of the pendulum angular position, like in the Izod test (Fig. 3(b)) and the transmitted loads. The pendulum can impact the hand in different positions (single phalanx or multiple phalanxes, single finger or multiple fingers) with several kind of strikers, that can be arranged for each specific impact experiment. The set-up allows also, to tune the mass and the starting position of the pendulum to perform impacts with different energy levels. Tab. I reports the testing conditions and pendulum configuration of each load level tested, e.g. mass, starting position, linear velocity at the impact\\\\n(\\\\nV\\\\nimp\\\\n)\\\\n, pendulum momentum and its energy. According to Fig. 3(c), it is possible to identify the severity of the impact as low, medium and high, based on the pendulum initial energy. In the following, the load level will be indicated with the reference letter as in Tab. I. Experiments were performed on two soft robotic hands developed by our group: the WALK-MAN Hand [2] and the Pisa\\/IIT SoftHand [12]. In the following those will be named WM and SH respectively.'\",\"862\":\"'https:\\/\\/git;hub.com\\/cuhde\\/chimp'\\n\\n'http:\\/\\/www.ease-crc.org\\/'\",\"863\":\"'(a) A prototype of JASR. (b) A test rig which includes 1. JASR prototype, 2. torque sensor, 3. absolute encoder, 4. pendulum, 5. base.'\",\"864\":null,\"865\":null,\"866\":\"'Flexibility torque\\/transmission deformation map obtained by imposing a set of known torques using a force gage and measuring the transmission deformation as the difference between absolute encoder readings at each side of the flexible transmission.'\\n\\n'Thanks to the use of two built-in absolute encoders we demonstrate that we effectively have a joint torque sensor, which leads to the possible future work of conducting decoupling and feedback linearizing control.'\",\"867\":\"'The actuator exerts a sinusoidal torque with a frequency of 0.2 Hz and varying amplitude on the ROFL mechanism. The actuator output angle, measured with an integrated 21 bit encoder, equals the prototype deflection. The torque transmitted through the ROFL mechanism is perceived by the force-torque sensor on the other side of the prototype3.'\",\"868\":null,\"869\":null,\"870\":null,\"871\":\"'In theory, the membrane configuration at any contact state encodes precise information about the applied and internal forces being applied to it. However, recovering the membrane loads that transformed it from its initial body shape to a subsequent deformed one requires complex procedures [24] such as finite element modelling and other numerical optimization techniques [25] . Although elastic shell theory can be leveraged in solving this inverse elastostatic problem given our compatible fingertip configuration [26] , it is still too computationally demanding to be applied in robotic applications that require real-time feedback and control. For this reason, we follow the approach of similar devices and utilize approximate but much simpler models to characterize the force-deformation characteristics of our fingertip. As shown by Vella et al [27] , the indentation of pressurized elastic shells exhibit two near-linear force-deformation regimes at small and large indentation depths relative to the shell thickness respectively. Their setup was similar to ours in that upon contact, the module preserves neither volume nor internal pressure. In addition, a previous iteration of the fingertip sensor in our group similarly verified near-linear characteristics that matched up very well with the analytical Hertz contact model [28] . Therefore, we maintain an approximation of the tactile sensor at each internal gas state as an equivalent linear isotropic elastic sphere. We also restrict indentation geometries in characterization experiments to simple spherical geometries, such that the following Hertz contact relationship between the load force\\\\nF\\\\nand the indentation depth\\\\n\\u03b4\\\\nholds:'\",\"872\":null,\"873\":null,\"874\":null,\"875\":null,\"876\":\"'An illustration of the hardware Platform (a) CAD model of the mechanical components of the leg module, which includes ABAD, HIP and KNEE modules and linkages of the leg (b) A picture of the assembled quadruped platform, which integrates a computer, sensors including an IMU and 12 encoders'\",\"877\":null,\"878\":null,\"879\":null,\"880\":\"'We have shown by way of a few examples how the rulebooks approach allows easy and intuitive tuning of self-driving behavior. What is difficult to convey in a short paper is the ability of the formalism to scale up. In our production code at nuTonomy, corresponding to level 4 autonomy in a limited operating domain, our rulebooks have about 15 rules. For complete coverage of Massachusetts or Singapore rules, including rare corner cases (such as \\u201cdo not scare farm animals\\u201d), we estimate about 200 rules, to be organized in about a dozen ordered priority groups (Fig. 10).'\",\"881\":null,\"882\":null,\"883\":\"'https:\\/\\/youtu.be\\/Zj8-az5mxpw'\\n\\n'https:\\/\\/github.com\\/libai1943\\/Tractor-trailer-trajectory-planning-case-studies-ICRA-19'\\n\\n'https:\\/\\/github.com\\/libai1943\\/Tractor-trailer-trajectory-planning-case-studies-ICRA-19'\",\"884\":\"'http:\\/\\/bit.1y\\/skid_steer'\",\"885\":null,\"886\":\"'The MBBR uses the following sensors to estimate the states: the optical encoders and IMU gyrometer and accelerometer measurements. In particular, the accelerometer is greatly affected by the dynamic of the robot. Therefore, we need to derive the sensor dynamics before they can be used in the EKF. The encoders measure the ball rotation angle with respect to the body frame\\\\n(\\\\ny\\\\nB\\\\nen\\\\n = \\\\n\\u03c6\\\\nB\\\\n)\\\\n, where\\\\n\\u03c6\\\\n\\u02d9\\\\nB\\\\nwas derived in (10). The gyrometer measures the body angular velocity\\\\n(\\\\ny\\\\nB\\\\ngy\\\\n=\\\\n\\u03a9\\\\nB\\\\n)\\\\nwhich was derived in (7). The accelerometer measures the linear acceleration at the IMU\\u2019s position about the body frame. Let pa be the position of the IMU in the inertial frame and\\\\nl\\\\nB\\\\na\\\\n=[\\\\nl\\\\nax\\\\n, \\\\nl\\\\nay\\\\n, \\\\nl\\\\naz\\\\n]\\\\nT\\\\nis the length vector from ball\\u2019s center of mass to the IMU. Then using a similar kinematics derivation to (11), we can derive the accelerometer measurement dynamics below:'\\n\\n'Plot of RMSE between both estimators vs motion capture measurement, RMS of the robot\\u2019s attitude from motion capture and RMS of the ball speed from the encoder estimates.'\\n\\n'Special thanks to Wowwee for supplying us with the motors, encoders, motor mounts and the ball. Also special thanks to Clark Briggs from ATA Engineering for offering our team to use their motion capture system.'\",\"887\":null,\"888\":\"'This paper presents a methodology for synthesizing a motion plan for a mobile robot to ensure that the robot never gets depleted with battery charge while carrying out its mission successfully. The specification of the robot is provided in the form of an LTL (Linear Temporal Logic) formula. A trajectory satisfying an LTL formula may contain a loop whose repetitive execution causes the depletion of battery charge in the robot. The motion plan generated by our methodology ensures that the robot visits the charging station periodically in such a way that it never gets depleted with battery charge while carrying out its mission optimally. Given a set of potential charging station locations and an LTL specification, our algorithm also finds the best location for the charging station along with the optimal trajectory for the robot. We encode the motion planning problem as an SMT (Satisfiability Modulo Theory) solving problem and use the off-the-shelf SMT solver Z3 to solve the constraints to find the location of the charging station and generate an optimal trajectory for the robot. We apply our methodology to synthesize energy-aware trajectories for robots with different dynamics in various workspaces and for various LTL specifications.'\\n\\n'In this paper, our objective is to generate a trajectory for a robot automatically from a given LTL specification that along with the functional requirement also takes into account the energy requirement of the robot. Our approach is based on the composition of motion primitives, that respects the constraints imposed by the temporal logic specification. The motion primitives are utilized to build a system of constraints where the decision variables encode the choice of motion primitives used at any discrete-time point on the trajectory. The system of constraints involve a Boolean combination of linear constraints, which can be solved using an off-the-shelf SMT (Satisfiability Modulo Theories) solver [29]. Our choice of the SMT-based approach has been driven by the recent success of SMT solvers to solve several task and motion planning problems (e.g. [30], [31], [32], [7], [33], [34], [35]).'\",\"889\":\"'Orthogonal to these methods are database approaches. Instead of modifying sampling, these methods leverage previous experiences by storing discrete paths or graphs in a database. This information is later retrieved and repaired\\/transformed to satisfy the new kinematic constraints. These methods can be thought of as hard-coded experiences compared to biasing the sampling.'\",\"890\":null,\"891\":null,\"892\":null,\"893\":\"'https:\\/\\/www.youtube.com\\/watch?v=SuQzxAusU_0'\\n\\n'Learning-based methods for various prediction tasks. Many learning-based prediction methods follow an RNN encoder-decoder structure [16] and use it for predicting the trajectory of pedestrians [17]\\u2013[19] and vehicles [2, 18]. RNNs are an alternative to the traditional methods (e.g., SVM, Gaussian process) [20, 21] for capturing maneuver patterns. Some works move one step further and consider the multi-agent interactions among pedestrians [9, 18, 22, 23] and vehicles [2, 18] in the RNN structure. The interaction-aware models for vehicles mainly adopt a social pooling strategy using RNN hidden states, which may be problematic for a highly dynamic scenario, as elaborated in the following.'\\n\\n'B. RNN Encoder Network'\\n\\n'For brevity, we do not include details of the RNN encoder structure, and refer interested readers to [9] and [18] for the details. It is worth noting that, during the training phase, all the RNNs share the same set of weights. After the RNN encoding, the maneuver history of each vehicle\\\\nv\\u2208\\\\nV\\\\ne\\\\nis encoded in a vector\\\\nh\\\\nv\\\\n\\u225cRNN(\\\\nf\\\\nm\\\\n(\\\\nz\\\\ne,v\\\\n0:t\\\\n))\\u2208\\\\nR\\\\nr\\\\n, where\\\\nr\\\\ndenotes the size of the RNN encoding. All the encodings can be computed and stored before the inference of the VBIN.'\",\"894\":\"'Autonomously driven vehicles are seen as one major potential solution for multiple issues in transportation. Having safer roads, reducing road congestion and increasing one\\u2019s mobility are all examples of these potential advantages. The classical autonomous driving solutions have an encode-decode pipeline process of two modules, the perception module and the control module. The former encodes the visual data based on human-selected features and feeds it to the latter which decodes the representation to estimate the steering control based on defined rules [1]\\u2013[4]. However, end-to-end steering couples the whole pipeline process in a single module that performs the encoding-decoding process through deep learning. Further, advances in deep learning for computer vision [5, 6] motivates the use of visual data in an end-to-end approach within the scope of autonomous driving.'\\n\\n'This dataset is provided internally by BMW and is not available for public use. The data is collected from an onboard vehicle camera on one of the BMW models while driving mainly on highway roads. The decoded RGB front camera frames are of the size\\\\n960\\u00d7540\\u00d73\\\\n. No extreme weather conditions were available in the dataset. Only lane keeping frames were used. The data was split into training and validation subsets of 140,201 and 11,425 frames, respectively. The steering data takes values between \\u201310\\u00b0 and 10\\u00b0, with a mean of 0. 11\\u00b0 and a standard deviation of 1.90\\u00b0 and 2.15\\u00b0 for the training and validation sets, respectively. Vehicle speed data yielded relatively high means of 106.76 kmffi and 132.64 kmm for the training and the validation datasets, respectively.'\\n\\n'Initially, we decode the BMW data frames into the RGB format with the size of\\\\n960\\u00d7540\\u00d73\\\\n. We crop the hood and the sky as they contain no information and resize the frames to\\\\n200\\u00d766\\u00d73\\\\n. Tests on the BMW dataset showed no significant differences between results for the YUV and the gray-scale color-spaces. Therefore, we use the gray-scale format. The work by Nvidia mentions that each frame is normalized. However, the normalization details were not stated. We perform image normalization using subtraction of the mean and dividing by the standard deviation. In the cases where the input consists of multiple modalities, such as that described in Sub section IV-B.4, we apply the normalization on each modal input separately and stack the normalized inputs to form a single 3D tensor. No data augmentation was implemented.'\\n\\n'The work in this thesis utilizes two sets, the BMW dataset and the open-source Udacity dataset. Figure 1 shows scattered samples of both sets. We use the former for the empirical analysis of our approach and the latter to validate our findings against previous research.'\\n\\n'We examine empirically the problem of end-to-end learning for autonomous steering using multiple deep learning concepts. We conclude that feeding a multimodal input of spatial and optical flow information benefits the learning process the most. We validate our hypothesis that the RNN-LSTM generates the smoothest output. Through the analysis of the results, we propose a multimodal recurrent model that outperforms state-of-the-art work on the open-source dataset of Udacity. Further, We affirm empirically an off-lane recovery mechanism through the inclusion of correction frames.'\\n\\n'Udacity organized a challenge where several teams proposed their solution for an open source dataset 2 and a leaderboard of the best results was posted. Other work on spatial neural networks was done by [10] through transfer learning using a pre-trained ResNet -50 [6] and by [13] through proposing a simple deep CNN. Both architectures yielded promising results in imitating the driver\\u2019s behavior. Moreover, the work by [10, 11, 13] examined the problem with spatio-temporal architectures, which are models that would not only encode the visual data from a single frame but also encode the relation between the different frames. For example, models such as a recurrent neural network (RNN) with an LSTM cell were found to boost the performance of the network in most cases. Moreover, adopting the concept of multi-modality was also reported to enhance the performance. For instance, the work in [11] coupled the loss with the torque and the speed while the work in [13] incorporated the speed in its model input. Previous research focuses on generating a model that imitates the learned behavior from the supervised data. However, it mostly lacks assessment of the smoothness of the steering command regressed. A continuous-smooth command is critical so as not to have a zigzag behavior and to provide a safe and convenient driving for the passengers. Thus, further analysis will be done in regard to this matter in this research.'\",\"895\":null,\"896\":null,\"897\":null,\"898\":null,\"899\":null,\"900\":\"'https:\\/\\/www.youtube.com\\/watch?v=bD7FItjOWY0'\",\"901\":\"'https:\\/\\/goo.gl\\/5F9dP4'\\n\\n'We aim to lower the barrier to entry into manipulation research not just by keeping REPLAB costs low, but also by emphasizing ease of use and reproducible algorithm implementations. In particular, we will release all code in a Docker image that runs out of the box on Ubuntu machines, for quick reproducibility. Our image contains scripts for automatic data collection, grasp success annotation through the trained classifier, camera calibration, noisy control compensation, and benchmark evaluation. Further, it includes REPLAB-specific implementations of several baselines for grasping, described in the next section. With this image, setting up an Ubuntu laptop to start collecting data on a REPLAB cell takes about five minutes.'\\n\\n'We also plan to develop a larger challenge dataset for grasping, and release open-source code for robotic control approaches such as visual servoing, video prediction-based model predictive control, and reinforcement learning on the REPLAB platform. We invite other dataset and software contributions from the robotics research community.'\",\"902\":null,\"903\":\"'https:\\/\\/github.com\\/boschresearch\\/STAAMS-SOLVER'\\n\\n'https:\\/\\/github.com\\/boschresearch\\/STAAMS-SOLVER'\",\"904\":null,\"905\":null,\"906\":null,\"907\":\"'In this paper, a vision guidance system with a deep reinforcement learning based visual-servoing method for robotic motion control is developed for personalized stent graft manufacturing. A hybrid vision system consists of a static stereo webcam and a dynamic moving stereo microscope is proposed for real-time target object tracking and feature detection of very fine robotic needle manipulation for stent graft manufacturing. The precision errors of the computed 3D positions of the tracked object demonstrated the accuracy of the coordinate mapping and object localization. In addition, the use of a DDPG algorithm for visual-servoing is proposed for automatic actuation of the robotic arm to reach the desired locations. The experiment results have shown the ability of the proposed for self-optimization of the robotic control through the learning process. This paper presents a novel intelligent visual robotic control system for stent graft manufacturing. Future work will focus on the implementation of an end-to-end policy for learning hand-eye coordination for robotic reaching and following, combining a deep autoencoder with reinforcement learning. Instead of using the conventional stereo vision technique for object identification and 3D localization that are applied in this paper, the state space representation can also be learnt directly from the monocular images independently of the camera calibration.'\",\"908\":\"'The proposed graph matching is based on topology, implying that it is sensitive to topology variance. The five specified main branches are required to be the longest ones in a skeleton during the registration. and other branches could be done semi-automatically with our open-source code. The proposed method might also be sensitive to transverse and sagittal transformations, as branch node assigning based on the projected path length of 3D branches. In practical application, this should not be a problem, as the C-arm is usually fixed at a frontal scan to save space for surgeons.'\\n\\n'A real-time framework of 3D intra-operative AAA skeleton instantiation from a single 2D AAA fluoroscopic image is proposed for 3D robotic path planning. Pre-operative 3D AAA skeleton is deformed to match the intra-operative 2D AAA skeleton with graph matching and smooth\\/length maintenance. Furthermore, deep learning is used to facilitate the framework automation. Comparable accuracy, robustness and time efficiency are achieved, however the proposed method could suffer from influences of topological variance and transformation in the saggital and transverse plane. The code is available and maintained online1'\",\"909\":\"'Object Detection And Autoencoder-Based 6d Pose Estimation For Highly Cluttered Bin Picking'\\n\\n'Camera viewpoint selection is an important aspect of visual grasp detection, especially in clutter where many occlusions are present. Where other approaches use a static camera position or fixed data collection routines, our Multi-View Picking (MVP) controller uses an active perception approach to choose informative viewpoints based directly on a distribution of grasp pose estimates in real time, reducing uncertainty in the grasp poses caused by clutter and occlusions. In trials of grasping 20 objects from clutter, our MVP controller achieves 80% grasp success, outperforming a single-viewpoint grasp detector by 12%. We also show that our approach is both more accurate and more efficient than approaches which consider multiple fixed viewpoints. Code is available at https:\\/\\/github.com\\/dougsm\\/mvp_grasp.'\",\"910\":null,\"911\":null,\"912\":null,\"913\":null,\"914\":null,\"915\":null,\"916\":null,\"917\":\"'Encoder and Decoder.'\",\"918\":\"'In this section we show how the proposed approach scales to complex problems using two different scenarios. First, a KUKA Youbot scenario, where we show the applicability of our approach on dynamic and unpredictable environments, highlighting the importance of continually planing and acting. Second, an ABB Yumi industrial manipulator scenario, where we highlight the applicability of our approach to real world plans that require the execution of a long sequence of actions. The experiments were carried out using the physic simulator V-REP, in-house implementations of low level controllers for actions and conditions and our open source BT library [20]. The action refinement algorithm used is a modified version of the one used in the HBF algorithm [38].'\",\"919\":\"'https:\\/\\/github.com\\/tensorflow\\/models\\/tree\\/master\\/research\\/cognitive_planning'\\n\\n'https:\\/\\/github.com\\/tensorflow\\/models\\/tree\\/master\\/research\\/cognitive_planning'\",\"920\":\"'The generic form of our model takes in an RGB image and outputs two sets of features: global image contextual features and an object-centric representation. The global contextual features are produced by a convolutional network over the whole image, followed by a global average pooling operation. The object-centric representation is constructed as described below to produce a fixed-length object-centric representation. The global features are concatenated with the object representation, and passed to a fully connected policy network which outputs a discretized action. For on-policy evaluation, a hard-coded PID controller converts the action to low-level throttle, steer, and brake commands.'\",\"921\":\"'Predicting RMPs. To address the limitations of the abovementioned approaches, we propose a new model that predicts RMPs from visual images. RMP has merits from both schemes. The acceleration component f in a RMP is the command applied to a control point, whereas the metric component A encodes the local geometry of that control point. Moreover, by incorporating the control point Jacobians and the kinematic model, we can solve the optimal control command for the vehicle by combining the contribution of each control point in a geometrically and kinematically consistent manner, potentially having a more interpretable and generalizable model.'\",\"922\":null,\"923\":null,\"924\":null,\"925\":null,\"926\":\"'https:\\/\\/sit;es.google.com\\/view\\/visionandtouch'\\n\\n'Neural network architecture for multimodal representation learning with self-supervision. The network takes data from three different sensors as input: RGB images, F\\/T readings over a 32ms window, and end-effector position and velocity. It encodes and fuses this data into a multimodal representation based on which controllers for contact-rich manipulation can be learned. This representation learning network is trained end-to-end through self-supervision.'\\n\\n'A. Modality Encoders'\\n\\n'Our model encodes three types of sensory data available to the robot: RGB images from a fixed camera, haptic feedback from a wrist-mounted force-torque (F\\/T) sensor, and proprioceptive data from the joint encoders of the robot arm. The heterogeneous nature of this data requires domain-specific encoders to capture the unique characteristics of each modality. For visual feedback, we use a 6-layer convolutional neural network (CNN) similar to FlowNet [22] to encode\\\\n128\\u00d7128\\u00d73\\\\nRGB images. We add a fully-connected layer to transform the final activation maps into a 128-d feature vector. For haptic feedback, we take the last 32 readings from the six-axis F\\/T sensor as a\\\\n32\\u00d76\\\\ntime series and perform 5-layer causal convolutions [39] with stride 2 to transform the force readings into a 64-d feature vector. For proprioception, we encode the current position and velocity of the end-effector with a 2-layer multilayer perceptron (MLP) to produce a 32-d feature vector. The resulting three feature vectors are concatenated into one vector and passed through the multimodal fusion module (2-layer MLP) to produce the final 128-d multimodal representation.'\\n\\n'The modality encoders have nearly half a million learnable parameters and require a large amount of labeled training data. To avoid manual annotation, we design training objectives for which labels can be automatically generated through self-supervision. Furthermore, representations for control should encode the action-related information. To achieve this, we design two action-conditional representation learning objectives. Given the next robot action and the compact representation of the current sensory data, the model has to predict (i) the optical flow generated by the action and (ii) whether the end-effector will make contact with the environment in the next control cycle. Ground-truth optical flow annotations are automatically generated given proprioception and known robot kinematics and geometry [22], [26]. Ground-truth annotations of binary contact states are generated by applying simple heuristics on the F\\/T readings.'\\n\\n'The next action, i.e. the end-effector motion, is encoded by a 2-layer MLP. Together with the multimodal representation it forms the input to the flow and contact predictor. The flow predictor uses a 6-layer convolutional decoder with up-sampling to produce a flow map of size\\\\n128\\u00d7128\\u00d72\\\\n. Following [22], we use 4 skip connections. The contact predictor is a 2-layer MLP and performs binary classification. As discussed in Sec. II-B, there is concurrency between the different sensory streams leading to correlations and redundancy, e.g., seeing the peg, touching the box, and feeling the force. We exploit this by introducing a third representation learning objective that predicts whether two sensor streams are temporally aligned [40]. During training, we sample a mix of time-aligned multimodal data and randomly shifted ones. The alignment predictor (a 2-layer MLP) takes the low-dimensional representation as input and performs binary classification of whether the input was aligned or not.'\\n\\n'Three modalities are encoded and fused by our representation model: RGB images, force readings, and proprioception (see Fig. 2). To investigate the importance of each modality for contact-rich manipulation tasks, we perform an ablative study in simulation, where we learn the multimodal representations with different combinations of modalities. These learned representations are subsequently fed to the TRPO policies to train on a task of inserting a square peg. We randomize the configuration of the box position and the arm\\u2019s initial position at the beginning of each episode to enhance the robustness and generalization of the model.'\\n\\n'We examined the value of jointly reasoning over time-aligned multisensory data for contact-rich manipulation tasks. To enable efficient real robot training, we proposed a novel model to encode heterogeneous sensory inputs into a compact multimodal representation. Once trained, the representation remained fixed when being used as input to a shallow neural network policy for reinforcement learning. We trained the representation model with self-supervision, eliminating the need for manual annotation. Our experiments with tight clearance peg insertion tasks indicated that they require the multimodal feedback from both vision and touch. We further demonstrated that the multimodal representations transfer well to new task instances of peg insertion. For future work, we plan to extend our method to other contact-rich tasks, which require a full 6-DoF controller of position and orientation. We would also like to explore the value of incorporating richer modalities, such as depth and sound, into our representation learning pipeline, as well as new sources of self-supervision.'\",\"927\":\"'Estimation of tactile properties from vision, such as slipperiness or roughness, is important to effectively interact with the environment. These tactile properties help us decide which actions we should choose and how to perform them. E.g., we can drive slower if we see that we have bad traction or grasp tighter if an item looks slippery. We believe that this ability also helps robots to enhance their understanding of the environment, and thus enables them to tailor their actions to the situation at hand. We therefore propose a model to estimate the degree of tactile properties from visual perception alone (e.g., the level of slipperiness or roughness). Our method extends a encoder-decoder network, in which the latent variables are visual and tactile features. In contrast to previous works, our method does not require manual labeling, but only RGB images and the corresponding tactile sensor data. All our data is collected with a webcam and uSkin tactile sensor mounted on the end-effector of a Sawyer robot, which strokes the surfaces of 25 different materials. We show that our model generalizes to materials not included in the training data by evaluating the feature space, indicating that it has learned to associate important tactile properties with images.'\\n\\n'Our proposed network consists of 2D convolution layers for encoding, 3D deconvolution layers for decoding, and a multi layer perceptron (MLP) as hidden layers between the encoder\\\\nf\\\\n\\u03b8\\\\nand decoder\\\\ng\\\\n\\u03b8\\\\n. Convolutional neural networks (CNNs) are neural networks that convolve information by sliding a small area called a filter. 2D convolutions are often used in CNNs for static images with the purpose of sliding the filter along the image plane. For images with time series information (e.g., a video), 3D convolutions are used instead to convolve information by sliding a small cubical region along 3D space [28]. Our network outputs a time series sequence of tactile data consisting of applied forces and shear forces, while the input is an edge extracted image from the RGB image to prevent correlation to colors. The latent variables z are calculated with training data\\\\nD={(\\\\nx\\\\n1\\\\n,\\\\ny\\\\n1\\\\n),\\u2026,(\\\\nx\\\\nn\\\\n,\\\\ny\\\\nn\\\\n)}\\\\nto minimize the cost function L as follows:'\\n\\n'Proposed network architecture for deep visuo-tactile learning composed of encoder-decoder layers and latent variables. Input is texture image of material and, output is the tactile data contains measured forces by a tactile sensor in the x, y, and z axes. After training, latent variables would contain tactile properties of materials correlating images with tactile sense.'\\n\\n'Comparison model composed of two encoder components for image and tactile sequences, hidden layer as latent variables, and classification layer for output.'\\n\\n'We proposed a method to estimate tactile properties from images, called deep visuo-tactile learning, for which we built an encoder-decoder network with latent variables. The network is trained with material texture images as input and time series sequences tactile acquired from a tactile sensor as output. After training, we obtained a continuous latent space representing tactile properties with degrees for various materials. Our experiments showed that unlike conventional methods relying on classification, our network is able to deal with unknown material surfaces and adapted the latent variables accordingly without the need of manually designed class labels.'\",\"928\":null,\"929\":null,\"930\":null,\"931\":\"'We develop a drift-free roll and pitch attitude estimation scheme for monopedal jumping robots. The estimator uses only onboard rate gyroscopes and encoders and does not ...'\\n\\n'We develop a drift-free roll and pitch attitude estimation scheme for monopedal jumping robots. The estimator uses only onboard rate gyroscopes and encoders and does not rely on external sensing or processing. It is capable of recovering from attitude estimate disturbances and, together with onboard velocity estimation, enables fully autonomous stable hopping control. The estimator performs well on a small untethered robot capable of large jumps and extreme stance accelerations. We demonstrate that the robot can follow a rectangular path using onboard dead-reckoning with less than 2 meters of drift over 200 seconds and 300 jumps covering 60 m. We also demonstrate that the robot can operate untethered outdoors under human wireless joystick direction.'\",\"932\":\"'S=ZV\\\\nZ\\\\nK\\\\nis the set of game states. Each state encodes the current DFA state, the domain state, and the number of human actions remaining.'\\n\\n'The monolithic approach fails to scale well because the formula we input to the LTLf synthesis tool is unnecessarily large. Using this insight, we propose the compositional approach, which is the main contribution of this paper. Since the planning domain is defined by functions and sets, we can encode the planning domain directly as BDDs, and pass only the task through the conversion from LTLf to a DFA. The task is often a much smaller formula than the domain, thus we alleviate the LTLf to DFA conversion bottleneck. Once the task is converted to a DFA, the compositional approach directly combines it with the BDDs representing the planning domain to generate a product DFA. The symbolic synthesis tool then computes a strategy on this product DFA. Specifically, we perform the following steps.'\",\"933\":null,\"934\":null,\"935\":null,\"936\":null,\"937\":null,\"938\":\"'https:\\/\\/github.com\\/EIT-team'\\n\\n'https:\\/\\/github.com\\/EIT-team'\",\"939\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/32368362'\\n\\n'Current limitation of our work is that the tool must be kept fixed relative to the robot end-effector and cannot rotate around its axis, so the tool coordinate frame would be always aligned with the body frame. Thus, we will have the measured sclera forces in the body frame as required for the adaptive control. In the future, by attaching a rotary encoder to the tool it would be possible to implement the same adaptive control strategy while the tool is free to rotate. To sum up, in this paper we explored the feasibility of extending a 1-D adaptive control for 3-D applications in robot-assisted eye surgery including the sclera force control, insertion depth control and the robot-assisted light pipe holding. Several experiments were then conducted to evaluate how these three applications may contribute to safe robot-assisted retinal surgery. In the future we plan to expand the range of tasks, conduct multi-user experiments to evaluate the control performance under different behaviors, and also transition to in-vivo experiments.'\",\"940\":null,\"941\":null,\"942\":\"'Image descriptors training with auxiliary depth data (our work): two encoders are used for extracting deep features map from the main image modality and the auxiliary reconstructed depth map (inferred from our deep decoder). These features are used to create intermediate descriptors that are finally concatenated in one final image descriptor.'\\n\\n'We design a new global image description for the task of image-based localization. We first extract dense feature maps from an input image with a convolutional neural network encoder\\\\n(\\\\nE\\\\nI\\\\n)\\\\n. These feature maps are subsequently used to build a compact representation of the scene\\\\n(\\\\nd\\\\nI\\\\n)\\\\n. State-of the-art features aggregation methods can be used to construct the image descriptor, such as MAC [5] or NetVLAD [2]. We enhance this standard image descriptor with side depth map information that is only available during the training process. To do so, a deep fully convolutional neural network decoder\\\\n(\\\\nD\\\\nG\\\\n)\\\\nis used to reconstruct the corresponding depth ma according to the input image. The reconstructed depth is then used to extract a global depth map descriptor. We follow the same procedure used before: we extract deep feature maps with an encoder\\\\n(\\\\nE\\\\nD\\\\n)\\\\nbefore building the descriptor\\\\n(\\\\nd\\\\nD\\\\n)\\\\n. Finally, the image descriptor and the depth map descriptor are L2 normalized to be concatenated into a single global descriptor. Figure 1 summarizes the whole process of our method. Once trained with geometric and radiometric information, the proposed method is used on images only, to create a descriptor tuned for image localization.'\\n\\n'We train the depth map encoder and descriptor\\\\n{\\\\nE\\\\nD\\\\n, \\\\nd\\\\nD\\\\n}\\\\nin a same manner, with the triplet loss of equation (1),\\\\nL\\\\nf\\\\n\\u03b8\\\\nD\\\\n(\\\\nq\\\\n^\\\\ndepth\\\\n,\\\\nq\\\\n^\\\\n+\\\\ndepth\\\\n,\\\\nq\\\\n^\\\\n\\u2212\\\\ndepth\\\\n)\\\\n, where\\\\nf\\\\n\\u03b8\\\\nD\\\\n(\\\\nx\\\\ndepth\\\\n)\\\\nis the global descriptor of depth map xdepth and\\\\nx\\\\n^\\\\ndepth\\\\nis the reconstructed depth map of image xim by the decoder DG:'\\n\\n'One advantage of the hallucination network over our proposal is that it does not require a decoder network, resulting on a architecture lighter than ours. However, it needs a pre-training step, where image encoder and depth map encoder are trained separately from each other before a final optimization step with the hallucination part of the system. Our system do not need such initialization. Training the hallucination network requires more complex data than the proposed method. Indeed, it needs to gather triplets of image, and depth map pairs, which require to know the absolute position of the data [2], [6], or to use costly algorithms like Structure from Motion (SfM) [28], [5], [3].'\\n\\n'Encoder architectures.'\\n\\n'Resnet18 (R) versus truncated Resnet18 (Rt) in combination with NetVLAD pooling: we show the importance of the spatial resolution of the deep feature maps of the encoder used with NetVLAD layer. The truncated version of Resnet18, more than two times lighter than the complete one, achieves much better localization results.'\\n\\n'Decoder architecture.'\\n\\n'Effect of fine tuning with night images on decoder output:. Decoder trained with daylight images is unable to reconstruct the scene geometry (bottom line). Fine tuning the network with less than 1000 pairs \\\\\\\\{image, depth map\\\\\\\\}acquired by night highly improves appearance of the generated depth maps. Maps best viewed in color.'\\n\\n'Results on Night\\/Day query set after fine tuning: we are able to drastically improve localization performance for the Night\\/Day challenging scenario by only fine tuning the decoder part of our network with weakly annotated data. Curves best viewed in color.'\",\"943\":null,\"944\":\"'https:\\/\\/youtu.be\\/MSvoQT__c9U'\\n\\n'https:\\/\\/youtu.be\\/MSvoQT__c9U'\",\"945\":null,\"946\":null,\"947\":null,\"948\":\"'http:\\/\\/tiny.cc\\/fast-periodic'\\n\\n'http:\\/\\/tiny.cc\\/fast-periodic'\",\"949\":\"'The pipeline of the transfer learning approach proposed. The human demonstrated motion trajectories are normalized and encoded by Self-similarity Matrices. The learned segmentation policy is transferred to segment new tasks.'\",\"950\":\"'The algorithm PC collects the subject\\u2019s metabolic data, runs the optimization code, and sends the assistance profile parameters that will be explored in the next condition to the host laptop. Ethernet communication is used between a real-time target machine, a host laptop, and the algorithm PC. Further details can be found in [18].'\",\"951\":null,\"952\":null,\"953\":null,\"954\":null,\"955\":\"'Insects and hummingbirds exhibit extraordinary flight performance and can simultaneously master seemingly conflicting goals: stable hovering and aggressive maneuvering, which are unmatched by conventional small scale man-made vehicles. Flapping Wing Micro Air Vehicles (FWMAVs) hold great promise for closing this performance gap. However, design and control of such systems remain challenging. Here, we present an open source high fidelity dynamic simulation for FWMAVs. The simulator serves as a testbed for the design, optimization and flight control of FWMAVs. To validate the simulation, we recreated the at-scale hummingbird robot developed in our lab in the simulation. System identification was performed to obtain the model parameters. Force generation and dynamic response of open-loop and closed loop systems between simulated and experimental flights were compared. The unsteady aerodynamics and the highly nonlinear flight dynamics present challenging control problems for conventional and learning control algorithms such as Reinforcement Learning. The interface of the simulation is fully compatible with OpenAI Gym environment. As a benchmark study, we present a linear controller for hovering stabilization and a Deep Reinforcement Learning control policy for goal-directed maneuvering. Finally, we demonstrate direct simulation-to-real transfer of both control policies onto the physical robot, further demonstrating the fidelity of the simulation.'\\n\\n'To facilitate the design of FWMAV platforms and the study of flapping flight control in general, we present an open source high fidelity dynamic simulation for FWMAVs and flapping-wing animals such as hummingbirds and insects. Using the flapping-wing robot developed in our lab [7] as a blueprint, we built its virtual counterpart in the simulation environment. The simulation is written in C++ with Python binding, using customized flapping-wing aerodynamic models and DART [14] physics engine to solve multi-body kinematics and dynamics. The physical parameters were obtained by performing system identification on the robot. The aerodynamic modeling is validated through wing kinematics and force\\/torque measurements. Open loop flight tests were conducted and state transition statistics was verified. Finally, we demonstrate that the fidelity of the simulation is suitable for continuous control tasks. A feedback flight controller is designed in the simulation to achieve stable position tracking for the robot. When transfer to the robotic platform, the same flight performance was achieved on the vehicle by directly implementing the simulated controller onboard the robot. We also developed a goal-directed flight maneuvering control policy using deep reinforcement learning. The policy was optimized in simulation and directly transferred to the robot. Successful transferring of both controllers further validates the fidelity and effectiveness of the simulation.'\\n\\n'In this work, we developed an open source high fidelity simulation with realistic multi-body dynamics for flapping wing flight. Instantaneous aerodynamics were simulated using blade element theory and quasi-steady aerodynamic model which was validated by force measurements. Open loop state transition dynamics of the simulation is validated by calculating the state transition error between the simulation and the vehicle. The error shows the simulation can accurately predict instantaneous state transitions and capture the dynamic effects. With successful sim-to-real transferring, we demonstrate the fidelity of the simulation in two controller design applications: 1) a linear cascading PID controller for FWMAV position control, 2) unique goaldirected maneuvering of FWMAV using a policy optimized by reinforcement learning. For both applications, no special treatments were needed in controller implementation. The experimental data match simulation results, proving the fidelity of the simulation. With motor and contact dynamics, the current feedback could also be used as tactile sensing to mimic animal somatosensory [27], which could be exploited in simulation for control and trajectory planning design. This open source simulation can serve as a design and flight control testbed for scientists and researchers interested in studying flapping wing animals and robots. The code, baselines, and data will be available online, and experimental support on the robot will be provided.'\",\"956\":\"'One of the main challenges in sampling-based motion planners is to find an efficient sampling strategy. While methods such as Rapidly-exploring Random Tree (RRT) have shown to be more reliable in complex environments than optimization-based methods, they often require longer planning times, which reduces their usability for real-time applications. Recently, biased sampling methods have shown to remedy this issue. For example Gaussian Mixture Models (GMMs) have been used to sample more efficiently in feasible regions of the configuration space. Once the GMM is learned, however, this approach does not adapt its biases to individual planning scene during inference. Hence, we propose in this work a more efficient sampling strategy to further bias the GMM based on visual input upon query. We employ an autoencoder trained entirely in simulation to extract features from depth images and use the latent representation to adjust the weights of each mixture components in the GMM. We show empirically that this improves the sampling efficiency of an RRT motion planner in both real and simulated scenes.'\\n\\n'B. Autoencoder'\\n\\n'Given a depth image, the decoder uses the latent code generated by the encoder to predict a clean input reconstruction xrec, semantic segmentation xsec and object boundary xbou shown in Fig. 5. The loss functions are smooth Ll, multi-class and binary cross-entropy respectively. On-line bootstrapping [28] is implemented in a113 losses, and only pixels with the highest loss per image are back-propagated.'\\n\\n'(a) Dense-layer with bottleneck; (b) Transition-layer in encoder and decoder. (c) Autoencoder with weight prediction head. The number of trainable network parameters is 1. 9M, which split equally (0.95M) among encoder and decoder.'\\n\\n'TABLE I Combinations of Multi-Tasked Decoders (Left) and Randomized Adversarial Augmentation Strengths\\\\n\\u03f5\\\\n(Right).'\\n\\n'The additional semantic segmentation and boundary prediction tasks force the latent code to be more informative without increasing the code length. This resulted in a more efficient database search as well as better robustness to noise.'\",\"957\":null,\"958\":\"'Improving Disparity Estimation with Sub-pixel Convolutions Using the insight of operating at high-resolution disparity regimes, we discuss the importance of super-resolving low-resolution disparities estimated within Encoder-Decoder-based disparity networks [9], [13], [1]. With SuperDepth-SP, we are able to achieve a considerable improvement in performance (0.112 abs. rel.) for the same input image resolution over our established baseline (0.116 abs. rel Furthermore, we notice that the Sq. Rel., RMSE,\\\\n\\u03b4<z\\\\ncolumns show equally consistent and improved performance over the baseline that utilizes resize-convolutions [11] instead for disparity up-sampling.'\",\"959\":null,\"960\":\"'Due to their success, DCNNs have been used increasingly in robotic applications. Early examples of this include learning sparse auto-encoder features for almond detection [17] and for using a pre-trained DCNN as a feature extractor [18], [19]. More recently, DCNNs have been used for a broad range of tasks including semantic segmentation [20], stem localization for plants [21] and trip hazard detection, and detection of dynamic object instances in urban environments [22]. Furthermore, semantic groups from vision data have been utilized for mapping [23], navigation and path planning [24].'\",\"961\":\"'ORB-SLAM2: We use the monocular version of this open source method [19]. This is a significantly more sophisticated pipeline than Algorithm 1, that includes key-frame management, different local\\/global bundle adjustment threads, and a place recognition module for re-localisation.'\",\"962\":\"'The targets collected by the robots are soft cubes with an AprilTag (2D barcode fiducials developed for robotics applications [38]) on each face. For the first two sets of experiments, targets were distributed in the arena in clusters of various sizes with locations determined at random. The collection zone in the center of the arena was a square area with AprilTags on its boundary. The camera detected the AprilTags which were translated into a location in space relative to the robot\\u2019s position. This allows the robot to pick up targets in the arena and drop them off in the collection zone. Physical robots used a gripper with an actuated wrist for grabbing and dropping targets, which was also simulated in Gazebo. Target collection is an error-prone complex task. The average number of attempts to pick up a target is 1:85 1:2 in simulation and is 1:96 1:2 in physical experiments. In physical experiments, although robots attempted to visually confirm that a target was successfully picked up, robots sometimes drop targets or detect that a target was collected when it was not. On rare occasions targets were dropped after a collision or robots would steal targets from each other. More commonly, once targets were deposited in the collection zone, robots could accidentally push them out again. We manually counted and then removed collected targets from the collection zone to avoid these accidents. Recognition of targets and the collection zone was impacted by light conditions, particularly the apparent contrast between shadows and lighted areas. While the Gazebo simulation was quite faithful to the rigid body dynamics of the robot and targets, it could not capture subtle effects of lighting and the full range of physical interactions between robots, targets, and the environment.'\",\"963\":null,\"964\":\"'Due to the image-like data structure of DOGMas, the task of predicting future occupancy grid maps, based on the sequence of past DOGMas, is comparable to a video prediction problem. So, the recurrent network part is inspired by the Encoder-Decoder framework introduced by Srivastava et. al in [4] based on [5] to predict future video frames using fully connected LSTMs (FC-LSTMs). We use the sequence-to-sequence mapping ability to train the encoder to observe the input sequence and the decoder to produce multiple future grid maps. LSTMs were first introduced by Hochreiter and Schmidhuber [6], extended in [7], [8] and overcome the problems of learning long-term dependencies, described in [9]. To reduce parameters, we use convolutional LSTMs (ConvLSTMs), which were introduced in [3] and outperform the FC-LSTMs in capturing spatio-temporal relationships.'\\n\\n'The recurrent network part is inspired by the Encoder-Decoder framework introduced by Srivastava et. al in [4] based on the general sequence-to-sequence framework in [5]. Here, it is used to train the Encoder-LSTM and the Decoder-LSTM with sequences of different length and time scales. The architecture of the two-layer Encoder-Decoder is depicted in Fig. 2c) with the unrolled structure during training. The task of the Encoder-LSTM is to observe the input sequence and save information of several time steps in its internal states, which can be seen as sequential filtering of the scene. In addition, the output of the Encoder-LSTM is used as static prediction ystat. Each time step, the Decoder-LSTM will be initialized with the current internal states of the Encoder-LSTM and produces a sequence of future predictions. In this work, the Decoder-LSTM is applied four times to produce the predictions for\\\\nt\\\\n0\\\\n+0.5s,\\\\nt\\\\n0\\\\n+1s,\\\\nt\\\\n0\\\\n+1.5s\\\\nand\\\\nt\\\\n0\\\\n+2s\\\\n, where t0 indicates the current time.'\\n\\n'Overview of the developed network architecture. The main parts are the down- and upscaling structure consisting of feedforward neural networks (red) and the recurrent Encoder-Decoder model, outlined in c). In the Grid Predictor Model, the feedforward skip connections are depicted in a), the extended version with recurrent skip connections is outlined in b).'\\n\\n'Both, the Encoder-LSTM and the Decoder-LSTM consist of a two-layer ConvLSTM [3] with kernels of the size 5 \\u00d7 5 and internal states h, c both in\\\\nR\\\\n18\\u00d718\\u00d7128\\\\n. Here, the ConvLSTMs exploit spatio-temporal information, but the spatial size of all internal states h, c and the outputs ystat, ydyn remain the same. The developed Grid Predictor Model (GPM) contains 14.65 million parameters and achieves an inference time of 63 ms on a Nvidia GeForce GTX 1080 Ti, which is sufficient for real-time application.'\\n\\n'In this work we presented a recurrent deep learning approach to produce long-term predictions of the vehicle environment in a grid map representation using DOGMas as input data. We developed a network architecture with ConvLSTMs in an Encoder-Decoder framework, yielding enhanced results with the use of less parameters compared to the previous work in [2]. In addition, a novel recurrent skip architecture is developed which shows promising performance in dealing with missing input data as a result of occlusions in the DOGMa. In this work, the data is only recorded by a stationary sensor, so in a following work the presented approach should be applied with recordings of a driving scenario.'\",\"965\":\"'For the experiments and development presented in this paper, we modified the code base released with [10] to formulate the problem as defined earlier on in the paper. In addition, we modified the code to convert the problem to a format where we could use the SDP solver within the optimization library Mosek [23].'\",\"966\":null,\"967\":null,\"968\":\"'Each asset is executed as a thing consisting of an asset driver for connecting the hardware to the superordinate system and a Digital Shadow in which the state is persisted. The Digital Shadow of the cell describes the steps for the task of the cell. In this case, for example, the product has to be detected by the proximity sensor. As soon as it is detected, the suction cup has to fix the product and then the camera has to read the QR-code of the product to request product related information so that the robot can work on behalf of the product instance related information. After the robot has finished its process, the suction cup has to stop fixing the product. The cell state has to change to be finished so that the handling robot can get the information to get the window again. Only if the window is removed from the cell, which is detected by the proximity sensor, the state of the cell is set to \\u201cfree\\u201d again.'\\n\\n'The aim is to implement a control loop for adapting and optimizing the process. The control loop is implemented by using a collaborative robot (KUKA LBR iiwa) on the Cleaner cell to measure the real shape of the trajectory on the car window and writing the measured trajectory into the Digital Shadow of the product. The robot on the Primer cell (KUKA Agilus fiive) that is a standard industrial robot can use this measured trajectory to draw a line on the car window that represent the application of the primer. The Cleaner and the Primer cell use the QR-Code of each car window to get the Digital Shadow of the product, which contains the information that is needed to fulfil the robot path and therefore describe a possibility to adapt the process'\",\"969\":null,\"970\":null,\"971\":null,\"972\":null,\"973\":\"'The prior knowledge we use consists of the information which of the tasks is currently active. The active task information is included as three additional input features to the multilayer perceptron regressor. The three signals are one-hot encoded inputs that indicate which of the three tasks the sensor measurement originated from. Using the extended sensor input we retrain the MLP on data from both the original Task 1, as well as the new additions Task 2 and 3. During this phase we keep the sensor sets fixed, that means we do not change the hardware that was found for Task 1.'\",\"974\":\"'The reconfiguration mechanism, with the rest of the leg extending to the right of the image. (1) Brushed DC motor and sprocket, (2) Encoder for positioning, (3) Aluminium rail and carriages, (4) Threaded rod and sprocket, (5) Limit switch for zeroing, (6) Nut fixed to the movable part of the leg'\\n\\n'The length of the reconfigurable legs are controlled using an Arduino Mega 2560 Rev 3 board with a custom PCB shield, which communicates with the software system through USB at 10hz. Limit switches are routed directly to the digital inputs of the microcontroller with internal pullups, and all encoders are connected directly to the analog inputs. The custom shield has twelve H-bridges to drive the DC motors in the linear actuators, which are controlled by PWM from the microcontroller. Since we are using a screw mechanism for the linear actuators with inherently high holding load and friction, a proportional controller for each prismatic joint is sufficient to achieve stable actuation with 0. 5mm accuracy.'\\n\\n'Contribution: There are two significant contributions in this paper. First, we demonstrate and introduce a practical robot system for researching self-modifying morphology. This system has been released as a fully certified open source hardware project, and can be used freely by other re-searchers. Secondly, we show through experimental results-both from the lab, and from field scenarios-that having a self-reconfigurable morphology helps our robot to maintain optimal performance when adapting to changing supply voltages and external environments. These experiments indicate that self-reconfigurable legs could improve the performance of robots doing complex tasks in dynamic environments.'\\n\\n'We also hope that these experiments inspire more research on real world mechanical reconfiguration, and that our newly developed and open sourced platform might help lower the initial investment needed to begin such research by allowing others to use or extend our robot design [19], either in simulation or the real world.'\",\"975\":null,\"976\":\"'Both the balance controller and motor servo require velocity as input. Velocity estimation of both absolute and incremental encoders is done using a simple numerical differentiation combined with a second-order Butterworth filter having a cut-off frequency of 3500 rad\\/s for the incremental encoder and 500 rad\\/s for the absolute encoders. The filter was converted from continuous to discrete time using the bilinear (i.e., Tustin) transform.'\\n\\n'The velocity servo is a simple proportional controller using only the velocity estimate from the Maxon ENX 16 encoder. The proportional controller has a gain of 0.003 and the output is the normalized duty cycle of the PWM output from the microcontroller to the motor driver, which is a number between -1 and +1.'\",\"977\":\"'https:\\/\\/wiki.eecs.berkeley.edu\\/biomimetics\\/Main\\/OpenRoACH'\\n\\n'In addition, open-source legged robots could enable wider research in legged robotics and locomotion and contribute to the overall popularity of legged robotics. We use the definition from the Open Source Robotics Foundation for open-source robotics: development, distribution, and adoption of open-source software and hardware for robotics research, education, and product development [2]. For open-source robotics hardware we use the definition from the Open Source Hardware Association: be publicly available in a preferred format, use readily-available components and materials, standard processes, open infrastructure, unrestricted content, and open-source design tools [3], [4]. There are only a handful of fully open-sourced legged robots, such as Marty [5], ROFI [6], Hexy [7], Metabot [8], Aracna [9] and Poppy [10]. They are all above 30 cm, mostly fully actuated with tens of servomotors, and controlled with an onboard microcontroller (with the bipedal Marty being the only one supporting onboard ROS).'\\n\\n'OpenRoACH\\u2019s electronic systems can be flexibly configured between using a microcontroller and\\/or a single board computer (SBC), all of which can be acquired off the shelf. Three configurations of the electronic system have been tested on OpenRoACH. These include i) a BeagleBone Blue board, ii) a Raspberry Pi 3 combined with an mbed microcontroller, and iii) a standalone mbed microcontroller. BeagleBone Blue offers the lightest-weight (40 grams) and the most convenient solution, while the latter two require additional components such as a motor driver. We have developed an open-source carrier board that mounts a motor driver carrier board (DRV8833, Pololu, USA) and an mbed microcontroller (NXP LPC1768, ARM, UK). The configuration of an mbed with the carrier board weighs 58 grams; that of a Raspberry Pi 3, an mbed, and the carrier board weighs 105 grams. In all configurations, OpenRoACH is powered by a 7.4V LiPo battery.'\\n\\n'Technically, OpenRoACH offers the following features: (1) fully open-sourced mechanical design, electronics, and software; (2) under-actuated with two DC motors actuating six legs; (3) folding based design; (4) laser cutting and 3D printing enabled fabrication; (5) ROS enabled by a single-board computer; (6) low cost of $150; and (7) short fabrication and assembly time. It has demonstrated reliable walking on flat ground surfaces with a high gear ratio and multi-surface running with a low gear ratio. It survived a 24-hour continuous walking burn-in. It can carry 200 gram payloads dynamically and 800 gram payloads statically. It has been tested with integrated feedback from a variety of sensors including gyroscopes, accelerometers, Beacon sensors, color vision sensors, linescan sensors and cameras. All of these suggest OpenRoACH is a suitable research tool for legged robot design, fabrication, gait control, and locomotion. For example, it can be used to study sensorimotor learning strategies on various surfaces and terrain, or in the event of damage to one or more of the legs.'\\n\\n'OpenRoACH is a 15-cm 200-gram self-contained hexapedal robot with an onboard single-board computer. To our knowledge, it is the smallest legged robot with the capability of running the Robot Operating System (ROS) onboard. The robot is fully open sourced, uses accessible materials and off-the-shelf electronic components, can be fabricated with benchtop fast-prototyping machines such as a laser cutter and a 3D printer, and can be assembled by one person within two hours. Its sensory capacity has been tested with gyroscopes, accelerometers, Beacon sensors, color vision sensors, linescan sensors and cameras. It is low-cost within $150 including structure materials, motors, electronics, and a battery. The capabilities of OpenRoACH are demonstrated with multi-surface walking and running, 24-hour continuous walking burn-ins, carrying 200-gram dynamic payloads and 800-gram static payloads, and ROS control of steering based on camera feedback. Information and files related to mechanical design, fabrication, assembly, electronics, and control algorithms are all publicly available on https:\\/\\/wiki.eecs.berkeley.edu\\/biomimetics\\/Main\\/OpenRoACH.'\\n\\n'A petal diagram showing the comparative advantage of OpenRoACH among representative legged robots. Low cost: below 500. Small size: 5-20 cm. Open-source: mechanical hardware, electronic hardware, and software.'\",\"978\":null,\"979\":null,\"980\":null,\"981\":null,\"982\":\"'The authors would like to acknowledge CNPq for the processes 139375\\/2018-0 and 141395\\/2017-6, as well as the S\\u00e3o Paulo Research Foundation (FAPESP), grant #2013\\/07276-1. This study was partially funded by the Coordena\\u00e7\\u00e3o de Aperfei\\u00e7oamento de Pessoal de N\\u00edvel Superior - Brasil (CAPES) \\u2013 Finance Code 001.'\",\"983\":null,\"984\":\"'https:\\/\\/www.xsens.com\\/'\\n\\n'We translated the position of all segments such that the pelvis is in the origin of the X-Y coordinates. Xsens offers kinematic data as 3-dimensional vectors. We enriched the data by adding further feature types which encode only the magnitude of these. Furthermore, a normalized version of each feature is added which is obtained by mean-subtraction and scaling to unit-variance. This has been done per person to improve generalization across different subjects. In total, up to 32 different feature types are available for each of the 23 segments, resulting in 1472 dimensions per sample.'\\n\\n'Action classification is known to be more accurate when the feature vector encodes not only the current sensor state, but the recent motion history. We performed preliminary experiments to compare the effect of different feature configurations. We evaluate the performance for encoding only the current sensor state versus stacking the features of the last 30 frames\\\\n(\\u223c0.5s)\\\\n) versus encoding the features of the last 30 frames via the five highest values of the discrete cosine transformation (DCT) [31]. The highest performance is achieved by the DCT encoding even though it uses a substantially lower amount of dimensions in comparison to stacking. Based on these results, we use the DCT encoding for the rest of the experiments 2. The detailed results are listed in the appendix (Table II).'\",\"985\":\"'While these are crucial issues in robot vision, solving them is not enough. Any robot, regardless of how much knowledge has been manually encoded into it, will inevitably see novel objects. This calls for robots able to know what they know and what they do not know, and able to learn how to recognize new objects by themselves. For instance, in Fig. 1, a 2-arm manipulator robot detects a novel object on its workspace. It then obtains the object label and images of the same object through external resources (e.g. a human collaborator or by mining the Web). Finally, the robot incrementally learns the novel object category and begins to detect the novel object correctly.'\",\"986\":\"'We explore initializing learning models using a supervised approach in which we use a simulator to generate a large number of arms with varying morphologies and learn a mapping of observables (i.e., simulated point clouds). We will possibly later also map to capabilities using an auto-encoder for which the inputs are depth images concatenated with a vector of link lengths padded with zeroes [24] or recurrent neural networks. Similar deep learning techniques have been applied with success in the robotic grasp problem space [25], [26], as well as generalized object recognition [27]. Prior research also demonstrates successful use of neural net-based deep learning for merged environment and behavior evaluation [28], leveraging Long Short Term Memory cells for sequential processing [29]. We believe RNNs can model temporally-robust morphology probabilities, eliminating discrete frame-by-frame calculations [30].'\",\"987\":\"'http:\\/\\/usa.honda-ri.com\\/H3D'\\n\\n'Data annotation verification by projecting annotation onto image and bird\\u2019s eye view (BEV), with color coded track ID in BEV'\",\"988\":null,\"989\":\"\\\"Kinematic trajectories recorded from surgical robots contain information about surgical gestures and potentially encode cues about surgeon's skill levels. Automatic segme...\\\"\\n\\n\\\"Kinematic trajectories recorded from surgical robots contain information about surgical gestures and potentially encode cues about surgeon's skill levels. Automatic segmentation of these trajectories into meaningful action units could help to develop new metrics for surgical skill assessment as well as to simplify surgical automation. State-of-the-art methods for action recognition relied on manual labelling of large datasets, which is time consuming and error prone. Unsupervised methods have been developed to overcome these limitations. However, they often rely on tedious parameter tuning and perform less well than supervised approaches, especially on data with high variability such as surgical trajectories. Hence, the potential of weak supervision could be to improve unsupervised learning while avoiding manual annotation of large datasets. In this paper, we used at a minimum one expert demonstration and its ground truth annotations to generate an appropriate initialization for a GMM-based algorithm for gesture recognition. We showed on real surgical demonstrations that the latter significantly outperforms standard task-agnostic initialization methods. We also demonstrated how to improve the recognition accuracy further by redefining the actions and optimising the inputs.\\\"\",\"990\":null,\"991\":\"'http:\\/\\/tiny.cc\\/zeus-y1'\\n\\n\\\"A CNN was developed for the purpose of lane marking segmentation. The network uses an encoder-decoder structure in which the encoder consists of convolution and max pooling layers and the decoder consists of deconvolution and upsampling layers. Skip connections from the encoder layers to the decoder layers allow the network to perform high-resolution upsampling as is done in [19]. This architecture is depicted in Figure 5. Given the recent success of deep learning in semantic segmentation, a deep neural network is a clear choice for this problem. However, the competition's requirement of using CPUs only represented a significant hurdle to this approach. Thus, we opted to design a custom CNN with significantly less layers and filters (see Figure 5) compared to the state-of-the-art. We trained the network in Tensorflow [20] on a custom hand-labeled dataset. Random rotation, flipping, contrast, and brightness was applied to augment the custom dataset. The resulting network is capable of processing frames in real-time (50 Hz) on CPUs.\\\"\\n\\n\\\"By focusing on simple approaches with added redundancy, we were able to build a system sufficiently reliable to win the competition in six months. Only two teams completed all three challenges: aUToronto, and Virginia Tech. Our future work will include incorporating an HD map which will encode the locations of lanes, signs, and traffic lights. We will be looking to augment the current pose estimation system, possibly with a vision-or LIDAR-based localization. The constraint to use CPUs only will be relaxed slightly to allow the use of FPGAs. As such, we will be looking to jump-start aUToronto's development using powerful deep learning tools for all visual perception tasks.\\\"\",\"992\":null,\"993\":\"'http:\\/\\/oscar.skoltech.ru\\/'\\n\\n'Ability of autonomous vehicles to operate in complex dynamic environments requires, among other things, fast and accurate perception of surroundings, which includes recognition and tracking of traffic signs.For development and testing of modern sophisticated computer vision systems large and diverse datasets are of the major importance. To test the robustness of algorithms, image data with different moving speeds, camera settings, lighting and weather conditions are especially important.In this work we present a comprehensive, lifelike dataset of traffic sign images collected on the Russian winter roads in varying conditions, which include different weather, camera exposure, illumination and moving speeds. The dataset was annotated in accordance with the Russian traffic code. Annotation results and images are published under open CC BY 4.0 license and can be downloaded from the project website: http:\\/\\/oscar.skoltech.ru\\/.'\\n\\n'As it can be seen, \\u201crgba\\u201d format encoded with FLIF produces the highest compression ratios for both sequences. This is why we have chosen this approach for distribution of lossless images in addition to lossy JPEGs encoded with 75% quality.'\\n\\n'type: sign type number as per Russian traffic code. (e.g. 3.24 stands for speed limit sign)'\",\"994\":null,\"995\":\"'https:\\/\\/www.youtube.com\\/waCch?v=r-gSUyFoK8Q'\\n\\n'A highly flexible optimization-based context reasoning process which incorporates a multi-layer cost map structure to encode various contextual factors.'\\n\\n'Our policy anticipation network is based on an RNN encoder structure [22]. We refer interested readers to [22] and [1] for the detailed structure. Note that the output layer is modified to a softmax layer to provide the likelihood for all the policy labels. The probability distribution is used in the interpretation of the policy in Sec. IV-C. We adopt negative log-likelihood (NLL) loss for this classification problem.'\\n\\n'In this section, we present the cost map structure, which encodes the whole driving context. We specify different kinds of costs by separating them into different layers with distinct physical meanings, for the sake of illustration. A toy example of the multi-layer cost map is given in Fig. 4. We adopt a four-layer cost map design in which we encode the cost induced by the lane geometry and static obstacles into the static layer, the cost induced by the moving objectives (MO) into the MO layer, the cost induced by traffic regulations into the context layer, and the cost induced by the vehicles\\u2019 nonholonomic constraints into the nonholonomic layer.'\\n\\n'Like red lights, speed limits should not be encoded in hard constraints when taking speed limit offences into account. We introduce the cost\\\\nf\\\\nv\\\\n(x)\\\\n, which is induced by the speed limit and should also allow the vehicle to stop in the case of a traffic jam. As a result, we model\\\\nf\\\\nv\\\\n(x)\\\\nas the quadratic error between the predicted velocity\\\\nx\\\\n\\u02d9\\\\nand a desired velocity vdes. The magnitude of the desired velocity\\\\n\\u2225\\\\nv\\\\ndes\\\\n\\u2225\\\\nis determined by the minimum between two factors, namely, the speed limit\\\\nv\\\\nmax\\\\nand the velocity trend Vtrend. Specifically, Vtrend is obtained by conducting velocity fitting for the historical velocity observations in Tobs, which captures the acceleration and deceleration trend of the predicted vehicle and is close to zero in the case of a traffic jam. The direction of the desired velocity\\\\nv\\\\n^\\\\ndes\\\\nconforms to the lane geometry. Mathematically, we have\\\\n\\u2225\\\\nv\\\\ndes\\\\n\\u2225=min(\\\\nv\\\\nmax\\\\n, \\\\nv\\\\ntrend\\\\n)\\\\nand\\\\nf\\\\nv\\\\n(x)=\\u2225\\\\nx\\\\n\\u02d9\\\\n\\u2212\\\\nv\\\\ndes\\\\n\\u2225\\\\n2'\\n\\n'RNN encoder-decoder trajectory regression. This method uses an RNN to encode the past maneuver history and directly outputs the future trajectory through the RNN decoder. This structure is popular, and is adopted in [1] and [12].'\\n\\n'RNN encoder-decoder trajectory regression'\\n\\n'Since the source code of [1] is not officially available and [12] is mainly tested in a highway dataset, we adopt the RNN encoder-decoder part in [1] according to the available implementation details [27]. We conduct the experiments in the form of case studies to show that our proposed framework can easily adapt to various traffic configurations, as elaborated in Sec. VII-B.'\\n\\n'We adopt an open-source urban autonomous driving simulator named CARLA [24]. In this section, we present our environment setup. For a scene containing n vehicles, the first n\\u20132 vehicles (agent vehicles) are controlled by the autopilot module provided by CARLA, the n\\u20131-th vehicle (player vehicle) is controlled by a human player and the n-th vehicle is an observer vehicle which is supposed to closely follow the player vehicle, sense the environment, and predict the trajectory of the player vehicle. We focus on predicting the trajectories for the player vehicle since it reflects real human intentions. Another reason is that the agent vehicles do not have complex maneuver patterns due to the fixed handwritten logic of the autopilot module. Hence, when presenting the experimental results (Sec. VII) we will focus on illustrating the prediction results for the player vehicle, to give a clean and informative visualization.'\",\"996\":null,\"997\":null,\"998\":\"'The final motor command is proportional to the instanteneous firing rate of the Mr population, i.e. the spike-count in 100ms time window. A more direct spike-based code can be used to control the motors in a more embedded neuromorphic realisation, as has been explored recently [9], [30], [31]. Our architecture adds space-coded representations on top of the rate-coded one: in the the Ms population, each neuron represents a different motor command. This enables online learning of a mapping between the task and motor command space using simple Hebbian learning rule of the on-chip plastic synapses.'\\n\\n'Finally, each motor neuron is connected to a different number of rate-coding motor neurons (Mr), depending on the represented motor command. Thus, the first space-coded motor neuron is connected to one rate-coded neuron, whereas the fifth space-coded neuron is connected to 5 rate-coded neurons that can be selected arbitrarily from the rate-coded motor array. The more neurons in the Mr array fire, the higher the overall firing rate of this neural population. This firing rate sets the instantaneous speed of the motor (Section II-D).'\\n\\n'We have used a thin software layer, running on the Parallella board, to establish interfaces between the robot and the spiking neurons on the ROLLS chip. Thus, the rotational velocity, measured at 200Hz using the IMU, was binned in one of 5 ranges. Depending on the interval in which the measured velocity fell, one of 5 IMU (Feedback) neurons was stimulated with a rate of 800Hz. On the other side, the activity of the Motor rate-coded array was calculated by counting spikes in 100ms time windows. The speed command sent to the robot was set proportionally to this spike-rate. This software solution was used in our prototype system that didn\\u2019t yet have a direct connection to the robot. When such connection is established, direct spike-based control will be possible [9], [30], [31]. The CMD signal (the goal velocity) was set by stimulating inhibitory synapses of one of the Goal neurons.'\\n\\n'Additionally to the feedback controller, we demonstrate here how a direct feedforward control signal can be learned on the neuromorphic device ROLLS, which features on-chip plasticity. Thus, we have added an array of plastic synapses between the learning command neural population and the rate-coded motor population (light blue dashed lines in Fig. 1). We set up learning to be active when the Delta-value is zero (meaning that the feedback controller has converged): the learning command population is inhibited by all other Delta +\\/- neurons. During learning, the plastic synapses potentiate (increase) between the active learning command neuron and active Mr neurons. These synapses store a direct connection from the task-level command to the motor population, leading to faster activation of the correct motor behavior after learning. Both controllers can work in parallel, complimenting each other. Note that learning here is controlled by activity of the Delta=0 neuron, turning on-chip plasticity on and off during behavior. Such online learning in a neuromorphic device has been demonstrated for the first time here.'\\n\\n'Fig. 3(a) shows the output of the neuromorphic chip (spikes over time) in a sequence of control tasks. Here, we set control signals 1-5 to the chip in an increasing and decreasing order, with a step size of 1. Other step sizes show similar results. Upper plot shows activity of 5 Goal neurons, one of which is inhibited and corresponds to the selected control signal (desired speed of the robot). The second plot shows IMU measurements of the robot\\u2019s speed. One can note that the controller overshoots first on the rising steps, but then settles on the correct speed and keeps it. In the decreasing steps, the controller reaches the desired speed directly. The third plot shows activity of the Feedback neurons on the ROLLS chip, which are driven by the IMU signal and faithfully represent it. The forth and fifth plots show activity of the Delta +\\/- and Result +\\/- neurons, respectively. The controller has converged on a desired value when only Delta + = 0 is active. The second to last plot shows activity of the space-coded motor neurons, driven by the Result +\\/- neurons, and the bottom plot shows firing rate of the rate-coded motor neurons, which drive the motor.'\\n\\n'During this experiment, each time when the controller settles on the desired value (i.e. Delta neuron 0 is active) learning is activated, triggering learning in plastic synapses on the ROLLS chip. Plastic synapses between the active learning command and the active rate-coded motor neurons are strengthened then. In the example shown in Fig. 3(b), just before the time mark 35000ms, we inhibit the feedback controller (note absent activity in the motor space-coded neurons, which drive the rate-coded motor neurons during the feedback control). The robot stops and when the learning command is activated (released from inhibition of the Delta neurons), the feedforward controller shows its effect. We can see that the robot\\u2019s speed is immediately set to the desired value. This is achieved by activating the correct number of rate-coded neurons through plastic synapses from the learning command neurons (note that the only active neuron in the architecture after 37500ms is the learning command neuron, which drives the Mr neural population).'\",\"999\":\"'https:\\/\\/gichub.com\\/openai\\/gym\\/wiki\\/Leaderboard'\\n\\n'Information-processing capacity in an ANN is encapsulated both in its nodes and edges. Therefore, similar to the original NEAT, AGENT uses a direct bi-structural encoding, where each genome comprises of a node encoding and an edge encoding. Where AGENT differs from NEAT is in how nodes are encoded \\u2013 in AGENT, the node encoding also defines the type of activation function and memory capacity of the node. To allow greater flexibility, one of three activation functions can be selected: modified sigmoid, (only option in original NEAT), saturated linear, and sigmoid functions. The memory is allowed to take one of three values: M\\\\n\\u2208{0,1,2}\\\\n; a memory size of 0 designates using the current weighted input incoming into the node; memory sizes of 1 and 2 respectively allow using the first and second temporal derivatives of the weighted input incoming into the node. The latter enables exibiting temporal dynamic behavior, useful for neurocontroller type applications. Equation 1 explains how the derivatives of each node are used. In this equation, for a node connected to ni upstream nodes,\\\\nV\\\\ni\\\\n(\\u03c4)\\\\nis the net synaptic input of node-i in time step\\\\n\\u03c4,\\\\nf\\\\nj\\\\nis the output of the (upstream) node-j,\\\\n\\u03b4\\u03c4\\\\nis the time step used when implementing this NN as a controller, and\\\\nU\\\\ni\\\\n(\\/r)=\\\\n\\u2211\\\\nj=1\\\\nn\\\\ni\\\\n(\\\\nw\\\\nj,i\\\\n\\u00d7\\\\nf\\\\nj\\\\n)\\\\n.'\\n\\n'Population diversity is paramount to effective neuroevolution. This calls for prudently controlling the population diversity \\u2013 an abrupt decrease in diversity can lead to premature stagnation, but at the same time, a steady (low) rate of diversity reduction is needed for exploitation and eventual convergence. The first step in diversity preservation is robust measurement of diversity. To develop a diversity measure, an approach is needed to quantify the differences between any two neural networks in the design space. In neuroevolution, since genomes encode different topologies, their basic dimensionality varies across the population. Hence, a distance metric similar to the novelty metric described in [25] is used here. The distance between two candidate ANNs, A and B, is thus given by the weighted sum of the difference between their node types, as well as the difference between edges connecting different types of nodes.'\\n\\n'OpenAI Gym is an open-source platform [28] that has been growing in popularity for benchmarking and comparing RL algorithms [11], as well as other learning and optimization methods [22] that can solve RL-type control problems. In this paper, we showcase the performance of AGENT on three problems curated from the OpenAI gym and compare with published results on state-of-the-art RL methods (summarized in Table I). These problems are very briefly described below; further information on these implementations, e.g., details of the state and action vectors, can be found at https:\\/\\/giChub.com\\/openai\\/gym\\/wiki\\/Leaderboard.'\",\"1000\":\"'For interpreting the neuron activities as motor commands, the output spikes should be decoded to steer the slithering locomotion of a snake-like robot.'\",\"1001\":null,\"1002\":\"'https:\\/\\/youtu.be\\/5sk1vKFLAXE'\",\"1003\":null,\"1004\":null,\"1005\":\"'In this paper, we identify visual signal detection as an important problem in self-driving. We introduce a largescale dataset of vehicle signals, and propose a modern deep learning approach to directly estimate turn signal states from diverse, real-world video sequences. A principled network is designed to model the subproblems of turn signal detection: attention, scene understanding, and temporal signal detection. This results in a differentiable system that can be trained endto-end using deep learning techniques, rather than relying upon hard coded premises of how turn signals should behave.'\\n\\n'Failure modes of the network. (a) Bright lights at night are misclassified as a left turn. (b) Bright reflection on the right side of a distant vehicle is misclassified as a right turn. (c) Right turn signal is missed on an unusual vehicle. (d) Actor pose is incorrectly decoded and output is flipped. (e) False positive left turn on a vehicle carrying a bike.'\",\"1006\":null,\"1007\":\"'Predicting the future location of vehicles is essential for safety-critical applications such as advanced driver assistance systems (ADAS) and autonomous driving. This paper introduces a novel approach to simultaneously predict both the location and scale of target vehicles in the first-person (egocentric) view of an ego-vehicle. We present a multi-stream recurrent neural network (RNN) encoder-decoder model that separately captures both object location and scale and pixel-level observations for future vehicle localization. We show that incorporating dense optical flow improves prediction results significantly since it captures information about motion as well as appearance change. We also find that explicitly modeling future motion of the ego-vehicle improves the prediction accuracy, which could be especially beneficial in intelligent and automated vehicles that have motion planning capability. To evaluate the performance of our approach, we present a new dataset of first-person videos collected from a variety of scenarios at road intersections, which are particularly challenging moments for prediction because vehicle trajectories are diverse and dynamic. Code and dataset have been made available at: https:\\/\\/usa.honda-ri.com\\/hevi.'\\n\\n'However, these methods model trajectories and context information from a bird\\u2019s eye view in a static camera setting, which significantly simplifies the challenge of measuring distance from visual features. In contrast, in monocular first-person views, physical distance can be estimated only indirectly, through scaling and observations of participant vehicles, and the environment changes dynamically due to ego-motion effects. Consequently, previous work cannot be directly applied to first-person videos. On the other hand, the first-person view provides higher quality object appearance information compared to birds eye view images, in which objects are represented only by the coordinates of their geometric centers. This paper encodes past location, scale, and corresponding optical flow fields of target vehicles to predict their future locations, and we further improve prediction performance by incorporating future ego-motion.'\\n\\n'We propose a multi-stream RNN encoder-decoder (RNN-ED) model to encode temporal information of past observations and decode future bounding boxes, as shown in Figure 2. The past bounding box trajectory is encoded to provide location and scale information, while dense optical flow is encoded to provide pixel-level information about vehicle scale, motion, and appearance changes. Our decoder can also consider information about future ego-motion, which could be available from the planner of an intelligent vehicle. The decoder generates hypothesized future bounding boxes by temporally updating from the encoded hidden state.'\\n\\n'In this paper, object vehicle features are extracted by a region-of-interest pooling (ROIPooling) operation using bilinear interpolation from the optical flow map. The ROI region is expanded from the bounding box to contain contextual information around the object, so that its relative motion with respect to the environment is also encoded. The resulting relative motion vector is represented as\\\\nO\\\\nt\\\\n=[\\\\nu\\\\n1\\\\n, \\\\nv\\\\n1\\\\n, \\\\nu\\\\n2\\\\n, \\\\nv\\\\n2\\\\n, \\u22efun, \\\\nv\\\\nn\\\\n]\\\\nt\\\\n, where n is the size of the pooled region.'\\n\\n'We use two encoders for temporal modeling of each input stream and apply the late fusion method:'\\n\\n'We use another GRU for decoding future bounding boxes. The decoder hidden state is initialized from the final fused hidden state of the past bounding box encoder and the optical flow encoder:'\\n\\n'RNN-ED-X is an RNN encoder-decoder with only past bounding boxes as inputs.'\\n\\n'RNN-ED-XO is a two-stream RNN encoder-decoder model with past bounding boxes and optical flow as inputs.'\\n\\n'We proposed the new problem of predicting the relative location and scale of target vehicles in first-person video. We presented a new dataset collected from intersection scenarios to include as many vehicles and motion as possible. Our proposed multi-stream RNN encoder-decoder structure with awareness of future ego motion shows promising results compared to other baselines on our dataset as well as on KITTI, and we tested how each component contributed to the model through an ablation study.'\",\"1008\":null,\"1009\":null,\"1010\":null,\"1011\":null,\"1012\":null,\"1013\":\"'https:\\/\\/github.com\\/tyler-hayes\\/ExStream'\\n\\n'https:\\/\\/github.com\\/tyler-hayes\\/ExStream'\",\"1014\":\"'While some semantic information can be hard-coded, large-scale and long-term deployments of autonomous systems require the development of computational frameworks that i) enable abstract concepts to be learned and generalized from observations, ii) effectively model the uncertain nature of complex real-world environment, and iii) are scalable, incorporating data from a wide range of environments (e.g., hundreds of households). Previous work in semantic reasoning for robot systems has addressed subsets of the above challenges. Directed graphs [7] used in [3] allowed individual observations to adapt generalized concepts at large scale, integrating multiple projects. Bayesian Logic Networks (BLN) [8] in [2] allowed for precise probabilistic inference and learning assuming knowledge graphs have manageable sizes. Description Logics (DL) [9] used in [10] allowed for large-scale deterministic reasoning about many concepts. In summary, each of these representations have limitations with respect to at least one of the three characteristics above.'\\n\\n'RoboCSE is a computational framework for semantic reasoning that uses multi-relational embeddings to encode abstract knowledge obtained by the robot from its sensors, simulation, or even external knowledge graphs (Figure 3). The robot can use the resulting knowledge representation as a queriable database to obtain information about its environment, such as likely object locations, material properties of objects, object affordances, and any other relation-based semantic information the robot is able to mine.'\",\"1015\":null,\"1016\":null,\"1017\":\"'http:\\/\\/ttic.uchicago.xn--educbschaff-9u8f\\/nlimb'\\n\\n'We consider the task of locomotion for the Hopper, Walker, and Ant morphologies on both a level and inclined ground plane (with a five degree slope). The environments are built on top of Bullet Physics, a popular open-source physics engine. We use the Roboschool default reward function, which is a weighted sum of rewards for forward progress and staying upright, and penalties for torques and for reaching joint limits. Every episode ends when a robot falls over or after a maximum number of timesteps.'\",\"1018\":null,\"1019\":\"'https:\\/\\/youtu.be\\/2iDUs7ppz9Y.'\\n\\n'FCAE concurrently learns a encoder\\\\nE\\\\nand a decoder\\\\nE\\\\nThe encoder maps an image into the code vector\\\\nz,\\\\nwhich is a latent image expression in the low dimensional space. The decoder reconstructs an image\\\\nz\\\\n. The leaming objective is to minimize the squared\\\\nL2\\\\ndistance between the input images and reconstructed images, namely,'\\n\\n'According to the leaming objectives, the discriminator of DCGAN learns the discriminative features which the real images own. The encoder of FCAE learns the representative features to model the image\\u2019s latent structure.'\",\"1020\":null,\"1021\":\"'https:\\/\\/pubmed.ncbi.nlm.nih.gov\\/31839700'\"},\"Stars\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":\"32\",\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":\"5\",\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":\"4\",\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":\"2\",\"65\":-1,\"66\":-1,\"67\":\"4\",\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":\"74\",\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":\"4\",\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":\"1\",\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":13,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":\"129\",\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":\"4\",\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":\"95\",\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":218,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":200,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":296,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":\"24\",\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":23,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":\"120\",\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":400,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":\"32\",\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":72,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":190,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":\"2\",\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":150,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":\"86\",\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":\"21\",\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":\"16\",\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":\"5\",\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":0,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":\"1\",\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":\"7\",\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":\"11\",\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":\"4\",\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":143,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":\"46k\",\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":\"5\",\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1},\"Forks\":{\"0\":-1,\"1\":-1,\"2\":-1,\"3\":-1,\"4\":-1,\"5\":-1,\"6\":-1,\"7\":-1,\"8\":-1,\"9\":-1,\"10\":-1,\"11\":-1,\"12\":-1,\"13\":-1,\"14\":-1,\"15\":-1,\"16\":-1,\"17\":-1,\"18\":-1,\"19\":-1,\"20\":-1,\"21\":-1,\"22\":-1,\"23\":-1,\"24\":-1,\"25\":-1,\"26\":-1,\"27\":-1,\"28\":-1,\"29\":-1,\"30\":-1,\"31\":-1,\"32\":-1,\"33\":-1,\"34\":-1,\"35\":\"84\",\"36\":-1,\"37\":-1,\"38\":-1,\"39\":-1,\"40\":-1,\"41\":-1,\"42\":-1,\"43\":-1,\"44\":-1,\"45\":-1,\"46\":-1,\"47\":-1,\"48\":-1,\"49\":\"31\",\"50\":-1,\"51\":-1,\"52\":-1,\"53\":-1,\"54\":-1,\"55\":-1,\"56\":-1,\"57\":-1,\"58\":\"11\",\"59\":-1,\"60\":-1,\"61\":-1,\"62\":-1,\"63\":-1,\"64\":\"8\",\"65\":-1,\"66\":-1,\"67\":\"38\",\"68\":-1,\"69\":-1,\"70\":-1,\"71\":-1,\"72\":-1,\"73\":-1,\"74\":-1,\"75\":-1,\"76\":-1,\"77\":-1,\"78\":-1,\"79\":-1,\"80\":-1,\"81\":-1,\"82\":-1,\"83\":-1,\"84\":-1,\"85\":-1,\"86\":-1,\"87\":-1,\"88\":-1,\"89\":\"135\",\"90\":-1,\"91\":-1,\"92\":-1,\"93\":-1,\"94\":-1,\"95\":-1,\"96\":-1,\"97\":-1,\"98\":-1,\"99\":-1,\"100\":-1,\"101\":-1,\"102\":-1,\"103\":-1,\"104\":-1,\"105\":-1,\"106\":-1,\"107\":-1,\"108\":-1,\"109\":-1,\"110\":-1,\"111\":-1,\"112\":-1,\"113\":-1,\"114\":-1,\"115\":-1,\"116\":-1,\"117\":\"3\",\"118\":-1,\"119\":-1,\"120\":-1,\"121\":-1,\"122\":-1,\"123\":-1,\"124\":-1,\"125\":-1,\"126\":-1,\"127\":-1,\"128\":-1,\"129\":-1,\"130\":-1,\"131\":-1,\"132\":-1,\"133\":-1,\"134\":-1,\"135\":-1,\"136\":-1,\"137\":-1,\"138\":-1,\"139\":-1,\"140\":-1,\"141\":-1,\"142\":-1,\"143\":-1,\"144\":-1,\"145\":-1,\"146\":-1,\"147\":-1,\"148\":-1,\"149\":-1,\"150\":-1,\"151\":-1,\"152\":\"1\",\"153\":-1,\"154\":-1,\"155\":-1,\"156\":-1,\"157\":-1,\"158\":-1,\"159\":-1,\"160\":-1,\"161\":-1,\"162\":-1,\"163\":-1,\"164\":-1,\"165\":-1,\"166\":-1,\"167\":-1,\"168\":-1,\"169\":-1,\"170\":-1,\"171\":-1,\"172\":-1,\"173\":-1,\"174\":-1,\"175\":-1,\"176\":-1,\"177\":-1,\"178\":-1,\"179\":-1,\"180\":-1,\"181\":-1,\"182\":-1,\"183\":-1,\"184\":-1,\"185\":-1,\"186\":-1,\"187\":-1,\"188\":-1,\"189\":-1,\"190\":-1,\"191\":-1,\"192\":-1,\"193\":-1,\"194\":-1,\"195\":-1,\"196\":-1,\"197\":-1,\"198\":-1,\"199\":-1,\"200\":6,\"201\":-1,\"202\":-1,\"203\":-1,\"204\":-1,\"205\":-1,\"206\":-1,\"207\":-1,\"208\":-1,\"209\":-1,\"210\":-1,\"211\":-1,\"212\":-1,\"213\":-1,\"214\":-1,\"215\":-1,\"216\":-1,\"217\":-1,\"218\":-1,\"219\":-1,\"220\":-1,\"221\":-1,\"222\":-1,\"223\":-1,\"224\":-1,\"225\":-1,\"226\":-1,\"227\":-1,\"228\":-1,\"229\":-1,\"230\":-1,\"231\":-1,\"232\":-1,\"233\":-1,\"234\":-1,\"235\":-1,\"236\":-1,\"237\":-1,\"238\":-1,\"239\":-1,\"240\":-1,\"241\":-1,\"242\":-1,\"243\":-1,\"244\":-1,\"245\":-1,\"246\":-1,\"247\":-1,\"248\":-1,\"249\":-1,\"250\":-1,\"251\":-1,\"252\":-1,\"253\":-1,\"254\":-1,\"255\":-1,\"256\":-1,\"257\":-1,\"258\":-1,\"259\":-1,\"260\":-1,\"261\":-1,\"262\":-1,\"263\":-1,\"264\":-1,\"265\":-1,\"266\":-1,\"267\":-1,\"268\":-1,\"269\":-1,\"270\":-1,\"271\":-1,\"272\":-1,\"273\":-1,\"274\":-1,\"275\":-1,\"276\":-1,\"277\":-1,\"278\":-1,\"279\":-1,\"280\":-1,\"281\":-1,\"282\":-1,\"283\":-1,\"284\":-1,\"285\":-1,\"286\":-1,\"287\":-1,\"288\":-1,\"289\":-1,\"290\":-1,\"291\":-1,\"292\":-1,\"293\":-1,\"294\":-1,\"295\":-1,\"296\":-1,\"297\":-1,\"298\":-1,\"299\":-1,\"300\":-1,\"301\":-1,\"302\":-1,\"303\":-1,\"304\":-1,\"305\":-1,\"306\":-1,\"307\":-1,\"308\":-1,\"309\":\"382\",\"310\":-1,\"311\":-1,\"312\":-1,\"313\":-1,\"314\":-1,\"315\":-1,\"316\":-1,\"317\":-1,\"318\":-1,\"319\":-1,\"320\":-1,\"321\":-1,\"322\":-1,\"323\":-1,\"324\":-1,\"325\":-1,\"326\":-1,\"327\":-1,\"328\":-1,\"329\":-1,\"330\":-1,\"331\":-1,\"332\":-1,\"333\":-1,\"334\":-1,\"335\":-1,\"336\":-1,\"337\":-1,\"338\":-1,\"339\":-1,\"340\":-1,\"341\":-1,\"342\":-1,\"343\":-1,\"344\":-1,\"345\":-1,\"346\":-1,\"347\":-1,\"348\":-1,\"349\":-1,\"350\":-1,\"351\":-1,\"352\":-1,\"353\":-1,\"354\":-1,\"355\":\"7\",\"356\":-1,\"357\":-1,\"358\":-1,\"359\":-1,\"360\":-1,\"361\":-1,\"362\":\"347\",\"363\":-1,\"364\":-1,\"365\":-1,\"366\":-1,\"367\":-1,\"368\":-1,\"369\":65,\"370\":-1,\"371\":-1,\"372\":-1,\"373\":-1,\"374\":-1,\"375\":-1,\"376\":-1,\"377\":-1,\"378\":-1,\"379\":-1,\"380\":-1,\"381\":-1,\"382\":-1,\"383\":-1,\"384\":-1,\"385\":-1,\"386\":-1,\"387\":-1,\"388\":-1,\"389\":-1,\"390\":-1,\"391\":-1,\"392\":-1,\"393\":-1,\"394\":-1,\"395\":-1,\"396\":-1,\"397\":-1,\"398\":-1,\"399\":-1,\"400\":-1,\"401\":-1,\"402\":-1,\"403\":-1,\"404\":-1,\"405\":-1,\"406\":-1,\"407\":-1,\"408\":-1,\"409\":-1,\"410\":-1,\"411\":-1,\"412\":-1,\"413\":-1,\"414\":-1,\"415\":-1,\"416\":-1,\"417\":-1,\"418\":-1,\"419\":-1,\"420\":-1,\"421\":-1,\"422\":-1,\"423\":-1,\"424\":-1,\"425\":-1,\"426\":-1,\"427\":-1,\"428\":-1,\"429\":-1,\"430\":-1,\"431\":-1,\"432\":-1,\"433\":-1,\"434\":-1,\"435\":-1,\"436\":-1,\"437\":-1,\"438\":-1,\"439\":-1,\"440\":-1,\"441\":-1,\"442\":-1,\"443\":-1,\"444\":-1,\"445\":61,\"446\":-1,\"447\":-1,\"448\":-1,\"449\":-1,\"450\":-1,\"451\":-1,\"452\":-1,\"453\":-1,\"454\":-1,\"455\":-1,\"456\":-1,\"457\":-1,\"458\":-1,\"459\":-1,\"460\":-1,\"461\":-1,\"462\":-1,\"463\":-1,\"464\":-1,\"465\":-1,\"466\":-1,\"467\":-1,\"468\":-1,\"469\":-1,\"470\":-1,\"471\":-1,\"472\":-1,\"473\":-1,\"474\":-1,\"475\":-1,\"476\":-1,\"477\":-1,\"478\":-1,\"479\":-1,\"480\":-1,\"481\":-1,\"482\":-1,\"483\":-1,\"484\":-1,\"485\":-1,\"486\":-1,\"487\":-1,\"488\":-1,\"489\":-1,\"490\":-1,\"491\":-1,\"492\":-1,\"493\":-1,\"494\":-1,\"495\":-1,\"496\":-1,\"497\":-1,\"498\":-1,\"499\":-1,\"500\":-1,\"501\":-1,\"502\":-1,\"503\":-1,\"504\":-1,\"505\":-1,\"506\":-1,\"507\":-1,\"508\":-1,\"509\":-1,\"510\":-1,\"511\":-1,\"512\":-1,\"513\":-1,\"514\":-1,\"515\":-1,\"516\":-1,\"517\":-1,\"518\":-1,\"519\":-1,\"520\":-1,\"521\":-1,\"522\":-1,\"523\":-1,\"524\":-1,\"525\":-1,\"526\":-1,\"527\":87,\"528\":-1,\"529\":-1,\"530\":-1,\"531\":-1,\"532\":-1,\"533\":-1,\"534\":-1,\"535\":-1,\"536\":-1,\"537\":-1,\"538\":-1,\"539\":-1,\"540\":-1,\"541\":\"41\",\"542\":-1,\"543\":-1,\"544\":-1,\"545\":-1,\"546\":-1,\"547\":-1,\"548\":-1,\"549\":-1,\"550\":-1,\"551\":-1,\"552\":-1,\"553\":7,\"554\":-1,\"555\":-1,\"556\":-1,\"557\":-1,\"558\":-1,\"559\":-1,\"560\":-1,\"561\":-1,\"562\":-1,\"563\":-1,\"564\":-1,\"565\":-1,\"566\":-1,\"567\":-1,\"568\":-1,\"569\":-1,\"570\":-1,\"571\":-1,\"572\":-1,\"573\":-1,\"574\":-1,\"575\":-1,\"576\":-1,\"577\":-1,\"578\":-1,\"579\":-1,\"580\":-1,\"581\":-1,\"582\":-1,\"583\":-1,\"584\":-1,\"585\":-1,\"586\":-1,\"587\":-1,\"588\":\"290\",\"589\":-1,\"590\":-1,\"591\":-1,\"592\":-1,\"593\":-1,\"594\":-1,\"595\":-1,\"596\":-1,\"597\":-1,\"598\":-1,\"599\":-1,\"600\":-1,\"601\":-1,\"602\":119,\"603\":-1,\"604\":-1,\"605\":-1,\"606\":-1,\"607\":-1,\"608\":-1,\"609\":\"149\",\"610\":-1,\"611\":-1,\"612\":-1,\"613\":-1,\"614\":-1,\"615\":-1,\"616\":16,\"617\":-1,\"618\":-1,\"619\":-1,\"620\":-1,\"621\":-1,\"622\":-1,\"623\":-1,\"624\":-1,\"625\":-1,\"626\":-1,\"627\":-1,\"628\":-1,\"629\":-1,\"630\":-1,\"631\":-1,\"632\":-1,\"633\":-1,\"634\":-1,\"635\":-1,\"636\":-1,\"637\":-1,\"638\":-1,\"639\":-1,\"640\":-1,\"641\":-1,\"642\":-1,\"643\":-1,\"644\":-1,\"645\":-1,\"646\":-1,\"647\":-1,\"648\":-1,\"649\":-1,\"650\":-1,\"651\":-1,\"652\":-1,\"653\":-1,\"654\":-1,\"655\":30,\"656\":-1,\"657\":-1,\"658\":-1,\"659\":-1,\"660\":-1,\"661\":-1,\"662\":-1,\"663\":-1,\"664\":-1,\"665\":-1,\"666\":-1,\"667\":\"10\",\"668\":-1,\"669\":-1,\"670\":-1,\"671\":-1,\"672\":-1,\"673\":-1,\"674\":-1,\"675\":-1,\"676\":-1,\"677\":-1,\"678\":-1,\"679\":-1,\"680\":-1,\"681\":-1,\"682\":-1,\"683\":-1,\"684\":-1,\"685\":-1,\"686\":-1,\"687\":-1,\"688\":-1,\"689\":-1,\"690\":-1,\"691\":-1,\"692\":30,\"693\":-1,\"694\":-1,\"695\":-1,\"696\":-1,\"697\":-1,\"698\":-1,\"699\":-1,\"700\":-1,\"701\":-1,\"702\":-1,\"703\":-1,\"704\":-1,\"705\":-1,\"706\":-1,\"707\":-1,\"708\":-1,\"709\":-1,\"710\":-1,\"711\":-1,\"712\":-1,\"713\":-1,\"714\":-1,\"715\":-1,\"716\":-1,\"717\":-1,\"718\":-1,\"719\":-1,\"720\":-1,\"721\":-1,\"722\":-1,\"723\":-1,\"724\":-1,\"725\":-1,\"726\":\"307\",\"727\":-1,\"728\":-1,\"729\":-1,\"730\":-1,\"731\":-1,\"732\":-1,\"733\":\"34\",\"734\":-1,\"735\":-1,\"736\":-1,\"737\":-1,\"738\":-1,\"739\":\"35\",\"740\":-1,\"741\":-1,\"742\":-1,\"743\":-1,\"744\":-1,\"745\":-1,\"746\":-1,\"747\":-1,\"748\":-1,\"749\":-1,\"750\":-1,\"751\":-1,\"752\":-1,\"753\":-1,\"754\":-1,\"755\":-1,\"756\":-1,\"757\":-1,\"758\":-1,\"759\":-1,\"760\":-1,\"761\":-1,\"762\":-1,\"763\":-1,\"764\":-1,\"765\":-1,\"766\":-1,\"767\":-1,\"768\":-1,\"769\":-1,\"770\":-1,\"771\":-1,\"772\":-1,\"773\":-1,\"774\":-1,\"775\":-1,\"776\":-1,\"777\":-1,\"778\":-1,\"779\":-1,\"780\":-1,\"781\":-1,\"782\":-1,\"783\":-1,\"784\":-1,\"785\":-1,\"786\":-1,\"787\":-1,\"788\":-1,\"789\":\"6\",\"790\":-1,\"791\":-1,\"792\":-1,\"793\":-1,\"794\":-1,\"795\":-1,\"796\":-1,\"797\":-1,\"798\":-1,\"799\":-1,\"800\":-1,\"801\":-1,\"802\":2,\"803\":-1,\"804\":-1,\"805\":-1,\"806\":-1,\"807\":-1,\"808\":-1,\"809\":-1,\"810\":-1,\"811\":-1,\"812\":-1,\"813\":-1,\"814\":-1,\"815\":-1,\"816\":-1,\"817\":\"3\",\"818\":-1,\"819\":-1,\"820\":-1,\"821\":-1,\"822\":-1,\"823\":-1,\"824\":-1,\"825\":-1,\"826\":-1,\"827\":-1,\"828\":-1,\"829\":-1,\"830\":-1,\"831\":-1,\"832\":-1,\"833\":-1,\"834\":-1,\"835\":-1,\"836\":-1,\"837\":-1,\"838\":-1,\"839\":-1,\"840\":-1,\"841\":-1,\"842\":-1,\"843\":-1,\"844\":-1,\"845\":-1,\"846\":-1,\"847\":-1,\"848\":\"23\",\"849\":-1,\"850\":-1,\"851\":-1,\"852\":-1,\"853\":-1,\"854\":-1,\"855\":-1,\"856\":-1,\"857\":-1,\"858\":-1,\"859\":-1,\"860\":-1,\"861\":-1,\"862\":-1,\"863\":-1,\"864\":-1,\"865\":-1,\"866\":-1,\"867\":-1,\"868\":-1,\"869\":-1,\"870\":-1,\"871\":-1,\"872\":-1,\"873\":-1,\"874\":-1,\"875\":-1,\"876\":-1,\"877\":-1,\"878\":-1,\"879\":-1,\"880\":-1,\"881\":-1,\"882\":-1,\"883\":\"17\",\"884\":-1,\"885\":-1,\"886\":-1,\"887\":-1,\"888\":-1,\"889\":-1,\"890\":-1,\"891\":-1,\"892\":-1,\"893\":-1,\"894\":-1,\"895\":-1,\"896\":-1,\"897\":-1,\"898\":-1,\"899\":-1,\"900\":-1,\"901\":-1,\"902\":-1,\"903\":\"12\",\"904\":-1,\"905\":-1,\"906\":-1,\"907\":-1,\"908\":-1,\"909\":42,\"910\":-1,\"911\":-1,\"912\":-1,\"913\":-1,\"914\":-1,\"915\":-1,\"916\":-1,\"917\":-1,\"918\":-1,\"919\":\"74.3k\",\"920\":-1,\"921\":-1,\"922\":-1,\"923\":-1,\"924\":-1,\"925\":-1,\"926\":-1,\"927\":-1,\"928\":-1,\"929\":-1,\"930\":-1,\"931\":-1,\"932\":-1,\"933\":-1,\"934\":-1,\"935\":-1,\"936\":-1,\"937\":-1,\"938\":-1,\"939\":-1,\"940\":-1,\"941\":-1,\"942\":-1,\"943\":-1,\"944\":-1,\"945\":-1,\"946\":-1,\"947\":-1,\"948\":-1,\"949\":-1,\"950\":-1,\"951\":-1,\"952\":-1,\"953\":-1,\"954\":-1,\"955\":-1,\"956\":-1,\"957\":-1,\"958\":-1,\"959\":-1,\"960\":-1,\"961\":-1,\"962\":-1,\"963\":-1,\"964\":-1,\"965\":-1,\"966\":-1,\"967\":-1,\"968\":-1,\"969\":-1,\"970\":-1,\"971\":-1,\"972\":-1,\"973\":-1,\"974\":-1,\"975\":-1,\"976\":-1,\"977\":-1,\"978\":-1,\"979\":-1,\"980\":-1,\"981\":-1,\"982\":-1,\"983\":-1,\"984\":-1,\"985\":-1,\"986\":-1,\"987\":-1,\"988\":-1,\"989\":-1,\"990\":-1,\"991\":-1,\"992\":-1,\"993\":-1,\"994\":-1,\"995\":-1,\"996\":-1,\"997\":-1,\"998\":-1,\"999\":-1,\"1000\":-1,\"1001\":-1,\"1002\":-1,\"1003\":-1,\"1004\":-1,\"1005\":-1,\"1006\":-1,\"1007\":-1,\"1008\":-1,\"1009\":-1,\"1010\":-1,\"1011\":-1,\"1012\":-1,\"1013\":\"15\",\"1014\":-1,\"1015\":-1,\"1016\":-1,\"1017\":-1,\"1018\":-1,\"1019\":-1,\"1020\":-1,\"1021\":-1},\"Citations\":{\"0\":8,\"1\":2,\"2\":33,\"3\":9,\"4\":4,\"5\":26,\"6\":5,\"7\":44,\"8\":8,\"9\":8,\"10\":29,\"11\":8,\"12\":4,\"13\":26,\"14\":8,\"15\":7,\"16\":14,\"17\":13,\"18\":8,\"19\":22,\"20\":22,\"21\":26,\"22\":0,\"23\":7,\"24\":24,\"25\":0,\"26\":2,\"27\":10,\"28\":1,\"29\":2,\"30\":4,\"31\":30,\"32\":11,\"33\":54,\"34\":17,\"35\":36,\"36\":41,\"37\":13,\"38\":3,\"39\":14,\"40\":29,\"41\":0,\"42\":5,\"43\":13,\"44\":1,\"45\":1,\"46\":10,\"47\":3,\"48\":10,\"49\":33,\"50\":17,\"51\":16,\"52\":23,\"53\":15,\"54\":1,\"55\":1,\"56\":9,\"57\":0,\"58\":8,\"59\":1,\"60\":3,\"61\":4,\"62\":5,\"63\":1,\"64\":6,\"65\":17,\"66\":1,\"67\":28,\"68\":8,\"69\":4,\"70\":5,\"71\":10,\"72\":8,\"73\":27,\"74\":0,\"75\":8,\"76\":89,\"77\":2,\"78\":8,\"79\":4,\"80\":16,\"81\":21,\"82\":32,\"83\":5,\"84\":70,\"85\":12,\"86\":13,\"87\":80,\"88\":24,\"89\":23,\"90\":17,\"91\":7,\"92\":22,\"93\":76,\"94\":2,\"95\":24,\"96\":5,\"97\":0,\"98\":4,\"99\":2,\"100\":4,\"101\":5,\"102\":7,\"103\":3,\"104\":40,\"105\":6,\"106\":7,\"107\":5,\"108\":5,\"109\":7,\"110\":2,\"111\":4,\"112\":7,\"113\":8,\"114\":3,\"115\":12,\"116\":11,\"117\":9,\"118\":4,\"119\":3,\"120\":2,\"121\":11,\"122\":1,\"123\":1,\"124\":36,\"125\":36,\"126\":72,\"127\":6,\"128\":0,\"129\":3,\"130\":13,\"131\":29,\"132\":3,\"133\":16,\"134\":7,\"135\":10,\"136\":14,\"137\":6,\"138\":24,\"139\":14,\"140\":20,\"141\":1,\"142\":4,\"143\":12,\"144\":8,\"145\":5,\"146\":3,\"147\":11,\"148\":17,\"149\":9,\"150\":16,\"151\":1,\"152\":1,\"153\":3,\"154\":5,\"155\":11,\"156\":19,\"157\":2,\"158\":5,\"159\":2,\"160\":18,\"161\":6,\"162\":1,\"163\":9,\"164\":4,\"165\":7,\"166\":6,\"167\":15,\"168\":2,\"169\":5,\"170\":63,\"171\":10,\"172\":1,\"173\":14,\"174\":15,\"175\":4,\"176\":2,\"177\":6,\"178\":9,\"179\":2,\"180\":7,\"181\":4,\"182\":14,\"183\":8,\"184\":7,\"185\":4,\"186\":1,\"187\":15,\"188\":13,\"189\":20,\"190\":3,\"191\":13,\"192\":2,\"193\":13,\"194\":5,\"195\":1,\"196\":12,\"197\":1,\"198\":4,\"199\":0,\"200\":17,\"201\":2,\"202\":3,\"203\":16,\"204\":2,\"205\":32,\"206\":29,\"207\":6,\"208\":1,\"209\":0,\"210\":7,\"211\":1,\"212\":2,\"213\":4,\"214\":6,\"215\":7,\"216\":2,\"217\":23,\"218\":245,\"219\":30,\"220\":0,\"221\":1,\"222\":120,\"223\":16,\"224\":16,\"225\":15,\"226\":8,\"227\":12,\"228\":9,\"229\":1,\"230\":8,\"231\":6,\"232\":14,\"233\":1,\"234\":22,\"235\":22,\"236\":5,\"237\":2,\"238\":0,\"239\":1,\"240\":8,\"241\":5,\"242\":5,\"243\":0,\"244\":56,\"245\":13,\"246\":21,\"247\":13,\"248\":13,\"249\":8,\"250\":18,\"251\":31,\"252\":1,\"253\":3,\"254\":2,\"255\":11,\"256\":2,\"257\":2,\"258\":2,\"259\":6,\"260\":23,\"261\":5,\"262\":3,\"263\":2,\"264\":2,\"265\":10,\"266\":14,\"267\":11,\"268\":5,\"269\":6,\"270\":9,\"271\":0,\"272\":14,\"273\":21,\"274\":1,\"275\":2,\"276\":17,\"277\":4,\"278\":8,\"279\":31,\"280\":1,\"281\":17,\"282\":7,\"283\":8,\"284\":9,\"285\":18,\"286\":3,\"287\":6,\"288\":6,\"289\":15,\"290\":2,\"291\":9,\"292\":9,\"293\":4,\"294\":5,\"295\":17,\"296\":10,\"297\":12,\"298\":6,\"299\":5,\"300\":8,\"301\":8,\"302\":22,\"303\":12,\"304\":2,\"305\":4,\"306\":9,\"307\":7,\"308\":2,\"309\":12,\"310\":1,\"311\":2,\"312\":20,\"313\":0,\"314\":73,\"315\":3,\"316\":0,\"317\":1,\"318\":1,\"319\":141,\"320\":24,\"321\":23,\"322\":15,\"323\":3,\"324\":23,\"325\":8,\"326\":4,\"327\":31,\"328\":7,\"329\":11,\"330\":34,\"331\":12,\"332\":1,\"333\":15,\"334\":32,\"335\":19,\"336\":222,\"337\":18,\"338\":8,\"339\":7,\"340\":4,\"341\":3,\"342\":7,\"343\":6,\"344\":8,\"345\":8,\"346\":7,\"347\":18,\"348\":1,\"349\":1,\"350\":3,\"351\":12,\"352\":6,\"353\":5,\"354\":7,\"355\":8,\"356\":4,\"357\":21,\"358\":7,\"359\":17,\"360\":13,\"361\":23,\"362\":10,\"363\":35,\"364\":3,\"365\":1,\"366\":1,\"367\":26,\"368\":21,\"369\":124,\"370\":25,\"371\":25,\"372\":96,\"373\":4,\"374\":0,\"375\":1,\"376\":4,\"377\":1,\"378\":1,\"379\":2,\"380\":11,\"381\":1,\"382\":12,\"383\":10,\"384\":3,\"385\":5,\"386\":4,\"387\":4,\"388\":9,\"389\":8,\"390\":11,\"391\":42,\"392\":33,\"393\":27,\"394\":11,\"395\":9,\"396\":7,\"397\":1,\"398\":43,\"399\":5,\"400\":7,\"401\":10,\"402\":3,\"403\":6,\"404\":3,\"405\":6,\"406\":10,\"407\":3,\"408\":8,\"409\":5,\"410\":3,\"411\":14,\"412\":12,\"413\":25,\"414\":9,\"415\":0,\"416\":10,\"417\":8,\"418\":4,\"419\":5,\"420\":3,\"421\":5,\"422\":5,\"423\":4,\"424\":3,\"425\":6,\"426\":4,\"427\":14,\"428\":22,\"429\":0,\"430\":3,\"431\":4,\"432\":13,\"433\":44,\"434\":6,\"435\":3,\"436\":2,\"437\":84,\"438\":6,\"439\":2,\"440\":6,\"441\":5,\"442\":0,\"443\":15,\"444\":2,\"445\":276,\"446\":3,\"447\":8,\"448\":4,\"449\":1,\"450\":1,\"451\":5,\"452\":1,\"453\":2,\"454\":4,\"455\":10,\"456\":0,\"457\":3,\"458\":1,\"459\":5,\"460\":4,\"461\":48,\"462\":9,\"463\":9,\"464\":6,\"465\":2,\"466\":5,\"467\":2,\"468\":10,\"469\":2,\"470\":15,\"471\":9,\"472\":2,\"473\":56,\"474\":4,\"475\":3,\"476\":4,\"477\":21,\"478\":4,\"479\":2,\"480\":1,\"481\":3,\"482\":7,\"483\":8,\"484\":29,\"485\":9,\"486\":12,\"487\":19,\"488\":67,\"489\":0,\"490\":5,\"491\":2,\"492\":0,\"493\":0,\"494\":12,\"495\":1,\"496\":1,\"497\":14,\"498\":16,\"499\":17,\"500\":18,\"501\":1,\"502\":24,\"503\":19,\"504\":9,\"505\":7,\"506\":5,\"507\":4,\"508\":4,\"509\":8,\"510\":11,\"511\":3,\"512\":15,\"513\":12,\"514\":12,\"515\":2,\"516\":1,\"517\":1,\"518\":11,\"519\":2,\"520\":31,\"521\":4,\"522\":1,\"523\":1,\"524\":14,\"525\":23,\"526\":7,\"527\":58,\"528\":18,\"529\":86,\"530\":3,\"531\":5,\"532\":1,\"533\":8,\"534\":5,\"535\":21,\"536\":2,\"537\":15,\"538\":3,\"539\":7,\"540\":21,\"541\":2,\"542\":0,\"543\":3,\"544\":2,\"545\":7,\"546\":5,\"547\":13,\"548\":10,\"549\":28,\"550\":30,\"551\":41,\"552\":23,\"553\":10,\"554\":14,\"555\":90,\"556\":8,\"557\":3,\"558\":1,\"559\":18,\"560\":10,\"561\":0,\"562\":2,\"563\":4,\"564\":24,\"565\":11,\"566\":17,\"567\":4,\"568\":0,\"569\":9,\"570\":10,\"571\":17,\"572\":9,\"573\":7,\"574\":3,\"575\":17,\"576\":11,\"577\":15,\"578\":15,\"579\":0,\"580\":2,\"581\":4,\"582\":14,\"583\":3,\"584\":1,\"585\":55,\"586\":3,\"587\":9,\"588\":2,\"589\":6,\"590\":12,\"591\":12,\"592\":36,\"593\":1,\"594\":5,\"595\":15,\"596\":40,\"597\":0,\"598\":14,\"599\":9,\"600\":15,\"601\":6,\"602\":83,\"603\":6,\"604\":3,\"605\":7,\"606\":0,\"607\":0,\"608\":23,\"609\":118,\"610\":32,\"611\":20,\"612\":21,\"613\":10,\"614\":27,\"615\":44,\"616\":70,\"617\":177,\"618\":163,\"619\":4,\"620\":0,\"621\":3,\"622\":7,\"623\":9,\"624\":21,\"625\":1,\"626\":20,\"627\":19,\"628\":141,\"629\":16,\"630\":5,\"631\":14,\"632\":4,\"633\":6,\"634\":1,\"635\":6,\"636\":4,\"637\":0,\"638\":2,\"639\":6,\"640\":1,\"641\":30,\"642\":4,\"643\":0,\"644\":125,\"645\":11,\"646\":23,\"647\":137,\"648\":0,\"649\":41,\"650\":4,\"651\":0,\"652\":4,\"653\":18,\"654\":18,\"655\":35,\"656\":9,\"657\":13,\"658\":18,\"659\":6,\"660\":19,\"661\":4,\"662\":2,\"663\":1,\"664\":12,\"665\":0,\"666\":11,\"667\":7,\"668\":3,\"669\":5,\"670\":9,\"671\":5,\"672\":23,\"673\":3,\"674\":1,\"675\":17,\"676\":29,\"677\":9,\"678\":1,\"679\":0,\"680\":4,\"681\":1,\"682\":6,\"683\":11,\"684\":7,\"685\":3,\"686\":11,\"687\":7,\"688\":36,\"689\":29,\"690\":6,\"691\":40,\"692\":23,\"693\":7,\"694\":9,\"695\":19,\"696\":88,\"697\":1,\"698\":15,\"699\":0,\"700\":45,\"701\":8,\"702\":7,\"703\":8,\"704\":1,\"705\":11,\"706\":5,\"707\":1,\"708\":14,\"709\":3,\"710\":8,\"711\":9,\"712\":20,\"713\":28,\"714\":12,\"715\":47,\"716\":29,\"717\":17,\"718\":2,\"719\":4,\"720\":0,\"721\":9,\"722\":20,\"723\":3,\"724\":1,\"725\":60,\"726\":56,\"727\":83,\"728\":16,\"729\":1,\"730\":35,\"731\":7,\"732\":8,\"733\":4,\"734\":6,\"735\":6,\"736\":6,\"737\":15,\"738\":14,\"739\":0,\"740\":15,\"741\":6,\"742\":19,\"743\":23,\"744\":6,\"745\":7,\"746\":129,\"747\":5,\"748\":5,\"749\":15,\"750\":90,\"751\":98,\"752\":38,\"753\":14,\"754\":0,\"755\":4,\"756\":11,\"757\":4,\"758\":13,\"759\":52,\"760\":2,\"761\":2,\"762\":0,\"763\":14,\"764\":7,\"765\":34,\"766\":1,\"767\":14,\"768\":4,\"769\":2,\"770\":34,\"771\":0,\"772\":10,\"773\":15,\"774\":2,\"775\":10,\"776\":4,\"777\":7,\"778\":0,\"779\":9,\"780\":1,\"781\":25,\"782\":4,\"783\":0,\"784\":4,\"785\":5,\"786\":2,\"787\":0,\"788\":16,\"789\":16,\"790\":7,\"791\":3,\"792\":8,\"793\":8,\"794\":0,\"795\":2,\"796\":25,\"797\":23,\"798\":2,\"799\":8,\"800\":5,\"801\":15,\"802\":1,\"803\":7,\"804\":9,\"805\":2,\"806\":3,\"807\":18,\"808\":17,\"809\":10,\"810\":3,\"811\":26,\"812\":11,\"813\":1,\"814\":2,\"815\":13,\"816\":2,\"817\":9,\"818\":4,\"819\":3,\"820\":13,\"821\":8,\"822\":22,\"823\":1,\"824\":7,\"825\":11,\"826\":1,\"827\":2,\"828\":1,\"829\":14,\"830\":2,\"831\":1,\"832\":5,\"833\":56,\"834\":14,\"835\":18,\"836\":20,\"837\":3,\"838\":42,\"839\":26,\"840\":8,\"841\":22,\"842\":1,\"843\":3,\"844\":33,\"845\":303,\"846\":47,\"847\":4,\"848\":2,\"849\":6,\"850\":21,\"851\":10,\"852\":82,\"853\":10,\"854\":4,\"855\":18,\"856\":7,\"857\":5,\"858\":4,\"859\":3,\"860\":6,\"861\":4,\"862\":0,\"863\":6,\"864\":0,\"865\":1,\"866\":4,\"867\":2,\"868\":5,\"869\":17,\"870\":14,\"871\":11,\"872\":4,\"873\":1,\"874\":5,\"875\":35,\"876\":36,\"877\":49,\"878\":45,\"879\":1,\"880\":45,\"881\":4,\"882\":38,\"883\":13,\"884\":9,\"885\":1,\"886\":1,\"887\":1,\"888\":13,\"889\":25,\"890\":3,\"891\":14,\"892\":10,\"893\":36,\"894\":3,\"895\":10,\"896\":14,\"897\":79,\"898\":4,\"899\":7,\"900\":19,\"901\":10,\"902\":16,\"903\":15,\"904\":23,\"905\":5,\"906\":7,\"907\":0,\"908\":12,\"909\":31,\"910\":3,\"911\":0,\"912\":1,\"913\":14,\"914\":2,\"915\":5,\"916\":7,\"917\":38,\"918\":48,\"919\":119,\"920\":55,\"921\":22,\"922\":5,\"923\":7,\"924\":4,\"925\":15,\"926\":202,\"927\":25,\"928\":70,\"929\":268,\"930\":3,\"931\":5,\"932\":19,\"933\":18,\"934\":26,\"935\":3,\"936\":40,\"937\":25,\"938\":9,\"939\":13,\"940\":15,\"941\":6,\"942\":25,\"943\":21,\"944\":33,\"945\":11,\"946\":14,\"947\":0,\"948\":7,\"949\":7,\"950\":12,\"951\":3,\"952\":2,\"953\":12,\"954\":2,\"955\":13,\"956\":5,\"957\":4,\"958\":126,\"959\":7,\"960\":10,\"961\":6,\"962\":9,\"963\":8,\"964\":27,\"965\":17,\"966\":2,\"967\":7,\"968\":13,\"969\":0,\"970\":4,\"971\":6,\"972\":2,\"973\":11,\"974\":20,\"975\":2,\"976\":7,\"977\":2,\"978\":7,\"979\":1,\"980\":1,\"981\":1,\"982\":0,\"983\":13,\"984\":4,\"985\":17,\"986\":0,\"987\":104,\"988\":13,\"989\":18,\"990\":96,\"991\":10,\"992\":101,\"993\":9,\"994\":20,\"995\":19,\"996\":2,\"997\":9,\"998\":18,\"999\":10,\"1000\":11,\"1001\":13,\"1002\":2,\"1003\":10,\"1004\":5,\"1005\":4,\"1006\":31,\"1007\":63,\"1008\":40,\"1009\":9,\"1010\":4,\"1011\":8,\"1012\":3,\"1013\":117,\"1014\":29,\"1015\":131,\"1016\":9,\"1017\":41,\"1018\":19,\"1019\":30,\"1020\":4,\"1021\":6}}"